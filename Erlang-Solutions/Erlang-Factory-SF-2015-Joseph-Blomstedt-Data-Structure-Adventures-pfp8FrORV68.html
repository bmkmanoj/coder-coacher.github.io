<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2015 - Joseph Blomstedt - Data Structure Adventures | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2015 - Joseph Blomstedt - Data Structure Adventures - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2015 - Joseph Blomstedt - Data Structure Adventures</b></h2><h5 class="post__date">2015-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pfp8FrORV68" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right guys yeah so as Nathan said
I'm a principal engineer I fasho been
there for years and dusted em and talk
about something has not relating to me
with react just Erlang dead structures
even then this talks act a little at
misnomer I came in hoping to talk a lot
about a lots of bait structures and the
end manic talking about one and then
talk about other stuff this is more
adventures in rewriting things as nifs
to make things fast not just data
structures but bear with me you'll see
how it goes alright so as we all know
Erlang's are really highly productive in
pretty awesome language and especially
if you you know are familiar with it and
get used to it you know it's scalable
it's distributed fault-tolerant really
nice but there's some performance that
do with their life you know erling is a
great language to be able to build good
medium performance highly scalable apps
quickly probably faster than any other
language I know of the problem is if
you're one of those few folks who gets
into the situation where oh wait I need
to be faster now it's sometimes really
hard standard library has you know data
structures that aren't the fastest in
the world you get the cases where you
run into a block contention beam you
know there's their son challenges there
that you know a few people who've been
with Erlang for you know 10 15 years you
hit that eventually you're like a
laconic that's kind of knowing and so
one thing that I really wanted to set
out and do originally a few months back
clashing a couple years back now when I
first started some of this work was to
say well could we build you know new
black boxes these nice components at our
highly performant where we can survive
motor application in Erlang but you know
or benefiting from you no more efficient
data structures more efficient message
dispatch systems and things like that I
don't know but that's talk we'll talk
about some of the work that I've been
trying to do in that space and perhaps
motivate other people to you know try to
be some of it too so you know the
solution here is hey that's right an if
of course sniffs are scary using with
extreme caution right you've all seen
this on the the you know for laying
website probably the funny part is
before this morning ever existed
originally and then it added later after
it cause lots of problems so nips cause
the problem and the thing here you know
the thing with nif so is you have to use
them sometimes you know I mean in bash
oh we use nips to interface with you
leveldb we use sniffs a part of bit cast
so most of the data databases that are
written in Erlang for example still have
their back end written in C because
that's just the way the world works
and so we use a lot for interfacing with
things but we really have a show haven't
ever used it for just high performance
you know data structures or for other
little things like that it's mostly just
meant for interfacing with existing C
code but we'll talk a little bit here of
using you know using it for different
purposes so it's actually three things
I'm gonna talk about today as i mention
data structures yes and then also going
to talk about a message dispatch so the
challenge here of wanting to send
messages to a worker pool of processes
have built an overload protection have
built-in load balancing and that's
actually some it's really hard to do in
pure Erlang efficiently trust me I've
tried a lot of us have tried and so
here's an example here of trying to
actually do it as an S and then today
I'm going to talk about also stats
collection or statistics system so
basically computing mins Max's and
histograms over events most of the
library's people might be familiar with
in this space like folsom at X a meter
or pure rolling situation they have
performance problems and so there's some
prototype here to a building a
statistics collection system as an if as
well to fill in that space so big
structures so we kind of know the big
standards here in you know standard
library we've got our dicks dicks and gb
trees and we have to have maps to all
the maps are we're slow for large sets
of data until apparently yesterday so
are 18 you know came out yesterday and
unfortunately I didn't find any time to
go and benchmark the new maps I really
wanted to update these slides button but
maybe maps are totally awesome now and
this whole rest this talk doesn't need
to happen but another case we're going
to stick with what we already do have
since also expansion but most let's
aren't running our 18 in production
right now either one day late and then
so if you were to go and just run a very
simple benchmark and this is this is an
forward insertion benchmark so just
you're inserting Keys one through the
top number here in a fold you know into
us that SAT so it's a forward for
denture interestingly you'll get
complete different performance results
if you insert the keys and reverse order
and so forth but uh and so forward
absurd here you know you we can as we
all know we know order de canto and the
numbers here in milliseconds so we know
that you know door dick gets terrible
when your key set gets large right I
mean after ten thousand keys don't
bother heck after a thousand keys don't
bother dick is pretty good mid after
skills more or less constant with your
rate and then it gets really strange i
wonder
it starts being not so great anymore
there's a pretty big jump here and then
gb trees you know wall or slow slower
than dicks was actually not so bad here
but they're slower than dick so if you
look at fetching so fetching this not on
these slides but fetching or something
dick is much better at and that's what
most of the time you don't use GV trees
we use discs but dick geometry is
actually scale very well for this
insertion case here even up to you know
the million nodes now granted that's
also because we're inserting them in
forward sorted order if you were to
invert this and reverse order gb trees
to actually much worse but if you have
all those other numbers although i think
i forgot to put him in a slide deck but
i can bring them up during any questions
if anyone cares about forward backwards
at random insertion and all the other
guys i have all that okay this is
interesting and here's the real answer
there is no way you're ever going to
store more than 10 million items in any
Erlang data structure on heat today yeah
i mean even dick just this takes more
than 10 minutes and I tried so yeah good
luck with that so what I'm going to talk
about in this talk and what of an
example is going to be at this newbie
tree structure which is you notice here
is actually not any faster than a gb
tree it's about the same because well
it's a tree data structure and so it
should perform about the same as a GP
tree the reason why we're going to talk
about it is this is the version here for
the trolling version which about 269
lines of code and it's a good place to
start and then this gets rewritten as an
if progressively and gets faster as we
do that and again as you expect it's
about the same as it hasn't really well
I should get him talk about what that
did structure is in a few slides alright
so why are what's one of those really
cool about functional data structures
and there's a true for lying true for
haskell you know it's true hero camel
you know is there immutable right there
mutable data structures looks like
you're getting kind of these coffee and
right semantics but you got official
term sharing going on beneath so the
structure shared and things are nice and
fast so you know standard people the
dipped you store some number and twice
even though we stored on twice these are
actually will take their copies right so
if i read from kicked one i get 10 hurry
from dick three I get 15 you know we
don't update it you know go do this and
you don't see was you know maps or
something and you know it would update
it's a copy it's an in-place update so
this is really cool and you know the ER
link via man has been all the functional
languages that do this after you
pretty well and it's nice that's a
really cool property because I mean I
don't know anyone's been our line
programmer for years you take advantage
of struggling although that's probably a
lot I mean there's lots of items that
are much easier to write when you're
just doing you know copy and write and
beautiful updates and you kind of said
second nature get used to that but one
of the things though is for performance
cases a lot of times we end up using s
in our code base which is fast here's X
the same benchmark wow it is really fast
this is actually the ordered ordered set
X be comparable with the gb tree so hash
that's actually a bastard about another
kassitus for this particular met shark
and it's the first actually you get to
10 million yeah I take about five
seconds but it can get there these these
guys can and that's neat but Weir's the
prom with that's right if you lose the
mutability property there's no X nap
shot right there's no way to update at
the table and have it efficiently give
you a snapshot view of that all those SS
have some benefits it's fast likes we
just saw and B it's concurrent so you
can actually update s from different
processes you can read between them
that's really cool you can't really do
that with you know dick right it's
copying right and the data stored off
peep which can be good or bad right and
in some ways it's great because it
reduces the garbage collection pressure
on a process you know normally although
it's bad because every time you access
it your then having to copy the data
into it or out of it off to the you know
been off processing p but usually that's
fine we have a really large data
structure that is 10 million keys you're
probably only ever accessing a fraction
of them a certain period of time and so
that's why it's generally works out well
for us so what I want to do here say
okay could we do something looks kind of
called above so something calves the
mutable structure so you can snap shot
it you can treat it like we treat dicta
Nordics but it also has an ostrich
structure but also can be used off it's
also faster and so forth and you can use
it concurrently between processes so
that was you know the first thing here
and so okay let's go and build a
copyright betrayed a cow be tree and so
this is fairly simple it's a b-tree that
is not really employed as you normally
implement as a tree think this is just
levels of sorted or dicks and this is
you know pretty straightforward so you
initially start out with a root which
has a this is an order it's tuples I'm
storing the value 1 10 5
5990 so the left value here is the key
the right is the value you know standard
tuple in or dicked and let's assume here
that this is a b-tree that only has up
to three elements per block so I guess
it's a 23 tree effectively that's only
because it fits on the slides in
practice should one use much wider each
race here up to probably 64 128 elements
okay and so we have the value here and
let's say we're going to go ahead and
insert other value with three I believes
at three insert yeah it's gonna be three
so we're going to insert three here
notice when the interpreter you can hear
this would be it would violate the size
property right it's going to go from
three elements to four elements or is
too big so you're going to split it and
so we're going to split it here into two
chunks the right half and the left are
the left and right half here as you'd
expect so one and two go to one brand
new or dicked and the other side goes to
another ordered over here we now have a
new route which is now referencing these
guys so blue or Internode's yellow our
leaf nodes so it's like a b-plus tree
kind of and you know again these are
these are references this is just me
trying to show in Erlang term structure
broken out and practices are nested as
you'll see in a few slides and then okay
we insert the three you know after we
doing the splitting their to us the
three okay as you'd expect again it's an
order okay now let's go ahead and insert
another number its sensor 26 and send so
which already space here we just pledged
right there again it's just an order to
update all right let's go and insert
some more numbers again now we're
inserting a number in 204 for would have
caused these things to split so we would
split and then insert the value there
and again we would now update the top
route again now the interesting part
here is the route now is actually full
so the next splits going to split the
route and the trees going to crease in
height via again so we do that where we
inserted would be injured miss Dennis
607 I should've touchy yeah it's a seven
so insert of the seven here which caused
this thing to split into five and six
and then the seven and nine and you know
you kind of get that and this is
conceptual geometries work to it it's a
slightly different approach and if you
know so there's just layers and layers
of ORD it's pretty straightforward to
reason about now this graphical
representations come strange
there's this and if you look at this and
practice this though in Erlang is just a
nested data structure which you know
internally as reference is right and it
looks like this I mean this is that
entire tree right there nicely pretty
printed on the prompt it's just a you
know our dicks of or dicks where the or
dicks in the very bottom are special
verses or dead set on the outside so
what's the actual state if you learn to
build this bee tree so I I think i only
put that look up cohere the store codes
not too hard either the splitting codes
not too hard in it erasure code isn't
too bad either the whole thing is 269
lines of code but this one's only one if
it's on a slide easily talk about so the
heel here actually is this tree is just
as data structure we talked about and
one other component it's a number it's
the height and so that's just when
you're recursing down the tree you know
if you're at the leaf or not yet I just
by knowing if you're at the height yet
so you start up with zero and increment
down and so for that we don't have to
have different two poles and pattern
match on if you're a leaf or not to
leave because if you do that you
actually it's slower so having the bear
unlabeled values with just the root
works fine so okay fine it's pretty easy
right you start up you just pull up
height and start at one and you just
recurse down the tree if you're at the
height then you know you're EE flow and
just do a regular or dick look up and if
you're not then you know you're at an
inter node which is still in order to
look up but it's an different here
because we want to actually find sort of
the value and where you are so this the
search is not quite it's not a Nordic
it's it's a lower bound look up so it's
where you are it's not an exact match
it's the place that you are that you
would fit in in short order that's not
higher than you and so you get that any
recurse count but that that's the lookup
mechanism for this the store mechanisms
basically the same thing except there's
ability to look at overflow and
splitting and then when you split you
can increase the root height and erasing
as similar as well and that code i'll
put up online for anyone cares about for
that but anyway if we were to implement
that and run it it's again the
performance numbers we've shown earlier
which again is about the same as a gb
tree although this is much simpler than
a gb tree and implementation i'm not
saying it's better the reason it was i
care about being simpler because i'ma go
direct cincy and I don't want to write
geebee geebee trees and see where is
this this nice simple or dicks of our
dicks is pretty easy to go and build
while you'd think
you taller so oh this one again no we
don't want a pit or line thing we want
something which is you know a concurrent
off heat awesome data structure that's
an S okay so we're going to write enough
to do that and once you go into growing
the seat becomes a challenge because I
wait okay now we have to do the fact
that we need copy and write semantics so
we need a copy stuff okay if you copy
stuff where are you just doing a Malik
or a new impending on your language you
know our know if you do that every time
you copy something the allocator is
going to kill you so you actually be
smarter now and do some kind of slab
allocation or both allocation so you
have to do that that's not too
particularly challenging these are all
fixed sized dick so certain size so you
can just use a fixed memory pool slap
it's easy hard parts of snapshots how do
you snapshot how do you this copy and
write semantics and do efficient
snapshot in a concurrent setting the
hard part is room for snapchat really
that's actually an easy the hard part is
how do you clean up the snapshots when
people aren't snapshot in anymore how do
you how do you free the data and form
anyone who's familiar with lock free
data structure work out their literature
you know this basically is what's known
as state memory reclamation it's the
thing we get for free in a garbage
collection language like our line or the
folks in Java get it wouldnt see you
don't have a garbage collector and so
you'd never know when it's safe to
actually delete it delete something if
you just remove it from your your route
from your tree and then delete it there
could be a snapshot that actually still
has that route in it oh wait it's
problem and then of course there's
dealing with Atomics and ordering that's
just tedious but it's not super hard
except to know what you're doing and
take care and gil it's a million sake
fault you run into along the way and
then eventually works but the
interesting part about the safe
reclamation here is that most of the
time in something like see right you're
going to use safe Lexi what plus today
much we're just games like a smart
pointer right you know a nice reference
grounded pointer and that's great the
problem is is for a giant tree structure
would you do when our smart pointer be
kind of annoying because if you wanted
to concrete a tree you want to just copy
the route you now then have to go and
copy all of the pointers or at least not
copy them but have to increment the
reference counts on everything it
touches which if you have a you know
really large billion you know element
tree which has you know maybe 10,000
actual nodes that represent that that's
10,000 you know-- reference count you
got to update and yeah you can do things
like proxy objects to try to reduce it
but still there's a lot of bookkeeping
you need to do there and so that's not
the approach you want
instead we want to look at is other
types of sick memory like impatient with
which is a few out there people might be
familiar with hazard pointers Epica
reclamation pass the buck there's
various ones out so you can go read
about we're going to go with epic
reclamation so epic reclamation I'll
explain the stop is what we end up
eating here although we extend it a
little bit so the idea epic automation
it's a grace period detection system SMR
system it's similar to our show you so
our see you mechanisms like in levar see
you or Libby you are see you or if it's
called arts another form of grace rekha
detection as well but implementing a
different way epic reclamation is
actually very simple the idea here is
basically like if I have three threads
here and there reading concurrently and
threads two and one here started reading
this data that's actually being deleted
by this writer right now but since it
was deleted while they were reading it
they actually still have pointers to
this data like you know they officials
say the root of your tree they still
have that route and they're still
traverse netroots even though you've
logically deleted it now you can't
actually free the data yet and at some
point in time this guy's going to call
this a synchronized that's the basic
primitive of epic Proclamation here or
any disgraced period systems where you
say synchronize an afferent
synchronization is done there's
basically blocks until it's safe for you
to free anything it's you've detected a
grace period at which anything that
could be deleted before you feel
synchronized can no longer be used by
anyone afterwards and so that kind of
makes sense here whenever this snapshot
ends in this snapshot ends its that's
when this this end so it's the
long-running snapshot there again
there's different ways implemented the
way this is implemented in epic
reclamation is based on this notion of
ethics so there's a global you pop that
you have and then when you go and when
any of these threads goes and goes and
modifies the data structure they
basically copy that so they contradict
will be bak they have their own version
of it and every thread that can modify
this data structure is known so there's
a list of all of them and they're
copying it to their own version of that
so if there's five threads you have a
little slot 25 slots of which each of
those threads when they go into doing
this contradict global epic into their
slot and what happens here is they just
do that the comp yet they do what
they're normally doing and then when
they're done and then the
reenter the region in the future again
you know they would copy it again what
happens here is the the writer the guy
who does the logical delete here when
equals synchronize here synchronize with
synchronous says that's where the smarts
are which synchronizes though is it's
going to block and tell it is seeing
that all of the possible readers are
either not active so they're not in a
region at all or they're in a region
where they have the epoch that matches
the global upoc so they're at the
current value if they're at the current
epoch then what the synchronize does is
it increments the epoch by one and then
it once again rejects and tell everyone
hits that epoch again to increment it
yet again the requirement here an epoch
reclamation is you have to income at it
twice before it's safe because there's
epoch that people were in before that
was unsafe and then there's all steep
awfully entered into when you were
starting summarization it's still
unsaved but it turns out tues enough
after to your fine and there's you
notice there's a research you know
thesis in the UK that actually proves
this so to joining but the deal here is
you go in here and you can see here
right this guy still has a reference to
this object he does he did here as well
he still does lean into here but this
one still does here while this one
doesn't have a reference to this object
this one does and so one is still not
safe because he could still have a copy
there but once they've both transitioned
to two at this point in time there's no
way they could have referenced this
value though it's logically to delete it
back here and so yes it is clearly safe
to delete it over here okay and the way
you type reclamation normally looks and
so free pop reclamation here after use
implementations and concurrency get so
concurrent sequence of well-known c
library that's out there and the
primitive though basically look
something like this so it looks kind of
like a critical section when you're
about to go and so a reader does it
again and an end kind of you to lock and
unlock around a region this is where
you're copying that you bak into it here
you're saying you're overactive
synchronize pieces single realize i just
talked about sir
and so you know begin and they do some
stuff whatever this runs forever the
writer comes in here you know he deletes
the old root here so he updates it's a
copy and right he does some switch here
to the new route some atomic you know
swap here the problem is is that you
know he can't delete this yet because he
there someone over here could have
actress it while he was updating it but
the synchronized after second eyes has
done it's safe to delete it you can do
it here now woohoo and it turns out you
also need to do synchronization blocking
this can also be dis pulling so every
time you come through here you just you
check to see can I do it yet oh it's not
safe yet okay oh it is safe okay well
then go and delete some stuff and she
can accumulate a garbage list and just
throw it away whenever it's safe but the
blocking the simplest one to just you
know conceptual II explain how a the
problem with epic reclamation is this
only works for short lived readers it
only works in a case where I'm going to
come in do something and then
immediately leave because this guy says
he's going to blah so basically the
writer can't block readers but readers
can block the writer indefinitely so
longer you have a breeder who's reading
into a snapshot window the writer of
this forever stalls and so this makes
sense you're a cocky and right structure
works like go in give me the current
pointer at least not not a persistent
snapshot like you would have an
immutable you know my dick version 1
version 2 into conversion three that are
on my he panned I'm use them however I
want to use them those are long live
snapshots those aren't what you want to
use hearing so that's generally the
problem with you by Proclamation is this
but we can solve that by extending the
declaration a little bit to also have a
notion of you know more separate
snapshot concept so we take this and we
extend it a little bit and to look
something like this so in addition to
their being for every node every reader
a slot where they're copying their their
epoch the current goal Levon there's
also this notion of where they have a
snapshot slot
you know in a dynamic system like Erlang
we're doin ifs every time you create a
new snapchat actually create a brand new
snapshot slot but in this example here
we assume there's a fixed number of
readers that have some fixed snapshot
set and the difference here only
difference here she begins to get the
route he calls the snapshot first and
ends the snapshot here is we've
introduced this new thing called the
global upoc over here probably should
call their generation so NE pas but
these no worky popped twice now but
basically every time the writer moves
along to a new phase he has a new
generation this is kind of a vector
clock would be in a distributed database
frontier a dot and the idea here is that
all the objects that you allocate are
now also have a birth date and a death
date so the birthday is when they are
allocated so when they're allocated they
get the birth of the box that they were
created out from the writer and whenever
the writer ends up deleting them
logically they get the death date at the
time that they they were deleted and
these are just pushed into some garbage
list that gets collected at some point
down the road otherwise so basically the
same thing right same logic your ex oh
I'm adding birthdays and death dates now
and an incrementing this epoch here so
what's the collective like well the
collect still has the epoch synchronize
we still need that that's to know that
we've moved to past win the last so the
death date is associated with the route
still so that route is still generated I
point in time we still have to wait
those two sm re box before we can
actually trust that death date where
that death date tells us that no one can
have anything before that death day so
we still need that so that's why we have
to synchronize but after that though we
can even garbage collect anything that
is no longer live live meaning that it
is not there's no snapshot that contains
an overlapping set from that guy's birth
to his death so pretty straightforward
you go through all of your list of your
snapshots again this is a list of
everyone who's a reader you know who
they are and you look at their that you
pop that's listed in there and you say
okay if this epoch is for every item in
the garbage list if someone has an epoch
which is greater than or equal to my
birth and less than my death then they
can still see me right there in still my
my lifetime and so that value must live
but if no one does that value can die
and then you just push these into a
little local state here and down at the
end you know if you live live still we
keep them if he's not we throw them away
you know and again delete could be
whatever we actually do here and then
you know the end we just
around the the ones that we keep and in
the future new garbage collection will
deal with that and so whenever a
snapshot goes away eventually you will
garbage collect everything you can the
nice thing about this is that if you
don't have any snapshots you're going to
be called reflected immediately because
you know the writer is always at a
certain point in time there's no
snapshots the second he calls epoch
synchronizing call it's clack he's been
free stuff she are not accumulated
garbage in the front likewise if you do
have a snapshot in the past you actually
going to keep old stuff alive but all
the new allocated objects in the future
that you create that that those gonna be
thrown away if you rewrite them so
anything's newer than that snapshot will
also have cleaned up quickly as well
unlike red lollipop reclamation wears a
red lollipop reclamation that's all you
were using you can't clean up even the
new stuff until everyone gets moved on
to the new be bak and so that's one of
the things that's nice here is that
we're keeping this epoch short this
begin end but this snapshot object we
created here this thing can live
indefinitely that can live for however
long we want to live on and that's we
can pass up der laeng as you're lying
you know resource or whatever that we're
going to use okay so this a lot of C
code I matter link factor on talking
with C code because it's the hard stuff
right so this is all the stuff to get to
to then say okay now i can write an if
in and that it has immutable components
and signal no commission so now let's go
ahead and start doing that oh if there's
one more thing to talk about and so
here's the thing that's all about
reading the problem is is writing it's
also a challenge right so there are
really fancy concurrent data structures
you can build that allow you to do
multiple writers be trees are a very
hard one to do that way they do exist
but it's a very complicated algorithm to
do concurrent p trees and even then
there's still blocks or just multiple
locks it's finally sharded locks and you
have just bunch of different locks and
so you're still locking your slot in
different paths and but pass they don't
overlap aren't blocking so one thing we
could do is we could just to have a lock
around a single writer and do a single
rider be tree with short that's what the
Erlang code was and that is my
additional plantation or you can
optimize it a little bit and use this
technical flat combine which is another
technique that is used to tour to
optimize it still has a single lock but
it works out better in practice this is
weird thing where you reach beverage
like yeah this should be unsurprising
but in practice that work that better to
suppose the way to process are designed
it's because you have less cash
contention you have less fighting over
this structure so what flat combining
basically means is that we have multiple
people who want to update a data
structure instead of having them all
weight on a lock we're one day gets a
lock does is one up to eight leaves and
next I get to lock this is when updating
leaves and
does it you do a case where you try to
actually call less it where the first
guy who gets a lock after he does the
work for everyone all right everyone
within a certain time bound and so he
gets a lock doesn't bunch of work real
quick which can you know it's all in his
cash you can dump it really quickly and
then he goes away and you're not ping
pong and it's locked back and forth and
there's a few aspects tape on
reclamation the main thing here that was
at each actor or thread they have their
own little state here that's threadlocal
that is where they put the little
request for a b-tree updated be right
this value or delete this value those
are the two writing type of operations
you can do and you would go and store it
in the request field and then you're
going to spin on this ready field so
you're going to do a spin way
potentially on this ready feel the
difference here is it's not a single
lock it's a single in your cash field so
if someone else is there no one else
gonna be ping pong it on that and so
everyone has their own separate one here
and again I'm tungsten weights in
practice interline you don't you spin
weights here but and I'll talk about
that in a second but it would happen and
then someone's going to the lock so
we'll just kind of illustrate this so
one guy comes in here he wants to do the
update he puts a value in the request
field here he also says as a structure
currently long it's not currently locked
so he actually becomes the right
combiner now so he emptied locks it and
he starts doing the work Wallace
happened someone else concurrently comes
in here he sees that it's already locked
though so he puts his request here and
just busy waits on his ready field here
same thing with this other guy he puts
every question in busy ways this guy
over here while he's running he's going
to do his request then he's going to
notice oh wait this request is here he's
going to do that guy's request and set
the red flag true and then he's going to
notice this guy his request to and he's
going to do that as well and so there's
one this one schedulers rights going to
the work for all these guys or this one
whatever not this one thread and then
when he's done you know he'll be done
these read these guys will come off
their way here they'll be done and
everyone I move on in life and then
you're faster because of the fact that
you did the right combining in practice
how this option implemented like Coach
Erlang is there is a brief busy wait
here but there's a lot more metadata
where after busy wait the scheduler the
NIF actually set something up or get a
message instead when it's ready and it
returns back her language special
applause the scheduler threat forever
and of course i was using are 17 or
newer there's actually built in support
now in the f api to actually do a
suspend from the NIF just like the biss
can do or you can break out of it which
i could do as well but you know this was
implanted are six
original cell responding back works but
effect Lee the same thing you coming
here and you spend a brief period of
time on that and then you're not just
run back another lying and an event
eventually they'll ping you when the
right combining stun okay so all that
background of how we have to handle this
to make this actually work the way we
want to a head of garbage collector now
let's actually talk about how it looks
like so once you build this you have
something looks like a dick but this is
actually an if with term sharing behind
the scenes this is concurrent off heap
data so you know you create a new one
you store value you store a different
value and you know you get 10 or 15 just
like you would with a dick which you
know would not be true for ads and then
these two copies here are very you know
if this was a million elementtree here
and you updated this you would literally
would have only one value different
between it's about one path the route
would be different and whatever values
got to copy down from the root down but
that's it everything else be eternally
shared so this is really cool so how did
this perform that well it's faster than
the b-tree code before the 19 f yay it's
still slower than s but it's it actually
is faster than any since we know that
this was basically as fast as the gb
trees which was fashioned dip to the
larger sets we therefore no Bisping
facets this is still faster than any of
the on heap in Erlang structures outside
of maps because I didn't test maps in
our 18 maps in our 17 were terrible for
this for a million keys but so maybe
they're faster but you know we get some
decent performance here we are slower
than that so because we're copying stuff
that's is doing in place updates you
know it's not it's not doing copying
right and so we're slower here but we do
have a benefit here that we are we are
snapshots right we do have a mutable
properties now so we did you wanted to
do we have an immutable data structure
now that is almost as fast that's great
too bad we couldn't get there but I
think with some more optimization we can
I think we can get there the interesting
part is is this actually is slow for
reasons that are a little silly this is
slow partially because in Erlang every
time we create this this is a this each
one of these is actually a new x alla
resource so x resources are sorry nif
resource and so there's a notion of nif
where it's basically a garbage collector
resource where you know whenever the
airline garbage collector collects that
term that'll eventually call it a
college instructor over and the NIF
which then does it and so each of these
are actually snapshots like back in our
old case here we were talking about
snapshots you know here like these long
live entities that prevent data from
being wrecked blamed the thing you need
to scan over this giant list here each
time those are every one of these
references that you see over here well
unfortunately if I have a loop that is a
recursive loop that you know comes in
has an accumulator adds value to it and
recursive you know standard tail
recursive loop you do interlink to feel
something up you don't immediately
garbage collect after you recurse you
just keep filling up a bunch and
eventually you garbage collect at some
point in the future when if your minor
collection or whatever gets triggered
and so if you actually profile let you
end up seeing them will get to a cage
will have like 200 snapshots that are
not even there but we have to scan over
them and keep data alive because of that
and then eventually or line clears them
all and then the garbage collection
around freeze them all and as you add
more and more of your data and your data
gets larger in your process gets larger
you have larger and larger these
snapshots that stay alive before they're
collected and so unfortunate early on
garbage collectors actually it makes it
harder for us because we're keeping all
these snapshots around you normally when
you think of I'm gonna build a
concurrent data structure you don't
think and I'm gonna have 500 snapshots
of it live at once you generally have
you know maybe ten you know in your
application logic but because the way
the garbage collector works here if you
want to do a thermal temporary copies
stays alive for quite a while so that's
one of the performers challenges we have
okay so let's say when not have the
immutable property yet let's get rid of
the mutable stuff and action say okay I
like at let me just make something looks
like X so you know this is now the M
version of this this petri nif so we
have a name table basically give me a
name and we just insert into that name
and you know these just come back as
okay you know you're not getting a
mutable thing this kind of gets its you
know mutable state somewhere and you
know just like that's it is a mutable
state so you can read it in another
process just fine you
over this value it is ordered though
unlike ettes though you can actually
still snapshot this there is a BTN
underscore and snapshot which does give
you a snapshot then which works like a
regular it works with the same API yet
here so you can fetch it although you
can't just do it anymore but you can
fetch from it that's cool so it is that
one nice features you can do snapshots
of it this looks like that's but it's
the slower the Nets but it's faster then
it's getting much closer now so before
we have the bottom nif down here which
you know still kind of terrible this
one's in striking distance you know it's
getting closer it's still copying that's
why you know at entity we're still doing
copying it's also order it's also
against next to work and it's not as
optimized as that's has been so you know
my set out to try to make something that
was faster and better than awesome and
well okay it's not faster but it does
have some interesting properties and
that's where we are and I believe that
is the end of the data structure part of
this talk alright so so now we can look
at a few other things these are other
nif what these are other nif related
things here that are more complicated
and just like a low level data structure
these are more like other perimeters
I'll be useful to have in a hive
comparing system hyperforin system so
one of these is a worker dispatcher
early so i'm gonna call this is a
problem that you know probably most of
stew user Lane have have to even have
this VF supervisors hot supervisors the
dealers you have a lot of processes
they're all sending messages to a worker
pool and you want to efficiently send
them to the message in the right worker
pool and you also want to perhaps at
overload protection as well so maybe you
want to have if i have i do have too
much work where everyone's message
queues are filling up it'd be kind of
nice it just discard some of these
messages so load balancing overload
protection etc in react we actually have
this we have a thing we built a few
years ago I wrote a few years ago called
side job which tries to solve this
problem and it was mostly a work balance
in / system there was also largely a
overload protection problem we dealt
with it after a case we're just people
have clusters a certain size and
suddenly someone you know they have to
get really popular and their user base
triples in a day and there's really
nothing you can do if you're if you're
accusing all that data in your view node
erling mailboxes hurling us gonna get
really sad really fast and so you need
to discard that data as soon as possible
so overly pressure so what side job does
and this was implemented many ways and
we written in many ways and review time
that's pure line until was fast and i
don't know if i can make it faster maybe
but what we did Evelyn is this is using
there is first set up of their set of
workers so in this case you're the six
and each one of these work with as a
counter associate with them and these
counters are sorta nets and these
counters are partitioned so it's a
different counter free cheese workers
and reason atomic x increment and anyone
knows anything what apps and play debts
matter that's like the fastest way to
dads is atomic comenta to partition
counters and hope we get lucky that they
don't map to the same sub lock a nets
most of the time they don't in your fine
and if you really care you could
actually control the names here because
we don't care about the names but anyway
atomic partition counters and what you
do here these are all representing
message counts so every time you're back
to go send a message you're going to
increment the count and and then every
time the guy gets the message he's going
to decrement to the count before we ask
you does the work pretty straightforward
right how does this work when you
actually want to go send a message so
when you want to go send a message again
we have lots of prosecco send this so we
do is we actually showered into this
table at a certain point we jump in a
stable at a certain point we hash into
it based on your scheduler ID that's the
hack we do so the act is aligning you
can do a rolling system info or one of
those calls and said can you give me my
schedule ID and you'll find it on to
that I do you one or two or three and so
it's a okay each schedule ID that's
running concurrently will jump into the
table at a different point and they will
try to send it to that guy first and
then if they can't send to that guy that
will move to the next guy round robin
and center jump in a different points
you know you're going to flow of
contention right let's assume in this
case here that the max count is eight so
after eight we throw messages away we
tell you overloaded so this guy over
here he tried to send a message he sees
the counter is still five he increments
it to six cool he can send the message
and so he sends a message to this this
this pit whatever it is 626 guys this
dude over here is trying to send this
guy its eight though it's full so he
moves on that's a that's full too he
moves on oh wait this guy's not fall he
can in command who sends it to that guy
okay it's a pretty straightforward
design here you know it actually works
pretty well I mean you know truthful to
build you perform status to do stuff
there's an X things you play around with
what's the right number of workers have
your prediction things I put your keys
but you know it works it's still
not the greatest thing in the world
right if you think about this that's a
lot of work for what is kind of just in
front some atomic counters and subtract
some atomic counters so which probably
just use an if for that right so we do
I'm not gonna show to see code but i'm
going to show the the API of this noon
if that we've built this is called the
dispatcher it kind of Sekhmet stable in
some ways you try to named dispatcher so
add resource in this case your pests and
then you can have listeners who register
to that so i can say i'm going to listen
as a test listener here's my Pig and
then who anyone who wants to send it
just says hey give me a give me a pig
one of the pits valid kids and
internally it's doing though the locking
in the load balancing and the overload
and this can fail so actually this
shouldn't be a fine that should actually
a case here but this actually can return
this after you returned air or
overloaded if it's overloaded and so
this may fail and then you're supposed
to throw the message away but for the
slides here it just we assume it works
and so not too terribly challenge oh
yeah and when you get the message you
have to a kit down here interesting lead
run as fast i thought it's only about
one point times fast or under contention
that's a lot of contention like you have
to have you know be like a 16-core a
larger box with the schedule thread and
lots of process running and doing it so
like on my laptop it's like forty
percent faster rather than like 1.5 x so
i don't know probably more to look into
their i would expect that would been
faster but but who knows there's some
more work to look into that but that's
one of the things that i think would be
a very useful primitive as well if we
can get this right in a highly
performant send message to someone as an
aside there was just in general using us
just as a way to discard messages is
useful we one thing we don't have in
Erlang is you know bounded message cutes
but this basically gives you a bad mood
message queue
I think we had a heated pull request a
few months back that try to have ended
message queues to der laeng and that
didn't happen and again it's hard to
change the actual vino vm edges and not
worry about breaking something but
having some external library we just do
something like this which then yes
behind the scenes it does send a message
but it before it sends us messages is
throwing things away if you're
overloaded might be the way to do it and
so that's you know just conjecture there
but one approach but for that problem
alright and so now we're going to on to
statistics so in react we use folsom and
then now we use X meter which I think
still uses fulsome to some extent
there's some weird mix of things there
and these are mostly pure or language
data collection system so we were a
database that likes to collect stats
metrics are useful right and you want to
know everything and we collect lots of
high expensive staff lindsay's we
collect histogram for everything because
just knowing counts is no fun we want to
know what your 90 percentile is your 50
percentile all that but that's kind of
expensive in fact it's been a huge
problem in general for real for years of
rewriting stats middle part times so the
challenge here is you want to do simple
stuff min and math is pretty easy means
pretty easy cowans are preferably using
the arcs of sashes latency percentiles
that's actually kind of hard and then
there's just the reality here that we
just keep adding new stats like every
new major release of react to go back
and look at it like when we have to
document the updated occupation like oh
we added 15 your status release or
something it's like something crazy like
that because it's good to have the
visibility when you're supporting a
product but these you know these aren't
free there's work that has to be done to
keep track of all this data and so the
spinner shivery up for years we fixed
counters counters we're really slow they
used to be a single X increment atomic
per metric and of course we're keeping
like 100 counters and so that still was
a problem even with individual account
errs per metric we optimize this in
react 13 as the way most people who
authorized counters would which is we
have scheduled partition counters in it
so we basically have the counter metric
name underscore a schedule ID so if you
have eight scheduler heads there's eight
counters and then each thread check
schedule 80 increments appropriate
counter for that and then when you
update the actual
counter we a great those ones that works
out very well and is been great of
histograms are so challenge they're the
problem because you can't really
partition them as easily and they also
have this this crazy state space because
we use red for sampling and retro
sampling is you know you you fill up a
red form of data and then once it's full
you then every time you got a new data
you randomly throw away your data up to
a certain size within that window and
that's challenging do that because you
need to know your size you need to
consistently be able to update it to
figure out the exact way you can play
games with it but to play games of that
you actually lose some of your
statistical correctness so we try not to
do that so let's write an if so you can
use min and match the Atomics there's an
atomic max you can also an atomic min
but basically while around in exchange
and you still do normal red for sampling
so these are non partition still shared
central is for sampling the difference
here is it's in c and we can use a
single atomic increment to figure out
where you are that's the really the only
thing that stone if it's actually shared
is the increment everyone needs to have
the correct increment and everyone then
has from that be able to figure out how
they can which thing to discard but then
having concurrency of who discards what
first and all that and the rest does not
matter that it doesn't change the
statistical sampling properties
whatsoever and so the only thing
actually need any consents around is
just that camp the counter and so the
atomic increment is fairly efficient
lease on x86 machines which have an
atomic in current internally might not
be worth anything on arm or something it
doesn't but at least for this if we were
to go and bunt this I can't compare to
react directly because this is in a
different form it's like stats as fast
as possible one second staffers react us
like 60 second stats but I can tell you
what a micro benchmark of this does so
Mike revenge Martha does 200 metrics
where each metric is doing 10,000
updates per metric or 10 million sorry
and you have anywhere between 8 to 16
workers doing all this work so lots of
concurrency it takes about 40 seconds to
run which ok I seems kind of long but
that's 50 million events per second
that's pretty good I don't think react
does 50 million stabbed updates per
second i think it probably does like
20,000 so but we'd have to see and
compare this but this is something that
I definitely want to try to get
integrated and compared to
what we use internally for this system
and so the future we could partition
retro templer there are ways to actually
do partition which were sampling where
you're doing in a fancy way that you
could guarantee your sickle properties
there's a blog post in 2007 by this guy
works at Google who talks about that at
some time I was wanting to try but
haven't got around to doing and then
other than that i'm basically done nips
can help us for certain things i think
we need to think about though how can we
build cool reusable components and black
boxes rather than like what a lot people
do eventually where they just abandoned
Erlang and be right there into our
application and see or something oh
let's print that by you know just fixing
the things that are that are pain points
so you have reusable components and if
you're kind of hard though because we
have to deal with things like you know
garbage collection said memory
automation and allocation copying things
that are lying rd that's rush for free
we got to now figure out ourselves so
maybe we you know either you'd have
taken ideas that already in Erlang and
actually extract adding them to the you
know the NIF api or you could ask
perhaps you some of those features would
be kind of neat like the Erlang
allocators you can't really use from
this i mean i guess you kind of can if
you know the right internal thing to
cast and use but it's not part of the
Blessed api you know there's just a
standard unit allocate and you know
things like save Emma Commission you
know what should we do that you know
what's the work in that space to do but
an earl n plus nips are even harder with
like the garbage collector problem
talked about earlier where you know
something that should just be thrown
away immediately and like it on the sea
version of that that I tested it Richard
rigidly which is just using you know Rai
I you know so the second one 12 stack
it's free erlanger is keeping 200 of
these snapshots around now which is
blowing up our snapshot scans that's no
fun but if we you know I think it's
worth it work it it's worth it yeah typo
there but it's more to do and I have not
yet pushed anything up there yet and I
don't know what that's going to be
called or where it's going to live it
probably ventually wunderbah show but
this will be a stable pointer that
points to where it will be at some point
when this code gets out next week or so
so my g2 pole / GF 2015 and that's it
thanks for the talk it's very
interesting heavy guys search for
research any potential memory
fragmentations while using any of those
data structures and those approaches I
have not looked at memory fragmentation
and then this work directly now not at
all it is just interesting because
apparently TS in some cases can actually
cause very serious memory fragmentation
memory doesn't really get reclaimed back
by the vm because of that it's actually
interesting to end of figure figure out
how you memory usage changes we use
different data structures like that now
if you do any research like that please
just post it somewhere is there will be
awesome more questions for Joe bones
dead
and they're making you walk in on the
next one's in the front I'm just
suggesting piston what was that there is
a very sampling algorithm that you use
for your histograms what about it what
percentage of sampling do on your
opportunity mr. Graham the fair data
if you have 100 hundred yeah sighs a few
hundred what what percentage of that is
that oh yeah actually i don't i don't
know if i only had what it is I just
caught me what we had in react and don't
remember what it was but that's some
implementation we had before I can look
it up I can look up offline taught you
mm-hmm
says that there is no
just concurrent
so we actually never really did direct
pinion right i mean we mostly use the
schedule ID as just a uniform
distribution function right i mean it
was just I wanted to partition updates
which does help that absolutely is a
massive win but it doesn't really needed
up anything to do the schedulers
scousers was an easy way to map
concurrency to unique updates aterna
there actually isn't the best approach
which is why the dispatch one's slightly
better than side job there your chant
there are cases when Erlang decides to
you know do thread compassion right i
mean it's very you actually wanted more
useful things about the schedule or
sometimes direct compassion is usually
good things so you have fewer run queues
running schedulers running your code the
problems are cases where you might get
down only like two schedulers running
your code and so now if you're using
your schedule ideas your metric to where
you hash into this range you actually
have a much smaller set of updates now
and even though you a bunch of different
process so it's weird because it's
entering trade-off it's like well okay i
don't have the concurrency anymore so
the concurrency problem isn't there to
up to workers is no big deal I don't
need to partition for concurrency
reasons but you probably the problem is
in that approach the partitioning also
determines how your partitioning to the
actual workers like the run to you the
message queues and so while concurrency
isn't a problem anymore contentions not
a problem you are now it's not really
doing any kind of like uniform
distribution of your work or spending
most your time sending your updates to
one guy and so his message cute gets
really large and you not have any old
bottleneck you know one person doing
that and so after I think scheduler a
threat to probably not the right way to
do that for the mapping now for actually
pinning things simpler threads we we've
done some work before with that but not
not in the context of this I can only
speak to that but I know we've done
stuff like pinning all of reacts vinodh
specific cpu cores and seeing how that
would happen and other things like that
and it's interesting because it doesn't
help sometimes it does sums it does and
you have to like get it just right and
you know that might be great if you're
the person running you know a system I
got in house but like for us will be
sell it to other people who run it we
have to configure every single cluster
in the world to be just right for how we
pin stuff and that just that's a
challenge no yeah oh oh the mapping yeah
yeah so the mappings actually bad for
not the concurrency reasons but bad just
because it also matt determines the
distribution of the data
and that's that's your problem something
we want to fix question
you had mentioned shared pointers did
you ever see her Sutter's talk from
Sydney con 2014 on using atomic shared
pointers to do the sort of thing that
you end up using epic score yeah and you
can this you can use atomic sure
pointers on the problem they saw the
same problem the issue though is the
issue is not the shared pointer itself
the issue is just that there's so many
of them you'd have a shared pointer for
every allocation you have and so if you
have a deep tree with epi proclamation
the only thing I need a copy is the root
node and it's epoch and I'm done I don't
actually ought to go and increment all
of the reference counts for every
allocation that falls under all the
paths of that tree which is I
traditionally would do it and so that's
the main challenge but yes you need
atomic shared pointers period to even do
it in the first place and that's after
the challenge katana mature pointers are
actually not supported every platform
right now either because it actually
either needs a lock or a double word
compare-and-swap or other stuff to be
inserted corner cases like this is weird
things about how it works so try and
like run atomic sure pointers on a on a
Mac that's not like 1010 it's very hard
for example see you lying to compile
that way but but yeah it's an approach
all right one last question anybody
I will ask the last question I was Kim
an hour 16 they added the native bum
production council mmm yeah I need to do
so I need to be some of that into play
with that I mean we've done years we
have years and years and years of that
plane but the schedulers properly do
introduction on all that Norman myth
black magic sorcery that you do so
that's kind of old hat and we could do
what we need to this I was much more
interested at least this work of this
point of doing this new stuff like how
do you remember acclamation efficiently
how do you make it work with a card
collector that's stuff that's new that I
don't have experience with ya reduction
counter than that are useful turns out I
don't that's a big problem though in
these benchmarks because these are these
nibs are fast like the you know I said
before I have the spin weights on the
flat combining but then if it goes out
for certain period of time it switches
to a message where it sends a message
it's never triggered this fall back
unless in the test case where I make it
have to trigger that because it just you
know you're going in your copying a
pointer and you're updating a few things
are done so you probably need a system
with a lot more concurrency than I've
tested on to have that problem all right
another round of applause for Joe long</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>