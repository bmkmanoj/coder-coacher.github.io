<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Dean Wampler - What's Ahead for Big Data? | Coder Coacher - Coaching Coders</title><meta content="Dean Wampler - What's Ahead for Big Data? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Dean Wampler - What's Ahead for Big Data?</b></h2><h5 class="post__date">2014-01-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PXpRq2LQJSE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right why don't we get started my
name is Dean wampler and I'm going to
host the track today so let's do the
first talk and I want to talk a little
bit about the talks you've got lined up
today after I'll tell you what I'm going
to talk about in a minute but after me
Stefan karpinski is going to talk about
Julia which is a really interesting new
language that tries to modernize
data-centric languages like if you've
ever used to our maybe the Python math
libraries that kind of stuff and Julia's
trying to aim at that market the same
way that say Scala is like a successor
to Java that kind of thing pretty
exciting work there I think cliff click
who was kind of famous for his work on
the JVM hotspot and so forth is now
working on a database system he's going
to talk about an API for distributed
computing let's see I think after lunch
we're going to have Simon will now is
going to talk about elasticsearch which
of course is one of the important
subcategories of big data that I'll
briefly mention this morning Jessica
karez is a really fun speaker to listen
to she's a friend of mine from st. Louis
in the United States and she's going to
talk about sort of what how working with
data affects your code you're the kind
of work you do and kind of relate its
really to what was discussed in the
keynotes as well they only have mark
Needham exploring neo4j closure or the
sort of graph computing with examples
from like football I think he means i'm
not sure if he means like real football
or hand egg that we call football in the
u.s. if you've heard that joke before
anyway all right so let's get started I
hope you'll enjoy the day whether you're
here or in one of the other sessions so
I'm going to talk a little bit about
what's ahead for big data you can
actually grab the slides right now at
that last link i like a lot programming
calm / talks and if read ahead side you
want to leave or not it's okay and you
can spam me at these addresses just
briefly what am I I bet this is now dead
okay yes I work for typesafe which is a
scholar and now reactive stack kind of
company I just joined them recently
before that I was basically doing
big data for the last several years
mostly in the Hadoop space I'm a
co-organizer of the Hadoop group in
Chicago where I live this is all these
moments I think all these photos are
from Chicago and also I founded the
Chicago user group and then I've written
a few books but two of which on the Left
need to be updated because they're
rapidly growing out of date all right
enough about me let's talk about big
data so here's a couple definitions big
data is anything which crash Excel or
maybe small data's been fit and RAM big
data is when not fitting Ram maybe a
better definition and I'll let you do
the math on the right to make sure it's
correct but maybe another definition
which is really better is that it's it's
a buzzword right and I actually don't
like the word very much because not
everybody has truly big data like you
know Twitter or Facebook scale but
essentially it's it's data that's kind
of stretching the limits of what we can
do with traditional tooling whether it's
by size or by cost and so people are
looking for alternatives and as far as
data goes and so I'm going to try to
build up a little bit with sort of
foundation about you know where we're at
and what we have to do differently to
address a data-centric computing is it's
I mean computing has always been data
centric to some degree of course but now
it's even more so than ever that's
driving this so there's trends in the
industry that's driving architecture
choices that will take us up to what
people are using today what's wrong with
what you're using today and some of the
things that are happening is we're going
forward to better address our problems
so I see three trends right now
happening just to frame the discussion
the first one is obvious this was
mentioned in the keynote by Francesco
that data sizes are growing people are
collecting more data there's they're
saving more data that they previously
thought was not useful and now
reassessing that like for example you
can get a lot of useful information out
of just server log data about doing
diagnostics predictive analytics a
really interesting application I was
loosely involved with and consulting a
few years ago was storage area network
devices send data back home constantly
and it's a lot of diagnostic data and
what they're
starting to do is predictive modeling of
that data to look for indications that
something's about to fail on the device
send the service guy out there to
service it before it fails so the
customer never sees any downtime other
than that you know the quick turnaround
time for the service call so a lot of
data is growing but an interesting trend
is that we're sort of D emphasizing
schemas a little bit so what we're
pulling in all these different sources
from raw text like I want to do text
mining on twits up to it well twits who
are tweeting let's put it that way I
want to on the other hand maybe pull in
traditionally structured data or semi
structured data which is like log data
you know it always starts with like a
timestamp and a server name and its
severity and then you get this noise at
the end it's the actual message so it's
kind of semi-structured another cool
trend I think is that we're right we're
starting to realize that building models
of the world in our code is not very
scalable and not very flexible it's
actually better to work with fundamental
algorithms and let the data drive the
behavior and one of my favorite examples
is self-driving cars this is the second
generation of Stanley the Google
self-driving car that's driven several
hundred thousand miles around the
western US apparently they've only had
one accident with these things and that
was when a grad student was driving at
the time so I'm actually you look at the
guy sebastian true he's sort of behind
this project a Stanford professor is
really very passionate about the idea
that a lot of the fatalities on the
roads could be solved if the cars just
throw themselves but anyway it's the
idea of these fundamental algorithms
that drive data and another very
interesting example of this is a little
feud that erupted not long ago between
the famous linguist Noam Chomsky you
know who built these very sophisticated
models of grammar and language and Peter
Norvig an AI who actually runs Google's
a research group of famous AI researcher
who is very much on the side of
probabilistic models rather than
building these you know all-encompassing
models of the world led basic
probabilistic behavior figure out what
what that next word is in the sentence
or how to translate chinese to
German or whatever and so this this is a
great blog post to read if you actually
get the slides and click the link it
will take you to this post but it's a
good example though of the sort of
fundamental rethinking of what's the
most effective way to work with certain
kinds of data like human language and it
really turns out the probabilistic
methods are the most effective and
successful of all the examples we have
whereas the modeling of grammar that in
the Noam Chomsky style is maybe less
effective and it may be that our brains
are actually working more
probabilistically than anything else but
anyway this drives architecture choices
and briefly I want to talk about what we
are used to architect things and how we
have to change that so it used to be
that we would basically build object
models our big model of the world in
memory try to represent the domain in
memory as it were and you know at some
point in the life cycle the application
somebody would trigger a query to the
database you know maybe they would log
into their account and you had to fetch
you know their account data and that
would go through some object relational
mapping API because we all hate working
with database people we hate working
with relational databases so we very
quickly wanted to get that data into our
pretty little object model right we've
all been there let's let's be honest
with ourselves so then we have this
object model and we do some hacking on
it and then eventually send it up to you
know the browser or whatever the problem
with that is it actually is very
inefficient to go through all of those
layers and building up all that internal
representation really what we should be
doing is embracing the fact that we're
pulling fundamental collections out of
our databases and that's true of the no
sequel databases as well as really
relational databases just embrace the
suck is sort of an a US Army term which
we put them through a lot in the last
decade won't go there but anyway and
then just work with this data in a very
functional way and one of the actually
interesting implications of this is that
we can make our code a lot more agnostic
about the world it doesn't have to know
everything about you as a customer with
a payroll with a HR application or
whatever it can just work with numbers
in a very generic sense and that makes
it more flexible and adaptable as we
evolve the systems
going back to what I said about
data-driven programs and it turns out
this is a much more scalable approach to
but basically we need to start focusing
on our fundamental collections and the
Combinator operators we have on them and
so forth like your flat maps and filters
and whatnot rather than reinventing the
wheel for every kind of object we
declare all of us have seen code like
this it's almost inevitable in every
almost pretty much every java enterprise
app I've ever seen was just a big ball
of mud because when you have a domain
model of the world it's really hard to
tease it apart into reusable smaller
pieces that you can actually distribute
remotely so inevitably everything goes
through this one big wad of Java even if
it's only going through a slice of the
actual behavior whereas if we do things
in the more agnostic functional way we
can split things to the side easier we
can have focused processes that are
scaled in different ways from other
processes depending on their load and
that supports you know all of these
trends that we've been talking about
where you know I can scale up by scaling
out for handling big data sizes these
processes are more agnostic about the
world and that lets us you de-emphasize
having to have the precise model that's
of course it's going to be obsolete in
two weeks anyway and then we can drive
are these programs more easily with data
and let the behavior emerged from the
interaction of data and core algorithms
so today this is how we pretty much do
it it's at least at large scale or what
we think is large scale yet I've
certainly worked with clients who
thought they had big data but really
didn't they could have used some less
some more boring less hip or whatever
technologies but instead they wanted to
build out big Hadoop clusters because
they liked spending money on hardware
we'll talk about why that's the problem
and shortly but anyway so Matt button
but nevertheless at least the conceptual
level and and truly at a practical level
two it does support this model of a very
generic compute model that lets a scale
horizontally so that we can take a big
job of you hundreds of terabytes of data
distribute the computing over a cluster
missing nodes and then you do our work
in parallel it's faster and then wonder
neath is typically a distributed file
system or some semblance thereof maybe
it may be a database like Cassandra or
whatever now an interesting side note of
course is that we've seen people embrace
javascript is the top today soup to nuts
turtles all the way down top to bottom
whatever metaphor you like approach to
programming where we might be using node
for essentially our business logic in
the middle using a JSON oriented
document database like Mongo at the
bottom and then of course this all got
started because we realized or Douglas
Crockford realized that we had this nice
data interchange format called Jason so
that's a very interesting trend too I
have a walkthrough of what MapReduce is
but I'm actually just going to gloss
through it quickly for time sake because
I wrote as usual way too many slides and
I'm kind of assuming most of you have
sort of know what MapReduce is but you
can get the slides and go through the
details if you want this is the one of
the most cool sculptures that i know of
in Chicago called the beam and that's my
brother right there with the sort of the
drool coming down the sides much anyway
this is obviously the platform people
are using it's open source right so it's
free of course except for the hardware
and just quickly you know there is a
cluster that basically has two big
pieces it has a file system that's
virtualized over a bunch of servers over
a bunch of just raw hard hard drives and
then it's got a job management execution
framework on the top which is doing this
MapReduce algorithm and so forth the
example that I have the slides for that
I just want to sketch through very
quickly as the inverted index algorithm
so this is the case where say I have all
of Wikipedia and I want to find all the
words in all the pages of Wikipedia so
when someone searches for the word could
do by know which page to take them to
you know exactly what you do with
elastic search or what Google got to be
billionaire's doing with their their
version of the salga rhythm so you'll
have something that crawls the interwebs
outside of Hadoop you'll finds all these
documents
loads them in some data store that could
just be a flat file even that has
essentially you know like a two column
record of here's the document and here's
the text of that file and then the
Hadoop part comes in by basically
splitting up the processing of all of
these documents into separate so called
map tasks these would be separate JVM
tasin hadoop case and all these things
are going to do is they're going to
tokenize that text they're going to keep
track of the document they're going to
write out key value pairs I'm really
going fast through this just for time
but the key value pairs will have the
key it's the word and each word that
it's found and then the document where
it was found and the count because you'd
like to you know keep track of which
ones are really obsessed with it do
versus pages that mention it in passing
to provide more relevant search results
and then you know it may be the ten
different documents and ten different
map tasks mentioned Hadoop so you want
to shuffle all those together and so
that when you write the final output
which is that inverted index you've got
words and then you've got all the
documents and the frequencies in those
documents that mention those words and
now you could take this put it on a
database for rapid retrieval when
someone does a search for the word
Hadoop or one of these other terms I
used from the MapReduce framework and I
realize the prints a little small for
those of you in the back again this is
just better to study on your own but
just briefly this is the this is really
all there is to a MapReduce job it's
both great in its simplicity and
horrible but as we'll see it makes it
really difficult to actually use in
practice where you have this so-called
map phase that's taking one input in
multiple outputs which should make your
ears ring a little bit if your
functional programmer that's what
mapping is not it's always one to one
yet we're doing zero too many or one too
many well actually we're doing flat
mapping and then the reducing is where
we collect these multiple inputs that
came out of the map tests and writer
final outputs I did go that pretty
through that pretty quickly but here's
some pop quiz so just this make sense
you know a MapReduce job without a
reducer this picture these are the
muffler's at the bottom are the
tailpipes all right anyway
so really Matt produces just a mashup of
our old friends flat map and reduce and
it should have been called flat
MapReduce but I guess maybe that wasn't
sexy enough or something alright so but
this is our best tool right now you know
that people are using it is production
tested it's you know Facebook ranch esco
mentioned at Facebook has like 700
petabytes of data in Hadoop cluster so
it does actually work but it's it's
obviously in my view of first generation
technology so there's much to criticize
about it this is my gratuitous beach
romantic scene from Chicago in February
two years ago that's a frozen lake
michigan there's this beach is not far
from where I live anyway so we have
these distributed file system that
doesn't quite meet all our storage needs
and we have this compute model that it
turns out to be hard to work with and
inefficient namely it's actually hard to
implement a lot of algorithms and
MapReduce the reason for that is that
it's a very coarse-grained sledgehammer
approach to computation instead of
having like fine grain Combinator's that
you're gluing together if you know what
that means you know little bits of
computation sort of like legos you have
these big things this big map step it's
going to read chunks of hard drives into
memory and you're this big reduced step
and it's actually non-trivial to
translate most algorithms that
conceptually might make sense like
something like a secondary sort where I
want to do a select statement in sequel
and I want to sort by the date
descending in the last name descending
that's actually incredibly hard to write
in MapReduce it's non-trivial and in
order to do it a lot of times because
they only get one map and one reduce
phase we have to do it in sequences of
MapReduce jobs and the problem with that
is the way this is all implemented is
it's going to write the entire data set
to disk between those MapReduce jobs
even if it's about to turn around and
read it all back into memory so it turns
out it's actually it's like shooting
fish in a barrel if you've ever heard
that expression to actually do better
than this and some of the alternatives
will talk about typically get order you
know
one or two orders of magnitude faster
performance just for that reason another
issue that turns out to be increasingly
a problem is that it's very much a batch
mode system where I've collected all
this data you know I walk the interwebs
I've got all these documents and other
contents and now I want to build up the
search index but that's no good if I
want to have reactivity on an event
basis you know if I'm building reactive
apps and I want to respond to each event
then that just doesn't work and Hadoop
is very poorly designed for this problem
so we have to find alternatives for that
and the last thing I'll mention right
now is a problem is that that Java API
is particularly ugly and I'll show you a
beautiful example of ugly here in a
second so we'd like to fix that so let's
actually look at code we're all
developers right we love looking at code
what I'm going to show you is a slightly
simpler algorithm than the one that we
just talked through very fast called
word count this is the classic hello
world if a dupe it's always the first
one people right because all we want to
do is basically the same sort of thing
we're going to read in all of those
documents and tokenize them but we're
going to forget the documents we're just
going to count the words we want to find
them all and count them and to do that
in the Java Hadoop API you write code
like this which is deliberately too
small to read and this isn't all of it I
left out the main routine that
sacrifices the goats to the cluster you
know to achieve good results and that
sort of thing one of the things I would
like you to notice as we go through code
examples here is the colors and they
don't quite show up as well as I wish
they did but they look better when we
get to smaller code examples but the
green is tight so this is Java so it's
you know hi ceremony typing there's the
yellows are actually doing the work
those are the functions or methods that
are doing real work and what we would
like to get to the point of just
relatively little green and lots of
yellow that's really what we want right
now where it's mostly green ceremony and
not much yellow actually just for
because you're reading carefully what
you're seeing here this is actually the
interesting bits it's doing the
algorithm the rest of it is just pure
ceremony when I first saw code like this
by
first thought was that 2000 called and
it wants it CJ bees back really this is
to me this is exactly what was wrong
with the ejb API it was very invasive
you couldn't write code that was generic
and it was sort of agnostic to the
infrastructure you had to just inject
all of this stuff into the code and
force fit your algorithm to the
framework and that's of course a
terrible way to write code the spring
framework solved that problem for the
java community by showing us a better
way than ejbs and I think there's better
ways coming and we'll see examples for
writing MapReduce or not it not
necessarily MapReduce jobs well we're
going to see both if you have to work
with MapReduce what's the better way to
write these applications and what can we
do outside of MapReduce to get better
performance and better you know
configurability and so forth well the
first example for just getting better at
writing MapReduce jobs this is a
Fermilab's cool building that Fermilab
is sort of like a baby CERN for you
Europeans anyway so here's a Java API
that at least gives us a higher level of
abstraction and hides lots of the
infrastructure and the metaphors and
cascading are we're going to build up a
data flow of pipes that will glue
together there's going to be some tap
that's a source of data that's you know
they're basically our files in this case
in a sink always where we're going to
write it it's all going to be an HDFS
the Hadoop distributed file system
technically cascading doesn't require
Hadoop in fact it's nice that you can
actually run in a local mode outside of
a cluster for testing and eventually
they're going to support other back end
run times but for right now it's
primarily a dupe API and what we really
if you think about what you would have
to do to do a word count we're going to
have to tokenize each line of text by
word maybe with a regular expression or
something more or less sophisticated if
you were if we were doing this in sequel
and in fact we're going to do this
that's equal in a little bit you would
do a group by operation to join together
all the occurrences of a given word and
then you could count the size of each
group and you're done well you have to
write it out of course so it's really a
simple algorithm and all that the
ceremony we just went through is
horrible
well this doesn't look much better but
actually it is a lot better for one
thing I'm showing you the whole program
I didn't leave off another fifty percent
for the main routine I did leave off a
few import statements but that's I hope
you'll forgive me for that let me just I
think this will zoom in yeah so just
briefly though there's still a lot of
green because this is Java but it turns
out some of this green these are
actually functions in the pre Java 8
nasty way of using anonymous enter types
for actual just functions so we've got
like something that does basically a for
each and here's a group by operation
that you would just use on your
collections API in a real language we're
counting things we're iterating again
over things so a lot of these green
types are really just functional
abstractions and we have here so you
know pipelines and taps and stuff so in
fact even though this code looks about
as bad as the other at first glance it's
actually much more appropriately leveled
in terms of abstractions and hiding
complexity and so forth but we can do a
lot better so Twitter wrote this API
called scalding that sits on top of
cascading unfortunately it's a hard word
to Google because you get all kinds of
sad web pages about people have been
burnt but if you google twitter scolding
you'll do pretty well and here's the
same program written in scalding and
that's the whole thing and now if you
look at the colors again scala does a
lot of type inference not completely but
there's not a whole lot of typing
anymore there's a class and a job that
we're running an organ argument list but
mostly we're doing what we really love
to do in functional languages is take
you know some collection and then
sequence operations or Combinator's to
read it in do the flat map of converting
each line to a word there's some
idiosyncrasies of the API like this is
the name of the input field that's the
name of the output field but it's
actually pretty easy to learn this API
and then be productive with it so that
was how we split into words and then
here's our group by operation like we
would do in sequel and we sighs the
group and then write it out and that's
all there is to it so I actually
think that I could take somebody who's
like a maybe a SAS developer that you
know they can sort of code but they're
not really developers and teach them how
to use this API whereas I never try that
with Java in fact that's actually
happened in some cases were large data
teams have embraced this there was an
informal survey done by the data main
data scientist at o'reilly publishing
and he found that basically data
scientists seem to be gravitating
towards python for all their big day to
work you know which has always been
popular but but our is kind of
diminishing in popularity and Scala
seems to be growing in popularity among
developers and I think it's because of
api's like this one at Twitter wrote
that really make it easy to turn what
was a big program into a script so
that's kind of the future I think for
Hadoop and going beyond to do and just
for completeness now i am a scala bigot
but I do actually love lots of languages
let's look at a an equivalent api for
closure called kaska log that was done
by nathan mars and it's even smaller now
it uses data log style query so all of
these ? symbols are you know the sort of
variables that will get filled in as the
logic query engine resolves information
I don't really understand this API very
well so I couldn't even explain it
what's going on here to you but you can
see that if you understand the idioms
involved man you can write some
incredibly concise code to do something
that's actually quite powerful and
useful well that's all of this is great
it's running on top of Hadoop it
leverages what we've got today but it
still has all of the legacy issues of
Hadoop being batch mode and very
inefficient so there are emerging
alternatives and one of them is a
Berkeley project that's now part of a
patchy called spark this is a framework
written in Scala the API is very similar
to the scalding API we looked at the
similar sort of conciseness there are
some interesting differences in how they
frame these different computations but
you know you sort of get the gist and
again the same body count in terms of
types versus functions and so forth and
I'll leave it at that as far as the
description but you could you can
bubbly if you know any functional
language you could read this code maybe
even easier to read this code then just
coming at it cold than reading the
scalding code because there's fewer
non-conventional idioms here but all of
our friends like mapping and reducing or
here and it makes it a lot easier to
just take the concepts we've all been
using with collections and now use them
in big data so to speak us the so spark
is a great example to of a system that
actually was designed with the notion of
these small Combinator's of operations
that work together and they also use
very intelligent in-memory caching so
that you can sequence non-trivial
operations together without flushing to
disk and so again like shooting fish in
a barrel you get you know typically 10
to 100 x performance improvement just by
not flushing to disk for example
excusing and it turns out that we'll
come back to this briefly but spark was
originally designed for machine learning
applications those AP at least
algorithms tend to be iterative and
that's really really horrible for
MapReduce again because it wants to do
map and reduce and map and reduce so
something that can iterate fast is a
huge advantage well so I said earlier
that we're kind of de-emphasizing
schemas a little bit that was sort of a
lie let me we are but we aren't we're
actually seeing a resurgence of sequel
and it turns out that you know I really
didn't respect sequel all that much
until I started doing big data you know
I confess that I felt that way but I
really started to love sequel by doing
this because sometimes when you have a
question to ask of data there's just no
faster way than to write a sequel query
as far as expressiveness and conciseness
and so it turns out that there's a whole
slew of sequel implementations that have
emerged starting with high which
facebook invented for their data analyst
people at to be able to use a dupe and
then it's been ported to spark that's
what the shark thing is Impala and so
forth and we'll talk about these a
little bit but basically they all use
this kind of odd dialect of sequel to be
honest but they really designed for big
data kind of problem solving these first
well hive is running on top of MapReduce
so it has
all of the latency issues that map
produced us if you're used to wear your
database will give you an answer in a
like a second sometimes you wait 20
seconds for trivial queries to come back
and hide because it's you know it's
firing up the Java Gophers and they're
you know running on their little tread
Wilson coming back in 20 seconds spark
is a lot faster because it's running on
a faster system and there's now new
emerging replacements for MapReduce
really our first examples of the
industry embracing what's coming after
MapReduce which are Impala and presto
where they've actually replaced the
MapReduce back end with customized query
engines so you have this general engine
called spark that could do a lot of
different kinds of things but for a
narrower problem of a faster query
engine that's being addressed by these
projects so they basically easily exceed
high performance by like a hundred a
factor of 100 and there is actually a
Nancy sequel dialect called lingle
that's on top of cascading I actually
really like this project but I think
there's probably too little too late and
they're probably not going to go
anywhere with it just my editorial
opinion let's do word count in sequel
this is the hive dialect of sequel this
actually works remember we had to
tokenize like a single field into a
bunch of fields and that's a bit tricky
with a sequel but it turns out hive lets
you nest arrays and structures and maps
inside records so it's actually easy to
split a line into an array and then
explode basically turns that array into
bids does like a pivot so it turns each
of those words into a record but
otherwise this is pretty conventional
sequel I even created the table up here
that's just basically an abstraction on
top of the file system I've got
documents in some path in my Hadoop file
system I put this abstraction of a table
on top of it and then I write a query
against it to generate a new table
called word counts here that where I do
the explosion Here I just split by
whitespace it was kind of a stupid
splitting it doesn't handle punctuation
but nonetheless it gets the idea done
and you know that I grew up by word and
like order by word so I'll see the most
frequently occurring words at least the
way I wrote this one so this is actually
almond
was concise is scalding and if you've
already got the tables created then as
you're just writing something like this
which is very fast I actually think
we're in an era that I just call the
sequel strikes back because what I've
seen going into big enterprises is
typically you'll have like a handful of
overworked admins you know sis Ops guys
and then you'll have like maybe a team
of a couple dozen developers and you'll
have hundreds of these data analysts
that all our programming you know sequel
and SAS and those kind of tools and
Hadoop would simply not be as popular as
it is today if it if it wasn't for a
sequel option so that they could write
queries that somewhat like they're used
to against this big wad of data and so I
think that hive is actually made Hadoop
the big industry that it is and if it
wasn't for high then it wouldn't be
nearly as popular because it wouldn't be
as usable by a wider class of people and
it's interesting to me that now we're
seeing a lot of the no sequel databases
start to add query languages of course
the document-oriented ones like Mongo
have had a query capability for a long
time although crucially in Mongo you
can't really do joins so that that's
actually turning out to be an issue for
some people I know whereas Cassandra's
actually got a sequel dialects now and I
think this is the right movement that
sequel despite all of its flaws that we
can cite it's really really important to
typical end users and even as us for us
developers a lot of times we'll write a
Java program when we could have done you
know one-tenth the work with a sequel
query so I really think we should
embrace sequel ourselves and fortunately
there's actually a new seek well a new
set of databases let's call them they're
being called new sequel because people
love buzz words that are trying to take
the lessons we've learned from scaling
no sequel systems you know horizontally
and what night and bring it back into
the relational world maybe the most
famous example is Google's f1 where they
actually have global distributed
transactions implemented in this
globally distributed database and of
course they can do badass things like
put atomic clocks in their data center
so they can try to do x
thinking you know over you no longer
than this well speed of light is
obviously going to affect how you
synchronize clocks this is an old
problem but they you know they have the
money and they just but I guess to
actually try and make distribute
transactions where it globally and the
real definition of global but some of
these others are startups that are also
pretty interesting and trying to do this
so what I said earlier about you know
formal schemas are being downgraded and
importance that's partially true it's
certainly true in some problems in some
industries but sequel or a model like
sequel is not going away and we really
need to understand that that's kind of
the future again for big data I also
mentioned earlier that it's MapReduce is
really oriented towards batch mode
processing it's not at all suitable for
real-time event stream handling and so
forth so some alternatives have emerged
and you may have heard of storm which is
the most popular example this is another
thing that nathan mars who invented
kaska log came up with storm is sort of
like the you call it the event stream
version of hadoop in a way it's it's
it's designed from the ground up to be a
distributed system to provide some
degree of reliable event processing as
opposed to like a single you know
vertical system on one box that has high
throughput and just very briefly the
topology of a storm deployment that they
use the term spouts for sources of data
this could be like you know logs that
are being streamed in or events that are
being triggered by web services or
whatever and then you have different
processing stages called bolts that
could be anything that you want to do
with this data filter it map it join it
even and maybe write it to other stores
nathan has a really good book actually
called big data that he's slowly working
on you may have seen this in it's a
Manning book and it lays out his vision
of how to bring like the hadoo batch
mode world together with the real-time
events streaming world to you know to
implement non-trivial applications
that's it's a good read and other
options us
bark has recently done some real
gineering of their system to make it
better for real time event handling and
of course people have been using event
queues forever another possibility of
those is to go back to databases should
we bring those back this is actually a
photo outside my condominium window in
Chicago these is a heavy-lift
helicopters are used to bring new air
conditioning units to the top of
buildings and stuff so every now and
then you wake up on Sunday morning with
this whack going on out special window
and these things are falling down at eye
level whatever it's kind of scary in
fact never less could we use databases
for event processing well in a lot of
cases that actually does work because
their they re tend to be very good for
fast writing then sometimes all we need
to do is capture the events in a
database like Cassandra or Mongo or I in
fact I did a project like this many
years ago with Mongo we basically used
it as a sophisticated logger and and
then we had the query API on top of it
so maybe we can write to a database what
about the you know we have this
distributed file system i mentioned
earlier can we use that for this purpose
well really no it turns out it was also
designed for batch mode writing a big
chunks of data not for incrementally
adding data to existing files so it's
really not a suitable choice for writing
updates we really need something in the
middle which complicates our
architectures if we're using ado so the
trade-offs are hadoo versus equal or
even no sequel well we do get a very
flexible compute model even though it
has its limitations it's really great
for batch mode like scanning table size
chunks of data whereas if we want to do
something that's more transactional or
event-driven something that maybe has
more of a predefined schema than you
know a database is often a better choice
now here's a really common scenario in
the hadoop world and I've worked with
customers on this where they they have a
very nice data warehouse appliance but
in order to get beyond the size
limitations they'd have to you know pop
another million dollars or something and
that's it often sucks so it turns out
Hadoop is emerging as a data warehouse
alternative because it's got the sequel
capability I mean the pros of these data
warehouse system so they do tend to be
very mature and have rich capabilities
but they do have size limits they don't
scale is as well especially in it costs
per terabyte basis they tend to be
rather expensive in fact depending on
who you want to believe that the numbers
are something like a hundred times more
expensive per terabyte per terabyte for
certain appliances that I won't name
versus I do well sequel is very
important for data warehousing because
you want to write an analytics and
that's what they've been using all these
years so they're not about to give up
sequel just to keep doing their
traditional back-end analytics but you
don't need transaction so maybe the
batch mode problem with a dupe isn't an
issue so much with warehouses and in
fact it's turning out that that's kind
of the case where we do get this benefit
but also this disadvantage of these two
disadvantages with data warehouses you
know limited scalability and and very
expensive costs well we get something
it's less mature but it's actually
getting better at doing sequel and it's
very scalable this much as scalable as
any system that we can trust you know
with hundreds of terabytes to petabytes
and it has a very low cost relatively
speaking so we're seeing Hadoop as sort
of the poor man's data warehouse engine
these days at least that's one of the
uses okay so another drawback of
MapReduce is that it really sucks at
graph algorithms and the reason is that
typically graph algorithms involved
traversing the graph where you'd like to
be able to step you know one very small
steps i'm going to i've got them i'm
starting it may be that whatever this
root note is and i want to walk the tree
to do whatever it is I want to do
whether it's sending messages to
neighbors in the graph or do various
graph traversal algorithms and that's
not so great if I have this system that
really wants to have one big chunk of
mapping and one big chunk of reducing so
people are inventing alternatives to
this ironically Google started using
MapReduce as a way of implementing page
rank their famous indexing algorithm as
well as other computations but PageRank
is really just I'm walking the graph of
the internet
effectively and that is really sucky for
something like mapreduce so it wasn't
ideal but you can sort of make it work
and a lot of systems have done that but
again to clarify the problem with Hadoop
is if I'm if I'm walking nodes then I
want to be able to do that in very
lightweight steps I don't want to have
to write one MapReduce job for step in
the graph traversal you were right all
that data back to disk read it back into
memory it's just really terrible to do
that so how do people get around this
well an emerging model of distributed
graph computation is bulk synchronous
parallel where it's basically a
synchronous in the sense that I'm
actually doing this stepping in parallel
but each series of steps is done
synchronously so if I start that one
node you know I'll step to all the
neighbors in parallel but but you know
one step and then in you know we'll
update our state and then we'll do the
next step in sequence and so forth and
prego is a google system that was
designed to do this i hear mixed stories
about whether they're actually using
this in production graph distributed
graph algorithms and traversals and so
forth the systems is kind of a research
cutting edge thing so no one's quite
figured out what the best way to do it
is but if you're interested in doing
this in like the hadoop world because
you have graphed problems check out
giraffe and hama now what they do they
actually cheat with MapReduce they don't
actually run a MapReduce job for every
step in the graph they just run this
long-running MapReduce job where they
just use that as a you know resource
manager and they step internally over
the graph data and then aurelius Titan
is a really interesting new database
that sits on top of Cassandra that may
turn out to be the best open-source
alternative for a distributed graph
database now we're going to have talks
in the conference about neo4j I didn't
mention that because it's more of a
single machine database but it's also if
that's the size of your data then that's
really the much better choice than going
with one of these okay to sort of get to
the end here sometimes we needs more
specialized tools that are built for
specific purposes and we saw in a little
example of this already with compute
engines that are oriented towards
queries like replacing MapReduce to do
sequel queries which is what Impala and
presto were doing and we're seeing other
customizations like this particular file
formats that are optimized for certain
query problems but a really good example
of this trend of focusing on a subdomain
of this larger big data problem is
really addressing search with custom
tooling because that's such a ubiquitous
problem and pretty much the two open
source leaders are leucine combined with
solar and then the elastic search
project and this track as i said earlier
will have an elastic search talk later
today i just wanted to mention this i
won't go into detail but if you're
interested in this topic come back later
today and then finally we mentioned that
MapReduce kind of sucks for machine
learning algorithms because they tend to
be highly iterative so what do we do
about that how do we do machine learning
better and in machine learning what I
mean is things like how did how could we
do better at doing movie recommendations
like Netflix or classifying things like
spam versus not spam or even clustering
like finding groups within social
networks since this this is not a good
fit for MapReduce are there alternatives
well once again it kind of boils down to
there are things that run on top of
MapReduce they do the best they can like
the mahout package of machine learning
algorithms mahout has some really good
api's for like matrix computation which
linear algebra which is very important
in machine learning algorithms there's a
system called pattern that runs the top
of cascading and then spark is really
eat if I was really building a machine
learning system as opposed to just a
general data system and I wanted to live
on the edge a little bit more I probably
go a spark over MapReduce for this this
reason that it's going to be much faster
and just to back up a little bit how
would this actually work well typically
you would use Hadoop into in batch mode
train these these algorithms like your
spam filter and then store that data for
the filter in some fast database so that
it run time when people are asking is
this spam or not you can get a very fast
answer for them and the last thing that
I think it could really interesting it
goes back to that debate i mentioned
between Norvig and
ski is I think we're going to see
emerging tools that make it easier to
write these probabilistic models that
people use for Cara autumn automatic
cars and and language analysis called
the Bayesian networks and Markov chains
are kind of the buzzwords for the models
these are heart these really require
expertise to write today and they're
actually attempts being made to build
languages that make it easy to express
these concepts that make them
first-class citizens okay so let me wrap
up quickly because we're at a time this
is the beach scene again well anyway so
I think hadoop mapreduce is really kind
of the enterprise javabeans of our time
which is good and bad i mean just like
ejbs in the old days they got people
going they were a solution to a real
problem but they definitely were that
were the first generation and up the
last word so we're already seeing
MapReduce kind of go away because it has
a lot of performance issues and it's
being replaced by things that are either
functional style languages or AP is like
the scalding and the spark or its sequel
that's coming back and I think that the
reason that the these language these
like functional language solutions in
sequel are so concise and appealing is
because basically big data at the end of
the day is mathematics and so functional
languages which are built on that you
know core principles of math are always
going to be our best tools for big data
not you know object oriented stuff a lot
of people said that concurrency in the
need to write you know scalable
horizontal apps is really driving
adoption of FP but I think the next wave
is actually going to be driven by data
centric problems because that's kind of
a bigger problem from that more people
have to be able to implement themselves
versus what kind of solve the
concurrency problem with actor libraries
and whatnot okay so we're kind of at a
time i'll just take questions you know
privately afterwards i don't want to get
between you and your coffee and i hope
you enjoyed the the rest of the day
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>