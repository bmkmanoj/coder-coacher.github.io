<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using Erlang, Riak and the ORSWOT CRDT at bet365 (...)  - Michael Owen - Erlang User Conference 2015 | Coder Coacher - Coaching Coders</title><meta content="Using Erlang, Riak and the ORSWOT CRDT at bet365 (...)  - Michael Owen - Erlang User Conference 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using Erlang, Riak and the ORSWOT CRDT at bet365 (...)  - Michael Owen - Erlang User Conference 2015</b></h2><h5 class="post__date">2015-07-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WXmO1IvzIZY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so first thing I'm going to set some
background information in order to set
the context in which this work has been
done so bet365 we were found in the year
2000 we're located in stoke-on-trent the
largest online sports betting company we
have over 19 million customers with one
of the largest private companies went UK
and we employ more than 2,000 people in
the tax year 2013 2014 over 26 billion
pounds of staked and last year is likely
to be around twenty-five percent up so
we're business that's growing still very
rapidly we're very technology focused
company and these big stats show about
this is a case so I won't go in from
every single step but you can see for
example our push systems vapors to
100,000 changes per second and we have
over 2.5 million concurrent users of our
push systems so we have two main
production systems that are using Erlang
and react they are cash out and as a
system used by customers to close out
bets early and a project recall stronger
and this is an online transaction
processing data layer this talk is
mainly based on the experiences with
stronger so why erlangen react why did
bet365 choose erlangen react so our
historical technical staff is very
pragmatic it's what would deliver
quality product to market in record time
it's mostly dotnet with some Java
middleware and it's lots and lots of
sequel server but we needed a change a
complexity of code and their systems and
we need to make better use of multi-core
CPUs we need you to scale out
we can no longer scale our sequel
infrastructure this is scaled out of pan
out as far as we could and it's like a
scalability cause undue stress on the
infrastructure Vince too late to a lot
availability so our early adoption I
won't go into ins and outs what airline
is or what it does because I think you
all know that already with some key
learnings for our use of Erlang so you
can get a lot done and a short space of
time and so we found this to be a plus
and a minus so the time it takes from a
developer to hate Erlang got first to
love it it's really really quick and we
found that be quite surprising I think
it's because Erlang makes concurrency a
lot easier and therefore I feel
developers feel like they can build
everything and then you've just got to
make sure that we don't and tooling is
limited so a lot of our developers are
used to visual studio and compared to
our Erlang does have limited hall in or
at least less polished however I feel
like getting developers do more home
sells out lower stack it's beneficial em
hot code upgrades upgrades which state
can be hard dependency management could
be better I here at me bar free could
help with this I feel it's important to
get as much visibility into a system as
possible and as early as possible and to
understand how airline works with this
data so for example just because you may
be seen cpu usage that doesn't directly
chorus correlate to actual work being
done use OTP and we use proven code so
erlangen OTP have a lot of useful
libraries and api's and get to know them
try not to reinvent the wheel and also
lead to good clean code I'd argue and
also keep the standards so if example
code layout seems trivial but I think it
helps new airline developers get on
board with a project when referring to
rank documentation
and it helps integrate with tools as
well check your supervision tree so it's
something that you normally create when
you first start a project you've got to
make sure you keep an eye on it and it
stays relevant as a project grows I said
that message passing is double-edged
sword so what I mean my heart is Erlang
I think makes concurrency a lot easier
and I do like the actor model and two
message passing but you're just got to
be careful how many messages you have in
your system and it can become a
performance bottleneck you've just going
to be aware about keep state small so
for example Jen service date it is
useful type of state you just could be
aware how much that you have and try to
keep to it to it keep it to a minimum
also if a state needs to be highly
available and persistent I'd argue that
you should be looking at things like
reactor for example binaries and garbage
collection and so we do like bonnet
binaries and we find them to be a
credibly useful and more useful than
list based drinks for example we've just
got to be aware that binaries up 256
bytes are stored on a process heap or
voice binaries are stored on a global
heap so you've got to be aware of when
they get garbage collected so bonhomie
is only maga global heap only get
garbage collected when all the
referencing processes no longer
reference them so you just need to be
aware of your boiler is and how big they
are and how it affects your garbage
collection in terms of the process that
processes that use them explore whether
you need to use the transparent huge
pages feature in the linux kernel so we
found post of my use cases that use a
lot of memory it was best to disable
this feature don't use arrillaga as your
main main lager so we found it was best
to use a log in dependency such as log a
log in I'm valide or data coming to your
length system at the edge general good
practice from a security perspective but
I think it also helped severe lung so
when you check those data type problems
further on up the stack I think it's
easier to debug and you get better error
messages out of a lang so our react
adoption some of you may not be aware of
what we lack is so a very brief summary
so react is their key value store it's
soon to be known as react kv if not
already it's inspired by the dynamo
paper so that's the Amazon paper it's
traditionally a eventually consistent
system so that's this AP from a cat film
and then within bet365 where you do use
it in eventually consistent way in react
to they introduce the option of having a
strongly consistent option as well so
that's a CP from the cat film with react
react cluster is a 160 bit integer space
that's known as being the reoccurring
it's split in two partitions a virtual
node what's known to be a V node is
responsible for each one AV node when
lives on one of the rear clothes and
then data is installed in normal
partitions and this is controlled by VN
valves setting and this has a default of
three a consistent hashing technique
then helps identify those partitions for
putting that's writing the data and
getting that's reading the data and
react would do its best effort to make
sure a data is stored on different
physical nodes so why we a qui dit
bet365 choose react so there's an open
source aspect to it it uses Erlang it's
based on solid ideas with the Amazon
dynamo paper it's horizontally scalable
it's highly available and there's no
global locks so performance is more
predictable it's designed to be
operational simple so that's taking
nodes in out of service for example and
their sport and community exists for it
so our key learnings from using react I
don't mention we use it in a mentally
consistent way so this means eventually
the data will converge to the consistent
value so practical things to keep in
mind i get i can mention that some read
may return an old value a put so that's
android may be accepted for a key at the
same time as another concurrent put for
a same key in a cluster there's no
global lock in so you might need to bend
your problem so what i mean by that is
for example if you've got a counter and
you're waiting for it to reach a certain
value before you do only one action in a
highly available way you need to
consider what happens if that action
happens more than once can you read a
side effects in such a way where you
only see the side effect once data model
for your use case so normalization isn't
key you'll need to trade off your puts
against your gate gets your use case so
with some of your use cases might be
better for you to store that data more
than once so when you come to read the
data for a particular use case you can
get all the data you need that use case
in one get or as minimal gets as
possible but obviously that's going to
impact your but puts so it is a
trade-off between your puts and gets and
it will depend on the use case we found
it's best to have no big object sizes
one megabyte and again that's going to
be a trade-off between your puts and
gets because you don't want your object
sizes be so small but you do have to do
loads of gets in order to get your data
for your use case so again it is a
trade-off store your data in a structure
which helps a vision upgrades so if you
want to change the structure it with
data stored within in react what's going
to happen to a data that's audion react
and audio in production how are you
going to upgrade that data so pre think
that out before you go live react
enterprise exists so something to bear
in mind Maltese data center replication
exists
and support as part of react Enterprise
consult we act system performance tuning
documentation so battle have some good
documentation but Adam point this one
out explicitly react is quite chatty
into node and so consider having a
different internal network this is you
inbound react is network and i/o bound
so make sure you monitor your network
and disk usage use a react admin diag
command so this will do some common
checks for you some common checks in
order to check common problems we've a
cut me at cluster bar shall have
something called battle bench so this is
an independent test tool you can use to
load test your way a cluster to make
sure it as you would expect before you
start adding variables in like your own
application on top of react form a bit
cast back end load test merging and two
of your use case and you can use a
setting called log needs merge we set
this to true it will explain analogues
why is the emerging and based on this
you'll be able to tune for your use case
allow siblings so that's allow mostly
true and try to resolve siblings as soon
as possible so you don't get sibling
explosion now this leads me on to what
are siblings so a sibling happens when
we act does not know which value is a
causally recent so if example which can
happen because a concurrent puts it uses
version vectors to know this so version
vector a is concurrent to version vector
B as opposed to descends or dominates
I'll explain later what a version vector
is it is reference has been vector
clocks in an react documentation and my
have seen some videos and literature
where it says it should have been named
as version vectors similar logic between
with too subtle difference
better vector clocks is about tracking
events to a computation version vectors
about tracking updates to data replicas
in react to they introduce the option of
having dotted version vectors instead so
this is a similar idea to me all SWAT
crdt dot functionality and I explain
that later on to it reduces the
potential number of siblings so it does
causality tracking a lot more accurate
and because of this it limits sibling
explosion so if you are using me out too
I would consider using dotted version
vectors when you are siblings or sibling
values are returned so that's more than
one value obviously that's a massive
difference compared to normal experience
with sequel type data stores you can set
allow Malta to be false so you can set
its own to know siblings and get return
to a client and you can set this with
last white wins to be false this will
use version vectors and a conflict is
sibling with the highest timestamp will
win and then you can use it with last
white wins set to be true too so this is
a subtle difference it doesn't use
version vectors and when new value over
wipes the current value not using
siblings isn't recommended because the
potential data loss so network problems
and complexity of time synchronization
across servers you perhaps might use it
if you are truly a mutable data with
separate unique keys however as a
general wall you should be using
siblings so how do you deal with these
siblings so as I mentioned sibling
values are returned on a get request so
you need to have a merge function which
will take these sibling values in and
produce the correct value it's been
shown that this merge function needs to
be deterministic by having a fun
in properties that's associativity
community and Ida potent it can be hard
to get this mood function correct if you
don't get it get correct it can lead to
potential data loss and incorrect data
so this leads me on to CR DTS so what is
a crdt so generally speaking they stand
for conflict free replicated data types
and they can be operation based and
that's community replicated data types
or they can be state-based and that
stands for convergent replicated data
types we reduce the complexity by having
no client-side siblings but you still
don't have that data loss so it deals
that moon type logic on your behalf you
can see on a presentation slide there's
a number of readings that I would
recommend to which explain where CRTs
came from so operation and based CRT
teams I don't mention they're called
community of replicated data types with
operation based all replicas of a data
are sent operational updates it relies
on a good network and reliably
delivering these updates also knowing
the current to membership is more
important we state-based as i mentioned
i could convergent replicated data types
the data is locally updated sent to epic
is emerged this update function must be
monotonically increasing it is generally
easier to understand the operation based
and it is easier to have elastic
membership of replicas however because
you're sending around the state are not
just your dates more data is sent around
the network so you can have different
data type implementations for crd teams
such as counters
sets maps for our core use case the sets
data type made sense however we were and
we still are currently using the app 1.4
and asset CLT wasn't available this was
something that was introduced in the app
2.0 and at the time when we're doing
this project react to wasn't even in
release candidate so what do we do we
decided to use the react dt determine
dependency that has the CL TT pump china
to avail it and integrate it ourselves
into our system using one react 1.4 an
advantage of open source different set
based implementations exist so it's a G
set so this is a groaning set you can
add elements but you can't remove where
Z all set and this is the observe remove
set you're able to add and remove
however when the element is removed you
still get a tombstone so a size problem
exists if you had a set of like 100 for
example and sort some elements out the
client would see those elements has been
removed but a size of a set would still
be the same and then there's the all
swap so your SWAT is the observer
moonset without tombstones so you're
able to add remove but you don't have a
time their tombstones so you don't have
a size problem and I can current Adam a
move on the same element but add wins
you can have implementations where move
wins but generally add wins and you can
see a reference on a presentation slide
which explained where all swats came
from for our core use case we chose to
use your swap so what is your SWAT you
can see an example on the presentation
slide so you have your we at key for
example and you have your value and the
value is your swap in your SWAT you have
a version vector
so you can see this in green and this
exists as part of your swap it's a list
of tuples each cheap will be in a unique
attic name and counter all also what
operations then happen through a unique
actor it's similar but it's a different
instance of vision vector to what would
also exist for the overall key value you
then have your AWS wat entries each
Ottawa entry then has your element your
data and it also has what's called dots
so you can see Miss on a presentation
slide in blue so dots exist for each
element of your SWAT set it's just a
minimal version vector and each dot pair
represents a tuple and we use in the
origin vector at a particular point it's
usually listed as one but it can't be
more and i have an example 81 where you
see where the dot a greater than a size
of one if you learn to add an element to
an all swap a version vector as part of
your SWAT is incremented so this means
the counter for we unique actor being
news is incremented or set as being one
if that didn't couldn't exist an
immersion vector this updated unique
actor named counter is then stored with
the element as its dots so best for an
example quite straightforward but you
can see one on the slide where we add
data to using unique actor why to exist
in all swap it was all time a new or
swap where a version vector has been
incremented so you can see why one
that's been introduced and this y1 is
then stored with the element as its dots
in this example you can see data to
being stored with y 12 to the client
where to be interested in the macaw data
so in this example it be just data one
data to
the rest of this data is partly all swat
is its metadata so how would you remove
an element so a version vector as part
of your SWAT does not change so a green
bit but of course when you change yours
what value and you put it back into
react the version vector as part of the
overall key value would change the all
the elements entry is then simply remove
on your swap again a basic example on
the presentation slide if we remove data
one from the existing or swap it results
from a new or swap where a version
vector in green does not change and we
simply drop the all swat entry for the
element data one so you can see the
element and its dots as simply removed
from your swap who is your third option
to be able to do delayed or deferred
removes so you'd want to do that when
the AWS what object doing a remove has
not seen in regional aunt for the
element EG because of becoming a being a
replica which hasn't seen the original
and last name with an intent
implementation option you can have and
it would add pure logic to remove and it
would add further logic to the next
slide is about merging so how would you
merge to all swats I do have an example
on this in the next couple of slides
which I think shows this Bell oven
listing out all the economy instructions
but I will briefly go through it so you
would merge all swats when you have for
example siblings which have been
detected by the overall key value
version vectors because you've had
concurrent operations on your swap so if
you were to merge all swat a and also
what be to get the third the merge
version vector you would take the least
possible comment descendant of all swat
a and also what be like i said i do have
an example the elements are then merged
so
for you look at me common elements and
the common elements are only kept if it
exists in non empty dots poem after
merging the following so with common dot
pairs for common elements in all swat a
and knows what B dot pairs for the
element only knows what a where a dot
pair count is greater than any count for
the same actor an all swap B's version
vector so you can peridot pairs in one
or swap to reversion vector of yoga or
swap then you do a similar thing to
adopt pairs for e element only knows
what B where a dot pair count is greater
than any count for the same map to an
all swat A's version vector you then
look at your elements only knows what a
and similarly they already kept if it
exists a non-empty docked firm after
keeping only dot pairs free element
where a dot pair count is greater than
account for the same actor in all swap
B's version vector and then you do a
similar thing for elements only knows
what be like I said I think it is best
explained an example so if we had Oswalt
a and this is seen adding element data
one by actor x I didn't del element
dated to buy out to Y and then add in
element day 23 by out to why again we
then have all swap be and it's a simile
scene add an element data one by actor x
add in element data to by actor why and
it's also seen adding an element data
three but this time right actor Z so
it's been a concurrent add of the same
data which is day two three it's also
seen adding data for my actors then
again and then removing element data one
also by
to Z so this results in the merge also
what I've seen on the presentation slide
so you can see that data one has been
dropped you can see what data to and
data three is being kept and you can see
that data for has also been kept even
though it only exists in all top be so
if we're going to explain that better
and to break it down so in the merge all
swap X 1 y 2 Z 2 is the least possible
comment descendant for the version
vectors or SWAT a and all swap be so you
can see this because you've got X 1
which is the same in a mb you've got y 2
because that's greater a greater y which
exists in or SWAT a and you can see
you've got z22 only appears and all swap
be we then look at the common elements
so first of all we've got data to data
to exist in all swat a and all swap be
and they both have a comment date dot
pair Y one go are no other dot pairs to
consider as part of also our a knows
what be for data too so we just have y1
that's obviously a size greater than 0
so the element is in we then look at
data to so day two three so for data
three or SWAT a has y 2 and this is
greater than the count y 1 in all swap
B's version vector so obviously y 2 is
greater than y 1 so that's considered we
then look at all swap be so that has a
z1 and this is included because also a
version vector doesn't have a Zed so
it's great it's great
and also I tazed version vector risk
results in y2 and said one this is
merged to obviously give a list greater
than 1 greater than zero Zoe it is 2 and
therefore the elements in if we went
look at elements only in all swat a so
this is data one so from autumn or a
there's no exact exists from x1 which
are greater than all swap B's version
vector so obviously x1 isn't greater
than X 1 which is in Swat B's version
vector so the element is not in emerge
or swap we then look at elements only
knows what be so this is data for so we
can see that from all swap be dot pair
z2 free element is kept because of what
aids version vector doesn't have a Zed
so therefore dated for is included and
merge or swap so our react dt
integration for this we needed to have a
new line middle layer between our client
and react this middle layer needed to
have unique actors we use Jen servers as
part of using this all SWAT
implementation now we need to get a
balance on a limb robin due to impacting
the all swap version vector size but
also obviously too little I mean would
impact the concurrency of a system each
unique actor needs to have a unique a
unique actor named and this was pre
defined as part of the server setup
configuration and we needed to make
these unique actor names as small as
possible due to also impacting the all
swap version vector size so this meant
our clients didn't have to deal with
siblings however this middle layer ieva
client to react still still still needed
bring back my siblings and do this CRT
team merge logic
so it's still not as good that's having
reacts service I crdt I didn't react to
but as I mentioned at the time that
wasn't available and as I suggested on
the previous slides we're still using
version vectors on the overall key
values to know for example to do the
alt-rock merge operation because there's
been concurrent operations on your swamp
so how do we get a system live like this
considering some mission critical system
and barely mine it's replacing the
current system which is always moving so
first of all to Lin so from day one we
had our own dog food so what I mean by
that is that we invested in tooling and
start the project and we used it
throughout the light project lifecycle
to make sure it's fit for purpose so we
invested in monitoring we invested in
performance counters so this airline
middle layer of example had stats and
counters functionality which allow us to
have basic counters and latencies at a
global level pull requests but it also
for a particular request if it was
deemed to be higher than the exceptional
latency it would print out a timeline
and this allowed us to debug problems
and this proved to be really invaluable
there's a report in including thinking
about correlation between systems and
how you track an arrow through a system
or its progress we built a custom ad-hoc
query tool so this went blank middle
layer and this was able to take
advantage of example and crdt
punctuality and for good practice we
also run this only replicated we at
cluster we built custom reconciliation
between our new system and old system so
given the same input do they eventually
have the same output do I have the same
data and we also created
building these scripts for automation we
released a project in phases so it's
allowed us to build on stable ground it
allowed us to get data to impact future
decisions they allowed us to build
confidence we were able to move
functionality using erlanger and react
in a phased way not all phases were
immediately business impacted but
overall we feel we're able to get
business impacting from charity out
sooner replay testing so we did we play
testing and we did this by capturing
logs asynchronously in one phase we had
a common interface between our old and
new systems as I previously mentioned we
were able to do reconciliation between
the two systems mystery playtesting gave
us a big range of test data and it gave
us a real estate low profile it was
useful functionality and performance
testing and we had different logs for a
long weekend one I would give his
advantage of him more test data this is
the crisp quick one which obviously
could had less data but we had a quicker
feedback mystery playtesting
complimented over testing such a
specific unit integration testing first
testing and form on uat performance
intestine obviously very important we
did it early I'm repeated so we didn't
leave it to the end of a project we
built a custom client using these replay
logs and we were able to low increase
low profile easily again we use our
custom custom tool in monitoring to make
sure it's fit for purpose so if any
problems came up during performance
testing dinner tall in I'm monitoring
what we'd have in production allow us to
find out what the issue was if it wasn't
and it's not a fit for purpose but it
did it helped us to get to a problem we
performance testing we identified but
NEX and we tested the horizontal
scalability of a system we also did ad
hoc changes because I feel this is
important to keep the momentum up during
performance testing we fed this back
into development before between his
adopt changes we had a Pacific profile
which would give us a fair test between
changes and again as premium previously
indicated we use it we use battle bench
in order to Sonny to test we at cluster
before we introduce our airline middle
layer as an extra variable failure
testing so we did beta testing also very
important this built confidence and also
our understanding of react and our
system we're trying to make sure I bayla
seen in production isn't the first time
is experienced we also tested common
procedures including that load and
that's including take nodes in our
service my message would be that
failures will happen to embrace them
don't want a rape on them and make sure
your system deals of them and also test
for them so today we're stronger so a
project was a success we now have a
system in production which is performant
and able to deal with many times our
peak load it's reliable and it deals
with failure using minimum human
intervention so if example we had a air
conditioning problem in one of our data
centers and it took to my servers
offline but to a customer stronger had
no customer impact we've also introduced
the business to a number of new
technologies and we've grown the
capabilities of our business and our
people so that's the end of my talk and
this now leaves me wanting any questions
okay one question here I'm not sure
whether it might work so if you say it
now repeat it
yeah sure of course so the question was
what do we do when SWAT values with
great in one wagon want one megabyte so
this technique sort of Chardon and so
are you char din and compression so you
can't use compression to any more
questions one more question
I see ya so the question was which part
of our system does this stronger project
relate to well it is a cross-functional
team project so it is like a middle
layer and traditionally we are a betting
company and it started off with me bet
placement side of things any more
questions I don't think so so I think
I'll call it a day thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>