<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>From Concept to Reality: Solving Enterprise Challenges - Jawad Yaqub - Erlan User Conference 2015 | Coder Coacher - Coaching Coders</title><meta content="From Concept to Reality: Solving Enterprise Challenges - Jawad Yaqub - Erlan User Conference 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>From Concept to Reality: Solving Enterprise Challenges - Jawad Yaqub - Erlan User Conference 2015</b></h2><h5 class="post__date">2015-07-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PKo2Cshfob8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm from Northern Ireland and we have
a slightly different way of approaching
highly stressful technological problems
but hopefully you'll find this relaxing
informative engaging as well I fully
expect lots of difficult questions I
fully expect not to be able to answer
all of them so fuel treat is stopping as
we go through um just a show of hands
who here has worked on an enterprise
platform that has made it into
production excellent so half of the way
through you'll all be putting your hands
up to correct me wonderful who here
believes that testing is an important
part of that process awesome that will
save on holy wars later on um yeah so
we're going to talk a little bit about
what it takes to actually build an
enterprise platform in this day and age
and we're going to base our platform
upon react because it's a piece of
technology that is a hundred percent
erlang the whole way through and it uses
a lot of the best features of or long as
well one of the key challenges is make
sure that we don't cover everything
under the Sun so we're going to have to
be very very focused this is bash oh man
I didn't draw him my artistic talents do
not extend that far but is very very
nice so our focus is going to be
primarily on design how do we design an
enterprise architecture which is based
and built on Erlang technology will
explain why we picked our rank a little
bit later as well look at some of the
challenges involved with building
something testing it making sure it
releases properly making sure it scales
making sure it's actually useful as well
we'll also take a look at some of the
key decisions that you need to make in
terms of things like strong eventual
consistency scaling factors availab
LD persistence those sorts of things as
well and finally we'll talk production
because most of you like me have had
that dreaded feeling of deadlines
followed by a live release followed by
lots of people wondering exactly how
successful it is fall by taking
responsibility for maintaining the thing
and the whole point of you know going
through testing going through the
process of making these decisions is so
that when you finally get out at the
other side and you've built your device
you do not have to worry so much at the
other end so let's get going let's talk
about the facts of life for architects
we're going to talk about one of the
hottest topics around at the moment ah
big data the reason we're going to talk
about big data is because it's
particularly relevant for the kind of
applications that we're going to use and
highly performant highly scalable very
resilient platform the key thing about
big data is that it is huge and we'll
talk a little bit about why that is as
well we'll touch on airline well
touching about real-time analysis and
real-time responses and analytics are
crucial for any kind of enterprise
platform at the moment and we'll talk
about the key pieces of technologies
that we're going to deal with as well
this is a good point actually to ask how
many people have worked with react
awesome but half the room how many have
worked with or played around with
reddest but third of the room and what
about apache spark okay that would be me
um apache spark for those that don't
know is nai machine learning library
which is maintained by apache and it
makes scaling up analysis and generally
producing intelligent results out of
very large data
is incredibly easy it's quite good I
hardly encourage you to try it out we'll
also do a little bit of a system review
or what the architecture is going to
look like what key bits need to be
billed what sort of comes out of the box
and finally we'll get to the painful bit
about testing staging and releasing the
final thing any questions good awesome
all right so why big data when you get
data typically it doesn't come in any
particular form that's immediately
useful you're going to have to sort it
analyzed it classified make a decision
based on it when you go through that
process it becomes information when you
actually find that information is useful
constantly then it kind of gets promoted
up to the level of knowledge but being
able to take raw data in and being able
to process it into information quickly
and make decisions on the fly is
incredibly important and that's one of
the things that we're going to aim for
our enterprise platform there's no point
in building it if we can't build with
vast amounts of data concurrently and
reliably the frank matter is that most
organizations nowadays deal with two
very very important types of data
there's data which is effectively
brochure where stuff that you push out
on web or mobile applications that just
tell people what something is the other
bit of data is what everyone really
cares what it's where money gets
transferred good transfer services get
transferred people complain things need
to be audited etcetera etcetera etc and
there are different enterprise
requirements through both of those bits
of data and you have to create an
application platform and an enterprise
platform that caters to both okay and as
we grow in the volume of data the
variety of data the fact of the matter
is you can have to become much much
better a dealing with it ever an hour
segmentation it's not enough to know
that your customer is a person it's a
good idea to know that there
market it's even better to know who they
are and it's even better to know why
they want that particular service that
you want and that involves work in
effort in your part so big data is
perfect for that we're also going to
talk about some of the sophisticated
analytics that you can use within spark
to be able to improve your decision
making and you can also use big data to
develop and refine your products as well
so it's not just a matter of selling
people and stuff that you already have
analyzing your data in real time allows
you to analyze the behavior of markets
in real time and that allows you to be
able to generate new types of products
which are specifically fitted to your
individual customers and clients you'll
notice is that the trend of increasing
more and more oh pardon me this is why i
bought some tea with me um this is the
evolution of no SQL over the past and a
final six years if you guys think back
to when you started your careers we're
typically dealing with much simpler
forms of data one of the biggest
challenges that we've all had to face is
how to connect to a database how to pull
back stuff and how to make it display or
dance in the way that we want and when
we started off the data that we got
those typically point solutions so you
pull it back tell you something yes or
no you make your decision later as you
had different decisions to make about
the data and things became much much
more complex you ended up with the
notion say of a person or a class or
people or a class of products with
different variations etcetera etcetera
etcetera so you had multiple models the
same underlying bit of data and now you
could try and look at even the same type
of thing that you were looking at before
let's say a person you could have
gigantic gigabytes multiple streams of
information which are relevant and
connected to that person you've got
location data you've got data on who
they are how much they own who they're
connected to social data things like
influence etc etc and that's just for
people you can apply that kind of
thinking to pretty much any type of data
that you have to deal with and as such
the individual items or data become more
complex and become harder to process and
the relationship between them become
harder to process in a simple
straightforward matter that used to be
the case with a point solutions does
that make sense okay cool so why are
lang the biggest reason is that it's
very very good like utilizing multi-core
systems that it grows very well it fails
fast it fails elegantly and typically
when it does fail you don't have an
impact at the application level it can
stay in continuous operation for many
many many years so I'm very reliable
it's responsive to users it's
upgradeable handles a large large number
of concurrent connections one of the key
problems that I had a lot of people had
especially with database technology and
building enterprise platforms was that
you often had let's see a very large
cluster database with the load balancers
I could only take a few connections at a
time and then as things got better and
better and better you got better systems
and you have to do things like thread
pooling etcetera etcetera etc with
Erlang if you have a piece of technology
like react and you have say five nodes
inside a cluster each of those nodes can
accept hundreds of thousands of
connections at the same time and serve
them back out at the same time our line
is awesome for loud because the overhead
for processes is incredibly low and
because there's a focus on making it
reliable any questions cool okay now
we talked about the models that we have
to deal with with our enterprise
platform becoming more and more complex
we also need to take into account fact
the volumes of the data become larger
and larger and larger add to that the
fact that a lot of this technology is
now available in open source to not just
your friends and your family but also
the commercial partners that are well
being actively with it and trying to get
the best that they can from it and
you'll see that the data volumes that
you have to process are getting larger
and larger and larger and at the same
time your ability to make competitive
decisions that lead to an advantageous
position for yourself is getting smaller
and smaller and more for that reason you
need to be able to relay your results
back out from your data set and that's
why we're talking about Redis and you
also need to be able to analyze those
systems and scale up your analytics
process alongside the scaling of Bieber
data store as well that's why we're
talking about spark that make sense cool
ok so but how have you already know what
react is and the way we're going to
build up our stock is to have react as
the data layer there are a few very good
reasons with this it's very simple to
operate typically only need one line of
code in order to be able to set up a
cluster add nodes or blue nodes you
don't have a lot of the master-slave
relationships the people typically worry
about when you're building an enterprise
environment how many people here have
worked with MySQL wow that's everybody
how many of you have had to build
master-slave relationship since I'd a
MySQL environment okay that's a lot
figuring how many of you found that
process successful yeah everyone's doing
that that's quite good acid compliance
is very very
ordinate people bang on about it all the
time right the problem with asset
compliance is it only applies to one
database doesn't apply to the whole
cluster you still have to deal with all
the problems react is very good in that
you don't have a master don't have a
slave simply add the nodes as you go
along to deal with the capacity that you
need and the demand that grows and then
you just let the database do the work or
replication making sure all the data is
consistent and of even things like read
repair etc so you can lose nodes bring
them back online add more etcetera
etcetera and that's the beauty of it
being simple to operate having worked a
decade and a half building these things
day in day out I can tell you there's a
reason why i put simple to operate up
the very very tall it's it's very
important when you build your database
there if you are going to use it as a
foundation then you're able to build it
and then you're able to move on okay
hands up if you disagree with me because
this is a point that I will talk about
great fantastic highly available we
talked about it being able to use
multiple concurrent connections from
each particular node we talked about
being able to add more nodes as you go
along the fact is that if you have more
and more and more data you want to be
able to process it in parallel and you
want to be able to serve it out in
parallel as well one of the nice things
about a data layer be it react or
anything else that does provide high
availability is that you have a lot of
problems in our category we just simply
go away if you put a load bouncer in
front of say something like react but
does this then you do not have to worry
quite simply you can simply scale up and
as long as you have enough network
bandwidth to be able to cope with it
your database will be able to provide
responses it's highly fault torrent
because of the way in which react works
and it relies on a lot of the core
strengths of airline if things fail
you're always backup processes and
backup process is
things like read repair automatic
entropy optimization where each cluster
optimizes where the data lives and make
sure that everything is synced up if
note goes down and there are copies of
data on that node and you have a
requirement for availability say of
three times across the network the rest
of the nodes will be aware of that and
we'll take that data from elsewhere copy
it out and make sure that your database
has those riyal alongside the existing
one that you originally looked for
that's quite important as well it means
that not only do you have the ability to
do away with master state relationships
you also have the ability to forget
about things like replication and
chardon and the actual management of
those things as they grow that's a big
deal it may not seem like a big deal but
if you've made the jump from MySQL do
you know SQL or you're making this job
from Point solutions to a very large and
structured data platform which is what's
going on nowadays that's huge um
horizontally scalable I think we've
beaten that one to death and low latency
unlike Hadoop and a lot of other systems
where you effectively have batch
processing by splitting it up over time
and over space reactants to deal with
stuff in flight and that's one of the
requirements that i found was missing
because you had to deal with things like
memory management process destruction
the fact of the matter was that you know
the individual processes were very very
heavy for things like mysql postgresql
oracle SQL server etc even in some of
the new SQL systems it didn't meet the
requirements with low latency which is I
get a piece of data my system actively
thinks about it and it responds
intelligently and gives me back what I
need straightaway based on the
information is there doesn't need to go
away to a Hadoop farm wait a few hours
and then come back with a response that
stuff is for its management you know day
to day you want your sister
to be active you want it to be live and
you want it to be responsive so low
latency well it's pretty low on the list
is definitely not the least important
thing there any questions okay cool so
if you have a database in the back end
that's where all your gate is going to
live a lot of structured unstructured
etc what do you plug into it and this is
actual data from bashas research as you
might expect the biggest thing that
people want to deal to actually do with
their data is index it and find out
what's actually in there whoops sorry
ran out of stage so you've got solar up
the very top the second thing we want to
do is once you've done particular
queries they bring that stuff back and
they want to push it out using lettuce
and basically say about data structures
based on the understanding of that data
so that comes up as well we have elastic
searching there which again does pretty
much the same type of thing that solar
does but maybe it's a little bit more
user-friendly which is quite important
spark is in there at about nine percent
now that seems very very low but there's
something you should know about that
spark is a very young project I mean
it's about 12 years old something like
that and for nine percent of our mission
critical enterprise-class clients one in
10 of them to actually start using it a
lot of the rest of them to start raving
about it that indicates that it actually
needs a bit of attention got things like
RabbitMQ and cough get cetera for other
and tries functionality but typically if
you got indexing which thankfully
something like reactors built out of the
box so we have so are integrated into it
you really only need to focus on how
you're going to cash out so that your
the rest of your enterprise platform the
services that you build your so your
front-end etcetera is able to call back
and get that data immediately and you
may want to think about the analytic
side so that's why we're focusing on
those parts okay so you have any
questions by the way cool okay so read
this is wonderful when I found it it
made my life a lot a lot easier before
it you still have to worry about things
like memcache and who here has used
memcache who here still likes memcache
wow that was cold redis is the key value
cash I store stuff in memory doesn't
just store simple you know buckets and
keys that actually stores data
structures now used to be you'd have to
write these down into a file or put them
into a back-end database but if this
stuff is live is being used and
manipulated retta's provides a really
excellent solution for not it's not just
a straightforward solution to have that
data available and be able to manipulate
it and presented it also provides
transactional support it is quite
important for already the data etc
provides automatic failover so you have
Redis clusters which tend to grow and
allow you to be able to provide these
kind of real-time results are two very
large systems very very quickly so the
name of the Gale game is actually very
simple if it doesn't scale it will fail
and that's quite important so react has
a focus on scaling whatever technologies
that we want which talk to our database
at an enterprise level have to be able
to scale as well and radicalize you to
do that okay any questions cool go ahead
hmm because it full sorry yep so if riac
is so scalable and so fast sorry the
reticence of scale once you pass if riac
is why do you need to use Redis right
very simple because they do different
things okay one is an actual repository
of the data which allows you to be able
to do multiple things like multi data
center replication and scaling up and
scaling out at an enterprise level you
can also serve many many different types
of requests at the same time redis is
much more limited in functionality its
primary focus is in dealing with rapid
manipulation of data of certain types
and actually allowing applications to
consume them very quickly so used to be
you had a very very slow database at the
back end and you had some sort of
caching layer CDN or something and maybe
some sort of you know in memory memcache
needs a drive gratis replaces all of
that with something that's actually
enterprise-class that allows you to
scale that part of the functionality out
properly but it doesn't replace proper
database it doesn't have for example
replication across the different parts
of a node it doesn't automatically
optimize it doesn't read your pear
doesn't do any of that stuff at all so
for keeping your data safe and actually
making it robust available cetera bava
you want to rely on a an actual database
not necessarily a caching layer like for
example Redis does that make sense cool
any other questions ok cool next step
right seeing as I'm the only one in the
room has actually heard a spark I could
pretty much say anything now until you
get access to Google so who here has
some experience with ai ai artificial
intelligence you know like the matrix
anything yeah okay um who here has done
anything in the way of analytics with
the data that they've got all right can
you give me actually doc gentleman with
a hollister shirt tell me what your
experience was was it a positive one was
it difficult how did it go positive but
hard wood I mean I would agree with that
hands up anyone who would agree that
it's positive but it's really hard to do
analytics well yeah that's about half
the guys like I have man I imagine the
rest of you haven't really tried it so
with analytics you take a very large raw
data set you process it you try and get
some sort of value out of it so you can
make a decision and if you can't do
those three things it's not really
analytics it's something else it used to
be that everything was on file systems
that weren't very scalable then people
invented HDFS and they put her dupe as a
framework on top of that so they could
chop things up and parcel them out then
you would have very large data sets from
servers logs etc the Hadoop then chomp
through and then post back the results
somewhere in the rest of your system
would make them pretty and show them up
that actually is about seventy percent
what happens in the enterprise today
spark is beginning to change all of that
what it's doing is allowing you to
analyze data as it comes into your
system in real time you can take access
of machine learning libraries that are
now proven to be reliable to be you know
customizable for your particular use
cases and then you can pull data out
analyze them
and then push that data straight back in
so if you know what your problem is if
you know what you're looking for it's
all those things in real time that makes
it about a hundred times faster than
your dupe and in a competitive
environment where you need to be much
more agile and the data volumes are
increasing in your decision space is
decreasing that's why people like it
that's why I suggest you look it up it
allows you to be scalable up to eight
thousand plus nodes get to that number
please give me a call i'm sure i could
you know work out a sport agreement with
you but most people won't it is huge so
as you grow your react and reddish
clusters you can also grow your sparkle
esters as well to cope with the analysis
and keep it at that cutting edge you
also have the ability to do streaming
analytics which plays a very very nicely
with Redis as well we talked about the
machine learning library and one of the
key advantages that it gives you is even
if you're dealing with data sets which
are no sql-based which are unstructured
you have something like an SQL based
querying system you can put on top of
your data sets to be able to pull data
out and then represent it which makes
life very very good for the vast
majority of systems that are out there
that have legacy code which is locked
into SQL way systems okay how many of
you had to work on systems which aren't
greenfield that are being you know
inherited from somebody else yeah see
you know what I'm talking about okay and
you also have the ability doodle would
not just flat bits of data but also
graph data social data of the type that
you get with social networks okay any
questions cool
so this is a basic system overview of
the kind of architectures are working to
build so now we're getting back into the
boring stuff so relaxation times kind of
over the bits in blue are the actual
client bits that you'll need a bit build
yourself the rest of it Retta services
spark services react etc typically come
out of the box so after this will go
through the process of talking through
where you get all the links in order to
be able to build this but you would
typically start any enterprise platform
by creating the database and then
working out what your particular use
case is and not allow you to pick the
data that you need and then you can
decide whether or not you need to
present it in real time or you can give
it a little bit later but Redis would
come later on sorry next after the
database and then after that once you
had you know a reliable system where you
were able to analyze it in prison
present results no problem then
typically you'd want to decide what you
did with the data that you can see and
that tends to be spark try to think of
Facebook Amazon Google Apple and their
infrastructures as complex cellular
organisms okay that process and feed off
of data that's ambient in their
environment and they tend to you know
put that data back out after they've
dealt with it and so they will generate
data for each other they will feed off
data from each other and whatever you
put into that space needs to obviously
be able to talk the native language to
be able to understand and consume that
data so all these different systems will
speak rest for example they'll speak
Google's protocol buffers stuff they'll
allow you to be able to process and
translate from one data set into another
and yeah so you guys want to take a look
at that for a second then ask me lots of
typical questions on it I already see
one in the back
okay so will react you get multi data
center application as well for the
enterprise version you simply set up a
cluster elsewhere use the command line
or something else to be able to connect
the two clusters up and you can do other
full sync or real-time replication
across what that allows you to do is to
increase your fault tolerance your
availability and it also allows you to
then separate out maybe your production
taking in of traffic from the actual
analytics of traffic so one of the key
trade-offs that we have to make that's a
really good question actually is whether
or not we want to slow down our
production environment by doing the
analytics there it's like somebody who
has to pack their tummy while rubbing
their head at the same time you know you
can do both things very very well but
generally you don't want to split out
that concentration on action processing
the data so what we suggest is you have
say a production cluster say about five
nodes or something like that and then if
you can deal with everything in-house
you don't need to replicate out etc
that's absolutely fine but if you're
going for something that's an enterprise
that people are investing in the one
security about have another cluster
elsewhere for backup and then maybe put
analytics on top of that cluster and
maybe put a lot of the you know deeper
processing the AI stuff on the second
cluster as well not only do you have the
safety of having two different systems
one to failover to but you also have the
ability to provide real live data back
to the existing production system
without impacting the performance does
that answer the question
yes that's right would be bidirectional
yeah okay so Redis would pull its data
out of the database that it's connected
to which other cluster it's plugged into
so it happens to be on the production
side it would pull that data from there
and present it straight away the full
sync or the real-time hot hot
replication happens at a different level
so it happens between react and react so
you wouldn't need to necessarily
replicate between reticent writers
clusters which i think is what you're
aiming at right now you just focus it on
the cluster that you want and you
present out to the systems or want to
consume it no wouldn't because gratis
would rely upon react in order to get
the information that it needs to present
out so let's say you have a cluster here
we have another cluster here and some
the other side of the planet right this
cluster syncs with this cluster okay but
remember it's this cluster that has
Redis attached to it and when those
things are synced up and they sync up
typically in a matter of seconds it's
not a big deal then reddit's will just
call out to its local cluster pull the
data back from that and push it out if
there's an update to it which there will
be if your analytics is going on
elsewhere and not impacting your
production stuff then when that happens
when not update occurs inside the
existing cluster then read us will
automatically pick it up and actually
reflect that no that's right you have to
it works for both so you have different
use cases you have ingress use cases and
our you know basically you Justin
projection those kind of use cases as
well but whether or not you want to do
it for that use case or the other use
case or anything else you still need to
write proxies that are able to talk out
to clients and that manage the actual
relationships themselves you still need
clients and talk out your front end or
your back end and you also still need to
manage that yourself but the key thing
is that about five six years ago before
react before red is before spark that
entire diagram would have been blue you
order have to build it yourself or you
would have had to try and take things
before maybe a little bit less good at
the individual tasks and how to bulk
them together the idea of using
enterprise class a I about five years
ago in this kind of system and it being
open source would have been unthinkable
does that answer the question cool okay
next let's get building I hate to
disappoint you but this is going to be a
lot of work all right so installing
react takes approximately five minutes
this will be available to you guys
afterwards so please don't really write
any of this down we have a five minute
tutorial there which allows you to pull
down the actual database itself build a
development cluster there's different
options etc you can do that we suggest
having a five node cluster for
production but you can have a dev
environment a very simple one for about
three notes as well and you can make
those three nodes live quite comfortably
on one laptop you also want to install
reddit so there's quick starts downloads
you these are all open source systems
you can quite happily download them work
through them in your own time install
them in whatever environments you want
and then play them through you install
react install Redis used Allspark
then the hard work begins because you
have to build the client applications
you have to pick a use case in order to
actually decide what your system will do
whether or not that be ingress or
something else and then you have to
decide what level of control you want
over your AI services or your reddit
presentation or anything else and those
are what the connectors do sorry let me
just go back on you have the clients for
connections you can connect in initially
to your data center or your database
using a vast variety of different
libraries and they exist also for Redis
and also respond but entered rise
platforms now is that they're not
usually just one piece of technology but
four or five years ago I would say yep
I'm a Java programmer I've built Java
Enterprise Services that's what I do
theories before that if I was in a bank
I'd be saying oh of course yes yes see
hash all the way through down to the
core you know and then those are the
systems that I've built now I see more
and more systems where you use
components of both ok I see more and
more systems where you have Ruby on
Rails we have micro sites where you have
a psoas of different types we have you
know zillion see architecture where you
have different even some systems have
mesh okay so you will find that there's
a lot of actual work to do with say
taking a system which is really simple
and clean and has a general purpose you
know base for building enterprise and
actually adopting it to the different
interfaces that you currently have to
deal with a lot of this stuff you can
build alongside your existing enterprise
platforms so you can have new database
I've read is there to publish results
out you can even tape you know results
from your existing systems push it out
you can have spark there try it out
player on to the different use cases
once out here once you're happy with
those you can take existing data sets we
ask you all systems or elsewhere and you
can start to process them in flight and
actually test this
out and that's really what what we try
and do once you pick the use case then
you build your kind for it you operate
upon the data you may be import some
data and you start to test out whether
or not the publishers are using red is
where publishers are using or it alters
using spark to make intelligent
decisions whether or not you can
classify things or not etcetera etcetera
you put an inflammation use case this is
pretty much what we're talking about
there's a good one at that particular
URL there other is as well if you want
to reach out to me afterwards which I
can send you out and then you also need
to create something akin to a sower now
because react and Redis and spark have
their own native interfaces for talking
to the outside world those systems are
quite comfortable in accepting REST API
calls and calls of other types as well
so you effectively have a de facto so or
three different sewers out of the box
using this system you can put another
layer on top of that to consolidate the
services and create an authentication
maybe a tokenized system etc and that
will allow you to control access to this
entire architecture that we had a look
at and finally you need to add a
presentation layer whether or not that's
Ruby on Rails or Java based or a/c hash
front-end application which talks rest
to the backend or even something like
PhoneGap if you want to do something for
you no iPhone etc it really makes no
difference the presentation there is a
rendering of the data structures that
you have the back end it's a
presentation that allows you to deal
with things that your typical users will
have use cases such as login processing
display decision making yadda yadda
yadda yadda that's kind of up to you and
finally the most painful part testing it
stays and getting releasing it okay do
you have any questions
no okay Granoff all right testing
staging and releasing hands of anyone
who's surprised by that list now that's
fine okay you start delving locally
typically use something like github or
in the past it used to be CVS before
daddy used to be sourced cafe for that
used to be just writing stuff down on my
arm and then you know go into testing so
every time you anticipate a new piece of
functionality don't write the
functionality don't try and assign it
for the love of all that is good and
holy if you walk away from this
remembering nothing not even my face
remember this write your tests first
okay write the test what it's supposed
to do before you do anything else
because that will give you your final
finishing line if you're going through
the process of coding and I have seen
this happen probably more times than I
can even realistically think about
counting that line always moves okay
always moves even if it's something
small as you're going through it you
will find ways to optimize it and that
optimization will move it backwards and
forwards and then this is a painful bit
this is a bit that you guys will
probably all remember someone will cut
across you and go we don't have time for
this we don't have resource for this we
can't do this in the way that you want
we have to do it the other way and then
you fail your original requirement
you're far better off writing out your
tests and making sure that you have a
set of regression tests that you can
actually take off and get done the next
thing you need is to test it in
performance there's no point in having
something that does exactly what you
wanted to do if takes 14 weeks to do it
and you need it in 14 seconds so you
need at least two separate performance
and regression testing environment
finally when you've done all this and
you're completely sure that everything
works Allah be everything is great you
take your designed fish and you actually
put it in a fish tank deceive it swims
to see if it floats to see if it does
what it's supposed to do because your
mechanical fish will have to deal with
an environment and that's where you find
out with things like load testing
network testing network partition test
failures outages whether or not you know
things like react or Redis or spark
actually do what they say on the tin you
can see whether or not it scales well
what the throughput is like what the
actual results are like as well and
finally when you're very very happy you
take your staging environment make sure
it's as close to production as possible
then you switch over and then you wait
for people to complain but that's life
here's a brief overview of it I like
lists but I like diagrams better I mean
Deb is where I've spent most of my life
to be fair and that's why I write my
tests before I even get down to tapping
so that I'm pretty happy that when it
gets to test functional that whatever I
write paid airline Java Sea hash or even
JavaScript god help my soul I used to do
visual basic in COBOL as well but that
was a dark time once you've done the
functional testing and you're happy with
it you test in performance and typically
you go backwards and forwards until you
reach some threshold where your
stakeholders we say this is good enough
for life that should be done before you
hit the staging environment and with the
staging environment the key thing there
is you just want to make sure that when
you push it life we will get no
surprises are there any network issues
are there any issues with scaling the
stuff up what happens if one of your
data centers goes down what happens if
your volume of traffic spikes overnight
do you have things in place to deal with
that
etc etc etc what happens if your
security is compromised etcetera
etcetera so you build all of those
things there and that's very important
gives you a chance to get it right and
to show that it's right and get sign off
before you push it into production and
then if you've done everything right
you've worked very hard you've uh you
know sweated blood and tears eventually
it'll get into production and you'll be
happy there are of course a few problems
with this wonderful fairy tale number
one it's a lot learn there's a lot of
actual hard effort involved in this I
mean I talk about it like it's very easy
you could just you know breeze off it's
just that afternoon you know sit down
there or down through the list will be
fine no it takes time and effort and
expertise most of you have that
knowledge already at some level but
things like spark the yarn familiar with
machine my machine learning etcetera you
have to actually go through and learn
that plus as most of you who've done any
airline no proper concurrency proper
scaling out proper resiliency involves
real discipline it's actually very hard
it is hard there's no getting around it
you need to actually think carefully
about it and be disciplined and be able
to take that thought process and
actually bring it back up to the meta
level how secure is it that's something
that we haven't even touched on so Rhea
is excellent in terms of being able to
talk out and talk back for security you
know secure channels in and out redis is
the same thing spark to the same thing
but they're not specific security tools
so there's a whole security layer you
have to deal with the reason I left that
out of this particular presentation was
because having worked with security
people for almost my career I've never
managed to change their mind about
anything so in general you get your
security requirements from the business
and from the security team that's
working with you and you try and work
very well to make sure that they're
happy and the business is happy like it
or not that's pretty much the
environment that we work in so the other
bits of it become
much more important to you and then you
made sure that the security is done how
do you scale this is an excellent
question that was asked earlier if riac
is so good and great why he need Redis
because they do different things than
any of them well spark also scales in a
slightly different way then react us and
registers so part of the reason for the
proxies that we talked about was to make
sure that scaling is managed instances
are managed well and that that happens
automatically and finally if you try and
do this all by yourself yeah my sympathy
you know it's a lot of hard work and
having you know some experience with
with yourselves about what it's like to
actually code in the early hours of the
morning or how you know something go
wrong or how people call you back you
need a team around you when you do this
and I strongly suggest that you do that
just for your own mental sanity of
nothing else but it's very difficult to
manage on its own so the obvious
question is well what do we do then well
we've got something in the works in
basho and we can talk a little bit about
that in terms of making it more easy to
use an automation so I'm gonna just
breeze through it fairly quickly I mean
this is typically how we imagine that
the stuff should look like right we've
got service instances for spark Redis
solar web etc etc up at the top your
actual core services things like
replication routing cluster management
locking etc that should happen pretty
much with all the blue bits and should
be built in from the word go you
shouldn't have to think about that it
should just be taken care of and then
you should just build your use cases on
top of something that works why do we
have databases because we don't want to
how to do the work of actually building
a database right why do we have spark
because we don't want to have to
reinvent all the machine learning
algorithms and stitch them together and
and it's the same thing with Redis
registers something very well but it
overlaps with something else there's
similar stuff very well but why do you
care the only care of the use case is
actually important here so it would be
great if you're able to pull it all this
if we were able to pull all this
together into one large system which can
actually even manage a lot of this
complexity for you and that's what we're
working on at the moment within bashia
where you have things like storage ins
this is like react object storage in a
document storage columnar graph all the
different types just create the
particular instance that you want let
whatever is underneath it the enterprise
platform take care of the scaling and
then just you know build your use case
on top of it and this is what it ends up
looking like we have the same Corbett
with Redis reacts park but again the
blue bits are gone because they come out
of the box with whatever platform that
you have because those Corbett's are out
of the box anyways and because the core
technology you need is to how to build
it and how to scale it effectively and
have a stable platform to put your
enterprise use case upon this is kind of
what we're hoping for now you can build
those systems yourself because it's
completely open source or you can do
what a lot of people do which is
sometimes use Amazon sometimes these
Dropbox sometimes use what's actually
there and deal with it you know and push
it out onto the cloud this is kind of a
platform as a service model most of the
people that you and I work with never
touch that with a barge full what they
would love is something where they can
how control of their Redis have control
of their spark clusters and the
machine-learning have ownership of their
data maybe have that deployed and
turning on own systems and then all that
other complexity happens the way it
happens visit the actual data platform
itself and yeah I mean there's a whole
bunch of stuff in this presentation as
well about the different sort of
functionality of the data platform
we talked about what react can do and
what red is can do and what spark can do
together but primarily you're talking
about core services of data replication
cluster management you have the internal
data store function which every single
enterprise in a world will need you have
you know message routing we looked at
Matt rabbitmq and some of the other
message queues which may or may not be
able to plug in and replace it i mean
people like to integrate message queues
at 0 mq there's even standard cues from
IBM etc those things will all have their
place but it would be great if we could
manage them all and actually tie up all
the loose threads right you have logging
in analytics right now you have to go
and build your locking analytics and
tailor it to your enterprise platform
that's a lot of work be great if we
could just happened straight out of the
box again this overlaps a lot with what
was said beforehand you have things like
cluster management so you can grow and
decrease the size of your cluster
automatically in relation to the actual
volume of data and the variety of data
that comes in you have data movers which
intelligently loads data into spark on
the basis of which data sets are active
at any particular time again advanced
functionality and you've got writing
back to be it react or whatever other
kind of internal store you have it
allows you to be able to take that data
play with it and then push it directly
back in without impacting any of the
other workflows in the system and that's
typically how you'd want it to work
again perform at scale operational
simplicity I think of labored laughs a
fair bet redis again for high
availability and incredibly fast caching
I may not have underscored this enough I
mean half of you already know what Redis
is but the other half may not redis is
awesome for not having to wait for the
database to pick the data up if the dot
is in flight and you're working with it
and other party system already need it
there's no reason to send it to the
North Pole and back again you know you
want it available you want your
applications to use it and when
reaches a checkpoint where it actually
gets dropped back into your database
again that's what you want it for and
for a lot of use cases once the data is
pulled out you want to play around with
it and unless it changes you don't
really care you know you only really
care if you have to do a write back to
it so rebus makes those use cases much
faster much easier to deal with with
things like sharding etc in Redis you
want something that will handle that
automatically out of the box so when the
key differences between we are can read
us again with radish you have to deal
with things like charting with react you
don't but you also don't get the
advantages of Redis or the advantages of
react across both systems so you want a
system that actually takes advantage of
both and serves it to your lease case
yeah and you've got things like
automated deployment a lot of the reason
for the entire system so that if you
have eight or nine nodes and you've got
them spread up let's say over two
clusters of red s and then two clusters
of spark and etc etc that's
micromanagement that's what you do to
make sure that what you've done is fine
that's like hitting the Save button
right that's great but you want to
actually play the game of winning money
of beating your competitors you want all
this stuff to be handled automatically
because the magic is in actually making
the right competitive decisions spark
will help you do that Redis will help
you do that react will help you do that
and the data platform that we're talking
about will do a lot of that extra work
for you that we talked about earlier ok
and the conclusion because i'm sure you
guys want to go off and do something fun
it basically makes life a lot easier if
you start from the bottom up and have a
solid database it's like stepping onto a
stage which actually goes all the way to
the wall you don't have to worry if you
make a misstep you know quite important
it means that everything that you build
up on top of it you can be more and more
sure off and you don't have to worry
about things failing
counters side to that is you have to
keep a track because with react you can
lose anything up to like sixty to
seventy percent of your actual cluster
and things will still operate normally
that's terrifying because if you lose
the rest of the cluster you will notice
until maybe a little bit too late so you
need to set up monitoring etc big Tyler
big data and big tech is this is so
obvious I don't even I don't even want
to complete it i mean it's here to stay
we live in a soup of information that
information is constantly being refined
and targeted to who we are and in many
ways a lot of the systems around us know
us better than we know ourselves and not
can be quite scary building your own
enterprise data platform understanding
what these challenges are and at least
having an idea of you know how people
deal with them gives you these some
notion of where the boundaries are in
that new space and how you can fit into
it and you guys have the tools to
actually be able to build these things
you know it's not hard now it's a lot
easier than it was five years ago and
five years from now God willing it'll be
a lot easier again and again building a
good enterprise architecture actually
requires discipline building just an
enterprise architecture requires going
to that list i said earlier in one day
and then running away finally i want to
reiterate that if your solution works
that's that's great for you but if it's
going to work for the world it needs to
be able to scale and not just scale in
terms of data not just scale in terms of
processing power but also scale in terms
of the nature of those processes
analytics etcetera etcetera and you need
to be sure that it'll scale to match
demand at every single point and scaling
up is almost as important as getting
down because if you don't scale down
quickly enough you lose a lot of cash
and then the people that find your
paychecks come and get very angry at you
and our work at the moment in in bako
has been up until now focused on making
react the best products that it can be
and it's built on Erlang for very good
reason and we're very happy with that
but what we want to do is to try and
take the complexity away from what you
guys and what I've had to deal with for
the entirety of our careers we want to
make these things easier for us to do so
that we can get on with a real task of
solving proper enterprise challenges and
actually enjoying seeing the results
unfold thank you very much um do we have
time for questions nope sorry go ahead
you just mentioned that scaling down is
important yeah your customers how they
do they deal with scaling down the
volume of data I mean they're they're
collecting a data filling up buckets
it's user and a captain or strategy for
dealing with when to yeah we do that so
you can AB data most of the data that
you add you won't touch more than a few
times and then after that it becomes
relatively rare to be able to pull it
back so your core data set is actually
your hot data set is typically bounded
between twenty to thirty five percent of
your overall data set at a worst case
for particularly specific use cases the
more specific your use case it can be
between five to seven percent okay what
that means is that you only really need
to have between five to seven percent of
the information that you pulled in
available to you either for processing
or for you know publishing or for
anything else that means that when you
get large volumes of data coming in
some of it you can actually archive
straight away which is the enterprise
equivalent of throwing away which means
you never throw anything away right and
the data that you're actually dealing
with you can apply spark and intelligent
analysis do that to determine which but
it's actually give you the best return
on investment so you can actually focus
on keeping those bits of data over a
period of time over a period of like two
to three quarters deciding what the best
strategy is for keeping our data inside
and and that's typically how you do it
properly if I was selling your product
i'd say go here download this your
problem is solved this is a real problem
that you need deal with intelligent
thing that means having serious
conversations people does that help
great next question okay I think so I've
been drinking coffee the whole time
thank you guys very much and I hope you
have a lovely year sometime think</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>