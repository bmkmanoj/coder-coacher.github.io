<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days 2018 - Tobias Pfeiffer - Stop Guessing and Start Measuring - Benchmarking in Practice | Coder Coacher - Coaching Coders</title><meta content="Lambda Days 2018 - Tobias Pfeiffer - Stop Guessing and Start Measuring - Benchmarking in Practice - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lambda Days 2018 - Tobias Pfeiffer - Stop Guessing and Start Measuring - Benchmarking in Practice</b></h2><h5 class="post__date">2018-03-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KSrImdsfjL4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this might be a situation that you're
familiar with you're somewhere in a
meeting you discussing something you
have your laptop with you maybe you're
taking notes and then somebody brings up
oh like this one thing is really really
slow in our application and for the one
thing you need to repeat something let's
say a hundred or a thousand times
and you know you're good programmer your
internet meeting already so yeah I got
this I know what I have to do so you're
like superhero programmer let me fix
this so you pop up your interactive
shell and then you just copy and paste
your Kunda moment a ssin of what your
repetition logic looks like so this is
basically just a recursive function that
executes another function n times okay
we got this so now let's see how fast is
this actually so you make a timer call
and you say like okay how long does it
take to repeat this one 100 times and
find out just for the simple function
that you put in it takes 210
microseconds
ok cool so but what if recursion isn't
the fastest thing so you're like okay
let's create a list in a real
application where you have a list and
then we say okay let's just use in on
each iterate over it and let's see how
fast that is and you do it and it's one
at 65 microseconds oh that's faster
interesting you're like a bunch of more
times and then it's like oh it's always
faster best thing ever so like we're
just gonna switch the code out we switch
it to this one now obviously we've has
to sell this is obviously like the
biggest success like was fast we did
good everything great and obviously this
is also the end of the talk because
obviously this is how benchmarking is
done everything is great and you can all
leave now
this is what I call efficiency no
actually this is what I call like I have
no idea what I'm doing there's so many
wrong so many things wrong with what
I've just shown you that I can't begin
to explain it so here's just a couple of
points of what was wrong in there so we
just took one sample of the recursive
call like maybe something kicked in
while we were executing that function of
the sudden was much much slower but
normally it's faster we don't know
that's repetition actually matter you
know like when you repeat a function a
hundred times and that repetition is
very fast most likely bottleneck is more
what you do in that function and not the
actual repetition there's other problems
like you were in a meeting so your
laptop was on battery power and on
battery power it works much much slower
so my laptop is about half the speed
where for a CPU intensive tasks when I
am without battery power so that's a
huge thing and yeah you just do in the
shell the shell is not your production
environment has a very different
performance characteristics depending on
your environment and so you should never
do that just in the shell also if not
repeatable is not checked in into your
repository
nobody can rerun it it's yeah it's one
of the worst things that I can see
overall but the sad thing is I mean I
made this example up but there's this
real things that people out there do
like sometimes I write a blog post about
benchmarking something then somebody
plays like Oh
executed this thing in my shell and it's
much much faster I go like where do I
even start right so let's fix this this
is an actual benchmark that would do the
problem we increased to some the size of
the list to 10,000 because we figured
out that actually what we need to and
want to do and then we have our you know
each sample so been she is a
benchmarking library in Alex your and
then we have you know each example we
have we add a list comprehension in just
for fun and we have to recurse an
example and what Banshee will do is it
will not only run each of these function
ones it will run them for a specific
warmup time which we get into later and
then we run them for another specified
period of time and then it will present
you created statistics about what
happened and so these are the results
don't know how real they are but there's
nice graphs that you can see up there
and we find out that actually recursion
is the fastest by far so our enemies did
one out there by luck or poor chance or
because if the environment was broken or
whatever it's actually 1.6 times slower
and list comprehensions are over two
times slower we can see there's a bit of
standard deviation going on but that's
fine one thing what I graph here is IPS
or iterations per second I like to do
that himself the average so that is how
many iterations of this on average can I
do per second
what's cool about iterations per second
versus average bigger is better so when
I give you a graph and then the biggest
graph is the best thing because it's the
most performant one as opposed to
averages where you know you want to hit
the smallest graph sort of that's not
what you want to do so we can see that
running a proper benchmark we get a
completely different result for what is
actually the best outcome so this talk
is titled stop guessing and start
measuring I'm Toby you can find me on
the internet as a track top I'm the tech
lead early free I run the Ruby user
group in Berlin and I do a bunch of
other things and you might have when you
were scrolling through the speaker's
page you might have noticed that I am
foxy and the organizers say actually
very very good organized because I
always sent them a selection of pictures
that they can choose from and really
good conferences obviously shoes the
foxy picture because it's the best one
and organelles took one step further
here they actually arranged everyone to
look at foxy
so you're in the center of attention
talk so to speak and ever if you think I
just put like one this on and run around
alone that's no I have my trusty
unicorns with me when I do that and if
you look closely you can actually find
another speaker on this photo but sorry
I have no one's with me today and you
know moving on with the real talk lots
of talk i love talks either talked about
one concept very broadly but then it's
very hard to you know bring it into your
daily life into your daily work life on
the other hand other talks are very very
practical talk about one true very much
in depth but then it's hard to sort of
apply that knowledge to another language
or to something else that you want to
and in this story i try to marry both so
we're gonna talk about the high level
concepts but i'll show you a lot of lots
of examples which mostly will be in alex
here and with the library banshee which
i am the creator of and one of the main
taneous along with Devon who is also
here and gave a talk about small words
small words networks in this room
yesterday yeah so before we go on we
want to figure out quickly what actually
is benchmarking so what is it that we do
in benchmarking and I have a little hope
for you to remember what benchmarking
actually is benchmarking is sort of like
we have functions and we feed them some
inputs and what we actually want to
measure is like how fast do this
function devour the inputs that I'm
giving them so in the case of these
bunnies we see what's up and how fast
eat it but we can also see that
sometimes you know the inputs don't
actually match the function so they have
some problems digesting them and then
you actually need to be very careful
because right about now come on ghost
yes you need to be also be very careful
with conditions that like your machine
needs to just do the one thing because
right now the resources here are clearly
competing and so are other benchmarking
functions can't really reach the full
potential you also need to be careful
that they're not distracted and so
there's actually my beliefs you can
follow leave them at building body trio
but yeah I hope that helps you imagine
what
functions are eating inputs and you want
to see how fast they eat their inputs
and yeah okay so one other thing that I
need to get out of the way as profiling
vs. benchmarking and profiling gets you
these flame graphs or something like
this and profiling is basically you have
an endpoint or you have something that's
slow but you don't know what is slow
what do you need to make it faster then
you can run profiling all sort of
tracing to see like which function took
the most time so this is always like
this function calls that function and so
on and so on and then you see like okay
this one takes a lot of time so if I can
optimize this I'm gonna be better that
is very good very helpful but it adds a
lot of overhead to what you're doing so
what I generally recommend you to do is
do this to find like first way the
benchmark to establish a baseline how
fast are you then you can run some
profiling if you have no idea like
what's wrong just leave what is actually
the slowest partner and then you you
improve it and then you use your
benchmark to verify that you actually
got faster so another big question that
you ask yourself when we talk about
benchmark is like what do we actually
want to benchmark we want to benchmark
runtime memory throughput or maybe
something custom and the answer as to
every question we're asking is it
depends you can make a genius benchmark
with something custom and my favorite
example is this benchmark that the
Phoenix team ran of how many concurrent
WebSocket connections they can make to a
single note and they started out with I
think they were able to make 10,000
maybe 30,000 connections and through
iteratively making better they were able
to maintain two million WebSocket
connections against a single node and
that even wasn't the end of the line
because it was limited by the new limit
file handle they said in the beginning
because therefore they would never reach
two million or go over it that is near
just run time or memory it's both of
them combined and so if you have a very
specific goal a custom benchmark is most
likely what you want to go for it's
where you ever read it's fascinating
we're gonna talk mostly about run time
here because one time is sort of the
most popular
and it's easiest thing and that's
already enough to talk about in general
so one question that you might ask
yourself when you benchmark on time is
how long will this take as in a worst
case scenario I know if you've been in
the long Widow time talk to talks
previously here they were always talking
about what's the worst case like I have
this time and then the garbage credit
can understand but I need to answer that
time so then a worst case is maybe what
you're looking for this is an excerpt
from a banshee HTML report so there's
also formatted HTML reports and I ran
sorting on lists of the size 100,000
men's 10,000 a million and 5 million and
I was actually just interested in the
worst case because it was a question if
could I afford to sort this list so I
wanted to get a value that is very easy
to compute if you have a sorted list but
it's very it is possible to compute
without a sorted list which is much it's
much much faster but it's also very
complicated I don't like complicated
code as long as I can get away with
simple code I always prefer the simpler
code and we can see here that the
maximum run time that we sort of
measured in our run the worst case even
for five million elements which I think
I'll never reach in that particular case
is still two seconds and for my use case
because it's actually four Benxi itself
it's okay I said that like the user
maybe waits for cover for a couple more
seconds they ran the benchmark already
it's fine that they wait for that amount
of time and other questions also do we
make it faster or another phrasing of
the questions like what's the fastest
way to do something and there I have a
little question a little example I
brought for you is what's the fastest
way to sort a list of numbers largest to
smallest so not ascending but descending
what's the correct from your point yes
okay so I bought I brought you a couple
of examples
so we'll take a list that is ten
thousand elements will shuffle it really
good so that we actually have some
sorting to do and then our first version
is we sort a list but we provide our own
sorting function where we sort of switch
around what is bigger
or smaller so very custom sorting
function second approach is we just sort
it and then we reverse it and the third
option is a very clever option instead
of doing any of these things we solved
by minus the value because if we have a
list of just positive integers or
probably all integers then doing a -
really results in that reversal in that
we were looking for and now I'm looking
for some audience participation who
things that the first version is the
fastest one person two three four or
maybe people just alike audience
participation here we will see second
option first sorting and then reversing
50% of the room maybe and third option
salt by - devalue uhm yeah another
twenty thirty percent and
good audience sorting and then reversing
is by far the fastest and when I first
encountered this I was like how could
this be like the other of tricks are
much more specialized like they do like
an extra reverse step how why and the
answer is most likely it's optimized
already on the Erlang B on because
sorting can then just be intermittency
and then reversing this also because we
need to do that a lot especially with a
recursive called tell recursive
functions so it's all highly optimized
so oftentimes your intuition betrays you
because it looks like we're doing more
but it's actually more efficient so it's
what
yeah exactly you need to run that custom
function a lot of times yeah
so that is a surprising result so at
that point some of you might interject
and say well say in that the root of all
evil I heard something like premature
optimization blah blah good or for evil
very very bad and for me it's most
likely that not reading the sources that
you're trying to quote is the root of
all evil and I'm actually very happy
because like almost everyone here like
quotes a computer science paper and so
now we get to do the same so that was
written in a paper published in 1974 but
I don't commute and he says we should
forget about efficiencies say about 97%
of the time premature optimization is
the root of all evil so yes he said that
that's true however if you just read the
next sentence he goes on to say yeah we
should not pass up our opportunities and
that critical 3% and in the previous
paragraph he actually mentions that a 12
percent improvement easily obtained is
never considered marginal and what we're
talking about here what you've seen
before is way above a 12% improvement
but he touches up on a good point
sometimes people just try to optimize
everything without knowing that it's
actually you know hurtful for the
application that this is particularly
slow so and also that's we need to talk
a bit about sort of different types of
benchmarks so who here is familiar with
solve the testing pyramid so unit test
integration tests the feature level
tests most people are I'll give it a
quick run-through so unit tests mostly
tests like one single function
integration test couple of modules or
functions interacting together and
feature level tests are oftentimes even
to the point where there are sort of
independent from the language so for
instance you have web applications you
have a browser that you know drives
through clicks through the application
or maybe for some embedded device you
just send it the packages and then see
if it returns to say they're correct
packages or changes something
accordingly and I have a benchmarking
pyramid pyramid that is not really
official really thing but it's how I
think about
marks so we have micro level benchmarks
that are react on like a very small
function level which is most of what
we've seen so far
you have macro level benchmarks which
are sort of on the integration level so
if I talk web application it's maybe
like one controller or what one
controller does and we have application
level benchmarks which is what that
fenix benchmark was because they just
had a couple of so I think it's called
towards connected or you have w RK that
just sends requests to your web
application and cheese how small and how
fast they answer so now what are the
properties of these benchmarks of these
different types of benchmarks first
obviously the number of components
involved goes up in a micro benchmark
you just have one function can be very
certain that like you change something
that function you change the performance
metrics of the benchmarking outcome on
application of like everything is
involved you have databases you have to
network there's everything is there also
at the same time the set of complexity
goes up in that example of the Phoenix
benchmark they spent days doing it they
had I think that more than 45 machines
running to actually do all of this it
was very very complex to do so that's
also the reason why I think they have
never done it again which is sad because
we might have some performance
regressions in there now and we just
don't know about it
whereas a micro benchmark is super easy
to set up at the same time also
execution time goes up because
exercising the whole application
obviously is most training than just one
little function and but at the same time
the confidence of real impact on the
application of it goes up because if I
exercise a whole endpoint and its
response time then I know like okay if I
make this twice as fast then that
endpoint is gonna be twice as fast for
my user so that is very very good and I
like that whereas when you do a micro
level benchmark you might increase like
your I don't know your reg X execution
time your eggs might get like 10 times
faster but that might be 10 microseconds
faster for your whole request and your
whole request might take I don't know 40
milliseconds to respond so it doesn't
really matter and that's also mistake I
often see people make like they make
something 10 times faster but by that
they save maybe 10 microseconds or maybe
even half a millisecond
but in the same week but the application
also double database query that takes
ten milliseconds so like what you gain
there is so little in the end it doesn't
matter and what also goes up is the
chance of interference so when I have an
application of a benchmark the network
could go wrong the database could
misbehave all these things could happen
to me whereas on a micro-level benchmark
I just have my one thing of function so
I'm very very certain and also Devin had
a bit of a similar figure where we were
the spectrum from like left to right
yesterday and most of the time when
people show you these spectrums they
tell you there's a sort of two golden
middle this is where so of the the
drawbacks and the good sites balance out
a bit so what I often do is when I do a
benchmark I do mark balelo benchmarks so
I have a controller that I want to make
faster so just take whatever service I
have to or the big function I just
execute that in a benchmark then I don't
have all the interference all the setup
is still am very very certain that what
I did actually make things faster or
making faster a lot so let's finally
talk about what good benchmarking looks
like and what does it look like well at
every benchmark before you do it
there's a question like what am i
benchmarking for what is my goal so you
want to make that endpoint faster
because your users complained at that
endpoint is slow or you want to have as
many as many connections to your Phoenix
web server as possible at the same time
those are all goals you can just go out
there and just benchmark for fun how
fast audience or whatever as a fun
exercise I sometimes do it because I'm
very interested in it but in the end
that doesn't really help your
application to be as fast as it possibly
could be another thing that people often
forget is the system specification like
what machine was this run on what
version of alexia what version of Erlang
or what version of Haskell whatever
people don't tell you these things and
they're an integral part of every
benchmark
especially also a few on the JVM the JVM
with new releases sometimes gets
drastically faster so there's benchmarks
when you had like an initial
implementation of a language on the JVM
1.0 and just through the new GBM
versions if
get four times faster so it really
matters which version it is on and you
are also CPU and RAM matter to an extent
which is why Benjy when you run a
benchmark of Frenchy it prints out all
these values so you know what's the
configuration what was this run on so if
people just copy and paste the output of
bench is somewhere we you can see
exactly what's up and I brought you to
benchmarking examples one users map dot
flatten so first maps and then flattens
and the other one uses or below flood
map and in this example sadness is much
much lower than map to flatten and here
flat map is much much faster it actually
twice as fast and it's the same
benchmark it's the same input size
almost everything is the same so how can
this be
well the let examples Alex here 1.4 and
the former example is Alex here 1.3 and
why well I open initial was like hey
flatten it should be much much faster
than depth of fled and like what's up
this is like the only constant that I
should know that is how things should be
it's how it's an airline in Rubin like
every other language there I know and
then just a took eighteen minutes to
look at the existing implication write
two new implementations that were both
faster then we benchmark them everything
is good and then we had a nice speed
improvement so that's an example where
like it's twice as fast now so all the
benchmarks from before without the
version information you can basically
throw them away another thing that is
very very crucial is that you have a
corrected meaningful set up if you
benchmark and you have a database and
your database only as a tenth or one
hundredth of the records that you have
in production it doesn't really help you
because you're really really likely not
to hit the performance problems that you
would have on production so the data
needs to be shaped at least a little bit
the same same goes for production
environments so especially lots of
compiled language have a flag for
optimization like really optimize this
code that's usually not used in
development and oftentimes these
optimization flags optimize a lot of the
things away that you can optimize away
look like locally in your development
environment but it doesn't matter
because the compiler optimization will
do so for you
also it's a very important event and in
turf interference-free environment so
don't have select running and your music
player and your email inbox and your
browser with 100 tabs open because that
can really hit the performance of the
benchmarks I once accidentally benchmark
on a smaller laptop with slag running
and my results were half as fast than
they were with when I shut slack off so
there's a real impact there I mean it's
partly slacks fault but that's another
story for another day that's that's also
particularly true for logging especially
on the Phoenix may analysts often ever
question like Oh Phoenix is much much
slower than advertised how come why is
this like I would cheating or whatever
and it's like oh yeah please run in
production environment and set your
local of words to production log levels
or turn them off so then you'll see
these because actually file i/o is
somewhat expensive and it's all these
things that people forget about and
really make benchmarking often very very
hard our specifically comparing
different languages even it's super hard
another interesting thing is garbage
collection most of our language these
days say for like rust c and whatever
our garbage collected and that can have
a real hard impact on your results
because at some point the garbage
collection hits here's a graph that
graphs the raw runtimes of a benchmark
that also put out by vinci HTML and
please note that down here the bottom
bars actually 251 so it doesn't start at
zero so that we get a better look at
things and you can see all these spikes
and I bet refused some of these spikes
as the garbage collector heading in and
that creates this your maximum runtime
that also increases your the standard
deviation
some of them might also be that our no
drop box started it wanted to sing and
thought it was very very important right
now yeah so that's something to be aware
of especially for micro benchmarks
garbage collection can make up a ton of
the time now let's talk about warm-up I
mention it briefly in the beginning that
Ben she runs for a given warmup time in
a beginner
why is this useful I often see
benchmarks like this in the wild where
you use the operating system time
command and then execute a program and
that's your benchmark and that's most
likely not what you want so unless
you're writing a command-line tool that
you just run once for one second or for
a type of milliseconds and then it's
good then that's maybe what you want but
I think most of us are either working on
web applications and MIDI devices and
we're working on long-running systems
they're up for hours days weeks so what
I don't care about in the performance is
how long it takes something to start for
instance so when you measure like this
this especially this is basically the
time that you measure now first you have
a start up time which might be very very
bad for for instance I mean last time I
checked especially for closure and Scala
style 10 was a bit bad and Ruby it's
also kind of bad so do you want to
measure that no like your service start
once and then a run forest doesn't
matter to me and then there's this
warmup time where especially if you have
a jittered language so we're just in
time combination at first it actually
gets slower because it does all this
work to do the just-in-time compilation
to get faster by the first that makes it
slower obviously and then afterwards we
have the actual run time so that's what
your system performs 99% of the time
that's what I want to measure
I don't wanna measure the other stuff
unless that stuff for some reason is
important to me that's not uptime for
instance Alex here and we don't have a
JIT sadly I mean we're still very fast
but we don't have a JIT but it's still
important because you know the processes
grow in memory for instance you don't
have as many garbage collections or like
that growing of the thing maybe you need
to start some processes in the beginning
your database also starts to build up a
cache or whatever so that's why the
warmup time is important and just as an
example so that you don't see like oh
yeah Bob time it's maybe like okay it's
maybe 20% slower or whatever this is a
benchmark that compares the Ruby
implementation to truffle Ruby which
once on the gravy M which is a very
advanced JVM or it's no part of the JDK
jdk jdk 9 in some parts as very advanced
there's lots of jetty
inlining optimizations and whatever and
I'm apt for you the raw runtimes of each
individual run of a specific benchmark
and we can see that truffle will be for
the first iteration takes 28 seconds
it's super slow and when you just
measure this you like oh it's slow like
I don't care about this this is
pointless but you see as time for
guesses and this age it's and optimize
itself more and more it gets much much
faster until in the end it only takes 2
seconds so if we just measure the first
iteration we're like okay this is three
times the slowest mjid but if we
actually give it a warmup time and then
measure the last iterations then we see
oh it's actually three times as fast as
ended so as a there's a really wide
spectrum here that you can hit with that
and it all depends on their method of
measurement so choose the method of
measurement that really suits your use
case or what you want to actually see
not just the results you want to see
another thing that I found to be very
very important is that includes matter
your inputs are basically like this
skyline of what was it New York or San
Francisco I forget but like some of them
are wide some of them are tall some of
them are small some of them are blocky
some of them are pointy so point being
they're all very very different and you
need to account for that in your
benchmarks you don't need to just
benchmark with one input because your
performance characteristics can be
wildly different and for that I have a
little story time and quick it's that
polish oh I hope there's nothing too bad
were in there like I don't speak Polish
I just can say it's much naked in Korea
Pasha and yeah once on a very very nice
winter day I was sitting there and was
like thinking nothing evil when boom a
slick message with a box neck popped up
and then went on and said like Alex EDB
connection connection I was like what is
that well I like my beautiful little
Alex application how does afro narrow it
can't be and I looked it up and it was
because actor has a default activist
sort of like the database tool for Alex
here it has a default timeout of 15
seconds so when the query takes longer
it throws you this nice I was like
18 seconds for Kirby what's happening
what am I doing wrong what's what's up
here so I looked at the application and
photo of background this is an
application that tracks with a career so
the career from our Android application
which by the way is written in Scala
they pushed the GPS location to us and
then we have a WebSocket on the other
side that connects the Phoenix channel
that receives these location updates and
when you connect to that socket it tries
to get you the latest location where the
courier was so that you have one
location that you can already look at
there was a coder was causing problems
someone like 15 seconds to get the
location of one courier what's up here
so I wrote a benchmark naturally and the
solution it was in production at the
time is the one above there so we have a
database view latest career location
which is basically a database view that
has the latest location of each and
every career and then we just look for
the career we want to have a look at and
then we get it and then I wrote two
different implementations the only
difference between the other two ones is
this the one at the bottom uses like a
normal worker ID is this query and the
other one uses a function every road
that basically does ends up doing an
array check with any it's fine but
otherwise what they do is they order
them by time descending so that we get
the latest one and then they just grab
the latest one
so we'll benchmark oh I found out that
it was because we had 2.5 million of
these locations because the claim was
misbehaving but still that's not too bad
like that's not something that Postgres
can handle so we were running on
Postgres 9.6 so that was weird and then
I wrote this benchmark and I see like oh
wow ok my new implementations are much
much faster so actually the old
implementation is 4,600 times slower was
like wow it's like the best performance
improvement I ever did is that I was
very proud of myself you know so again
this is obviously another job well done
so I implemented and very fast because I
hate it when I see bugs and I deploy it
was like oh well XD DB connection oh
wait I fixed you i fixed
know this like totally old applications
to running then other ones that go maybe
like because I'm deploying right now and
then boom as I go what is happening holy
I shouldn't spray so much and I was like
what
so I go back the changes and I'm like
what's happening here like I know but I
write a benchmarking - I benchmark this
like what what am i doing how could this
possibly go wrong and so I went back and
actually used multiple inputs which
finally enough I didn't do before and
this is the lesson to learn also for you
and so now I take a variety of Korea DS
of Korea's that have a differing amount
of application so we not only take the
big one with 2.30 2.5 million we also
take a career that has no locations
twenty thousand and two hundred thousand
locations why did you do it why did I do
that specifically because when I look at
it I say I saw there was failing
specifically for Korea's that had no
locations whatsoever in the database
which might happen because they don't
use our app or the external couriers or
something like that and what you can do
in Banshee because I realized that there
was a very important thing so we have a
map of input name to the actual inputs
then you can say Benji please use these
inputs and then it will pass it into D
functions - benchmarking functions where
then can use them and then cheerier run
it with all these different inputs now I
run it and I get these results a bit
shortened for you and we see that for
the biggest we're the fastest by far but
when we go all the way down to no
locations we are 36,000 times slower
than our database we're implementation
we're just like who would afford like I
didn't do the inputs before because I
was like okay this is obviously the case
that takes the most time so I'm just
going to benchmark this one the other
ones I don't care about because they
will be faster so when you encounter
something like this and there's
basically not benchmarking Alex here
about benchmarking Postgres
you have a very neat tool that's
explained analyse which sort of says
like do post graphs what do you think as
a career planner do you want to do with
this and how does it actually pan out
and so I got the query from
Hector put it into my post Cassell and
then the first thing that we see is it
does an index can index can is always
good because we use an index that's fast
and it says backwards using current
locations time index on career locations
that's good but not so good as then we
have a filter condition that just says
okay so it's sort some sort of backwards
and then it goes through like are you my
career yes no no no no no no so if
there's no locations in the database it
will basically go through the whole
table and be like are you my career no
no no no no so we end up scanning the
whole database with its millions of
locations basically which is bad why
would there but at the time we had two
indexes one on Korea D and one on time
and it decided for the sorting to just
use the index on time and it couldn't
combine them both using the index on
career ID what's the fix for that the
fix is actually pretty easy there are
something called combined indexes or
multi-column indexes where I can say
like here you have an index on career ID
and on time so and then the leftmost
leftmost is always the most important
one that should have the Equality
condition it should be do the most
filtering then it can actually use both
of them so world of migration added that
index removed the other two because I
didn't need them anymore and then these
are the results and we can actually
because of a new banshee feature where
you can save the results of a previous
benchmarking run that are now annotated
with before here my way I tweeted these
slides if you wanna have a look at them
already and we can see that now for no
locations also our for custom
implementation is the fastest with Korea
DS is very fast and we just kick
everyone else's ass basically for 20,000
beats the faster for 200,000 we also
still faster and also for the big 2.5
million for custom is still the fastest
so that was a very hard lesson learned
for me and why do I need all these
inputs talking about input so now we
found an implementation that is fastest
overall for all the inputs I could think
about but let's talk about K take hold
optimization so for those that don't
know
zatia niz if it reoccurs a function
calls for the function to itself and if
that's the last call and if the
professor and gay we're all happy
first let's look at what a buddy
recursive function is and so we
implement map and the map of the empty
list is the empty list and the map of
a-lister has something applies the
function to the head and then calls
itself on the tail and so on and so
forth why is this not tail recursive
because the last oops yeah call to my
body because the last call actually is
this list to parent operations so that's
why it's not Terri cursor now to the
tech optimized version this is the
function and then we have our own
internal function do map TCO because we
need to add an accumulator to all of
this to sort of get our results in the
way because they are in the wrong order
so of the empty lists we will just
return the accumulator and when we don't
have that we call ourself we're the last
call nothing else happens we take the
hats and we prepared the phone the
result of applying the function of the
hat to the accumulator and then we go on
but because we're then backwards because
the first is the last we need to reverse
the list and I brought you another
little example and take this
implementation and I changed the
argument order around so now the list is
the first argument and not the
accumulator anymore I'll call that doom
a PCO argument order and we see what
happens so this is the benchmark again
up here we have a variety of inputs from
ten thousands elements in a list to five
million and then we have our
implementations the tail recursive one I
put in the standard library map by the
recursive and the one where the argument
order is reversed or reverse is
different so let's have a look at the
results and we can see that for smaller
inputs actually our distended lie we map
and the body recursive one a faster than
our tell recursive ones pretty
consistently that's the first really
interesting thing that we noticed only
when we get into bigger territory five
million ten million and so on
only then is
characters of coal faster which was
fascinating to me because I always felt
like if I can make it ever occurs if I
should because it's optimized and
obviously much much faster isn't always
the case and the other thing that we
notice is that the one with the reverse
argument order is always faster than
enormity recursive one and why is that
and thankfully me how told me yesterday
there is a special optimization in the
LLVM we have like the first argument is
like a list destruction or whatever you
get ahead as a special performance
improvement for that compulsion is asian
but you also told me that in long 21 it
will be optimized for all points so this
again will not be useful much anymore if
you run on PBM 21 ok have a look at the
graphs and there we see it yep much much
faster unless we have a very big input
now you might ask because tech
optimization what about memory like we
don't only care about one time I said
this is mostly going to be about one
time know about memory so right now we
have a pull request by Devon to add
memory measurements to bench e with lots
of help also from me how to figure out
how we best do that and let's see how
our memory scales with that and what we
can see is that the tail recursive
versions because they also need to build
up that accumulate a list and then also
do the additional reverse oftentimes
much much worse memory wise as well so
in our big example they are the fastest
but they also take twice as much memory
as our body recursive functions so its
tail recursion bad no obviously not this
is just one example it could be very
different another example and also very
importantly if you have a function that
has to run all the time so that never
finishes like in the gen server or
something without elko optimization
everything will blow up so it's still
very important very good but just be
just don't take it as like a granted
knowledge that this is always what you
should do
so nearing the end of the talk what even
is a benchmark it's not just bunnies
eating stuff it's also something else
and I found a very good answer for
myself when I was designing Vinci
so this is also an official interface
invention you can use it and this is
sort of the steps that in my mind a
benchmark goes through first we have
some sort of from configuration that
tells me like the warmup time and a
bunch of other things and that I just
take an initialize Banshee with it then
I get a bunch of system data so I can
see like okay this is on the CPU this LX
your long version whatever then I define
my benchmark so I have a job and then
the function whatever it does I've made
might have a bunch of those then I
measure so then I get all the raw
runtimes or the raw memory measurements
then we compute statistics based on
those so Sally didn't get to talk to you
about statistic but we have coup status
like maximum minimum 99th percentile
media and we also draw box plots for
instance and then when I have all these
statistics I go I can go through
different formats that just print them
out in different formats so we have four
meters for like the console formatter
that is built in HTML JSON and sees me
so that's audio information you can get
and that's how a good benchmark should
look like in my opinion before I leave
you here I really want to encourage you
to run your own benchmarks don't trust
just people that put it online or
somewhere and then trust I go this is
faster this is what I should always do
your environment might be different your
use case might be different everything
else might be different like the
benchmark might have been wrong with an
old Alex URL and version you might not
really care about it so please run your
benchmarks check them into your git
repository we have a bench or benchmark
benchmarks folder that has the
benchmarks and also sort of as a comment
the last result when we ran it the last
time so at any given time they can be
rerun against new library versions
against new inputs or whatever so you
can always see is our assumption from
back then that this is the fastest this
is still the fastest and then a further
adieu please enjoy benchmarking
hi there thanks for the lecture one
question the Vinci tool that you're
presenting is just for elixir or it can
be used for benchmarking on Elim on what
underneath or near land on their base I
for databases you can use it as long as
you have an apparent elixir or a long
driver for it but generally it it
executes Aleksic code so clearly it's
used for benchmarking elixir code but I
bench the database was just a fun thing
that you can also do if I think I've got
results out of it alright thank you no
please coming to your thing
yes how do you determine the warmup time
of the thing that you have benchmarking
and but now it's set in the
configuration so it's not dynamic or
anything so the default is two seconds
but you can set it to whatever
it's basically then in experience so if
you see benchmarking you run it and as a
high standard deviation try to increase
the warmup time but there is approaches
where you can like you can have a
confidence interval where you can say
like okay I want to have this with isn't
that confidence and then you can
basically benchmark for the time that
you need for that and you can take
measurements so like how big is my
standard deviation and then do the
afterwards that's not implemented yet
but that's an idea I have for fall in
the future but I have way too many ideas
for Benji and I there's so many things
already and it's still a pre wondered
over j'en because I always one like I
want to have this other feature before
we go 100 and I don't want to break
compatibility for benchmark for all the
plugins so I really need to like
definitely needs to shut me up and then
be like okay we we release 100 now so
yeah thank you okay I think we have time
for one last question
hi I went into the continuous
integration
sup like to see if some person was
broken and started and like to detect
performance degradation it's a very
interesting question that I get a lot
and it's very hard because often times
the CI service are very overloaded and
so then the times fluctuate greatly so I
li what you want to have is you want to
have like one server that just sits
there is all alone and then executes
your benchmarks but for what was the
event called me how spawn fest thanks me
how for sponsors actually build a
service like as a service that so does
that for you you define your benchmarks
and it runs them I but I think it just
rather just plots them right yeah and so
what I like as an approach for that is
that you don't fail the build
immediately because it might have been
some interference but you plot the
results so then you can see a trend if
there was just one spike once maybe
something was wrong with the service and
then it's bad it's back again but if
it's consistently high up after a point
and you see you have a problem you have
the build number the commit hash
whatever that introduced the problem you
look at it you fix that that is sort of
my preferred approach okay thank you
once again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>