<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory 2014 -- Riak Ensemble A Paxos based Framework for Scalable, Consistent Systems | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory 2014 -- Riak Ensemble A Paxos based Framework for Scalable, Consistent Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory 2014 -- Riak Ensemble A Paxos based Framework for Scalable, Consistent Systems</b></h2><h5 class="post__date">2014-03-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ITstwAQYYag" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so yes thanks thanks Russell so yes
today I'm gonna talk about ryokan sambal
and actually the interesting part about
ryokan sambal as you'll soon find out is
reacting samba laughs she doesn't build
on react it's a separate thing we built
to add shrunk consistency debris off
because adding strong consistency to an
existing code base it's hard it's
actually good to have a much smaller
build from scratch library that you can
test and build from the get-go to
actually provide the guarantees you want
but we'll talk a little about that so
what is Rio yeah I'm Joe a bass show but
also I'm Jay two people on internet many
of you may or may not have follow me but
I'm Jay to Paul and Twitter and github
IRC channels and so forth so if you
follow me this is me hi so anyway I'm
reactions on bow is a patch this bitch
framework for scalable consistent
systems and we'll talk about what that
is in this talk but before we do that
it's useful to understand sort of the
motivation what's the problem we're
trying to solve and you know why are we
and you're not that where we are so of
course in Erlang we know early makes it
very easy to build distributed systems
scalable tribute systems in this case
here we have eight nodes you because of
the after model you can you know have
the same communication pattern for local
actors which would a little blue boxes
in this picture here or remote actors
and Erlang just works it's easy to scale
this to 60 no 32 no 128 knows whatever
there's some communication they can run
into in the end and there are people
working on that but you know in general
you can scale at least four
shared-nothing kind of systems where you
know message-passing works for you which
is great the problem of course is what
i'm swen you have state right why don't
you have shared state well crap state is
not a shared nothing problem anymore
it's a shared problem and so most of you
imagine and most of us in general when
you want to solve this problem and up
using a database right you might have a
set of nodes in this case here for
Erlang nodes that you implement your app
you have a database behind it maybe it's
a single node database maybe it's a
really cool awesome scalable database
like react which in this case there's
also written in Erlang great and skills
to eight nodes here and that works well
and this is you know what most you
probably do and should do build things
on top a database is that someone else
is handling the scalability problem for
you problem is is this always doesn't
work right what happens if you're
actually writing a database
what I'm sure if you're me we can't make
reoffer by adding a database to react
otherwise just use the other database
that react is using right that makes no
sense also what happens and you know
this is a database any other kind of
thing you might want to build to you
know a messaging system or any kind of
infrastructure component always use you
know yeah we're going to bring in an
external
a basic component may not always be the
right idea and of course what I'm safe
is you know small state right a lot of
applications are just small amount of
metadata configuration information
things like that you know data in the
kilobyte 2 megabyte range you don't
really want to then say oh yeah you need
install it's heavyweight database just
to manage my you know 150 K routing
table consistently that's a little silly
so it would be nice to have an embedded
database library that you can use and of
course our language would say hey that's
what monisha does which is true Monisha
is the thing that's been built into OTP
which allows you to do that the problem
is if you build a lot application on
mini Jean there are plenty out there
you'll eventually run into an error that
look like this which basically says
here's the deal you had a split hide a
network partition it leads to a split
brain scenario both sides of the network
partition allowed rights to occur
Monisha then sees later gee I have two
inconsistent views of the world what do
I do and now there are things that were
added later on OTP where you can set
settings to mineva that say ok fail
rights if there's not a majority people
connected that's not fail proof there
are still corner cases will still run
into this scenario fundamentally why
does this occur well it's because the
cap theorem which probably heard a show
people talked about before but the cap
theorem it's an acronym see ap this it's
consistency availability and partition
tolerance and fundamental here the
theorem says you can have two out of
three and in the real world
unfortunately never partitions our
reality of life even if you have the
best network equipment that doesn't
prevent a janitor coming in and
unplugging a wire in our traditions can
occur and when they occur you have a
split brain and so the whole concept
behind cap here is that you really only
have a choice between building system
which are always consistent but not
available during a network partition
so-called CP systems and systems which
are always available but are
inconsistent or a network partition like
AP systems which react today is one of
those and you pick one now again neither
of these have to do with high
availability right both of these
scenarios can be highly available you
can build a highly available system that
can tolerate you know node failures
three out of five nodes failing and
still be up and be consistent there are
plenty of systems that do that yackin
Samba does that as we'll talk about but
the issue is during a network partition
you won't be available because a node
being offline is actually an easier
thing to handle then a node being online
and not being able to talk to anyone you
know that's the harder scenario so just
to illustrate this real quick this talk
isn't really about cap but you know
there's a simple scenario here where we
have five nodes and in this case here
nodes 3
four and five are the primary owners for
some set of data we care about in this
example and we have three clients trying
to read or write this data so in the
case here where everyone can talk to
everyone both a P and C pieces were
totally fine everything works the
world's happened but let's say we have a
network partition now you know and for
those who aren't quite formal what I
mean by never edition think about if
you're on IRC and you see in xsplit
right and suddenly half of the group
goes away and later people come back not
just you know servers not be able to
communicate to each other so in that
scenario here let's say notes four and
five can't talk to nodes one two and
three well when a CP system you can't
let both sides succeed here because one
side be inconsistent so only one side
gets to succeed in this example and most
systems is going to be the right-hand
side here because it's a side with the
majority of the replicas which in this
case here we're three four and five so
four and five but again as far as the
clients on the left-hand side are
concerned your database is unavailable
right it's not it's not highly available
stars they're concerned they try to
access the database and are told sorry
database unavailable try again later
okay whereas an AP system like react for
example works just fine in this
partition you even works fine refers to
partition it's even works fine if you're
in a very unlucky and you're in this
scenario where you have four partitions
but again you were to give enough
consistency right a right that occurs by
you know the client talking to node one
is not going to be seen in the reads
that the client talking note five does
at least not right now of course
eventually the partitions will heal and
eventually the data converges which of
course is eventual consistency and you
know the whole idea behind eventual
consistency is basically what I just
said there but to make that concrete
imagine scenario we'd have the value
it's currently a you do a network
partition and we update the value on two
sides of partition one side sets it to
be the other side sets it to see ok
that's inconsistent but what happens
when the partition heals well what
happens here is you end up just merging
the values and then your future reads
sees both both values that's a little
interesting and that seems kind of odd
if you're coming from a consistent world
but there are a lot of people who do
build apps on react they can handle that
because there's a lot of classes of
applications that are actually very
tolerant of this generally these five
types of applications applications are
you only write your data once you never
have conflicting rights because it's
fundamentally by definition impossible
applications where your data is
immutable or idempotent it doesn't
matter because every time you write a
value it's the exact same value you're
writing and so again two conflicts you
can just pick any of them the same
so those are straightforward and a lot
of content systems or that there are
cases where you can just in your app
maybe just pick the last one the most
recent one or pick a random one that
works fine too and of course there are
cases where you might have business
rules where it is just up to you we give
you all the values and you pick one
because you have business rules that can
choose that may be your most valuable
customer is the one who gets to see it
on airplane not the other guy okay and
then also there's a lot of research out
there to build eventually consistent
data types someone earlier in this room
entrance ERT teased
I think this interest Michael Johns talk
which is a set of you know data types
you can build on top of adventure
consistent systems in which for a period
of time the data diverges but eventually
it converges to the exact same values
that would have had if you update it
everything sequentially these are what
counters and stats and maps there are
ways to build those where while they are
temporally divergent they always
converge to the correct count in a bit
and and so because of this really well
for a lot of systems the problem is is
there's a lot of apps which aren't one
of these five cases or maybe they're 95
percent of these five cases but you know
they still need some strong consistency
and today what a lot of people will do
is they won't use react or they'll use
react and tied into Postgres or my
sequel or whatever and so what we want
to do at bash o lycium add both of these
at AP and CP to react and again as I
said earlier we chose to do that by
building a separate library that
provides CP and integrating that into
into react and that's reacting sama
there's the motivation let's actually
talk about a little bit better react and
Samba now action before we do that let's
talk about consensus algorithms so the
whole problem with you know strong
consistency or any consistent system
really comes down to distributing
consensus if you have multiple actors in
a system multiple replicas and you want
them to agree upon the state of the
world and as far as consensus algorithms
are concerned there's three main
categories now there's a few minor ones
too but it's quorum consensus algorithms
there's a chain replication and there's
a virtual synchrony
we're gonna talk about quorum consensus
algorithms today because that's what
react ensemble uses and that's what most
systems use in the world as well
although there are some database that
use chain replication hibari for example
as an early length database that
actually use a chain replication for
consistency in hyper decks another one
that's not not renewing that does and so
as far as quorum consents this concern
most you've probably heard some of these
before but there's three main form
consensus algorithms there's
paxos there's a zookeeper atomic
broadcast protocol and then there's
raffle which is fairly new and of course
I said I'm slide to this whole dock then
this is a Paxson's based framework so
obviously you know we're going we're
going to be going to paxos but we'll
talk about paxos but we'll talk about
the other guys in a minute to just
briefly and so paxos right path is a
sort of the granddaddy of distributed
consensus algorithms corn-based
consensus algorithms and a lot of folks
stop what PAP's has been complicated and
Patos can be and they're interesting
parts of paths that can be that we'll
talk about a few slides but very simple
trivial paxos is actually not that
complicated the at least theoretically
it becomes complicated implementation
but it's not too hard in fact here's an
entire slide that shows you the sequence
diagram for basic boxes which would be
nice by animated this but well I'll just
use a nice little pointer here yeah so
here's the deal that's the height you
unpack those what's really interesting
is perhaps this is a state-based or a
value-based consensus Agra and the idea
here is you're trying to agree upon of
the state of a current of single value
you know and so what you do here is you
have three nodes in this case it could
be five notes could be seven nodes
usually it's an odd number of nodes that
she'll kind of use and we can talk about
why later but in case you have three
notes and no one here wants to change
the state of the system and so what he's
gonna do here is he's going to go ahead
and he's gonna increment what's called
the ballot number in this case here is n
because n shorter than ballot number and
he's gonna increment the end and he's
gonna send this value to all the nodes
which includes himself so in this case
here himself and notes too and dream
with a prepare message and what those
notes you're going to do here is when
they get this this this prepare message
is they're gonna compare that number to
the highest known ballot number they've
ever seen if this is higher than any
value they've ever seen they're going to
accept this and they're gonna send back
a promise to node one with two things
saying here's the ballot number you just
gave me and here's the current value I
have for this this value or the state
and if node one gets back a reply from a
quorum of nodes before it times out then
he can proceed if it doesn't then the
request fails and you know you give up
or maybe you retry you know that depends
on the particular system you're in but
fundamentally you go back to step 1 re
increment the ballot number and continue
on but assuming this eventually succeeds
you move on and node 1 decides where the
next value should be and you can pick
whatever you want usually though it's
some function of the values you got
he picks the most recent value and then
from that apply some modification
function to it and knows what the new
value is but this could have just easily
been this was a request coming in from a
client that says hey set the value to
ten in which case he's gonna ignore the
new values anyway and just say the new
value should be ten but it's really
matter anyway no one picks whatever he
wants it's his choosing and then he
sends a commit message back to all the
other nodes and says hey commit this
value and here's the the ballot number
you know again that we agreed upon
earlier and again at this point in time
if all of the nodes still haven't seen a
ballot number higher than that and it's
coming from no node one again everyone
accepts it they write the value the node
send back and accept and if you get a
quorum it accepts back then you know
that the system is consents consistent
and there is a you've you know wrote
this value that is the current state of
the value as far as the the quorum is
concerned and you can reply back to the
client or whatever the system is running
on top of this that and say hey the
current value is you know VN whatever I
just wrote and again if this fails if
you didn't get a quorum back the request
fails you know and it's a failure or a
timeout however you you give that to
your to your users so okay that's kind
of straightforward I mean there's some
implementation details here although a
lot of implementation details are harder
in you know Java or C right and Erlang
you can implement a basic Paxos diagram
like this with it you know a couple Gen
DEP stems you know run at some nodes and
boom you have a little prototype
practice thing that you can hammer out
and it kind of works so it's kind of
cool
the interesting part here though is you
know you do it for every single request
the whole roundtrip and the problem with
that is two round-trips per request
first round-trip to prepare second round
trip to do the commit it isn't too bad
but it turns out you can do better in
most real-world systems
and that's where multipath this comes in
which is an optimization and the idea
here is multipath this is if you have a
case here where you're gonna have the
same node proposed multiple values over
time then you can have a long-lived
ballot number which we generally call
epochs or terms when you're talking to
multipath those and so you can skip the
first round after you at once so there's
two phases phase one where you're
establishing your term and then all the
subsequent phases where you reuse the
existing ballot number from before so to
show that briefly on the first request
we do it this applica we just did a
minute ago it's the full two-phase
prepare promise commit except except
we've changed a little bit here now
there's now two things that matter
there's an N which are still the ballot
number and then there's the I which is a
sequence a sequence
so when you want to start a new phase a
new round a new epoch we increment the
ballot just like before we set the
sequence to zero and we do the full
prepare promise except now we're
providing the NDI instead of just the
end and then again we do the same thing
before we're providing both the enemyÃ­s
now in the later stages down here but
again if this is what we just talked
about effectively before the difference
here though is that for each additional
request we reuse the same end that we
already agreed on and so we're not going
to increment that anymore and so we can
actually skip the prepare phase and we
only need to increment the sequence
number and directly do commit and get
the app back and as long as it's still
node one with the same end with an eye
which is higher than the previous eye
everyone accepts it if any of those
conditions are wrong then people will
not accept it you won't get a chorus
months you fail node one is no longer
the leader for this particular epoch
someone else will eventually come along
and restart back at the phase one where
they'll rien come at the ballot number
which I'll again and establish a whole
new epoch that is logically higher than
any request this node could have done
again in a well-behaved system where no
one's lying which is we're assuming here
so that's multipack so this now again
details but you know this is a forty
minute slot talk so that's about what we
can do on the high level here but it's
pretty straightforward right
two slides here's what you can do corn
consensus um why do people talk about
caps just being hard well it turns out
the main problem you have here isn't the
details the main reason why people
usually make it hard is because
integrating pathos and real systems lead
to some challenges the main challenges
in database systems
oh sorry real quick we'll talk about
that's one record by the way in the
common case duh we just saw that so
that's why it's great anyway the problem
is that shipping the entire state is
expensive your state is big right so for
cases we're talking on metadata routing
tables that's great but what I'm safe
your state is a database you know you're
using PAC with your database and you're
you can't ship your entire database
around each time that would be a
gigabyte a terabyte petabytes that would
be silly and so using pure direct
value-based paxos to do consensus over a
database has always been a problem
people don't want to do it that way so
what do they do well they change it from
using Paxos to directly agree on the
value of the database state to using
paxos to implement a replicated state
machine where instead what you're doing
is you're basically doing a replicated
log of commands and you're agreeing upon
each consensus round on
command should be at a particular point
in the log effectively and so that
everyone is coming to a consensus on the
sequence of commands in which case you
apply all of those commands
independently in all the nodes you get
the same state okay that makes some
sense obviously high-level hand-waving
here a little bit if you want more
details of that they'll paper I can show
in a second the problem is when you go
and do that you end up with a lot more
problems now because this is actually
hard because now I have to do a whole
lot of failure scenarios so the nice
thing about the Paxos thing we talked
about before pure state based pathos is
a node can go offline for years or
habits data delete it or be partitioned
whenever it comes back it only needs to
go through one full round of proposed
commit accept and it has all the state
because you always provide the state
each time but when you move to a system
where using passes and it log the log
you're not shaping around each time
you're shaping around just the most
recent thing and so you have to
implement all these other things on top
of that system that deal with log
recovery so there's various issues
there's lock recovery log recoveries an
issue where a node is missing some
entries in its log and need to figure
that out and bring it up to date there's
log trimming which is the case where a
node actually has entries in its log
that no one else agree should be there
well you need to identify them and roll
them back there's also the fact here
that you know your log can't just run
forever you'll eventually run out this
space so you periodically need to roll
it up and your snapshot it what you need
to deal with how does that fit into your
whole scenario here and there's a lot of
challenges with that and there's a paper
that Google wrote patches made live
paper there many years ago which
actually talks about most of these
although they leave out some interesting
tidbits which are still exercise to the
reader but you can certainly go and look
at that paper and see sort of hey even
Google found is really really hard and
so that's what people rag friends when I
say patches is hard because doing that
is really really hard and doing it right
is really hard the good news is there's
a lot of other smart people realize hey
the problem with this too is there's no
formal proof of Enya's working it's sort
of hey the only thing we really have is
Google wrote something and it kind of
worked for them awesome so the better
way to do this might be to actually
build the whole notion of log base
replication into your protocol and
that's actually where the other protocol
is I mentioned earlier come into play
zookeeper and raft both do that they're
actually formal proofs which are
effectively a consensus system which is
very similar to patch those tied to a
log replication system where they're
both part of the core protocol they're
not here's packs those plus the
separated logs I'm sort of tied together
and there's still a lot more slight
differences between it but conceptually
you can think about that way
and of course zapped the zookeeper tonk
broadcast protocol which is using
zookeeper which many of you probably
know about it's an Apache project in
Java if you want to learn more about
that I don't any time talk about today
although I can offline you can read the
zookeeper paper or the other zookeeper
paper or the other zookeeper paper or
the others to keep your paper and
there's actually two others and there's
another one I think that's been
published soon let's have a lot of
papers yay
one of the interesting things that's
actually very new paper is actually
dealing with dynamic memberships the
zookeeper today has six membership you
have a quorum of three nodes you can't
ever add or remove a node later there's
a whole new paper that how you can do
that that's actually in zookeeper to
five and will come out when zookeeper to
five comes out there it is on the
development branch now with the paper
for that to which is that up here but
there's a lot of really good research
that shows hey we formally proven a lot
of things so that's one approach if you
do want to build a system and don't want
to use what I'm gonna talk about today
go go look at zookeeper
of course if you don't want to go look
at the java code of zookeeper you can
also go look at react SAP which was
actually a port of the zookeeper atomic
broadcast protocol that I did about
three years ago to Erlang it's three
years out of date we've been touch up
sense and but you know you can at least
hey here's how you might implement this
in Erlang if you were so inclined of
course it's also raft so raft is the
protocol that is a new another consensus
protocol it's just all so similar to
packs those or consensus with a log but
it's a specific focus it's the thing
it's trying to solve is being easy in
fact the one paper that exists for it is
called in search of an understandable
consensus algorithm it's whole point is
to be easy to teach easy to use easy to
build it's pretty easy it came out about
a year ago as far as this paper and
since that time there's been
implementation of raft in almost every
language you can imagine you can go to
raft consensus that github died oh and
find one and if you care about one for
Erlang there's one called rafter which
is actually written by a colleague of
mine at basho andrew stone and it's
actually pretty cool implementation and
you can go and take a look at that
sebacio we've tried rehab stack we've
tried aft and you know there's different
code out there different qualities and
these are interesting things to go look
at as well so of course all right I've
talked about all these there's some
entry things patches and logs that's
cool and there's differing ways to do
that you can do passions logs you can do
raft you can do zookeeper knockin Samba
now I already told you earlier react and
Samba is paxos based why you know didn't
I just tell you there's a better way to
do it shipping the entire state around
it's kind of crazy
sure
it is entirely crazy if the thing you're
trying to deal for your state is your
entire database if you treat your entire
database as a single monolithic entity
paxos doesn't work very well you have to
try the more complicated algorithms to
do that however Patras itself is fairly
simple and fairly easy to write if that
is something you can use if you can you
know just use state-based packs those
it's a much simpler and easier to
maintain code base so it's in some ways
a good thing to do if you can and so we
ended up doing with fasho for for Rioch
and then what ended up being part of
rapid mumble is we realized oh we can
solve this in a different way which is
to focus on microstates here basically
the idea here just make sure you have
small states and instead of having your
database be one giant state you just
have your database be a bunch of
microstates and what you're doing pact
is n depending for each of these which
actually works very very well for aria
because we have two key value database
the nice things is also solves a second
problem number I mentioned it's a
scalable protocol if saw the scalability
problem because the problem is is that
in traditional consensus rafts App
zookeeper doesn't matter you know the
more node you add your consensus group
well the more reliable you are you can
have more and more nodes fail and still
be up which is awesome but generally
after about seven nodes it makes no
sense at anymore so usually you do three
five or seven you need an odd number so
that you can you know tolerate the up to
F failure so to F plus one it's
basically you want so tell you it one
failure you three know it's a Tyler -
Tyler change five notes tolerate three
failures you need seven notes after that
and you really expect for nodes to fail
all at once probably not I mean you can
you can certainly go with nine nodes but
you basically eventually run into you
know a stomach reason for reliability
don't add more so performance though
right a lot of sugar database you want
to add more and more notes get faster
the problem is if you add more and more
nodes to a single consensus group that
can sing that script gets slower so you
can't really add more in scale on the
other hand if you're doing a bunch of
separate complete independent packs of
transactions of complete separate states
you can just add more and more nodes
which are part of complete separate
paxos groups and you know you can scale
it out that way i scaling out the number
of sort of independent decisions you're
making sure that doesn't help you if you
need to scale out you know agreement on
one value but most real-world systems
right scale on the data set size not on
everyone touching the one dollar in one
over one name and this fits well in
react because again key value database
so each key you
an appendage day and again this doesn't
have to be a key-value database um I
think you know but you said this is a
generic thing sure and this could be
used for you can build a gen server on
top of this in fact I had a prototype I
tried that where your keys of hid and
the value is the state of the gen server
right and what you do is instead us any
message to it you go on to react
Ensemble and the the update function
which we'll talk about later is actually
just something emulates like a gents or
a handle call and suddenly have what
looks like a you know scalable dancer I
mean each independent Gen server would
be an independent access group so
there's you know this is somewhat
generic and key-value yeah but key value
you can extend that in many ways so of
course that's a little bit the semantics
of what this gives you so we're really
focus in here is on conditional single
key atomic updates and again this a
single state doesn't have to be a key
but that's just the phrasing we use here
and it's can its atomic so it is you
know atomic just the same as if you were
doing you know compare-and-swap kind of
things on a CPU you know it's the same
level atomicity per guarantee in here so
full sterilized ability to a single key
Oh
button sorry guess there's a weird
button that gets rid of the slide show
but doesn't ring it back are you
watching the video I'm sorry okay so I'm
anyway um so we have a thing with atomic
updates here and what this also means
it's basically a read-modify-write cycle
where the modify here is after your
Erlang function so you apply in our link
function and what happens here is we
actually get the most recent value
therefore degrees on we apply the
modification function to it and we write
it back and so it's a consistent update
to that so it was consistent
read-modify-write and of course this
function you have here can either return
into value or can return fail generally
you can think of this as fail if a
conditional X for example if a value is
modified concurrently so in real for
example our modify function here is just
checking our certain die that's passed
in and seeing that if this value has
changed that means someone else
concurrently updated the data so it
gives us sort of conditional
compare-and-swap property but you can do
other things here too you know you could
prove example just always overwrite the
data I never failed if you wanted to but
you know it's fairly generic
get modify important kind of approach
and of course the design so how is
actually built so go back to multipath
so this is multi Paxos simple multi
packs those perky of course it that
means if we have a billion keys do I
have 1 billion complete separate
independent consensus groups
all with separate leaders all with
separate parent promise phases it could
we don't probably doesn't scale very
well centers know what we do instead is
we actually sort of we group things
together we actually have you know some
subset of consensus groups and your you
know your partition your states across
them a nice thing this is dynamic it's
entirely up to you to add consensus
groups as you want so and by the way
consensus group is a large word so
actually we call them in Samba 'ls which
sends the name react and samba so we'll
call them Sowell she was point on in
this talk but so you can have as many
ensembles you want and you know you just
say hey in Samba a owns these keys it's
up to you to decide how you do that
routing as far as mapping keys so in
Samba l--'s we rockin trembles mostly
focus on creating assembles and send a
request to them by you know a sambal
key-value sort of your your your three
things that you give it and so again
each of these in Samba 'ls actually
emulates the perky Paxos so how does
that work well there's two things we
care about you know there's a pop
there's the sequence numbers and there's
the actual key value state and so the in
Samba itself actually handles the leader
election which is sort of the guy who
becomes the main person who does the
first round of paxos and stays the
leader and subsequent request that in
samba land so he elects the leader he
does the first round of Paxos the reason
you have a leader election rather than
just relying on the stuff I showed
earlier is if you just rely on the stuff
you have earlier without a leader
election without gating that then you
can have a bunch of nodes try to all
become the new you know the new state
they'll all try to propose a higher
ballot number one will fail you retry
retry eventually you might succeed but
you're gonna get a lot of failures so
generally what you do is you have some
kind of leader election that gates it
the leader election isn't used for any
guarantee any safety guarantee the paxos
round first-round guarantees you to
safety
the leader lap ssin is just designed to
make sure that you're on high like
probability don't have five nodes all
trying to become the leader at once you
know proposed a new ballot at once so
the become leader they established a new
epoch that's doing the first round of
multi PAP so that we talked about
earlier and they support you know the
whole kit modifying put thing we talked
about a minute ago so to establish new
epoch it's what we just talked about
it's literally the first round of multi
Paxos that's what the sample will do the
key thing here is that different samples
do this independently so you might have
been someplace on will be in Samba lei
will do this to establish the new epoch
for that particular Samba for that
picker leader but a complete different
Samba would do this and a penalty from
that
these could be talking to different
nodes or two different pits on the same
nodes you know it's it's concurrent and
you just wants to establish a new leader
of course and the state that we're
agreeing with here is basically just
that he popped the sequence a member in
the leader that's the state that we're
using during that multipacks those
transaction and then of course we have
the actual value associate with the
microstates the so-called key value
objects at least the parlance we use in
reactance omble this includes the epoch
of sequence and Akina values again it
says packs us right path just you have
the epoch the other sequence it's the
end of the eye
we talked earlier and you have like the
key that's just the unique name and then
we have the value which is you know the
thing we were passing on the V impact us
the idea here is this epoch in sequence
are sort of a subset of the epoch in
sequence within Samba so the in Samba
when it writes a value writes the
current epoch in sequence of that in
Samba to that key but in the future when
you read the key off disk you know
you're gonna treat the epoch in sequence
of that key to be what's stored in that
key not the value of the current in
Samba so you sort of our emulating you
know the actual current state of this
key is what's stored not the actual live
state and we'll show that in a second so
to do a put you know to do a write to do
a modify and whatever you want to call
it we do something looks a lot like this
is for a key now this is how we email
eight Paxos on that sample we just
looked exactly an two round-trip looks
very very similar to multi Paxos here
the first day to multipe axis but it's a
little bit different so the idea here is
let's say node one against our leader
here's an ensemble leader now for a set
of keys and a request comes in it says I
want to modify you know key five node
one here it's gonna read key five
locally his local value off disk and
it's gonna check the epoch of the key
name of that object and he's gonna see
is this epoch older than the epoch that
the the the leader the current ensemble
is so perhaps the current Samba was at
you know we bought five the key you read
off is that epoch five or perhaps a
teapot for you know from a previous
leader who wrote it in a previous epoch
in any case if it's before the current
epoch this basically means and this
particular key is not current it needs
to go through the full two-phase multi
paxos just as if it was an independent
access group basically and so what
happens here is we do sort of what looks
like the first round of multi path so as
we send a get we get a reply the
difference here is this get sort of an
embedded prepare because these guys were
following this in Samba only follow the
only reply to the get if there's
following him I despair and ballot
number so there is sort of an implicit
ballot number in here that's not shown
and they will reply back and say yeah
your current leader we're following you
here's my current state and here's my
current object and of course each of
these objects have come back they have
the ah you but they also have you know
the epoch that was part of those objects
as well and what node1 will then do in
that case is he'll pick the latest of
all of those so he picks the most recent
value of all those um he updates the
epoch with the object to the current
epoch and he rides it back and so this
is effectively emulating the very first
round of multipath so this is emulating
you know the prepare proposed thing
where you're establishing a long live
epoch but you're establishing an epoch
per that particular microstate in that
ki by just realizing you that it hasn't
been done yet and doing it the key here
is you only dosed the very first time
this particular leader sees that ki if
this leader were to go and do this again
a second time your requests can look a
lot like this he's gonna you know look
at it it's gonna the object is going to
be this current epoch great so he
doesn't need to do the first phase
anymore to reestablish it he asked who
just does the modification and does the
write again similar to the multipath
this case of you know round two where
you just have the single round trip so
we're emulating the same thing we did
before getting the same benefits we had
before which is yes you have it you know
two round trips in the worst case but in
the common case you know in the case
we've known one stays a leader for
forever
you know a long time weeks time days
it's it's one round trip in the common
case so again what I just said it's a
pretty straightforward there's one thing
to mention about gets I put in the
slides here but one of things we also do
here is the leader here is actually we
use a stable leader that establishes
leader leases there's actually details
of a way to do that sort of in the
Google Maps made live paper they do that
as well in there and so the idea here is
that as long as there are folks
following that leader he remains the
leader so if someone else shows up and
try and say hey it left me the new
leader the other guy said no I'm still
falling this guy they you have to wait
until everyone times out falling for him
before you can potentially ever elect
another person so the old least has to
time out first there's a slightly
different raft raft doesn't have stable
leaders if someone shows them says elect
me a new leader he wins that's how that
author and works because it's a simpler
design to do it that way but anyway the
reason that matters because that affects
how it gets work in just a second so the
first round of get looks comic we just
talked about
you look at the current debt data if
it's from an old epoch we have to do the
full two rounds again so we read we
refresh to get the object we update it
to the current epoch we rewrite it again
we do that both forgets or puts to
figure out what is the current state of
that object and then once we've done
that we happen to be mine we're actually
writing here we're doing a write on a
gift because that's the only way to
bring that object up to date at the
current epoch again it's basically the
first round of multi path just emulated
in the key state itself so that's kind
of weird but again do the full round and
it's not after until after you've done
the right and it succeeds can you
actually return the value for the client
so a little heavy-handed but again you
only do this once in the case where it's
an old value in the common case the
really nice thing here is that yes in
theory you would have to do a good and
do the two round-trips would be one
round trip but since we're doing stable
leaders and so I mentioned a few minutes
ago the current note actually has the
correct value the leader has the most
recent value if the epoch matches and so
he doesn't need to talk to anyone and so
actually the common case just returns
and so I say I got the value and so in
the common case reads don't talk to
anyone except a local leader yeah which
actually been better than regular
example red Lyria we always need to talk
to all of our brother Lucas in the
dungeon system case so this is an
interesting sort of benefit here and
again two round-trips worst case zero
round trips best case which is really
kind of cool and at any point in time if
any of these you know round trips fail
in aggregate quorum the leader will
always abandon this leadership which
will cause you know a new leader took to
be elected which will then increment the
ballot number which will increase the
the ballot number that everyone agrees
to which establishes you know whole new
set of sequence IDs that are larger than
any previously written ID and of course
that new leader will see on every new
read or write operation that an object
is from the previous epoch now and so he
will rewrite the object each time and so
that's how the new leader after leader
change sort of refreshes each object to
the current state and resolves any
ambiguity so the way this is helpful for
example for the partial write failure
scenario yeah I dimension here so
partial write scenario here looks up
something like this so partial writes
are an interesting thing that a lot
people don't think about in the
shrewdest systems and particular could
you think oh acid database doesn't have
this problem no I'm like Postgres if you
were connected to push Chris Masters in
a server over TCP you can never get a
partial write problems idea of a partial
write is basically a write that failed
actually succeeded so the dataverse
still did failed but then later on you
read the value notes there and you're
thinking can't have a post risk as post
which does roll back transactions if the
TCP socket disconnects during you know
the transaction that's true but they're
actually a really interesting race
condition even poetry can hit it where
postage does the transaction begin it
does the commit it's definitely written
now and as it's sending that act back to
your client that's when the TCP link
drops you never see the packet your
client library says hey the write failed
but then if you go and read you see the
value and so there's this interesting
ambiguity if you don't actually know on
failure did it actually succeed or not
and the good answer is the next
successful read you do will see it and
by the way when I'm interest in a talk a
few years ago something that's not true
about pushers
well Kyle Kingsbury if any of you know a
fer he does a blog series on Jaspan
where he actually picks databases and
does nasty things to them and see what
what they can do and as serious on post
grits after shows it yes that's true
in this scenario whoo you can provoke a
case where you get an unacknowledged
write failure that actually succeeded so
it is you can do that you can solve that
by doing two-phase commit to the client
but no one does that cussed and
dreadfully an expense or dreadfully
expensive so so in some ways we always
live with this partial write problem the
thing we want to do though is make sure
the fact that once you do the future
read whatever you see that's the truth
and it never changes so and it's just
unlike Rioch like today eventually
consistent react there's a scenario
where you can do a right that's
unacknowledged and so you don't actually
know if the right succeed or failed and
a future read you do shows the value of
being the old value okay fine but then
you wait a week and you read it and it's
actually the new value that you failed
to write which is sort of annoying
that's not a useful thing to build the
systems off of if you need the stronger
guarantees so then I some of the this in
Samba case here is that solves that
problem we get the the better benefit
which is it can be either or but the
next read is the correct value so here's
how it works
so let's assume here that I have the
values currently X and it's a teapot too
and I'm gonna go ahead and write the
value to Y and I wrote the value looks
as soon as never partition wasn't here
originally so I go wrote it and the
value somehow got written to this side
over here so why I got written on that
side the partition I'm a teapot - but it
didn't get wrote to these these two guys
over here wasn't written to these guys
now I'm gonna go ahead and do and of
course since it failed I mentioned
before whenever you fail the leadership
is given up it increments the epoch
number the ballot number and so vo
have you pok3 is the new epoch that any
future leader must have no one can still
be a teapot - and still be a leader all
right so what happens well let's say now
we go into a reed and the reeds
happening on this side of the partition
okay and so again the answer right now
as far as the user is concerned is the
value could be X or Y I don't know well
but I care about is whatever I am told
it is it better not change again in the
future okay so what happens here is the
user does go and read and this time he's
reading over here on this side and he
actually does a read the reader does his
local read he sees that the value is
currently X at a pop - epoch - which
before the current epoch we have here
remember the the refresh kind of thing
we checked that it's older than the
current epoch so what do we do we send
to get to a quorum of nodes which in
this case are these two notes they both
reply back and so we have the value is
currently exit two and so the leader
says cool X is the right answer X wins
and so we rewrite X back at you puck
three because that's the current epoch
so now we have exit three and return X
to the client and so the user now sees
the X it is you know x1 not Y the
entering thing here is they're still
this Y over here but Y is now older than
X it is not an older epoch so in the
future when this partition heals no
matter what happens you always will get
a quorum which is at a higher epoch than
this Y and so you effectively implicitly
rollback this Y and so without even
being able to touch other side of the
partition we rolled that Y at the read
time and so we always submit X for all
time as the correct answer that
partition value doesn't come back
where's in regular like we are you would
could see that value come back and win
now again we could have just as easily
read Y here and rewrote Wyatt
you know epoch three that's fine - again
the semantics are either X or Y but once
it happens you cement it for all time so
that's the the semantics that are we a
consultant gives you so let's talk a
little bit the architecture real quick
how this is built and you're all
Erlanger so talking about a process
trees so this is what react and stumbles
high level process tree looks like we
have well we have to have the app but
then it spawns a supervisor and the
supervisor has three things directly
under that's a gen server called the
ryokan Samba manager and then there's a
reactance omble peer supervisor and the
react ensemble router supervisor peers
are basically the things which are part
of in Samba which are the things that
were node one two and three on all those
things they're actually pids they're gen
their gen FSM
and the router plunge router processes
imagine that these are gender birth and
the peer surprised are spawned these
pairs which are Jennifer's n so you know
standard OTP kind of app you would have
here
so what is a peer so appear here and so
and then Samba wasn't really a thing as
a concept there's a constraint I have a
notion in Samba and it's in sambal has
some number of peers three five seven
whatever again and Sowell's are created
dynamically and I'll show you in a
little bit how it's done but what then
sambal is it's a react avian Samba peer
it's a set of those Jen servers run
either on one node or multiple nodes and
this is you know this is implemented in
ryokan sambal know that it appear the
peer behavior just does the whole
multipath so simulate a thing we just
talked about
well this peer FG also does is because
again this the scalable framework is
then actually has a back-end this is a
it's a behavior it's a behavior your
implant that says okay here's my objects
look like here's how I get input objects
here's how I synced objects there's no
chef think and then it kind of glossed
over in this talk I mean there's a few
other things but you can implement a
back-end and that is how you connect
some of the high-level pure concept of
you know and saam bowls and peers to
whatever you want you know you could
build a distributed scalable Paxos
system uh nets you know or you're
building X back end with this and for a
react case we actually have a special
back end here which basically interfaces
react and Samba with react where your
apps you having this thing which looks
like sort of separate database system
after storing data in really long side
or existing react objects in reacts
existing back-end but it can be you know
that's the thing your implement to
attach this to whatever it is you're
building and again it doesn't have to be
an actual data store it could be where
this is actually like your emulating gen
server status and there's entering
prototype I should put some code up it
shows how do that so I don't have well
looked in the back but real quickly
here's the here's the behaviors you
implement so there's the combat init
function which just returns a state just
like you would do in gen-f us a more
dense server there's a set of things
called objects accessors so the objects
you know the things that we were passing
back values they emulated you know there
was a paw sequence key and value the
value was sort of an opaque thing right
well it turns out internally this
actually a single record we should send
back and forth which has to contain all
those properties in it it's up to you to
use what you want though it's a blob so
whatever works for you in react we
actually have our existing react object
which is that blob but you know in your
own
system you can have whatever that is but
react ensemble needs is reacting sample
needs to be able to access those three
there's four things it needs to be able
to get at the state it needs to be needs
to be able to get at the you know the
key the value the the epoch of the
sequence and so you need to sort of
implement things that just take in this
opaque value and return each of those
things
likewise you need to provide functions
which if we call them in to say opaque
thing opaque value set it and so you can
set all of us do so again
opaque as far as react instalments
concerned implant those in your back-end
likewise the get input these are the
things which you know are the update the
value so get is retrieve the current
value put is up take this current value
that you have and you know store it
whatever that means
again if the Nets back-end you put in s
if it was just our state in our server
you just say okay cool it's it's in my
server now I'm done and then there's
notion of thinking what I'll skip over
but you know you can go look at the code
there's comments yeah but basically
there's this notion of cases where you
need to sync peers to bring them up to
date and there's some things you need to
do if you want to implement that and
then there's a notion of ticking and
pinging so I basically periodically the
the peers the leader has a periodic tick
which is using react ensemble this is
where it does sort of leadership extends
the leadership Elise this is what it
does in real compal we do parametrize
that where you could if you want to also
have it call back into your back-end if
there's something you want to do at a
periodic interval on the leader you
don't have to implant this this could
just return okay but it's there likewise
if you're the nice thing about react
assemble to is that this can doesn't
have to be a pure function this whole
back-end could be something which then
sends messages to an external process
like a gen server and gets replies back
but if you do that we need to be able to
know if that external process is still
alive so I mean we can do monitoring and
monitoring we do but there's cases right
where there's a pig that's up it's alive
but it's stalled right maybe it's stuck
on a back end right that for some reason
it's taken a year and you should just
kill that pit or something or you're
back in training slow and so we also
know this no shut pain so periodically
we also send a message to that that just
basically calls a reply saying yep I'm
still alive and at any point in time if
a certain number of pings fail we we
would give up leadership and we move on
with life so briefly just talk about
clustering and then we're all done here
for questions so the idea here is
internally one of the things we do here
is klas training like I said memberships
dynamic
you can add or remove members as we'll
talk about not just two nodes in a
cluster but also nodes prints omble so
you can scale an Istanbul from ABC to
abd remove a that's all built in but
there's also the notion here of you know
dynamically created in Samba 'ls and one
of things we ended up doing here is
happen to deal with some kind of cluster
state because in Erlang here you want to
be able to launches no to join them
together and sort of have this cluster
state propagate and it turns out you
can't easily do that consistently you
can do some of it consistently but
there's still some interesting failure
scenarios where someone may not be part
of an ensemble but still needs to know
who to go talk to he still needs to see
who should I talk to and so we do that
by gossiping this cluster stayed around
periodically and this cluster say
basically consists of a notion of a node
ID the set of nodes these are the nodes
in the cluster that you've cost between
decided currently known in Samba 'ls and
if you're enabled and by the way that's
what it looks like when you first
started on a certain node and this gets
node 1 node 1 he has an ID
he has no curtain stumbles and it is not
enabled oh there had to go and enable it
yeah you send an able pitch then the
manager it runs the node we end up
basically setting enable to true and we
created us an example this is the route
in Samba and so the route and sample
here is basically the leader of the
cluster now and he's the one who
establishes all the other metadata and
so to do that the manager you basically
when he sees a state change periodically
sees this he actually realized hey there
should be a route in Samba running now
the manager will see this whenever that
state eventually propagates that node he
will then add that to the peer
supervisor that peer stretchable then
small and that UPS we actually
dynamically add the peers that are part
of these in Samba 'ls as part of the
state as it gossips around and then
eventually if you wanted to join nodes
you you can only join nodes that are not
enabled because if you have to complete
separate consists assists and histories
that aren't compatible this will make
sense to join them together and so
there's a property here where the node
has to be false and you can join them to
a node that is true and to do a join you
basically send a request to this guy who
then sends a state to this guy and who
basically just whole only replaces it
and so both nodes now sort of have a
consistent view of that state and then
after the fact and then doing that you
know sort of see that you basically
start with three independent clusters or
three independent nodes which you know
this one would be enabled these wouldn't
be you call join you sort to have one
global cluster now and you can add or
remove nodes if any of you're familiar
with reaction
for which we'll use real forever use Rio
Rio has something very similar we just
start notes up and then join them
together later dynamically rather than
sort of at the get-go saying here's my
fixed never and I'm running out of
time here so I'm just going to go
through this real quick
basically the crane fumbles you send a
message to the group here this is the
route built-in in Samba who is
responsible to creating other in Samba
we have to consistently create new
ensembles and so to do that that's why
we start with just the single group here
but he can create other in samples so to
send a request to him he sends a request
to this directory this directors
actually a state that was part of the
manager in the other slides
it was just broken out here it goes to
the directory the managers eventually
see this that this is basically a table
that's why it's separate they'll
eventually see that tables change they
will then gossip around eventually all
the managers see that's been gossiped
around and then what they do here is
they all spawn you know the other peer
supervisor or the new peers in this case
ears they dynamically create these new
peers each of these peer start up with
the state saying hey here's my three
peers now but there's no leader so it
ends up happening is I do an election
and this guy eventually becomes elected
leader and he's the new leader and so
you can dynamically create you know
hundreds of in samba 'ls if you want or
a few and still up to your app to route
through these and samples and that's
basically it the last slide is just this
one we also again support dynamically
adding removing members and we do this
to be a joint consensus so if you read
the raft paper it talks about joint
consensus and the idea here is basically
you need to have an overlapping set of
members so you might not for example a
foo we saw earlier it had three nodes
ABC perhaps you add in two nodes D and E
and remove node C so actually do this
consistently you actually have a period
of time where you overlap both
membership so you first have ABC commit
the view that is both of these then all
future operations need to succeed in a
quorum of both sides
what's the side on the side and then you
eventually what you do is you commit
sort of the Future view and for this
future view to commit it has to get a
quorum from both ABC and a quorum from
be de and those are separate you know
could be separate quorums but you have
to see from both before this to succeed
and there's a nice proof of this in the
raft paper sort of why you need to do
that and that's what we end up doing to
do dynamic membership what we have
zookeeper does and their new stuff is
this somewhat similar to this to join
consensus and that's it so again this is
react and Samba passions bridge
framework for this and again I didn't go
into much detail some of the API just
because I tried to cram as much I could
into this talk but there is Doc's that
you know in the process of being written
that'll have all the nice you know
here's how you can actually build
something on top of this and hopefully
code examples as well and then of course
react itself serves as sort of the
canonical example of how to do this
they've heard of looking at code base
and anyone who wants to stay around ask
questions rather than go and drink
coffee feel free to</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>