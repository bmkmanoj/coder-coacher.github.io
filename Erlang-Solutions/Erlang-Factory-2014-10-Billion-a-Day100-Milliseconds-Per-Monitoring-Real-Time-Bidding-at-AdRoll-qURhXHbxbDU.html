<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory 2014 - 10 Billion a Day,100 Milliseconds Per: Monitoring Real Time Bidding at AdRoll | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory 2014 - 10 Billion a Day,100 Milliseconds Per: Monitoring Real Time Bidding at AdRoll - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory 2014 - 10 Billion a Day,100 Milliseconds Per: Monitoring Real Time Bidding at AdRoll</b></h2><h5 class="post__date">2014-03-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qURhXHbxbDU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Brian and my talk is ten
billion a day 100 milliseconds per
monitoring real-time bidding an ad roll
so some of you know me I am trout line
in air Lange and air lounge channels I
do things too and with computers in
particular I am very interested in
reliable complex and critical systems so
these are systems that must always work
are very difficult to make work and if
they do not work something disastrous
happens these are difficult systems to
make which makes them fun to make right
now I work for a company called Admiral
so Admiral is an advertising company but
it's less this this being a style of
advertising where you buy large blocks
of what we would call impressions
eyeballs on an ad so in the old days you
used to go to say the New York Times and
you used to say I have 50 million
dollars how much does 50 million dollars
gonna buy me where the vad vert izing
space in your paper which is fine if
you've got 50 million dollars or even 50
thousand dollars but there are a large
group of people companies that don't
have such large advertising budgets they
have $10,000 $100 so online advertising
has moved increasingly to cater to these
smaller advertisers and as a result
advertising has become more this this is
data oriented advertising advertising
exchanges they're what we call exchanges
started to appear about 2009 it became
impossible to buy individual impressions
so instead of going to the New York
Times and saying I need you know 50
million dollars worth of advertising you
go to Google and you say I need 10
dollars worth of advertising in the next
10 minutes which makes advertising if
you're say a small Etsy seller and you
make up two posts bracelets out of forks
much more approachable so you need much
less budget and many more people are
able to do it and instead of managing
these large relationships with companies
and with large sales teams we use
real-time system engineers which is me
mathematicians which is not me Admiral
is an ad tech company so that's what
this style of advertising is it's just
technology that we happen to apply to
advertising this type of advertising is
called retargeting and it's all about
data so a dural that uses what we call
first party data we collect from the
pixels that we have scattered around the
web information about people that browse
and we correlate them inside of our
system so our customers are the people
that want to show ads to people who
might care to see them so as an example
I subscribe to The Economist The
Economist has advertising that is really
expensive it's like $100,000 per month
for a full-page spread they advertise
$40,000 watches even though I buy The
Economist I will never buy a $40,000
watch that is totally wasted so we
partner with these exchanges these are
people like Google Twitter Facebook that
run these ads thought auctions so an ad
slot is where in your web browser and ad
appears when you start loading a web
page there actually isn't an ad that's
scheduled to appear there what happens
is there's an auction that's held this
auction takes 100 milliseconds so while
your web page loads real-time systems
all over the globe are bidding furiously
to show you their ad that's real-time
bidding so this is what sophisticated
shops like Admiral do it means that we
take all the day that we have our
mathematicians build models they predict
whether or not an individual person will
care to see a certain ad from one of our
customers and in honest-to-god real time
we bid while the users web page loads so
the nature of the problem domain is it's
it's low latency we have 100 less than
100 milliseconds 100 milliseconds is
actually our deadline to get the request
from the exchange over the Internet
do all of our computation and then send
it back to them and that entire chain
including the two hops over the Internet
has to take less than 100 milliseconds
it's a firm real-time system so for
those of you that aren't familiar a soft
real-time system real-time systems
you're always hustling on a clock you've
got some deadlines soft real-time system
the computation that you're performing
if you go past the deadline your
computation still has value you don't
care too much but you try and hit the
deadline a hard real-time system is one
in which when you go past the deadline
your computation has no value and
something disastrous has happened so
think about an avionics system if the
pilot you know jerks the the airplane
this way it should go that way a firm
real-time system is one in which when
you past the deadline your computation
has no value but nothing disastrous
happens at least not immediately so
that's that's what we do it at all our
bids have no value past 100 second 100
millisecond excuse me point but nothing
disastrous happens if we miss one or 5%
of them how we're highly concurrent so
the title of the talk is actually wrong
when I submitted it we had less
transactions per day than we do now and
this slide is actually wrong we peaked
over 42 billion recently I think and
we're growing all the time which is
really fun and we never shut off so
we're 24/7 and we're global so if you
think about quantitative finance you
know they run 1012 hours a day
we run constantly so the problem with
building these types of systems is that
we are very good at producing complex
systems that we have no understanding of
so Carlos bueno at this fantastic
optimization book for Facebook where he
said that humans are bad at predicting
the performance of complex systems our
ability to create large and complex
systems fools us into believing that
we're also entitled to understand them
you can substitute performance for
maintainability you can substitute
performance for anything that you would
actually care about for a real-time
system and when we try and overcome this
we have multiple methods for doing them
that are all insufficient for instance
ahead of time verification it's not
sufficient that's testing documentation
type checking these are all valuable so
we can't scrimp on them but they only
demonstrate the the limits of our system
under certain artificial conditions
anything that we're able to think of
ignorance and complex interactions with
the external systems
are why we can't have nice things these
are why ahead of time verification only
addresses part of the problem
so our ignorance reduces our imagination
for test cases we can only test what we
dream to test up and the interaction
with external systems introduces
uncertainty into the boundaries of our
system so this becomes even more
pernicious as your system grows in size
or ambition so in our experience at
Admiral and in previous experience in
similarly large deployments at scale
even rare events happen frequently so in
testing if you have an event that's
going to happen every one millionth of
the time it's going to come up in
production and it's going to come up a
lot but you might not catch it in
testing and that's a real problem at
scale bad things happen then humans
faster than humans can respond so the
worst thing about these rare events is
that they just start pouring in and
there's no way to handle them without
some form of automation the results are
often not very pretty either these rare
events or things that you have not
thought of
I have no systematic theory for why this
is the case but we humans seem
particularly good at imagining the not
so bad things that can happen in our
systems but we are very bad at
predicting the disastrous things that
can happen more to the point these
causes are very rarely quick to discover
which is a problem when you have tens of
thousands of these pouring in and
something terrible is going on so for
those of you that are not familiar with
American history the the photos behind
these slides which I think you can make
out are from the American Dust Bowl the
Dust Bowl is an excellent example of
individual action in aggregate causing a
disaster you had farmers who
individually would strip their land of
native grasses and plant wheat they
would leave basically dirt most of the
year and Oklahoma and the middle of the
United States is very windy there aren't
actually trees there naturally so you
have an individual farmer do that you
know maybe the farmers kitchen gets a
little messy from dirt blowing in but
when you have tens of thousands of
farmers over hundreds of thousands of
acres leaving just raw
earth combined with wind you have dust
storms that blackout Chicago several
times a year you have President
Roosevelt getting dirt in the cup in his
cups of tea massive massive problem so
what can we do to overcome our ignorance
and our lack of imagination and avoid
systems faults how can we make reliable
rather than perfect software so perfect
software is this ideal on the mountain
that I contend we will never reach
simply because the world in which we
live is more complicated than we can
imagine so I advocate that we have to
observe our systems while we run while
they run we have to be scientific about
this so we monitor our systems and we
provide sufficient instrumentation to
apply the scientific method to our
creations so we can create hypotheses
about cause and effects and we can
experiment and watch data that comes in
and out it's just science at that point
it ends up being an artifact of the
universe problem is not all systems are
moderate rebel just as not all are
testable so correctly instrumenting
effectively instrumenting a a project is
a matter of design and as a matter of
culture so a design that's monitor Bowl
is one that takes stock of its internal
tolerances and its external interfaces
and exposes these to an operator so an
operator is a distinction that I'm
making based on a fantastic book called
digital Paolo which I can't really get
into but I have a bibliography at the
end of these slides if you would like to
read all the books that I reference so
in engineers responsible for building
and designing systems the engineer the
engineer is the person that dreams them
up and the person that executes on them
and the operator is responsible for
running them so ahead of time
verification testing type checking these
give the engineer intuition about the
system and this is vital intuition about
the system before it runs allows the
engineer to assert things about
tolerances it allows the engineer to
assert things about how the running
system should look because if we don't
know how the system should behave you
can't say how it shouldn't or isn't
monitoring gives the operator
information about the running system so
the operator being responsible for
running
system actually develops more affinity
with the system over time because they
have more experience with it and this
gives them a different insight to the
system which is valuable extremely
valuable thankfully the operator and the
engineers are often the very same people
so this holds not just in software one
thing I'm very fascinated with and the
sort of critical engineering culture is
the u.s. space program which you can
probably tell eventually from these
slides Mission Control is staffed with
astronauts that aren't flying at the
moment so you have this concept of I
know the system very well and I am
operating it or I know the system very
well and I am in the middle of it you
see this in nuclear engineering and
other settings as well but it's a very
powerful thing to operate the system
that you have also created because
there's a potential for a positive
feedback loop of quality here
so when we can combine practical
experience intuition with the deep
knowledge about systems we can do really
amazing things so case in point the x-15
was an early NASA experiment this is
NASA spelled with a C which was a thing
the x-15 was basically a plane that
would go to the edge of the atmosphere
and then was intended to come back down
it was one of the first computer
controlled computer Fallone airplanes it
was thought perhaps that the airplane
could be completely automated but it was
found that when the computer completely
controlled the plane it just skipped off
the atmosphere like a stone it was also
found that when you disabled the
computer the human operator the pilot
was not able to re-enter the atmosphere
either ended up just skipping off the
atmosphere like a stone so a combination
of the two a human man a man-machine
interface was necessary and that's
basically what instrumentation builds
that's a much deeper talk and I would be
very happy to talk about that later so
what in particular we doing in Admiral
one of the main libraries we use is a
library called XO meter so historically
actually we didn't use XO meter
historically we pumped all of our
metrics coming out of our systems
through our sophisticated machine
learning HBase cluster problem with that
is you get delayed times you have to
wait for your metrics to batch up and it
takes 30 45 minutes for them to compute
out which is disastrous
in a situation where your system is
exhibiting behavior that you didn't
expect or that you declare to be a
problem so XO meter is it's a metrics
collection aggregation and reporting
library and it's got excellent pedigree
and a fantastic focus for the scale at
which admiral is working so it's been
created by four-year labs it's open
source it has an extremely responsive up
stream on multiple occasions I have
reported an issue and it's been fixed
within an hour or so I have no idea
actually how this has been done because
I know I've been reporting this at
upstreams midnight and it still is fixed
so the metrics aggregation the
collection and the reporting these are
all decoupled so when I say that I'm
collecting a metric
I don't also have to declare where it
goes I can simply collect it and decide
at a later date where to send it or at
run time decide to send it to a new
system even if I've already declared
where to send it
it has static and dynamic configuration
so dynamic configuration is what you
would expect the static configuration
everything is brought online when the
application comes up which was
remarkably useful and it has a very low
and very predictable overhead at run
time which is is valuable for me where I
am extremely latency sensitive and I'm
extremely concurrency sensitive so I'm
able to predict the behavior of this
tool that I'm introducing into very hot
short code paths so some terms about exa
meter so metric is just a measurement it
is just a value at time of a thing an
entry is a receiver and an aggregator of
metrics so an entry is something like a
histogram a spiral a counter a reporter
is an entity which samples the entries
on a regular interval and optionally
ships these to some third system there
are several reporters already baked into
XML
collecti stats d local file and they're
also very easy to add we added the stats
T integration ourselves and it took me
about 35 minutes a subscription is a
definition of the regular interval on
which the reporter samples entries so
the metrics are not reported as they
occur they are polled but they are
pulled in an extremely efficient way so
you have a chance to aggregate over some
time interval and then ship it off the
Alpha Network so this is static
configuration of entries a particular
interest are function definitions this
has been one of the most marvelously
useful aspects of XO meter and I don't
know that it's been replicated anywhere
else in any other open source system so
a function entry allows me to define a
function as a point of measurement I
invoke the function so in particular
here I'm invoking err laying memory
which is a built-in function and I'm
feeding it to arguments it's in binary
so what I'm doing is I'm going to record
the value of my X consumption I'm going
to record the value of my binary memory
consumption and then I'll eventually
report that there's also a match syntax
so because functions don't of course
have uniform return types you need to
have multiple ways of dissecting
function returns so match allows us to
so this is for airline statistics and
I'm looking at garbage collection
because I'm very curious about the
garbage collection of my systems if I'm
creating too much garbage that's a
problem garbage collection railing
statistics excuse me if you don't know
it returns a tuple it's sort of peculiar
in that regard match allows you to just
match values out of a tuple this was
recently added this was an example of me
saying I have a need and then a couple
of hours later finding a present under
my tree so a spiral so one thing I
forgot to point out the pre defines
there the functions those are all VM
supplied but you can also of course
supply your own aggregation points so
these the the Budda Budda is the name
the real-time bidding system it's also
the name of a delightful dog that is
often in our office frequency cap is a
thing that's internals a system that I
won't get into but these are
application-specific
the spiral here is an aggregation type
that allows you to collect values for
some fixed time so by default a spiral
is 60 seconds long if you insert
something at time zero it reflects in
the aggregate metric and then at time 61
that value has been dropped out so it no
longer reflects in the aggregate
statistic so reporters reporters if you
remember are things that report values
over the network
this is report stats D we use a product
from a company called beta dog they are
a fantastic dashboarding company they
have a daemon that runs in your local
box and it exposes a stats D interface
so this is why we use stats D and you
can see that I'm configuring it for
localhost and things like that
Here I am telling the reporter to poll
for the entry named airline G cito call
and I am telling it to report this over
the network as a histogram so when this
is pulled out of the VM it's of course
just pulled out as a singular value I'm
telling stats D which does sum of
aggregation on its own to compute a
histogram of the values stats D
aggregation if you can avoid it is
probably a good thing because you don't
have any configuration of it I actually
don't know what the time width of this
is which is a problem but it's not been
so much of a problem that I've moved the
source of truth all into excel meter but
you can see that I actually am pretty
finicky about some things so if you
don't know stats D gauge is a stats D
type and it is the sole type in stats D
that assumes that the client is going to
compute the aggregate value so if you
need your application to be the source
of truth of aggregate statistics you
have to use a gauge it just as an aside
if you ever design a protocol please
look at stats D and then don't do that
you cannot for instance insert a unix
time value so the value at which a
metric occurred is simply the value at
which it was received so you always in
eventually you come to the realization
that you cannot use stats D except but
locally which is madness subscriptions a
subscription is the polling interval so
you see here again that we're telling
we're telling stats D to poll airline
statistics and run queue in this
particular case the subscriptions occur
in milliseconds which is pretty typical
so every one second I'm getting the run
queue back up of my system which has
been really useful and will actually
talk about that in a second and again so
when you when you configure one of these
subscriptions you give it the metric
name XO meter is very interested in
metric names that is the primary way
that you end up configuring XM meter and
discussing metrics and one each of the
aggregate statistics has multiple
attributes you'll you'll see histograms
in a second one happens to be one of the
aggregate statistics of spiral of course
you can do this all dynamically you see
here that I'm I am creating a new
histogram metric called a histogram and
get value is a function in XM e that
allows you to see all of the aggregate
values in for some metric so this is
actually how instead of digging through
the documentation which is very large
complex it's also extremely well
documented Mike it's easier just to see
at run time what's available and then I
configure a reporter this is report TTY
report TTY just prints it out to STV out
please never ship a production system
that has this enabled but it's so useful
at development time and you can see that
once I enable it just starts printing
out because I've never put any values
into a histogram it's always zero so in
particular at Admiral what are we
looking for how are we using XO meter so
we're looking for VM killers and all of
you should have some idea of what the VM
killers are our atom tables that get too
big
reaching your Ed's limits things like
that we have those instrumented and we
have alerts for those actually we have
alerts for those if we even get remotely
close to them because that's probably a
bug in the system and we have to address
that so we're looking for system
performance regressions so that's we use
Amazon ec2 if our CPUs become slower for
some reason we want to know this but we
also want to know if our network ability
has dropped and we want to know for some
key metrics in the application so bid
request latency is very important for us
like I said we're very latency sensitive
so we monitor how long it takes us to
respond and if we find that we've got a
spike in response times even if it's not
causing timeouts we need to know that
because we need we either need latency
to stay the same or decrease over time
so we're looking for abnormal system
behavior as well we'll get to this point
in a second and we're also looking for
surprises so surprises are just things
that are clearly wrong and clearly mean
there has been a bug introduced into the
system either through system updates or
shaadi deployments so what in particular
do I mean by abnormal system behavior
this is actually one of the most
important topics and one of the most
difficult things to discuss it actually
requires the combination of the
intuition of the engineers and the
operators you have it is it is sort of
the core consideration of testing and
monitoring and it's hard to decide what
your system should be doing and it
shouldn't be doing so for the real-time
bidding system at add roll no spend
where we we don't bid and we don't spend
any money is an abnormal behavior we
always want to be spending money to high
spend our campaigns have budgets our
campaigns are associated with our
customers you know if they're making
octopus breaks list out of Forks and
they have a hundred dollar budget and we
go ahead and buy $10,000 worth of
advertising for them we're eating that
overhead it just continues drops and
spend so if we see that we're spending
at a certain level and then blam we're
not dropping or we jump way up with no
explanation that's also something we
need to understand and node failures etc
things like that
so case studies how is this impacted us
at Admiral so for the first one the
timeout spike so by timeout I mean we've
gone over our 100 millisecond limit
there is an acceptable background of
time out spike simply because the
Internet is unreliable exchangers
usually give us some fixed percentage of
timeouts that can be allowed to occur if
we go over that fixed percentage then
they do what's called throttling they
cut our traffic and that is a serious
problem because the amount of traffic
that we are able to pull in from the
exchanges directly relates to our
ability to survive as a company critical
systems so this is a graph of our bid
timeouts it's or our yeah our bid
timeouts it's a period from 6:00 a.m. to
9:00 a.m. for some exchange I can't
actually tell you which one so what we
should see is this being nice and flat
some you know Brownian motion around
basically zero but what we see here
instead is periodic peaks of timeouts
now we never win over the the throttling
limit this is about 2% of our total
traffic in the worst case at about a DM
which peaks at 5.8 K but it was enough
of a concern that it was always just
below being inserted in our sprints and
then once we hadn't increased our our
instrumentation it became sort of first
project to demonstrate the efficacy of
this so we examined our system load the
the boxes have pretty continuous load so
we're not seeing that the boxes are
overwhelmed at any point periodically so
when you do this sort of debugging of
course you have to make hypotheses maybe
we're running out of CPU no we are not
but we can correlate that we have
network traffic spikes with these these
same timeouts I happen to know that we
are not getting anywhere near our boxes
Network IO limits so it's not that but
we do have some something interesting
there is something else going on to
cause this spike on the system and this
was new so we had only just recently
charting our run cues and you'll notice
the run queue spikes are pretty
correlated with these spikes and
timeouts that was a big deal so what
happened here so we have our scheduler
threads locked to the CPUs of our boxes
and we have a background process that
kicks on every 20 minutes and it does
things to logs that we produce but it G
zips them so it consumes quite a bit of
CPU time unfortunately we didn't have a
CPU shield enabled in our boxes so you
know we have n number of CPUs and we
have n scheduler threads in the VM and
they're all bound to those CPUs we have
gzip come on and the operating system
kicks them off so we have one scheduler
thread that gets bumped off of its CPU
it backs up and because it backs up it's
not able to process its requests anymore
so the process requests timeout and then
we get that spike the gzip turns off
everyone settles back onto their
processor cores and then we keep on
humming so we eventually enable the CPU
shield we have dedicated processors
simply for the VM solving this actually
required quite a bit of knowledge about
the system as it was deployed and about
behavior of the system another example
so exchange throttling so this is kind
of one of our disaster scenarios right
so this is healthy bid request it's it's
diurnal right you know people are not
browsing the web as much at midnight so
each one of these troughs here is people
sleeping in a geographic location each
one of the peaks it actually Peaks
around 11 a.m. so people browse the web
a lot more while they're at work I guess
so this is good like this is this is a
good day for us this is a bad day for us
this is actually a space of about three
bad days for us so we found for this
particular exchange that without any
alarms on their part without any
notification on their part they just
tanked our traffic they stopped sending
us as many bid requests so every time a
web web page loads you know five ads
come up we get 5 bid requests unless
we're in a throttled scenario
so we can be throttled because of
various things primarily we time out but
we didn't see any time out Peaks here so
this is the first differential of the
the bid request rate on the right hand
side you can see the previous diurnal
chart and on the left hand side you can
see the throttled trough of doom it is
very useful to be able to apply
aggregate statistics to these types of
things because the human visual cortex
will convince you of things that don't
exist or that you want to exist so you
have to dissect your data and
unfortunately we confirm that we did in
fact have a problem and we confirm that
with the external system with the
exchange so especially when you're
dealing with coupling with an external
system you have to request information
from the external system because maybe
they have a problem or maybe you have a
problem that you're unaware of
so we looked at all the other metrics
our run QR CPU or network i/o they were
all fine and we hadn't deployed so all
of our boxes were unchanged they were
humming along fine and then blam there
crashed out so we also asked Amazon and
we asserted that there was no network
problem so from every point of view our
our stuff was good so what happened here
we hit an implicit limit with the
exchange arguably it's a bug so we have
this external system inside of a troll
called the cookie Mac system so in
exchange keeps an ID for every
individual user and Admiral keeps monkey
for every individual user and we ship
these IDs over to the exchange and we
say when a bid comes if there's a
correlation send us send us a bid we had
shipped 1.6 billion cookie matches over
to the exchange unfortunately their
limit which was not advertised I will I
will point out was 1.5 billion so this
triggered an edge case in their system
where we went over their implicit limit
and so they thought that we had a bug
and decided to throttle us so that we
weren't spewing silly ads
we didn't have a bug we just have a lot
of customers unfortunately they didn't
send us any emails we didn't have
alerting well enough to detect this and
just report it to them we do now so
lessons learned from all of this so it
is possible to have too little
information available to you that's the
case with the implicit throttling we did
not know what had occurred until we've
reached out over a long holiday weekend
and heard back eventually after after
too many days so you see this
historically as well so during the turn
noble accident the firefighters were
sent up to the roof and they were
kicking radioactive burning graphite
back into the reactor they had no idea
this was a very dangerous thing to do
there was no no good education of the
population even while the reactor was
burning a radioactive volcano
people were going out and they were
buying bread carrying it out in the open
and coming home and consuming it so it's
possible to have too little information
during a an accident it's also possible
to collect too much information or
present it badly so you see this at
Three Mile Island Three Mile Island had
a control room that was basically this
stage and then doubled and all along the
walls you have these lights and you have
810 people that stand in the control
room and they monitor a certain section
of the wall and to correlate what they
do is they yell across the room so
during during normal operation you know
Bob valve number ten ten percent check
but during an accident everyone's
excited everyone's afraid because they
don't know what's going on and you know
they don't want a nuclear volcano right
so they're getting loud they're yelling
and they're missing things they're
missing information but because you
can't have one person with an overview
an extremely complex system like a
nuclear reactor
they missed what happened and it caused
a meltdown in particularly had a valve
stuck open and it was easy to mistake
the value of a stuck open for reasons
that it shouldn't been stuck open for
reasons that it should have been stuck
open indirect knowledge may not tell you
the whole story or
worse it may make you doubt what's
plainly before your eyes so that's the
that's the the first differential thing
right you have to confirm in multiple
ways that what you think is happening is
actually happening so we see this
actually in the Apollo 13 disaster the
Apollo 13 popped an oxygen tank and blew
off half of the ship Mission Control for
the first 12 minutes of the accident
debated very hotly whether or not the
Apollo service module had been struck by
a meteor
they were so certain in the design of
their system that they could not believe
that a fault had occurred in it and they
wanted to rescue the mission they wanted
to land on the moon they eventually
didn't of course because while they were
differing over whether or not a meteor
had struck the ship they lost oxygen in
oxygen tank one which popped an oxygen
tank two which is an interesting example
of if you're going to make a redundant
backup system for God's sakes actually
make it redundant they were connected by
the same series of pipes and they were
considered to be redundant enough so
some things with instrumentation
monitoring don't work quite like you
would hope so problems instrumenting
increases code size it takes time and it
takes effort to insert this stuff into
your code and that's code that you have
to maintain while very low the runtime
impact is not zero in certain critical
paths in the Admiral codebase we can
detect when we monitor because we see it
elsewhere in the system it backs us up a
little bit so we sample some of our
instrumentation we run it every one
ten-thousandth of the time which if you
remember back to at scale things happen
very quickly we end up getting multiples
of those a second so the instrumentation
of dependencies if you pull in libraries
which we do pretty extensively
those aren't instrumented and you're
going to have different ideas from the
library implementer about what is
valuable to instrument so possible
solutions to this dtrace system tack
code system tap hookups so these are
facilities provided by Solaris OSX and
Linux that allow you to have the kernel
examine a running Pro
the airline VM is instrumented with
these in a month or so I'm actually
going to start digging into this more
deeply if anyone here is doing this
currently production please talk to me I
would love to talk to you I've lost
another possible solution is a cult
strim culture of instrumentation by
default so just as we built a culture of
testing by default we also have to have
the knowledge that these things are
going to be running for years especially
for airline deployments and we have to
be able to get information about how
they are running and then if neither of
these will address your problem the
airline virtual machine has tracing bibs
which I'm sure a great many of you have
used they do have unfortunately runtime
impact but they were remarkably useful
we trace in some example in some code
paths we do trace live insanely useful
another possible solution wombat it was
mentioned briefly this morning if you
want to know more ask an airline
solutions person I've seen a demo of it
it's very impressive
Duncan McGregor right there has also
seen a demo of it he was also very
impressed it won't solve everything but
it goes a long way toward so
implementation by default is not easy
and it's not free possible strategies
for that application configuration so
you you have your application take a
module that it will simply call in to
periodically behavior callbacks
so cowboy for instance when you
implement a request handler has
functions that you can optionally
implement to instrument the flow of a
request we do this at Admiral and it is
remarkably useful and the value at time
functions so these are the the built in
airline bits which you saw that we have
hooked up into XO meter the airline
virtual machine is studded with
information about itself it is one of
the most reflective environments that
I've ever deployed anything in I
strongly suggest you take a look if you
aren't doing it already so I've talked a
lot about design but what about culture
so it's possible for an organization so
a complex organization of humans we have
irrational behaviors we're actually
pretty vicious creatures sometimes
it's possible to misuse this data that's
coming out of our systems so if a fault
occurs and some somebody happens to be
the poor schmuck sitting at the computer
when the fault occurs it is common to
blame the operator and then scapegoat
them and just say well you know is his
fault but I contend that if your system
has a fault that a human can cause it is
still a fault with the system you have
to design for humans just passing out
and mashing their head on the keyboard
so don't look for scapegoats look for
solutions don't be fooled by
overconfidence in your work so we've
worked very hard to build very complex
things and it is possible to feel that
it is infallible but I guarantee you it
is not because you have not been able to
think through all of the possible Massa
nations of the universe in which we live
similarly document and discuss all of
your failures every time a failure
occurs it is an opportunity to learn why
it occurred and then how to avoid it in
the future if you do not do this you
will continue to fall into the same
traps everyone has to work to the same
end in the team so for example if I am
interested in taking the real-time
bidders and making them extremely highly
concurrent but I ignore latency right I
can potentially increase latency to the
point where we actually crash the
company so everyone has to have the same
idea in the real-time bidding team of
targeting this nice middle ground
between latency and concurrency and we
also have to decide what is a nice
middle ground so everyone on the team if
you're going to be instrumenting has to
have the same end in mind because you
have to instrument with that end in mind
similarly know some basic mathematics so
statistics when when someone asks what's
the 99 99th percentile of a certain
metric you have to know what that is you
have to know some basic calculus so you
can discuss rates of change in complex
systems it gives you a relatively
unambiguous in language for discussing
concepts which is remarkably useful
especially in stressful system accident
times
so this gets harp harp done a lot
undergraduate computer science all the
way up but it bears repeating prefer to
be loosely coupled when possible and be
very explicit about your tight coupling
you can't always avoid being tightly
coupled to an external system but you
have to know that you are tightly
coupled a lot of system accidents occur
because of implicit tight coupling you
don't know that there's some system that
you depend on has developed a fault so
when a fault occurs and you can measure
that it's occurred you don't know how to
diagnose it because you're not aware of
what's happened loose coupling gives you
some breathing room in that situation
you still continue to work as an example
there real-time bidders do not actually
display any ads they only bid on the ads
and then there's a notification path to
what we call the ad servers the ad
servers actually display the ads they
are loosely coupled if the ad servers go
offline the real-time bidders continue
to bid even though we're not displaying
the ads and vice versa if the real-time
bidders go offline every ad that we've
guaranteed we're going to show we
continue to do it it's somewhat of a
problem that we can continue to bid but
not show ads but our ad servers have
never gone offline so we're not super
concerned about it measure much but
present only that which are certain to
need so this is the same problem with
there's too much information available
to make to understand it but when you
are digging into a problem if you have
not measured something you cannot make a
hypothesis based on it you can but
measure don't guess if you are ever in a
conversation where you say I bet the
system did stop yourself just go measure
it you can measure it this is really the
empirical problem if you have not got
data about something and you are trying
to make a decision based on data that
you're just dreaming up you are going to
make a mistake you're going to make a
decision based on data that does not
reflect the nature of reality similarly
to that be accurate in what you measure
you were making decisions on this data
if the data is not reflective of the
reality in which you are operating you
will not
make good decisions and not making good
decisions is really the problem of
building these complex critical systems
we can't make perfect software but if
you are able to provide enough
information to people so that they can
make reasoned decisions about the
software you can make reliable software
and reliable is really the best we're
going to be able to do unfortunately it
would be nice to make perfect software
questions
Dada's just pouring into your operators
yes yeah so the question is you have a
massive data coming into the system how
do you handle that onslaught and how do
you make sense of it am i gotcha and and
how do you so aggregating ahead of time
is not premature optimization so instead
of recording every metric and pushing it
out over the network aggregate in say
histogram or something like that if you
have data that you do need perfect
representation of consider that very
carefully so you aggregate ahead of time
and then so with data dog we build
dashboards that have sort of overviews
and you can dig in to more complex
metrics but you you have to at that
point have the understanding of the
engineer to know what's valuable to look
at and sometimes of course you're wrong
because you have an imperfect knowledge
of the thing that you've made so that's
that's the fun part you can't actually
then feed all this back into the system
and make the system decide what to do
about itself so yeah you can you can do
that and it's fantastic you can give the
knowledge and give the system self
knowledge yeah tell the auto scale group
we need more boxes we need less boxes
throttle the number of requests that
we're bidding on things like that it's
good stuff yeah
if you monitor each individual box
so each individual box has an agent on
it that is provided by data dogs that
happens to collect system statistics so
we're not using the virtual machines
onboard instrumentation for that we do
have alerts that fire per individual
node but because we're running in Amazon
ec2 we're relatively unconcerned about
an individual node and more concerned
about a group of them so we tend to be
concerned more about an auto scale
groups health rather than an individual
nodes health because we'll just kill it
and have a come back online but we try
to kill it after we've got enough
information to understand why I went
dachi because we don't want to have to
be killing them hundreds of times a day
ideally we would never kill them did I
address your question cool yes
mmm-hmm that is the case as far away as
possible so the question was we have an
implicit throttling limit which each of
the exchanges how close do we walk to
that limit before we decide to make a
federal case of it and the answer is as
far away as possible we stay from it so
about two percent gives us three percent
of Headroom and we had seen that
particular pattern for quite a while and
it was considered to be a stable
divergence of the system so we were able
to live with it but if we ever deploy
and we noticed that our timeouts have
have spiked up or our we're not brownian
walking around zero then we make a case
of it because that is a new behavior
that's been introduced into the system
that is divergent from its goals yes
yeah so it depends we push data into a
couple of different places in in where
it goes into HBase we have never deleted
anything since the company started for
data dog I believe their retention rate
is three months maybe it's longer than
that
but it's quite a while and if we
actually care about anything long-term
we will push it through the HBase side
of the house where we never delete
anything questions yeah so so the
question was to elaborate on a CPU
shield Linux has the capability of
dedicating certain CPUs in a box to the
system and application specific things
so you could actually cut up the number
of CPUs into different groups and say
that certain processes will run in a
group so a common configuration is route
which contains all CPUs system which
contains the background processes on the
box and then application which is the
way we're deployed but I I know I have
seen situations where people will have
32 cores in a box and they'll have two
cores for background two cores for
moderately important background and then
the remainder for the the primary
application yeah exactly yeah I don't
know a production system you know your
background shouldn't be that hefty right
your your box should be primarily
dedicated to the system that you're
working on and we can't always make that
so she'll them yeah no more questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>