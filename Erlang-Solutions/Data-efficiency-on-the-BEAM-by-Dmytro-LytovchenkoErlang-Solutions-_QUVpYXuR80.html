<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data efficiency on the BEAM by Dmytro Lytovchenko/Erlang Solutions | Coder Coacher - Coaching Coders</title><meta content="Data efficiency on the BEAM by Dmytro Lytovchenko/Erlang Solutions - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data efficiency on the BEAM by Dmytro Lytovchenko/Erlang Solutions</b></h2><h5 class="post__date">2017-09-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_QUVpYXuR80" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I will present the talk about how to
make your data in Erlang efficient and
as small as possible and as fast as
possible this will be based on my
experience with the low-level details
and I also try to implement some Beaman
limitations make some beam applications
myself with different level of success
so I will skip this slide quickly
because it was before the presentations
about me and this talk will be about
data most mostly data I will just
mention some performance so when you
make your module you write your code you
mentioned some data values in it you you
write some literal tuples some strings
some nested lists something your heart
code in your program these literals are
saved in literal section in your bin
file so when you load them in into the
virtual machine they are stored in a
special section of your runtime memory
which is called constant heap I think
what's good about this that access to
these values is very cheap and referring
to them takes only one word of your
precious process memory but there is a
drop when you do this trick you can use
it for example to create literal tuples
with lookup values you can do quick
lookups by index like random choice of
names of addresses random choice of some
values you can look up I don't know
hexadecimal to decimal and so on so the
use to pools
little tapas are fast and efficient but
there is a drop when you reload this mod
module often and you store this literal
reference somewhere garbage collector
will copy these references values to
your process hips and then it will
reload the module so you will
potentially have a copy every time you
reload
storage somewhere so this this is not
noticeable on unless you have hundreds
of reloads but just be aware and again
yes this is these this litter was a
great way to do to store big amounts of
data in your memory and cheaply refer to
it a few words about immediate these are
one of value types in Eric Bischoff
machine they are so small that they fit
in one machine register they can be 32
bits or 64 bits depends what virtual
machine you have that's one word less
the the common name for machine register
size value this one word so the small
integers feed their local pits and ports
they are small enough to feed atoms the
indexes in a turntable and nil-nil value
this also it has a special immediate
value so when you store nil somewhere in
your program it will take only one word
it's it's an empty list and a virtual
machine treats it as an empty list but
it's just one word it does not take any
heap space a bit more about immediate
they not not full word width is
available for you well when two bits are
reserved and two more bits specify the
type of value this also explains why
integers that you put in your machine
register can be as big as 28 bits on
32-bit machine because 4 bits are taken
away or 60 bits if you go any any bigger
it will be automatically converted to
big integer I will mention it on a later
slide what what dangers this is hides
this is the red one the cave it when you
have some counter in your very big table
for example edge table or some database
table and this counter was integer and
it keeps growing and everyone is happy
takes one word your memory is it some
gigabytes and
you see no danger but then it reaches
twenty eight bit boundary it's out it is
converted to begin done automatically
and that explodes begin to becomes like
three or four words of size that like
triples your column size immediately so
be aware of integers crossing this size
when you have millions of them also
floats in air language from machine
flows are not efficient data type
they don't occupy one word they are
stored on on the process hip and it
takes two or three hip ports that 16 or
12 bytes plus the reference to it
because you need the pointer to point to
the float so don't don't store a lot of
floats if you value a memory it's much
more efficient to store as a fixed point
or a natural fraction and divide by M
but yeah well you still have the
possibility to use floats just remember
they are not efficient items I think
everyone knows that items are not
garbage collected but they are quite
handy they occupy just one word of
memory that's just 8 or 4 bytes and they
are very cheap to compare for equality
they are very cheap to compare to
another type but when you compare two
items it will compare the string
representation and that's not efficient
best is the chemical to just compares
two strings of atom names and also keep
in mind and when you store atom in
network message or on disk it will be
converted to string with the tag that on
load it will become atom again what is
not efficient so that that may explain
why some companies use integers instead
of atoms in database keys and this
complication is just to save memory and
save disk space box values this is a
general class of values that
are too big to fit into one register on
machine register so virtual machine
creates a piece of memory to dedicate it
to the value well you can be I will
mention them the they are listed below
and the blue one is your value data
manipulating and programmed is the
pointer the pointer to to process hip
that store something it always except
for the list box value always has one
extra word the red one it always has the
one extra word specifying what is inside
so least cells are boxed values they
don't have this header but they know
they are always two cells I will show it
next slide
remote peace and remote reps they are
too big to fit in one register they're
placed on hip with several words
occupying float numbers I mentioned
before it's like two or three machine
words and they can place in your hip as
well in big knobs and also those small
integers that accidentally grew over the
limit and became big noms they occupy
hip space and binaries I will go in
detail later on them so these things are
consists of two parts the pointer which
you manipulate and the piece of memory
the he pre space where your data is
stored lists a list element is always
list cell is always two two two two
cells to towards of memory so the the
first one on on the left and the right
first one is the value you hold in your
variable the value you manipulate it
points to heap that test to towards off
for the head and for the tail so air
long calls to head and to tail of the
list they are very cheap they efficient
and they implemented as virtual machine
up codes so if you call HD or TL on list
value that's this first this efficient a
proper list always has an eel as its la
tellement and proper lists are expected
by most functions in Erlang but you are
not constrained to this UVU you can
freely place any value in the second
element of last element of list or
second element of Lisa so improper list
that does not have Neil it it's lost
element it it's a bit more efficient you
don't need to build the second cell to
hold the second value you can just put
two values in one cell that's just two
words of memory you you use and some
libraries like leaked in Erlang I think
they use this way of storing or JibJab
edicts as well so summer long libraries
also do it just need to keep in mind
that last element will not be nil so you
cannot stop a new anymore in your loop
and storing as this is is more efficient
so list has property that it's forward
linked you can only go forward if you
store any element of your list in any of
your variables from it you can only go
forward if you need to find nth element
it's Owen complexity if you need to prep
end to this chip because you just build
a new list so you build a new list cell
you don't modify any more memory
prepending is very cheap and appending
you have to traverse the full list and
find the last element to open something
the - - operation is the product of N by
M it has complexity the more complexity
the the bigger your second argument is
so when you have to subtract two lists
and second list is big try to avoid - -
and instead you can use some better data
structure like GV sets or probably odds
as but i am not sure about that one so
it's fast ins if you're a right argument
is small but stay aware it can be slow
+ + operation I just mentioned append
this oven complexity or end because you
need to find the last element
something and you need to rewrite from
there you need to rebuild part of your
list and flatten and reverse operations
are cheap I will mention it right here
so again the tricks I mentioned it you
can use both both cells of offer list so
both parts first for storing the value
if you have just a pair it's more
efficient than tuple it's also more
efficient than a list of two elements
because the list of two elements will
take two cells there's four words in
memory and to protec three words in
memory and x-bar why the improper list
will take us to just two words in memory
for four two cells that's most efficient
way to store a pair reversing is cheap
because you just need to compete it will
naturally reverse if you just take from
head and put it from two to the head it
will reverse not naturally and this is
this function is implemented in C if you
have any pointer to any list cell the
tail and so if you have any member of
your list and you reuse it it will be
efficient it so from from your pointer
to the end it's never modified it's
always constant so whatever you build
from it is is also constant so it's
efficient and also a list comprehensions
compiler is smart enough this is the
piece of wisdom from rolling efficiency
guide compiler is smart enough to
optimize this comprehension if you don't
use the result it will just drop the
values it will turn into just a simple
loop a word about memory locality this
is important when you have big arrays of
data and so big that you actually start
noticing cache cache latency and cache
misses and for processing big big arrays
of data you are interested to have your
data linear in memory that CPU is able
to prefetch it and use cache lines more
efficiently so
when you build the list quickly in the
loop there is very high chance that it
will occupy contiguous memory block
that's good that's if there's a lucky
coincidence and it will be fast if you
iterate forward so your CPU will be able
to predict this and and prefetch this
this memory and the garbage collector
will attempt to rebuild this if even if
you have list which is has its cells
thrown all around your process heap
after garbage collect they may occupy a
linear block of memory they will stay
together and that is that is good for
for your loops IO lists iris for those
who just know the name but never use
them please do they they are great way
to manipulate strings and build more
complex documents from parts of things
and they they are very similar to
standard ropes data structure this is
essentially three inner long this will
be pieces of nested lists which hold
pieces of strings why they're good
because you don't need to rewrite and
rebuild your string every time you want
to append or prepend something and
appending something to our list is
cheapest or one appending something to a
string is not cheap you have to rebuild
the whole thing and why they are handy
because most of our lungs and our
functions take our lists easily just
have a quick look in the commentation
all the sockets functions or most of the
file functions except io lists and there
are some some helping functions to work
with them so you you will not feel
yourself limited when you manipulate a
lists and they are much faster than a
single string that you have to append it
constantly so it's good for your program
it's not just good for you it's also
good for your program
tuples tuples are essentially erase that
you can only write once when you want to
modify it opal it will be rebuilt and
the first read read the Bulacan on the
picture the the the the one that
contains three is a box box header tuple
laser is a box on on the hip and it
contains the size there are editing how
many elements are in to Interpol and for
compatibility reasons is limited to 32
bits world so 26 bits is the biggest
tuple you can build that's if you
probably all know that 24 bits is 16
million from all times the display
colors were 16 million this 24-bit color
and the 26 adjust four times that's 64
million at most you can fit in in a
tuple evidence even on a 64-bit machine
even because though you're bigger word
size allows you to have bigger tuples is
limited to 26 bits and there's another
example of more complex to pull where we
have another nested tuple they will
occupy two boxes in memory and if you
build them together they will most
likely stay together so accessing them
will be cheaper first or if you build
them from different parts that come from
elsewhere from some deep nested function
call they may accidentally occupy
different places or in memory that's a
tight loop that runs millions of
iterations you may notice some some
difference in in in runtime when you
copy a tuple the virtual machine will
make a copy unless it can notice the
patterning occurred that did that kind
of pattern in in in the smaller letters
is how compiler creates tuples
it creates at an empty in block on
memory and fills the it will in
descending order with values when
compiler sees they see
will optimize this code to modify the
same copy of tuple and this moment it
becomes effective efficient also a note
when you use records and when you modify
a field in a record it's also a tuple
and try to avoid modifying definite
state record members because it will
incur modifying all the paths from the
your record to containing record too to
continue record above it and so on and
try to keep your modified the data
members as close to top is possible to
make it do less copping tuples are
really fast to build and read that I
mentioned before it's compiler is able
to optimize this when it's not notices
but I think you can simulate the same
behavior in your program and it will
recognize it and also optimize it if you
modify this end in order and you don't
do anything with it tuple until it's
finished
when you have a big tuple because it's
array it convenient to have your
constant data in a tuple but when you
have a big tuple and you need to do more
manipulation than just change in a
single field or single element you may
consider converting it to a list it will
grow up in memory a bit but you get much
more instruments to work on it a memory
locality is perfect because they they
allocated a single book of memory you
don't need to worry and garbage
collector also will attempt to drag all
them and referred values together with
the tuple when you run a garbage collect
your to possible group with their values
the elements and nested elements and so
on so when you store something is just
just an outcome just when you store
something with many many nested members
you can run a force among garbage
collect to make it behave better
I don't have much information on mops
and because they're fairly new and it's
I didn't go into the source code to dig
on them but they they they start in two
different ways depends how many elements
a map has if it's two smaller than 32
elements it's stored as a list of key
value pairs
I think it's sorted for for for
searching reasons but if it becomes any
bigger its converted to the harsh array
three they are slower than records of
course because you have to do more walk
up stand just index read or index right
but the good part when you have this
harsh array three try and you modify a
some some key in it it will rebuild only
path to the root it will not modify all
the ball all other branches so this is
sort of all sort of efficient it's much
much better than rebuilding the whole
stuff full structure like you could in
chibi Tresor Jebusites and there is a
drop here when you use mops is state in
your chain servers or in your modules
they're very convenient it's easy to add
keys it's easy to match its you can even
spec type some keys and values but you
have to control yourself manually to not
add any extra keys in the runtime
because then it becomes quickly runs out
of control and begins problems appear it
gets harder to track where you added
this key and where it was modified and
and why why it even is here so it
relaxes the discipline that you had with
records if rules fuels when you have
data copying in the beam virtual machine
that dashed line is is the invisible
border between your process and
sports other processes or finer heap
when your value crosses this line it
will be copied so when you send a
message it will be copied when you spawn
a process arguments will be copied when
you interact with ads or debts or a port
it will be copied except the binaries
and on binaries next slide wineries have
some tricks to do to optimize passing
data a simple example of a binary as
much as they remember there was a second
header work to specify a byte size and
probably it also can specify last byte
for bit strings because bit strings have
incomplete last byte so I think it also
goes in that second world and it's how a
binary of five vices start on in memory
you have something like headers stating
that it occupies two words there's
mistake it should be 3 3 words binary
this header that says how many bytes and
the delica combined there is no terminal
terminator like it would be in C there
are 4 probably five types of
manifestations of binaries in your
program
they all look same in your inner long
syntax but they are stored differently
for optimization reasons so imagine you
created one large binary that's bigger
than 64 bytes this magical threshold in
virtual machine source so anything
that's bigger than 64 bytes goes onto a
separate binary heap that's a dedicated
place in memory where all big binaries
come to live and die to indicate that
you own it because you created it you
get a blue one on top the hip bin all up
the proc bin that's the pointer to
binary on the binary heap that you or
your process owns you can manipulate it
you can pass it around you can send it
as a message it's small it's just a few
words of memory
and it holds a reference to the main
body of data when your process does
garbage collect and notices that it's no
longer used it will die and if reference
count reaches zero the big binary on the
right will also die now there is a trap
when you start sending the binaries
between multiple processes I will
mention it on the next slide so and then
he been the top one the blue one is when
you have a small binary that can fit in
your in your heap and will not slow down
your data processing they are smaller
than eight words of memory 64 bytes and
they are completely contained in your
process and the way they are copied
every time you pass them around between
processes and there is also sub binary
the arrangement below
the binaries up produced when you call
binary : part and also there are binary
much objects I know less about those but
they are created when you do binary much
or binary search I think as well so they
are they behave like say binary they
refer the bigger binary they say from
this position to this position this
position this is data now compiler is
able to optimize chain manipulations on
a single binary if it is able to prove
that binary is only ours and that it's
not sent anywhere in between if you have
if you create it in your binary and you
do Li a row of operations on it it will
be optimized to happen in place so as
long as you don't have don't share it
listen next slide as you as long as you
don't share it with anyone in this
moment it will be optimized
also when compiler processes pattern
matching and you have in all closest you
have an unused the underscore variable
compiler is able to find it and skip
processing it completely so this will be
this part will be efficient and you can
see which optimizations I used if you
have this highlighted compiler option or
set the flag compiler will print what
happened to your binaries what it was
able to detect an optimize so the the
famous drop with sharing big binary
between multiple processes because when
you do this it will only share that this
Prague bin that the dark part when you
send it to different processes but every
time you copy it the reference count
will increase and processes will will
store it until all proteins are
collected by garbage collector than it
only then it will be able to reclaim the
memory I also pay attention when you
grow a binary it will copy in these
three situations when the binary is sent
when you manipulate the binary and send
it as a message in between it will
create a copy in memory that's just like
you you break you break your
optimization so move your sending to the
end of of operations on the single value
when you say insert it into ads it's
same case a copy is created then this
copy will remain in your memory and when
you do a binary much it will create a
list of matches that are effectively sub
binaries holding reference count onto
the main body of data so when you have a
network server that processes a big
stream of data you want to keep an eye
on where your sub binaries go and where
you send copies and and who holds the
reference so the memory will be freed
eventually that's here performance on
ads ads copies all the data you put into
it and it also copies all the data you
read from it so having
big rows of date a nets or really big
values is expensive or maybe imagine you
have a big role big data structure in
each row of your at stable but you
modify only small piece in this
situation you may consider creating a
second table and placing mutable Fielder
to avoid copying the whole thing out and
in all the time
it is also it's it's efficient to store
items in ads because they are memory
they share your item table or your
virtual machine they store they only
occupy one word of memory but when you
write it on disk it explodes because all
atoms are represented strings so
remember this don't don't write many
atoms to network it's bad for your
network an external term format it was
it's created when you call term to
binary it's also used in cross not
communications and it's used in debts on
disk and by media and by react as well
and programmers use it when they store
binary blobs in databases but yeah again
the atoms not remember they get
converted to strings and it's also its
CPU intensive but since like 17 or 18
they're long waiting I think they made
it turn to binary less a bit slower but
it can now it will not now block your
your scheduler so when you encode the
gigabytes or many megabytes of data it
will not block your CPU so this part is
well done in recent couple of fair long
versions and there was slide it's less
it's more performance than data so local
function calls are fast this when you
call something in your module without
mentioning the module name this is the
fastest when you mention the module name
in your call it becomes external
function call and this incurs a lookup
in module table and that's three times
slower and when you do apply call it
also
apply calls re-written as a module colon
function call with variable names
instead of atoms and then it also incurs
a function lock-up so it does double
lookup and then is six times slower and
when you do create an closure when you
create a lambda inside your function
that captures some values from your
function it could it could be it could
be a binary it could be big binary that
just arrived over network to you and you
created a lambda that that captured this
value and store this somewhere in memory
and this means it will hold this data
for a long time possibly blocking the
garbage collector from reclaiming the
binary memory so keep an eye on what
your lambdas capture they may take some
some valuable resources and hold them
forever
and yeah they behave like they have an
invisible arguments that this'll distort
values are passed as invisible arguments
toe-to-toe lambdas and it was it and
thank you for watching okay
who has a question for Dimitra no was it
so so shocking
so my question is like all this stuff
it's amazing really interesting half of
it I knew already half what was
absolutely new for me as well as
probably many people out there and while
right in our code or reading someone
else code we can notice things like that
and try to optimize them but the most
often problem that I encounter at least
is when I have existing system and it
has performance issues and many of them
are because of things like this how do
you go about profiling a record no
details but just like if you have some
general guideline on how to you have an
airline program and it's slow and you
suspect that it probably has one of
those things but this this should come
at the end of your investigation you
begin for example you don't know you
have no idea what's happening you you
play some metrics you do some time your
TC calls in your code you you graph them
you see some spikes on graphs or maybe
you just notice that this takes too long
so when you deep dig deeper your time
your TC again then it becomes too small
for timers TC eventually you you notice
that this is not right here so then you
then you come to this point where your
data is wrong or maybe it's right but it
could be hundred times faster because
cache misses is no joke in CPU okay so
there is no magic there just now in
regular profiling yeah you begin like
you regularly weigh like you normally do
but you come to some piece of code like
this it has one of these data problems
it's not even problems it's just less
efficient data structure or or maybe
it's is just spread in memory that
memory locality is broken then you have
like three times slower code no what I
meant is that like for example binary
heap you can detect that something is
wrong with that by looking at airline
called memory and then it will show you
that you have huge binary heap and that
probably points to something if you
don't expect to
that your system has huge binaries I was
more referring to tips and tricks like
that if you have any but okay yeah well
there's this one thanks anyway we'll
talk later
thank you hey another question from Joe
I'm gonna be careful it's it's soft if
he few comments but one was I think you
should always measure because
performance estimation is just about the
most counterintuitive thing on the
planet because I mean you haven't just
got one process you've got all these
other processes doing things at the same
time and in answer to your comment I'd
say you know when you design a program
if if you if you make an abstract
interface to that doesn't expose the
underlying data structures of what
you're doing and you just use the most
inefficient algorithm possible to do it
and then you measure and if it turns out
that that's not good then you can just
swap it with another module so
deliberately the the interface is to
several of the ailing modules are
deliberately chosen so these you can
swap dicks to or dicts and your program
will behave exactly the same and that's
deliberate so so the the the the the
moral there is you know choose the
shortest and most beautiful most elegant
code first and then measure and then if
it's not efficient then then go in and
change that keeping the same interface
and and then if you're if you're an old
man like me just just wait 10 years
gives you a factor of a thing I mean
still things are getting faster at a
rate of about 15 percent per year if you
wait 10 years it'll be a thousand times
faster you wait twenty years will be a
million times faster so the airline now
if you measure it is several million
times faster than the original one that
I wrote and it's it and it's not due to
the cleverness of the implementation of
virtual machine if you look at the ratio
of clock speeds the the I think that
today's airline is about six times
faster than the original one and all the
other games come from increased crop
clock speed so I mean really clever
programming has improved the performance
of the VM by a factor
increased clock speeds has increased by
a factor of several million because when
it was done one point six megahertz was
fast and now they're running at 3
gigahertz so I mean there's no way that
you can do that the other thing about
optimization is programmer time costs an
awful lot of money and just throwing
throwing new processors in it is much
cheaper than the number of hours to
optimize it so in fact it's it's it's
just not economic to spend programmer
hours and optimizing code rather than
just you know buy a via you know if you
have performance problems and you're in
a company as opposed to research just
buy a 24 core machine because it's
cheaper you know that is equivalent to a
few hundred hours of programmer time and
that fewer as a programmer time can at
most give you a factor maybe five or six
but the hardware will give you a factor
of 24 by going from one quarter 24
course and it's much cheaper so it's all
economics actually and patience just
wait 20 oh you wait another 20 years
it'll be another million times faster
hey can anybody top that any more
questions okay thanks to Metro so like
to thank all of our all of our speakers
Melinda you want me Joe and Magda and
elspeth is she out there for organizing
this and clan a bank of course for being
our hosts thanks a lot
now you'll meet outside food and drink
and we can talk thanks a lot for coming
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>