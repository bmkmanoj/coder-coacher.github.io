<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2015 - Scott Lystig Fritchie - Actively measuring and profiling Code | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2015 - Scott Lystig Fritchie - Actively measuring and profiling Code - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2015 - Scott Lystig Fritchie - Actively measuring and profiling Code</b></h2><h5 class="post__date">2015-03-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/q64f1_7HK24" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so i made my present a tarp the proposal
to Erlang factory and I realized that
Wow I wrote something for like two or
three hours worth of time and so i'll
have a lot of text on the slides and
some of them i'm going to gloss over you
can find a viet via twitter i just i
just mentioned where to find a link for
this so if you want to use it as
reference material or for chasing down
things after the talk feel free to do so
i'm a senior software engineer at basso
japan and i've been using unix since
1986 and using relax since 2000 I've
given talks before it rolling user
conference and and and erling factory so
I hope that qualifies me for something
feel free to to check whatever box it is
on the on the rating thing as you as you
go out we really appreciate the feedback
so i have this online both the long-term
URL and and today looking at Twitter if
you want to go find the full text of
this and i'll include both here and
later through the presentation links to
all of the github repos and and whatever
else for the tools that i talked about
so today's focus is on profiling airline
code so this is code that you've already
identified as being suspect using the
use method or some other kind of
measurement based methodology and i'm
going to emphasize measurement a lot
because your guesses are always wrong
your guesses are always wrong my guesses
are always wrong so i'm going to be
recommending multiple methods and these
flame graphs are one tool that are a
little bit knew that many people maybe
are not quite familiar with but they're
not perfect either in fact they lie all
the profiling tools that we have in our
tool chest as Erlang developers they lie
and I was putting some photos together
for this talk and then I realized that
everyone had a beard beards are not
required
for this so what they do help in very
cold weather but aside from that so
given given that we're interested in
profiling airline code very specifically
then we want to identify Erlang function
or functions that are consuming the most
cpu resource so you can measure that in
terms of wall clock time or on scheduler
time and in fact depending on what
measurement technique you're using you
may get one or the other and they will
tell you different results as that in
the in the in the profiling so if you
don't know Brendon Greg where's his
photo he is bearded sorry about that
he's written an excellent book I highly
recommend this book if you have to do
code profiling or systems profiling or
benchmarking so what he says about
benchmarking I'll take some quotes and
I'll probably use the word benchmarking
and profiling a little bit
interchangeably and I shouldn't do that
but I probably will and I apologize for
that did I say I recommend this book
this is a really good book so he
describes passive benchmarking as you
pick a tool and you kind of run it you
follow the readme or something and you
make a slide deck of the results you
give it to management and you're done
and you have what actually happens is
you do this thing a but you've really
measured some other thing be and you
conclude well I measured see right so if
you're doing this benchmarking in the
passive manner you know here you you
tell management I measured see and
really you've measured crap you didn't
know what you were measuring so you know
this is kind of a conundrum whether
you're doing profiling for a small
amount of code or profiling for a larger
system and Greg recommends active
benchmarking where you analyze the
performance while the system is running
and you're not doing post mortem but you
use other tools to actively monitor
probe the system make certain that it's
behaving the way you expect it to is the
test doing what it's supposed to do like
a test case load generator or whatever
does the system under test actually
respond to the test in the appropriate
way are you calling like no
lop functions or something like that so
then you can understand what are the
true limiters of the system under test
and the true limiters of the test itself
maybe you're using a load generator and
the load generator is misconfigured or
has a bottleneck where it's speaking
over 10 base-t ethernet rather than
gigabit ethernet or something like that
that stuff happens now and then so for
active profiling we want to understand
what the workload does what the code
under test how it actually responds to
the workload what are the true limiters
of the code that's that's our goal point
number three is the goal that we've set
for ourselves but we also need to know
some of the limiters of the profiler
tool itself you want to measure because
your guesses are wrong and these tools
they have some limitations and so
they're going to lie to you Martin
chillin in the river to Elwha they good
presentation about a year ago you see
and they're stressing measurement to
right so don't guess trust your
measurements and know what you're
measuring so Lucas Larson who's maybe
here in the in the room hi so so a talk
he gave last year and EU seed about
minute 31 he says profiling is
inherently difficult you have to be
aware of the entire stack I've been
using Erlang for 15 years I don't know
the whole stack either I mean this is
just sort of a short list of parts of
the staff in in a Erlang system memory
management operating system threads the
erling process schedulers file i/o is
this deep dark box that not many people
are aware of it it doesn't work like it
does in Python or parole or C so skip
ahead didn't I have a shoot i had
another quote from well that was a quote
from garrett I'm sorry erling is an
operating system for your code so if you
don't know where to start Brennan Greg
in the system's performance book he
recommends one method out of several
it's called the use method utilization
such
raishin and errors so the errors are
usually the easiest define they're
usually loud especially in an erlang
system the the sasal error lager if
there were errors there you should be
looking at those first you don't have to
use fancy tools frequently to to see
that there are errors that may be
perturbing your system then more often
than not looking for saturation rates is
easiest to do and utilization is the
third place you would look but if you
you look at these three things and find
the iterate on the use method to to take
a closer look at the saturation rates
for the error rates perhaps fix a
bottleneck then go back to step number
one and repeat until your system is fast
enough and it's a longer blog post about
this so she'll Annette and ly they say
well at the up earning system level you
can look at these things in the Erlang
world there's a lapsed wall clock time
for execution of processes message queue
lengths across the system number of
reductions executed by processes garbage
collection events how long they are and
so on so the talk proposal I'd also
mentioned kind of making a survey of
tools available today I don't have time
for that and I'm sorry so I'm going to
just kind of mention them here a lot of
text feel free to look them up later if
you're not familiar with them the second
half of my talk I'll focus most of the
time on flame graphs so I have a couple
different categories one for things that
are bundled along with OTP in the
distribution one way or another except
for percept sorry but since so many
people involved in the project in the
research group surrounded I included in
the OTP source especially since version
1 a percept actually is an OTP so some
of these measure activity by airline
code kind of more directly instrument is
for looking at memory allocator
activity inside the virtual machine the
lock counter for looking at data
structures in the virtual machine that
still use locks and since each iteration
now with 16 and 17 and 18 are using
fewer and fewer lock based data
structures that that tool I don't know
what you're supposed to use for looking
for contention on the on the luckless
ones is it gut feeling oh great guessing
right don't do that okay so and then
it's also the efficiency guide to to
take a peek at outside of her lying from
a big fan of the ypres package should
specifically red bug Fred Hibbert's
package recon and his electronic book
stuff goes bad or lying anger highly
recommend that to someone at lunch
Dmitry was telling me about EEP that I
need to do some research on but one of
the things that they can do is take
tracing information from curling and put
it into a format that you can then feed
into the cake cake ash grind tool for
visualizing the stack trace staff aware
profiling information if you want to
look at the Erlanger chiral machine
internals themselves you can use any of
the tools that are available in looking
at C and C++ programs G prof valgrind
dtrace and system tab and so on there
are other measurement techniques on the
command line or in compiled code you can
use the timer TC function please be
aware of that code that is executed as
you type on the CLI is interpreted and
executed in a different way and has
different timing characteristics than if
it's compiled so please be aware of that
if you're using the timer TC command on
the at the shell I'll talk a little bit
about the latency histogram tracer which
is quite useful for executing a function
a bunch
and getting detailed statistics about
the latency of each one of those calls
you can also use a dtrace and system tap
add some annotations to your code so
here I have examples of adding for probe
number 25 of the Erlang user trace we
activate the probe with one at the start
of what we want a time and a 2 when
we're done and then in a deist DTrace we
would look for this probe that fires the
Erlang and that with pin number with the
argument of one or two which would tell
us to start attending time and then we
have nano second resolution granularity
for for calculating latencies but it
does require having a the diné trace
dynamic tray support compiled into your
virtual machine and here's some output
from the tracing module that I'd created
so I have a react running I'm trying to
test function a bit cask put arity three
run it for ten seconds wait for 10
seconds and I get a histogram the
minimum latency time in apartment
milliseconds was zero maximum was 48 the
mean variance standard deviation 50th
percentile was three milliseconds 98
five milliseconds 99.9 that 14 there
were fifty one thousand some function
calls and then a breakdown in terms of
at one millisecond there were thirteen
thousand calls and so on so this gives
you a much more detailed picture of how
long that era d3 function was was
executing each of those fifty one
thousand times you did turn off the
variable cpu clock speed feature right
when you're doing this testing how many
people forget to do this when they're
benchmarking or profiling their code
turn it off turn it off
any questions at this point I'm going to
kind of segue into the next point and
talk about why so many of the tools that
we have are based on Erlang tracing why
we use it why we think it's cool and
some of the limitations oh sure they
should when I say profiling airline code
I mean beam code more specifically so
they should be applicable to to elixir
whether or not they are nice and dealing
with the function naming scheme that
elixir uses but I suppose I should be
more general in in this day and age I
mean it's only 2015 right that there
there are multiple languages running on
the beam these days thanks so we're
lying tracing it seems a little daunting
because it's complex but really it's
simple really honestly there's not much
to it except that there's a whole lot to
it so if you're not familiar with it I
want to spend a few minutes talking
about Erlang tracing and just kind of
basically what it is so for each event
and we'll talk about what what kind of
conditions events are what happens so
whenever an event happens we send a
message which is a tuple and we send it
to an event tracer thing and the thing
can be an erlang process or can be a
port either a file port or TCP port and
you can think of that event tracer thing
as the consumer or sink or destination
for trace messages so if we've got the
sink the sink is pretty straightforward
the source or producer what can that be
and it could be in Erlang process or a
port or the scheduler threads there's
probably one or two small categories of
things that I've glossing over but since
we're really interested in Erlang
processes and
beam code execution will focus on that
so I can choose any one process to trace
or some processes or i can say trace
them all trace the ball there are
problems with tracing at all and i'll
talk about that in a little bit but you
can do it you also have the option of
saying any processes that are spawned
after tracing starts or another
variation if a process is linked after
tracing starts and there are a few other
options as well but so now we've we've
chosen the we know about the sink we
know about the sources so some of the
conditions for the source or the
producer calling a function whether it's
a global in terminal local intro module
call it can also filter based on
function name module name number of
arguments using Erlang style pattern
matching sort of the match match spec
style matching sending and receiving a
message process is started or stopped to
GC it's like a collection either a minor
or a major one a process is running or
stopped by the by the scheduler if the
process is exited and a few more that
didn't fit on the slide they're just
tuples it's simple they're just tuples
so the simplest one the first element is
Adam trace a nap it'd and then Adam call
and then a mfa of 32 bola module
function and the args now it's a little
cool is that the tracing system will
give you the full list of arguments all
the arguments so if you want to have an
additional kind of tracing logic based
on on the arguments that you see there
the virtual machine will give them to
you which is kind of cool there's a
variation you can ask for the timestamp
and some additional information so they
add them at the at the beginning changes
a little bit to tell us that there's
this time stamp thing at the end and
that's the micro second or lang style
three to bolt the now
3 2 full four times C so that gives you
the microsecond granularity as long as
the operating system isn't lying to you
too much garbage collection events when
it starts and when it ends and there's
some additional information in the stats
term to tell you what what happened at
the beginning and end of those GC events
when a process scheduler takes a long
process and schedules it in starts
executing and when it gets switched out
for whatever reason so again here's an
example showing the arguments that you
can get in the trace message if I turn
on there's a there's an option that will
give you just the air d number so if you
have this really big data term you end
up logging a lot of data you know
sending these big messages and big
messages mean larger overhead and if
you're sending millions of these
messages a second you probably want them
small so you can ask for just the era T
so here's an example an monitor of air
d1 and then the return from item if you
if you enable that option the second to
the last item there is actually the
return value from the function so you
can do some additional post-processing
of these events if you if you care about
that value just kind of cool so you can
turn this on and off anytime on your
laptop in production anywhere it's
really cool if you haven't experienced
this or talk to someone who foams at the
mouth about how great this is to be able
to do on a on some customers system off
in Siberia while you're you know
comfortable at home in Miami it's really
cool the pattern matching is really
quite powerful even though the API is a
little weird it's also quite nifty that
the event tracer if it's an erlang
process it storing complete you have the
entire power of Erlang and elixir and
LFE to to process those those messages
and that's that's really pretty cool you
can turn it on in production system and
then go for coffee and forget that it's
on and so you don't necessarily need an
event tsunami which is another problem
of generating millions of offense and if
you're sending them to an erlang process
you only have one cpu cores worth of
work for that process to consume these
events and if you end up filling that
processes mailbox and killing your
production system because you ran the
process out of memory don't do that
right but you know you're aiming the gun
at your foot when you turn this on in
production without some sort of other
tooling around it to to automatically
turn things off if you forget and that's
why I recommend using a tool like red
bug or recon the API is also frequently
difficult to remember it's probably why
there are so many tools for managing it
is just there's probably 10 or 20 people
in the world who actually use it
frequently enough to remember what all
the cryptic little API things are so the
f prof tool is included in the OTB
toolkit it is the most useful of the
profilers for our purpose of profiling
or line code it's mostly easy to use it
measures wall clock time for calling a
module function with some argument list
so it's really easy for for measuring
execution time of pure functions
especially both time executed by that
function and then by all of the the
functions called by it and get the stack
trace and being able to get a report
that tells you a profiling data like all
stack the the report is a little
difficult to read Earl grind and the EP
tool and using a together with K cash
Kate k cash grind have is a better tool
when I talk about flame graphs
I think that frequently there are better
even better tool for visualizing and i
hope to you'll be a little excited and
fired up about using them as you as you
leave the room f prof is really not
designed for multi-process measurements
the call stack interpretation can be
okay or terrible due to how it
interprets the call stack and again this
is implemented by your line tracing so
you only have one consumer process one
tracer and if I turn us on in all
processes and on a react system I can
slow it down by two to three orders
orders of magnitude it's really painful
if you turn it on for all processes in
the system and all newly spawned
processes so I'll switch to flame graphs
now here's some hyperbole from matt
raney who is using Flame graphs to debug
a problem performance problem with
nodejs and I don't know how he had a
function that went from four hours to
one second for her fifteen fifteen
thousand times speed up but he was
really psyched and excited about the the
power of this tool maybe there's some
hyperbole there maybe so flame graphs if
you haven't seen them I'll have a lot of
examples coming up shortly they're
invented by sun microsystems for being
able to visualize pro flying data but do
it in in a much more kind of visually
obvious way than the tools they had been
using at the time it was originally
based on dtrace and Solaris and it's
it's kind of gone viral because a lot of
people have found them very useful
including myself and some links to more
information about them so here's a flame
graph of a react system and the the
colors the colors don't mean much
they're they're just help the I kind of
differentiate the different levels on
the graph you do this all on the same
color and it it's not quite the same so
if we zoom or
the y-axis is the call stack depth and
the y-axis left left to right order it
doesn't mean anything so something is to
the left of whatever these things are
actually sorted at Alexa graphic order
bye-bye function name so anything that's
to the left or to the right it doesn't
matter the width however has to do with
the proportion of samples and it again
this depends on the the the technique
used for gathering the samples it could
be simply a number of samples if you're
sampling on a time-based like a
frequency based sampling or it could be
execution time wall clock time or
virtual CPU time or something like that
so we zoom in a little further so here
we have process to 88 90 and it is
calling react course stats which is then
calling Folsom metrics which then uses
timer TC goes back to react kv stat and
so on and then these things down at the
bottom each one of these represents a
different process so this was 28 28 894
and then these were just other other
ones and they're too skinny to to see
the tool creates SVG images and they're
actually clickable so you can zoom in
and see well what I can zoom in meaning
isolate only stack frames that are
immediately above or below where I click
I didn't want to jeopardize the demo
gods in actually showing the the SVG s
here so I'll fake it by doing
screenshots as I'll see going forward
but they are sort of interactive I mean
you can click on these things they also
have mouse overs so I can mouse over
here this this use of lists fold for
example and then it would tell me that
there were X number of indications for
this percent of the total number of
samples seen so here's here's a example
I give in the repo for the e flame to
library
this is a small react system and all the
processes in react as I'm running a
small benchmark load all those purple
things at the top are sleep time so most
of the time the system is asleep or you
know any individual process is is
actually asleep most of the time but
there's so many processes I mean you
can't see anything here so we will will
one of the things that i'll do is i'll
post process the results to remove all
of the sleep samples so now will only
look at active rolling process
computation and it's a little less
cluttered we got rid of all the sleepy
stuff some processes are actually big
enough now that we can we can actually
kind of read without even zooming so as
i was setting up the test i was doing
some stuff at the CLI and this time
right here for process 1333 is related
to that as I was as I was actually doing
the samples react course statistics is
this chunk this chunk that chunk and
that chunk so we're spending a lot of
computation time actually doing stats
inside a react so I then post process
the results again to take all of the the
and I'm doing this with auken said by
the way to collapse all of the samples
to remove the individual process
identifier from the from the stack trace
so now all of those stats things all
fall into one great big lump here and we
see that it's taking 25 almost thirty
percent of the CPU time and if I mouse
over it would tell me it's like 27 point
point whatever it is and it we can see
that lists fold is doing it quite a bit
and we have some Erlang nifs also taking
a fair bit of time and if I were
interested in that part of the subsystem
measuring at this time I could zoom in
and do some more work there but I'm
actually interested in another another
problem right now is a particular vinodh
is this is the code i'm interested in
profiling and if i look at it react
vinodh process 450
it's sleeping most of the time okay so
we'll we'll ignore the sleep time and we
get something that looks like this so
it's spending all of its time under this
current workload doing Jen fsm handle
message and I know from the work that
I'm applying doing puts to the react
database this is exactly the function
that should be executing and it splits
its time between doing a get because the
react able do a get to see if the key
already exists from the back end and if
so if it does exist then it will merge
values based on V clocks and whatever
else before doing the put and we
actually have the put activity happening
where is it actually happening it's
really small actually I had also
captured some background process asking
for statistics about how many keys are
in this particular vinodh this
particular portion of the database and
so that actually was taking more time
than the puts that I was doing work it's
a little scary maybe we should look at
that instead of the the puts that I was
trying to do so there are three methods
for for creating these things of you can
profile the vm using standard techniques
using Brendan Greg's flame flame graph
toolkit it's really cool but it tells
you what's happening in the guts of the
virtual machine it doesn't tell you
things about particular airline
processes or what airline code they're
executing most of the time metatube.com
stack reporting I've got a variation of
it called a flame to it has a couple
different methods for call stack
calculation it can use or lang tracing
or not with all the strengths weaknesses
you can use a patched vm to have lower
trace event overhead I'll talk about
those patches in moment I hope maybe it
has easier to use API but I'm the only
person really using it aside from a
couple other folks at pasho so I don't
know actually if it's easier to use or
not pull requests are
gratefully accepted it's not always
accurate about the call stack reporting
either which is really annoying but
that's why you should always measure and
don't trust anyone tool but try to get
some confidence by confirming results
with multiple tools so to create flame
graph you start work load on system keep
it running for the duration of the
measurement use a measurement framework
from previous slide then you convert the
raw measurements to the standard flame
graph input file format generate a SVG
image and then if you need to convert
that to another graph format you can do
that if you wish here's some goop about
how to do that I hope it's not too
difficult you can look at it later or I
can talk about it during the question
period so the flame to library one of
the things I did for the sorry the
current version current method without
patching the vm uses a very scantily
documented a match spec option called
process dump and this creates the same
summary text that you would see in a
neural core dump a text file and on a
react system each one of these messages
can include a blob of this info that's
15 kilobytes or more for each trace
event that is what it is so I'm on
really really interested in the in the
call stack and all this other info which
looks like this if this text looks
familiar this is the this is the same
text that is delivered to you in these
trace messages and I don't care how many
messages are in my mailbox q I certainly
don't want to know the contents in CPU
expensive to compute ASCII format or the
process dictionary or you know a lot of
other stuff I want the call stack that's
what I want want the call stack so we're
going to hack the vm
and I added another match spec called
process back-trace that only formats in
C code the stack trace and stack trace
only and there couple of different
functions with a slightly different
suffix in the flame to library to use
this style of generation so like I said
I can really slow down a react system a
hell of a lot by turning on tracing on
all these processes time based sampling
like g prof has been using for four
decades or DTrace have very little
overhead and if you just have enough
time to gather the samples you can
generally statistically valid profiles
i'm using this method but g prof and
dtrace they don't give you any
information again about what erling
process is running at a time and what
the what beam code is being executed at
that time if you're DTrace magician or
no one who could help with this problem
please let me know but i'll gloss over
the vm hacking here and how to build it
i created an async thread that all the
code i added by the way uses the word
goofus to make it very clear that this
isn't erickson approved code at least
not quite yet but it basically sleeps
for a period of time and if there's a
global variable that is set by a
debugging biff if that's enabled then
for each of the schedulers for there for
there's scheduler for their their
scheduler process a structure and c we
set a flag we set it to 42 because
that's answered everything and then we
go back to sleep we just loop and inside
beam emu for interpreting every bean
bytecode we insert this check called
goofus check and if that flag is set
then we write a time sample and then we
just add goofus Chuck do a lot of lines
in this already really huge and not
particularly beautiful source file and
then here's what happens when we
actually want
to write a particular time sample we get
the process information coming from the
vm create a temporary buffer we print
some stuff we f write it directly to
using the c library fwrite function we
destroy our temporary buffer and we we
go on and it's really ugly you know even
for a hack I suppose with some macros if
we wanted to pursue this technique for
real and can hide it a little bit um it
doesn't see a lot of stuff that happens
inside the virtual machine garbage
collection happens kind of in-between
execution of like beam opcodes right it
can't see a sync file i/o activity it
can't see port activity or deferred work
auxiliary work items all that stuff so
there's a lot of stuff that you can't
see with this method it might have some
bias in favor of functions that are that
are immediately scheduled in by the
scheduler went immediately after the
flag is that maybe I don't know it does
have very little overhead so on my
laptop instead of 50 to 300 times slow
down on react I see something like two
or three percent even sampling every
seven milliseconds or every three
milliseconds so that's very nice it's
more accurate a lot of the time but not
all the time that's a measure that's a
thing of ongoing work here's what these
new trace files look like they contain
the pig program counter continuation
pointer additional stack trace doodle a
dot with a on a line all by itself
because if we had an SMTP aware parser
we can reuse some code and then we just
repeat it's not too hard and I already
step through some of the most of the
graphs in this tutorial but there's a
lot more explanation to to go along with
it just as a release and at the end of
my talk this is the last slide
be active in your profiling in your
benchmarking in general whether it's
Erlang system or anything else but
please don't assume that some function
that you're particularly interested in
that it's actually being called I
confirm it and don't assume that it's
particularly fast or slow like by by
guessing or something like that measure
it and also try to be aware of that any
measurement technique even even ones
that have been around for decades they
all have their flaws so if you can
confirm your measurements using multiple
techniques you can get some extra
confidence in what it is you've actually
measured and that I went really fast I I
can go back and look over some later
slides any questions you have I'll take
them out</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>