<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What NOT to do when building large scale systems in Erlang - Chandru Mullaparthi | Coder Coacher - Coaching Coders</title><meta content="What NOT to do when building large scale systems in Erlang - Chandru Mullaparthi - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What NOT to do when building large scale systems in Erlang - Chandru Mullaparthi</b></h2><h5 class="post__date">2015-11-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EBQKlyaM7bY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone my name is Chandra mother
partly a software architect that by 365
a little bit about me I've worked in the
telecoms domain since 1995 I've been
working without Lange since 1999 and
currently I work for bet365 a little bit
about by 365 it was founded in 2000 it's
a privately held company located in
strokin Trent in the UK it's the largest
online sports betting company both in
terms of number of customers and revenue
we have over 19 million customers is one
of the largest private companies in the
UK I think it currently we are at about
2,500 people and about 300 order in the
technology department just an
interesting stat in 2014-15 I think
about 34 billion pounds was staked in
gaming so it sort of gives an idea of
the amount of revenues and traffic
flowing through it the systems and it's
a very technology focused company oh
yeah actually before we dive in I have a
small request from the audience if I
crack a joke
can you please and he liked it can you
please laugh loudly because I'll tell
you the reason early this year I
presented at the UC so that was recorded
and I have a nine-year-old son who's he
is into programming so he watched my
video and said oh it was really nice I
showed it to my friends there's a bit
awkward when you cracked joke and only
you were laughing because I think the
microphone was only catching my voice
and what the audience's so just so I
don't have to face that comment again
from him and right so let's dive right
in
so this talk is about basically what not
to do so that you end up waiting at 2:00
in the morning
because you've been called out lang as
some of you if you're using it is quite
powerful so you can do a lot with it but
you can also build a system which goes
horribly wrong so you get called out at
all times so firstly starting with
message passing that's what you know
that should be its a selective receive
as we call it you can you can choose
what patterns you want to what patterns
you want to handle at any one point and
it simplifies writing your code but if
the incoming message rate is greater
than the speed of execution of each
message well you know queues build up
and one of the last things you know you
want to do in an hour long system is how
process building of message queues
because generally after that the system
becomes sluggish and hard to recover so
be amah one of the fundamental principle
is when building in Erlang system do not
allow large message queues to build up a
process the reason is once your message
the message queue is large um depending
on how you've written your code it you
could end up every time you're looking
for a message in a message queue you
could end up scanning the whole message
queue which can take a long time if it's
big
also our line has this feature that if a
process has a large message queue and
another process is trying to send a
message to it it end it incurs the
reduction count penalty a reduction
count is basically if you like it's like
a time slice every process spawn with in
Erlang has gets a 2,000 reduction quota
and each reduction roughly corresponds
to a function call but if it's but if it
sends a message to a process which
already has a large mailbox it gets a
penalty that's our lungs way of coping
that slowing down systems of the process
which has a large man
you can catch up so one way to reduce
massacres in your system is to actually
suppress unnecessary messages now this
is an example of say what a symbol
server might do you have a message
handler and it's doing a receive Clause
so it there's a this a message coming in
which is the Taplin from and message and
what it does is it a its forms another
process to handle it and goes back and
it's received you it does a spawn link
because it wants to know when the
processes and did or if the process
terminated abnormally and the spawn
process starts the worker it does
something and sends the result back to
the parent and the parent then sends the
result back to the caller which you know
this is a fairly straightforward way of
writing it which is fine it will work
but the problem is for each incoming
message if you count how many messages
you're passing around so there's the
message coming in which is the tasks to
do you spawning a process it does
something it sends the result back to
the parent process so that's two
messages and when it dies because you
spawn linked it sends an exit signal so
that's that's three messages being
passed for each task you want to do in
the system which is fine again if you're
handling hundreds of messages a second
or maybe even a couple of thousand a
second but if you're handling tens of
thousands of messages a second instead
of 10,000 messages a second you are
actually we think the system is dealing
with 30,000 messages per second written
here so another way to write that is
again you there's a receive and you just
spawn the message but sorry you spawn
the process but along with the data
required to execute the task
you tell it who was calling it and
within the spawn process which is the
worker you kid that's where you can you
can detect any crashes and then call
their a handle from there and send the
result directly back to the caller in
this particular case you're really
there's only one message you're handling
ok this end result is common to both so
if you compare the first in the second
version the first version has four
messages being passed on the system for
a single task execution the second one
there's only two and like I said a for
moderate loads this is not a problem
but if you're handling really high loads
you this is unnecessary work and if you
can cut down on it you can get better
capacity out of your system one thing to
be beware this is the number of ways you
can spawn a process so this the is a
spawn spawn link which are Biff's within
the Erlang model but there's also a
module called proc lib where you can
spawn a process but what it does is it's
very convenient when you're developing a
system actually because if a process
crash is a process is spawned using proc
Lib crashes you get a nice error report
but their reports are sent to what's
called the centralized data logger and
error log is really bad at handling a
high value of a high volume of crash
report so for instance say your system
is running at a high load you're
handling tens of thousands of messes a
second someone's introduced a new
configuration item which actually then
starts crashing every process which is
being spawned because either you the
whatever your configuration is wrong
it's unexpected now suddenly you're
generating tens of thousands of crash
reports a second and that's one way
- very predictably kill your landlord so
protolith nice when you're doing it
during development but I'd never use it
when in code which is running it
handling high volume of messages another
another nice trick because so the reason
message queues cause problems is because
the receiver of a message receiver of
message has no control over the sender
anyone can send a message and there's
nothing I can do about it
so in such cases where where you want to
make sure that a consumer is not
overloaded by producers say because of
traffic spikes maybe your regular
traffic patterns are pretty predictable
and your system can deal with it but if
there's an unexpected spike of messages
and you want to be able to deal with it
without without killing the consumer
process say one way to do that is get
all the producers messages into any case
table and let the consumer pull messages
off it so that if there is a temporary
spike of message from the producer the
consumer can still keep working at its
usual pace and your system can behave in
a stable manner over load control is is
again you know I my background is mostly
in telecoms and one of the biggest
requirements there is that the principle
of graceful degradation that if you get
a traffic spike you don't suddenly die
and then restart barak but you you
continue running but maybe with a
certain level of service degradation so
if you own if your normal traffic levels
are X and suddenly you're getting 5x
even if you can handle point 8x of your
traffic and still reject the rest of it
that is still better than coming to a
complete halt and causing an outage so
there's a built-in Oh Lord control
module within within our line which I
would suggest take it as a take it as an
example implementation it's not really
suitable for production use for a number
of reasons firstly it's implemented as a
gen server so basically the way it works
is you you sending it a message saying
can I handle this
ironic thing about it is if you've got a
traffic spike you might end up actually
overloading this particular server if
the one which is your gatekeeper so yeah
I mean it's not really suitable and
there's a lot of extra message passing
as well so every time you want to make a
decision on should I accept this next
request or not you're you're passing two
extra messages and again like I said
when you're dealing with very high Lord
that's that's amplified and it again
caters for global load within within
their landlord it's not interface
specifically if your landlord has a
three or four different interfaces the
cost of handling a message from a
certain interface might be a lot more
than the other so or you know some
interfaces you can handle thousands of
requests a second but some of them are
more expensive you can only handle
hundreds of them so you want to set
different limits for each one of them so
there's a but it's not actually that
hard to to roll your own overload
control and that to interface specific
that's really a very simple example of
how one might write an overload control
module it's named NPS which is num
number per second how many you willing
to handle and all that it does is
basically every time you ask it can I
handle this request it's sort of
checking how many requests have I
be handled in this particular second and
it gives you a verdict of either allowed
this or deny it
and you can have multiple instances of
it and you know the way
roughly works there's a test being shown
there which is if you set it to say you
know I'll handle ten per second and you
may can handle fifteen you can see the
first ten it will give a verdict of
allow but the rest of it is it will deny
and it's it's quite cheap to implement
there's no message passing in world is
just well as if you don't count how the
time is being generated and if if
figuring out the current time is not an
expensive operation native RPC again
native RPC is a very the RPC in Lang is
a big feature of it it's it works but
again it has its problems when you
wanted to use it at at high load this is
a description of how well how two nodes
connect to each other the first time
they want to they want to talk to each
other so there's something called EPMD
which runs alongside every airline north
there's one instance of ATM be on every
host regardless of the number of airline
nodes you have EPMD stands for line port
mapping mapping daemon and basically a
node the a at host two when it comes up
so is there a pointer yep
now it's fine on this point so if it
host two when it comes up it connects to
its local local host on port four three
six nine and and says yeah I'm listening
for connection from other line nodes on
port say you know random port thanks
right and if and if another node called
a add host one wants to connect the
first thing it does is it figures out
that host 1 is what it needs to talk to
again it connects to the well-known port
4 3 6 9 and then ask the question well
where is no de any BMD says well in node
a is listening on 2200 it connects
checks cookie and connect size and that
and then you can have our PC working and
native RPC works a bit like that sorry
can people at the back see it sorry I
didn't realize the room was that big so
basically what happens is when you when
you call and you do an RPC call about
two processes spawned which done then
does a gem serve a call on a well-known
process on the remote node which spawns
a worker process it takes care of IO for
example if your code being invoked here
is printing to the console the native
RPC actually takes care of sending that
back to you as well which is which is
kind of one this is not what you want
really when you are executing a system
on a high load so I mean I guess the
point I'm trying to make with this slide
is that native RPC while it works well
actually they does a lot of wasteful
processing and the biggest and the
bigger problem actually is that is the
well-known process here called Rex every
every RPC call coming into this node
ends up as a message on this so you can
imagine what happens if a number of
client nodes all want to take to the
talk to the same server Nord
and we've had this in production a few
times that AE
this process used to build up a huge
message queue this is a problem because
not only not only is your articles going
over it actually if you're using amnesia
in replicated Mord that is you know any
amnesia or presence all are also backing
up here in the same way the way our
landlords detect whether they are sort
of working well or not is again a
heartbeat message over the same
interface again they start backing up
and suddenly your whole node becomes
unresponsive because this process is
getting is becoming a bottleneck and
it's not just the incoming messages
Raley's you see what it does yeah it
does a spawn monitor of a worker process
and this one not only does it get a
reply back but when this process dies it
gets a down message as well so in effect
for every incoming message here there
are two messages going back so so every
RPC call
results in three messages being handled
by this Rex process so yeah yeah as I
mentioned earlier there's the head of
line blocking problem if if messages are
getting backed up and and say one of
them is actually one of your RPC calls
has generated a massive response all of
that has to come down that single socket
and your rest of your operations are
getting backed up behind that so we
implement our own RPC mechanism which
which was quite simple because we didn't
want some of the wasteful stuff which is
in the native implementation which is it
had its it would set up its own socket
connections I mean I've just put Rex
over here just to illustrate how you
could modify the native implementation
to make it easier but we had our own
it was a it was our own application and
all that it did was it basically it
would look up a socket a connected
socket for that node
I remember the require a request ID and
the calling processes ID into that and
directly send a message over the socket
the the receiving and all that it does
is it takes one message at a time spawns
the process and lets that spawn process
do all the work and when it's finished
it sends the result back directly on the
socket so actually this particular
process is not really doing much and
actually this this implementation gave
us a lot of stability in terms of angles
so we left we left the native RPC
connections just for amnesia to do its
replication so it improved the stability
of our Lord and the the advantages are
that suddenly now that you have control
of that socket you can you can introduce
overload control into it so that you
know the server side has control and how
much it's handling and can rejected you
can use a different transport protocol
instead of using TCP you can use SCTP if
you want and you can have a clean load
balancing you could use multiple
connections and we had the side effect
of using multiple connections is that
you can avoid the head-of-line blocking
problem because if you know it's an
expensive request or the response is
going to be large you can actually send
it down a different connection this is
another thing that are along that
generally at least in my experience what
I found is that it's much harder to
implement long-lived stateful processes
because you have to there's a lot more
to think about this state to manage and
if the state of that process becomes
especially large
there are garbage collection issues well
you know garbage collection is per
process in Erlang but still if the state
is big enough the pauses can become
significant that
and again if garbage collection takes
long you to get a higher reduction
penalty which means you're contributing
to again message queue build-up and
Messick you build up again is is the
actually seal of overlying it it doesn't
recover well from it and generally if
you've got a long little process you
want it supervised so and again you are
putting more effort to make sure that
you get the correct supervision strategy
so I mean it's hard to avoid long-lived
stateful processes but generally the
rule of thumb is if you can design it in
a way that actually most of your work is
being done short-lived processes and
your long lived processes are simple
then the chances are you can get it
right easily than long live complex
stateful processes so the next thing is
a bit about amnesia there's there's
recently a long thread between the
mailing list about you know is Emily
suitable for the job and stuff like that
so well I'm Nisha is a built-in key
value store within Erlang it supports
acid transactions and it supports
real-time replication of tables it it
allows you to have three types of tables
in RAM copies as the name suggests the
table only exists in RAM this copies is
basically the table is hell okay a copy
of the table is held in memory but it's
also persistent and disk only copies it
only exists on disk and the way amnesia
manages tables for each table there's a
file on disk with a dot BCB extension
and that's where all the data for it is
stored all modifications to persistent
table
are written to the file called latest
dot log that's like a right ahead log
basically and and occasionally the
contents of the latest log are are
written to table specific log files if
you like and these when I say
occasionally again that's controlled by
configuration parameters but that's the
idea and and occasionally when the DCL
files become about 25% of the actual
size of the table then the entire
contents of memory of that table are
dumped to disk
that should sort of give you an idea of
where the problems occur with amnesia
the way does table management can cause
amnesia to be overloaded if you've got a
write heavy application because every
time the number of changes on a table is
about 25% of the size of the table it
takes the entire copy of Watson memory
and dumps it to disk and if your table
is very large it can take time the
bigger problem though is that amnesia
doesn't really deal very well with net
splits it's when a net split occurs
between a replicated amnesia cluster
basically the only way to resolve it is
to restart one of the no or one or some
of the nodes and they copy
they copy the latest copy of all the
data in a table from another node which
is believed to be which is believed to
have the latest data now that that means
data loss basically every time you have
a partitioned network within amnesia and
you've got writes going to both nodes
the only way to recover from that is
restarting one of them and you lose the
writes done to one of the nodes there is
an open source contribution from a
figure which can do the unsplit for you
but again it doesn't it doesn't really
work well for large tables just to give
you an example of a use case where I am
work really well so so whatever I've
said is not to say that I'm Malaysia is
not fit for purpose but it's actually
there are some there are certain use
cases where it really shines and it's
hard to beat it but there are some use
cases you really shouldn't be using it
for so for instance with an EEV had this
customer database of bout a eighty
million records but the main
characteristics of it was that it was
read heavy we would probably have about
half a million writes to that database
in a day but it would probably handle
something in the region of half a
billion or three-quarters of a billion
reads a day so it's there's a huge skew
between the two and we had a single
provision ignored as well so that all
the rights used to get used to come into
a single DB node and that was controlled
manually by the operations team which
means that if we have a higher net split
between the two database nodes they knew
exactly which one to restart and the
bottom are all the climbed nodes there
and what we did was well that's your
shared amnesia cluster what we did was
we put what are called database
interface nodes and these exposed
various protocols like LDAP and RPC and
HTTP but between the DB nodes and VBR
nor we had our own RPC application which
I talked about earlier so what they did
was well they used to load balanced
l'ordre read traffic across the - but if
if one of them had to be restarted like
say to resolve the partition network
none of the clients would know anything
about it because the RPC application we
had would immediately detect the loss
here and just redirect all the traffic
to this and because M Nisha is really
really good at serving read traffic
none of the client systems ever really
noticed that we had a problem in
replication there so in in read heavy
scenarios and means
really really shines well regardless of
the size of the tables enrolled well I
say that but I don't know if I'd put a
billion entries into it where it didn't
work I mean this this went quite badly
wrong so this was basically our system
which controlled data policy on the
network this is actually very simplified
so these are our core network elements
like our packet gateways in our in the
mobile network and there's a bunch of
protocols which come in here you know
and this is basically every time you
fire up your phone and you're using it
you're consuming data these are sending
signals about how much you're using and
these would basically decode the
protocols coming in you know normalize
the data into an internal format pass it
on to the core processing nodes which
again word interface with a bunch of
external systems again talking various
protocols and ultimately well that's
that storing customer usage data saying
you know how many how many bytes have
you used you know how much what's your
allowance per month or what's your
current allowance if you bought a beta
pass for example so this architecture
was really good I mean it's it started
of really well until we started getting
traffic spikes now what happens is if
say a base station or something went out
you know lost power or whatever suddenly
a whole bunch of customers would migrate
to another base station and you get a
signalling spike now depending on how
far up you move in the radio network if
you go to if you go to packet gateway
which restarts you potentially talking
about half a million customers moving on
to the next packet gateway and all of
them trying to reestablish their data
sessions at the same time
and that causes a huge spike which is
fine I mean we had overload control
which I described earlier on all these
edge nodes we we knew how many we were
letting in but still at a certain point
what would happen was amnesia would
start getting overloaded over here and
that would cause that would take a long
time to resolve because this was being
used in a right heavy and read heavy
fashion and that is a use case with
amnesia doesn't handle well at all and I
wouldn't do it again I mean around the
time I left EE we were trying to swap
this out for react that I think that was
the right thing to do the reports are
here are that actually react solve that
problem so yeah don't use amnesia in use
cases where it's right heavy so yeah I
mean replicated read heavies okay
like I'd mentioned or standalone write
heavies okay - to a certain extent
because amnesia on a single Nord fares
pretty well you can make it work
but if you combine the two it's just
asking for trouble hot code loading
again a very useful feature of our line
but there are a few things to take into
consideration I mean a if you're running
a couple of nodes or you know a handful
of nodes in production and you wanna
upgrade the code on it you know you can
do it by hand and you can sort of
manually manage the thing but there are
a few things to think about when you're
using hot code loading that is the
process state management when you load a
new version of the code how do you have
grade state from old to new and if
something goes wrong and you want to
roll back how do you roll back the state
as well and the the bigger thing is I
guess
traceability as in you know if you keep
patching individual nodes and nodes with
individual beam files you need a
mechanism so that for any given node you
can say well this beam file came from
this point in the source tree if you if
you can't do that then well
it's asking for trouble like I said
beyond a certain amount I mean a handful
of nodes you can probably manage but if
you're talking tens or hundreds of nodes
if you don't have the ability to trace
back a beam fine back to your source
repository then you get in trouble so
the advice is don't use hot code loading
to patch your system unless you've got
automated installation and and rollback
scripts because even for tens of nodes
rolling back one node at a time is
actually asking but is it asking for
trouble as well because what happens is
half as you are rolling back half your
system is in in the upgraded state and
half in the rollback state and your
state starts your shared state become
really messed up and TCP sockets that
wises don't use the active true mode on
sockets in production I mean this should
be fairly I think it should be obvious
to most people who use our line for
anything serious but it's I'm just
putting down a record active ones is the
safest one where basically you're asking
the socket every time give me a message
so you have full control over how many
messages you're taking off the socket
but I recently discovered actually that
using active ones we saw performance
degradation and this is when moving from
Red Hat Enterprise Linux five to six I
was quite surprised by that and actually
digging into it I well I suppose in
retrospect it's obvious but every time
you read from a socket there's a spin
lock being acquired in the kernel and
that was that was killing performance
for us
I still quite haven't got the bottom of
why it wasn't a problem in a previous
version of the operating system but I'm
still working on that but we worked on
that by using active end which is
basically you can say well instead of
saying to the socket give me one message
at a time you can say well actually give
me a hundred and so the worst case what
again have is from your socket you're
getting a hundred messages dumped into
your process message queue well that's
still manageable and once you process
through that hundred you can then say
right give me the next patch 100 and
that actually is giving us the highest
performance at the moment and yeah I
mean with everything it's important to
think about the overall system design as
well because what I've found is when
you're prototyping it's fine to bung
everything into a single node and say
you know I'll just get it working first
you don't really pay much attention to
how you are you're splitting
responsibilities between nodes but what
I found works best is that if you make
each other like node as independent as
possible and that also means not doing
an easier application in anger
then it works really well each node
should really be an independent unit of
computation so that you can actually
scale scale out so here's an example of
system which again is badly designed
from an airline point of view so
basically you've got data feeds coming
in and we had a 18 of these nodes and
they were all exchanging state with each
other and if node one went down data
feeds all migrated onto say node two and
the ready phase were not small that's
tens of thousands of matches a second I
I like to call it distributed monolithic
system even though we had 18 nodes
if one of them died it caused a domino
effect the next one would get overloaded
that would die and eventually the whole
thing would die which is which sort of
defeats the whole purpose of using
airline so a better way is to give each
northern independent unit of work and
then have a sort of a distributor or
load balancer so that you're handing our
partials of work and each one does it
really well and behaves predictably so
the recap prefer lots of short-lived
stateless processes over a few
long-lived ones
beware of message queue build up in mass
airline processes
beware of native RPC limitation again
like I said if you're dealing only with
the hundreds of or even a few thousands
of messages I can you'll be fine with
native RTC but if you're running into
tens of thousands of those it will not
work amnesia is awesome but only for
certain use cases activin were expressed
for TCP sockets and or overload control
is not optional if you're running at
scale you know you really need to do
overload control at the edges so yeah
and a brief word about open source
efforts at bet365 we are working on
better or DBC support we have a lot of
SQL databases in our in our business and
we'd like to be able to access them from
our line but there or APC support is
kind of is not great at the moment so
we'll be working on that and we will
open source it at some point we're
working on a proper soap implementation
this open the soap support in Erlang is
non-existent officially and if whatever
open source stuff you can get is very
hawkish and you need to know the right
spells to make it work and they're also
assisting in Ericsson in developing a
package manager for line yeah that's the
end of it any questions
yeah yeah I mean yeah so the question is
did you ever make use of OTP well yes I
mean OTP is being used extensively
purely because of how much it simplifies
the writing your code made supervisors
and Jen servers are large or what get
used most
but yeah without OTP I think it would be
quite hard to build complex systems in
our line
nope okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>