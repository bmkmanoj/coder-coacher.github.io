<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Mark Callaghan - Choosing between Efficiency and Performance with RocksDB | Coder Coacher - Coaching Coders</title><meta content="Mark Callaghan - Choosing between Efficiency and Performance with RocksDB - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Mark Callaghan - Choosing between Efficiency and Performance with RocksDB</b></h2><h5 class="post__date">2016-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tgzkgZVXKB4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I work on transaction processing at
Facebook
my focus is storage efficiency because
we have a lot of data the key problem
for us to solve is to be efficient with
storage I am a software developer I used
to spend too much time in production not
so much anymore
spending time in production was a great
education so for anyone working on
database internals I've been doing web
scale databases mostly MySQL with some
MongoDB and rocks DB for 11 years
between Google and now Facebook before
that I spent about 10 years between
Informix and Oracle and never got near
production and really once I got near
production I realized how much I was
missing so I encourage anyone if who's
thinking about a database internals
career to spend some time on production
here I'm going to speak about the
relationship between the trade-offs
between performance and efficiency
mostly with storage in the context of
rock's TB and nodb
so I'll talk a bit I might be doing a
little bit of marketing for some of
these projects
ROCs TB is an embedded key value storage
engine it was forked from leveldb many
years ago since then it's diverged
significantly the focus with rocks TB is
server quality workloads the focus with
level DB has been really to favor more
maintainable code simpler code base so
we have a more complex code as we are
pursuing more performance my rocks is
the rocks TB storage engine for MySQL
we've been working on this for several
years the storage engine ape API in
MySQL is extremely difficult to
implement many projects have attempted
to build a new engine
my SQL and failed we are running in
production in one data center on more
than 5% of the tier which is a lot of
machines already so it's in production
for the user database tier which is data
we don't corrupt and data we don't lose
so at this point that's a strong
statement it's it's going to replace in
ODB so we'll continue continue to use
MySQL I'm excited I I want other people
to be able to use my rocks so it's going
to show up in percona server and maria
DB server which are proper distributions
with expert support so other people can
use it Mongo rocks is a rocks TB storage
engine from MongoDB the engine API and
MongoDB is much easier to implement this
one from idea to running in production
in less than one year and it is in
percona server today so rocks TB when I
first got started there was a lot of
focus on performance but once we began
considering using it from MySQL really
it changed to the focus on efficient
performance or even just efficiency the
user database tier is tens of petabytes
at Facebook so that's a lot of storage
that's with compressed I know debate so
we've already compressed it by a factor
of two the goal is to compress this by
another factor of two and we are
reasonably certain that we can do that
from all of the evaluation we've done so
far this pursuit for efficiency not just
the amount of storage we have but for
transaction processing it's very hard to
find anyone willing to use disk arrays
today if you give them a choice they
will choose SSD so
with rocks tbh we are pursuing good read
efficiency better write efficiency and
then best space efficiency so best space
efficiency means we use less space when
you're using SSD using less s SD is a
very big deal better write efficiency
means write endurance for the SSD is
much less of an issue with my rocks than
it would be with n o debate the log
structured merge tree algorithm that
rocks DB uses was invented for disk
arrays the idea was that using less IO
capacity for writes saves more i/o
capacity for reads that's not the
selling point for an LS M with SSD it
still turns out to be a great fit for
for NAND flash because the write
efficiency means endurance is less of an
issue and the space efficiency means
you're going to use less so we got lucky
the algorithm turned out to be a good
fit for SSD one of the things I do is I
blog
aloft and one of the things I blog about
is bench marketing and by bench
marketing I mean the use of unexplained
performance results to claim that one
database product is better than another
it happens a lot I do it a little bit
but it's and really the key is the
results have to be taken out of context
show a graph show a few pictures and so
sometimes I don't in response to this
sometimes I don't even publish graphs I
want people to slow down when they read
what I write because I don't want to
fall into that here says bench it's a
silly synthetic benchmark it has some
use it's widely used in the my sequel
community this benchmark has about 2
gigabytes of data so it's a very small
workload I didn't run it until a few
months ago and I was happy to see that
my rocks in blue is actually competitive
with nodb and my sequel 5.6 so I was
surprised then this is the
number of threads going from 1 to 128 so
even with concurrency my rocks held up
but I don't want to explain too much
because I'm just marketing here the
problem I see is that we get stuck in
this rut about something is faster so
it's better really throughput average
throughput or peak throughput is used to
rank database systems when we really
care about throughput quality of service
and efficiency an efficiency by that I
mean how much hardware is used per query
or per transaction and so something that
I'm trying to practice and I hope other
people begin doing this as well is you
know stop stop doing these graphs and
start at least to complement the graphs
publishing throughput the first column
here this is TPCC MySQL thousand
warehouses the database is larger than
Ram I had about 50 gigabytes of RAM on
the server so the workload is somewhat
IO bound throughput the TPM see my rocks
is a little bit ahead of nodb
I don't think that's five percent more
throughput I don't think is a big deal
the really big deal is if we look at the
hardware efficiency metrics so here I
use IO stat and vmstat the are KBT is
kilobytes read per transaction and their
nodb is doing a little bit more but
they're not that different
the next column wkbt kilobytes written
per transaction nodb is writing
significantly more data back to storage
per transaction it's less efficient CPU
is a little bit worse for my rocks the
CPU time divided by the throughput it's
a little bit higher from my rocks that's
somewhat expected with an LS M the other
really big deal is the database size at
the end of the test nodb was was almost
three times larger so the write
efficiency in ODB is almost three times
worse more than three times horse space
efficiency in ODB is almost three times
worse
and so these are the metrics I care
about in addition the throughput
continuing with my rant just performance
or peak performance is overrated I think
a lot of us what we do in production is
we target good enough performance we are
not trying to run our databases at peak
and we are not necessarily seeking the
solution that gives us peak throughput
we have response time goals and going
let's say from 100 microsecond response
time to 10 micro second response time
might have no benefit to the business so
once we hit the performance goals then
we want to optimize for performance e so
can we use less hardware to serve this
workload or can we continue to use the
hardware that we installed this year for
another few years as the user demand is
growing from this focus on efficiency we
had one team in pursuit of additional
database compression two teams one team
was working on nodb we got compressed
nodb deployed in production we did a lot
of engineering work to make nodb
compression performant for readwrite
workloads
that code was contributed back upstream
we thought it wasn't clear that we could
have additional improvements on nodb
so we were looking at alternative
engines we had a team that began working
on rocks dB
we wanted these teams did not get stuck
on faster equals better and we wanted
these two teams to be able to discuss
with each other the differences of yes I
got better performance but at this cost
of the worse or better efficiency so we
wanted to make sure the teams were
having the full discussion about
performance efficiency and quality of
service how did that came what was
described as the rum conjecture in a
paper that was published earlier this
year I am Cole last author on this paper
came out of researchers from Harvard the
point is that one algorithm or one
storage engine
cannot be optimal for all of read/write
and space amplification and I'll go into
more details on the slides that follow
sometimes I use efficiency and
amplification as synonyms for each other
when talking about this the paper use
drum R er um for read update and memory
internally I've been using for a longer
time RW s read/write in space so in the
for the rest of the talk I will use our
w and s but if you look at the paper
it's using R um so by optimal you know
this is an area a topic I think that
calls for more research and more
formalism I'm not in academia I'm
dealing with production so I
occasionally try to be formal but not
really optimal here is for any useful
database algorithm there's another
database algorithm that has that's
better at readwrite or space efficiency
or amplification and I limit myself to
the useful database algorithms I don't
care about the I mean we can invent a
brain-dead algorithm that's worse that
is worse at everything by amplification
I'm usually talking about a ratio so for
read and write amplification it's the
ratio of physical work either to storage
or CPU per logical request and I'll
define in another slide what I mean by
logical request but it's really a ratio
and we use the term amplification
because the an algorithm can one
algorithm compared to another might
actually do more physical work per
logical request space amplification the
site the ratio is the size of the
database files over the size of the data
now the challenge with this is it can be
tricky to compute the size of the data
and the database I mean you might have
to do a select over all the rows and and
add up the column sizes that you see
from the return data you also might want
to consider this
include secondary indexes as logical
data that you put in your database so
this can actually be difficult to
compute for running systems work so I
said amplification was physical work for
logical requests the work depends on
whether we're talking about storage or
CPU so for storage
how many random operations this is much
more of a problem for disk and for SSD
how many disk reads or seeks per second
or per query am i doing how many bytes
are being read from storage how many
bytes are being written to storage for
CPU we can have the notion of work at a
higher level how many pages am i
compressing or decompressing how many
memory or cache operations am i doing if
I have a hash structure how many
searches of the hash table am i doing
per quarry if I have a tree a tree
structure how many key comparisons am i
doing the challenge with these higher
level metrics is that it it can be
difficult to extract them from the
database engine and associate them with
individual query so the the one that I
end up using most of the time it's just
CPU utilization or CPU seconds how much
time is spent per query on the CPU so
amplification is work per operation what
do I mean by operation it would be great
if everything could be defined in terms
of basic operations point read range
read put and delete and actually the
basic operations here are the API that
rocks DB provides I think of this as a
universal database API someone could rip
out the implementation we have four
rocks DB in theory re-implement that for
something other than a log structure
merge tree and they could plug that in
with my rocks and have a faster way of
building a new my sequel storage engine
the challenge with using basic
operations is they're hard to count like
for the workloads I run I use
we get from the benchmark utility number
of statements per second and number of
transactions per second so it's more
common for me to use complex operations
when measuring when talking about
amplification so how do I do this in
practice well in practice knowing that
rocks deep my rocks does one disk read
per query in isolation is kind of boring
I can't use that information what's that
well if I'm doing long-term performance
regression testing I can use it in
practice what I'm doing is comparing my
rocks to nodb
with MongoDB I compare wire tiger and
Mongo rocks and so I just look what
ratios are interesting and the ratios
are interesting when they're far from
from one and really it's in two cases
here the kilobytes written per
transaction the ratio is three point
seven one meaning nodb is doing almost
for writing four times as much two
storage per transaction and then the
ratio of for database sizes nodb is
almost three times larger than my rocks
so what I've done is I've used the
complex operations transactions per
second or transactions per minute for
the notion of logical request for
hardware utilization I just use what I
can get from i/o stat and vmstat and I
divide one rate a
one rate by another rate to get physical
work for logical requests so stepping
back trying to talk at a higher level
about a b-tree and a log structured
merge tree and readwrite and space
efficiency using nodb as my example for
b tree for talking about read efficiency
if I have if I have n rows in the
database I would do about log n key
comparisons on a search for writes
efficiency or write amplification the
worst case happens when the page that
gets written back to storage only has
modified row in that case the write
amplification is the size of the page
over the size of the row if I have a 16
kilobyte page and 256 byte row that's
now a factor of 64 the write
amplification if I'm using the nodb
double write buffer to get torn page
protection so in case there's a partial
page write on a system crash if I want
to protect against that that doubles the
write amplification so the key here is
that the b-tree we are transferring
pages back to storage and the page might
be mostly clean when we're writing it
back which is a source of amplification
space amplification a b-tree subject to
random updates will become about
two-thirds leaf nodes will become about
two-thirds full over time or between 1/2
and 2/3 so assuming the leaf pages are
two-thirds full the database space
amplification is 1.5 X meaning it's
using 1.5 X more space than optimal
where you need to fully defragment it so
just at a high level how I think about
the B tree these are the things I keep
in mind whether it's an update in place
B tree like in ODB for wire tiger which
is a kind of one variation of a copy on
write B tree or LM dB this description
actually has to get modified but I don't
have time to go into that so for a
leveled LSM so Y leveled LSM I mean
leveled compaction which you get with
rocks DB and Cassandra the read you can
think of there as being there's a
sequence of trees and each by default
will be 1/10 smaller than the previous
tree so the read cost would be log n for
the largest tree log in over ten for the
next larger tree log in over 100 log in
over a thousand it the hand-wavy answer
is you're going to worst-case do about
twice the number of key comparisons for
an LS M search then you would with a be
tree now the bloom filter
it more difficult to figure out what's
actually going to happen in practice but
this is the cause of using more CPU with
rocks TV right efficiency so the beat
rewrites pages back LSM writes rows
however it doesn't write them once
they're going to get written multiple
times there is a significant benefit
generally with an LS m and write
amplification but really the key thing I
keep in mind is in one case I'm writing
pages in the other case I'm writing rows
write amplification for leveled
compaction is worse than size tiered
compaction which is on the next slide
space amplification you can't beat what
leveled compaction provides assuming
each level is ten percent larger than
the level above it about ninety percent
of the data is in the max level of the
tree if you have a an update only
workload so the database is not growing
you could assume that all of the data
above the max level is redundant or
garbage so about 90 percent is live data
and 10 percent is garbage space
amplification is about 1.1 X definitely
better than what we get from a b-tree
sized tiered LSM we get we have this
with rocks DB we don't use it much
Cassandra and HBase offer this read
amplification is more than a level del
SM it's a little bit harder to predict
it's I think it's going to be highly
dependent on the configuration again you
can benefit from a bloom filter for
point reads write amplification still
rewriting previously written rows or
writing once and then rewriting it tends
to be less than level del SM and
definitely much better than a b-tree
space amplification is harder to reason
about I'm just going to say it's more
than what we see with level del SM for
my workload I care about space
efficiency so I want level leveled
compaction rather than size tiered the
summary is interesting depending on
which one you want read writers face
efficiency which is most important to
you you have a different choice if you
want read efficiency you're probably
going to choose a b-tree at least based
on what I predicted if you want space
efficiency you would choose leveled
compaction within LSM and if you want
write efficiency you would choose size
tiered the challenge here is that this
frequently is a per server choice you
might have many indexes in your database
workload and the indexes might each
require a different choice and that's
just not supported today I think we are
moving towards enabling multiple
behaviors in the future so I did a lot
of hand waving the problem is the hand
waving doesn't always match what I
measure in production and I do I spend a
lot of time measuring and explaining
performance it's very painful or
expensive takes a lot of time but what I
say is usually backed up by measurement
the challenge here is that when you talk
about complexity theory or you try to
model how what iopa from performance you
will see the impact of the cache is
extremely hard to predict that with a
b-tree we want to have the indexing
cache and with the LSM with leveled
compaction we want to have everything
above the max level in cache if you set
of satisfy those constraints you can you
can understand the worst case the other
challenge is access distribution the LSM
benefits greatly from skewed
distributions most production workloads
have skill within LSM if you have a
small right working set compaction
doesn't necessarily move all the way
down the trait here is one example link
bench is the benchmark based on the
production database workload at Facebook
a graduate student for the summer turned
into like a six month long project for
him implemented did
the measurements published the paper in
sigmod implemented the benchmark here
I'm providing throughput efficiency
quality of service metrics TPS is
transactions per second comparing my
rocks with compression versus nodb with
and without compression so here for
throughput my Rox wins for reads per
transaction storage reads per
transaction they're all similar the big
difference is kilobytes written to
storage per transaction nodb is almost
15 to 20 times worse than my rocks
that's really hard to believe and that
was from iOS tab if you look at the
flash device and include the extra
writes from flash grooming or garbage
collection the ratio actually increased
to like 20 or 25 and again it's really
hard to believe that nodb is doing that
many more writes it's hard to understand
CPU was about the same for all three
database size my rocks was almost 2x
about 2x better than compression for
nodb and almost 4 for x better than
uncompressed nodb and the last the most
popular update transaction response time
in milliseconds at the 99th percentile
my rocks was was better so the next
question is why why did this happen for
space efficiency one problem for the B
tree is fragmentation
there's wasted space for pages on disk
over time
another problem if I don't know who's
familiar with compressed nodb is it has
a fixed page size so if we compress a 16
kilobyte page down to 5 kilobytes and
we're still using 8 kilobyte pages on
disk so that compressing down to 5 K
doesn't do us any better than
compressing down to 8 K nodb has 13
bytes of metadata that cannot be
compressed per row roll back pointer and
a transaction ID and then nodb doesn't
do prefix encoding for the 4 keys my
rocks does rocks DB does prefix encoding
my rocks only rocks tbh only writes out
the compressed output so there's no
fixed page size there's a target page
size before compression and then we
compress and just write out the number
of bytes that compression output gave us
so for right deficiency the database is
larger with nodb there's more to write
there's another important thing here we
run databases so that the working set
doesn't fit in Ram if the working set
fits in RAM and you have fast storage
you're spending too much money on Ram if
you have fast storage you should make
the database use it so with a node eBay
potentially pages or dirty pages are
written back earlier than they would if
the working set fit in cache and so they
have fewer dirty rows per page and again
page over row size is the worst case for
write amplification and then finally the
double write buffer so nodb writes every
dirty page twice one experiment I did
was to look at the impact of page size
on write amplification for nodb the
default at the bottom is 16 K and that's
where nodb compares worst in the number
of kilobytes written to storage per
transaction 19.7 versus 1.25 for my
rocks when I shrank reduced in ODB page
size to 8k almost cut the right overhead
in half and then to 4 K there was an
additional reduction so this is at least
one of my speculations on why nodb is
much less write efficient it's a
function of the page size the problem is
I can't use 4 K pages in production nodb
needs rows need to fit and app - lob
columns rows have to fit in half a page
so if the page is 4 K the max row size
is 2 K and that's too small for me read
efficiency so I said before that if you
look at an LS m the file structures
you're going to say that oh it's going
to be less
in a b-tree well the problem is there's
a lot more going on with performance and
in some of my measurements it's not
always the case that the bee tree beats
the LSM the bee tree advantages fewer
key comparisons and potentially less io4
range queries the LSM advantage at least
with my rocks and rocks DB is we have a
prefix key encoding so we fit more data
in cache because even for uncompressed
pages the prefix key encoding is is
still done
we don't have page fragmentation so with
a b-tree in the buffer cache you
potentially have pages that are 2/3 - 1
a full meaning you're wasting between
1/3 and 1/2 of the memory in your buffer
pool because of bee tree fragmentation
we don't suffer from that within I deal
with my rocks another point is that
being more efficient on writes saves
more i/o capacity for reads which was
actually the initial argument for the
LSM with disk arrays secondary index
maintenance is read free with the B tree
it's read-modify-write you have to read
the leaf page modify it put in the
buffer pool eventually it gets written
written back with rocks TV and my rocks
we don't have to read the old version of
the index pages and then finally we have
a bloom filter just one experiment that
I did I ran the same workload from my
rocks on a disk array that could do one
or two thousand ions slow SSD that could
do about 10,000 I ops and then fast SSD
that could do more than 100,000 I ops
and I ran it from my rocks in nodb the
server's all had the same amount of
memory similar CPUs the interesting
thing is a performance ratio between my
rocks and nodb the ratio gets better for
my favors my rocks as you move from
faster storage to slower storage so as
storage becomes more of a bottleneck the
more storage efficient solution
it looks better in comparison
so my whole point is that you know we
are trading between readwrite and space
efficiency and depending on the
different choices we make either between
algorithms or even when we're
configuring a database
the first is indexes indexes gives you
read efficiency at the cost of write
efficiency meaning index maintenance and
space efficiency you have to store the
index on disk compression makes reads
less efficient in some ways because if
you're reading from storage you have
decompression latency
it improves space efficiency right the
trade-off a common trade-off between
right and space efficiency is when you
format a filesystem for a storage device
how much over provisioning do you do and
if you use half of the storage device so
your percent full is 50 space
amplification would be to you're using
twice as much space as needed because of
the over-provisioning but you know if
you think about worst-case cost of
garbage collection with the firmware
that's reclaiming space to make it on
the SSD device the write amplification
the next formula would be to so we are
already to the right on the graph if we
want to minimize write amplification
we're wasting space if we end up using
let's say about 90% of the storage
device then worst-case write
amplification is going to increase to
about 10 so just setting up formatting a
filesystem honest and SSD you have a
choice and the vendor is making a choice
for you in some cases between write and
space amplification so what what comes
next I think we have well for one I'd
like to see a product that gave me a
b-tree and an LS M in one instance in
theory wire tarik why
Tiger can do this they have an LS M they
have a b-tree they just have not enabled
that it's not clear to me that we're
going to build a b-tree as part of
rock's TB but I'd like the best the best
of both worlds the other thing is tuning
rocks DB is extremely difficult it
requires a lot of expertise so I'd like
to move to the model where we have the
DBA setting high-level goals for
read/write than space efficiency and
then putting the smarts in the algorithm
to tune itself in pursuit of those goals
so adaptive algorithms is something that
I hope we see in the context of rocks DB
and other database engines the final
thing is more open source so my rocks is
coming to Moorea DB server and pro kona
server it's available today but it's in
the Facebook my sequel branch and that's
nothing I would suggest anyone try to
use except for an evaluation Mongo rocks
is in percona server so if you want to
use that it's there we have a lot more
innovation coming to rocks TB hopefully
we'll have what we have optimizations
that we think will work well for tiered
storage for example if you have 3d 3d
crosspoint and NAND flash device
combined but we're really waiting for
these devices to show up that we can
evaluate them so I blog a lot my blogs
on the right-hand side small datum I am
a small data engineer in contrast to the
more popular Big Data I tweet a bit I
market the work in my team rocks DB has
a pretty nice landing page now and the
landing page for my rocks is not very
good yet but really the goal is to get
people to use Percona server and marija
DB server if they want my rocks so we
have active mailing lists we have some
early adopters who are most of them are
kind enough to work with us because it's
not easy to adopt my rocks at this point
thank you and I'm happy to hear
questions questions how does the hab
seat perhaps the performers compare if
you compared it with the white tiger and
it sent they we sponsored the LSM a long
time ago before before they were
acquired right so it's funny because
we've sponsored it when we were working
on rocks t bay they don't they don't
suggest using it with MongoDB today
because they're busy I talked to Keith
and Michael a lot
they're busy fixing performance bugs in
the B tree so it's just their LSM it's
sized heerd so we favor level 2
compaction which would be a problem for
me I just think they need a few more
years before they can focus on it
alright
with your right through puts numbers or
IO through put numbers does that include
the rights to the redo log or writer log
yes it includes the most of my tests now
have replication enabled so I Laura the
replication logs enable TOI include the
redo log for the database and the
replication log except I almost always
run with fsync uncommit disabled Epson
con commit is another source of right
amplification so I'm not counting that
so I get some benefit
cool could you explain in a bit more
detail the difference between size
teared and leveled compaction so and I
debated having I didn't want in the past
I've tried to describe leveled like in
my slides and I left them out on purpose
although then the challenge is if you
don't have experience if the audience
doesn't hasn't used it so well sized
here it is what HBase does and so you
have an in-memory table and when that
gets full you flush it create a new file
and think of there being a list of files
new files are arriving at one end of the
list from when the mem table is full and
then something looks down at this list
and picks adjacent files from the list
two or more and merges them and replaces
the files it consumed with a larger file
that contains mostly the same data it
can drop some data when it's doing the
merge so you just have this list that's
slowly getting new files and then being
shrunk into smallest smaller number of
files but it's what you if you have an
update only workload you have this huge
file at the end of the list which has
all of your data and eventually that
huge file has to be combined with most
or all of the other files and it's
called major compaction and so the
problem with this is when you're doing
that temporarily you're doubling the
amount of disk space you use so you're
essentially creating a copy of that huge
file so if you have a terabyte sized
database you're going to have a terabyte
sized file and when that file eventually
is compacted might happen once a
temporarily you're going to have to
terabyte sized files so the this
doubling in size and the fact that
compaction tends to be compaction you
generally don't have multiple threats
consuming of one file for compaction so
how long does it take one thread to
process one terabyte a file is a problem
level 2 compaction there's more
structure you there's a notion of levels
so mem table puts files in the level 0
every file in level 0 is a sorted sorted
run but each file on level 0 can overlap
with every other file in level 0 you
move down to level 1 and then level 2
and level 3 these levels grow by a
factor of 10 so if you have a terabyte
size database at the max level you'd
have 100 megabytes above it at 100
gigabytes and then 10 gigabytes and then
a gigabyte so you'd have this reduction
in size like a pyramid each level is a
sorted run but physically range
partitioned into many files so the
terabyte level at the bottom might be
split into a thousand files and so then
compaction doesn't have to process the
entire level at a single point in time
it can do it incrementally this is done
by uh maybe done first by level DB so
level compaction has this more
incremental compaction process that
doesn't temporarily double the size of
the database which we like it tends to
bound the number of levels you will use
and so it gives you a more predictable
worst-case read penalty and size teared
but it tends to do more rights to move
data from the top of the tree down to
the bottom then sized here so it's going
to cost you and write amplification
any final question
well it's not a question it's just a
comment or a proposition this a
read/write space trading sounded to me
pretty much like the case EAP in the cap
theorem and you could will initiate the
theorem so we have with this we had a
paper rejected initially they said the
ROM paper took like two years to get
published and one of the rejection from
one of the reviewers was oh this is like
cap but you didn't prove it
reject we were sad when it happened but
I think there is more algorithmic work
academic research to be done to
introduce more formality to explain why
this is true for me it's true in
practice but that could just be an
implementation Horta fact from different
algorithms so I hope people have I hope
it's an interesting problem for
academics to pursue we have the the
Harvard researchers who do the paper are
continuing to work in this space and
they've done a few other papers but no
no attempt at formally proving the rum
conjecture okay yeah we can do one more
yeah okay say it yeah some when you're
comparing nodb and whereas any tree I
guess in any NSM I'll kind of expect the
term sequential access to kind of come
up simply because my understanding of
LSM is it it favors sequential writes is
that is that more because we're talking
about a copy-on-write be tree or is it
more that your experience is that it
doesn't matter so much there being a
special mother than rather than a
workload which sort of randomly
addresses so the a trigger claim for me
in the past
LSM does sequential il and logically it
has n streams of sequential i/o is the
problem so a busy LSM with level 2
compaction you might have 8 threads all
doing compaction concurrently each
thread has a stream of rights it's
generating but there's eight concurrent
so if you're the storage device looking
up you see
interleaving of the rights that are
being done potentially from these
streams depending on the file system and
whether it does extend allocation and
whether but what you're going to see is
from the storage device looking up
you're going to see if there's eight
sequential streams unless they're
collaborating to only have one pending
right at a time there's going to be
interleaving so if you are a disk the
diskette is still going to be moving
more than you want it to so even unless
you have one compaction thread only
doing one compaction stream at a time
there's still a concurrent i/o on the
right side but in addition to that let's
say you have 100 concurrent users
running they're going to be generating
read requests so if let's say you have
one disk head and even only one
compaction thread doing this sequential
write concurrent with that it's going to
be interrupted to do reads so there is
and then you have the redo log so that's
another source and then you have the bin
replication log so you have n plus 2
write streams and many read read
requests interleaving there is this
multi stream proposal there was a nice
paper from Samsung and the idea is you
could tag write requests or files based
on the predicted lifetime for levelled
compaction files at the top of the tree
have a short lifetime files at the
bottom of the tree have a long lifetime
if you then so what you're doing is
telling the storage device keep short
live files in one erase block on the SSD
and keep long the files and other erase
blocks so when
so files would be more likely to die
together so that the worst case is to
mix everything completely if you want to
minimize the amount of copy out you have
to do in flash the firmware when it's
copying it it picks in a race block - to
clean - so we can be rewritten it has to
copy all the live data out of that erase
block and copied elsewhere you want to
minimize the amount of copying out it
has to do because that's an additional
source of write amplification so if you
can co-locate short-lived files separate
from long the files you achieve that
property the problem now is the SSD
vendors are arguing debating over how to
put this into the standards so we know a
way to do it that works great for LSM
the vendors don't just want a solution
that works great for rocks DB because
the world is not only running rocks they
be on persistent storage so I understand
the delay I'm just hoping it gets
through to co-locate there's always
concurrency
I just want to co-locate short and long
live writes separate from each other ok
let's like the speaker again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>