<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bottleneck Whack-A-Mole at Scale - Cliff Moon | Coder Coacher - Coaching Coders</title><meta content="Bottleneck Whack-A-Mole at Scale - Cliff Moon - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bottleneck Whack-A-Mole at Scale - Cliff Moon</b></h2><h5 class="post__date">2013-04-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/p7ZocmkXgWY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">next up we have the moon and are you not
in Oakland check it out one two three
dude watching okay so y'all don't know
I'm the three of you owe me a founder
and CTO way back to where we started the
day he's going to be talking about what
you actually do yeah so how'd everybody
so yeah I'm cliff and I'm from boundary
you'll have to excuse my voice and
getting over being sick but so boundary
just the quick plug we we do network
monitoring as a service and we have a
booth downstairs please come by and
visit and check out lots of pretty
pictures about what's going wrong on
your networks and that's my sales pitch
thanks so whack Amole anybody everybody
familiar with this game does anybody not
know whack Amole is yeah so one mole
comes up smack it on the head two more
pop up that's that's basically
performance tuning right so this talk is
about my my own personal production
experience and so I sort of want to
qualify everything here I'm not giving
you I'm not giving you sort of anything
generic or anything that may may apply
to your stuff your mileage may vary this
is all folklore right like this these
are things that help me they may not be
true and you know they may or may not
have actually had an effect on what I
was doing at the time but you know when
you get woken up in the middle of the
night you'll try anything to get the
software thats quiet down right and so
and also the other sort of meta note
here is that you know the version erlang
for most this stuff is our 14 b 04 but
when I've when I've made claims about
what airline does under the hood I've
checked up to our 16 and most of it
still true and also this talk is very
much down weeds so if you're not geeking
out about the internals of the vm you
should probably head out so
yeah so anyway folklore right I am just
a shaman trying to you know I start some
sort of primacy over nature right so uh
to the collectors so at boundary what we
do is we take in a ton of data from our
customers we process it and you know and
we we we have to be basically up all the
time or as much as possible because
we're on monitoring service our
customers rely on us to keep their
systems up so so the collectors what
they do is its first of all to the
oldest piece of code oldest piece of
running code in our in our environment
what they do is they terminate the SSL
connections from our customer meters so
customer has a bunch of meters they
connect up to us they start spewing out
data over an ssl connection we parse the
ssl certs we pull data out of there in
order to figure out who's who so we
don't have to like you know trust you
when you say you are a certain customer
and then what it does is it takes this
format called IP fix transforms it into
an internal format based on our
long-term protocol exposes pub/sub
interfaces and then uses another library
that I wrote on the Scala side called
scaling which I talked about last year
the air before to to talk to basically
to build a big Erlang distributed
cluster of you know scala nodes and
Erlang nodes all sort of working
together to process a bunch of data so
cool so yeah two and a half years in
production this this code base it was
our one of the first things that I wrote
for boundary and so the early versions
were pretty crappy fell over at about
700 connections or so benched out at
about 10,000 records per second before i
would start you know slowing down and
causing some issues and so today I'm
actually just benched it out yesterday
and we're currently handling somewhere
around 3,000 connections per machine
300,000 records per second per machine
to qualify a record is somewhere around
anywhere from 50 to 80 bites a piece so
plus some overhead you can do some math
anyway the the throughput as measured
from boundary is about 100 Meg
bits per second ingress per machine and
then 300 megabits per second egress into
our internal network so on the pub sub
interface so these guys they're doing a
lot of stuff that's still too slow
actually but it's you know it's better
than it was yeah same hardware mmm so
Erlang there's always a queue somewhere
getting backed up right this is how I
feel and if anyone was at jeffs talk
earlier today this allow this will feel
very similar I'm sorry if I'm rehashing
the same stuff but it's probably things
you should hear again anyways so if
we're trying to find the queue that's
overflowing what do we what do we use
how do we get there right like so I'm
just going to assume that there's one
here no remote shell yes anyone here not
know what remote shell is okay that's
probably a better question so remote
shell is absolutely magical it's one of
the best things about Erlang and one of
the things that no one talks about
outside of the Erlang community which i
think is crazy you know you get a remote
a remote repple into a production system
and you get right are you can run
arbitrary code which is frightening and
awesome at the same time these are some
of the things that I use when dealing
with with my production systems so so
that that I function up there sorry I'm
in the ways that i Function up there
that'll actually print out the process
table it's like ps4 your Erlang atop if
you can get it to work I've never gotten
it to work quite properly but it works
really well with react which is nice
process info so you can actually see
there's a nice little hack so you can
actually say pit and then give it three
numbers and will transform it into a pit
for you that's fun so you get process
info you can also get a back trace I
don't know if we knew that like all the
way back to our 14 like even without the
even without the new exception stuff you
can get a back trace printed out for the
console and actually figure out where
the call stack is so that's really cool
you should use that and so other stuff
here so this that's top stuff that's all
run time you got woken up oh my god
what's happening I'm things on fire this
is stuff you do beforehand right so you
got to make your own escape hatches so
when you build stuff you need to build
it
it's the idea in mind that at some point
you're going to be running around like
with your head on fire and you're going
to want to break encapsulation right so
basically what i do is any single gen
serve or anything that i build i will
just have state clobbering methods hi
haters and then admin functions right so
like you know if you find yourself doing
the same thing over and over again when
stuff catches on fire you right admin
functions and ops playbooks so that when
someone other than you gets woken up
they'll have some idea of what to do so
the escape hatches yeah clobber state
frickin can people read that I'm sorry
this this I was not expecting a
projector that sucked so so yes it's
basically just handle call clobber the
state or get the state out of a gen
server right because there's no way to
inspect that a run time that i know of
without actually just you know sending a
message and getting something back
here's a fun one from our collectors off
the i'll post this code up later since
if this is all screenshots and stuff but
anyway the you know admin functions so
what these do is they dumped the memory
of the usual of like the usual suspects
right like there's two things that are
going to overflow at any given point in
time so when things are going wrong what
are we doin and then admin meter memory
and then admin sub memory and it lists
them in order of who's consuming the
most heap up at the top so immediately
you know okay here's the guy and oh my
god he has a gig of memory holy crap
we're going to die right so yeah so
playbook admin stuff this is all
necessaries that you won't have on day
one but you should develop as time goes
along right DevOps anyway so failures
right like how do how do I Lang
processes fail in production I will tell
you how they failed for me typically
what you see is there's some sort of
overloaded process right we're going to
go through the ways I've seen overloaded
processes Go Go Ape so basically the
generalized version of this is that for
some reason an overloaded process cannot
keep up with its incoming rated messages
and the Erlang memory model makes this
problem really bad am I
and so the the heat / process that
part's fine but the message cube being
stored on the heap makes this thing
worse and I'll tell you why because the
problem here is that garbage collection
puts the process to sleep and if your
heap is being consumed by the queue
guess what's going to happen you're
going to do more GC and you end up in
this situation that I like to call the
process death spiral and people who are
Jeff's talk earlier this is a bit more
of a dissection of what actually happens
but basically at any given point for any
given process you know let's say you can
do n messages per second if the arrival
rate is greater than n then the messages
are going to start queuing up and if
that and if that persists for long
enough then the queue will get larger
and larger and it causes the process to
go into the garbage state which I don't
know how many how is everyone here is
dealt with a crash dump right yeah okay
yeah so guarding will have a slide on
that but yeah it'll cause it to go into
the garbage state which basically means
it's garbage collecting garbage
collecting and possibly reallocating as
well which is the fun one and basically
the what happens is is that as as time
goes along it spends more and more of
its time slice in guarding and so but
the sender doesn't stop sending messages
and so not only so not only is your
prato knee can your process not keep up
at a baseline your it's getting slower
as time goes along this is terrible and
so basically and so basically as time as
time goes along you know you can you can
do less you can do less messages per
second so this problem gets worse and
worse and worse until eventually you end
up with a crash dump and you see a lot
of you see a lot of processes in state
Garmin so this is from the Erlang docks
where they talk about what do the
different process states mean and I've
always found this to be hilarious this
is bad luck
this is bad luck the prot you just you
happened to catch the process garbage
collecting lucky you know actually the
process was like ninety percent of its
time and garbage collecting you know
you're not like but anyway so this
continues and then and then the the you
know similar slide to Jeff's earlier but
you know Iggy palak it asks the
operating system for eight gigs of ram
and it's like me Malik is just like
sorry dawg I can't help you and yeah and
that's where that ends up so okay cool
that's that's how things go terribly
wrong what do we do about it right so
basically there's there's three reasons
in my mind three generalized reasons of
why you can't keep up either you're
you're putting your process to sleep I
my calling receive somewhere within the
the main loop of the gen server or
whatever it is you're doing too much
work which in our line usually means
math or your sender is just too damn
fast right like you know one of these
things and such right one of these
things has to give so how do we how do
we mitigate how do we mitigate this
thing and so for the various failure so
like how do we mitigate the receives so
I have a quiz for you receives in gen
server with which which which of these
can cause or receive who votes for one
no no one thinks a jet yeah okay ones
okay who votes for to any votes for
three ah how about all three anybody
vote for all three yeah okay yeah so
it's all the above all of those state
all of these statements can actually
incur receive and you know the first two
that makes sense right because
underneath a gen call you're receiving a
reply back right Jen TCP receive you
know you're actually receiving a message
from a from a port somewhere underneath
the hood but put the send operator what
the that yeah so that actually so
if you if you look deep down inside the
ER TS you're going to find this code in
erlanger al and so this is actually from
our 16b i I'm a
sure I was looking at the code today
made sure this still existed so inside
of Erlang Earl there's this function
called descent and if you look at bif
dot c you'll find where it converts the
the the bang operator in to ascend and
under certain conditions not all the
which I actually understood from the sea
but under certain conditions I think
mostly having to do with the fact if the
port is a remote pit I'm sorry if it's a
remote pit or a remote name and node the
name a node temple if it's if it's
either those things it can cause this
this to happen here which is the descent
which degenerates into a net Colonel
connect and an erlang send the problem
here is that net Colonel connect is neck
Colonel is a gen server under the hood
and so you're actually doing a gen
server Hall with a default I'm out
inside of the inside of the bag off and
that only happens if you're calling a
remote node right so so it degenerates
into an erlang send and uh so basically
it's a gen server call an erlang send so
yeah and I actually found this by
profiling it out and like where the hell
is the to receive and finally that was
that was a bad week I was in like twenty
eleven that was bad yeah so that stinks
and so if you're sending or so if you're
sending something to a remote node user
line colon sent because you don't want
to get into the neck neck girl stuff
yeah cool other other ways to mitigate
errant receives so what I found is very
very good thing to do is separate the
control plane from the data plane and
we'll get into that in a moment you're
going to know what you're calling right
so if you're using third party AAP is
you have to understand you know what are
they doing under the hood are they
talking to other jen servers is this a
net stable I mean you really have to
just know all your code like I wish
dialyzer would just have a mode that
says follow all the code paths and find
the damn receives that would be cool or
it might I don't know that thing's huge
but and then basically you're with your
own gen servers like find the hot path
and cut it down to his little code as
possible so that you don't need to
do as much static analysis beforehand to
make sure that your you're going to be
all right so what I mean what I mean
what do I mean by separating control
plane from data plane basically control
plane and data play and have I'm sure
these are sort of generalized terms but
but what I mean here is that the control
plane mean is something that needs to be
low latency and the data plane needs to
be high throughput so these are at odds
with each other especially if they're
pulling off of 1q and so the basic way
to do this and and sorry the best way to
do this is separating out in two
processes and then glue them together
with a net stable basically you set
Rican currency on that stable and then
away you go so that the way that so what
we did here is that remember earlier one
of the things that our collectors do is
that they they have a pub sub interface
internally so that pub sub interface
multiplexes on on both the customer name
and a number of topics per customer so
if we have you know somewhere in the
area of several thousand customers and
you know say five topics per customer
and then we have a number of downstream
machines that need to sort of do that
need to subscribe to a bunch of that
stuff we're going to have a lot of
subscriptions in that table the problem
though is that when one of those
downstream machines dies goes down for
some reasons JVM blows its heap or
whatever the hell's going on then we're
we're in trouble because then we have
several thousand subscription requests
in coming to the thing that's handling
the high volume the high volume data
plane so yeah that's that's really
really bad and so that's that's
basically when you want to you know pull
apart that state and that you know
thundering herd right it's a thing so
doing too much stuff right this is kind
of the the one that you always end up
with you know you're trying to put 10
pounds of into a five-pound bag and
then that doesn't really end up going so
well so obviously the preferred solution
is do less stuff so I don't know if you
noticed earlier I sort of sailed it by
you with my numbers
our ingress is 100 megabits per second
but our egress in the back end is 300
megabits per second so the Stewart
observer would say what what are you
doing between here and there to make
that bigger than that was coming in
right and so part of that is a little
bit of that is replication but most of
that is actually just inflation in the
format to make it self-describing and to
give you an idea of earlier versions it
was ten to one so you know we we did
some changes to the internal format
about a year ago cut that down by half
and bought us you know about six months
of good good runway to keep these things
running and so that's that's that's
basically the that's what you want to do
obviously these changes are harder but
you want to do less stuff spit out less
data through less math you know touch
things less because that's the easiest
way to you know deal with performance
problems sometimes you can't though
sometimes you have to mitigate the
overload so what do you do in Erlang
well there's sort of this the flip thing
here is to just say just spawn a process
all right so you're in you know you're
in a gen server you're in a gen server
call you have some piece of work that
either needs to talk to another gin
server or needs to do something do
something long-running or do something
that generates a lot of garbage and
that's and those are those are important
things to actually profile and
understand because so say you're in a
you know say you're in a gen server and
what you're doing may not take a whole
bunch of time but you generate this
tremendous spew of garbage well every
every so often you know that gen server
has to go back into garbage collection
in the garbage collectors in Erlang
really stink because mainly the whole
thing is oriented around the fact that
you should just be spawning off these
short-lived processes to do this kind of
work so you know spawning off the
process is actually ends up being a
cheap operation most of the time and it
can be cheaper than incurring all these
garbage collections but the important
thing is is that there you know there's
a price to be paid which can be context
switching overhead right so there's
copies involved here and you can you
need to copy data from one place to the
other and then obviously the spreading
the load across CPUs can
it can also incur some overhead as well
right and so the thing here is that you
have to measure this you know make the
change measure the effect because you
don't actually know what's going to work
before you do it you know you have your
theories and then the system will
actually prove you're wrong another way
to do it is worker pools which are
actually a bad idea and are lying
because of that before mentioned GC
problem you know processes are
lightweight and so this idea of having a
pool for something is really just sort
of a bad idea that you is a hold that
can be a holdover from other languages
and other run times because spawning is
so cheap and really the worker pool is
only there is only really useful when
you have expensive stuff you're trying
to rap like a socket like an if the
animal like a port something with with
significant setup and teardown costs
that you don't want to be you know
flapping up and down all the time and
then another thing you can do here you
know I hate to mention it but like you
can screw around with process options
right so and I've definitely done a lot
of this and mainly I found it to be
shooting myself in the foot when I've
actually measured it so you can set
things like minimum heap size you can
set things like full sweep after you can
set the priority of the process most of
them are just going to end up being
either step backwards or equivalent to
what it was already doing they will
really help out a whole lot and in some
cases then you know it's just sort of a
bomb a trap you've set for yourself
right that's going to go off a month
down the road in any event you got to
test and measure them if you do want to
make those changes but that's not where
you're going to get your performance
gains from so that's that's a lesson
that you don't have to learn of course
the other end of things is that you
could always write you know the end of
day you can always write an if right
obviously you can do the work faster you
get access to system calls so if you
need to do things like writing the files
you can set the proper modes and then
map things do all sorts of fun stuff of
course this starts that slippery slope
down to a bunch of C++ glued together
with Erlang looking at you bash oh I kid
I kid
okay so fast senders what do we do about
the fast sender well flow control right
so in depends upon what the center is
the pencil what the sender is how we do
our flow control so we could be reading
from a socket and obviously as has been
drilled into many folks use active once
I was not using active once for a while
in this code I thought oh I'll just said
active true and then I won't have to go
to the trouble of sending active once
because that's just going to slow me
down it's work I'm not stupid and then
you know suddenly we we had we had
customers start up that were spewing you
know gigabytes of data per day on a
single node and like just like like like
you know like 10 megabits per second
into a single Erlang process right for
fro from from a single meter like like
crazy like that and we had
customers that would spin up with these
with these nasty data streams that we
can only handle oh that we can only deal
with on one process and like you know
what active true is not helping us out
there so yeah we're using active once
don't you and here's the thing use one
of the actives though do not use gentian
TCP receive because one of the main
reasons why is because jen tcp receive
is a blocking is a blocking as a
blocking call and typically what you
want to be doing is you want to have the
messages delivered to you as fast as you
can consume them you don't want to be
going back to the queue and trying to
receive something and doing some sort of
select or receive against it you just
know you just want to work the queue
head to tail fast as possible and then
yeah use the framing socket options so
you don't have to do your own framing
with our protocol I have to do my own
framing haha joke's on me and yeah and I
mean basically the great thing about
active once is that you leave the
buffering to the tcp colonel the great
part about that is when you leave the
buffering to the the tcp stack in the
colonel it is able to exert back
pressure through the tcp links to the
sender so TCP has these really cool
option has this really cool stuff I can
do with the windows
eyes we're basically you know you have a
TCP buffer size of however many k and
then once that gets overrun the window
size is set to zero and it will tell the
other side to shut up and just so long
as there's no timeouts on that so long
as nothing times out and says oh this is
done and you know cuts the connection
it'll stay like that basically forever
until you process your cue so you know
that there is there is there is help
there's help available TCP is nice and
then if you're doing process to process
messaging you know there you can find
the mailing list allusions to to flip to
flow control and back pressure inside
the vm i don't i don't buy it it's it
all seems a little bit too magical and
ill documented so basically what you
want to do is to have your control you
want to basically implement your poor
man's tcp inside of your the protocol
between your notes right so again the
what Jeff was talking about earlier in
his talk you have you make a request for
a certain amount of data the sender
sends that amount of data to you you
process it then you make a request again
and that ensures because you're making
the request that ensures that you're
able to process data at the rate that
it's getting sent and so you don't end
up in that position where you have the
the buffer overspill yeah and then and
then you know the built-in flow control
so Erlang send when you're sending to a
remote node sometimes that can suspend
your process so that is the one piece of
built-in flow control that you don't
really know about until you read the
docs closely so if you're sending to a
remote node and suspending the sending
process is not an option you need to use
send no suspend send no suspend will
return an error value instead of putting
your process to sleep so if you're on
the you know if you're on the far
sending end of a data pipeline and you
need to send stuff across the wire the
buffer is full instead of the yeah the
buffers full like even if like the
Erlang distribution buffers full no
suspend will make sure that you don't
put that process
to sleep and then cause the cascading
failure of death and it's like it's the
question like the end of the day what's
better lose a little bit of data you
know wait to send or you know I gigantic
explosion right like lose all the data
you know you got to contain the pain and
then of course the the one thing that I
like to do here and again this is kind
of hard to read so I apologize the
blow-off valve so mem soup is there I'm
here familiar with MEMS you pee from the
OS mon package ok so mem soup actually
works it's it's surprising it works
really well it has hooks into the ER TS
so it actually looks at allocations and
so you can register an event so if you
start up so if you start up mem soup
swap in an event handler your own event
handler and de men suit then you can set
up set the call is set proc mem
high-water mark and then give it a
percentage of main memory so this one is
set the two percent of main memory so
what that saying is is that any time a
process tries to allocate more than 2%
of main memory as its heap set you know
alarm the event handler and let me deal
with it and then inside the coolest
thing is inside the event handler you
get the message and then you get the pit
of the process and so you can actually
inspect the pit to say what the hell is
this thing that's trying to take up all
my memory and then you can do and then
you can actually do real mitigation in
there right so you could you could shoot
the process in the head obviously which
is pretty cool or you could you know you
could do any any number of things
depending upon what the actual what the
actual goal is obviously shooting the
process in the head is one thing you can
spin up more processes maybe that will
help I don't know or you can just
basically like if you have access to the
center you can put the center to sleep
or tell the center to slow the hell down
like there's things that you can do to
keep this death spiral from from
spinning out and going out of control
because you can execute arbitrary code
here and it actually works which is cool
so yeah so stop the vm killings death
spiral of memory so that's really cool
moving right along let's say that your
Erlang is mysteriously your Erlang app
is mysteriously unresponsive right so
we're out of the realm of
memory consumption CPU consumption even
disk consumption resources across the
board lo you at low utilization and yet
nothing is working what is happening
well you're probably deadlocked and so
I'm going to ignore the classic cases
deadlock where you have two things
depending on each other and they're both
waiting on something because if that's
what you're doing then god help you you
shouldn't be writing code honestly but
but but basically what what can happen
here with with deadlock you can get into
some very interesting cases and again we
get back to the generalized form like
you know within the given time period a
gen server can process n calls right so
if your code sends @ m where m is larger
than n then M minus n calls will fail
right because the timeout happens and
and if you don't deal with it the
failures will propagate to the to
whatever is making the calls let me
illustrate this so I know it all ends up
being timeouts right so and this is this
has bit me so many times because there's
a default timeout on gen call so if you
don't set a time out one is set for you
at five seconds is five seconds the
right value for timeouts oh it's one of
those things and so you know the
knee-jerk reaction is to set the timeout
to infinity that can actually make this
problem worse because then you're
waiting forever for something that might
not get there especially if it's a
remote call and so basically we have to
do here is first of all set a time out
that make sense for your application you
probably don't even know what that is so
maybe five seconds is right handle but
you got to handle the failures which is
the important parts don't just let the
process die do something about it please
I know this is Paris II right but you
like log the air and then maybe retry
the call if appropriate and and again we
ran into this issue where back when I
was talking about the control plane and
data plane so what would happen is is
that we'd have we'd have this situation
where we'd have things trying to
resubscribe to
to the to the subscription controller
and they were getting backed up and
there's this big backed up Q of stuff
waiting to get done and the subscription
requests were sitting in the queue
waiting to get done and then they would
keep timing out because there was a five
second timeout set at the other end and
soho and so the subscription would
eventually go through but the other side
wouldn't recognize it because the call
timed out and it would keep trying to
resubscribe to a subscription that was
already there and then and this and this
condition would persist and go on and on
and on and it would never stop and we
didn't even know about it because we
weren't we weren't logging the failures
properly so you know the question here
is you know I mean really honestly the
ultimate thing is that do you even need
to be doing a call like do you need a
response to this you know or you know
one way to do it is have a deferred
reply have it have a have a deferred
reply so if you're talking one gen
server to the other you don't have to do
the call-in line and block and with a
receive you can just tell the other guy
hey do something give me back a ref and
then in the handle info block I can just
match on that ref when it comes in and
then do something with the reply later
right and so you can defer that reply
possibly forever but you can still work
the queue without having to wait around
for this one thing to show up so you
know this pattern is really really good
for things that yeah sure a reply would
be nice but we don't necessarily need
one and I got to do so I can't wait
around for you and then of course the
big one your code does it even need to
be a process right obviously the the the
big you know the elephant in the actor
room is that wrapping state in a process
implies a mutex around that state so any
kind of access read or write is always
miu text there's no you know there's no
read write lock so you know you you
basically just have to dig in to
sometimes you just have to dig into
Erlang's concept of shared memory which
is ettes and so this is what we did with
Folsom by the way so Folsom was having
issues with being basically updating
counters at high frequency and so we
just said screw it we're getting
to the processes and putting it on that
and hey guess what it worked and then
just refactor your code to pass around a
pit instead pass around a table
reference to the pit it sucks but hey
that's the way it works the network side
and actually I have some slides missing
here but i'll talk about them anyways so
one of the things that we found out with
the Erlang distribution protocol that
really screwed us over with our internal
stuff so we talked early and
distribution internally to our other
nodes so early and distribution protocol
is tuned for low-latency so TCP no delay
is appears to be on by default because
what we saw is anytime a single message
would get sent out on Erlang
distribution protocol would end up being
one packet on the network and so if
we're sending a ton of messages over the
time to tiny small tons of tiny messages
each one of those is going to end up
being a packet on the network and your
switch is going to die so and those are
called runt packets they're terrible
they're the entire reason that nagel's
algorithm even exists so basically what
you have to do here or what we found we
needed to do was you do one of two
things you can either buffer in the gen
server which you're doing more work but
whatever you know or you can you can
actually use an intermediate process to
do the buffering and then flush that out
to deal with GC concerns or just open up
you know open up a jenti CP socket and
send over that with you know with Nagle
turned on and then that will help you
with the throughput but won't help you
the latency the other thing that I found
out this is actually recently and I
forgot that it to my slides today but
TCP backlog anybody here know TCP
backlog is yeah so there's there's
there's a there's a queue uh-huh there's
another queue here so there's a queue in
the TCP stack of the operating system
for incoming connections right so if you
have incoming connections the TCP
backlog is the queue of connections
waiting to open these are half open
connections where where you're you're
waiting on your application to to
basically finish the connection and then
open it up into a full socket right from
from the listen socket and so the TCP
backlog on normal UNIX operating systems
tcp backlog is set to about 128 bit
meaning that you can have 128 half open
connections waiting on your application
to pick up and as long as it doesn't get
over that you'll be fine however that
option can be set in the bsd socket API
and for whatever ungodly reason the
default Erlang when you use gen tcp
listen is 55 connections so you
can only have five connections waiting
in the queue and so if you're doing
high-throughput stuff with a lot of
clients say 3000 that I'll need to
connect at once because you just
restarted you're going to have a bad
time and then just gotten up with a lot
of connections that are just going to be
just completely closed and they're going
to keep spinning on the other end and
it's just going to keep being hang-ups
on that guy like you know uh closes on
that on that connection because you
don't the the operating system for you
and I'll modern modern operating systems
that that that number can actually be
set to like thousands right not that
you'd really need that but yeah five so
no the defaults because sometimes are
really bad and anyways in summary if
you're going to take something away from
this talk other than nonsense just grind
the loop right so observe that there's a
problem find the q that's overloaded and
then mitigate the bottle and I mean
honestly you know I could you could say
a lot of stuff about Erlang but at least
it makes it really simple because at the
end of the day it's just a cue somewhere
that's overloaded so find the Q fix it
and that's all I got any questions
I'm sorry what's that it's in the
operating system so it is so the setting
yeah so they're there you could get
there's no guarantees there right so
like you could very easily get yourself
so it'll be fine for months and then all
of a sudden like a bunch of stuff will
happen and then lay bunch of connections
will start up and then you'll have a
customer complaint they can't connect
and they're they're stuck in a loop and
you won't know why right because that's
actually it's it's a you it's like it
will only show up and like netstat if
you if you put in the list an option
because that that's the only time it'll
show you the listening the listen queue
and then you got to know that it was set
to five because so the answer there is
is that I mean yeah the typical way is
to have and it's like to have acceptor
pool right like like to just pull stuff
off and that will get you pretty far I
mean we had this in production for two
for two years with the default set the
five and we didn't even know right it
only just really screwed us in the past
couple months so you can get you can get
a pretty long ways but but honestly
there's no reason for it to be five it
should just be you know 128 at least
oh yeah I mean yeah I'm not yeah it
might have been a long time ago that it
was five i'm not sure but when i check
the recent kernels though the default
ends up being on in 28 so unix
archaeology should be like a thing they
should teach that in college i'd be cool
huh yeah absolutely yeah exactly like
again this is a degenerate case of 10
pounds of in a five-pound bag right
like there's there's a certain amount of
capacity in the machine and if you
overload that then you're done right
like you buy more machines what why is
work or pool worse well so the reason
that I say worker pool is worse is
because long long live because you have
long live processes that might be
spinning off a lot of like a lot of
garbage and instead of doing all the
full sweeps on those which would happen
if so a full sweep will happen every
couple of every couple of GC cycles
right and then reallocations will happen
too so you're doing all this work
basically in terms of GC and
reallocation basically for nothing just
because you have this thing sitting
around instead of just springing it up
and then throwing it away right so when
you when you when you just spawn it up
you know the overhead is like I like 300
and some bytes or whatever but but but
it's you know the it's just a data
structure in C right you're not spinning
up a new thread so it's it's fairly
cheap and the I mean yeah it's got to go
on a run queue and all that and yada
yada so maybe the context switching will
kill you I don't know that's why I going
to bench it out but the reason that I
say it's better to spawn a process to do
the work and then just throw it away is
because you don't need to go through all
that all those markets four phases and
and all that other crap in GC you can
just throw it just throws away the whole
heap and those the bounds and just all
back to back on so it's less work in the
end for the vm
we so we use worker pools mainly just to
wrap like connection stuff right like we
use pool boy for our react clients and
that sort of thing you know I believe me
I've tried using worker pools for doing
actual generalize like cpu-bound work
and it's just it it ends up being more
trouble than it's worth like you spend
more time managing the goddamn poll more
CPU time managing the pole than actually
doing the yeah it's awful oh yeah well
we use hot code upgrade extensively so
hot code upgrade is great because it
lets us because we have 3,000 SSL
connections on a machine it's it the hot
code upgrade ends up being less
complicated than all that is just you
know have these things because I mean
these are connections we're taking it
from the internet rights it's like you
know we have we have a public IP for
each the collectors we gsl be across
them from from dns and then connections
come in and it's just a long live
connection that stays open for a long
time and so that ends up being a pretty
decent model for us if he asleep rooves
but the ends up being a pretty decent
model for us and then just doing the hot
code upgrades it just works so we end up
with you know taking no downtime we
excuse me when we have to upgrade the
code for these except when the how to
code upgrades fail and then then we take
some downtime
what's up well yeah we use like rebar
and I mean I wouldn't call them proper
but I mean we do use the release
infrastructure we i would say abused
maybe you know we don't like we don't
know we number things based on like get
Rev and stuff like it you know but but
we use rebar and we use a lot of the lot
of the chefs chef stuff that Joe
Williams put together for us to do the
hot code upgrades you put a lot of work
and actually making that stuff usable
it's pretty indecipherable sometimes
most of the ones without state
transitions yeah yeah exactly the ones
where there's no state transitions you
know when there are state transitions
then you actually have to test the
release and that's just a big pain in
the ass you know gotta fire up a real
one like we got to fire up the current
release and then test against their
figure out why it's this that's a big
pain in the butt so try to try to avoid
those I'm up yeah
okay the history about the five seconds
yeah oh yeah right really so you so you
would rather know I mean that's it
that's a good point yeah if it's a yeah
if it's a gen call that's a good point
you'll detect if it crashes right so but
it doesn't detect the case where the gen
server chokes on that bit of work and
just spins fair enough yeah yeah yeah
that's a that's a good point that's
interesting with the history there so
for people on the internet he was saying
that the history of the the gen server
time out is the fact the back in the day
you didn't have monitors or links and so
if you had you know Jen server somewhere
I could crash in the middle of
processing and you had to have the
timeout in order to keep in order to
keep the the client from waiting forever
all there was links okay yeah the link
is bi-directional instead of me Yeah
Yeah right now exactly all right yeah
you have the tools were using for
dividing your huh have I tried out red
monk no a red bug I have not tried out
red bug okay yep all right yeah right on
all uh yeah i'll give that up give that
a whirl next time things that I go when
I oh with out having to door them janna
cool
yeah plus one on that yeah yeah I'm so
I'm sorry what was that the with with
what do you mean by clustering do you
mean like yeah oh yeah yeah exactly it
was really inefficient
yeah so are there any other benefits to
just opening a socket well I mean you
know well is there any other I mean you
can certainly you can certainly twiddle
the knobs a bit in terms of you know
send buffers receipt you can you can say
can't will the knobs a bit in terms of
send buffers you know you can you can
you know you get a little bit more
control over it then you would if it was
the airline distribution protocol I yeah
exactly you got to do the framing
yourself so it means the trade-off
between you know code complexity and
performance right you know make the
trade when you got it anything else oh
ok thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>