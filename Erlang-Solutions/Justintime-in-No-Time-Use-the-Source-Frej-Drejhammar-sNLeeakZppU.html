<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Just-in-time in No Time? &quot;Use the Source!&quot;: Frej Drejhammar | Coder Coacher - Coaching Coders</title><meta content="Just-in-time in No Time? &quot;Use the Source!&quot;: Frej Drejhammar - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Just-in-time in No Time? &quot;Use the Source!&quot;: Frej Drejhammar</b></h2><h5 class="post__date">2012-07-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sNLeeakZppU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what is just in time completion it's a
mechanism where you want to before you
execute something perhaps in an
interpreter you want it compiled to
native code mainly for performance it's
a very common implementation techniques
all the cool languages have it for
example Python small talk small talk as
I've had it forever java javascript and
when you want to do just-in-time
compilation you have to decide when does
it pay to actually compile your code
it's probably suboptimal to compile code
you will never execute or it only
execute once and you do that either by
heuristics perhaps with help from your
compiler but also the main technique is
runtime profiling so I can notice all
I'm executing this code a lot let's
compile it and when you have decided
what to compile you should also decide
on how much to compile and it's usually
trade-off because if you're having an
interpreted language your byte code is
probably much more compact than the
native code so you don't want to compile
everything because then you probably run
off the memory and the scope you do this
on is typically anything you decide to
compile a complete module either you
compile just a method or function or the
minimum version is that you just compile
the current execution trace you have
discovered that is a hot piece of code
and the tracing strategy is what we've
decided to use in binge it tracing means
that we use fairly lightweight profiling
to discover one we are executing a code
path that for
sequently and then when we have
discovered that we follow the execution
path and record the execution trace
until we get back to the same place we
started from because then we found a
loop and it's probably a good idea to
natively compile a loop and some
assumptions for tracing is that when you
trace you're probably in that execution
trace that is the one that's frequently
used so it's the most common one and if
you do happen to select the wrong trace
then you trace problem becomes too long
and then you just abort the tracing and
goes back to interpreting your code but
if you could we get back to our starting
place then just compile it so that was
the quickie three minute version of
just-in-time compilation the background
so why would you want this for for
airline we already have high these are
the number are at least four main
reasons hype doesn't give us cross
module optimization which a tracing it
will do more or less automatically will
also same memory because if we're just
using a single hot path to our code will
only compile that to native code and
also if we decide to enable tracing in a
running system we can just do that we
don't have to switch completely from
native code and a high compiled code to
the interpreter if you are lacking our
 works as it's designed to do we
might even compile parts of the tracing
also a big win is when we're deploying
our systems there is no need for cross
compilation for example if you develop
an x86 and you want to deploy an arm you
don't have to have a cross compiling
beam that runs on your x86 machine and
fritos produces binaries for are also
the bynars we produce are no longer
strongly connected to the actual
emulator that precise compiled version
of the emulator you using so if you fix
a bug with changing some data structures
in the beam emulator you don't we won't
have to recompile all of your code just
because the height code is so strongly
connected to it so I hope that satisfies
the sum of Costas objections so the next
part is the beam emulator so you know
what I started with to implement the JIT
compiler beam that's name of the current
Earl nvm it's a register machine it has
roughly 150 instructions which during
load time or people open the mice to
macro instructions using a small
optimizer thing so when then you have
around 450 instructions and as i think
joe said this morning there is no formal
specification or no authoritative
description or what was it can it you
said that there was no full formal
description of the current vm the
instruction set yeah and you usually see
that there is once a year there is
someone asking on the mailing list
for such an description which is kind of
a drawback if you want to do a jit
you're not really sure what you're
trying to integrate with or emulate and
also a key thing about at least from the
from my part from my point of view is
that the beam implementation it uses
something called a directly threaded
interpreter meaning that if you look at
the main loop of the interpreter you
have something looking like this you
have an instruction pointer you have
that points to code memory and oak codes
are implemented by blocks of code with
with a label at the start it does
something if that updates your program
counter and then it just dispatches
looks up the current PC gets an address
which points to another up code so
that's mechanism using to actually
advance through your code well why that
is important I'll come to later on
during this talk so now over to the
binge it prototype well how what is the
goal of this jit compiler at least from
my point of view and what should steer
my implementation of the JIT compiler
well my goals were that I want to do as
little man and work as possible I'm
Lacey I want to preserve the semantics
of plain ordinary beam and i also want
to stay automatically can sync with this
standard beam implementation so i don't
want a if a bug is fixed or if a new
instruction is added to DB enemy
interpreter I don't want to have to do
any man
work I just should have to do make and
everything should build and then we
should now support that new instruction
will have that bug fix in our JIT
compiler and I also want a native code
generated at the state of the art so how
do I swing this well my plan we didn't
have any a exact and correct description
on the virtual machine so what do what
do we have we have the source of course
we parse the source extract the
semantics from that that works and we
use that parsed source to actually
transform we transform it to get those
pieces of functions and implementation
that we need to create their jet this is
somewhat in contrast to other languages
languages where they've actually tried
to write their interpreter in the
language itself and then using some
tricks they actually make the
interpreter written in the language
itself to book both SS the interpreter
but also as a JIT compiler there are at
least two two examples I no call for
example for Python it's called pipe I
it's a Python jet written in Python
itself and there are also another one
called cog for small talk and this is
perhaps quite a tall order and so how do
I get this working well I use a very
powerful tool kit called llvm it the
name stands for low-level virtual
machine but doesn't have very much to do
with virtual machines anymore it's a
compiler construction tool kit it
contains a lot of optimizations native
code generator
and other two chain technologies it uses
internally an abstract form of assembler
targets independent code LOL are i R
stands for intermediate representation
Alabama also comes with a front end for
c4c style language is called clang and
that compiler it parses C C++
objective-c and perhaps something else I
don't remember right now and it produces
as output LOV my are so combining flying
where that will them you get a full C
compiler clang is also wrapped up in C
library called lib its originally
intended for doing refactoring but you
can also use it to just extract the
abstract centrix three of your parsed
see functions or C modules and then
traverse it and transform it as ever in
which way you want so if we get back to
the problem problem at hand digit for
beam what do we need well we need a way
to profile our code we want to find out
where the hotspots are we want a way to
represent the emulator source code so we
can actually generate the functional
parts that we need we want a way to
trace execution we want a way to when we
have traced an execution path to convert
it into native code and we'll also want
the way to share emulator state between
the interpreter and our native code all
this without slowing down the emulator
too much and profiling is actually
easiest way to implement you just ask
the compiler to help you so you ain't
letting compiler insert profiling
instructions at
head of loops and remember this is
airline the head of a loop is always the
head of a function so just have the have
your compiler insert a special profiling
instruction a tough ball each loop so if
you look at this example here you have
on the right or your left you have a
small loop which when we compile it to
beam good we get this thing on the right
and you can see here that the head of
the function we've added a small
profile instruction that distraction
oops
yeah jumped ahead so we went at low time
we associate each instance of this
profiling instruction with the counter
and each time we pass through it we
increment the counter and when that
counter exceeds a threshold we say that
hey this is probably something hot now
we want to trace it so if we switch to
this example again we see that you have
this profiling instruction and the first
instruction we execute after that is a
test we check that is register a kiss 0
equals to 0 in that case we fall through
otherwise we jump to label f3 down here
and in during the rest of this talk I'm
going to just go deeper into this test
instruction are not going to go through
all the whole complete trace and that
instruction it looks pretty generic but
during loading it's actually translated
to a more specific version of a test
it's actually implemented by this is
equal exact immediate FRC meaning that
it takes an immediate value namely the
zero and checks if register x0 does it
equal the constant that is inside our
code is the instruction pointer if that
is not true we go to label here where we
update our instruction pointer and then
we dispatch a way to some other opcode
and this is actually somewhat compressed
source code that's what you get when you
compile the beam emulator when you must
run it through your the preprocessor and
that this is the semantics of this
instruction and this is what I have to
work with to simplify my further work I
processes the bit so i use lib clang to
parse to see file and then I traverse
the absurd syntax tree and I simplify it
a bit so you can see if we had this
beforehand I have lifted out local
variables for something this one next I
moved it out to the top scope I have
removed all for thrus so I have a
explicit go to I remove all loops
there's no no example on this slide and
replace them by just simple conditionals
and go tues i turn my struck that the
reasonably structured c.c implementation
into spaghetti of just labels and
golders is pretty evil but it turns out
that is more or less the control flow
graph of the complete emulator and when
I have this a control flow graph I also
do lightness of analysis on all these
lifted out variables and I'm going to
tell you what I use that for later on
when I have it in this form it's pretty
easy to implement tracing
if we look at each of these labels
either created and added here each label
actually corresponds to a basic block so
each each each label ends with a
conditional or non conditional control
transfer to another basic block and if
we want to trace the execution flow we
just tag our transformer at the start of
each basic block we add code to actually
record that we are here in our
interpreter so we save the instruction
pointer and we saw save the identity
which is just a magic number for this
basic block so you can see this basic
block got that part this one got the
same i omitted they actually
implementation here and here so how do
we turn this song we don't want to run
this all the time well we generate two
versions of each basic block one without
the trace code and one with and one with
the trace code and we'll also to be able
to switch between these forms we change
her interpreter from our directed
threaded code previously into I'm
indirectly spreaded code meaning that
previously what we stored in our code
memory was the address of the label
implemented that of code now switch to
something that's just an index into
table and that table stores the address
to the opcode to the address to the
label which is at the beginning of the
code implementing the code that means
that a
want to switch from from the tracing to
the non tracing version we just have to
update this ops pointer from to select
one of the or the other that seems it
actually adds an indirection and that
could be expensive but it turns out that
at least on modern CPUs you don't see
any impact on performance probably
because this kinds of lookup is what
every Java program does when you do a
method call and also virtual methods in
C++ so processes are pretty good at
optimizing this so the part that remains
about tracing then is how do i represent
traces currently it's very naive I just
want to get it working first currently I
only handle one Hong Kong trace /
process traces I have a fixed maximum
length and I only support one ongoing
tracing operation starting at the single
profiling instruction and that is a
large field for further improvement
later on so now we know how to profile
we know how to to trace and now we want
to get into generating native code and
that I want to use llvm optimizer and
native code generator but we have a
small problem here lvm only understands
elodie mir our interpreter is written in
c and we have C parse trees I don't want
to implement the c compiler but we have
clang so let's generate small stub
functions that allows us to to get the
corresponding I our code and then at
runtime during an association we let our
runtime system reading these stubs
pre-process them a little to get that
the actual implementation the code
inside there these stubs and then when
we have have a trace we traverse the
trace and glue bodies there and then
when we have a big function then a
consisting of all these these things
then just hand it over to l of m for
optimization and a to code generation
and here's some example of these stubs
so this same old implementation same old
instruction here we convert now each
block here into function so we have one
function for each block that one that
one and that one to make it simple for
our work when we actually try to extract
the code we let them this dubs return
either if the end with a conditional we
let them return the result of that
conversation or if they transfer code
somewhere or to transfer control
somewhere we let them reach just return
the new instruction point deduction
point
and when we glue these functions
together which corresponds to basic
blocks in the interpreter we have to
make sure that we are still on the first
path the path we traced and it could
happen that we actually fall off that
pause if for example we reach the end of
a list then we should probably not go on
as but that word as if we were still
traversing the previous elements in the
list so we have to when we glue be basic
blocks together and we encounter a
conditional branch we have to make sure
that we that the result of that
branching is the same as before and we
do that by inserting conditionals that
were still on this false path otherwise
we fail and return the identity of the
basic block where we should continue
execution inside the plain old
interpreter so assume this trace
function is our compiled trace I've
omitted the end of it i just have the
start part which is this test we do the
test here is it so that x0 will equal to
this constant we get from code sorry
this means if that test is true that
means that we are no longer on the first
pot which means we should should crash
out of the native code and go go on
executing in the interpreter and in that
case we just return the identity of the
basic block where we should continue
execution
I've shown this as C but I actually
stand on llvm I I or but then no one
would understand it now remember that
this is a trace it's code where we
actually follow a previous execution
that means that we can optimize this
pretty heavily by exploiting the fact
that at each step in the trace we know
what our instruction pointer is and if
we for example at the start of each
basic block insert assignment to our
instruction pointer to a constant which
we get from our trace then the constant
the constant propagation inside alluvium
can propagate that and if we also teach
our optimizer that anything access
accessed by a constant offset relative
to our instruction pointer is a constant
it is because that's code we haven't
changed the code means that that means
that we have a great amount of
optimization we could do so if we look
at this example you see there there are
lots of stuff we're accessing relatively
the code pointer but if it turns out
that is on constant and also the stuff I
points to is constant we can reduce all
this stuff to just a single test a 60
equal to 0 that's all we need to know
and that's pretty or optimal that's very
close to what we actually need to do
that means that there is only one step
left namely how when we have compiled
this genetic code
what should we do we have to execute it
in some way and that is added
complications as the interpreter keeps
much of it states in local variables
they are not visible if you do a
function called somewhere else so we
need to wait past that state to the
native code we could do it by collecting
all the states in to see struct and then
share that through a pointer between the
interpreter and the native code that
turns out it's not very efficient as it
turns out that the compiler tends to
introduce too many redundant stores into
that structure even if it's a value that
no longer alive and no one cares about
and it really hits us hard on list
processing then we have almost an
overhead of two hundred percent or so so
that's not a good solution but a better
solution is actually track what is
needed by the native code and then copy
those particular values over and we do
that through marshalling the state
through a specialized buffer which we
use to initialize the local variable
scenes inside your native code and we do
the opposite when we fall back and it's
here we actually use the lightness or
less than analysis we did on the control
flow graph to actually know what we need
to copy both when you we enter enter
native code and when we exit from
ethical so we only copy these things
that are actually live and of course
like anything else for this prototype we
generate that we can extract that and
generate just from the interpreter
source code
so does this work yes it does I but only
just I had the first working version and
week ago today so then I execute my
first Victoria only there are quite a
lot of limitations only unicode it gives
a fairly need tracing system traces are
never G seed so I haven't dared running
the test suite on it yet because I run
over out of memory it lacks these
previous optimizations I described the
lightness analysis fairly conservative
so sometimes a copy too much and
therefore I have decided to not run any
performance measurements yet as I can
only be worse than the interpreter right
now there's quite a lot of stuff still
to do I want to implement these
optimizations I described previously I
want better management of the traces and
compile native code mainly to limit
their memory use I want to improve the
lioness Ilana analysis i also want to
actually understand code updates so
right now we just lose the native code
and it's just garbage I want to do real
performance relations I also want to add
SMP support and all this has also all
only concern yourself with an inside the
body of the main interpreter loop I
could use the same method to actually
get into into and inside bit so it
should be in the same way possible to
optimize and for example the big enough
math and things like that inside this
so fairly straight forward and extension
so that was that but before going for
questions I like to thank ericsson for
financing me to allow me to actually do
it such cool hack on my day job and also
i want to thank you know strawberry for
the title of this talk so questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>