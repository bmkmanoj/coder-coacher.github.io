<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Staying Afloat with Buoy A High Performance HTTP Client - Louis-Philippe Gauthier | Coder Coacher - Coaching Coders</title><meta content="Staying Afloat with Buoy A High Performance HTTP Client - Louis-Philippe Gauthier - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Staying Afloat with Buoy A High Performance HTTP Client - Louis-Philippe Gauthier</b></h2><h5 class="post__date">2017-05-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9p_qNtsazeQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone
so I'm Luisa difficulty I'm director of
platform engineering a tad here today
I'm going to be kind of unveiling my new
httpclient I've been working on for the
past three months vote on and off so
let's start please the name of the
client is Bui so the heads all days is
basically why I built this a little bit
of a story so in 2011 when we started
building our platform using Erlang the
first client we use was called eyebrows
which actually still exists and is still
active which is pretty impressive I did
look at the issues list and my issue
from 2011 about leaking connections is
still there so I don't know if you
should use it or not in production but
we'll see later that it actually does
fairly well in the benchmarks starting
2011 fred's threaded bath used to who
used to work with us at add year but
decided to leave was or actually built
something called discount that he
presented here a couple years back
discount was a way to add back pressure
so that we don't get overloaded by
sending messages to processes and so in
an effort to scale or HTTP client we
fort LH CPC and added a pool mechanism
using discount when Fred left in 2012 or
at the end of 2012 I took over his fork
so at this point it was a fork of a fork
of a fork and I kind of changed to
everything to use binaries and also to
not spawn a process on each request so I
introduced something called like a timer
killing functionality pool to receive
timeouts anyway it's it's super
complicated I do not recommend it for
anyone and so this year finally we've
been kind of hitting some walls because
of
partners that would go down or be too
slow so because of the functionality I
didn't want to add more to it I decided
to just reimplemented new one so in 2017
we're going to start using Bui currently
we're not using it yet in production I
have done test and prod but just because
of lack of time I haven't had time to
deploy it so just to go back to the
problems for DL htpc one of the biggest
problem is that it does not support
pipelining so with the model of
discounts discount adds back pressure
like I said but only allows one request
at a time to go to the worker and
because of that when there's more than
one request sent instead of climbing out
or waiting it just returns error busy
for now we'll see you later on with
benchmarks I actually found that in
really heavy condish conditions there's
actually like 97 percent error rate so
like almost no request goes through
which is really not efficient the other
problem was it reconnects on request
so whenever connection closed and the
other server is unreachable or timing
out we just hammer it until we can get a
connection but often we just end up
running out of like file descriptors on
the other side or causing more issues
and yeah when we forked it initially we
did not fix the test so when I forked it
again I did not fix the test and the
test suite is just a mess and there's no
way no ways to know if the code is
actually okay and basically it's just
hacks on acts on hacks so we decided
it's time to write our own so Louie's
architecture the architecture is really
simple basically if you have a process
that wants to do an HTP request it has
to call a module called Bui pool utils
which will do the translation from the
URL to the pool name
which is a shackle pool so I'm going to
explain the after lifts what shackle is
if you were here last year I presented
about the framework have been building
over the last three years but let's see
in a second so shackle framework so what
does it do if you have an application
you want to talk to an external service
shackle is probably what you need as a
basis to build your clients to have some
properties so before we go further let's
of service so a service and shackle it's
something that communicates over a
socket this could be UDP TCP and
recently we added SSL it can speak
either in a text protocol or in a binary
protocol and it can be a synchronous or
asynchronous protocol the global so why
did I come up with this framework
basically at Adyar at this point I think
we're up to seven different clients that
we maintain all the time it's very
painful to add an optimization in one
and then poured it over to the other
ones if you were in the previous talks
with the react upgrading to nineteen
dependencies become really complicated
and it's really painful to open six PRS
every time to kind of patch the
different places so the first goal was
really reusability so that I don't have
to reinvent the wheel every time I write
a client the second goal was speed and
by speed I mean low overhead so speed of
the request itself and needs to be as
low as possible the pool cannot add more
time we don't want to be stuck in a
queue and we want it to be as fast as
possible as optimized as possible speed
is great but without concurrency you're
very limited so if you can do one
request in one second and you have ten
cores then you can do ten requests per
second but if you can do ten concurrent
requests now you're up to 100 requests a
second so it's really important to also
think about concurrent
see which Erlang is great at so it's
sorry it's it's an important concern for
me to keep it as highly concurrent as
possible without adding any locks or any
kind of bottlenecks in the code
finally safety so safety is two parts
one part is the back pressure mechanism
that's implemented in shackle so if
you're asking a million requests a
second to your to your clients you don't
want to queue up too many messages in
the queue so there's a lose mechanism of
a backlog that will limit the number of
messages you can send and therefore
you'll never end up running out of
memory so that was one of the problems
that we actually solved initially four
years ago with discount and this is kind
of the evolution of it the other safety
net is the fact that everything is
really there's error cases for anything
it should never crash I know you've
probably heard talks about let it crash
it is a great functionality of Erlang it
makes sense for parts of systems but if
you're doing high-performance Erlang and
you're trying to keep your system up and
running
often the crash pad will be much slower
you'll be logging you'll be getting a
stacked rates you cannot do that 10,000
times a second so a little overview of
shackle so last year when I presented
these were basically the the features
that were supported so it's multi
protocol TCP UDP there's a very fast
pool implementation that uses some some
tricks that we'll explain later
supports round-robin and random
selection of workers it supports request
pipelining which is very important if
you want to have high concurrency
supports back pressure of your backlog
so it's perfect from out of memory
errors and finally there's a smart
reconnect mechanism so the
exponential back-off and the connections
are remain team by the client it's not
on request basis if we look at this
diagram that's basically how it works
right
well last year so you have a process if
it wants to do a request it's actually
going to talk to ETS first ask about the
pool options from the pool options it
will find out what the strategy is so
how to select the worker the pool size
and the backlog size from this
information is going to generate a name
for a server and then call ETS again to
check what the backlog size is if it's
below the maximum backlog size then it
will send the request to the worker so
when it does send them the the request
it's a message sent to the client these
client I'll show you a little bit later
how it's implemented and how we
implemented it Bui these clients are
just small servers using proc lid so
that they can be supervised and finally
they just send messages via sockets to a
service so that was last year this year
some features that were added during the
year
so now we have managed time out so time
I used to be just on receive you'd have
a timeout which kind of made it
difficult because if you call the call
or a cast the behavior was different
you had to implement yourself the
timeouts and which meant that you had to
clean up the backlog of yourself you
have to empty the queues yourself it was
complicated and error-prone so this year
now we use Erlang sent after for every
request and because of that everything
gets cleaned out properly and all the
calls are kind of symmetric in their
implementation we also added SSL support
obviously for HTTPS it's very useful
another feature is that we added a error
in the handle call back handle data call
back I'll talk about it a little bit
later this was also for Bui
and we did a lot of optimization one of
them is using grand or grand rule as an
if to do random number generation but
very fast not to not be used for crypto
please we added a cache so instead of
doing previously we would do a call to
ETS to figure out the pool options now
we don't do that
we use a little trick that will explain
later of a basically we compile the ETS
table into a module that we read at
runtime and instead of doing a call and
having synchronization between processes
now it's a module it's locked free and
there's a much lower overhead and the
other kind of major optimization
wherever we had a state that was a
record that would get mutated we try to
split these records as much as possible
so that as little as possible to be une
mutation so set element three call if
you write Erlang you if you you know
that this the the function call that's
called when you modify a tuple and this
is costly because it occurs a lot of
copying so if you can remove as many
mutation of tuples you gain speed
because of less GC less memory pressure
another change if you look at this graph
here
it used to be proc Lib it's still using
proc live but I add a layer called
metal-metal is a little server
I guess behavior I implemented that I
noticed I was implementing and
everything has been building recently so
I pulled it out it's just a simple
server behavior that is close to the
metal so today when you're doing an
actual call with shackle this would be
an example for Bui so you have your pool
name so this is what gets generated by
Bui
utils that we'll see later but this
would be the pool name beside you have
the message the request you're sending
to the server the next thing it does is
it tries to find a server to send this
to so it will call options will call
shackle pool util options to get the
pool options and like I said this is now
a module if we did the same factories
last year it would have been an ETF call
now from these pool options we know that
the pool strategy is random the number
of workers the pool size is 16 and this
is the name of the pool so we call a
random 16 which is backed by grand rule
now and we generate the server name
which is just in that Tom from this so
we get this name we call back lock check
with the maximum backlog size and we
just do an update counter to an ETS
table to check what the backlog sizes if
you're wondering what these options are
a little trick you can do to know if
your counter is going to go over without
going over so you can do two operations
at the same time adding a limit and
setting what the limit is is the team
where it should go back so doing this
call if it increments let's say you're
at zero they'd both be at 1 and 1 well
let's say you're at 1024 one would
increase and the other one would stay
the same so other features that are
upcoming because my co-workers decide to
keep using new protocols all the time so
SCTP network protocol is coming because
one of our services is going to use a
CTP now there's more optimizations
coming I really need to redo the test
suite it does an ok job at covering but
it could be much cleaner much more
complete and better documentation
I've been looking like a bunch of new
frameworks coming out for rust and stuff
and they have marketing materials and
documentation with fans
images right now I just have like a get
up read me so I need to step up my my
documentation game just a little note on
the usage so we use it really
extensively adhere that was really early
this morning so the requests are not too
high but so I've at 4 a.m. I guess it
was 2.5 million requests per second peak
I think we we can reach five 5.5 million
requests a second with which shackle we
have a bunch of different clients you
can see or maybe you can see so for some
of the requests it's even in
microseconds so speaking to our IP lists
service a request takes an average or
that's the 95th percentile actually two
hundred eleven microseconds so I do
spend a lot of time optimizing this code
performance is really important for me
so and especially when you're doing that
many requests per second so that's it
for the shackle intro now I'm going to
talk about Bui how its implemented using
shackle so to implement a shackle client
you have to implement a behavior that's
defined like this so the first thing is
there's an init state the init state
will initiate basically the state of the
client so if you need a record to keep
the state or if you need to do any kind
of operation this will be the first
thing then there's a setup phase so the
setup states will receive the socket and
then that's when you can set up stuff on
the connection so if you're speaking
Cassandra for instance it could be that
you set the key space so a default key
space for the whole pool would be done
in this space after that the two really
important ones are handle request handle
data so handle request is the the
callback that will receive the actual
requests from shackle call or shackle
cat so that's the one that's going to
serialize in
to the protocol that the service expects
when the service receives the request
sends back the response the response
goes to handle data so handle data will
be decoding the response and sending in
the information back to the colleague
finally there's terminate so whenever
the connection closes it calls terminate
so you can clean up the state you've
created and in it so for a Bui
it's fairly simple so in it just create
a new state in the state what we have is
a buffer so it's just a binary buffer
because often TCP well HB reef less will
span multiple TCP packets so we need to
buffer in case it doesn't fit in one and
then we keep two counters so requests in
and requests out and these are used to
match the request with the response for
the Kali so if your protocol allows you
to send a request ID with it and when
you receive it back you can do that
Association it easily but for HCP you
cannot send any data and receive it back
so since it's supposed to be kind of
linear so you send your suppose we'll
you're supposed to receive the the
response and order what we do is when we
send a request we increment the counter
at request out and when we receive
requests we increment requests in and
this is how we know the right ID for
each request this only works because the
ordering is supposed to be maintained as
the order is not maintained that this
doesn't work and you need to send a
request ID in the protocol an old
request so handle requests like I said
receives a term which will be encoded
into the request you have to send to the
server so in this case I do all the
encoding of the request outside of the
sir
so that the server can stay as lean as
possible and only process requests
ending as fast as possible on the socket
the only thing I do is I request ID and
I I just keep it under 32 bit for no
reason because I copied that for another
coin I think but it would work without
this and just sending the other ID
actually this is a low opportunity to
optimize the code and then what we do is
you send okay the request ID the request
and then you can update your state and
once this done so the the request ID
will be will be queued up and then we'll
yeah the request ID will be queued up in
the state updated and you'll see the
next is handle data so an old data will
receive a HTP request that knee it needs
to be decoded so we'll take the buffer
keep the buffer add the data to it or
append data to it and then call
responses and from there this is
basically the all of the code in the
client responses we'll just call buoy
protocol response to decode as a
response and either it gets a response
right away or it doesn't not have enough
data so we need to buffer for the next
time we get a packet if if for some
reason the request is invalid we'll
return ear error reason state which will
actually shut down the request right
away and then sock it close to all the
the caller that are queued currently on
that worker so previously that's what I
was talking about this is the new state
that you can call on that callback for
handle data that was added for a buoy so
that if the protocol if for some reason
you're talking to the HTTP server that
doesn't respect the specs or set or
that's corrupted through a proxy or
something it doesn't bore
all of following requests buoy protocol
so encoding a request I really kind of
went basic here what we do is we
calculate what the content list content
lint is which is the the size of the
body the body is a nihilist because i/o
lists are great for high performance
because you don't need to create new
binary so basically whenever you have a
bunch of binaries that you need to glue
together instead of appending them
together you can use an i/o list and
generate it without incurring the cost
of creating a new binary it right
because the parsing it will see the
parsing code after but the parsing works
on binaries unfortunately you can't do
binary matching on Io lists yes so it is
at that moment right but I I it's like
one step before after that what we do so
we add complement to the the list of
headers you'll notice here we hard code
connection keep alive this is one thing
that's really important Louie is made as
an HP Klein for like high-performance
between servers if you're not doing keep
alive there's no point of using this
you're wasting resource opening
connections all the time
it's not going to work great your logs
are going to be full of errors asked
like trying to reconnecting all the time
so this is one of the things I decided
when building this keep alive only it
supports only it should be one point one
don't even try with one point zero it
will tell you it's not supported and
then if formats the headers again this
is just kind of a map over the headers
and it builds an i/o list that then
is put in this big Iola so it's really
really simple and we don't create any
binaries at all during this the the
coding of the response so the HP parson
I wrote is very small it's about 150
lines tops even I think it's a little
bit less than that I evaluated a lot of
parsers that exists already most of the
newer ones go with the streaming
approach there is value to a streaming
but the problem with streaming usually
is that you have to maintain the state
and maintaining a state usually means
that you have to mutate something so I
did benchmarks against acnes parser
that's basically Cal libs parser with
some modification and really like you
could see well - ten times faster for
the same requests and basically it's
just because of mutating states and
always calling the same functions over
and over because you need to keep a
state thing okay like one header was
parsed another header was parsed another
and you're just looping always until
it's done
instead of doing kind of one big
function I I split mine it is a
streaming ish I do the headers and then
the body but I don't do ten passive
headers and I continue that way to
minimize a function called minimize sub
binary creation and minimize set
elements calls another thing I do which
is to minimize sub binary creation is
use binary splits and because of speed
binary split ends up being faster than
actually just binary matching on new
lines and iterating until you find a new
line I did some benchmark just to make
sure it was actually faster before the
stock this morning because somewhere
else on the internet I read that it was
slower
so I didn't want to say stuff that
wasn't true so I did benchmark these to
a simple kind of minor match that we
iterate until we find / r / n and then
return and the same kind of function
just using binary splits doing it really
a fast kind of micro benchmark using one
of my libraries I'll talk later the
split takes on average sixteen point
three microseconds yeah the split with
hype yeah this is wrong should be okay
so split two should be sixteen so the
fastest is spidery split split with hype
is a little bit slower and split is much
slower
I'll fix these numbers after but I'll
talk also about timing HDR it's a little
library of wrote to do micro benchmarks
and this is basically the graph that it
outputs if you do using the HDR esta
grams you'll see that split two is a
little bit faster split hype is close by
and then split is slower at least over
like the 98 percent off so to continue
with the protocol it implements a subset
of the RFC 2616 some of the unsupported
features some of them I do not plan on
doing ever so arbitrary number of new
lines and headers so if you're using
some weird web server that just adds new
line and enters at the end it won't work
I've tested with most traditional web
server like Apache nginx whatever it all
works but if if you're building your own
and you're doing some weird stuff it
probably won't parse capitalization of
headers so if we're looking for
continent it will be all just the first
letter capitalised
or none of the letters capital
if you're doing random it won't work
there's no protections against malicious
servers this is meant to be server to
server speaking to a partner or someone
you trust if they send you I don't know
count little end of six billion then
you're going to read six billion and
you're going to run out of memory and
finally chunk encoding for now is not
supported this is kind of because
shackle can't support it currently if
there's enough demand for it all
implemented so one of the cool
optimization that has been done in
shackle and also in Bui is something I
bought and this is called Bui compiler
but basically for states that are only
read-only that usually I would store in
ETS I move them out of ETS and I use
Earl syntax to actually generate ASP
on-the-fly and compile them there's some
nice tricks that you can do so this is
kind of the code example so my pools are
in my ETS table pool I do tab to list on
that from there I send it to pool you
tools form which will generate the AFT
and I send it to compile and load forms
which will load the new module and this
has been working great in production for
about a year and a half now so it's
fairly safe I haven't tested on like
super old versions of Erlang I know it
wasn't
I think pre 16 module were not loaded
asynchronously so maybe that would be
problematic but in the latest
erlangs it should be okay I use Earl
syntax to generate the ast it's low
verbose but works fine if you're using
if well if you don't care about
backwards compatibility I think there's
X Merle now or komuro
to do this which is a macro library on
top of Earl syntax you can see here this
is basically the module definition so
it's building a module called pool you
chills this is the export list though
it's going to export one method called
name with three arguments and this is
going to be the function definition
itself and then I'm just going to
generate the name clauses from the pools
which is what comes from ETS and the
compile and load forms is basically a
compile the forms from that AST it
generates a module and a binary well the
module which is the name of the module
and a binary we purge the all module
create the the module name because we
need a module name to do load binary for
some reason and then we load it
dynamically at runtime Louie's
performance so like I said performance
for me is really important one of the
tools I have is I have a make profile
target so I can run whenever I do a
change I I type Mike profile and then I
can look at what change and see if
there's an effect for this profile
target I use F cross X which is a fork F
cross that adds suspend and GC time and
from this output I use K or Q cash grind
to visualize the output so this is kind
of what it looks like for my make target
I'll just call so I have a module to do
the actual profile call I run it it
outputs the file F prof analysis I send
it to Earl grind X which is the utility
that comes with F prof X to transform
the output in something readable by K
cash grind or Q cash grind and what you
get is this nice little graphical output
so I can figure out which function was
called all many times the included time
of all the things on the stack below and
how much time was spent on itself it's
also great for visualization you can see
really visually where the time is spent
also gives you these nice little graphs
where you can see all the Cawley's and
how it functions so earlier I showed you
guys the shackle call so shackle call
Kolchak oh cool server pool server
called server index back lock jack pool
options and the random call so you kind
of see where the time is spent and it's
great this is same output for the server
so I said we're using metal now so
there's metal the metal loop which is
implemented by shackle TCP server
message and then there's the three
branches of where work is actually done
so the TCP send so this is sending data
this is receiving data and this is
processing response another tool we have
well we use that add gear is a extra
spec beam it's it's been written by
Julien Squires at EDD gear he presented
it a EUC last year a really cool tool
that works on well fortunately or
unfortunately only on Linux uses perfect
on Linux to sample what's happening in
the VM so it's pretty easy to use you
can use there's a utility called Erlang
right perf map to write the perfect map
to map the functions in the VM to perfe
vents and then all you have to do is do
perf top and the pit and you can see in
the VM itself what's happening but also
in your beam so there should be like so
you can see shackle talk head call but
you can also see a lot of allocation and
scheduler getting data and stuff that
normally you would not have any
visibility on this tool also comes with
something to just sample Erlang so with
this tool you can do Erlang sample only
Erlang and passive FID and now you only
get the the Erlang functions you can see
like I said I use binary split heavily
in this it looks like that would be
kind of my next optimization would be
either to get rid of binary split I'm
guessing I could write like a simpler
NIF to do this work although I do not
want to introduce more NIF's in our code
so we might just have to live with it so
the fun part benchmarks the I built a
suite called HTPC bench a youth this
project called timing which is on my
github I had it a new module in there
called timing HDR which uses HDR
instagrams that was initially written by
Gil teen from Azul systems that was
written in Java there's a seaport that
was written a couple years ago and I
think a year or two ago there was an if
written to wrap that C code this is
actually one of the only ways I found to
do kind of scientific histogram it's
really hard to get it right I trust that
the site or like the stats behind it or
right and so far I haven't found
anything that proved it wrong this suite
is on github if you have an HP client or
you think you can improve the results of
another clients feel free to send a pull
request I really want it to be kind of
as representative representative as
possible for HP clients the client side
benchmark this year so DL HVC to acne
162 HTPC which is the Erlang client
eyebrows capital and Bui the bench
server so I use two of our product
serves that are not in rotation yet so
they're 40 Hz on course 256 gigs of RAM
SSDs and the latency between the server
is under 0.1 m/s and I realized this
morning I was wondering Erlang 18 3 and
not 19 I'll rerun the benchmarks to see
if it makes a difference but hopefully
it doesn't
what HP HD PC does is it will do a
number of iteration and it will do for a
number of concurrency so a number of
process doing requests or doing in
operations for those iterations and then
there's a there's also pool size so how
many connections should be open how many
workers and the client should be there
so if we look at the LHC PC we start by
a pool size of 8 concurrency 32 I can
get 16,000 950 requests a second and the
error rate is 91.3% so this is awful
like 91 percent of the requests do not
get a reply you'll get just error busy
this is how this count was implemented
it's unfortunate but it keeps the
latency really low for the few requests
that make it which I don't know I guess
it's good but kind of bad you'll see
that as the the pool size increase well
you'll see as the concurrency increases
and the pool size stay the same the
error rate increases because there's
more and more requests pass but the
number of workers does not increase the
more you increase the pool size you'll
see the error rate dropping lower and
lower but there's always a threshold
where it can't handle as many and
although increasing the pool size seems
to be helping eventually will be a
trade-off we're having a million workers
is not going to okay not going to help
out apparently there's not much time
left so I'm going to go a little bit
faster for acne I think the problem is
there's no pipelining so the results are
not super exciting HPC pretty brutal
eyebrows does okay for a fairly old
client I think max we can do is 55 or 58
thousand requests
second but increasing the pool size does
not seem to help it actually decreases
cat opposed so that one is actually a
port driver rapping lib curl it does
okay with a small pool size in the other
one full size also equaled connections
number of connections in this one I
cannot control the connections I think
there's a patch that was merged that
I'll be able to do it but until then the
benchmarks is a bit flawed for this one
but if you have a pool size of eight you
can reach up to 96 thousand requests a
second which is by far the the fastest
of all the other ones but as you
increase there's no increase it actually
decreases so it's really I think you hit
a bottleneck serializing between the the
sea port and the VM finally buoy so the
exciting part everyone's waiting
so for pool size of eight you'll see it
gets overloaded at maybe like 512 so it
decreases the more you add though the
more it ament's you can see with 128
pool size concurrency of 248 requests
per second is 604 thousands a second
which you can see with this nice pretty
graph increasing the second number is
the pool size so increasing the pool
size usually will make the latency
better because there's more well there's
less requests in the queues and
therefore they can be answers quicker
final results so buoy the max in this
benchmark was six hundred five thousand
second which is six point three times
faster than capo eight point six times
faster than DL htpc ten point two times
faster than out grouse eighteen times
faster and forty toxic forty three times
faster than H CPC which i think is
pretty cool too
get these results the three are that you
get 10x on anything and some tips and
tricks' pattern match everything it's
really one of the most efficient ways of
parsing or doing anything so as much as
possible usually I try to pattern match
using i/o list I said it before I keep
your server loops lean so if you have a
loop that's receiving messages
specialize your loops don't do one big
loop that will receive all the messages
the specialization will be more
efficient because it does not have to
match messages that it doesn't need to
see use ETS or shared States compile
magical module of modules for read-only
States instead of ETS cool trick
actually learn working on this is
there's been opt-in flow compile option
that will tell you if a sub binary will
be optimized because if you're creating
sub binary the compiler can be smart
enough sometimes to optimize and not
create the sub binary it will tell you
if it's able or not to do it so if
you're creating sub binary try with this
option you'll see maybe the compiler can
optimize it and you might be able to
change your code to optimize or measure
out everything intuition is great but
often it's wrong it's much easier to
just run a small benchmark and figure
out what the reality is some links I
have a lot of projects on my github
julien also has some interesting stuff
i'll be publishing then of the slides on
my twitter after so if you have
questions or you want to find these
projects still free to send me a message
and thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>