<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Mark Priestley - New Problems, New Paradigms | Coder Coacher - Coaching Coders</title><meta content="Mark Priestley - New Problems, New Paradigms - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Mark Priestley - New Problems, New Paradigms</b></h2><h5 class="post__date">2016-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AZZThgj4Jw4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'll just say a couple of words about
myself I start I've been a programmer
I've been a lecturer and software
engineering computer science the last
few years I kind of got interested in
history and I've kind of migrated into
being a historian of computing and when
I got into this field it kind of struck
me that the history of computing is so
much about the history of computers and
so little about the history of
programming and software without which
computers are just heaps of junk so my
kind of little mission in life really is
to sort of get the history programming
and history of software more mainstream
were kind of wherever I can when the
invitation came I was kind of writing a
paper about what is a software para your
people talk about different software
paradigms functional paradigm obvious
oriented whatever so I was writing a
kind of um a paper about that and this
is this is not the talk about this paper
this is going to have a more
lighthearted sort of talk but it but the
work it comes out of is questions around
programming styles language paradigms
what are they and where do they come
from and it's a kind of early project I
don't really have kind of
philosophically satisfying answers to
those questions I'm sort of gathering
case studies and examples and materials
and what I plan to do in this talk is
just sort of go through some of the
origins of functional programming which
seemed like an appropriate sort of topic
for this conference was like I
understood it so this talk is basically
a historical case study of looking for
the origins of the functional
programming paradigm now obviously other
people have thought about the history of
functional programming but they tend to
be practitioners like I think these two
people both language developers so Paul
Hudak if I'm right is the co-developer
of haske how many years ago and
practitioners have a particular kind of
view about what history is how we ought
to be written what sort of stories you
tell in history so Hugh Dack says indeed
the lambda calculus is usually regarded
as the first functional language
although not of programming language and
he says in any case modern functional
languages can be thought of as
non-trivial non-trivial embellishments
of the lambda calculus
technology ain't here that sounds more
like it yeah my good friends yeah so
Paul Paul to Dax view is it's a lot to
do with the lambda calculus and it's
kind of keynote thing at the end of that
paragraph is functional language that
are non-trivial embellishments of the
lambda calculus David turn that starts
off by saying the first functional
programming is a programming languages
it's gone African the first functional
programming languages Lisp invented by
John McCarthy at MIT so these are sort
of things that people involved
professionals involved in functional
programming community say and you can
say a number of things about the kind of
history that practitioners right it's
well it's one thing is it's they're very
interested in first it's a kind of
backward looking history if you're if
you're an academic or computer scientist
writing new functional languages you're
in your interest your interest quite
properly is in the work you're doing
right now and your historical interest
is is how did we get here so it's a kind
of backward looking view on history
that's always looking kind of for the
first example of things working that
where can we assign credit to where do
people get influence and inspiration
from and I get a lot there's also a kind
of very traditional view that our Theory
comes first and then practice so
technological innovation it's often said
to be science comes first and then
science gets applied in technology and
that's that's innovation the same thing
is visible in these kind of little
extracts about the history functional
programming so Hugh duck there is saying
functional languages can be thought of
as embellishments for lambda calculus
the theory comes first the lambda
calculus comes first in the programming
languages are kind of the applications
of that it's a general thing I think the
scientists practitioners write this kind
of backward looking history but there's
another way of doing history I'm not
saying it's right or wrong the right
there were different people write
history for different purposes the kind
of history that you need when you're
building a scientific disciplines one
thing the kind of history you right if
you think of yourself as a historian
rather than a computer scientists do
something a bit different
so for example it may well be true that
functional programming is it's
understood now is theoretically rooted
in the lambda calculus but it doesn't
mean that the history where it came from
has got anything to do with the lambda
calculus at all or least not in the way
that people are now kind of present the
background to these languages so realist
so what I do is a kind of
forward-looking history you try and put
yourself in the mindset of computer
scientists 60 years ago who didn't know
they were inventing a functional
programming language they were doing
something quite different you know I
thought about titling this talk the
first functional programming language it
was not a functional programming
language because you know it can't have
in because there wasn't a category of
functional programming languages then
that it could have been one of so it's
this kind of forward history you don't
you never quite know where you're going
it's much more sort of tentative and
exploratory and in some ways more
interesting well interesting in the
enema holman so my story starts with in
1954 with those two guys I'm sitting
down playing chess the guy on the left
is Herbert Simon and the guy standing up
is Alan Newell who were very influential
in the early AI world in the mid-50s and
really when this starting point 28 to
December 1954 a report from the RAND
Corporation a I haven't really been
invented yet there were a number of
people doing things like working out how
to play chess and write love letters and
stuff like this but AI was not yet a
thing so they didn't know that we're
doing AI they actually had a different
name for it it started off with Newell
thinking about how you would go about
writing a chess program so some people
have written like baby chess programs
that could just about make legal moves
but not play a sensible game of chess
Newell was interested in what sort of
approach you would have to take to write
a program that would play a sensible
game of chess and he thought about it a
bit more generally than just jets and he
came up with the term complex
information processing so he recognized
that chess was just an instance of a
whole bunch of different problems and he
gave lists of things like learning
problem solving pattern recognition
little tasks like that that we here we
sold for long time ago and the key thing
about that was he thought that you
couldn't describe what you were trying
to do so the mainstream programming in
1954 is writing payroll programs on one
side or it's writing programs that
solves differential equations on the
other hand you know exactly what you're
trying to do you can write down the step
you can write down you know it's there's
no the algorithm the algorithm is not a
mystery getting it to work right might
be a problem but the algorithm is not a
mystery if you're writing chess programs
or pattern recognition programs or
learning programs the algorithm is a
mystery and you need a different that
they started from the I Dean you need a
different style of programming which
they called this is where heuristics
comes from and they came up this term
heuristics to describe this new style of
programming as opposed to the
algorithmic but the the category of
problems that are interested in with
these complex information processing
problems or in the lower quote they call
an ultra complicated programs and what
they worked out was in what Newell
decided was in order to address these
problems you needed a new kind of
programming and the practical example
made them famous wasn't in fact yes it
was a kind of fear improve on the first
theorem prover called the logic theory
machine and they called it a complex
information processing system and
basically fact the story goes at herbert
simon had a copy of Bertrand Russell's
principia mathematica on his shelf and
he was really interested in mathematical
logic so he said I know we'll all match
a seer improver and he got the book off
the shelf and swaps are betting ready
through the first 30 theorems and I know
what to do this you know and they
weren't when wrote a program I wrote a
program to do it and the famous there's
a famous anecdote where Simon comes into
a lecture at the beginning 1956 and says
to his class over Christmas Alan Newell
and I invented a thinking machine which
was just like they say machine rather
than program which is a bit of an AI
thing in the 50s but it was basically a
theorem a theorem proving program and as
a result of that they actually kind of
writing down in the stage in pseudocode
what this theorem proving program would
be it allowed them to articulate a
little bit more precisely their idea
about what constituted a complex
information process the kind of program
that's going to solve these ultra
complicated things it needs lots of sub
pro so there's lots of subroutines a
little different yeah
you're not it's not like it's not like a
strict subroutine hierarchy usually the
bunch of subroutines to do different
things the amended individual
subroutines can be quite simple a
subroutine to make a move in chess is
really simple deciding which move to
make is really complicated so you've got
simple subroutines we need to put them
together and really complicated
unpredictable way and the third one says
there's different levels you know there
are sub routines which look at other
subroutines to work out what the next
heuristic on the next strategy to apply
is and in nineteen for nineteen
fifty-six this is kind of revolutionary
because people obviously knew about
subroutines you know that subroutines
had been in programming since the adult
well since the year nineteen forty-five
but they were kind of big things and
they lived in libraries and you know you
you would spent like getting a book of
mathematical tables off the shelf if you
wanted to call a subroutine you had to
kind of go out and find about it and put
it on the tape that you fed into your
machine and all the rest of it you
couldn't just write a subroutine and
call it in a program so this this idea
that to write this they're proposing a
new style of programming that's going to
have massive certain massive amounts of
subroutines in it basically so year
later they'd actually written the
programming language which would
implement this program and they picked
out these two major characteristics so
bottom right here is a subroutine thing
flexibility in the specification of
processes it should be possible to give
a name to any subroutine use its name in
building other subroutines there should
be no limitation on the size and
complexity of hierarchies of definitions
it should be possible to define process
it's implicitly EG by recursion it
should the programmer should be able to
specify any process in whatever way
seems natural to him him in the context
of the problem so it's um you know
that's this really in terms of
programming practice this is really
quite a revolutionary thing to say in
1956 / 7 the top left quote is the other
big innovation about this logic theory
program they invented lists basically
they a statement in the propositional
calculus which is a logic they were
working is just a formal language
statement and we all know how to pass
these America cannot pass tree out of it
but they want to do that and they
invented basically invented
I'm a list data structure to store these
things in there's one of their original
diagrams about what lists look like
surprisingly familiar so that the top
one that got these location elements
each of which has got two words one
giving you the pointer to the next thing
in the list and the other one pointing
to a word that stores the data at that
point in the list and they wanted this
because as you as you as you are doing
theorem proving the you're working with
lots of formula and they change so you
might you might want to do a
substitution so you've got a simple
axiom and then you put a complex sub
expression into this axiom by
substitution you examines it got a much
more complicated data structure to hold
that that new expression which needs sub
trees and complicated things so they
needed these done to do this theorem
proving application they need a dynamic
data structure which would allow you
know all the things that we know you can
be done with lists and can't do without
lists basically so there's an example of
their kind of past tree if you like a
very simple propositional logic formula
at the top not P implies Q or not p and
they've got they've got two levels of
notation here they've got they've got
the notation on the previous slide which
in hopes notation in the previous slide
which shows you have a lists look in
memory and they've got this one there
which shows you the kind of abstract
structure of this simple little
propositional logic expression and no i
didn't coffee that someone the papers is
like there's a coffee where they show
you how the two come together and
exactly which words the connectors and
the variables are stored in but you can
imagine that so you say so yes 1956 this
this seems to me quite sophisticated
they're talking about sub elements they
got kind of dotted line around the thing
you might want to substitute or a place
in an expression as your proof carries
on so that all looks very modern they
invented this kind of language called
the logic language which became a series
of language is called the IPL event no
one's ever heard of that and it's a
different story altogether I logic
language IPL one
like that which looks really not modern
at all it looks like it is basically
machine code with a few mnemonics for
for instructions and addresses and
things so they got you know very
sophisticated about data structures and
program structure and a really kind of
primitive language but even within this
language note that this almost have this
thing I'm going to trip over that thing
as a talk so listen this is the
definition of a subroutine called CX and
the whole program is made up of
definitions and subroutines they've got
to quote summer weather say a program is
a complex dynamic system of subroutines
everybody else the time was saying a
program is a sequence of instructions
one of the idea so they've got a
completely different it looks like
clunky old fashion design code but
there's a really sophisticated idea
about program structure behind that so
that mad example there defines a
subroutine called CX and then here just
to make the point you know they're using
recursion to take the sub check for
equality down the tree give it again
down the expression tree and this
expression calls itself on the left and
right subtrees name so it's some you
know very innovative programming
practice in 1956 but it's not artificial
intelligence because artificial
intelligence was only just being
invented by these guys John McCarthy
Marvin Minsky the lesser-known Nathaniel
Rochester and Claude Shannon of
information theory fame they proposed
having a conference on which they
actually called artificial intelligence
in the summer of nineteen fifty six at
Dartmouth University in America and they
were going to spend a few weeks air
discussing what they call artificial
intelligence and at some put that so
Newell and Simon weren't involved in
this proposal and nobody seems to know
exactly when Newell and Simon got
involved with these guys but they they
got invited to New Orleans Simon got
invited to this AI thing at Dartmouth in
the summer of nineteen fifty six and
they can stole the show because they
were the only people who'd actually got
something working
if you look if you look at the detailed
proposals at mccarthy minsky rochester
and shannon made about what they were
going to do in this summer school none
of the mentioned programming that kind
of theoretical researches into language
or theoretical researches into
information theory i think rochester
makes some mention of programming but
the idea that AI was anything to do with
programming hadn't really taken place
yet people interested in neural nets or
people interested in information theory
though people interested in logic and
formal languages for representing
thought the idea that i was about was
about programming really came about
because of nil and simon turned up at
this thing with a working program that
prove theorems and you everyone else
would really annoy kind of stolen the
show here a bit but they got over that
and started writing i'm starting writing
other other kind of AI programs but one
of it one of the issues was this
language this terrible clunky machine
code light language that Newland Simon
had used as their basis for the logic
program no one likes it they turned out
that wasn't the problem because 1956 so
happens that 1956 one of the coolest
languages ever invented in the history
of software had just been published or
was just being developed and for time of
course was the first really the first
the first got high level programming
language really they actually gave
programmers are completely a notation
for writing programs in that would
actually work and more or less tape them
above a machine code level it was the
first one it didn't work the first few
years very well but you know it was the
gum at the first programming language
and it was often described as an
algebraic language which you know if
you've got your 20th cent 21st century
prejudices about for train you thinking
you must be joking you know what what
but what they meant about that was the
clues in the name Fort Tryon of course
stands for formula translation and the
one of the major design goals of Fortran
was to allow programmers to use
mathematical notation formulas you know
an ordinary mathematical formula Andrus
medical formula with plus and minus
divided operators function calls arrays
and stuff like that normal mathematical
notation in a program in a program
language and the the 1950s buzzword for
that was algebraic language which is
kind of you know less exciting than it
my son to us now but yea big quite a big
deal back in the day I thought so from
that point if you said so with the
example of Newell and Simon's our
working program and the hope that this
cool new language Fortran might provide
a better place to start a whole bunch of
other projects got started one of them
was this thing called it's a geometry
theorem proving system and the key name
here is the first name on that list
Herbert gelernter who had a very famous
son who did some stuff about you know
storing your entire life on a video
camera or something and what they did
what what is the USP of this program so
it was a kind of development of new
orleans Simon's theorem proving machine
but to prove theorems in basic Euclidean
geometry about shapes and so
representing the data it's not longer
representing a straightforward
expression you've got to find a way
formulating properties of a geometrical
shapes they wanted to use lists but
rather than use the rather than use the
IPL language that we decided we thought
we'd use this Johnny Johnny axial name
vote one of these old computers for use
for the IBM 704 but John McCarthy who is
M consulting to the project suggested
that Fortran could be adapted to save
the same per serve the same purpose he
pointed out the nesting of functions
allowed in the Fortran former that's a
function call makes possible a
construction of elaborate information
processing subroutines within a single
statement so basically what gelernter
his team did they made a bunch of lists
handling subroutines in Fortran and then
wrote this geometry theorem prover using
dynamic lists in Fortran thanks to it
essentially a subroutine library and the
Senate that the the good the selling
point here was that fort to an allowed
you to say f bracket G bracket H bracket
thing and then return the value you
could nest function calls with what they
really wanted to do here so you get
examples it's not very big font but in
the middle of that you can see
functions being called and if this one's
been called in Thai for its side effects
so the the value is assigned to a
variable called junk value the entire
function is B to 21 since we do not wish
further to process this number it's
discarded into a bottomless pit by
setting the function equal to a standard
variable junk but you know but yeah you
can see with with with with fortran
you're beginning to get bracket it must
be a lisp right you're getting brackets
so just a little bit of fortran eulogy
at the top fortran is an information
processing language of great versatility
and sophistication oh it's worth
remembering and just in case you didn't
see it there are two familiar functions
on that previous slide and disguised so
we've got one of them is called X could
or F and one it's called X car s which
of course the Lisp functions could are
in car I honestly can't remember which
bit of the Fortran rules meant i think
the f at the end means it's a floating
point function perhaps or maybe fixed
point guys for talking demonics never
remember what they were i've had that
fear reminding you of and the x is
something else so the axe in the air for
meaningless junk that four-ton made them
put in at the name car and kiddo which I
think referred to the two bits of the
two halves of a 704 instruction word are
in there that's where that terminology
comes from and there's some other
definitions in this fortran list
processing language of other familiar
lisp like functions okay but for trans
so provided some benefits but not all
and in particular we didn't allow any
recursion 1957-58 John McCarthy turns
his mind to the problem of chess and
he's writing he's trying to write a
program which tells you whether a chess
movie legal as it's not massively
ambitious program he's writing in for
trial and he runs out he doesn't like
the for trans if statement so he reckons
it in order to do this he invents this
thing which are all familiar with called
the conditional expression and
implements it as a Fortran subroutine
exif which if already ends with if so it
doesn't have to be exif
it can just be X if it's a subroutine
first one it's a kind of the predicate M
is the predicate it's 0 or 1 and the
value of the expression is either n1 or
n2 so that's the beginnings of the
conditional expression again a started
life as a Fortran subroutine by McCarthy
writing a chess program something to do
with chess and of course you know from
our point of view that's a bit odd
because you always evaluate n1 and n2
which I didn't find it quote when
McCarthy actually says that's bad but
probably he did something I was bad
later on the next year he McCarthy again
is writing a program to do symbolic
differentiation so again this is to do
with expressions and changing the form
of expressions not proof now but
differentiation and so he's not entirely
sure exactly what language were the way
there was in Fortran here or whether
he's doing some kind of pseudocode but
he's using lists lists to represent
these expressions and in the way that
Newland Simon had proved was useful and
feasible three or four years before i
was using conditional expressions to
define recursive functions which he
wouldn't he wouldn't been able to be
before was for Trentham have recursive
function calls but by this time he must
have worked out that you know recursion
his age taking recursion which comes
from again from Newell and Simon putting
it together with this conditional
expression it is invented to find a new
way of defining recursive functions and
as part of this he wants a function
which will apply a function to every
element of a list so if your
differentiate differentiating expression
if the top operator is a multiplication
and you do something in the next term
down is a plus and you do something else
my maths has gone a long time ago but
you work away that you work your way
down the expression tree applying the
same function the differentiate function
on all the sub expressions so he comes
up with this idea of a function called
map list which will undo precisely this
so in the middle of 1958 then we've got
five new programming techniques which
are come from various places invented by
different people lists coming from the
New Orleans Simon program Fortran which
gives you the idea that you can have
actual programming notation which can
doesn't have to be machine code it can
be can be expression based or statement
based or look like something that might
be comprehensible to a human recursion
again from newland you'll and simon and
conditional expressions and sorry i
should have said on the previous slide
the map list thing this is wary this is
very decides you want to use lambda
expressions to represent the function
that's going to be the argument to map
list he decides we'll come back to this
but this is where the lambda comes in so
we've got these five different
programming innovations in the mid-1950s
no one's talking about functions no
one's talking about functional
programming no one's even talking about
writing another language till the second
half of 1958 and 58 when McCarthy
decides to put this all together in a
new language and the other thing there
is it's you know it's the really isn't
the project here to invent functional
programming these are five useful things
that been invented on different
applications by different people people
finding them useful passing them around
from one language to another different
techniques in migrating and it's it's
it's it's the moyel near the the mangle
the process of people just doing
experimental creative research but of
course in McCarthy festo McCarthy is the
guy I think who puts it all together and
says ok let's we can make a single
language which will make sense of all
this so this is the very first memo
proposing what was not yet called Lisp
in September 1958 and I've cut and
pasted a bit here but you can see that
the section 1.2 of this memo said
features of the language and then the
five points there are precisely the ones
I conveniently during the previous slide
for you and that but but I actually made
that list before looking at this memo so
this is like empirical history you know
here's the evidence that proves i was
right all along that's not that's how
science works isn't it so let's carries
on there's a lovely dense slide view to
read but this is this is the this is now
from 1959 by which time it is being
called Lisp and this is the classic Lisp
definition of lists we're talking about
expressions and car and cooter in the
dot notation and then the the the comma
or well it was commerce then but then
the the more familiar list notation with
round brackets and spaces and here we've
got the three function of three famous
functions car cutter and cons so he's
narrowed it for you know from the if you
go back to the gallant no one which side
was it was this one there's a whole raft
of these functions not only could are in
car but cuts good as I hope there's our
range of different functions to for
these obscure things about the IBM 704
McCarthy whittles that down to just the
three functions car could or and cons
whose is dinghies ding you think some
you know work is thing that he's doing
to a work of refinement here this Lisp
is not this massive invention out of the
new it's it's it's a kind of tinkering
with stuff that's there in the
environment and he's putting it all
together making it work together
refining it in various ways here's a
nice little thing which is going back to
map list and they need to have some way
if you've got a function which has
functions as arguments you need some way
to represent those functions and this is
where we get lambda calc 1 what even
lambda calculus just a bit of notation
that you've stolen from this logician
Alonzo Church so the church Lando's it's
not not a reference to the religion of
phillip waters religion of lambda is
hector it's a famous magician was Alonzo
Church who invented the lambda calculus
potentially McCarthy says in order value
forms functional definitions in the
definition of functions which are the
arguments are map list it's necessary to
use functional abstraction as developed
by church we digress from the subject of
programming into mathematical logic in
order to explain the idea of functional
abstraction so so functional functional
lambda calculus is a digression from
from the work of building Lisp it's not
the thing on which list was built and it
is a digression so that you can Nick
this bit of notation this useful bit of
notation this there's the symbolic
differentiation program in Lisp the
bottom four lines and if you it's it's
actually kind of recognize I mean Lisp
is like Fortran that's rather charming
is
languages that go on forever you know
Fortran program now my son is just
started doing a PhD in astrophysics and
apparently it's for time hacking and you
can look at this and it still looks like
a list pro a lisp program you can kind
of understand what's going on the only
way the only about that apart from the
kind of cool thing of doing
differentiation in four lines that's
really they're just so you can see as
there's lambda in the middle of this
program so that's kind of like 1959 way
all right now I think so not but not
only is Lisp not only the lambda
calculus i digression in the creation of
lisp it's also not the whole story at
all lisp it's not as we all know this is
not purely functional language and so
again i think this is this is undated
this memo but i think it's 1959 issues
well possibly very early 1960 so talking
about that what became the prog feature
in lisp its objective is to add to the
system that report a program this takes
the form of a lying functions to be
defined by programs including sequences
of fortran like statements so just
normal programming with a sequence of
statements and a notation using a
semicolon surprise surprise to express
that in lisp another lovely another
lovely played football since all
cognitive computable functions can be
expressed in Lisp without the program
feature this feature can only be
regarded as a convenience however it is
a convenience in which we cannot afford
to sneer so McCarthy you know he sees
these may he's not a functional zealot
he's um well well aware of the
convenience and the use of kind of what
we know practical imperative programming
as well as the as well as the functional
programming so not only was Lisp an
imperative language not only was the
land occultist digression but McCarthy
did have some theoretical foundations in
mind when he was creating lists and they
weren't a lambda calculus it was rather
what's called recursive function theory
so we backtrack from home into the 1930s
in the 1930s there is a bunch of
logicians working out trying to give
mathematical definitions what
computability is so true
ring is a famous example with cheering
machines of course Alonzo Church and the
lambda calculus is obviously another one
known example but really the kind of
first example of this was what's became
known as recursive function theory a
kind of formal theory what how you
define a recursive function what it
means to be a recursive function what
class of things can be calculated by
recursive functions and the so good l
started this and then her brand and
clean steam the clean star in regular
expressions even other logicians are a
number of a number of people did this
and then church got involved in but the
lambda calculus is the kind of more
shoot cheering got involved chewing did
his work trying to you know just fill in
a hole in the corner of this theory of
computability so recursive function
theory is is the one that says okay you
want to define that for numbers you've
got zero you've got a successor function
and that gives you all the natural
numbers and then we can we can talk
about you can make functions by recur
she and you can make functions by
substitution that's all you need and
then we can get the whole of computable
functions from that tiny tiny foundation
McCarthy is trying to do exactly the
same thing but not for natural numbers
but for lists so instead of a zero and
the successor function he has the middle
list and the cons function and then car
and could her to kind of break these
things apart and in addition to
recursion and substitution he has this
notion of conditional expressions so so
McCarthy's theoretical theoretical model
of Lisp is yeah I think it's like I
think it's like 21 nil five functions
and three ways of building functions
recursion substitution you know one
composition and conditional expressions
and he says this is a way of well it
doesn't quite saying that but it's a way
of specifying all the recursive
functions have some bullets and bullock
expressions it's related to the lambda
calculus but it's a different
theoretical foundation lists erotical
foundations are not the lambda calculus
but it did bring the lambda calculus
into the game you know II reference
church he used a lambda keyword in Lisp
and as the early 60s ruled on the lambda
calculus kind of took off and became
really quite influential in approaches
people thinking about formal programming
language semantics and I think the
really really the application the lambda
calculus the programming language really
started with Peter Landon's work in for
the 1964 1965 some famous papers about
um the next 700 programming languages
which were all going to be sort of
unix-like syntactic sugar on top of his
language so you know modest guy okay I
think I've got five minutes left is that
right 12 I could go on I could have gone
long I could have taken more time ah
normal that's far too very mad a way so
this is actually my time I'm winding up
knows I hope you've got lots of
questions otherwise I have to go back
and show you some all of these slides so
that said that so that's the UM so
that's that's my story that's that's
really the end of my story about the
origins of Liske McCarthy wasn't
inventing a functional programming
language because that category of things
didn't exist it's not based in any
terribly meaningful way on the lambda
calculus it'sit's comes out of a whole
bunch of ideas something some
theoretical some practical and you know
obviously massively influential okay so
so winding back from the history to the
bigger the kind of meta story that I was
talking about here which is what our
language Styles the language paradigms
and where do they come from I don't have
this this is this is much more the
previous slides have been history which
is you know as accurate as I can make it
this is slightly more speculative now
but from this case study and a couple of
others I'm not trying to give a
philosophical definition of what a
programming language paradigm here is I
did a half a degree in philosophy and
that kind of gave me a lifelong aversion
to the concept the idea that you might
try and define any word well there might
be anything useful to be gained from
trying to make a definition of 0 because
all people do is then argue about the
definition instead of arguing about the
interesting stuff that lies behind so
I'm not trying to define I'm not trying
to define the word paradigm here
what I am trying to do is kind of
abstract from the history some of the
things that are involved in the making
of a new paradigm and what seems to me
clear in from from this the example of
functional programming the history of
which I've just gone through is that it
starts it's not a theoretical thing
leading it's not a theoretical advanced
leading to a practical application it's
not innovation in that sense it's a much
more messy complex ad-hoc sort of
process it starts with problem it starts
with a problem it starts for the very
practical Bob maybe maybe writing a
chess program is not maybe that's
practical problem in 1954 and it can a
week sense the special sense of
practical but it started from a very
particular programming problem and
things that people wanted to do that
we're kind of pushing the boundaries of
what was had been done in programming
practice how to program these complex
ultra complicated complex information
processes in order to work out how to do
this programmers came up with it rather
in a karateka old hat or ad hoc way with
a bunch of new techniques and that you
can't point to any one person and say
you know they invented functional
programming its techniques from all over
the place so it looked from the extent
which I've gone into the history so far
lists and recursion come from Newell and
Simon of their chess program other
things come from other people
programmers come up with new techniques
to solve these problems in a quite sort
of creative ad-hoc sorta way as
experience is gained people realize well
actually it'd be so much better to do
this we didn't have to kind of write a
buzzy fortran subroutine library we had
a really cool language that we could
just write this down directly and and
this is McCarthy's role in this process
i think is to package up these previous
bunch of innovations into a new language
a new syntax that would make it usable
would seem attractive would have no
doubt all sorts of reasons when it was
and the success it was but it's a kind
of package is in no way to kind of
minimize McCarthy's contribution to say
that a large part of all into inventing
list was packaging together
a whole bunch of different features that
you know some of which he'd invented
some witching happened it's different
you know there are two different
activities going on there and the fourth
thing that arguably might be involved in
what we think of as being a programming
language paradigm is a computational
lang a computational model that sits
behind it which crystallizes the new
idea and this came along kind of much
later so it was a 56 years later with
peter london i would say that people
started thinking about the lambda
calculus itself as a theoretical thing
which could be a foundation for
programming languages and i suppose
there's a counterfactual i would that
happened even if lisp hadn't been
invented who knows that's that's the
point about can't essentials it's only
the case if you look at some of the more
theoretical writings in the first part
of 60 don't mean Lisp is mentioned as a
sort of influence on you know in
discussions about semantics and things
Lisp is certainly there in the ballpark
so that's that's might so this is I said
this is that this is a rather tentative
slide saying these sorts of things seem
to be involved in creating new paradigms
or how we might think about what a
paradigm is and even more tentatively
bingham being assigned a scientific sort
of historian you can have you can think
about finest for another paradigm the
object-oriented one which kind of went
in the same as to the extent on I know
the details of history seem to go in a
similar sort of way I mean the
traditional history of object
orientation it takes you back to
nineteen 61 or 62 with I forgot his name
Nygaard what's the other guy doll doll
and my god I'm doing discrete event
simulation programs for which there is
no language it's something that's never
really been done in any you know it's a
new application and you know you can do
they start off doing machine code and I
guess or Fortran even worse sorry we
shouldn't sneer at Fortran John McCarthy
said
but it's a new problem and they they
come up some new techniques for doing it
the notion of an object or a class
whatever that exactly that meant in in
similar the notion of message passing
also concurrency was much stronger in
similar than it was in small talk later
on I think it's but it's you know a
bunch of programming techniques and the
history the history plays out slightly
differently here because darling i guard
seem to that in the functional
programming the history looks to me as
if you've got this second stage where
different techniques are being applied
in different languages at the same time
if in the object-oriented case the
history seems to suggest that these
techniques did come ready package in
similar that was the first kind of thing
people now want to call object-oriented
but in the same way that function in the
same way that the people who want to use
lists extended Fortran similar reason
it's just an extension of Algol 60 it's
not a brand new language it's it's a bit
like sort of music you know putting
lists in Fortran you put co routines
into Algol and that's what you get but
nobody really talked about object
orientation until small talk I guess 10
years or so later so small toy in this
analogy small talk is the Lisp where
some somebody comes up with a kind of
defining language that really gets
people's imagination says there is there
is something completely new here
computational model of object wanted
paradigm I'm not a person that can
really I know people have sort of tried
to make computational models of object
oriented on how successful they be not
what people what use people make them i
don't know but the interesting thing
there is I think maybe when we have an
interview with Alan Kay tomorrow we can
hear more about this but I think when
Alan Kay was coming up with small talk
he explicitly said in he gave some
presentation in the 80s at the history
of programming languages conference we
said that one of his inspirations and
small had been Lisp and what he was
trying to do with small talk has come up
with a language it would be as smoke you
know kind of capture the foundations of
this new can a paradigm in the same way
that McCarthy r done with Lisp you know
come capturing all the recursive
functions of lists in five functions and
three methods of function composition
apparently Carrie wanted to do for to
explicitly forsyth himself is doing the
same sort of thing for what was not yet
that well maybe it was called object
orientation by then so you know there
are differences there are similarities
but but you know these sort of things do
seem to be in the mix when new paradigms
are being discovered yeah so that's
that's me thank you for listening
curing those before death
I'll come back to that yet and I can
come back don't talk to that and another
question or I've got I've got a jacket
that's the variance an interesting
question so I'm going to start with the
observation she's easier to answer
you're absolutely right about cheering
and in fact the the the kind of but the
the very first presentation i gave on
this material was a present that work
what I was really struck by the way that
I've been looking at kind of online
men's programming in 1945 and then the
ACE report and it did it did occur to me
that cheering's kind of style i called
it then the programming style and it
particularly is used to subroutines was
completely different from von lehman's
and exactly as you say was involved
unrestricted very idea like in a way
everything's a subroutine featuring the
whole thing is recursive call
subroutines call it completely
unrestricted Soloway so you're
absolutely right I don't believe that
work had any you know historically it's
kind of dead end which I you know in the
space of a 45 minute talk I didn't want
to talk about anything I couldn't talk
about everything but also there's a
question about you know backward-looking
history which i'm not i'm not i should
emphasize i'm not denigrating any way at
all is looking for examples and
precursors and who might have done it
first the kind of history that i find
interesting and attractive is
forward-looking and the thing about the
turing stuff which i think is
fascinating and you're absolutely right
it's at that moment in time it didn't go
anywhere for the next 10 years
programming technique ideas about
programs ideas about subroutines were
very much in the von linemen model and
it was kind of you know so the new if
you want the new land slamming thing
it's like an independent rediscovery of
cheering's programming style in a
different historic in a historical
circumstance which made it possible to
go on from there into you know glossing
into a whole new language paradigm
whereas insurance for all sorts of
reasons no doubt that didn't happen the
cheering's work
I don't really have an answer to that
question actually I mean a lot a lot of
I mean I do history because I just I
just find it fascinating I mean I spend
a lot of time as a programmer around
programming languages in one way another
in my life and I do history jitter just
had to pure academic interest to be
honest and for me that's you know people
people give all sorts of answers about
you know you can learn from the mistakes
or you can you can people people you can
say you can you can look back in history
and find examples that great great ideas
people didn't have which didn't take off
of it at a particular time but may now
give us inspiration in two ways for now
maybe the cheering thing would be an
example of that perhaps if you know you
can can't actually imagine somebody in
1953 saying arse is rubbish you know
they hate these subroutines looking back
at the ACE report and saying all right
this is history but this is this is
actually giving me a really good idea so
that's one possible justification
another thing is you know the other one
is that you know the famous learning
from mistakes thing personally I'm not
massively convinced by either of those
and when I have talked about history to
students and people like that I've never
found that a terribly convincing way to
motivate them you know people tend to
like it yeah I'm an independence KY do
it cuz it's cool and I find it
interesting I'm afraid I can't really
give you a massively successful way of
motivating motivating of students to be
interested in this I mean there are
answers out there but I'm personally not
not massively convinced by them
I think another another thing that
people certified very interesting is the
fact that the sort of place as beta
structures are can be used represents
the alleged programs so there's this
sort of like lists the program and the
data no I don't think so I don't think
if you go back to go back we're back in
1956 here this is this is the one of the
logic theory programs and point three on
this slide is the component processes
are applied to the highly conditional
fact fashion in fact large numbers of
the processes have the function to
determining the conditions under which
other processors vol parade so you've
got subject you've got subroutines you
know at least knowing what other
subroutines around in the program and
calling them kind of dynamically and so
that there's that kind of you know
levels of some metis of meta subroutines
whatever I also believe it was the case
that a list and IPL I don't really got
an example of a later IPL function on
these slides I believe it is the case
that the IPL functions were themselves
to find those lists in some way but I
don't exactly know the details of how
that was done so that's I'm just quoting
where I research that in detail so i
don't i mean i think i think that idea
was there an IPL but i think the lisp
people really made it it really became
big deal for Lisp when forgot his name
Steven Simon Russell something like that
it wrote the first Lisp interpreter I
think and started defining app apply an
eval and it's at that point I think they
really starts it's okay we can you know
we've got this thing going on so again
it again there's a technique which is
from from IPL i believe and then focused
and given a point and made really cool
and sexy violet by lisp i think i need
to check the history that
yeah yeah yeah yeah yeah yeah
I think it's I think I think it's also
the idea that the program can kind of
name it name itself I mean the thing
about godell is it gives giving names to
expressions which are then the names are
in the data language so it's yeah sorry
looking anxiously yeah there's a crowd
of people by an open door</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>