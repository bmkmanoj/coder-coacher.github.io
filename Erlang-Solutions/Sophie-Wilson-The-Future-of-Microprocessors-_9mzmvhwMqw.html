<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sophie Wilson - The Future of Microprocessors | Coder Coacher - Coaching Coders</title><meta content="Sophie Wilson - The Future of Microprocessors - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sophie Wilson - The Future of Microprocessors</b></h2><h5 class="post__date">2016-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_9mzmvhwMqw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">since I started working in the industry
mic processes got around 10,000 times
faster so this talk is basically about
that and would it happen again when you
get 10,000 sounds from where we are now
so how did it happen how did we get
microprocessors that were 10,000 times
faster well it's all down to the
so-called Moore's Law the observation
that Gordon Moore made while he was at
Intel simply pointing out that through
enhanced processing and bettered bits of
engineering you could fit more
transistors on the same area of silicon
now it's not a law in the sense of an
actual science thing it's a an empirical
observation it just said well this seems
to be what's happening nowadays it's a
self-fulfilling prophecy
so the international technology roadmap
for semiconductors treat for filling
Moore's law as their raison d'etre so
you have a 300 billion dollar industry
that he's trying to fulfill Moore's law
and they Bend material science and
really arcane bits of technology to make
it happen and they've been doing quite
well since the 1970s sort of era they
really have been on track for keeping
the law running well be it we had to
change the words of the law a few times
along the way when you first propounded
the law it wasn't double every two years
it was double every year then he
adjusted it to you and a half an hour
two years it will keep running for some
variation of that so what does it mean
the law we can have more transistors in
a given area so on the wall behind me is
a plot of arm one
and in my hand is a piece of paper
bearing a plot to the same scale of
cortex-m zero plus in today's 20
nanometer process it's about 70,000
times smaller in area so that that's
basically what it means we can have a
microprocessor fairly decent
microprocessor and we can make it
jolly small alternatively we can spend
lots of transistors on doing things so
what does that bit mean what does
spending transistors on it
implementing micro processors mean
what's it all about
what are transistors how do we use them
so on the left-hand side of the picture
is a die shot of the 6502 it's full of
wild colors the wild colors are real in
one sense none real in another we've
done the wild colors by shining very
bright lights onto the silicon and so we
get all sorts of diffraction and
reflection effects so where we have fine
metal gratings we get some diffraction
where we have pieces of silicon we can
refract the light through it and get
different colors so you can see that
there's a green blob here that's the
6502 s instruction decoder and that
green is got a fine mesh of metal across
it and there's some yellow at the top
and that's silicon bulk silicon with
light being refracted to it so the nice
colors we try and make it happen
but it got it's fake and if you look at
a piece of silicon particularly nowadays
it's this copper colored dull slab
without any fancy colors so everything
on here is built out of transistors
uniquely on the right we have the
circuit diagram of the 6502 the actual
thing the engineer is built and we've
topologically distorted that to match
the dye shot so you can see this bright
green thing is this thing here full of
dots and rows
and that is as I said before the 6502
instruction decoder and you can see
structures around but it's not really my
purpose to tell you how to design a
microprocessor because that's my job
so what do we get with 4,000 transistors
we won't have to look at circuit
diagrams for the rest of the
presentation we'll be looking at block
diagrams so on the right is a block
diagram of the 6502 in the original data
book published all those years ago in
1974 and you get a sequence of blocks
connected by either thin lines or thick
lines thick lines represents bus with
more than one bit on it thin lines just
a bit and the blocks are variously
processing registers or function units
so that's what a microprocessor is it's
the set of function units connected by
buses with signals going along it to
cause things to happen so with the 6502
we had transistors that were fast enough
to make it run at two megahertz when we
built the BBC computer two megahertz
doesn't sound like a lot now but it was
quite fast back then it could do 8-bit
operations so the size of these buses is
eight bits and it would take two clock
cycles to do the fastest operation it
was capable of so add immediate nine to
the accumulator with carry and was a
fundamental operation of the 6502 and
that would take two clock ticks one
microsecond the thing was twenty one
spare millimeters in size a relative
giant but then it was built back in the
day when we called the process six
micron you notice I say six micron I
don't mean transistors are smaller six
rikon then much much bigger than the
smallest feature size on a process it
back in these days a transistor was more
like 60 microns across huge horrible
great big things
so that that's what that's what we got
now how did we use the 6502 obviously I
spent years slaving away writing in
machine code you could do fascinating
things with the 6502 in a BBC machine
brave Annabelle managed to write elite
in a BBC machine so that a full 3d
graphic space trading game coded like
that in those transistor but if you're
going to write a programming language
you might be interested in what it does
so the popular programming language
where I come from is C so we're going to
illustrate what you have to do to get a
6502 to execute a bit of C so the bit of
C's at the top left we've got three
integers processors lightning-induced
and we're going to add to them together
to get the third so what would a
compiler spit out to make 6502 that do
that well it spits out this long train
of things I don't intend to go into the
details of it it's perfectly accurate
6502 your machine code you'll notice
there's some repetition going on it
deals with all the addresses then the
addresses plus one the addresses plus
two and nearest verse three because it's
only an 8-bit machine if it wants to add
32-bit integers together it has to do it
sequentially so that takes it a long
time the 6502 was reckoned to be really
quite a fast machine for its hearer but
to do this a single statement from a
high-level programming language takes it
all those instructions which take twenty
six cycles best case to execute and yeah
fundamentally the reason is that we
don't have very much storage in the
machine and it can't throw it on very
big values at a time so Gordon Moore's
waves his magic wand we get some more
transistors we can go off and design a
microprocessor using lots more
transistors what do we do so on the left
is the Fuhrer's original pencil
sketch of the arm on one on the right is
the die shot hadn't come out that treaty
we didn't understand the importance of
positioning the lights correctly when we
took this shot you can see immediately
that there's something radically
different about arm compared with the
6502 however if I just knit back 6502
that's pretty much a sea of gunk isn't
it there were some structures like the
decoder but in there it was all dunk on
arm it looks quite neat they're
structuring they're from all sorts of
levels there's regularity this is
because arm has been laid out with the
help of a computer the 6502 was built by
people who sat in front of a big drawing
board and stuck Jubilees tape to
translucency to make the photo masks for
it so it was designed completely by hand
arm was designed by people who were
working with transistors directly but
with a computer to help them so the
computer like straight lines likes
laying things out
so what do we get when we use more
transistors we use 25,000 transistors to
make arm one we got a machine which is
full of all these blocks but this time
things are bigger 32-bit the buses of
32-bit single signals are just one bit
but that's fine with a 32-bit machine we
also spent some transistors on making it
just go faster so we can we can not only
improve the Machine give it more
registers let it handle 32-bit numbers
but we can burn transistors to make a
better implementation so where the 6502
which was quite hot through its day uses
up at least two cycles to do anything
we burned enough transistors on arm one
so that it could do something in one
cycle by which I mean of course that it
took at least three we burn transistors
in a pipeline so this machine can fetch
instructions decode instructions and
execute instructions while fetching the
next instruction decoding and so on so
there's a pipeline one instruction can
be completed every clock cycle the 6502
can do that it was the first
microprocessor I ever saw with any form
of pipeline it pre fetch the next
instruction but otherwise it's sequence
through multiple clock cycles without
running any power ilysm inside so arm
one is relatively large 37 square
millimeters that's bigger than the
twenty one square millimeters of the
6502 meant it was a bit more expensive
and it was built on a three micron
smallest feature size process so still
big hungry transistors as you said you
could see I mean the picture that we
took at the time that is a picture taken
with a camera with a lens on it but no
microscopes being used it's big so when
we look at our piece of program we can
see the arm is a magnificent advance
over the 6502 we don't need to do three
sets of operations we can get everything
done much quicker
of course arm which is a RISC processor
where RISC stands for reduced
instruction set complexity so it's not
the arm has fewer instructions than the
6502 it has a lot more but the
complexity of how the instructions are
implemented has been reduced and so one
of the things that arm does that the
6502 doesn't is arm restricts access to
memory to just load and store
instructions and then everything else
operates on internal registers the 6502
has much more complicated instructions
which actually add locations in memory
directly to things so you had all so
that in itself is three times faster and
I could just Lee claimed that that was
my responsibility to make it so of
course the fact that the transistors are
so much farthest faster that's good and
more responsibility so that ten thousand
times performance gain is brought it
like that the transistors go a lot
faster and that's most of the
performance gain and implementers like
me use the transistors in smarter ways
make the pipelining work better make the
processor work better and that's the
small gain so we could probably say out
of that 10,000 times gain 10x is down to
me and a thousand X's dad Gordon more
obviously we still make arms today there
are 95 billion arm powered chips each of
those chips probably has more than one
arm core on it so the number of ARM
cores is now essentially unknown we
don't know that at all modern
implementations of the processor they
vary from things that are as simple as
the cortex-m zero plus which is the
Neer's modern equivalent to the arm one
that we made all the way up to the
hurricane process of the Apple designed
to run in the iPad pro and the iPhone 7
actually there are pipette pros
currently limited to twister power Kings
only in the iPhone 7 until they 10x
comes out so by burning lots and lots of
transistors we can speed this up by
another factor of sort of four and a
half I did say that the processor
architects don't do well out of this
whereas the people who give us faster
transistors they're the real heroes so
the magic wand will be waved again what
can I do if I'm given more transistors
and I want to implement a more complex
instruction set so this is what happens
if I get six million transistors I
design a processor like the fire path
now it seems to gone back to being a gun
G splurge this processor is almost
completely designed by computers so
there's none of the sort of almost
anal-retentive for aspect of laying
things out in straight lines for this
the computer just puts the transistors
where it knows they'll be fastest we
have got some evidence of high levels of
structure in the machine and you can
sort of see that the top and bottom of
it look the same and there's these big
yellow things at the right-hand side
which are the register files and if you
think about it those register files are
now massive because this is six million
transistors and I'm using a jolly good
portion of them for the register files
so transistors there so what do we get
well for a start off were in a much
nicer process this was designed some
time ago it's in point one three micron
which at the time sounded very small
hard difficult to design but nowadays uh
so easy we use six million transistors
but the transistors have got so small in
the point 1 cm icon process there are a
fire path is only seven square
millimeters fractions of the size of arm
1 and the transistors have got really
really fast and we tried really really
hard to make this machine go fast so we
spent an awful lot of transistors on
making it go fast and they were fast
transistors so it runs at three thirty
megahertz compared with arm ones
pedestrian eight megahertz what it's
designed to do is very different however
arm one is essentially designed to run
sequential programming languages this
thing is designed to run parallel stuff
so this this line beginning AB W what
that instruction the machine to do is
one instruction one indivisible
instruction it's not multiple
instructions it's one thing so
so it says take the register pair r2r
see the register pair r4 r5 add them
together concluding considering them as
packed 32-bit words in a 64-bit
container twice over and generate to
64-bit answers on or r1 meanwhile take
the address in r8 and load two registers
from it are 6 r7 64 bits that's 1
instruction in the machine
ok so you can design with 6 million
transistors even if you spend a lot of
them on very fast implementation and a
shedload of them on doing lots of
multiplies you can design very complex
instructions you may have noticed that
there's no programming language in the
world that that's anything like close to
if you want to program this machine you
talk in its language you either talk in
fly path assembler or you talk in C with
intrinsic swear the intrinsics are the
machine operations and you're just using
the compiler to sort of automate the
register allocation scheduling you know
modern programming languages are scalar
basically they're very few that try to
be parallel at all let alone parallel in
the ways that machines like this can
handle so if we look at our piece of
scalar C well it's a bit boring the fire
path translates in something that looks
uncannily like the arm code there's
obviously a reason for that we can use
some of our path parallelism that the
first line LDL our naught and LDL r1
that's a single instruction to fire path
tells it to load two different things
from two different places it's fine with
doing that but then we have sequential
nature in the program that we're trying
to compile so fire path doesn't get to
use much of its parallelism and finally
only had one thing to store so in a
sense fire path isn't very at home with
writing scalar C the machine could have
done that instead where it did four
two-bit loads and for 32-bit multiplies
in a single cycle so fire path is really
quick the the machine back in 2003 would
do that in six cycles which was quite
performant and it's not surprising that
we could use that machine for a lot of
signal processing if you have dsl
broadband
coming to your house the processor that
runs the DSL broadband at the other end
of the line is the fire path so we get
more transistors what can we do with
them even I fail to design instruction
sets much more complex than that what I
can do with more transistors is to put
more of them down on a piece of silicon
so this is a picture of one of the first
fire path chips
the first massively successful one that
took over the world and now you've never
heard of it it's got to fire paths on it
one here and one over here I said about
the colors there's lots of pretty colors
on here caused by diffraction and
refraction this block of stuff down at
the bottom is data memory for the
processors the equivalent block of stuff
at the top is instruction memory for the
processors so this chip needs no
external memory and then the stuff up
there is what we call our Hardware
assist and that helps us with doing
crosstalk cancellation at the front end
of a DSL line but it doesn't do anything
else the rest of the functionality
making this machine run 16 lines of
adsl2 to your home is all software
there's no other hardware in the system
so that's what happens if if I'm given
enough transistors to do that in fact we
never built a fire path powered DSL chip
without at least two of the things if
I'm given more transistors therefore my
obvious step is to have more processors
so here we are a little way
we now have four fire paths
this thing's doing VDSL it does sixteen
lines of vdsl2
so that's a hundred megabits down to
your home 50 megabits back up it's still
you know still masses of memory
processors those little hardware
accelerator things that's what it's
doing can I keep doing this forever
can I keep just being given more
transistors and put more fighters there
oops there's a law in the way and this
one really is a law it's got a graph and
an equation and everything I mean how
else would you know it wasn't a law so
this is gene Amdahl's law it says the
sky speed up when you use multiple
processors is limited by the sequential
part of the program so if you think of
your program as having a parallelizable
bit and a sequential bit and you
paralyze the paralyzer will bit then the
sequential bit is what slows you down so
it's kind of obvious but it did take a
genius to point it out so the speed-up
for n processors is that for those not
used to looking at equations and
instantly knowing what that means in
real life let's take a program that's
massively parallel something like great
racing often quoted as something that
you do massively parallel and it is
indeed massively parallel the tracing of
a single ray through the scene is
completely independent for every ray
though of course the scene needs to be
distributed ray tracing is in fact about
95 percent parallel so 95 percent
parallel is the Green Line the Green
Line starts off at one processor as we
add processors the doing line Rockets up
yeah it's very tracing we can run ray
tracing on parallel computers lovely we
keep adding parallel computers and oh
the green lines sort of leveled off it
levels off at 20x no matter how many
processors I apply very tracing ain't
going to go there any faster than 20x
faster well ray tracing is a special
case it's really worth building parallel
processor forms for ray tracing as a
single task cut cutting to the chase
what does everybody do with their
computers nowadays they run web browsers
web browsers have lots of parallel feds
in them don't they
they're about 2 to 4 Way parallel so
that's somewhere between this light line
here that light line there says 50%
parallel half of your half of your work
can be done in parallel and half can't
and the red line here which is 75%
parallel 75% of your work can be done in
parallel and the rest can't so what
we're saying here is that web browsers
on multiple processors sort of a capped
somewhere between 2 and 4 X faster than
they would have been on a single
processor yes n is the number of
processors P is the percent power we
have a mathematician in the room so
that's that's kind of bad news
you know we're dead set on selling you
smartphones
with ever increasing numbers of cause in
it the this is a monster octa-core the
10 core staff and beyond is coming
really soon now and you will go and hand
over your money to buy multi-core
processors in your phone and in your
tablet if you still buy them in your
computer you've been buying 2 to 4 way
powell l computers for quite some time
so you'll hand over your money for this
stuff and it ain't going to do you any
good if you've got lots of things to do
in parallel if you're a server farm and
where you've got zillions of things that
are completely independent this is good
news if you're a consumer you're going
to hand over your readies for the latest
greatest technology and it's pretty much
not going to be any good that's a bit of
a shame there are of course other
problems
so that's what I just said it would be
nice if you could go out and take your
programming world and split it
automatically across all these parallel
processes that you just bought and
there's been an awful lot of research in
the academia about that for the lab
about the past 20 years we're no closer
to a good solution so if we continue to
use scalar programming languages which
we do because people are being taught
them all the time
then we have this problem that scalar
programming languages simply don't fit
on the machines that you're buying that
the industry is making that we don't
give you any choice of that and we can't
automatically compile from a scalar
representation of the problem into a
parallel machine it's it's proving to be
enormous ly difficult it would be nice
if you guys went off and learned how to
program parallel machines directly but
of course just like in original errors
where gotee was considered harmful
sequential execution is considered
harmful and it's the first thing you all
taught that a program is a menu with a
series of steps that are performed in
order
I mean recipe not menu too much food on
my mind well if that were the only
problem that would be bad enough but we
do seem to have another problem
you'll have noticed when you are holding
your smart phone and using it it gets
hot we've been getting more and more
power efficient transistors all the time
but we use so many more of them that
that little square centimeter or so of
stuff in your phone that's doing all the
work gets really really hot so this is a
graph of the power density of processors
so as we make our processor use more and
more transistors we reach the power
density of a hot plate it's not as hot
as a hot plate because it's small but
it's got the power density of a hot
plate if you put your finger on one of
those it would burn
straight through Intel kept going even
as they crawled this graph up they kept
going they didn't quite make it to his
hot as a nuclear reactor not quite they
tried really hard it went badly for them
the machine that didn't nearly get to be
as hot as the nuclear reactor was the
ill-famed Pentium 4 they had to
completely throw away the way they were
building microprocessors and go back and
start again come down over here so this
is the beginning of the core line so
Core 2 Duo being the first of those
there they literally had to stop doing
it the way they were doing it and start
doing it a different way and that was
because of power well that's pretty bad
mmm maybe I shouldn't take a bet on the
industry getting a lot better in the
future
so we're going to go on putting more
transistors into the space and and now
we're going to sell you more and more
complicated machines
with more and more transistors in use
and they're going to get really really
hot and and to stop the melting we're
going to turn them off so you're going
to buy the 10 way 18 way multi-core
processor that's the latest thing
because we we told you you could buy it
and made it available and we're going to
turn some of those processors off most
of the time so you're going to pay for
the logic and we're gonna turn it off so
you can't use it so as we go forward
even as we've moved from ordinary planar
transistors to these things called
FinFETs we may transistors better so
transistors used to be flat so we used
to have a depletion layers in the
silicon we used to slap a metal gate on
top of it and that was a transistor so
as we put a charge into the metal gate
current would or would not flow across
the depletion layer depending on what
charge we used
and that was getting really leaky so
nowadays we make a transistor vertical
so this is the transistor and then we
wrap the gate round it so got much
bigger effective gate size so we FinFET
we got better transistors that leak less
and didn't get as hot but we still use
more of them and it still gets hot so
seven nanometer right around the corner
in 2020 we're going to have to turn half
of them off so we're going to sell you a
die that's the maximum envelope will fit
in a desktop computer 125 watts and
we're going to have to turn half of it
off otherwise it will mount that is
going to be fun now the industry may
have noticed is trying to get denser
still and do CD stacking that creates
even hotter hot spots where you have to
turn more of it off obviously this
already afflicts your phone your phone
only has about a - to see what TDP for
the processor so it gets turned off most
of the time one of the most expensive
processes you will ever buy and not use
oh no not another problem it's getting
really expensive to do this stuff so we
don't have any physical limits
there's no scientist coming along and
saying we can't scale things down what
we do have is a bunch of people a bunch
of Engineers saying it's getting really
expensive to do this stuff 28 nanometer
a set of masks cost five million dollars
20 nanometer 14 nanometer now in
production Intel Apple and Samsung are
all in bulk production at 14 nanometer
third-party chip companies like Nvidia
AMD have just started using 40 nanometer
FinFETs and their latest things built by
TSMC and you may have noticed that these
things have come onto the market
particularly if you've looked at nvidia
pricing and discarding that any
distortions in the pound dollar ratio a
brexit now notice these things are more
expensive than the things they replaced
this is because the transistors
themselves on the smaller processors
that better transistors sure but they're
more expensive transistors it's got so
expensive to do the processor on a piece
of silicon that the transistors
themselves actually cost more so we get
a better thing that the a-10 is built by
TSMC for Apple it uses these lovely
FinFET transistors that turn on or off
better Apple have been able to ramp the
speed of hurricane up compared with
twister and use more transistors and
turn on more transistors without power
problems but it costs more so we're
obviously going to work on it deal with
the processing and I could describe the
arcane things that we do to even get
this far but it the takeaway is this is
sort of the end of Moore's law only a
few things in going to be worth for
greater expensive process geometries
smartphone apps processors yes IOT
device now when you find your arduino
built with a seven nanometer FinFET
device probably not the volumes just not
enough to make it worthwhile
so for Internet of Things and and cheap
devices the economic picture looks like
28 nanometers forever this this is a
well the dense slide with lots of
information this is from TSMC and what
it basically says is as we come down the
process geometry size.we obligin you get
more and more gates per square
millimeter
down to seven nanometer where you get 17
million gates per square millimeter
so one square millimeter 17 million
gates or as I like to think of it
through fire paths we have difficulty
packing them together so you can see the
gate utilization has fallen this is
because the rules we have to obey to
get the thing manufactured as so much
harsher so they used gates on scaling as
rapidly and we can also see the cost per
gate it fell to a minimum in 28 and is
beginning to rise again that's really
bad
so as we look forward here's next year
2017 will be seen only a few things left
on 20 most people translate into 16 but
instead of 16 going up in volume across
the industry it's falling and 28 lives
on so but get used to making things in
28 so predictions are very hard to make
especially about the future in April
2002 the head of Intel who must surely
have known a bit predicted that they'd
have 30 gigahertz 10 billion transistor
processors by 2010 you would have
noticed that we didn't have that and in
this presentation we've seen a few
reasons why didn't happen and there
again is their turn off to the side when
the power density got too much for them
have the clock frequency so they never
managed to get even close to it
in 2006 I bought a laptop had a Core 2 0
in it it ran at 2 gigahertz that's
pretty much the state of the art today
so conclusions from all of this well
we're going to get more transistors we
can have increasingly specialized oops
smartphones that fall to the floor
increasingly specialized microprocessors
fit for purpose
you've all got used to the fact that you
buy something and it will have a CPU a
GPU if you brought a phone there'll be
DSPs running the baseband and all sorts
of other things sitting around so you
buy a big theory of heterogeneous
processors that's that's great because a
lot of work for people like me and for
system-on-chip designers and a lot of
work for people like you programming and
all these beasts together but even so
performance related to parallel or
special purpose programming only going
up users of computers will have to
adjust their expectations the gain I saw
in my lifetime 10,000 X increase in
scaler performance this is not going to
be repeated and we're going to have to
pay more for our toys EE more for our
ties with brexit so that's the end of
the talk
queuing machine there we are so here we
have a microprocessor actually executing
stuff and it's at this sort of time that
you can just gazing wonder at it this is
this is an arm one and it the tracers
light up has signals stimulate them so
that's the end of the formal bit but you
can ask questions
oh yeah that works what is it that has
stopped what are the do you know what
the economic factors are that have
stopped the manufacturing density from
from continuing to shrink so we're
making things with conventional
ultraviolet lithography well in fact
deep ultraviolet lithography so you know
we're working on processes that have
transistors built out of elements where
we want to resolve down to the nominal
process number so fourteen nine a meter
seven nanometer structures and we're
doing it with 192 193 nanometer light so
that's really quite big it's far bigger
than the structures we want to resolve
so we're having to use multiple steps of
lithography to make those features so
that's what's causing this explosion now
the thing to the rescue that hasn't
happened for about the last 15 years is
extreme ultraviolet lithography where we
use shorter wavelengths of light and the
reason that hasn't happened is we
require about 125 watts per square
centimeter to expose the masks through
the photo masks in the photo lithography
step 125 watts of extreme ultraviolet is
very difficult to produce
I'll give you an idea of what what
happens because I think of this as more
like a laser space weapon from Star Wars
to build an extreme ultraviolet source
we build a vacuum chamber we scatter
molten tin into it we excite it with a
low frequency laser and zap another
laser off it to vaporize the molten tin
that's in space and the beam that comes
off that is then collimated and it has
extreme ultraviolet so we pass it
through a beam fitter to split that bit
off and we're expected to get 125 watts
per square centimeter of light
illumination out of that
so I did say there were some arcane
things going on we haven't been able to
get that up to capability so far people
are working on it quite hard the the
semiconductor market is 300 billion
dollars per year so there's a strong
incentive to get this thing right when
it comes along it will be far too large
for probably seven nanometer chips to be
made with it so we'll still have to have
multiple masks but we just don't need as
many we're using four at the moment you
will probably be able to drop two but
the light source is really expensive so
it have to pay for high um do you think
there's a chance that we'd ever see a
non silicon microprocessor at all well
mark processors although we call them
silicon they're mostly other things
silicon is acting as a base to create
some sort of semi conducting medium on
which we slap a lot of other materials
so we're looking forward and we will use
other materials in the bulk silicon so
for one nanometer somebody's made a
single one nanometer transistor and that
is a large amount of bulk silicon a
molybdenum disulphide gate on top a
layer on top of it to form the depletion
layer we're wearing bulk silicon with
CMOS today we diffuse different
substances into the silicon to create an
N and P but now we're going to use bulk
silicon and a single layer of molybdenum
disulphide and then a carbon nanotube
through the bulk silicon to be the gate
so we will have to be molybdenum
disulphide Valley but it will really be
bog silicon so you can have excellent
properties for handling
any other questions man down front at
the Teva just recently announced the
1000 core coprocessor the Epiphany chip
I'd love to have your view on that in
general and around coprocessors in
general yeah I mean so parallel
processing is a thing there are many
problems such as deep learning where
parallel processors work quite well that
they're fundamentally parallel your
brain is fundamentally parallel and we
can use a lot like that but they're only
in special-purpose zones
the people who have the biggest numbers
of truck motor processors on chip are
collectively the human brain project in
Europe and the equivalent one in the
States so IBM's TrueNorth chip has
250,000 processes on there each
processor is designed to simulate a
spiking your neuron at such as your
brain does the spinnaker project in the
human brain project in Europe is doing
much the same thing but they are using
pregger or micro processors so they can
vary the model of the new one
IBM's neon is hardwired and Spinnaker's
isn't both people have made million yuan
systems and they can train those things
that's a million million neurons you're
sort of a cat level of intelligence a
single spinnaker chip is sort of a fruit
fly a board full of spinnaker chips is
heading towards being Drosophila I've
got the wrong ones there it's a small
tapeworm on a tiny Bush portal and then
you need a whole lap full to be a mouse
you'll see a lot of things happen in
learning systems in the future but of
course you won't be able to run Erlang
or C on them is that what's driving this
thing a deep learning stuff do you think
cause otherwise it sounds like a bit of
a vanity project to get yet more
transistors in to lessen that space that
we can't use it it's one of the areas
where there's significant hope we're
still learning about how to make these
things so there are big design changes
going on inside GPUs in particular
Nvidia a few years back
started making GPUs in a different way
that made them much more power efficient
they've kept that advantage over AMD as
big GPUs ever since AMD is sort of
groping towards the same type of
technology
signal processing digital signal
processors like fire path we're also
able to make better and better ones of
those and they're far down the energy
consumption scale things like fire paths
do their work using stopping far less
energy than a general-purpose computer
they don't have cash hierarchies for
example and that burns a lot of power
and I guess if there's one more question
or why so we'll close and say thanks to
Sophie for an amazing talk</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>