<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang does Nuclear Physics - Controlling Medium Scale Nuclear Physics Experiments - Jared Kofron | Coder Coacher - Coaching Coders</title><meta content="Erlang does Nuclear Physics - Controlling Medium Scale Nuclear Physics Experiments - Jared Kofron - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang does Nuclear Physics - Controlling Medium Scale Nuclear Physics Experiments - Jared Kofron</b></h2><h5 class="post__date">2013-04-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_VKGOTl3jGg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so thanks very much and thanks for being
here
I'm going to discuss today it's kind of
a mishmash of things a little bit of
case study in architecture a little bit
about how we started using Erlang in a
fairly large scientific organization and
i'll even throw a little bit of neutrino
physics in there so what you're looking
at here is a picture of what's called
the Sudbury neutrino Observatory this is
the first detector that I worked on and
one of the two detectors that I know
erling is deployed on now it's a huge
underground ball of extremely sensitive
detectors basically so this particular
picture is taken in a cavern two
kilometers underground in Sudbury
Ontario in a nickel mine if you sort of
squint down at the bottom right you can
see a couple of people this detector is
10 meters in diameter each one of the
little I don't know how well you can see
but there's essentially if you look you
can see a bunch of little what look like
little white cylinders each one of those
wide cylinders is the base of a device
called a photomultiplier tube which is
an exquisitely sensitive detector which
is capable of resolving even a single
photon hitting it to produce a a really
macroscopic charge this detector is used
to study some of the fundamental
properties of neutrinos and because of
its remote location and it's very high
demands on performance it just so
happens that Erlang was a really great
fit for running it so before i get into
exactly why that is and how we went
about deploying or lying to this
experiment i want to talk a little bit
about neutrino physics to sort of
motivate why we're doing this in the
first place this picture is essentially
the simplest pictographic description of
what's called the Standard Model of
particle physics which essentially is a
complete theory which describes all of
the known forces and matter in the
universe up at the top left you got your
quarks quarks are particles subatomic
particles which make up the protons and
neutrons which you find in the nucleus
of an atom off to the right you have
your force carriers these are bosons are
they are called which mediate forces
like a photon which mediates the
electromagnetic force there are gluons
which are strong for us if someone can
figure out how to do it there is a
graviton that belongs in there that
mediate the gravitational force and then
in the bottom left you have the leptons
these are fairly familiar components of
matter like the electron which I expect
most people will have at least heard of
is a lepton in the center of it all is
the Higgs boson which has gotten a lot
of press recently some people call it
the god particle which is just sort of a
tongue-in-cheek way to describe it the
Higgs boson is the particle which is
responsible in the standard model for
giving everything else mass which makes
it a very obviously a very very
interesting particle and sort of the
linchpin of the entire theory what else
is interesting though is it turns out
that the neutrinos kind of stand out and
they are the second row of this little
picture in the bottom left it turns out
that the Higgs boson gives mass to
everything in the universe except
neutrinos something funny is going on
with neutrinos that we don't fully
understand and it makes them a really
interesting target of study so even
though we have now discovered the Higgs
and we sort of you know nailed the final
nail in this coffin of what gives
massive things in the universe it turns
out that the neutrinos are simply not
the case you still have work to do there
and so they're a very interesting target
of a lot of study it turns out that that
weirdness does not end there if you have
a a particle traveling in air for
example electron proton whatever it
turns out that in general you need only
sort of a few centimeters of material to
just stop it cold it turns out that for
a neutrino in this incredibly
complicated and sophisticated graph here
if you think of the neutrinos this
little blue dot on the right the
equivalent amount of material you need
to stop it is about one light year of
lead which is roughly the distance from
here to the nearest star or six trillion
miles roughly so that's already
something that's extremely peculiar and
and worth thinking about the other very
bizarre thing is that it turns out that
they have a mass but it's an
extraordinarily small mass so it turns
out we know
the electron for example weighs five
hundred thousand electron volts which is
a unit that we like to use in physics
the neutrino weighs somewhere in between
forty thousand seven electron volt and
two electron volts which is extremely
small to put that in perspective if I
were to put the neutrino with so the
neutrino mass lies somewhere in between
these two red bars if I wanted to put
the electron mass on the same scale it
would be 30 miles that way and that's
extraordinary because most of the
particles that we know they're not
separated by that kind of a gap there's
maybe a factor of ten or a hundred
between a particle and its next heaviest
particle so something very weird is
going on neutrinos the saving grace of
studying them because it's obviously
incredibly difficult to study something
that you need a Lightyear of lead to
stop and it's they're so small that it's
not what you can really take them and
just you know put them on a scale the
saving grace is that they are as
numerous in nature as they are reluctant
to interact with the rest of it so even
though they interact extremely rarely
the amount the sheer number of neutrinos
that are present in nature means that
every now and again one of them will
stop in a reasonable distance so if you
had the number that I like to throw
around as if you had a dollar for every
neutrino that was passing through your
body you could pay the United States
national debt ten times every second but
in your lifetime one of them maybe we'll
stop in your body so obviously when
you're talking about numbers as hard
this big it's really hard to wrap your
head around that but that gives you kind
of an idea of the note the sheer number
of neutrinos that exist in the universe
so in order to feasibly study these
things you need to build really large
detectors because you need a lot of math
to stop a neutrino and actually be able
to study it you need many many many IO
channels because in general the events
are not only rare but they're very weak
and as a consequence the signals that
they produce are small and you need to
make sure that you capture essentially
all of the energy that is released in
those interactions so you need a large
detector which is just absolutely
covered in instrumentation they're
extremely rare events as I mentioned so
in snow for example snow plus which is
the detector I showed at the beginning
despite the fact that is 10 meters in
diameter you may have 10 or so events
per year and because they're so rare
it's important that these detectors be
well shielded from the environment
anything that may mimic a signal so you
need to basically bury them which is why
that one for example is like I said two
kilometers underground in a nickel mine
they're also really cool to work on
because making measurements of this type
really requires that you are on the
absolute cutting edge of technology and
science to be able to actually
reconstruct what's going on in these
incredibly rare events and make
conclusive predictions so what this
detector would actually look like I
showed you the original picture this is
kind of a very schematic representation
of what of what it would look like
underground that giant sphere in the
title slide is again this sphere here
above that sphere there's an area called
the deck and the deck is where we
basically have all of the support
hardware that runs the experiment so on
that deck if you go on the deck what you
see is a number of all sorts of
different systems for monitoring
pressures to to carry liquids into the
detector and out of the detector and in
particular one of the things you notice
are there these huge racks of
electronics on the top right corner it's
a little hard to see but if you look
carefully you can see that there's
essentially rows upon rows of cards so
each crate there there are these racks
and two crates within each rack each
each crate has 16 cards in it and each
card has 4 daughter boards and the cards
themselves are essentially they have a
number of ziling fpgas on board that are
programmed in VHDL which are responsible
for controlling the photomultiplier
tubes each of the daughter boards then
is sort of an interface to six
of those so you have 16 channels /
daughter board for daughter boards per
card 16 cards per crate and you have
something like 50 crates which are all
full of electronics so understanding a
physics signal that's coming from your
detector requires having a precise
understanding of the state of this
hardware at any given time which is not
a small task given how complicated it is
slightly even more complicated than the
hardware setup is the software setup so
this is an incredibly heterogeneous
group of tasks that are all
cooperatively acting to try to take data
and control the experiment you have the
data acquisition software is written in
objective-c that communicates directly
with the cards that I just showed you in
the last slide that which we call them
the XL 3s those are written in a
combination of a dialect of C and
hardware description language then
physics data is basically streaming
through those XL threes over to a thing
which we call the event builder which
basically just packages all the physics
data that it's seeing and turns it into
an event that's written in C++ now the
event builder barfs out those events
into analysis programs which are also
written in C++ and in order for the
analysis programs to make sense of that
data they need to communicate with
essentially or understand precisely what
the state of the hardware was at the
time that the event occurred which means
you need some logging infrastructure to
be able to communicate with the the
board's themselves now those are
typically written in very high level
scripting languages like Python and the
data is stored in Postgres database so
in addition there are then monitoring
tasks which are used for real-time
analysis of the of the state of the
hardware which are written in Python or
pearl or whatever the detector operators
favorite languages basically so you need
some system in the middle here this
control block that I've thrown up there
you need some system which is capable of
interacting with all of these different
pieces simultaneously to present a
consistent state of the hardware so that
you can actually perform a physics
experiment so the requirements for this
thing are fairly straightforward you
need something which will fail in a very
graceful way and restart in an even more
graceful way this detector is as I
mentioned it's two kilometers
underground and you have something like
six to eight hours of access per day to
actually go down and work on the
detector so if something fails while
you're not there you're stuck with it
there's no way to go down and fix things
please go ahead sure to huh that's right
no so sure yeah yeah all the time and
actually those two questions are are
almost the same question it turns out
when you have so everything around you
is radioactive right even the materials
that are used to construct the detector
are radioactive and what you're trying
to detect is very weak radioactivity in
the detector so if you I said oh there
are only tens of events but those are
neutrino events the non neutrino events
that can happen at rates of sort of tens
of kilohertz or even up to megahertz
where you have essentially the
photomultiplier tubes are just being lit
up constantly and so the stream of data
coming out of the xcel threes is huge it
saturates our network links no problem
and then it is up to the Builder
essentially to aggregate those events
decide which one of them fit together
and construct some coherent event out of
it that is worth looking at so there's a
series of essentially boolean cuts that
you have to pass i was it this many
photomultiplier tubes going off or more
if i try to reconstruct how energetic
the event was is it above this number
and so on and so forth and if something
passes all of those cuts it becomes an
event that gets passed on for real
analysis
yes yes so those event know there's an
event there's real event selection and
some of the events some of them are you
can really use pattern recognition to
get away with but we don't that that's
not common in this particular experiment
sure so so as I mentioned you need it to
be a very very graceful system it's it's
Underground as two kilometers
underground is connected to the above
the outside world by a single fiber
which runs up a mine shaft that gets
broken all the time and so you need a
system which is capable of failing in a
way so that things are safe you also
need as i mentioned easy
interoperability it's incredibly
important that you be able to interact
with the system from a number of
different environments and languages you
need to be powerful enough even though
it has a very high level interface you
need to be powerful enough to twiddle
bits on embedded hardware because that
is a large subset of what you're working
with and you also really ideally want a
paper trail when you're dealing with
complex hardware setups like this often
often times you find that your hardware
will fail in a particular state it may
be that state or it may be the chain of
events which led you to that state which
caused the hardware to fail so it's nice
to be able to reconstruct precisely how
you got there is where as well as where
you were when hardware failed in our
case we decided to solve this problem
with single board computers so there's a
company called a kromagg they make a lot
of dedicated hardware this in particular
we used a product of theirs which is
single board computers with basically
field I 0 points so this picture up in
the top right there are four slots on
the bottom of that device each one
there's a little card that fits into
that slot and then you have a nice jack
on the front that you can plug into each
one has either an analog to digital
converter in it or a digital analog
converter or digital i/o and each one
provides something like 24 channels of
i/o so you have 24 channels times 4
cards /
little single-board computer this was
really nice for us because it's it lends
itself very nicely to a very simple
protocol all you're doing is you're
either reading or writing from one of 24
channels on one of four cards so you
really only need sort of two bits to
specify an operation you need another
two bits to specify which card you need
24 basically a 24-bit bitmask to decide
which channel you care about and then
there's sort of an optional blob at the
end which tells you you may want to
configure something set the gain for
example of a channel and so that's all
contained in that payload it's very very
simple and so when we came time to
actually implementing the software
system that would control these things I
said let's use erlang this is a great
case we need a really robust system that
is deployed in a remote environment
where failure is not really an option we
needed to be have both high-level and
low-level characteristics that were able
to use to our advantage so when I
brought this up the response was fairly
straightforward people basically looked
at me like I was crazy and said what are
you talking about the physics community
is very much AC community like virtually
everything is written in either C or C++
Python is like really the new kid on the
block in the physics community so that's
sort of where we are in terms of the
chronology of programming languages and
and so the idea that we would not use oh
yeah for trans absolutely still kicking
around yeah yeah and some people are
even using Fortran 90 now which is big
deal so yeah there's there's a lot of
legacy languages kicking around people
are not really comfortable with new
technologies in particular because if
they didn't write it then there's
absolutely no way that it could be
correct and so new languages are very
weird and mysterious but I really wanted
to use Erlang and I so I was just
looking for an excuse and I got a great
piece of advice which I'm glad I got at
the time which was basically it's much
better to beg for forgiveness than to
ask for permission so just do it develop
a prototype show that it can work show
that it has the features that you want
and go ahead
so what I dedup doing was basically
using web machine and ports each one of
those channels was mapped to a unique
URI and the web server and then if you
wanted to get the value of a particular
channel you would simply issue a get
request on that particular URI and you
get back a JSON formatted document if
you wanted to change the value of
something or write to a particular
register you would post to that same URI
and you could accomplish it that way and
you'd get back a JSON formatted response
it worked extremely well and if the nice
thing was you got a very simple fixed
layout that for the hardware because
it's you know you have four cards and 24
channels per card which was great and
the sort of killer demo that I gave was
like you would have this Python script
which basically was just pushing random
numbers into the server the web server
and causing a bunch of LEDs to flash and
you'd login to the little single board
computer which was running Linux look
for which process was flashing the LEDs
and just killed Ashley net and what
would happen is people would say okay
well what's what's the big deal and
you'd say well the lights are still
flashing okay so nothing happened but
then when you look at the process table
again it has a new process ID and then
when you say what happened was I killed
that process but the erlang runtime
system restarted it so quickly that the
Python script didn't even know that the
server was down they were pretty much
sold at that point there's not much
better demonstration you can give than
something that is resilient to a kill
dash line in Linux so that worked really
well and after being deployed I can say
that we have literally one hundred
percent uptime this listen has never
gone down and has never been down for
maintenance or any other reason so it's
an incredible success so what happened
was on the good side we had like I said
fantastic uptime literally a hundred
percent excellent interoperability yeah
go ahead it's been in deployment for
four years now excellent
interoperability in terms of just being
able to just fit right in and you know
obviously it's just HTT
so if you can do HTTP you can talk to it
and it was a great management success
from the standpoint that you know when
we started everybody sort of looked at
me weird when I said Erlang and at the
end of it I think people were sort of
coming around to the idea that this was
really a great language for this sort of
thing there were bad sides too I was the
only &amp;amp; M the only Erlang developer for
that collaboration sorry yeah you'd
think that would work right but it was
basically you know they were left with a
system it was difficult for them to
modify and luckily they didn't really
end up needing to but when they did or
when they wanted things to be modified
it was just tough for them to to get it
done it turns out that having a really
static configuration where things really
never change I thought that would just
make things so easy and it's not easy it
turns out that doesn't make things a lot
easier they had these static
configuration files and and it was
difficult to work with because if you
wanted to change anything you you've
really needed to log in stop the server
change the configuration files start the
server again and wait to see if you
screwed anything up which you know at
the time didn't seem so bad but with the
system that I'll describe next which are
sort of the next iteration of this this
project it just it just seems like
absolutely medieval and also the fact
that there was there were protocols
within protocols here which made it
difficult to understand exactly how data
was moving through the system so you
would post JSON to a web server which
then would translate that to a binary
protocol which went over a port the port
then translated that binary protocol
into its internal format that called
some see the data came back translated
back into the binary protocol back to ER
language got turned into JSON and came
back to you and it was just difficult to
figure out exactly how data was flowing
and where you could get tripped up and
then there was of course the terrible
really terrible things there was no real
possibility of service discovery with
this particular application so if you
wanted to know what cards were installed
what they did what they were connected
to you're pretty much out of luck you
had There was just Excel spreadsheets
around that told you this card this
chain
was this and so if you rewired it you
would have to go and edit a spreadsheet
somewhere and God forbid that you get an
old spreadsheet or someone forgot to
edit the spreadsheet because you would
have to essentially get deep into the
bowels of this detector to actually
figure out what's connected to what and
it was extremely inflexible for the same
static configuration reason I mentioned
before you couldn't just change
something that was not a function you
really had to restock the whole system
change some variables edit a spreadsheet
and so on and so forth so it was just
kind of a mess and that wonderful paper
trail I was talking about was rendered
non-existent by the fact that we use
just pure HTTP there was no barrier in
between a client and the server to
maintain that paper trail so it turned
out in the end that there really wasn't
one so although there were some really
great successes with this it turned out
that when I went on to do my own thesis
project which is the system all talk
about next I was in the situation of
having to to do something similar I
thought back to the experiences I had
and I realized that this would
absolutely never work so this is the
picture of what my actual a thesis
experiment looks like which is kind of a
mess so if you essentially what you're
looking at is a big chunk of stainless
steel sitting on top of an old
superconducting magnet from the 1960s
some of the equipment in here is sort of
a year old some of it is literally 50
years old there's a variety of protocols
and a huge number of standards there's
rs-485 there's gpib there's Ethernet
there's a number of different things and
the other important thing is that it is
a a prototype experiment which means
that not only is there a very
heterogeneous hardware situation it's
changing all the time ever it feels like
every week we're getting either a new
instrument or that instruments no longer
necessary or we need to change the
configuration of one of the instruments
and so the old way of doing things where
you had static configurations that never
really changed and you needed to you
know stop your server and so on became
such a headache in
this particular setup that we went a
completely different direction so when
we started looking at this and how we
were going to do this we realized that
running this experiment running this my
thesis experiment is like a script
there's basically some very discrete
steps that you want to check at every
point and then you want to take data you
want to do it again and then start all
over so you check your experimental
health is anything on fire is this
voltage in spec on my amplifiers
actually on things like that you then if
you fail obviously you fix it or go home
in the worst case if it's okay you run
in a particular configuration you switch
to another configuration you then make
sure that what you did made sense and
then if everything looked great you
basically start at the beginning so this
looked to me like I should just be able
to script this I want to be able to
write Python or something that can
actually do this the unfortunate thing
is that i use i showed you that nice
binary protocol earlier that we used to
communicate with the devices and snow
which was just a few bites and you'd
send it over the wire and then things
would get understood nice json-rpc at
the very top and simple binary at the
bottom that's really not the case with
the instruments that I've got presently
so this is the string that I need to
send over telnet to an instrument to get
it to read one channel so I basically
have to tell the instrument to clear at
state reset turn on some particular
formatting rules make sure that the time
is absolute not relative I need to have
units i need a scan list i have to
trigger it i have to tell it where the
trigger is coming from and then i can
read data back from its internal buffer
it's an absolute nightmare so writing a
simple python script that manipulates
the hardware is just not an option
because we have somewhere the vicinity
of 80 different channels and maybe 20
instruments and i do not want to write
code that just sends tell that commands
to all these things they're like this so
I wanted to kind of combine what I had
done for snow plows on what I needed to
do for my own thesis work and the basic
the big idea was I want to combine
something that looks a little bit like
JSON RPC with
a paper trail that I would get from a
database to be able to expose the
interaction between me the experimenter
and the hardware this led to a an
application which is now in production
and has been really nice called drip
line and the basic model for drip line
is that a client is interacts with the
the application by posting a document to
CouchDB and the CouchDB database that
has this nice changes feed where you can
essentially in-stream the changes that
are being made to the database to a
client which is listening the drip line
application is itself listening to the
changes feed and thereby can see what
changes you've made to the database it
can respond to those changes by
interrogating hardware setting hardware
values and so on and so forth and then
respond to the client by simply updating
the document in place I'm actually going
to try to demo this which very likely
will not work
that's the first thing that's wrong
okay so here's this CouchDB instance so
I want to tell my system that it's about
to get a command and I want to tell it
that I want to get the value of a
channel called Hall probe voltage so we
want to read the magnetic field that is
present in my experiment right now so I
do this save the document and like magic
you can now see that the system in very
little time so it's essentially able to
respond so quickly that couchdb just
thinks that the document hasn't even
always had this form essentially what I
got back was something that tells me
that there's about 30 Gauss of
electromagnetic field straight like in
my experiment there's a raw value
there's a timestamp and so I'm able to
basically keep a perfect paper trail of
what I did because each command that I
send to the system is logged so to get
back to this the way that drip line
basically abstracts the experiment is it
as that the fundamental unit of
organization is called an end point so
this might be for example the hull probe
voltage that I just showed you it may be
something completely different it's
capable of running our data acquisition
software as well so it can do for
example spawn a subprocess to take data
and report back the status of that data
taking the endpoints are grouped into
units which are called providers so the
provider for a particular endpoint might
be an instrument that has say 20
channels in it and each one of those
channels is an endpoint and that
instrument itself is called a provider
the providers are then grouped into
nodes which is exactly the same
terminologies in Erlang node a drip line
node is an erlang node and then there
are theirs
one further sort of layer of indirection
which is the bus the bus is how data is
actually sent to instruments and how
data comes back from those instruments
so there is not native Erlang running on
these instruments there is some
communication layer that happens over
telnet or gpib something like that that
that the bus abstracts away so the
client io is done by communicating with
a couchdb instance which then the
changes are propagated to a node and the
server i/o is done by the bus the bus
basically interrogate the state of an
instrument and the the value is
percolated back up through drip line and
it was sent to the client the
configuration is also done in CouchDB so
the the particular endpoints are
actually described by JSON documents so
it this one you'd see that there's a the
type is channel the instrument which is
essentially what instrument is this
endpoint attached to is a DC MUX there's
a locator field which basically
translates the high-level name which is
in this case a hull probe ol tidge which
is that alias field translates that into
something that the instrument can
understand the instrument has no idea
that there's a hug attached to it it
knows that on card one channel eight
something is attached to it there's a
sensor type which essentially tells it
are you reading a voltage are you
reading a current are you reading a
resistance and then there are these
things called post hooks which are
essentially ways to tell dripline that
the value that it gets back from an
instrument is not the value that an
experimenter is interested in so for
example when i read that Hall pro
voltage that's a voltage and I don't
care about the voltage I want to convert
that to a magnetic field so I there's
literally a function and written in
Erlang that is in a module that is
called NMR Hulk al 77k it's a
calibration that we took of this hall
sensor which means that when drip line
reads this value back it will apply that
function to the result that it got back
from the instrument and then the final
value that you get is actually the value
after having the function applied so
it's a nice way to have things like
calibrations if you own
care about part of the response so on
and so forth so to kind of break down
what actually happens to give you back
this nice this this nice result the idea
was and is to try to provide as
expressive and interface to the hardware
as you could possibly get you really
don't want to be sending huge strings or
things that are not human understandable
to the database because that's just
gross so you want to be able to send
queries that look like this where you
just say get and then the name of
something so it's essentially a verb
which could be get set there's a run
verb which is to use to start a
subprocess for example there is an info
verb which allows you to ask questions
about the dripline system itself is it
healthy is it running and then there's
an endpoint which is like a channel or a
particular thing that you're interested
in about the system it's also the case
that you can send configuration so one
of the nice things about the system is
as I mentioned before in the past we had
these very static configuration files
that you you had to update like on disk
in order to change the configuration of
a running of a system with drip line you
can actually change the state and the
configuration of the system without ever
stopping the application itself because
the configuration is actually comes
through the changes feed as well and
drip line will start and stop
instruments and buses as necessary to
provide the channels that you're asking
for so when we add a new instrument we
don't even need to start stop our slow
controls we literally just change a
value in a database drip line picks that
up the node that is responsible for that
particular thing decides whether or not
it needs to start any hardware and then
the next thing you when you try to ask
for that value it simply goes through
the same machinations that it would if
it already knew about it and gives you
the result so the way that's actually
implemented is fairly straightforward
actually there's essentially the the
JSON documents that come across the
changes feed our go through a parser and
semantic analyzer which is just ajs
X call back module if you're not
familiar with j sx it's a json parser
for airline it's a great library it's
really nice if you want to perform some
kind of semantic analysis in line with
parsing json because it allows you to
define a callback module for doing just
that so for us what we want to be able
to do is parse the JSON and at the same
time change that into an internal
representation that our system actually
understands we tried using json
validators but it turned out that what
we wanted to do was just not a good fit
for the validators that were out there
if you fail at that point then it's
simple you just report the error so you
may say clearly you don't understand the
format of arguments that are supposed to
be passed to the system or you forgot to
set a particular field you just said you
said set but then you didn't tell me
what you wanted to set it to things of
that nature and that can be reported
directly if that works then there's two
functions that are then generated the
first function is the function to call
to retrieve data so this might be for
example you you need to call this
particular instruments this particular
instrument module the function inside
that module that you need to call is
read for example and the argument is
whatever the name of that channel is so
this is nice because you go essentially
from this high level get hall pro
voltage after the parsing and semantic
analysis stage what you have then is
literally an mfa that you can use with
Erlang apply once you've generated those
functions sorry in the second function
you have is the response function so the
the second function is essentially a
function that the the worker can call to
send data back to whoever requested it
at that point you just spawn off a new
worker you give it the function that's
supposed to apply at the beginning and
the function that's supposed to apply at
the end it goes off queries the
instrument using the first function
which is of zero arity and it literally
has no idea what it's doing or why it
just gets data back and then calls that
second
function on the data which generates a
response which can be sent to whoever
called the function essentially that
then in general for example does
something like update couchdb document
or there's actually one of the other
plugins that I'm working on right now is
for IRC so you can actually have an IRC
bot sitting in an IRC channel and the
result of calling that response function
will be to send a message to an IRC
channel telling you what is going on
which is just kind of cool so the
instruments themselves what we what I
ended up doing for these is actually
writing behavior modules that deal with
the abstraction of the instrument so I
apologize for the wall of text but I
think it's interesting to look at an
example I showed you the sort of
disgusting string that has to be sent to
an instrument in order to get it to read
a value but what's important to
recognize is that even though there are
these big strings that need to be sent
in general there's a very consistent
behavior there's a very consistent
interface to hardware you have a bus it
may be Ethernet it may be gpib it may be
something entirely different but what
you do know is that the instrument wants
a certain string or it wants a certain
sequence of bits or whatever so the
instrument modules themselves actually
do no communication with the hardware
they only tell the bus modules what
strings are supposed to be sent so
what's great about that is writing a new
instrument is almost trivial in fact it
can be automatically generated in most
cases and the modules that you wind up
with looks something like this so this
is the entire module to control the
superconducting magnet controller which
we use in our experiment it is very
short basically you see that's the
module the name of the module is the
name of the instrument so in this case
it's an ami 420 its behavior is Jen
prologix so it's a generic behavior
which is specific to a type of bus which
is the prologix boss
it exports two functions just basically
a handle read and handle right and then
it also just has an in it and start link
it's very straightforward and this is
essentially a wrapper around gem server
so when you get a read the handle read
function is called of course and it may
be supply current or supply voltage or
coil constant so this is nice because
these are very human readable
easy-to-understand names that actually
wind up in your configuration database
so you'd say I want this channel to be
mapped to supply voltage on the am I 420
instrument and then when you call that
when you interrogate that channel you
wind up in this function clause and that
function Clause says I want you to send
to that instrument over whatever boss in
this case it's the gin / logic bus this
string and then of course the state data
is returned and the state that is
trivial it's an empty couple in this
case because you don't need to keep any
state some instruments have a much more
complicated state so some of the
instruments need to cash their results
because they are slow and crappy and you
can't send too many requests at once
because I simply can't keep up so you
want to be able to cash values so that's
why that you want to be able to keep
state data around in some cases to
handle cases like that and at the very
bottom you see that if someone tries to
write data to this particular instrument
there's an arrow tuple that's returned
that just says unsupported method right
and that is because the superconducting
magnet controller is not something that
you want people to screw up and be able
to write arbitrary values to while
you're in in production so where we've
gotten with this is we've we've managed
to come up with a system which gives us
a really nice extensible framework for
communicating with heterogeneous
hardware systems which is built on top
of really nice rock solid couchdb and
just Erlang OTP libraries it's nice
because the previous method of taking
data was base
tickling me sitting in front of a bunch
of hardware twiddling knobs and making
sure that things were working which
after a few days and a few hundred
cycles is just exhausting and
error-prone it's now just scripted and
it's great because in Python for example
you can just use CouchDB kit and talk to
your hardware using very nice existing
libraries and what you what we've wound
up with is a very flexible and
dynamically reconfigurable experiment
which is great when you have hardware
which is changing all the time so at
present we have essentially this is
despite the fact that it is a the
libraries fairly old in terms of when it
was deployed it's not old in terms of
developer time so I'd like to think of
it as an alpha version but it is in
production it's been working quite well
for some time and Erlang not only in my
thesis experiment but also in the snow
experiment in Canada has proven itself
to be an absolute just slam dunk when it
comes to managing and configuring
controlling hardware and I would urge
anybody who's thinking about a system
like this to definitely think about
Erlang and certainly I don't think I'd
have trouble with this audience but some
are more reluctant and so what we've
wind up with is a very flexible and
powerful tool next steps there's a lot
of refactoring that needs to happen you
know there's this kind of tyranny of
working code if you saw Garrett's talk
earlier there's a you know a lot of
discussion about you get to the easy
part and then it works and it's tempting
at that point to basically just walk
away and say great we're done so drip
line is sort of just past that where
we're getting it definitely works and
it's starting to become nicer and nicer
but there's a lot of work to be done we
want to be able to support among other
things in band alarms and notifications
so right now you essentially need to
query the state of the system check to
see if it's good or not and then get go
back to the system using out of band
communication to change things it'd be
nice if in band in process in Erlang
process you could basically inspect the
value and decide what to do if it is if
it's some out of some specification and
more comprehensive testing libraries
but I also have to graduate somehow so
I'm in my last year or so of graduate
school and so it's really kind of a
challenge to find the time to develop
this even though it's really been really
a lot of fun and really nice so if
anybody is curious or wants to
contribute or just look at the code and
give feedback I would certainly welcome
that and thank you very much for your
kind attention
so at present there are three there's a
just raw Ethernet where you connect
directly to advise and send packets to
it there's the prologix bus which the am
I 420 is an example of where you have an
ethernet to gpib bridge and so you have
to deal with the bridge in the the GPIB
is is essentially an old very much like
token ring way for communicating with
instruments it is a terrible map onto
Ethernet but it's people did it anyway
reason and so you there was a lot of
implementation details there in terms of
getting that logic right in
communicating with the bridge and then
there's raw gpib as well where you're
sending commands over gpib to
instruments to to get responses but
there are more that that are on the way
so we have some custom hardware coming
from the shop that we're going to need
to implement yet another protocol for
which is going to be essentially cereal
it's been a real challenge to figure out
how to test it so one of the things that
I've been playing around with a little
bit is using a common test test harness
and combining that with I started down
quick check but I then decided to try
proper instead for doing some kind of
property based testing of these things
but it's really difficult because in
general you can't it's really hard to
simulate instrument responses and how do
you even start to talk about how to do
that without absolutely going down the
rabbit hole so it's been difficult to
figure out how to do that I can test
most of the configuration stuff that I
update a configuration variable it
should change in the following way
that's straightforward but testing the
hardware has been a real challenge and
where I'm still trying to figure out
exactly how one does that huh yeah
there's been a couple of other people
who have been interested and so I'm
unfortunately I'm a little reticent to
give them something that I'm not sort of
a hundred percent like yes this is a
version that you can work with so it's
going to be deployed on an experiment
that's coming up at MIT but it's it's a
work in progress so you know if people
are interested in you know contributing
to a project that will be used in
nuclear physics experiment because
that's kind of cool than you know shoot
me an email let me know because it will
it seems like it will get deployed in
other systems but yeah I people or I
found that in general when you when you
when I talk to other physicists about
the project at first they sort of think
oh that's that's a funny that's kind of
a novelty it's this weird program
language and they've never heard of it
and what what is this really for but
this domain in physics which we call
slow controls absolutely everybody that
I've ever met in physics has nothing but
complaints about their slogan trail
system so when you say that it's really
robust and it's
courts multiple protocols and you know
you just did talk to a database to
communicate with it people get
interested and so there's definitely
been a lot of interest I'm actually
giving a talk tomorrow at Berkeley about
this system among other things no when I
initially got started down this road I
had no or Lang experience I knew of the
language but I didn't really have any
experience using it at that point I had
written almost all C and C++ and a
little bit of Python but reading about
it I've always been kind of a hardware
guy so reading about it I thought this
is a great map too because solving
problems is so much easier when you have
the right tool when you when you're
working in the right language solving
problems is so much easier and reading
about Erlang I just thought this is such
a great map from the language on to the
hardware domain where you really want to
control things in this way so I sort of
learned it as I went which means there's
some real there's some real doozies in
the first in the first iteration in
terms of what the code looks like that
nowadays I look at them like oh my god
that's awful but it works
okay well thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>