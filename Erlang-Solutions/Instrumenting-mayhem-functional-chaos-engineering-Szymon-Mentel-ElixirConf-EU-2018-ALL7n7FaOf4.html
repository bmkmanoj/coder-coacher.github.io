<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Instrumenting mayhem: functional chaos engineering - Szymon Mentel - ElixirConf EU 2018 | Coder Coacher - Coaching Coders</title><meta content="Instrumenting mayhem: functional chaos engineering - Szymon Mentel - ElixirConf EU 2018 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Instrumenting mayhem: functional chaos engineering - Szymon Mentel - ElixirConf EU 2018</b></h2><h5 class="post__date">2018-05-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ALL7n7FaOf4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm here today to tell you about
chaos engineering so that you can
improve your systems resistance to
failures and how I get to talk about
this so I work at they're like solutions
and for last year and a half I've been
involved in developing continuous load
testing tool and this is how we came
across Kerr's engineering as such
starting and experimenting with it so we
read some books some blog posts so what
I have to tell you what I'm what I have
to tell you about today is based on my
experiences as well as knowledge that we
gathered over time so let's start with
some basic questions what what why and
how and if you think about today's
systems we very often deal with large
distributed distributed complex systems
which are based on micro service
architecture and to serve a single
request multiple services have to be
contacted and it's often the case that
big companies in big systems know no
longer one person is able to capture the
system as a whole and that's why we can
say that chaos is inherent in them
whether we wanted or not and then these
different micro services may function
properly on their own they may be
shipped by different teams may be test
tested in isolation then integrated on
some integration platform and then they
come live and then they eat interactions
between them especially when compounded
with some real-world events like server
crashes disk failures net splits may
lead to unpredictable outcomes and to
give you an example let's imagine that
we have some client getting into our
system sorry and it calls micro service
a then micro service a cause Beadon
called C equals D equals E and at some
point C crushes is not able to satisfy
the request but it has fall bug in place
which is F so it calls for bug but it
turns out that the fallback is msconfig
somehow it the the bed configuration
made it to the production builds up the
queue because it's not able to handle
the load of the requests that she was
and finally crashes in chaos engineering
is about experimenting on a system
provoking scenarios like this one so
that we learn the system behavior in
turbulent conditions in formal terms
chaos engineering is a discipline of
experimenting on a distributed system in
order to build confidence in the
system's capability to withstand
turbulent conditions in production so
it's empirical because it says about
experimenting it's also about the
learning system behavior because we have
the building confidence in it and
finally it's about experimenting at
scale because it tells about turbulent
conditions in production so this
scenario could be chaos engineering
scenario in which we deliberately on
purpose disrupt see observe the system
and we learn the weakness related to
default back and and in chaos
engineering will focus more on exploring
than to testing assertions so we are
aimed at discovering and observing our
system and learning what it does if
something bad happens
okay let's have a look at a dy question
what is the rationale behind the whole
concept first is about building trust
today we have great development velocity
and complexity in this in some systems
and as I said it may happen that one
person is no longer no longer able to
capture the system as a whole so we
don't have this trust in the first place
especially when we have different
components and they are shipped and
brought live on a daily basis and that's
why we need to build that trust so this
is workers engineering helps is about
being proactive because because we want
to discover some system in weaknesses
before they manifest system-wide because
there is some hidden vulnerable Vernal
vulnerability in our system and finally
it may be cheaper to put some some users
at risk of experiencing poor quality of
our service than to have entire business
taken down because of hidden hidden bug
in our system and let's have a look how
to actually practice it but before we
get into it let's look at steady-state
first so system steady-state refers to
some property of a system which which it
tends to maintain within a certain range
of values or pattern over time in our
system is the human body we can say and
do we and that the property is the
internal body temperature by observing
the body temperature we can say whether
the body is in is in steady state so
it's roughly when we when our body
temperature is around 37 Celsius degrees
so the the steady state refer to due to
the system's normal operation to give
you concrete example this is the graph
representing request response time of
some system and
you can see that it is around 40
milliseconds because we have we have
been observing the system for a long
time we have learned that it is this is
the normal operation of the system is
the steady state and now if you look at
the second one we have some spikes
reaching up to 120 milliseconds and
because we previously learned what is
the steady state now we know that our
system is out of the steady state that
something that is happening and chaos
principles dork defines four four steps
to practice the discipline and the first
one is to define the steady state of
your system in other words in other
words it's about understanding some
measurable output of a system indicating
its normal behavior once we have that
the second step is to hypothesize that
the steady state will continue if we
break something in our introduced
so-called variables and this is the
third step which is about introducing
these variables real world events like
server crushes disk failures net splits
and maybe other things and having them
in place in our system we can learn the
behavior that the system exhibits when
we have these variables enabled and
finally the final step is to try to
disprove the hypothesis by looking at
the difference between the steady state
and the output of the metric that we
picked that we actually based our
steady-state on and if we are able to
see the difference we have uncovered a
weakness and we have target for
improvement and if there is no
difference or there is acceptable one we
learned that the system is able to
withstand a given turbulent condition
associated with the D variable we
introduced so we can say that the harder
it is to disrupt the steady-state the
more confidence we have in the behavior
of the system
let's have a look at some example from
from the Netflix platform how they they
do it so let's imagine that they have
client which caused the gateway then
gateway calls API and API finally
reaches ratings service the rating
service is responsible for calculating
some some rates of movies that you see
when you are looking at different movies
in their up and what they would do using
and that they would ask a question how
API handles failure of the ratings
service so this is the one of the occurs
engineering scenarios they would run
said that they would ask this question
and then using chopped which is chaos
automation platform they have they would
spawn two additional clusters of the API
service one is the control one and the
the second one is the experimental one
and what they would do next they would
decorate the requests going through the
Gateway so that some fraction of them
goes through the experimental cluster
the other fraction would go through the
control cluster and the rest would go
the normal path then they would scale
this the number of requests going
through these different clusters
appropriately so that they minimize the
blast large the blast reduced because
they want a small number of users as
possible experience any failure and
finally they would introduce this
failure is about simulating that the
ratings fail that the ratings service
has failed and it means that all the
users going through this experimental
cluster would not be able to get the
output from the rating service and if we
think how this maps to these four
principles so this this question is the
device for the hypothesis that they
could hypothesize that API will behave
let's say well if rating service is not
not available meaning that the user will
we'll get a response with some data
missing but they can live with that
they know they are steady state because
they have been observing we studied
symmetric of the API cluster over time
so they have learned what the
steady-state is and they would introduce
some variable into the system which is
the simulation of the ratings failure
and finally they would compare the
outcomes of this dis metric that was a
base for the steady-state and know and
actually learn what's the how the system
behaves if there is this kind of a
failure and this brings us to the second
section of this talk so you saw what is
what are the basic principles and now I
would like to share with you my
experiences or maybe our team
experiences with applying chaos
engineering principles to one of our
systems that we develop at erlang
solutions so what we what we do we have
a messaging server mongoose i am which
is messaging XMPP messaging server which
allows clients to exchange messages like
for instant messaging and it connects to
the proxy authentication service to
authenticate these clients it talks to
some database to store messages the
user's exchange so that there is a
archive feature and finally in some
installation it would lock some events
that happen in the server like user
logged in user enabled something to the
streaming service like Kafka and now
something when when the system is
running something wrong may happen on
the connection to the authentication
service for some reason we may not be
able we may not not be able to talk to
the authentication service because of
network glitch or the database may
become not reachable or we get some data
that we cannot persist and we have an
error at this at this place or something
similar may happen
with these trimming service and all
these places and now we can simulate
these failures in our system in all
these places in the code actually
literally all called injection points so
these are the places where we can enable
some faults we can simulate thus these
bad things happen and having all this we
can set some hypothesis that our system
will behave in a certain way given one
of these turbulent conditions occurs and
we simulated these failures by injecting
them into the airlock VM running our
server and before I get to the actual
techniques we used to inject the faults
let's have a look at how we what was the
environment we were running our chaos
engineering in so basically we we use
our load testing infrastructure to run
the experiments and we have scenario
which describes a load test which is a
code that describes the users that
connect to our server that emulates the
users and they do different stuff we
feed load generator with this scenario
which in turn we put the scenario into
the load generator which in turn
stresses the server and we learned the
measurable output of the system and this
is our steady state and this is our
control load test then we take the same
load test scenario we run the same load
test but this time we had another
component which inject faults and this
is our experimental load test with some
hypothesis set and at the end we compare
the the outputs the DES metrics that we
base steady-state on in the first place
and in our in our particular system we
are we treat two metrics as as
indicators of steady-state the first one
is the time to deliver which which tells
us that the time between the clients and
the message
and it that this is the time that passes
between the client sends a message and
recipient gets it and the second one is
this session account this is the server
side metric which indicates how many
sessions are during the server so in
total we have around 110,000 sessions in
in the server on three nodes and they
fluctuate a little bit because action
users reconnect during the load test and
the first hypothesis with indicator
engineering frame we said was that
failure to write to the database won't
disrupt the service meaning that if we
cannot write to the database we are
still still able to deliver the core
functionality which is which is allowing
the the users exchanging messages that
was the first assumption we said and in
elixir Erlang based systems you would
have workers connecting it to the
database in our case that was Cassandra
and pool of supervisors on top of them
and to simulate this failure that we
cannot write to the database it would
kill the workers at interval sorry it
would kill the workers at interval while
the system is running and this is the
the code that actually implements this
fault so what we would do we would
define kill pool anonymous function that
would basically get all the workers that
connects that connect to the Cassandra
pick the random one and kill it and
having this fun we would rub it into
another one which would basically kill
random worker every 15 milliseconds and
then call it serve itself recursively
and finally we would rub that into a
function which is our fault so once we
spawn this function we start the
experiment and when we are done we kill
it
and what we learned from such an
experiment so this is the time to
deliver metric this is where the chaos
started and we saw that the performance
was better in a sense because the time
to deliver dropped then when we looked
at another metric that indicated the
number of archived messages so in other
words the number of writes to the
database successful writes it sort of
stayed the same which was a little bit
strange because we suppose we sought
that we were not able to write a
database because we're killing workers
but the other metric was telling us
something different so we came to
implement some client-side metric to
understand how many messages are read
from the archive because the clients
would read the messages in the fashion
that they would read chunk of messages
remember the timestamp of the last
message they read and then after some
time would come again and read from that
timestamp which effectively means that
that they would read all the messages
they would have in the archive
associated to the account and we see
that a few minutes after the chaos
started there was a significant drop in
the number of messages read so the
learnings were that we had wrong metric
indicated how many messages were
actually stored in the archive so the
previous one on this slide
it was the implementation of this metric
was wrong this is what we discovered so
we had wrong data here and we also
learned that if we have this kind of
failure in Owens in our system where we
can't write to the database the
performance is preserved and the the
core functionality is still there so
that was good and and the other thing
was that the database operations were
simply dropped so the server has had led
work to had less work to do that's why
we saw the increase in performance so
quite interesting learnings about the
system for us
okay another hypothesis we said was that
if there is a delay on the connection to
the authentication service the users
will be still able to log in so another
metric indicating steady-state for us
was the number of connected users and
similarly as with the database in the
database case you would have a pool of
workers connecting to the authentication
service supervisor on top of them and
every time someone logs into the server
the server calls the authentication
service and to simulate this kind of
failure he would add a delay at the
worker level and this is the code that
that we use to achieve this goal
so first have a look at this delay oath
fun so basically we use the mocking
technique what we did we replace the
original function that would call the
authentication service with made-up one
that would have delay added in it and
what we would do using the Mac library
which is the Erlang library for mocking
would install a new mock mongoose oath
is the module responsible for
authentication and we would Mock the
authenticate function and provide our
own that would take user and a password
the delay one and the delay one would
basically sleep for 100 milliseconds
this is the delay and call the original
function this is what the Mac
pass-through does so it would call the
original authentication function from
the Mongoose oath and we would take this
composition of anonymous functions and
put put into a process spawn it this is
how we run our fault once we are done we
would stop it
and this is what we learned this is
where d.ko started and we you can see
that there is a drop in the number of
sessions on one of the servers because
we run this fold just on one server and
so the it dropped to some level but
because just fraction of users would
reconnect to the server so this is why
it didn't go down to zero and these are
the not affected notes the effect the
affected one and this is another graph
confirming that actually the number of
logins per second dropped at one note
and what we learned from this experiment
was that one hundred millisecond delay
at that point it code was enough to
prevent users from logging in this is
something we need to investigate further
as we didn't actually understand it
properly yet and the other thing we'll
that we discovered was that in the load
generator we had we had the bug at
because it would expect that the user
that cannot connect to one server would
eventually pick another one and connect
to it because we thought we had load
balancing in place but in fact we didn't
actually didn't work so the users would
try reconnect to the same server all of
the time so these were the learnings
from this experiment for us and the
third experiment we ran was the third
hypothesis we said was that spikes in
message rate sent to Kafka want to
disrupt the service so we sent we sent
records to Kafka on different events in
the server for example when user login
when it set some setting and to simulate
this fault you would spawn an internal
process a flood err that would stress
Kafka with some made-up messages and and
just send it to to the Kafka lair
internally
this is how we would basically verify
this hypothesis and to achieve that we
would define a produce fund that would
construct some random message and then
call the API to send stuff to Gothica
and then we would construct another fund
that would call produce 50,000 times
then sleep for some time and call itself
recursively so this is how we simulate
it like spikes in messages sent to the
Kafka lair and again you would take this
composition of anonymous functions spawn
it in a separate process and once we are
done we would kill that process and
capsule lighting default and this time
this is where the kill started we
learned that it has no and that the
performance is preserved the the core
functionality were looking at is still
there and this is another graph
confirming that we really had a huge
increase in in number of messages sent
to Kafka so we learned that our system
is able to withstand this kind of maybe
not failure but situation in which the
the Kafka lair has to deal with much
more messages that that we expected it
to do and all this fold injections
techniques you saw basically didn't
require any any change to the existing
code base meaning that you you wouldn't
have to recompile anything you would
just construct anonymous functions spawn
the process and once you are done with
experimenting you will kill it and we
were playing with it by we were playing
with it by writing the code directly in
the shell but this is not handy when you
want to automate things and you could
you make use of the RPC Airline module
that allows you to call remote functions
on the on the affected note or spawn
remote processes using the node spawn
spawn API
and is similar to what we did defaults
could also be weaved into the code in
the sense that the messages or different
events could carry along some metadata
with it that was triggered is false this
is how Netflix approaches the problem
but it would it was not required in our
case and this is why we didn't have to
tamper with the existing code base so no
recompilation was needed and so we
learned some things about chaos
engineering when we experimenting on our
system and then we and now we are ambi
we have ambition to automate the whole
process so we designed some solutions
own system that would basically run care
engineering experiments experiments for
us that could be pluggable in our
continuous integration pipeline and in
this section I would like to share our
thoughts our design of this kind of a
system so short recap so to run these
experiments we make use of our load
testing infrastructure we run to load
test one is the control one the other
one is the experimental one with some
hypothesis set and the the second one
made use of default injection component
defaults are being injected into the
server and we compare some measurable
output to see whether there is a
difference in some key business metric
and this is for us chaos experiment and
this is the new component we are working
on this chaos automation tool we call it
typhoon and this is just an elixir
application which is phoenix-based it
comes with a front end and a back end
and on the back end we have three
components chaos manager fold the
injection component an infrastructure
component and the infrastructure one
brings two models I forgot to say in the
beginning that at the beginning that we
run our load test in in a docker
environment so for every load as do
a spin of a bunch of containers running
different services and when we are done
we wipe we basically tied everything up
and store some metrics on the side so
the test setup defines some
configuration files for for the services
that do that we run in the load test
docker images some metadata and wild-ass
topology describes the the the load test
when it's running so different
so like services names like IP addresses
ports that got assigned to different
services and things like that and then
we have called the injection component
that defines fault protocol which is an
interface of all faults it's an alux
reproter call it manages collection of
faults and this is the extension point
so that users can add new faults to the
system if they want experiment with
something that that is not yet covered
and it also and my fault is an example
fault and finally we have chaos manager
which describes which brings experiment
model which holds things like IDs of the
load test we set up the list of folds
and folds offsets so that it knows when
a particular fault is to be injected and
if you were to to work with this system
you would configure your load test setup
your faults their parameters and chaos
manager would create an experiment out
of it run the control load test through
the infrastructure component you would
learn the steady state and then you
would run another a load test the
experimental one and using the topology
and the list of faults it would inject
them through the fault injection
component you would learn the behavior
of the system under these particular
faults and you would be able to
compared to and either build the
confidence in your system or find a
weakness of it and few elixir bits so
this is very basic implementation this
is a very basic implementation of the
protocol of default protocol we have it
just say it just defines one function
apply so it takes so with treat faults
as a data types as structs so it would
take the default and the idea of the
test to apply the fault too and this is
how the example implementation looks
like so we use ecto to verify parameters
of faults coming from the UI and to
build the district's representing
defaults and each fault
is to implement the fault protocol and
to wrap up I hope that you got to know
some basics of the chaos engineering as
a concept you have an idea how to apply
it to beam based systems and you you
know you would know how to automate the
thing and I encourage you to go and try
it it's for everyone not only for
Netflix or Google or companies like
Facebook and try to play with the basic
techniques and if it works for you added
to your continuous integration pipeline
so that you bring your software to
another level this is all I have for you
thank you
so we have plenty of time for questions
hello
so the version of the caves engineering
that you did doesn't actually work with
life on the production system right but
in a yes that ogre that's correct
so we abuse the rules so it will be they
forgot to mention that but it's not
about sticking strictly to the rules
it's still we can still benefit from it
so our business guys so to speak is that
we we have an open source XMPP server
which which clients install at their
premises or somewhere and and then it
would be hard to experiment there so we
know different architectures that they
would use so we try to so we have like
let's say common one and we try to
continuously experiment on on our system
at our load testing infrastructure
because this is this is it it is the the
the setup that resembles the the live
system the production one so we're
trying to be as close as possible but
for us it's not a live system thank you
and very small related question is when
would you actually start using this when
you develop a new system like when it's
a system complex enough in your opinion
to start spending some time making this
if you want to be strict so you if you
want to be tricked then you would have
to practice case engineering when you're
alive but during the development phase I
would say once you have some stable load
testing environment then you could have
some simple scripts that would let's say
kill a note or something and the other
thing you would like you you would need
to have is the is some are some scripts
for metrics that you can easily store
them and then fetch the historical one
and compare the two sets of metrics so
that you know that you can judge whether
the experiment discovered something or
not
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>