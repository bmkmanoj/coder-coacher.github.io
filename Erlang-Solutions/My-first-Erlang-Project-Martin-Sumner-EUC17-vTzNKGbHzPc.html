<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>My first Erlang Project - Martin Sumner - EUC17 | Coder Coacher - Coaching Coders</title><meta content="My first Erlang Project - Martin Sumner - EUC17 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>My first Erlang Project - Martin Sumner - EUC17</b></h2><h5 class="post__date">2017-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vTzNKGbHzPc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so yes I'm going to talk about level ed
and there's kind of two aspects to my
talk one of them is that I've taken
quite a sort of statistic umming an
airline developer so I'm going to tell
you about something about that path and
about the experience of write my first
project and and the second thing is just
about level ed itself why I chose to
write yet another airline key value
store and and how that key value store
has has turned out hopefully that will
come back again so some history about
how I got here and back in 2004 spent
the sort of first ten years of my career
as a network engineer and I was working
in a network engineer company that
suddenly won a very large project for
the National Health Service in England
to build a new national messaging and
database solution and so I was the
network guy on that project and the
contract would orders made in December
2003 and in the summer of 2004 we went
live and as you can imagine with a very
large distributed system all written in
sort of enterprise Java stuff with
Oracle and lots of suntan lying around
the place and there are a few problems
with that having gone live just seven
months after contract award and one of
the unfortunate things is that because
every sort of aspect of the application
relied on communication between multiple
servers every problem looked like a
network problem so me the network guy
got involved in every problem and I soon
decided that I rather spend more time
sorting out these application things
than being blamed for the network going
down every time the application didn't
work so and I decided that I was going
to spend some more time and application
I learned a lot of applications on that
but then on that project in early 2005
some new management came and they
decided they could fix all the problems
in the project by adding more process
more managers and more dishonesty and at
that point I thought I'd rather go and
work on something else and so I left and
sort of wandered around for
while and during that wandering though I
kept thinking about my time on the
project and thinking that sure is a
better way of doing things and I read
various stuff about different approaches
to projects and I also stumbled across
some new databases that were written in
Erlang and I was asked thinking about
how we did high-availability systems and
stumbled across history of Erlang in the
telecom sector and that had sort of
quite an impact on me
and so by 2011 I was now working for the
health service itself and the contract
was coming up for a new for the original
project so they were looking to secure a
replacement and and so had the changes
of the previous management had an effect
well they had as sorts I mean it was
pretty much highly available and we not
entirely highly available were pretty
much highly available and they weren't
meant to lose any data and they hardly
lost any data so that wasn't too bad and
but that high availability and
protection from data loss had come at a
cost the addition of all the process and
all the management and perhaps some of
the dishonesty men that there was now
three thousand servers required to
support the system across all the
environments and the running cost so far
got to over a billion pounds and the
supplier themselves was bragging in his
case study that it had spent over
eighteen thousand person years to get to
that point and so that the question
really that was going around the health
service organization at the time is was
and is this the cost of availability of
doing large-scale systems in this way
and surprisingly a lot of people thought
well yes I'm sure that's it I mean you
know if you're going to have all that
process and all that management that
must be necessary if you don't have all
that process and all that management
well it we chaos worms it so and so a
lot of people thought that it was and I
kind of disagreed with them and I kind
of got on with building replacement and
eventually got permission for that for
replacement to happen and so by 2014 we
built a replacement for this solution
and we went live and in the sort of
three years now since we've gone live
we've met that five nines availability
target but we've met it now efficiently
so we met it without all the side
effects that we had before so we're now
releasing once a week not once every
other year of 30 million pound
transition costs we're and it took less
than 100 people years to get to that
point we now have a fairly small team
working on it and 90% of that teams
activity is spent on development time
rather than operational time and how did
we get there well yeah there were a lot
of changes in our approach and not just
technology change this kind of stuff but
there is a you know an underlying theme
in the solution that the main products
that we used are built on Erlang and
although the code that we've actually
written most of it's written in Python
and it was written with and some kind of
erlan architectural principles in place
so you know we still generally speaking
all the code does massive passing
between processes and this kind of stuff
and our approach to failure handling is
very much an airline inspired so and and
so we've done a big change and we now
had a a much more efficient solution and
that had sort of reaffirmed my faith
that there was some things we learn from
this this sort of strange esoteric
technology and programming language
called Erlang and so I found myself sort
of last year or so thinking well what
should I do is my next challenge and I
didn't really want to go and do another
sort of big project that required other
people and that kind of stuff I'd quite
like work in my own for a bit so I
decided to challenge myself and say well
you know I've talked a lot about why I
think the ideas in Erlanger a good idea
and that kind of stuff but I've never
myself developed something in Erlang and
I mentioned before I don't have a
traditional development background I've
mainly been a network engineer on my
career so I thought well let's give it a
go let's try and develop something in
Erlang and you know what do you develop
if you're going to develop something
airline well everyone has to have a go
at a key value store at some stage I
guess so I thought well I'll write an
airline key value store and and yeah the
reasons for for going for a key
value store were in part and you know
the main earner and product that got
used to this stage was react and we've
had a tremendously positive experience
from react Cavey in the in the National
Health Service on the spying project
most of our and the majority of our know
to react nodes and it's done a great job
for us in terms of providing the
durability and availability we need
without any operational overheads and so
but having used react for a while I've
got a bit use of the code base and this
kind of sauce are felt well that's some
that's an area where I can actually you
know I understand well now I can do
something with and an Erlang has this
concept of pluggable backends and so
fort well you know why don't we have a
go at plug will doing one of those
backends and of those backends one of
them is written in Erlang which is bit
cast but still requires a lot of CP a
lot a lot of C code that sniff doubt for
a lot of the core bits and it's not
fully featured and the only fully
featured one is is level DB and that's
written in C++ and has become an
increasingly complicated piece of code
and originally level DB from yo from
Google was designed to run on mobile
phones and and obviously running it
multiple nodes on a single on servers
clustered through react it's a very
different type environment so nvm at
bash Oh whose demotion the working this
has done an incredible amount of effort
to extend and improve level DB for using
in react but there's a bit me that
fought well rather than you know putting
all that effort and changing level DB
what if we started with similar
principles of level DB and but try to
think from the start about something
that was better suited to react so I'm
not trying to write a generic key value
store what if I'm just trying to write
one specifically for the purpose of
react so I started with the concept of
the log structured merge tree
so um level DB is based on this idea
called log structured merge trees which
comes from a paper from about 25 years
ago I think and the logical chemistry
paper looked at the hardware economics
at the time so looked at the cost of
memory the
of dis space the cost of random and read
and writes to this and the cost of bulk
read and write to this and said based on
these costs this is probably how we
should be structuring our stores yeah
and the idea is is that we want to and
we want to have all our information
sorted on this so it's fast to find and
but random writes are expensive so we
don't want to write them into that
sorted place so how do we make that
compromise well in the log structured
merge Ettore is when we receive stuff we
store it first in a very small tree
that's e0 tree and in that tree
it may not even be ordered a bikie may
be ordered by sequence at which it was
received so it can be append only when
we write it and we'll store in memory
and that information where it's cheaper
to organize we can store it in in key
order there and when and but memory is
expensive so when memory gets full we
need to merge that down but we so when
we merge that down to another level to
another file and clear that out so we
can accept new writes in and when we
merge that down will we writing it
sorted but we'll be writing it in bulk
sorted and because bulk writes are
cheaper than random writes that would be
better and but we don't want to write
too much in bulk so we'll make sure the
next level downs bigger than the
previous level will not that much bigger
yeah and and when that level gets full
then we'll need to write down to the
next level the next level will need to
be bigger and this kind of stuff and
that forms the basis of a log structure
and merge tree and and so based on our
experience of running level DB and level
DB's log structured merge tree
what would we change in terms of
principles to get something that's more
fundamentally suited to react well the
first thing that we find when we run
react in a production environment is
that the react database the bottleneck
is almost always disk i/o so you know
and and disk i/o is it kind of its kind
of a hard unpredictable bottleneck to
deal with it tends not to behave in a
stable way as you start hitting limits
and it's very hard to make it make it
smooth and that kind of stuff so we
reduce the disk i/o on what's the big
driver of disk i/o so though we're doing
with through logical mercy who made
things more efficient by by batching up
these writes still that write activity
is a dominant factor and when the reason
why it's a dominant factor is that every
time in leveldb when when we're in that
merge tree we've sorted the keys and
values in order so we write the keys
that that tree down to next level we're
rewriting keys and values so the volume
of writes that we're doing is
proportional to the size of our values
yeah and if we've got larger values then
and that's going to be a dominant factor
and other people have been thinking
about this as well so there's a couple
of other alternative log structured
merge trees that are out there at the
moment one of them whiskey which come
out the University of Wisconsin and the
other one which is badger which is a key
value store written and go by the
digraph people and and they've both
followed the same principle which to say
right amplification is a problem so
we'll actually put our values to one
side and we'll only merge down keys and
references to where the values are in
the merge tree and we'll get rid of that
right amplification that way and there's
nothing new about that idea because it
was actually in the LSM tree paper and
in the first place it said maybe you
should do this maybe you shouldn't I'm
not making that decision for you and and
now increasingly people looking at so
we're going to split values out the
start not putting onto the merge tree
the other thing for those who are
familiar with the react database is that
when it writes something to react you
actually going to write it to three
different stores on three different
nodes and when you're going to read that
you're going to read it from three
different stores you're going to deal
with probably the first two responses
and you're going to try and work out
from the vector clock information which
one is valid response to send back but
in order to do that because it is used
to using generic and key value stores
that support only get requests it gets
the free objects from or gets the object
from all three stores so it's performing
that get three times and sending it
three times over the network but for two
of those objects it's going to ignore
the value and it's really only
interested in the
o'clock so if we have a key value store
that supported ahead requests so a
request that only returned the key in
the metadata then potentially we could
run react in a more efficient way so
that's the second part of the efficient
hypothesis can we support ahead requests
and then the other thing is increasingly
in react we're looking at of a data
objects and data types and they may need
to be handled differently in the data
store so merge in a different way and
that kind of stuff
when we look at different types of CR
DTS like big sets and this kind of stuff
and general and generic key value stores
don't really have any concept they can
help with that so we want to have a
value store where we can attach a tag to
our keys and then attribute different
behavior in the store depending on that
tag so they were the sort of three
founding ideas that have why there might
be a good reason to write a different
key value store and so that's what I
started with and so if we look at how
leveldb works in my scruffy drawing and
in effect at the start a put is going to
go into the underscore mem in memory
table which is just a Skip list and
which gives you a sort of view of the
current of the most recently received
keys and the key and the value is
written to a log below which is a
ordered by sequence which they find
which they which they received and then
when that in memory store is full it's
pushed to one side and made a mutable
and a new story started but now that log
that nursery log is a candidate to be
merge down into the tree and then they
all ripple down with those keys and
values so level ed is is is very simple
but now to Simpson I've drawn some top
hats on there which is representing our
actors in our level ed store so when we
get the put that goes to this top hat
character called the bookie he's going
to receive it and the bookie wants the
key and value permanently written
because it's got to be a reliable store
so we still want to have that nursery
log so he passes it to a character
called the Inca over on the far side and
the ink is going to write it into a
journal
the equivalent of the nursery log and
and when it's written it into there the
inca can tell the bookie this is a
sequence number of that right and the
bookie then takes out the metadata from
the value it's got the key it's got the
metadata and the sequence number and
it's now going to write that into its
merge tree but rather than write direct
direct into it directly it puts in its
own sorted memory view it doesn't need a
persisted view of that because the ink
has already got a persisted view yeah
and it can now acknowledge the pot and
then once that in memory thing gets full
it gets pushed down to the pencil and
the pencil er is our actor who's
responsible for keeping the merge tree
and the merge tree as I say just
contains keys metadata's and sequence
numbers so if we want to do a get in the
future the get will go to the bookie the
bookie needs to look in the merge tree
first so it look in its in-memory cache
first for that key if it finds it there
it's got the sequence number if it
doesn't it goes down to the pencil and
the pencil looks down it's its stack of
the previous caches and then the
persisted view of that merge tree and
eventually it will find the key we're
looking for and that will give it the
key the metadata and the sequence number
and with the key in the sequence number
it will go to the inker and it will say
use that key in the synchronous number
to fetch the value okay and whereas the
nursery log previously was removed after
a while in level in level DB in level
edie the journal is now perpetual
journal we cut new files every sort of
500 megabytes and we keep a manifest of
which sequence numbers are in which
files
so with that manifest I can actually go
to the file and extract the value from
there and return that to the customer
and if the client had just done a head
request I can do just the same thing
only now I don't need to fetch the value
from the inka so the head requests are
now more efficient and because this
merge tree is much smaller because it
only contains keys and metadata it's
much more likely to be covered in the
page cache and that's much more likely
to be an in-memory lookup so it means I
can get very consistent fast response
times
head requests and slightly slow response
times to get requests so so we support
these basic operations put puts it into
the journal book attach to change get
I've been through their head is is
faster than again we also wanted to be
able to support indexes so when we put
an object that object may come up we may
have some index changes so we write the
object into the journal so we got it as
a permanent record but then when we
write the changes into the merge tree we
don't just write the key in the metadata
in the sequence number of the primary
object we also write additional keys for
any indexes that have been added so that
we can query those indexes in the future
to do efficient fold and well the
ledgers entirely in key order so we can
fold over there as those indexes and we
can also fold over the keys in an
efficient way if we're interested in
just the keys or the keys on the
metadata we can't fold over the keys and
the objects in an efficient in an
efficient way but we can fold over the
keys in the metadata but to support
folds obviously we don't want to block
up the penciler or the book II or the
inker whilst we're doing those folds so
we need to be able to clone the store so
we need to able to do snapshots and it's
interesting in I think in banja the go
vase version they said well we don't do
snapshots that's too complicated but in
in Erlang that's actually really easy
because the pencil and the Inca both
have a manifest of all the all the files
and every for every file we start a new
process so that manifest is actually
mapping in the case of the Inca from
sequence numbers to the process IDs of
the finite state machines that sign in
front of those files so if I want to
clone the database I just start a new
Inca or start a new penciler and I hand
it the manifest and that can talk to
those FSMs
directly okay so cloning is very easy to
do and the only thing I need to do I
need to keep a register of what what's
manifest sequence numbers the clones
were taken for by snapshots so that if
any if any finite state machine decides
it wants to die because it's been
finished with it knows it gets informed
to hold on until that snapshots expired
so
we can do clones in a very simple way
and then when we come to the to the
react world
yo react is yeah we end up with many
physical nodes down the side there and
vinhos running on those nodes and and
now when we want to do a get is we have
in the old world we've got a finite
state machine get started somewhere in
the cluster and it goes and calls for
three gets those three gets have to lift
all three values off the disk wherever
they are and then they have to return
them back and commonly they return them
back at the same time to the calling FSM
and that can cause some interesting
problems on your network when sort of
free gigabit connections going to one at
the same time and the problem called TCP
in caste that's caused by that so and
ultimately one of those is probably
going to get discarded two of those are
going to get looked at and and in most
circumstances only one of them will be
chosen so instead what we do now is we
do three head requests when we've got
two of those head requests from back we
can work out if we can actually satisfy
the answer by just asking for one get
request because if those particularly
those two head requests reveal they've
got the same vector clock or one
dominates the other and now we only do
one get request to fulfill that so we
don't have that race condition of free
responding at the same time across the
network and sending a large object same
thing when it comes to fetching the
values only one of the stores is doing
the value so as values get larger then
we do less and less this guy
overactivity to support those gets it
means that we have a slightly higher
median latency for gets because we have
to do ahead and then we do again after
so you know level DB with react is
faster in terms of median latency
forgets and but level IDI ends up with
some interesting characteristics so
where have we got to with level ed so
it's still a descent extent a work in
progress and it's largely a functionally
complete back-end for for react now
and the symmetrical testing that I need
to do around the object expiry logic and
because I think there's still some
things that are on solving react about
how to correctly do object expiry I've
done some initial integration testing
we've react and but I focused on doing
some some volume test really to compare
what happens with react and level ad
versus react level DB because and and
though is not necessary for I didn't
think it was necessary level II to
outperform level DB you know clearly if
we write something purely in Erlang our
expectation is when we though can go
then go and compare it against something
written in C++ or C when it's mainly
doing low-level IO type activity and
mutating stuff in memory that we may be
outperformed significantly so I wanted
to make sure that we had some kind of
parity with with level DB and as a
project in terms of test we've sort of
about that sort of 99% sort of test
coverage around CT any unit tests but
the main things we want to focus on now
is property based testing so Russell
Brown has helped with some initial
property based testing and hopefully
we'll be expanding that out going
forward so I'm so doing volume tests so
we know the primary advantage of level
ed is that it should reduce the activity
on disk okay and so the first thing I
test it with is you can have various
different configurations with react and
one of the configurations you can have
is have sync enabled which basically
says every time you append a new write
to the end of a nursery log or a
transaction log or whatever your
back-end stores make sure you flush that
to disk now we do that in the NHS
because you know data loss is a real big
headache for us and it's something that
we don't ever want to have so we flushed
that to disk every time and you know
we're helped by the fact we have flash
by write caches in our servers and to
assist with that
so the first testing I did was with
solid-state drives and we've sync
enabled
now in that environment we got a
significant throughput advantage with
level ed over level DB so that's running
react based tests are not necessarily
testing level ed just as a just the
backend testing it via react with all
the other optimizations for level ed
built-in to react and but that shouldn't
be surprising because the one thing that
flushing to this is going to do it's
going to hurt your capacity to do disc
IO and similarly without flushing to
disk and testing it on on traditional
hard disk drives spinning this we saw a
big performance advantage with react and
level ed as well on there but you know
hard disk drives are slow so perhaps we
shouldn't be surprised by that so I
focus inst on doing different tests
without sync being enabled and on solid
state drives to try and see well what's
different so that should be a better
environment for for level DB and perhaps
going forward be it will be a more
common environment for users of react
and and I've focused on situations where
the value is more than four kilobytes so
in the NHS our average value size for
our different clusters tend to vary
between about eight kilobytes and about
100 kilobytes and you know I think if we
go below four kilobytes there's on life
is the any advantage of separating out
the value in the key value store because
once you've gone down the merge key to
read the key you know it's you know it's
kind of a block size away so you may as
well have just read the value so I've
been focused on stuff more than four
kilobytes so all the testing I've been
doing has been on 8 kilobyte object
sizes as kind of the first multiple
beyond that and there it's the note and
when we do volume tests we tend to get
these types of shapes so this is a
volume test that did a series of gets I
think with five gets for every port and
then also run a load of two I queries in
parallel so the top line the green line
is the throughput that we're achieving
during the test so with the far side
being react combined with level ed and
the near side being react combined
level do leveldb and what you can see is
that there are definitely times when the
the kind of overall throughput is better
in level DB than we've level ed but we
consistently get a more regular
performance with with level ed so you
know these intervals are Thinker every
every 20 seconds so from 120 second
period to another with react and level
DB we're getting fairly significant
variance in performance whereas we react
in level ed we're getting very
consistent performance and over the
course of the test what we generally
find as we get towards the towards the
end of the text we get towards like I
think this one was a six hour test so in
the fifth and six out of those tests as
we've now got a populated database and
that kind of thing as we've grown beyond
the size of the page cache is we've been
tended overall throughput improvement in
react level out of between 20% and 50%
compared to react and level DB so given
that we've sort of introduced an
inefficiency of using Erlang and the
fact that we've actually gained a
throughput advantages is quite nice to
see and the bottom two lines are the
effectively the tail latencies so and
the bottom line be the maximum latency
in the lineup up in the 99th percentile
and you know the reason for that
volatile Atilla T and throughput is
really down to the to the big increases
in tail latencies we see on react on
level DB whereas react and level ed has
much more stable tail latencies so
though it started as a kind of personal
challenge and you know I think the
results that we've seen are genuinely
interesting and perhaps an indication
that there's there's some value in
pursuing it as a store and so so this is
kind of my first major Erlin project so
what did I find hard well and yeah
picking data structures I found hard you
know because obviously you know as soon
as possible in the database I want to
make things immutable and then it plays
nicely with early
but but at sometimes they have to be
mutable and and yeah I just made loads
of silly mistakes there and and use the
wrong data structures and so I've slowly
enhanced and improved that over time the
midst misfortune that anyone that works
with react has to deal with is that
react still only supports OTP 16 so
originally I wrote level ed for OTP 18
and I'm now had to make it back was
compatible with 18 and 16 and I found
that whole backwards compatibility thing
fairly hard to deal with and I think you
know it's now holding me back from
moving on to OTP 19 because I think
that's going to make life even harder
and be the hard problem that I've kind
of didn't really explain what what
happens is that we the ink is keeping
this journal this this long history of
keys and values but sometimes we update
our values and sometimes we delete them
yeah so that that bit in history that
previous transaction record is now
garbage so so how do we clean that
garbage up and so to clean the garbage
up to clean the garbage up is that we
have a background compaction process
that occasionally kicks in picks a
random set of keys and sequence numbers
from from each of the journal files a
sample from them and then goes takes a
snapshot of the penciler which has got
control of the merge tree and ask the
question are these sequence numbers of
the latest ones and based on the answers
he got it can make a decision about what
what amount space should be reclaimed
well from that journal and once it's
made that decision it can then decide
well that journal I can clean up for 40%
of the space from it so it goes so
that's worth compacting so it then rolls
over asking for each key whether it's
the most recent sequence number and
writes a new journal and then removes
the old one but that was a relatively
tricky to do and it's one of the reasons
why I passed sequence numbers
to the merge tree rather than just
passing over references to which journal
yo pointers to the journal and the
offset where the actual value resides
because if I was to use those offsets
the coordination involving in compacting
the journal would get too hard and I
don't know how whiskey and other stores
actually manage that because they don't
explain it in the paper and vino
coordination issues one of the things if
you're writing a back end so for react
typically you've got a dozen back end
starting upon one node and they all
start up at the same time so you know
we've hand sight it's fairly obvious if
you set a consistent buffer size they'll
all do their first right to file at the
same time and you'll end up with very
jerky performance and that kind of stuff
so there's a need for a certain amount
randomization to make sure that vinos
don't end up coordinating and
synchronizing and and that same thing
with doing compaction work and a
research stuff and longtail blocking is
that ultimately the bookies a single act
of a single cue at the front of the
store at the moment I haven't thought
about multi-threading it even the
ability to clone it for long-running
queries the reason why it's not being
multi fed is that is that it's designed
for react where in essence we've already
got multiple stores per node so that's
what's providing sort of concurrency
there so I didn't feel that sort of a
need to allow sort of a you know
concurrent updates to an individual
store so and but you know we've got to
make sure that that those actors aren't
blocked and I did silly things like you
know galling casts and going well wow it
took 40 milliseconds to get response
from the cast why's that well you know
you passed a massive you know 4,000 item
list what do you expect sort of things
so and I kept blocking it accidentally
were doing silly things okay and then
finally naming things I'm useless at
naming things so I regret every name
I've given every part of the service
including the name as the name of the
database itself
and so was it worth it I'm I'm glad I've
done it I've learned loads about Erlang
so you know for me not being a developer
by training being a network engineer and
you know it was kind of frightening
getting into the idea of writing a big
program like this and in particular
writing something in a language which
sort of other people were saying well
that's that's hard and and giannis I
didn't really find it anywhere near as
hard to get used to Erlang as I expected
the the ax model really worked through
it for me you know in my head it made it
much easy to think about how the system
would work I don't know how people
understand objects that makes no sense
to me whatsoever so so perhaps there's
just something odd about me and I've
been I'd say really pleasantly surprised
by the throughput comparison and the
fact that actually without doing any
special stuff we've managed to get
Erlang to perform and I did yet one
stage you know so I've got to do NIF's
here and that's spent a load effort
doing and file Ionis and this kind of
stuff and then found it didn't make that
much difference and you know added a lot
of complexity so it is still a pure
Erlang system at the moment and
obviously for those of us that use react
at the moment we're currently facing
some interesting challenges with the
future of supporting react as they may
know more basho going forward and so and
that's in some ways increasing the
relevance of having at the potential of
a pure Erlang back-end store because it
means that you know in order to support
react going forward if we have a pure
Lang store we only need to understand
airline we don't need to switch between
understanding Erlang and then
understanding some relatively
complicated C++ code so we are now
thinking of progressing and doing some
pre-production testing on the spying
project at the NHS and so I can't say
that it will ever make it into
production but it's now something that
we're seriously thinking about as
potential as a production back-end so
that is me
and you can find level they don't get
hub and you can find me on Twitter so
thank you very much
and so the question is have I tried
using it to replace over react stores
like the core metadata and and no I
haven't thought about that yet and react
also uses leveldb
in exactly of anti entropy store and and
and that's something that I'm actually
looking at at the moment I'm looking at
how active and entropy works and trying
to think is and you know should we carry
on using leveldb for that whatever
they'd be better or maybe does that
really need to be in that type of merge
tree type store as it suited to that and
so but I've never I haven't thought
about the core metadata at all and and
so
you
okay so yeah if you if you speak to
afters and that'd be great thank you
yes Oh memory management I decided that
I was too stupid to do memory management
but and but the page cache is my friend
so if we go back to the picture and I
had here so and the the only stuff we
keep formerly in memory so you make a
conscious decision to keep in memory is
the bookies current cache of the most
recent keys and metadata which is only
about sort of two thousand three
thousand keys and metadata and then the
pence the stack of that cache which is
about 32 thousand keys and metadata so
it's not a huge amount of memory and
then at that point once we get to 32
thousand keys it gets written to the
file and all of those things are written
to file and we don't do any caching of
it whatsoever
we rely entirely on the page cache so I
do some F advised stuff so we F advised
so that we prefer to cache the stuff in
the in the in the pencil as merge tree
in the ledger and in the over in the
journal which is the transaction log of
all the keys and the objects and they're
actually using DJ Bernstein's constant
database format and and that has a
lookup tree at the end so we F advised
the lookup tree and don't F advised
anything else and then we just leave it
up to the page cache to decide what
should be in there and what shouldn't
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>