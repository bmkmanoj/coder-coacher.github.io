<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Robust Data Processing Pipeline with Elixir and Flow - Laszlo Bacsi  - ElixirConf EU 2018 | Coder Coacher - Coaching Coders</title><meta content="Robust Data Processing Pipeline with Elixir and Flow - Laszlo Bacsi  - ElixirConf EU 2018 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Robust Data Processing Pipeline with Elixir and Flow - Laszlo Bacsi  - ElixirConf EU 2018</b></h2><h5 class="post__date">2018-05-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3XL-31nqcX4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Lascaux is case
science is a cooperative of developer of
around 10 developers and reworked
positive for all client projects being a
cooperative means that it is it's a very
flat structure and us partners are
responsible for everything from finance
to business development development
leadership it's about robots to the
position is here and flow do I have some
kind of microphone problem yeah okay can
we fix that should I change to the N
throat microphone
okay so let's hope it's it's going to be
okay so before I started talk I've been
a naughty boy and I didn't actually
upload or send my slides to the
organizers so if you want to follow
along in your devices okay so does this
work
this is close close that's good okay so
before I started talk let me start with
a few disclaimers this is my first
Alexia talk so please be gentle to me
but if there are things that you think I
could improve or if you point if you
find errors in my talk please come up to
me afterwards and I'm happy to receive
constructive criticism also there have
been a bit of
error in keys introduction is that this
is not actually based on completed work
unfortunately so when I when I submitted
this talk we had this project lined up
and we thought that it would build this
VTX here we only had a pilot done at
that point in Ruby that proved that
concept was right it's a project that
that the client needed but for various
reasons the project on akane started
effectively being worked on in january
and even in the last few months we had
to have to spend a lot of time not
actually doing elixir development but
figuring out how to work with redshift
how to how to deal with the data etc so
there are some things that that we
actually haven't tried in production yet
but we we have played with bit a few
things and in our point out when we come
to that so as I said this is based on
client work in the client is a parcel
service company in the UK their name is
Colette class and I also mentioned that
we are we are moving data into redshift
and that will have some peculiar things
that we had to deal of it and also
lastly in this as a disclaimer I also
wanted to say that all pipelines are
custom and you can't really expect to be
able to use exactly what I show you in
this talk in your situation so be sure
to to research your pipeline make sure
to to find the right values for fine
tuning the pipeline but I hope that
there are some bits that you can take
away here and be able to use that in
your work so what are we going to talk
about first we want to build a mental
model of a pipeline how to approach a
data pipeline problem then if it's
necessary i we can talk about some
basics of alexia flow obviously about
that
and specifically I want to talk about
mostly about how to compose flows how to
create maintain or maintainable flow
architecture and talk about failure
tolerance how to deal with errors issues
supervision and lastly to hope to be
able to talk about some kind of
scheduling as well some ideas for
scheduling so a little bit a little
exercise first back when I was really
young I think this was probably
preschool or elementary I was already
fascinated by by mathematics and
unreasoning and I loved these kind of
exercises that we had it was it looked
like there was a machine and the Machine
received some kind of input and produced
some kind of output and there was always
a rule table that attached that you
could you could see and and try to try
to understand what the machine what what
a machine did and then we had to find
out what it was and followed that rule
and apply it to the rest of the rest of
the input and output values so in this
case we have three freeze brackets with
columns that are already filled out so
we know that this these are the inputs
the green ones are the inputs and the
red ones are the outputs so who here can
tell me what this machine does
okay I'll give you another few seconds
yeah there's the hand there yeah it's
again it it removes what's sorry like
yeah nah not necessary so I want to
remind you that this is a test that
elementary students get so preschool
it's not difficult
yeah it turns green things in turn
things yes that's one thing that it does
Thank You Keith what is the other okay
so if you look at the first three
columns you'll notice that the
difference between the number of green
circles and the number of red circles is
its constant so I think it what it does
it's going to subtract how many eight
eight green circles and turn the rest
into red so these were always simple
machines but that's what you get when
you're only six years old but I was
always fascinated by this and I was at a
young age very interested in knowing
what these machines nod and did and and
how they did it
it's it's very interesting that because
even though I remember this now I never
thought about this before starting to
use Alex here so in Alex here you always
work with some kind of data input and
want to turn it into into some kind of
output and this is as ASCII said this is
something that that we are very used to
in the next year working with pipelines
working with data transformation so
there is the same thing but when you're
trying to approach a data pipeline a
data transformation pipeline on a very
high level if you want to move data from
one place
and movie 2 into inter data warehouse in
our situation is the same kind of
approach you want to take even at the
highest level so how do we approach data
project what we eventually want to
achieve here and this is the basic
requirement that the client is giving us
is that we have some kind of data
somewhere sitting in some kind of places
and then we want to understand things
using this data we want to get some kind
of insight out of this so this is the
highest level of the picture and our
what we want to do is we want to find
out what is this machine doing what do
we want it to how do we implement this
how do we how do we know how to do this
machine thing here so fortunately for us
we can go one level deeper immediately
because we don't have to care about how
to how to do the inside part what we
want to do is get the data into a place
where in the marketing department where
the business can run the queries that
they want to run some analytical queries
so we can simplify our pipeline into
into this machine which takes input
force from source feeds these are feeds
that look almost like CSV but they are
not and then we want to do the machine
thing and turn this into or put this
into into a data warehouse so again what
does this machine do we know that we
have source feeds and we know that we
want to put this into into redshift
OData warehouse and we know that the
source feeds are not exactly in the
format that is easy to import into
redshift because as I said it it was not
proper CSV and ratchet has a very very
useful command copy command that can
take thousands of files at once and and
load them into interactive very
efficiently so you want to make use of
that so first we can turn this the Tonys
problem into
into three steps first we want to
prepare those feeds they want to import
them into interactive and then we have
to do some kind of processing on that
data some kind of transformations
aggregations etc here the what we see is
as a is the regular pipeline ETL
pipeline some some people call it it
here some people call it ERT yet is
usually what people do nowadays with
with cloud providers we don't care
really about the transformations first
we want to load the road a tie into the
into the database and then do the
transformations later and that's what we
can as well achieve it with redshift so
let's also let's again go one level
deeper we only going to look at this
that step one the prepare flow and this
is when we are starting to see things
that we can already easily express in
Aleks here as a pipeline as a data flow
in step one what we want to achieve is
we get the source width and we want to
turn them into into proper CSV feeds how
do we do that this is this is going to
be the first our first glimpse to flow
so in flow with Aleks it flow you can
express a complex data transformation
that runs potentially in parallel in
multiple processes has potentially
multiple map and reuse stages and you
can still express it in a in a very
familiar elixir pipeline way I don't
only want to point out for now that the
basic steps of this transformation is
taking the feet making sure that it's a
it's a it's a feet file then up
downloading that form from from the from
the s3 bucket we have to do the prepare
preparation steps on it and uploading
back into an s3 bucket so this is the
basic pipeline and we can already
Express it
in a very very easy way those are the
map stages and in a review stage we just
collect those those feeds into you into
lists so what you see here is all the
way from the highest level and all from
there all the way to to the to the
lowest level in a next year what we are
able to do is express everything as a
pipeline it's a series of cubes if you
were around a decade ago when this was a
running meme so it's pipes pipes and
more pipes so if you go if you go one
level deeper into into one of those
functions prepare feed again what we see
here is basically beginning one level
deeper but we're still just seeing a
pipeline we are expressing the
transformation as a pipeline again okay
this is a very short glimpse into
flowing so let me ask you now whoo-hoo
of you know what Alex if flow is raise
of hands okay that's about half of the
people he who has used Alex if flow
okay lot less so let's do a quick crash
course on Alex year flow when I was
trying to research Alex tree flow what
the first thing I did was what every
other developer does do a Google search
for it and it was very interesting
because in the first ten results
I think only half of them was about
actually the Alex T language flow
library there was a cocktail called Alex
fear of love cocktail flow on website
and also there is a Hungarian result
there the second one is asking if flow
Alex here is a drug or not okay but
while doing a lot of research on flow
and searching for for what people did
how people solve certain issues I was
able to train Google into knowing what I
care about and now I think these these
searches are only in the second or the
third page okay so what is Alex hero
the documentation flow allows developers
to express competitions and collections
similar to animal stream models although
computations will be executed in
parallel using multiple stages I would
add to this that yes this is the main
purpose of flow but I think what is also
important about is that it allows you to
express these computations in the same
manner as you approach very simple
problems in an exhibit with the pipeline
operator and I also wanted to say that
this is usually the documentation what
it says here is that it's usually used
it's used for allowing computations in
to run in parallel and that's certainly
the place where you should be using flow
so you might get away from this talk
after after I finish and go home and try
to rewrite everything in Aleks a flow
please don't do that so there are places
to use flow and there are places where
it's it's probably an overkill so it's
usually best use it when when this is
your only simple way to express the
computation that you are wanting to do a
parallel computation in a in a simple
manner but in our case I also wanted to
say that I think that in some places
where we are still using flow it might
not be the best tool to use because of
being an overkill but in our environment
we don't actually deal with I know
he said billions of billions of rows and
that's certainly true in a historical
scope so we have data up going from 2012
and it is more than a billion rows in
total but on a day to day basis we are
dealing with much less data where Elesha
flow is not actually not actually
running into into any of its limits so
there we can just use it because it's
it's it's it's something that that it
allows us to express it from
computations easily ok so let's look at
the current canonical example as way of
doing a crash course
so the Carioca
in an example on the on the in the
documentation deals with the problem of
counting words in a file we want to know
how many times different words occur so
if you do this with the built in a new
module what we do would would be
probably doing a flat map splitting the
strings into words so we now have a list
of words on all the words in the file
let me do a redo step in which we create
a map of those words and each value in
the map would be the number of
occurrences of that word so this is
simple this is this is good but if we
are dealing with a large file this would
take a lot of memory and it would it
would iterate over that list multiple
times so we can easily make this better
by using stream instead of ennum so if
you use stream here just doing the flat
map operation and stream instead of enim
what we're getting is a limited usage of
memory because we are not we don't have
to deal with the whole file in memory
but we can just go line by line and and
to count the words in each into those
lines and only do the reduce operation
as one snap so this is already giving us
some benefits but it's not giving us
concurrency parallelism
so this is where flow comes into picture
with flow we are able to we are able to
run this same pipeline or can the same
words by using multiple cores that are
available in the machine and and I'm
doing both the map and the reduce
operation on multiple processes so I
advise you to go to the documentation
read it it's it's really excellent
documentation as usual in Aleks here and
after is very short crash course next
let's try to look at the things that
make it easier to work with flow and
I'll have two main sections
maintainability and failure to arms
maintainability what I want to talk
about is composition composition of
flows
and branching off loads in composition
why we had to look at composition is
because we want to be able to use this
flow in multiple modes of operation in
our project we have as I said historical
data that goes back to 2012 it is a huge
data set and we potentially have to load
this data in one go from doing doing
doing going through the whole pipeline
but we also want to retain the ability
to do just certain bits of the pipeline
at any stage so we might we want mi
sorry we might want to do prepare import
and process all data from 2012 to 2015
or we could we would want to readjust do
the import in the process step report
and reprocess everything for year 2016
we also want to use the same flows in
our real time operation and in real time
what we are looking at we want to
monitor the source wise it's a stirry
bucket so we probably can use something
on as AWS to do that and we want to pipe
that into into our whole flow and run
the incoming data through that so how
can you solve composition what we found
that it was very easy to do this with a
villa next year because of pattern
matching and functions so the commercial
that we ended up using was to create a
module for to to deal with each of those
steps in our pipeline so the first step
would be preparing the phase that will
be one module then import feeds would be
another module and then other process
feeds is again another module and each
of these modules we have the convention
of having a flow function and the flow
function can take two types of arguments
it can take a regular enumerable
whichever enumerable you you want to
pipe into that it could be a list could
be stream usually it's a stream but it
can also take a flow which is also a
memorandum herbal and if
the flow that we get as an argument then
we actually do the implementation of the
flow in that function but if it's any
other in any other it's okay so the flow
takes in a member and returns a flow
struct and if it is a flow that we
received and we actually do the
implementation of our flow but if it it
is if it is any kind of enumerable then
we first turn that animal into a flow
structure and pipe that into into the
other implementation now that we have
this this works we have a source of
feeds we can pipe it into prepare feeds
that flow supply pipe its result in to
import feeds that flow and remember that
the output of prepared feeds is a flow
and that would be the input of the next
step in the pipeline and lastly we can
just call flow run to to run this in the
cinder in the process we can also do
this we can just run the second and the
third step in our in our pipeline and if
you want to we can also extend this
further and accept different again
different kind of input we can we may
decide that we want to handle the input
to our flow as a gen stage producer and
if it if it's that then we can extend
this function to accept a giant stage
process ID and use that to prepare our
flow for us now let's move on to
branching and in branching what we
usually want to achieve is we have some
kind of place in the pipeline where we
want to make a decision and deal with
certain kind of data in one way and
other kind of data in a different way we
might have a diagram like this where
branch a only has two stamps why branch
B has maybe three steps it has to do so
for this there are multiple cases and
strategies I think you probably want to
think about your problem and and decide
which of these or maybe which which what
other way would
would make more sense to do I think the
simplest case is if you keep the clip
keep the branch is inside the same flow
and I think this works for were simple
cases especially if you want to join the
data together later on in the flow but
there are situations when you probably
want to deal with the data entirely
separately especially if you if you
don't want to rejoin them together and
in that case is flow into stages might
work and then you can create gen stage
consumers that will take on take the
output of your flow and deal of it a bit
differently let's only look at the first
example because that's what we're
actually using in our project the
branching of the fit simple branching
example in our case we when we receive
the feed we want to be able to do two
separate things on those features are
very similar but are have but but have
some different implementations based on
on the types so what we're doing here is
first we are duplicating each of the
items in our flow and annotating them
with a branch identifier it would be it
is
simply type A and type B in this case
then we create a partition exactly into
two stages we have a max demand of one
here because we know that this is a
bottleneck in our flow and dB we want to
be able to to join all the all the
streams in this part of the flow into
into a single stage later because then
we hand it over to redshift for import
and that's that's the most efficient way
in our case that redshift works so what
we do here we partition and we specify a
hash function here to make sure that
type a is going to be directly into into
stage zero and type B is going to direct
be directly into stage one we could use
a key function as well but if when I
looked at the implementation of
partition specifically partition
dispatcher
that is going to take this I saw that by
default if you use key it's going to
hand it over to an airline function that
hash is that term and create a hash
which might return the same values if
you have a very limited number of off
stages so in this case it's better to be
specific and use the hash function and
specifically say which stage you want to
direct items to then we have the map
stages and these map stages actually
happen in parallel so they work in
parallel on type a and type in in in
separate partitions and later we group
them back into that together to create
the output of the flow s as the map as
the original items items keys so the
simple branching example now let's move
on to failure to learns and let's see
how we can deal with issues that might
happen in a pipeline I think there are
certain kind of issues that there are
very easy to handle then other types are
you be probably here are going to have
have a hard time we had a hard time
complaining so I think there are two
sides to failure tolerance one is error
handling how do you deal with errors and
the other one is supervision which is
usually a very close very close topic to
you to failure tolerance let's deal with
supervision first because that'sthat's
probably the simplest topic how do we
start a flow in a separate process that
is very easy to do with flow startling
so this is similar to flow run but
instead of running it at the same
process it's going to start a new
process and link it to the current one
this is still an unsupervised process so
if you want it wanted added to a
supervision tree we probably want to do
something like this so we have a start
prepared flow function here which would
start our first stage in the you know
pipeline if you remember
and this create this creates the input
for our flow and turns it into into the
actual flow of its speed phase flow and
we want to do something like this at the
esterline steps start flow supervised so
how do we do this we create in our case
we created a dynamic supervisor because
what we want to achieve is to have to
have the ability to to run one of these
flows maybe run multiple flows at the
same time
other times we probably won't going to
be running any flows it is supervisor is
responsible for our historical imports
so we're not going to use this for for
the real-time processing so in this case
it's it's usually just at most two flows
that that would run in parallel so
dynamic supervisor fits this this
purpose very well so we can think the
time is super ready now in our main
supervisor here and then we the
implementation of start flow supervised
is actually just starting a child in
this supervisor we first need to create
a child's pack it's a simple choice pack
that has a random ID with make graph the
start function would be the one that
actually says actually actually it
passes on pass this is the flow into
into the start link function and okay
and let's see if you go back to you
might see that we are using a temporary
starting strategy this is probably if
what you want if you don't care about
the can't care about restarts for this
flow it might be the case if if you are
looking at a very big job so for example
if we are trying to import years worth
of data and some issue happens close to
the end you probably don't want to rerun
everything if it's trend already for for
multiple hours or maybe days
so let's make sure just make sure that
you're using the right restart strategy
okay now let's go let's move on to
Edward Henry strategies here
I think the sleepless one is let it
crash and this is the the airline mantra
so this might work in simple cases we
also have another symmetrical flag and
filter we'll talk about that more and we
can use the previously mentioned
branching strategy as well so let it
crash it works well if you have an item
put on pipeline which is usually a very
good idea anyway if you are working with
the data pipeline you never know you
usually don't know if if if you only
receive an input but a single time or
multiple times so it's always good to
have right about that pipeline pipeline
and if it's apparent that you can just
restart then maybe let it crash is what
works best for you it's also good if the
occasional crash has a very low impact
flag and filter this is a bit more
interesting so what you can do in a
pipeline you probably don't want if it's
a complex pipeline if it's something
that is running for a very long time you
probably don't want to let it crash
because that way it means that every
work that the program did so far you
might have to repeat again in that case
a flag and filter works good and what
you're doing here is you create special
functions mapping functions and maybe
reduce functions that deal with with
error handling in itself so here each of
these functions get with repair feed
upload feeds would accept two kinds of
arguments it would be a video to poll
okay top of it a value or an error top
orbit an error reason and the
implementation is very simple here if
it's on okay topple then you are just
doing the work that you're supposed to
be doing if it's an arrow top or you're
just passing it on to the rest of the
pipeline
then at certain stages in the pipeline
you insert filter from filter steps
where you look if you want to every
error that you received so far and then
filter filter them out and the
implementation again is very simple it
is an opiate Apple then you're just
returning through saying that this is
something that you want to keep if it's
a narrow tuple then you lock the error
and return for us to reach reject those
errors from that point on in the rota
pipeline the benefits of this is that it
has minimal impact on the pipeline
structure so it's almost the same as we
saw before and fill your hand having
finished any checkpoints in the pipeline
makes it actually easier to comprehend
what's going on and how we deal with
errors and it's also recommended to put
the checkpoint just before the reduced
steps because that way in the reduce you
are dealing with clean data you don't
have to deal with with how to how to
collect the error tapas unless that's
something that we want to do when you
actually interested in in knowing how
many of these errors were there and
third strategy dimension was branching
the errors now this is a more complex
approach and I wouldn't really advise
you to do this unless you have a very
good reason for for it because this is
where you are starting to lose the
expressiveness of the flow so in this
case what you are doing is using the
same strategy as mentioning branching
you are branching on the okay and error
error atoms in this topple and it's I
think it's this is only advanced if if
this situation calls for it and if the
number of errors that you're seeing is
comparable to the number of success
values that you're having because that's
when the partitioning that we use in
branching works works the best lastly
let's talk about scheduling in
scheduling we can already do some kind
of such class scheduling very simple
scheduling by using those functions that
we created that compose our flows so we
can
we can have a have a mix task or a
production test that runs one of those
flows manually if we if we call it on an
API or or some other way
but for real-time processing we probably
need something more robust and in this
case we are looking to use RabbitMQ tmq
here I say we are looking to because
this is the part of the project that we
haven't actually started yet we sat down
with with Olin solutions I think a month
ago and thought of talk through much of
these and and they were suggesting to
use RabbitMQ and the idea here is that
each stage that you want to represent as
a single step you can create an
application for that which which
contains that that flow for that stage
and in that init application you have a
consumer which would be a rabbit MQ
consumer and that consumer could be
actually a gen stage producer for your
flow here so it consumes items in the
queue in the in this case in the that's
the feed skill and get the names of the
files from that queue in the consumer
and passes that on to into the flow via
a gen stage producer and the output of
the flow would again be against this
consumer which in turn is a producer
into the next queue which would be the
input queue for the next step this might
be a bit bit much take into at once but
the idea is very simple we are using
example producers but really them may be
temp you consumers as producers into the
flow and using gen stage consumers as
the last step of the flow which is again
a producer into the next queue so this
is still to be developed and we haven't
actually started on on on this yet of
it's still in the drawing board and
adjust the other day I found a package a
library that might actually do exactly
what we want to do and this is called a
rabbit it's on github and I think what
it does if I see it clearly is that it
is a gem stage producer that consumes
consumes a rabbit mqq you say yes you
have to try it as a sell so in summary
we've seen that Alice if flow is more
than just parallel processing it allows
you to be able to express your
computations very similar way than
you're used to in your other elixir code
it it also allows you to break up
complex complex flows into composable
stamps as we've seen and if you have to
deal with errors and in development we
all have to deal with errors then it's
best to do it in the right place we also
seen how to supervise flows and we also
had seen an idea of how to combine this
into some robust scheduling with the use
of
rabbitmq if I have more than that later
I might have another talk next year
thank you for your attention
and these Stanford and reading I only
want to point out here the documentation
again and there is a very excellent
series from Ronnie dr. Rainey urinating
on architecting flowing in our exit
programs now here in this case flow
doesn't exactly mean the library flow it
is it means flow and flow of the program
for the data in general and he has I
think three or four articles in this
series which is a very good series and
actually one of them about the token
approach might apply great for for even
for LHC flow the library flows and you
can also find these slides on the link
delights I gave you at the beginning
thank you
okay we've got a minute for just one or
two questions if anyone's interested
okay hi my name is Anders thanks for a
nice talk
so I'm asking out of ignorance what's
the cost of using flow it sounds like
you get multiprocessing and all sort of
great stuff but is there what's the
downside good question so the downside
is that you what did the usefulness of
flow is that it creates those processes
for you but what it does behind the
scenes is actually creating gen stage
producers consumers and and creating the
fluid those for you have many advantages
that for you mean in a super vision tree
the downside is that there are times
when just overkill you actually could
just do the calculation the processing
in your in the in your own process it
might be cases when it's actually
simpler to just create the stages for
yourself if it's just a simple simple
matter and and that's it really so you
have to be aware that it's creating
multiple processes in in in the in the
behind the scenes and actually this is
something that we also looked into you
because when we were talking to you to
people about flow some people mentioned
that was a some somebody mentioned that
it's probably not optimal to do what I
was doing here let's look at that okay
so if you remember we had this map stage
is lined in to each other
filter map map map so they were saying
that in this case we are actually
passing messages between all of these
map steps here but created to you to the
supervision tree that I see in observer
and and the flow code that's not
actually what's happening so flow is is
very intelligent about how to map
out those those stages so it's actually
use it's just a single process for each
of these stages in a map and would
combine those operations together okay a
lot of questions
I think well yeah I think we're out of
time now okay thank you okay thank you
out of applause please
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>