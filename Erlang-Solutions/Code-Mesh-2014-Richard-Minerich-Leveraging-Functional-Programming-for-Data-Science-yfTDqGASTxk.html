<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Code Mesh 2014 - Richard Minerich -  Leveraging Functional Programming for Data Science | Coder Coacher - Coaching Coders</title><meta content="Code Mesh 2014 - Richard Minerich -  Leveraging Functional Programming for Data Science - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Code Mesh 2014 - Richard Minerich -  Leveraging Functional Programming for Data Science</b></h2><h5 class="post__date">2015-01-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yfTDqGASTxk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm a director of research development
at Bayard rock we do anti-money
laundering research and development
we're owned by another company that
deals with all the banks and so we get
to do lots of greenfield stuff and it's
a lot of fun we do solve a lot of
problems that are pretty annoying in
their own way I'm going to walk you
through it first what we do so there's a
lot of kind of risk that banks are
worried about their is not only law
enforcement and terrorists and drug
lords they also like mandated by law to
monitor people that could be doing money
laundering like ambassadors or ministers
of finance in their relatives so there's
a very sort of complicated system around
all of this I'm not going to go into the
details of how the banks deal with it
they have their own internal sort of
processes also based on the country
they're in for exactly the action they
take but the pretty much the main goal
of what I'll be talking about is finding
certain people inside bank data so we
use primarily the flag a center model
there's a lot of newer techniques than
this but they don't scale nearly as well
and we're dealing with you know for some
banks 200 million records versus our
lists which are well over two million
and so if you talk for some of these
like N squared then it's not really
doable under any circumstances the first
sort of phase you have to have some sort
of heuristic blocking essentially what
that does is use some sort of fash
string search to determine who is
similar in some way for further
evaluation later the most important
thing here is you have really high
throughput and it's fast then you take
that and you like derive some sort of
probability for each of those pairs and
what you get from that essentially you
carve up into buckets in the bank's deal
with those buckets accordingly this is
the general process that almost everyone
uses except that most people don't have
a scoring phase most of the people that
do this they just have some sort of
blocking and they think
well and then they write a lot of rules
but they don't have any thanks until any
machine learning features or anything
and it's really kind of sad because they
drown in all these false positives so we
help them with that um so blocking
there's a lot of techniques that we've
investigated primarily what we use today
is a suffix array based represent their
suffix tree based representation because
its memory efficient and also pretty
fast suffix arrays are also quite useful
but we're also looking into some other
technologies with partners mrs. some
graphs from an eval that I was doing and
essentially what this partner does is
they can match across languages which is
really quite nice if you want to work in
other countries but your list data is
primarily in English or non not even we
have quite a lot of Unicode extended
Unicode but not like a lot of Arabic
names and such right as you can see on
the bottom left here the black part is
how much they got and the red part is
how much they missed out of we had a
complete truth set for this and so I
knew how well each of them did across
all these languages as you can see
chinese and japanese are not so hot
they're not that much of a concern
what's really impressive is how will
they do in arabic which is quite hard
but their chinese really if you look at
the threshold versus recall here just
drops off suddenly in a weird way and
that just is very indicative that it's a
there's something wrong with their
algorithm I ants working at a small
company where you have to make
everything and deal with all of the
problems not just play with fun
algorithms um plus the reality is kind
of stark when it comes to play gate
sumter as well this is actually from a
William Winkler paper who works at the
census any pioneered a lot of this stuff
and you can see that there's two
gaussians and one is a little bit bigger
and then there's one that's a little
more spread out on the right the right
is made of the truths and the left is
made of the false positives from this
blocking process now this is a real data
this bottom part is real data that we
have
and you can see over here that our two
gaussians are incredibly tiny and
they're hugely dominated by garbage in
general and so we work really hard to
try to separate these two things I mean
it's very beautiful in the theoretic
picture or even what they're able to
achieve at the census but they have a
lot more data I love the bank data is
very bad and yeah and as the numbers bro
it gets harder subdivide things out more
easily than us because they have so much
more data and also they don't care as
much about missing people like if you
missed in the census it's just like oh
you're off by some margin of error but
if we miss you know some like some
al-qaeda or Isis terrorists it has money
in the u.s. they're going to use for
some sort of action then well that's not
going to look very good in normal virus
software ever again that's not very good
um so like to deal with this problem we
take a number of steps one of them is we
include a risk measure along with our
ranking measure and the risk is
essentially extension to page rank this
is just a visit from the patent and
essentially it's just an iterative
process though it takes into account
information flow over a graph that has
website and it has people and their
connections so you know if you're
connected to Osama bin Laden you're
gonna go man I could be pretty risky
because a lot of that rissalah's flow
unto you so don't make friends with
terrorists right especially really nasty
ones that are well known and this works
quite well actually this is one of our
core sort of features that makes us
stand out some of some companies just by
our rank data most of them use our
entire solution but we do have a couple
customers fittest by everything's so
what we do this is also data from a
customer site is we project this wrist
first probability log probability pretty
much we scale it so that bankers can
understand it better and then we carve
that up into zones essentially generally
anything that seven are above is
considered automatically true with just
past
review but then they have these armies
of people whose jobs that are to review
everything else now normally they'd have
to review all that green stuff too
because they don't actually use any kind
of probability measure at most of these
banks but when they use our software
they only have to look at those yellows
which is a tiny tiny fraction actually
this is a problem for us in some ways
because people don't want us to come in
and destroy the size of their teams of
these banks because they're important
when they have a lot of people under
them so yeah that's one of the usually
it's only when our best customer
acquisition is when they're under threat
by the government because then they are
like panicking to deal with this giant
pile of the green stuff over there and
we come in were like no problem we could
handle that at ball we have a database
that's nicely viewable through all of
this this person is not a nice guy he's
part of a Mexican drug cartel or
connected to it he's a part of the Hells
Angels quite an exciting guy right um
and yeah definitely a very risky person
um we have lots of citations to indicate
that he's risky from the Federal Bureau
of Prisons the Department of Justice the
New York Post even you know and we have
full text for all these articles for
information extraction as well these are
some people that this person is
connected to I don't really know these
guys well are in Gauss but you can as
you can see from there AI scores they're
quite risky and if you clicked on them
you would see like their connections and
who they are also connected to most of
those I'm sure like Hells Angels
especially in Canada and Mexican drug
cartel people in Mexico and such with
all this information and some additional
digging we sometimes put together
pictures like this just to entertain our
banker friends essentially you know this
is like the flow of drugs and money this
person was facilitating utilizing
different prime families and cartels in
different parts of the world
you know in Canada they would use the
Hells Angels in the New York area they
would use the Bonanno crime family and
then they would use the Sonoma cartel in
Mexico and essentially just this the
cycle of money where it would be like Oh
cocaine to marijuana and then back down
and yeah it was pretty pretty amazing
the unfortunate thing so I really like
this part of my job you know dealing it
feels good to find bad guys and we don't
we were like really careful about ethics
we don't find people that are like oh
man you were late on your taxes one time
I'm sorry yeah it's not like that these
are all legitimately bad folks with lots
of citations but we end up spending a
lot of time on the munching of data and
I don't know if like most of you guys or
gals do like data science stuff but I've
found that to be like the biggest time
sink like just converting stuff too
right format making a fit memory all
those problems and also there's a very
subtlety of the work itself like you're
using tools you have to configure them
properly and you make one mistake and
then a week later you realize it and
that's a week of wasted effort right
it's it's very difficult myself you know
you like miss some fields that are
really important or you get like
unprepared Atta and shoved in the sequel
fields even like the wrong versions like
if you're if you're comparing to
blocking engines and you're using one
data set from last year and one from
this year even if it is the same
underlying data set it's going to skew
the results badly and the bad
configuration dependencies is always
problematic because we have to work with
all these different Bank blocking
engines essentially the things they
already use to define bad guys kind of
with lots of an army of humans reviewing
so configuring those properly or
convincing the banks to configure them
properly can be incredibly difficult so
I've become very paranoid double
checking things making sure that
everything is correct before I put
pounds of timing cuz I've already lost
months of my life to various problems
there's no good functional programming
language solution to that I mean what I
really want is something like it for
data I think that would be beautiful but
it's really there's nothing
quite like that yet some stabs have been
taken but the other problem we have is
every single bank customer has
disgustingly bad data but every time
it's different and how it's disgustingly
bad just recently we had these cities
that were jammed in with all of the
information and for the Canadian
customer is just jammed and city and the
address was null and you're like oh my
god or like the one of the other ones
that we see often is 11 1979 for the
date of birth which means like not like
we don't know the date of birth but
there actually are people born in the
day so it's like why did you choose that
right and also we deal with a lot of
episodic data from mainframes so like
this larry o'brien his middle initial
isn't actually oh it's a he's like just
there's no apostrophe so it's just a
space instead and you know you have feel
that resolve those kinds of subtle data
problems and not miss the bad guys which
actually can be pretty difficult on site
at the bank that's the other problem we
have two kinds of customers for some
banks they host their data and me to
like have control over it make sure
everything goes well push things as
necessary and lunge it and for other
customers we send a couple of couple of
people to the bank to work on site on
whatever Hardware they give us excuse me
not so good and like whatever data they
give us and sometimes we even have to
put the thing into their subsystem to
get results out we can't reconfigure it
it can be a real nightmare so we work
really hard I'm trying to make sure that
we can address these kinds of problems
on the fly as necessary so all this
motivated something we call safe alert
manager it's a lazy functional program
with a gooey on top or a console on top
depending on how you want to run it it's
extensible for extracts games from the
load it's got additional ad hoc machine
learning features as necessary and
programmable data cleaning this the
program el data cleaning stuff is
brand-new and I'm going to talk about
that a bit because I'm excited about it
as far as the data in and out those are
just like kind of like I think of them
as codex because I used to work in which
processing company and so you know you
can take CSV
custom files from these blocking engines
or whatever else so in general we can
break down this sort of process into
five different components this is just
the scoring part that's our scoring and
chunking is what Sam does the blocking
engine is usually left to the bank so we
have to extract stuff from the records
and depending on what this is from that
can be completely different like if it's
a swift message there might be stuff in
additional fields we need that need to
pull out or if it's a one of them was
like okay we have data multiple dates of
birth per customer and there's dilma
device semicolon and you can't just like
like put a patch in you have to figure
out a way to clean that out on the fly
separate those out essentially it's like
a collect and functional programming
where each thing turns into more than
one you can't just wave out everything
after the semicolon then we have a
cleaning phase which essentially or
structuring phase essentially you take
things you pokin eyes them you move
characters you don't care about you make
sure the date of birth is something
valid you actually sometimes we have a
used regular expression histogram to
pick the correct date of birth if
sometimes data sets are merged when they
have different date of birth formats in
the same data set things like that then
we feature eyes it we score it and then
we slice it up into those things before
I seen on the graph but each of these
phases is extensible slicing is only
configurable you can specify your slices
that's quite easy but the rest of these
were quite a challenge to figure out how
to turn into something that is
extensible now features are also pretty
easy those generally predicates right
it's really easy to write predicates but
to pull in complete a vehicle to pull in
sort of functions from a configuration
file to like change how the data is
clean took some thought so you know each
customer or listed record can have names
dates of birth country states and cities
in reality the states and cities and
countries can be linked together or not
depending on if we
have any information to thank data but
this is pretty much what it looks like
and then we have a collection of
functions kind of like weed
configurations sort of like Excel where
you can pick from a drop-down list of
the different functions and we can just
specify oh okay for the customer names
we want to strip out the accents and
then we want to do a one-to-many
conversion with like this nicknames file
or something like that and it's it's
pretty simple overall but to use to make
it it was actually pretty hard
underneath the hood this uses F sharp
quotations which are kind of like lists
lists quotations essentially it's a AST
that you generate on the fly and a
compile so then I as I rebuild this list
structure I inject these functions into
it in order to transform the data as its
rebuilding it now the benefit of this is
that everything is only allocated once
it goes through all these transforms but
there's only one allocation for the
entire structure the other way I was
thinking out doing and initially was
with lenses and the problem at lenses
each lens requires a new allocation of
the entire subtree like everything up to
the top node and that actually was
pretty slow and this is much faster I'm
much happier with this two kinds of data
cleaning there's advanced quote unquote
which means don't touch it unless you're
a programmer and there is a basic and
this is an example of a basic data
cleaner where we it's a map right and we
know it's a map because it just works
over strings and so what we're pointing
it at is a string array and we know how
to map over string array so we can say
you know for everything in the array
apply this to it and stick the result
back in its place that's pretty easy to
do and we parameterize it with some
additional well in this case one
additional parameter which is to space
and that's just a character array of the
things we want to turn into spaces it's
pretty simple overall but in the user
interface it's dropped down and you can
only put it on the things that are the
right type and the types really help out
there because we know ahead of time what
the types are and we can't like they
can't apply convert two spaces or space
characters to the date of births for
example if the date of birth for
structured I doesn't work on integers
it's like it would explode right and we
well it's good to be able to limit
what the user can do with that I intend
to release most of this work as an open
source library hopefully before the end
of the year if I can get around to it
it's all in and working and great but
you know of course before you open
source something you want to pretty it
up and make sure that the API is nice
and there are a couple sharp edges right
now github you can find everything I'm
talking about there unless it's not from
bayard rock which will be stuff at the
end so it started on that so the first
sort of problem that we ran into was
that you know using like normal
programming techniques for data science
totally sucks like its I mean the F
sharp script files kind of are okay but
like there's something really nice about
that I Python notebook experience where
you put things in and then you can see
graphs and results mix with code in line
so we started in on trying to do better
at F sharp web intellisense because in
general there was nothing good
everything was bad and we built this
sort of abstract framework for it that
works with a sore code mirror and you
can just integrate it into whatever
project you want and so I have sharp
notebook uses this and I will be
demonstrating that first off
let me just make this little smaller
term isn't that big so that could can
you guys read that my back yes maybe I
see some shaking heads yes so I'm going
to go for all right so you can do some
basic stuff with this I'm going to go
through this really fast because it's
not really that technically impressive
in this form it's just to kind of show
off what you can do so we can go into
stick images in line that's our brand
new F sharp logo it's very nice the
community fought over it for months and
months we can do lytham lay tech in line
and we can automatically display the
results of sequences these are these
sequences are lazy evaluated so it only
shows you the first four it's not going
to evaluate the entire thing because it
could be infinite and as you can see
here this seat dot the infinite is
actually infinite we can do some nice
little charts very easily with the chart
Combinator's in tables we can take that
same data because it's just a list of
tuples and shove it into a bunch of
different things like a table or a bar
graph or a line chart or whatever this
is just some function definitions then
we can apply that and look it's very
pretty cosines and we also this didn't
work last time but I'm a label to try
now it's souls I think when it's zoomed
in it doesn't work properly but this is
actually an animated gif down here it's
just kind of making my scroll bar go
nuts we'd like to be able to do more
animated stuff inside of this and that's
one of the things that we're focusing on
we're not quite there yet as you can see
language that I wrote essentially the
problem was is on.net there wasn't any
good options for like a user predicates
language essentially like you have a
bunch of Records and you want to filter
over them or make lift machine learning
features yes ya ass kiss or no questions
about these things and I tried all these
but they're like so overkill except for
a dynamic link which is kind of close
but still it's you cannot ask an end
user analyst to know that are seen an
integer and a float it is not an
acceptable option and that's what the
problem is dynamic link was and it's
also just like a beta project Microsoft
that was on a blog once and never talked
about again so not really so great but
it's actually kind of a hard problem
right like it needs to be simple enough
that people that are not regular program
is like bash script level programmers
can handle it to ask these questions and
express their ideas about what they want
to know about the data it needs to like
not be able to touch anything that's
also really important that it can't
actually break the program because when
the amateurs are fiddling around if they
can mutate things that could be
disastrous you know they could just make
a ruined data in memory write it out
over right things and not even know and
that'd be my fault so now I just know it
and also like terseness is extremely
important as well so I named it bar back
my pet bird also it's sharp and layout
c-sharp I guess yeah anyway so i'll show
you some examples of barbed action
mention is that we get intellisense with
pound are so we can go
this is a brand new feature so I'm there
we go is there my current directory
stuff well we can plug is on right like
that's pretty neat it's it's really
useful when you have a bunch of
assemblies and you're like which one do
I want anyway that's not really about
this demo so we're just going to get
that assembly in and then open some
namespaces this is just a simple record
type so location has a city which is a
string estate which is straying as if
which is a string of the person and I
have one or more name or zero or more
names which is just an array an age
which represents like a by an integer
option which means they may or may not
have an age and then some set of
locations and so I put me in my coworker
here and I've got a couple of aliases I
got one Elias I guess my nickname and my
age this is actually I made this last
year I should update this but i'm not
going to do right now you know i live in
hoboken new jersey but I work in the
Empire State Building essentially and
that's where those come from and this is
similar for my coworker mark shipper who
actually invented that page rank thing
he's quite a smart guy so let's load
that data in so we can play with it a
very simple barb so this part is the bar
query just the string in here and then
this is a function from person to
bullying and that's what we're saying
we're saying build an expression which
is a barb query from person to boolean
I'm so we taking the absolute value of
the age- 30 and seeing them is greater
than five right very simple thing and
it's mark very surprising I know it's so
this is more impressive actually so it's
got sort of set Combinator's or set
operators in it this is the has
intersection operator there's a couple
of neat things here I'm actually so this
is the has intersection and then you
might notice that there's two dots here
that's because there's multiple
locations in the structure and you want
to look at all the states so what this
says is like get all the states
essentially you're mapping over with
this sort of property and saying I want
a collection of the states and we're
saying are any of those New York
I'd rather like this dot dot syntax
because it really is good for reducing
down those collections of things and if
you have a collections of collections of
collections you can put dot dot dot so
it's a you can put as many dots is
necessary to get your flat data
structure it was kind of inspired by
matlab even though i hate matlab it's
got a little bit going for it I guess
finally we have a more complicated
structure here with an and and a knot
and it does do short circuiting so it
won't like evaluate parts that are not
necessary and here's a just a quick
example with multiple dot dots like we
can say we we want to call to upper and
variant on all the cities so we put a
dot dot here because there's multiple
cities if you want to call a method on
one thing you just use one dot if you
want to call on a collection of things
you use two dots and we're just seeing
if that compares to New York and so barb
does a bunch of other little things but
this is essentially what it was designed
for and I think it does a pretty good
job of it the hardest thing in writing
is language was making it so the error
messages weren't terrible and that
actually took months of work it's really
surprising with putting language stuff
what is hard and what isn't and because
we're the mighty mighty is a MIT project
so for a long time the state of art and
information extraction was GPL which is
not so good for business but might like
MIT recently came up with mighty which
is a nice MIT license library that does
really nice information extraction it's
quite good at it and it uh so I just
wrapped it up and for use in f-sharp and
I'll show you that quickly
so we got our mighty net and we're just
going to free this is just a little
pretty printer we're saying we have out
some tokens with a certain tag and an
offset for our pretty results and we're
showing them how to write those because
otherwise the results look kind of gross
next we need to pick a model file
there's a bunch of different models
available this is not a full rapper from
I yet because in the latest version 0.2
which was released about two weeks ago I
think they added a whole bunch of sort
of pairwise information model stuff and
none of that is wrapped yet this is
still just the base labeler but it would
be pretty easy to extend it that way and
we probably will it's just a bunch of P
invokes and then some classes on top of
it to make it nice to use we're just
going to make a new mighty engine with a
model file and have a nice little
display because it takes a little while
look up like a 10 seconds or so probably
to load all those fast nice okay so now
we have a sample um the first two
samples are actually the baked in
samples and these are good to show that
it actually does well enough on its own
this little bug in our our code coloring
because this triple quote means until
you see a triple coat everything is in
the string and I need to report that
actually but in any case we're going to
just try on that sample one and we'll
look at what the results are for the
information extraction
so it did pretty well pegasus airlines
is an organization istanbul is a
location sochi russia turkey all
locations Transportation Ministry is
also an organization in general the new
version it wasn't nearly as good before
at 0.2 and it's been really impressive
to me what they've been able to achieve
with this new release also it was rather
unstable before that was the other big
thing like it would crash quite often so
here we have another included in the box
example which is still not that
impressive because it is obviously tuned
force to some degree as you can see they
got pretty much everything right I'm not
going to spend too much time on that
because I don't have that much time and
this is something I took from Wikipedia
and in the last version it did it kind
of terrible job here it does a much
better job actually there's only one
thing that's questionable and that's
that the OS februari is arguably not a
location but still you know you're never
going to get perfect and yeah it's
definitely a date not a location but if
you trained it more I'm sure you could
train it to understand better these
these sorts of bracketed notes and such
for now it does pretty darn good job on
its own highly recommended for anyone
who wants to do the sort of information
extraction without worrying about GPL s
by us except for the matlab type
provider we did make the matlab type
provider but i'm not really going to go
that much in detail with that so let's
talk about thai provider is because
they're pretty interesting essentially a
pipe provider is a compiler plug-in that
goes out to the internet compile time or
to cache data depending on how you can
figure it and pull generates types now
those types can be real types like
dotnet true-blue types that you can
interrogate with reflection and whatever
else or they may erase types which can
be some efficient representation after
you're done interacting with them
we can be whatever you want and
essentially it's just like doing reverse
reflection they're just emitting out the
things you like it has this property in
blah blah blah and it's pretty easy
considering how neat it is and how
powerful it is so this is just a quick
example of something you might do you
can just use our odata service type
provider point out netflix and then when
you go netflix dot you see titles and
like it'll show up with the intellisense
i'll do a demo of this in a minute but
not with netflix and this is the query
syntax which essentially is very much
like link and c-sharp pretty much the
same thing this actually gets converted
into an AST and then depending on what
the service can handle will push as much
to the service as it can so deedle is
the next demo and this was made by other
people in new york city besides myself
but I'm applaud them for it at Blue
Mountain capital it's essentially one of
the people who worked on pandas actually
helped out with this there and it's a
data.frame library for F sharp and we're
also going to show it with some type of
ITER action so this is something I
didn't mention me for if you want to use
nuget packages and I F sharp notebook
you just do pound r and then the name of
the package and it'll get the latest one
you can actually include like the
versioning information if you care and
such but it's actually super handy when
you're in a notebook and you know you're
in your constrained environment where
you can access the internet which is
where our bank data lives and you we
have some custom new git repository and
we just want to go get these packages
and not worry about it it's just pound n
write like not a big deal so we're going
to do here is rating f-sharp data which
is where a lot of the best thai
providers for F sharp live all the data
ones essentially deal which is the data
frame library and then we're gonna make
a new world bank data type provider
instance now the world big thang pie
provider is unique among many in that
you don't have to parameterize at all
because there's only one world bank's
site that it can access and thankfully
have internet access so this will work
nicely
so what this does is it goes out to the
world bank and it gets the data
essentially to represent the schema and
as necessary it'll get more so we've
gotten all our nougat packages
everything is loaded looking good we
don't actually have to pound our to load
them because town den implies a pound r
which would be like the you know pull it
into the repple or whatever so we can
see from this there we go you can look
at the top level of the World Bank we
can see countries we can see all these
different countries here this is not in
a file on the computer this is being
pulled off the internet on the fly lots
of information here it's quite useful
but we just want all the countries so
we're just going to go countries then
we're going to make a function that
essentially gets the population for a
given year we're sticking in this series
so that we can stick them all together
and data frame make them easy to
manipulate later we're going to get to
the population in 2002 in the population
in 2012 anything nor than that isn't so
great yet because it takes a while and
then we're going to compare that
information so essentially this little
arrow means it's like a little dsl for
the data frame that says like this is
the column name and this is the content
so we're saying population for the year
two thousand two it's pretty simple now
we're going to take a diff of the tube
and we're going to sort and take the
last five so we're looking at who have
the most population loss over time of
all the countries in the world
and we can see here Ukraine is kind of
winning winning I guess what pressures
not doing so hot either in terms of
population loss Germany is actually kind
of surprising but I guess people are
moving out because with taxes are high
or something I don't know I really have
no it's like reason to know what why but
this just kind of shows how easy it is
because essentially with that data frame
we were able to just go oh we want to
take these dips which essentially is
this thing right here and we're going to
just turn them into series of
observations and we're going to put that
in a chart that's a column chart with
size and automatically pulls the the
names of the the fields out so you know
all that was not specified we've just
gotten directly from this which is
pretty handy
so just similarly we have the most
population rope I'm sure most people can
guess these countries oh wait what oh
sorry pop back up does that sometimes in
a weird way I think that's just it's
only wondering demos I think it's demo
demons or something yeah India of course
and China Nigeria Indonesia Pakistan
lots of population growth but India is
definitely winning that game so we can
take those and we can look at a couple
of different instances like we can say
okay we're gonna this is listen tax in
f-sharp and we're saying oh we want this
is the World Bank tie provider for the
country's you want India in particular
and we want Ukraine and want United
States the reason for these double back
tix is that in f-sharp you can have
identifiers and spaces and other
characters that are not normally allowed
and identifiers as long as they're in
double back ticks so when ty providers
in order to preserve all the information
in turn sort instead of turning them
into underscores or other stuff we just
put them in double back ticks and then
you have the actual whatever was in the
database or data set without really
worrying about it then we're gonna get
the pub the population total over time
and we're going to combine all these
charts so essentially we're mapping over
that list of countries or making a line
chart for each one where you know the
number is the population the name is
just the name of the country and we can
see here you know we get intellisense
here so we can see I don't want to be
the capital city instead or something we
just want to name them and then we're
gonna go buying all those charts with a
legend you know population over time we
want to put the left so it doesn't block
our view and we're going to show it
we can see your India is going up United
States is going up with leveling off and
then I probably should've put the UK for
this demo I need to think of that but
you know we can do that right now
actually so let's just do that we'll do
that right now it'll be really easy so
let's go countries united kingdom okay
got that it's taking a minute to settle
down with the intellisense air but don't
worry about it it's fine it has to do
with the fact that it has to talk to a
server to communicate about all the
errors and such we'd like to actually
one of the projects that we'd like to do
is actually move that for compiler
itself into JavaScript so that we can
kind of do this stuff on the fly oh we
can already compile if trouble
JavaScript an f-sharp is written in
f-sharp so why not right but yeah the UK
is kind of just pretty flat similar with
Ukraine a little bit better doing a
little bit more population growth there
I think it is actually in trending
positive it's interesting right but you
can just toss things in and out play
with data with this and it's really easy
the combination of detail and type
providers is extremely powerful the only
downside to it is for very very large
data sets like I have a machine at work
with 256 gigabytes of RAM we just bought
a new one with a terabyte of ram and you
know just what the old one has 32 pores
and they're hyper threaded cores and
it's got a lot of processing power you
know raid 5 SSDs and whatever else this
is kind of a waste because it does do
it's not the most efficient
representation so like we're using a lot
of bank data for analysis a lot of times
we'll use this for on a sample and I'll
play with it to see some information but
it's not the ideal solution in that case
but it's really nice for certain kinds
of data
so the last thing I'm going to talk
about ranking versus regression which is
came from a paper from google and I
think that this is a really interesting
idea I mean at first I didn't really
understand the relationship or the fact
that you could use regression for
ranking and the fact that they could
actually be mixed is a very interesting
idea because when it comes to well I'll
show you it's just hard sometimes to
pick because I'm you care about the
ordering but you also care about the
magnitude and regression and do a very
bad job at ranking because they might be
slightly out of order a whole lot and
that would just be a much worse ranking
or but like the the ranking itself might
be a very poor regression because it
might not the line you like very well
regression has its problems of course
everyone knows that is in this sort of
domain we're just gonna ignore that
because we're assuming that we know what
we're doing in this case okay so sharp
is an excellent language for algorithms
I am like super happy because of what
we're able to do with this I also like
that we can use our Greek letters right
in there but um so we're just going to
do like a standard ordinary
least-squares regression we just make a
single step as a function oops not yet
okay so we have some matrix of features
and matrix of labels and then some theta
which is the waves on those features in
order to get those labels which is
essentially this formula right here
where that why this beta is actually the
theta okay once in the lap of mood k an
alpha is like a learning parameter that
Tunes how fast you learn what we do here
is you're making an infinite sequence of
Thetas so that we can just pull some off
and essentially this function just has a
recursive function inside that keeps
calling itself with the previous theta
generating a new theta pulling that next
one out and calling itself again it's
very simple this is called the sequence
expression in this yield bang means it's
going to return many different Thetas
because this function returns many
different Thetas meanwhile we just want
to yield out
single theta so that's why they're just
regular yield it's sort of similar to
that dot dot dot thing we're talking 90
for any case this will just give you an
infinite sequence of Thetas that
hopefully asymptotically approach the
best possible regression and then we
just pass in using math net which
actually supports MKL so it's blazingly
fast linear algebra it's all free
download it's very nice we just passed
into Thetas or x and y and then we have
our dense vector which is zero which is
the column count for the for the theta
which of course starts out of 0 and then
we just learn it it's quite simple but
the cool thing that I thought was wow
you can use regression for ranking and
all you have to do is subtract things I
actually never it's never occurred to me
and I like studied regressions and
rankings quite a lot and until I read
this paper I had no clue that this was
actually possible but it totally makes
sense because he was looking at the
differences in things and it's learning
how to place them near each other in the
correct order because you're penalizing
on the order of the features themselves
and the weights will sort of figure it
out automatically so yeah this is very
simple table you know if the names are
the same result 0 addresses are not the
same but sample one it is and it's one
sample two is the other negative 1 and
if they're both 0 you're all sort of 0
it's quite simple so in order to do this
we're just going to use the same
ordinary least-squares function but
we're just going to generate a bunch of
samples first right it's quite simple so
we make a new system random which is
like the dotnet way to make random
numbers that's not very pleasant but it
works and then we're just going to
generate random indices and then we're
going to get to random indices subtract
the different X's essentially this is
linear algebra except we're doing the
rows for those particular exes so just
like vector subtraction
and then the wise which are just scalars
and then we're yielding out those values
and this syntax is like the sequence
syntax but it's a raise instead what
would get at the end is an array so
we're going to make a thousand samples
here then we're going to split them off
because we want the X samples and the
Y's samples separately so we're just
going to map over the first side turn it
into a list and then turn that into a
dense matrix of row vectors because
that's essentially what it is and then
on the y side this comma means this will
be this and everything down here will be
this and we're just doing it we're
mapping over the second half essentially
just like pulling off all the second
things because it's an array it's pretty
much free like it'd still be fast and
we're just gonna shove it an inspector
of wise this is the paper I was talking
about the impressive thing with that I
was not able to reproduce this results
with at least the datasets that I work
with I found that the combined ranking
and regression did a lot worse at
regression but the idea is still cool
that you can use the rankings I actually
got were not bad when I did it like a
pure ranking mode but there's something
fishy about the combination of the two I
still like was blown away by the idea
that you can do this essentially take
the thing I was so familiar with and I
was doing a lot of investigations into
ways to rank these datasets essentially
what i want to be able to do is ranked
customer data and i want the most simple
possible thing so i can explain how it
works to bankers like I could use very
sophisticated methods but we have
trouble with our page rank stuff
explaining that the bankers they they
want to know exactly how we get our
numbers and the regulator's want to know
how we get our numbers and sometimes
that can be rather difficult for very
complex models so it's good to have the
simple model start with and if that is
not sufficient keep moving and this is
pretty much the simplest ranking model
you could possibly think of pink
subtraction it's extremely simple where
it's yielding out all those values
making it a rain we do the same exact
things and that in that way I get got
the kind of poor results but it was
still interesting so I guess that's
about all i have are there any questions
about any any subsections of this
no I could show off a little bit more
bar I think that would be cool recently
the barb we've added a couple of new
things like arrays and you can do
essentially you can do unions and
intersections and has intersections so
let's see
so if you want to do just an
intersection which is actually commonly
used because we would have something
where we have a customer and a provider
which is a list record and we want to
say like are they both in New York City
or are they both in a large city this is
like a common thing because large cities
tend to be have a lot of false positives
in them right because there's a lot of
people in the city like if you're in
like some small town and your name is
likely be much more unique just in
general not always does you know John
Smith's and whatever or families that
make up those small towns but it can be
very different so we could do something
like if we had two names you would
pretty much do locations city to upper
invariant and we'll say like customer
just one dot
and then we want to find the
intersection with that with the provider
it's actually doing the intellisense and
that's not a good thing because this is
not Sam but I don't actually make the I
sharp notebook thing it's one of my
co-workers LT LM
so you would do something like this and
if you want to be it looks going to go
left to right so it'll just make take
the union of we want the intersection of
these things and then we'd say we want
the to make sure it's New York
essentially and that's quite simple
right like you get a lot of information
packed into a small space and you would
use this in the context of safe alert
manager actually i'll show you that real
quick since we have some extra time oh
really i don't i didn't see the hand
signals i guess my timing was good i
thought it was way under i was like oh
that's true and i want to also say real
quick that RF sharp user group we have
tons of videos online and even some
about deedle and such so if you go to
our meetup page each of our meetups if
you look in the comments will have a
link to the corresponding video and
they're all in my vimeo account and
their high quality usually 1080p so
check those out if you're interested and
also f-sharp the org has a whole data
science section so if you want to know
about these tools in the vast majority
of other tools they have auto dip we
have a lot of the same things as Python
go to F sharp that org or follow me on
Twitter and listen to me talk about if
sharp all the time
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>