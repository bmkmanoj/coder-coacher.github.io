<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>London Erlang User Group meeting -  Choose Your Own Consistency - Christopher Meiklejohn | Coder Coacher - Coaching Coders</title><meta content="London Erlang User Group meeting -  Choose Your Own Consistency - Christopher Meiklejohn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>London Erlang User Group meeting -  Choose Your Own Consistency - Christopher Meiklejohn</b></h2><h5 class="post__date">2014-05-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/a0tNH3rBi94" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I guess we'll get started trying to
figure out the best place to stand where
I can like pointing things about messing
it up okay hi everybody I'm Chris Fickel
John my engineering Bachelor of
technologies obviously working on react
I work on the multi data center
replication right now but I move around
a bit i do a bunch of work on CR DTS and
things like that so i'm primarily based
in the States and I was in Europe
traveling for a conference related to
some of the stuff that we're going to
talk about in this talk day so um so
this talk is primarily about the strong
consistency functionality and crdt
functionality which are kind of two of
the in addition to yoga center are two
of that kind of like flagship features
of react to Davao which is in beta
release right now beta 1 so you can play
with all the things that we're going to
talk about in this in this deck today
and there were a couple example repos
that show you how to play around with
this stuff and I'm happy to talk more
about it especially the crdt stuff which
is which I'm very fond of cool uh all
right so what we'll do is we'll do a
brief overview of react in case you
don't know how many of the mechanisms
inside react work if you do this will be
a very brief review so getting started
so again you've probably heard this 100
* summary Alcazar erling implementation
of dynamo this is
dynamo dB this is the original demo
which is outlined in an academic paper
that Amazon published in SOS be in 2007
and for some reason the slide is
duplicated and what that people
basically talked about was how to build
a database system where you basically
sacrifice consistency strong consistency
here linea lies ability for a system
that has higher availability so when you
don't need to coordinate operations
across a large number of notes you can
basically guarantee higher availability
but you have to relax some of these
consistency guarantees so the core of
how reacts stores data in the back end
is this thing called a rant object so if
you look in the Erlang code you'll see
our object or react object coming or 42
there's a few different formats that
we've gone through and what it kind of
is is it stores a key which is a bucket
in key which is kind of the key but the
bucket is just a namespace applied to
the key and that has this value and what
this value is going to be is completely
opaque to the system except in certain
cases of CR dt's which we won't talk
about because it's for level 2 details
but this value is going to be a big can
be binary we have customers from store
audio data we have customers who use
semi-structured data such as json or xml
it can be whatever you want and when you
go to write these objects you specify a
content type and you know in the case of
a binary binary data you would use like
kind of lime standard content types
using a complication octet stream or in
the case of JSON you take application
JSON or application XML things like this
so brief review of the consistent
mechanism so how do we distribute data
across of these notes you have a react
cluster the cluster is going to be made
up of four to five nodes and we need a
mechanism to determine this elite
deterministically route data to knows
that minimizes data moving around the
cluster to give you better performance
so what we do is we take this integer
space its to the 160th so you imagine we
start mapping at this the top of the
ring the zero location and we map all
around it to the 160th and then what we
do is we have some number that the power
of two that we subdivide that integer
space into and each of these is called a
partition and then what we do is we have
a series of notes and we distribute
these partitions as evenly as possible
across all those notes so if you've seen
that we commonly recommended five nodes
and stead of three weeks recommend three
notes we commonly recommend five notes
as a smallest react cluster size the
reason for that is because with five
with a power go to ring size it's the
oath to partition size it's the only way
to guarantee that neighboring replicas
you see the of these three colors the
neighboring partitions are not on the
same physical note so it's a property of
the algorithm that's used to distribute
those partitions out which is kind of a
two-phase commit gossip gossip based
protocol so dynamic membership which
we'll talk about here but i'll leave the
graph up to demonstrate so dynamic
membership is the ability to take one of
these nodes so let's say the blue note
and remover from the cluster of all the
systems right and redistribute that data
across the ring similarly you can add
notes and redistribute it again and we
want to keep the system available while
we make all of those changes and finally
with the replication factor we have a
series of replicas and we want to place
those on positions of the rain so that
replicas of the same data ion are stored
on different notes and what your reason
about where those replicas are located
by saying this pink one here we're just
going to walk the ring clockwise to find
within next two locations
replicas are so this gives us a
deterministic way to say given any piece
of data we can find its location on the
ring find the neighboring replicas and
then return those back so that's kind of
the react overview probably very
familiar if you've ever worked with the
system so we're going to kind of talk
about what so we're making these
trade-offs between strong consistency
intellectual consistency and it you know
we have these two definitions that will
use to kind of frame the problem here so
we have high availability and high
availability is commonly been defined by
Gilbert alleges that not every non
failing node in the cluster can
potentially respond to a response
similarly if we look at eventually
consistency eventual consistency is from
Wikipedia but water Vogel's that said
something very similar to this I think
it's a paraphrase from what he
originally said is that mental
consistency as a model used in
distributed computing that informally
guarantees if no updates are made of the
system eventually all accesses that data
item will return the most updated value
so I window just think about this you
know like in layman's terms is that you
know if i go to write a data i have and
i write data item X and then I check am
I right at values 1 and then I write it
at value to I'm AC value one for a while
value to might take a little bit of time
to appear that's essentially eventual
consistency that's a lioness property
sale eventually we'll see the right
value but we're never going to return a
wrong value which is kind of a safety
property so that these are the kind of
two trade-offs that we make eventual
consistency enables high availability if
we say that items can be still for a
while that means we don't have to have
everybody agree on something at the
exact same moment in time and by making
that trade up we get higher availability
because everybody can respond to a
request uh-huh so the two things so the
two main modes that react operates in
that are kind of cord and the strong
consistency eventual consistency in CRT
tease our last writer wins and allowable
so we're going to talk about what these
means so I'm going to use this tuple
structure which would be familiar for
all of your earlier programmers you'd
like to explain this a little bit more
incentive
like groups this is super nice so we're
gonna say we have a writer we're gonna
have a value and I'm gonna have some
time and we're just going to use like
some simple to deal with what this means
so in the case of card fighters that are
writing they each have to write to the
same replica set so they throw right to
this replica of three nodes of using the
default react configuration so we're
going to say we have two rights so we
have a right that's done by a so if you
remember we have writer value time so we
have a as the writer values v1 which
means that this is the first value go
right into the database and I'm not
saying what that value is I'm just
saying it's its value one written by a
and I say that right occurs at time to
time T 1 so this should be t100 ignore
that some Tennessee one so what happens
is we have concurrent rights and they
basically race for writing so you can
end up with a situation where you have a
be an a on the series of replicas and
you have the all value one its A's first
right and its B's first right and they
all occurred at time one so the two
strategies we have class ruins and allow
them all panelists differently so with
the last writer wings these are the
values that remember this is supposed to
be a t1 in last turner ways these are
the values and arraigned in the database
but these are the values that result in
so after all the rege repair is done and
we're going to read the values from the
database you get be across across
everything so is it is t2 supposed to be
greater than t1 is at once yes that's
correct so maybe teasers correctly
moment ah yes your gratitude grisons yes
I realize his life like 30 minutes ago
so I screwed up but yes your I didn't
screw up I did it correctly right so b
is going to win because time 2 is
greater even though they're writing the
first value of the object B wins because
it's logical it's physical clock an
actual timestamp wall clock time is
greater so that means however in the
allow multi strategy where we take both
values when we return them out to the
user and the user
to reconcile these values we end up
storing both of them so we end up with a
v1 and t1 and the VAP one and teach so
we'll store both of these values and you
retreat them so last for your voice is
obviously bad because you're Luke you
lose updates you have updates that get
Britain you can never read that lay
immediately get overwritten or results
or be prepared and a lot of month is bad
because programmers don't like having an
API we have to deal with getting
multiple objects back when you perform a
bright it's kind of counterintuitive to
say oh hey I have this concurrent right
that added something I had a concurrent
right on this under note that added
something and then one of them wins that
I lose the other updates that's not good
but if I keep both of them how do I know
how to reconcile these values and what
we've seen if you watch any of the talks
that most of our conferences with
customers who have implemented this is
that stand upright emerge functions all
of these users and a bride a
user-specified merge function they don't
want to lose right so they have to run a
little a vault and then when they read
values they have to have mechanisms for
reasoning about which update was right
for how to merge that state so examples
of this are you know Yammer has had to
do things similar a bump had to write at
front row to proxy servers in Haskell
that although reads went through that
new had a deterministically resolve
every possible emergence correctly in
addition to that companies like Clara do
something similar where they model their
data in a mechanism where every update
commute so they don't have to worry
about this trying to think there's a
bunch of other so there's numerous other
cases that we can talk about but those
are the three that I remember offhand so
so what's the problem with
user-specified merge function so
user-specified merge functions are bad
because they're not always correct
programmers don't always right correct
code all the time in case you haven't
realized this and writing these are
specified works
going to resolve values and write them
back to the database potentially result
in data loss you know and relying on
perma intuition to say I'm going to
write a data structure where I guarantee
that every operation I can do this data
structure commutes all the time is
really hot examples of this operational
transforms google took 15 years to get
it right and then Mark Shapiro and his
group comes along and write the paper
and says we perform Lee proven through
inductive like proofs that is wrong it
doesn't work it's not correct this is a
really really hard thing to do so what
are we trying to do so in the eventual
consign of things we're trying to
provide a mechanism that allows us to
not have to write these march functions
as a permitir you want to work with an
API that's straightforward that is like
a counter it's like a set it's like a
map but you want these convergence
properties to say that all of the
updates can happen anywhere and I ended
with a right state because that's a nice
programming a model for a user to have
from you allow two values to be delayed
but you guarantee that no updates are
missed and you eventually end up with
the correct value so that's where crdt
is coming to it so this is the first
part of the talk that talks about of
using eventual consistency providing a
better data model for working with your
data she don't have to worry about
getting multiple values back and
reasoning about metal parts of those
values so what are serious ease so uh
excuse me sir dt is coming two flavors
they come in this convergent flavor so
there's convergent replicated data types
which are cv rtts and then we have
commutative replicated data types which
are c m RT tease but they're all part of
this general thing called conflict-free
replicated data types and and what these
what these data structures are there
synchronization free data structures
their data structures that model that
model some sort of primitive data
structure in a way that it preserves
state and state always progresses so we
can reason about the ordering of events
ensure that the
I use merge correctly so we got super
Matthew for a minute what does that mean
CEO duties are monotonic their state
always it moves in one direction
presumably increasing is the useful
direction for the stink of event and
their conflict when you have things that
are monotonic and Compline or
convergence so this is what we care
about here is that we want data
structures that are convergent and what
does this mean in terms of the rant
datastore it means that we can run with
allow vault we can create a ton of
siblings and we know how to resolve
these siblings we have a mathematical
foundation for saying I know how to
merge these things and get the right
value so I got two sets that exist
across datacenters a replication between
the datacenters I can have inserts and
updates and deletes happening on both
sides as soon as those values merge you
guys crack up so what's the theory
behind all this stuff so there's a bunch
of papers this is what I do for fun
because i'm also a grad student his
papers like this so area published his
first paper or comprehensive study this
has a large majority of the theory
behind it if you're interested you
should read it this is the optimized
come through replicated data set that
allows us to do things without creating
tombstones which allow us to not create
garbage which allows us to actually use
this stuff in practice so this is a very
important favorite or offered research
and finally done in Persian vectors
expanding from version vectors which one
for vector ploxo react used to use a
primitive vector clock mechanism
virgins we switch to virgin vectors and
well the library provides of the vector
clock library which is equivalent to a
version vector semantically but has some
different operating properties about how
you work with it and finally in react to
dot 0 we have Donna Berger vectors and
what Donna Berger vectors to do is allow
you to reason about another level of
concurrency we're not going to talk
about what that particularly is but if
you've dealt with the sibling explosion
problem disparate this professor so so
the math behind us we'll do a quick
little math rundown for those interested
kind of give you an example why this is
interesting and why this work so down to
join sending Lance's this is a concept
from discrete mathematics this is
basically a partially ordered set that
has a least upper bound function his
least upper bound function has three
important properties associate inity
commutativity and item phones will go
through this quick because I know this
is not very relevant early programming
but it's interesting about how this
works so what is what is what's
interesting about associativity so
associativity if you remember from math
is that if i patch up things this way
again same result so if you think of
this dot being a binary operator and
operator that operates on two values you
remember that addition is associative
you know it doesn't matter what order
that you add values in you get the
correct value for positive not negative
four negative negative integers or
natural numbers commutativity has this
property where I can reorder the two
operations at work so addition adheres
to this property as well distributed
systems we deal with events coming in
different orders at different machines
all the time so this is a very nice
property to have when you think about
the analog to computing associativity is
dealing with batch insensitivity and
finally an impotent we all like this in
our programs at influences the ability
to have operation take a value and then
again and take it again and take it
again and still get the same value so I
potency is very important as well
because sometimes messages get sent more
than once for the same operation so
these three things are really nice and
have a very nice analogue to distributed
systems are concurrency or friends or
whatever you have that you do
concurrency with so if you combine these
things together we have these objects
that accumulates date / time and we have
this bad a joint semi lattice which
computes a least upper bound function
this gives us this monotonicity and
confluence property which gives us
convergence and what we get is the
ability also to map these lapses into
each other so we can compose these
lattices and all sorts of fancy ways as
well so we're going to walk through an
example so this is all matthew playa oh
my god this is really hard or maybe you
know it all if you don't then move this
example that will make it super easy to
understand it so just briefly define
confluence zone yeah confluence is
values moving in the same direction
values basically murdering together so
yeah by merging together our values and
then you have values that are always
increasing or monotonically increasing
its state really and then you get this
convergence property so this amount of
joint semi lattice for set the merge
function for a set is Union so find the
set with be a 4c and then I received
these results so these represent merges
with additional state so if I have a and
C and I merge them I get Nancy is I
queuing in the sense if I have AC and bc
i get a B and C right so I can
continuously merge these things state
always increases and I get this property
where the value is continuously move up
if you imagine these things happening in
any order I end up with the right
results you basically continuously move
forward accumulating state and apply
these much function step straight
forward everybody got that it's pretty
easy you probably do this in programming
all the time here's another one that's
pretty easy increasing natural numbers
so positive non positive
integers non negative integers have
emerged function max that computes this
least upper bound so if i have three and
five and I merchant i get 5 i'm never
going to get a value less than five if
one if i'm always merging with five or
refining operation with five five five
and seven I got seven and five 5s mism
right so max always moves in one
direction you never call the max
function and get a value less than one
of the followers again we could do this
opposite we could say main is a function
on decreasing numbers as well boolean is
another one the merge function for
boolean is or so if I have false and
false and I ordered I get false if I
work with a true i got true and then
once i get true i get true forever
because anything or but true always be
true so we also can map these together
so we have a function that allows that
preserves this monotonicity property so
this is this is the definition of less
kind of monotonicity as the inputs
increase the outputs also increases some
monotone function we can map these
together so we could say so so sighs
onset that can only ever bro size of the
monotone function because the side you
will never get smaller on asset that can
never drink so I could have a function
that computes the size at every one of
these merge points and they can form
another lattice like this so we have
this composition property that's very
nice about it as well so will alter an
example of how this works then we'll
look at what you can do to use these
data types in your application so i will
look at convergent replicated data types
we don't talk about commutative data
types because the commutative data types
rely on all sorts of fancy distributed
system things I'm causal delivery which
we don't have in react or in early
may you have you can make some
guarantees in her life but you can't
make causal delivery guarantees on a
series of processes so we're going to
look at the whole are set so our set is
the set we can insert an item you can
remove an item if you observed its
addition into it and then you can do
this infinitely many times so how does
this look so imagine this line separates
two replicas so these are two relativism
but we pretend they're running on the
same database node without a different
partitions we can pretend they're
running on two different threads which
I'm running on one is in North Virginia
easy to zios in Oregon so what we have
is we have so this is like Earl a nice
in Texas look familiar this is a set of
two sets or rather a list of two lists
and these are two tuple the first item
in the tuple represents a unique
identifier and the second one represents
the element we want to add to the set so
we say unique ID 1 so this is a this
just any unique identifier we say one is
added one marks the addition of a so
this sack rubs represents additions this
empty list here this empty set
represents removals so that gets
replicated across the network so now i
can have concurrent removals of that a I
could say replica a removes it we're
sorry repulsion we'll use a other two
other names for the replicas will say
brother the X on this side and why here
we say they both are concurrently
removing it so they both say I've
observed a added at one in the asset and
I now are observing a being removed at
one in the remove set so if I actually
try to compute that what's the value of
this set right now has no elements in it
because I've observed the addition of
and the removal of a so that's
replicated or can happen currently still
works fine the merch is fine so now on
replica X here I add a back in and I use
a different unique identifier so I say I
observed
I don't have one I've observed a removal
of a at one and now I've observed the
addition of a I too so now when I emerge
this with this I met the correct value
even if these operations have been
completely independently and didn't and
didn't merge until the very very end
when you wanted to get valuing as a
friend buddy and the value here is that
a is in the set because one replica out
of it the other one added it sorry
wonder what I added it replicated it
stayed over they both removed it and
then one guy added it so if I didn't
model this by saying I keep track of
every edition of removal you have an
interesting problem that you have to
deal with right so if I say I don't
model this as removals and I just you
know go back here and I say well to
remove it I just removed from this set
then later when I go to merge it how do
I know if I've observed the removal for
I've never observed the addition or
observe the addition of the removal you
don't know you can't breathe it about if
you've seen something and removed it or
never observed it being there in the
first place so this is why it's
important this is why i said that
monotonicity property is important
because it will accumulate state you get
to track the history of how the events
observe so we actually don't model this
in this way in the erlang library
because this creates a lot of garbage
for values that have been removed so we
actually have a much smarter way to do
it with DVDs but the smarter way is not
as easy to explain in two minutes to
talk from 25 people so we'll do another
quick example a is added in both sets at
time one or unique identifier one we
have concurrent operations happen where
V is added at value to a is removed at
value one and I got the correct value
when I merge those i end up with the
correct value which is that b is in the
second day is no longer the sign for
purchase okay so what do we have that
you can actually use so that is not wind
that'd be a funny joke i was just like
there's no Erlang library you can't use
any of it though there you go so we have
a library called react dt is a Erlang
library
add it as a rebar dependency or whatever
you use for your dependency tracking
that's the repo fully open sourced it's
EQC it's tested very heavily and
implements a bunch of these data
structures so what is reality provided
so this is a this is the interface that
reactivity programs are provides a bunch
of CRT teas that you can build your
applications with each of these has to
implement a series of function so it has
to implement this is a behavior to react
ET behavior that they all implement we
have a new operation that creates a CRT
TV so all of our CRUT is implement that
we have the value function which allows
you to get what the actual values if
this complicated set thing that's
tracking all this history sometimes you
know when you want actually usually you
want to know what values are in the set
C call value this update which takes the
series of operations those operations
are dependent on what type of crdt
you're working with there's a merge
function this this is that function that
has to compute that least upper bound
has to do that merge and get the correct
value we have an equality function which
test the equality between two CR DTS and
then we have a two binary from binary
which are used as to provide the
encoding mechanism that Briac actually
used the sense of this data on the wire
so this will binary encoding uses some
uses like the whenever the compression
is in the binary terms that term the
binary binary returning compression
inside react so it's pretty easy to work
with so this library is used inside of
react it uses the react 14 kind of use
the lab didn't use the library but use
some of the files that are in the
library now to provide counters so I've
had to counter that going to go up and
down that's a pn counter it was built
with the gene counters so if you
actually look in the library you have
the grub only counter and then you have
the positive negative
on the tracks positions in the focus uh
these counters are not item pony there
is a way to have an idempotent counter
we don't currently have an item folk
encounter in there it's really easy to
make one with us all you do is model a
counter as a set and then you have one
that supports it um the counselors have
time complexity a space complexity of
sorry space complexity of old actors the
number of actors because we actually
model account director it's basically
black ski about characters okay no
that's sure if you're using a library
it's whatever you send is the actor
argument in terms of react it's going to
be it's going to be the index partition
which is the V node identifier the veto
an identifier is fixed to your NFL but
then that video could be running on a
fallback so found it on your number of
all bags so it's kind of like the space
complexity work yes basically black
going to be your n val x number of
possible fallback servers which is
essentially the number of actors you
have in appearances so if we look at the
interface for the g counter and the pn
conair there we implement that interface
we just talked about it exports them
type signatures that you can use when
you want to do your dial izing and it's
model is basically an orbiting so we
just kind of sugar up some values but
use the actual data structure that use
inside is a nordic and then it has these
two operations and increments and then
an increment by an integer you can say
increment to incremental one so you can
say increment five to add five of the
counter p.m. counter same thing all we
do is basically compose 2g connors so we
have to grow only counters one that
tracks the positives and learn the
tricks the negatives we take the
difference and so we have these two
operations here so we extend that to
have a decrement which the decorative
ases calls an increment on the
decreasing set so pretty easy you see
well by actor incrementer that stuff is
changing because i use the dvds now too
so what do we have on react to go so and
react to oh we have a lot more than just
counters discounters oh damn interesting
anna requires bucket type so bucket
types and react are a way to set bucket
properties so like replication factors
and things like that but ensure that
they are the same across all nodes in
the cluster so it actually uses like a
two-phase kind of commit academic
protocol to ensure that all the nodes
agree on what a particular Keys
namespace should be stored as and make
sure that all the notes green before it
allows you to use it so types
you'll probably hear more balance
required for your kizuna is required for
yes question the plum tree pretty sure
we can talk more about that after
there's a there's an actual video on the
whole talk about how its implemented I'm
pretty sure that someone has used
there's a paper on it as well Jordan
let's get to talking a movement at all
so reactive Oh has a cluster of metadata
mechanism which allows us to store
metadata and ensure that it's replicated
across the cluster that supports MKT
function out and it uses this this
gossip protocol that we this epidemic
broadcast protocol mmm it supported by
both of our api's so there's HTTP API
and there's a protocol buffers API as
well so what are the caveats MapReduce
into I don't work on these on so to I
doesn't work on the crd T's MapReduce
doesn't as well the JavaScript MapReduce
definitely doesn't in early you can do
it because you can just call the
functions to get the values but through
the normal JavaScript like HTTP API you
can't unless you have a custom function
that you upload and the function camp
javascript unless you write a parser for
our binary format that we use for the
CRT teas which you can do but if you're
going to go to that effort you should
just do it in our life which I shouldn't
have sex you're all earlier brokers and
you want to do it in her life yeah so
what do we have so we have sex these
sets of support additions and removals
so this is the observed remove set
without tombstone so it's the same thing
I just showed you in a much more
efficient manner what it doesn't create
garbage because distributed garbage
collection is hard these sets are hiding
phone so you can obviously a set is
idempotent you only can ever add an
element to it once and that's a lot of
save on set ah the sets have a bias so
for a lot of these operations to work
when concurrent operations with the same
actor or heard of the data structure we
need to apply a bias to
deterministically kind of merge this
without there being uncertainty so some
biases are done in time based on time
and some other implementations of CR DTS
ours just hasn't had one bias so if you
have an addition of in karan addition
and removal of a set the app or an item
in the set the ad will win concurrent
removals with the same actor so space
complexity Oh of actors plus elements so
it stores basically a vector of elements
that are in it and then stores Avenger
of dots from the dotted version vector
to track what updates we've seen him
from what actors in the system so it's
modeled differently than the one that
you saw before here's the API for the g
set so this is the grow only set that
you only can add to it only has one
operation had exports the same API so
you can use this API on your application
this is the 0 are set so this is the app
remove that creates garbage so you have
this API that allows you to add
remove from a set your ads you send an
actor the removals find me last observed
addition use that as the removable actor
and finally this is the ore swamp the
observer moveset without any stones the
or spot as the same API that has better
space complexity so what's the real
exciting thing that we have or max so
Matt would be like a dictionary we could
store something like a JSON object maps
are very exciting this is something that
nobody else has is doing sir duties we
actually just wrote a paper about it
that's why we hear hear so this is
really fantastic these maps are
recursive so it allows composition of
crdt so i can have a map and then inside
of that map i can have a set which is a
whole are set without tombstones or i
can have a counter and it will
recursively update all the values and
ensure that emerges correctly
essentially yeah it's nessa balls an
associative array if that is a better
description of how you think about Max
or dictionaries max have an update have
an update wins edition bias so in the
event so this is a yeah this is
interesting because we have an add
remove API that we want to get rid of
the add one specifically used to have to
say add this item add this as like a
counter so I'd have a map and you say
add key X and the value is going to be a
counter and then you would update it we
have an API that just allows you to say
updated immediately and it will add the
element if it's not there so we're
originally the update operation moving
over the add or remove but now is we're
kind of narrowing that down to just the
update API so the update will win and it
has the set semantics so that up and
on an update wings and there's some
interesting things here because a map
can see how to get there some
interesting things you have to think
about so the mapping codes are tight
because what happens if I say me and
somebody else with me and Brian we can
currently operate on this map I say add
the score as a counter he says add the
score as a set but what happens now so
there is a way to resolve that you can
build a lattice of lattices that allow
us to say well I can take a counter and
I can map that into a set or i can take
asset map that back into a pattern
that's really complicated people I want
to think about that so we encode the
type in there so you say when I want to
get the score you saying to counter and
you say it's counter when you work with
it and it just works so again this is
modeled using a set as the underlying
data structure so it has exactly the
same space complexity actors by element
so this is what the NBI looks like it
allows you to specify update you get to
spay this mat field update which I
believe I left out yet or this mat field
operation which allows you to compose
all these data types so we say you can
add keys the keys are just normal names
are atoms or strings of using one of the
api's that prefers you know HTTP degree
strings and you can say make the values
a counter to register or register is one
that never changes we'll talk about that
next boolean another set another matter
you can recursively nests these things
as deep as you want could I think that
to mean it's going both of the buddies
and perpetuate it and you just choose
which one you want if there's two types
you choose the type that you want it
then we'll for both of them it will
store both of them that's correct yeah
because I basically uses that and it
kind of hashes those values and uses the
key so it does store books back yet so
um so the maps support composition of
the register the boolean the center of
the map so we talked about the map
already we talked about said already the
boolean is just kind of going to be a
value that
is true and the Burges and or so will
merge to Chitina will start at false and
worked up to true oh you have one that
starts at true and moves to false which
just applies a not operation to the
semantics of the one that goes from
false the truth and finally the register
so the register the lww register is a
crdt that is a last writer wings
registered so this is the memory of the
beginning we said last hour winds is a
semantic in the database that is
biological iPad wall clock time take the
last value observes this is a register
that's going to say by wall clock time
tastes the last value search so we will
store a bunch of values and the merge
function will just automatically make
the last one so this is a very special
form of a lattices or some very
interesting lattice forms one is this
last right wins where it says the merge
function is deterministic based on this
time that's passed in the other one is a
one that can never progress so it says
once i get the value that's immediately
the top value i ignore other values
which is an immutable one we don't
provide that mechanism but it's a very
useful lattice property it's a lattice
type reaches its top value immediately
never precedes so the last Raider wins
Register has the same API you say a sign
and then you specify what you want to
assign it and that can be any binary
data store whatever the last ones
Julian's so boolean's they can be unable
to disable they move in the opposite
direction and this is time complexity of
factors such as modeled as a set all the
people who do updates and this is what
your API looks like those same API so
every room has the same API uses the
behavior if you want to use this in your
early programs you can a couple examples
I have a project that provides an
alternative to Jeep Rockets are familiar
G profits of global process registry it
has problems where it doesn't have high
availability I version
built on crdt so that's an example of
another application that's not react
using this because we provide it as a
general or like luxury another one is
you know Heinz guys who's another line
factory regular in addition to me he has
a project called Project fifo which
provides configuration management for
why I'm ever going to name start of us
smart OS instances he uses your duties
as well for storing state ensuring that
concurrent updates that things always
end up of great value and there's a
bunch of other cases of people using
security teams and specific specifically
our lang library so finally this along
this research that we've been doing here
is partially funded by the same freaking
sore sham which is a European Union
funded research project in the seventh
framework programme me Russell Brown and
sean Crips are primarily under the
contributors on The Bachelor side of
things but this is a partnership with
rovio and try park as the industry
partners in the academic project
partners being a public universities in
portugal in rien terrace kaiserslautern
and germany and kashi diversity correct
so that's how we can provide higher
availability through eventual
consistency um there are alternatives
though sometimes you need to read the
values you write immediately so this is
this is the second half of the top the
strong consistency half of the talk so
so why do we want strong consistency so
strong consistency is important because
it provides three it provides a solution
for three kind of problems I was in a
meeting yesterday and two of the
problems that I'm going
about here were mentioned so first one
is at Missa t sometimes you want to do a
right and ensure that every replica has
that value and it gets that value
immediately and you know it has that
value at that particular time this is an
important property that databases that
have strict consistency or
serializability or linearize ability
have it's hard to reason about writing a
value and not seeing the update
immediately I'm getting an earlier value
crdt is allowing a guarantee that you'll
eventually get the right value and that
all the operations will eventually
converge but it doesn't place a bound on
when you're going to see that value
which is the second one which is recency
some applications have a guarantee what
you need to write a value and you need
to be able to read that value
immediately and you need to be able to
read that value in failure conditions up
to a particular bounds of number of
failures and finally partial writes this
partial rights are problematic and if
you're not familiar with marked
impartial right phenomenon and react you
have this situation where you say store
a copy of my object on a series of
replicas two of them say yes I did it
and the third one doesn't respond
because of a TCP timeout well then you
don't know if that value is actually
ringing then you're going to read it
next time or if the update failed
because you don't know like how far it
got so partial brights are a problem and
they're difficult to reason about so you
imagine this an example we talked about
before we have three replicas there on
three different servers we go to write
this B value to the three replicas we do
this and then we end up with this so
this is a possible scenario where the
right as the user sees it was determined
as a failure because we couldn't write
to a quorum of the notes we only could
write be 21 however that value is there
and if you ever did a read and you
specify it a read quorum of one and you
happen to get the
from this guy first or you're dealing
with a more common scenario where you
have a part we have a network partition
and your partitioning guys who didn't
get the right are separated from the guy
who didn't get the right and imagine
there's a bunch of other servers here
are other replicas all you're doing that
value the reef valium of one this is a
bad situation to be in and the
mechanisms that allow us to repair these
things or things like active anti
entropy and read repair where we can
based on a quorum of values or based on
vector clock subversion vectors or
active anti entropy we can eventually
fix these values so everything agrees on
the right value but that's a situation
that's that's difficult to reason about
and programmers don't like thinking
about that because I you end up running
like a lot of code to work around these
problems so this thing is getting all
confused I don't know why okay so strong
consistency so so what is strong
consistency in terms of react so it's
single key atomic operations we want
Adam icity of a single change to a
particular object in the datastore and
we want that change to be across an
entire replica so we want all three
objects to all three replicas of that
object to get the correct value
immediately so more succinctly as it was
ringing on the slide any good is going
to see the most recent foot you're going
to write about you it's going to ensure
that all the replicas immediately you're
in that Valley
and then your next free is going to see
that observed value immediately and the
way this is supporting into the
mechanism is that it's going to have
this get modified put cycle so you're
going to read a value here you're going
to get the vector Clark which is
basically a context of the read you
modify the object and when you write it
back you specify this context you sure
that context is we assure the object
hasn't changed since you observe the
read and we write the value and ensure
that all of the replicas I grew in case
we have no right the object that you
don't specify the context will refuse
the room so if you read X and the cut
and it says the context is one ms it's
the first value of x and then we say
write x again and we get a no context
the system says what you didn't read
this one potentially we don't know how
to like totally order the rights
essentially and it will refuse the
rights you always have to specify this
context and the crdt world you have to
specify a context as well when you're
doing removes because we have the reason
about which things you've observed being
added to allow removes and that's a very
hard concurrency case but oh well api
all of our libraries automatically take
care it's you have to think about it so
this is again this idea of the consensus
problem that we all love talking about a
computer science this is the wicked this
is the app this is the quote from the
flp paper which proves that it's very
hard to reach agreed and when things are
failing and messages already synchronous
I apologize for the colors I didn't
expect to protect her to be this dim but
essentially consensus comes down to
three guarantees that you want to
provide in your system and this is
termination agreement and validity so
what are these things so termination
basically means that if I'm going to try
to achieve consensus all of the people
that I'm participating in this consensus
agreement with have to agree on value
determinate so if I have an algorithm
that's going to try to achieve consensus
this algorithm has to terminate this is
obviously a property that is very
important when it turning to ask to
terminate with the value the agreement
property said is that if all of these
things do terminate and return a value
they need to return the same value it's
not very useful consensus algorithm to
have a bunch of computers that agree on
a value that isn't the same across all
of them and finally validity is that one
of the servers participating in this
consensus section needs to a grip needs
to propose the value it's very easy to
write a consensus algorithm that always
agrees on the same value all the time
and it's never one that's been proposed
there's a funny post that peter baelish
wrote about this saying that like you
could write a perfectly acceptable
distributed consensus algorithm that
always agrees on the number 42 but if
none of your systems ever written 42 or
never plan on writing 42 it's a pretty
useless consensus algorithm so validity
is a very important property that these
algorithms need to have I'm not to bore
you with all the details but there's a
bunch of them so there's a million
variants of Paxos that you can google
about there's a zagg which underlies
zookeeper so that's the zookeeper talk
broadcast protocol and finally there's
raft which is like the more kind of
fancy new kid on the block which tries
to simplify Paxos but is essentially
kind of the same mechanism it's a
simplified API to the same consensus
problem if you're interested here's the
paper that started a lot of a part time
by the ice and land port you google
packs less you can see there's a lot of
different versions of it that are
wonderfully available for you to find
taxes made practical paxos fishin pole
pillars Texas made simple there's Paxos
made five Byzantine taxes there's a
million versions so you can read those
to your heart's delight Zab there's a
bunch of papers on it's a bouzouki power
Thomas broadcast protocol this is Hannah
invented a yahoo and they go to keeper
there's three or four papers that talk
about how this works and finally wrapped
the new kid on the block 2013 how's your
house dongara at Stanford University and
the main purpose of this algorithm was
to simplify pack so so it was easier to
implement because it Suntory ously
difficult to implement systems there's a
bunch of failed or like versions of it
as well we're going to talk about passes
we use taxes so it's a bunch of
coordinating the crests across the
cluster there's a bunch of leaders that
are elected in normal Paxos this is two
round trips per request so here's a
little process diagram you go to write
an object to the system we increment a
valid number we basically tell a bunch
of notes to accept that valid number and
no number greater than that palette we
got a bunch of promises from systems to
promise to accept that valid number or
no earlier ballot comers you have this
function that computes over the ballot
numbers basically sends a commit message
out to all the parties and that day
except so this is the basis of how taxes
work so every right into your system in
normal paxos uses this diagram here so
it's two round trips and it ships the
entire state when I say it's hard state
I mean the entire state normal Paxos
works on the state that the system
should agree on so normal Paxos if you
want a table my deal with react is kind
of the entire database so we'll talk
about how we get around that so Matt
multipack so this is a paper that came
out a little bit later and this is an
optimization because it says
two round trips for every single right
is expensive so we don't want to do that
so here's our state diagram we extend
this to have a punk that gets shipped so
we're also shipping this I value around
this is an epoch that kind of
monotonically increases and what we do
is we say we do this initial thing for
the first right and then this is done to
a leader election we have one prop
person who says I'm gonna do this right
and then for every additional right all
we have to do is increase along is that
notice dylan Rieder all we have to do is
increase that valid number and do the
one round trip process so this allows us
to make passes a bit more efficient
because we're cutting out one graduate
request all the notes / / right so again
this has this idea that way the
algorithm was written originally was
that you want to ship all the state that
you want to agree on and this is kind of
an academic thing with a gloss over the
practicalities of doing this like kind
of in a real system so we'll talk about
how multipacks us works in react so
react is key value all the keys in the
system are independently providing no
multi key transaction ability we provide
no operations over multiple keys every
value is basically independent in the
system and the mechanisms that operate
on these keys basically are these read
repair and anti entropy mechanisms may
prepare being Atlanta value gets written
and we want to ensure that it's I've
done all the nodes we get that value
back when we write into all the notes
active anti entropy being the mechanism
where we can examine entire partitions
of data and find missing data or
incorrect data and true them so what
does this give us so if you have
individual keys and none of them are
related that means that we can treat the
keys of atomic unit of state and this is
how we're going to operate our
multi-purpose implementation so
multipacks is perky so so to achieve
this we need groups consensus groups so
the party's in our consensus
who are going to agree on a value are
going to be the replicas for a
particular object in the system so how
do we know what group to make these
parties so we do this based on the
replica sense which I refer to as the
preference listen react so here we said
that if I write to this paint value I'm
also going to write a copy of the data
item to the blue and green value so what
we're saying is this will be a consensus
group this art preference list will be
the consensus group so object a has a
replica on these three the consensus
group that's responsible for making sure
that everybody agrees on off your day is
going to be those 3d notes and this kind
of repeats so you have n partitions
which means that you have n groups of
these so you essentially have any
consensus scripts in your cluster so if
you run a 64 partition ring you'll have
64 preferences and 64 consensus groups
and these consensus groups are called
ensembles so this is another react
library that you can use in your
application that's not super react
specific that's called react ensemble
it's available on github it is a
multi-axis implementation interlink that
is used to build distance tested
independently and shift as a defensive
end so you're free to use this library
so again just repeat ring size of 256
256 ensembles so it's the struffolis
preference list idea so um how do the
ensemble's work so in this group here
let me go back in this group we have
these three things these three denotes
the primaries will be running on usually
3d physical nodes guaranteed depending
on your cluster size there are different
processes their actual or Langton
servers what we do is we run a near
election so there's react ensemble
library gives you the ability to do a
leader election across a series of
processes so this is a mechanism that
allows us to this leader election so we
elect a leader and we
as multipacks let's use this we have
these monotonically increasing epochs
and these leaders coordinate all the get
input operations so as you're right
values to the API through things that
you want to be strongly consistent this
leader that gets elected / that group is
responsible so the leader is going to
read the local object off this because
it knows it will have a copy of their
other requests morning I've got ripped
rabotu it if the epoch is a little older
than the oldest it will go and refresh
the object across the replicas fine the
most up-to-date committed value and then
update all the replica so in the worst
case where the leader has been reelected
and all of the value is considered and
validated every request has a two round
trips where it needs basically update
the values in the breadth of you and
write the new value so you see here the
process diagram so if the object epoch
is less than epoch we get the key bring
it back we compute the latest update and
mark the epoch the epoch is stored as
metadata an object and then basically
hood we're already running really late
so I don't go through the whole process
diagram in detail but we can see how
many talk more about it but again I just
want to emphasize here that in the worst
case scenario we have to go there to
find the most up-to-date object that's
been written committed and then write it
back to all the replicas on the worst
case man there's two round trips in the
best case we can just start it up
locally so if we look locally and we say
hey the value i have on disk has the
latest epoch i can return that
immediately to the user and i don't have
to do it so this is interesting because
this means that compared to our eventual
consistency if you have a leader that's
been elected for a while and you have
rights coming into it this actually will
outperform the eventual consistency
version because it doesn't need to make
the round trip requests however you
start to pay the penalty when you start
having your failures and
you also lose availability because we
can't write if the majority is not
available because that is a core
principle of Paxos can't wait to the
majority can accept the right so put
operations again will really update it
well modify the object will commit the
modified object these objects you have
to send that context object so we know
that you've read the most up-to-date
value and we can kind of build this
total order of rights over the partition
and process diagram very similar get the
latest value right it back update epoch
associate that epoch the metadata put it
back so put operations in the worst case
it has to update the object at the
latest V clock verify that you read it
down betta spawn and write it back in
the best case you have one round trip we
just write it to all the robots so again
this idea of fail form if the leader has
to get reelected the epoch advances and
now you have this uncertainty across all
your data so you need to do that process
again you read off the disk when the
right comes in update the value update
the epoch in the metadata and then
continue receiving so almost done closed
for membership so how does it close your
membership work so you remember that
those notes you know if we go back to
that go back without my controller
failing you remember that in here we
have these three notes when I said at
the very beginning in react that these
partitions can be redistributed as notes
online so if the it greens the leader
and you know I want to remove green from
the cluster I'm going to force a leader
election if I haven't remove all three
of these and I add three whole new
servers that I have to do
with that scenario to where potentially
the entire quorum of nodes with the
entire preference of the entire
consensus groups going away so if I
advance to back where we were so how do
we do this so there's a mechanism that
exists called multi paxos joint
consensus and the way this works is that
if the preference list in the existing
cluster was that nodes 1 2 &amp;amp; 3 with the
three replicas and I'm changing the
topology by adding removing a bunch of
notes and I'm moving to the 7 8 &amp;amp; 9 are
the notes are going to be responsible to
replica joint consensus says build the
two lists of what you think those
consensus groups will be excuse me and
then you create one group we do
consensus against all of them as the
transition is happening and then
basically as nodes become unavailable
you remove them so this was just a
mechanism for mitigating dynamic
membership while doing cluster tuality
changes so you can read more about
multipacks let's join consensus so
there's a little bit better diagram when
we say you iphone this whole group and
then as these notes offline I start
removing them from the consensus group
and then I eventually end up with our
thumbs up oh so this is how we mitigate
those cluster changes while allowing the
system to marry okay um and that is
basically it so that's the overview as a
marathon 54 minutes 54 minutes of some
good computer science I'm happy to take
questions on eventual consistency or
strong consistency or serio tease or
whatever what would happen if you've
never been done amidst it up
joining the two consensus groups so if
while it depends on if it's a non leader
note or leader now so you're going to
have that two-phase times that two phase
commit cut off its not release commit
but you have a lot of two phases of
prepare and homemade operations so if it
goes down and doesn't exceed it's
handled in the same way that acts those
handles undated not basically responding
when access being tolerant uh whatever n
/ 2 plus 1 or whatever failures or no is
yeah in the three in the in the three
node version the multi vectors will be
tolerant to two failures right in the
sub reactivates too much possible to i
plus 1 to x plus one time the number of
those sir ready to excess one though
Jack was wine and tolerating that
fabulous the terrifying SMS it's a
little more something luck is the X on
the simplest one
so sometimes just a few witnesses they
don't need to do the operation you just
need to remember that vision taxes may
be preferred provider stand it you need
to have equal synchronize welding
operations have enough people pass type
of thing for keep your nose are not
really synchronized on the keyboard how
to deal with that and then maybe some
comments on the draft a distraction
device the things really efficiently so
I don't know the answer to the first
question because i did not implement
that part of it i find i really do crdt
side of things the strong consistency
stuff is implemented by Joe but I'd be
happy to get the question answer for you
in terms of the semi cases was wrapped
like if if it implements it so that I
mean the main point of the raft paper
was to simplify the terminology and like
the a lot of it and it's different it's
very so I'm very much just of the same
place in it because I need somebody
that's do certain things to say sure
that's not right but it's an interesting
paper trails it's very similar but it
isn't different algorithm but it's
interesting because the evaluation of
the paper is not done based on it's not
based on like performance or correctness
of the algorithm the paper is evaluated
the paper was evaluated on an
undergraduate like quiz at Harvard
University or maybe I what maybe it
doesn't say harder
it actually was Harvard and basically
they explained this video's to get level
get scripture numbers to the same people
and they lost the questions that let's
see which one understood better right
and this assignment it's so interesting
but but but both oh you think police it
boy whining research you know what I'm
thinking the wonders these little
doesn't allege conditions that everyone
gets wrong and you thought about it
makes you right and the papers the
papers mandatory ously refused by a ton
of conferences that's a bit the road
yeah so and there's an interesting story
there so that's all I can really say
about it I don't really know much more I
mean I know like people who have
implemented in one of the engineers that
lives pretty close to me in the States
because we all look pretty far apart as
was working on the earlier version of it
so I know people are worked on it but I
never implemented it I read I read the
paper briefly
you mentioned before that was there's a
property oberlander didn't support
something causal delivery oh yeah so our
line has some interesting so there's
interesting if you read the research on
cloud Haskell they talked about this a
little bit as well so if we're like as I
property with causal delivery between
processes it says that if process k is
sending to process B process B will
receive processes A's messages in order
to have this like causality guarantee
between a but you don't have that the
causality guarantee is not transitive so
if you happen to send if we have three
processes and I sent you a and then I
said to be I and a is going to send to
be as well when it receives my message
so i have this transitive message
sending here it makes no guarantee that
when that messages delivered so you
actually don't have causal delivery as
you'd imagine like like other types of
yeah so that's the property that makes
it very difficult to do certain things
and some of the work on I think it was
gem leader if you read the knob if you
read some of the early generator paper I
think the whole foyer on svensson paper
yeah Thank specifically talks about that
being one of the challenges another one
is like the failure detector is working
their way so but yeah so that makes it
hard to do certain things also like no
ability to do atomic broadcast so like
Isis stops I'm a broadcast where I could
say send a message to five processes and
they all get it so like in Erlang Erlang
has this module PG that exists that's
still in the runtime or maybe it's not
our 17 anymore that's been replaced by
PG too but PG originally wanted to
provide the ability to say like bang a
list of processes and they all get it or
didn't so wanted to have reliable atomic
broadcast that's a really whore
thing to do in our line and there's a
whole paper about it I take it to
Haskell is this no but when a building
cloud Haskell they had a deal with some
of the weird things because her legs
behavior with sending messages like
there's what is it there's some sort of
thing that like depends on time to you i
forget i forget the specifics but if you
look up loud haskell stuff where they
try it they take like the unified
semantics of Erlang paper and try
implementing it in Haskell's it has like
the actor model would like a lot of the
same mechanisms you run into problems
where there's a bunch of stuff that the
behavior is like this behavior for the
first 30 seconds but then it changes
this other behavior once you past 30
seconds and there's like a lot of weird
things like that that the house cool
guys don't like because it's hard to
model so they diverge a bit from the
same semantic from the genetics but yeah
so yeah so some of the crdt stuff
there's much papers on commutative
replicated data types and some of the
commutative updated data type rely on
causal delivery or and in this research
project we've been doing some work to
examine whether whether like Steve a crd
tease the convergent ones that we merge
rather than the commutative ones were
just all ops get reordered whether like
trying to build some cause of delivery
mechanism sacrifice
availability so we can explore some of
the commutative data structures rumors
were flying out so all the time you
mentioned si si si de artistas does the
athlete referred to the ones all right
merge the state that are the bound to
join services those are the convergent
ones those are stated right not
commutative yes the state-based ones are
where you shift the entire state of the
object around and then you can program
the commutative ones are you guarantee
that all the replicas observe all the
updates and those all of those updates
are commutative an idempotent if you can
guarantee that you can purchase the
right state but there are a bunch of
gotchas when you start modeling more
advanced type of data structures where
you have to do room kooples you have to
do removals you can't really necessarily
guarantee that every operation is
commuted reminded but if you need to
provide a garbage collection mechanism
specifically a garbage collection
mechanism that doesn't require consensus
so again if you think about these two
approaches that's the interesting that I
think the highlight right is that one
way to clean up the state the garbage
that the crdt generating the eventual
consistency world is to say well let's
go strong consistency has raised taxes
and clean up the state the problem is
that's not very interesting research
problem and it's also also not very
interesting from the scalability for a
bit as well specifically some of the
work that I've been exploring is a what
some other people is using see our
duties as client-side states you have a
rich JavaScript application that can go
offline and you want that to come back
online and you want all the state to
emerge so it's interesting crdt is
provided very useful mechanism for that
you use it for offline games we have
mobile devices and things like that like
they're very interesting data structures
to use there and in those cases you
can't clean garbage you can't get
everybody or you can't tell everybody
you know sign on with your mobile phones
at 7pm Rhenish meantime because I want
to run the garbage collection algorithm
doesn't work so so yes this is why we're
trying to find ways to avoid creating
garbage this too too funny very
soft-spoken cmpt you did the co Kiki I
did solve this your dt work mainly Sean
Crips and Russell Brown did the Erlang
implementations I've been doing other
research related things like prove
ability and stuff like that soon
everyone looks like boots but I do you
know how many I've worked with that I
might be able to answer the question
well it was it was just what did you
find satisfying but since objection so I
implemented the instructions for yeah
well i saw i implemented them I've done
implementing implementation of the co 2
t's enclosure and I've read record it as
well and I also did a moment ation of
the crd tease in the theorem prover
so so yes I did find it very satisfying
the Erlang one I did some somewhere so
my not the main role I work that I've
been doing is trying to figure out how
we can get your duties to cross multiple
key user partitions because when you can
when you have the ability to arbitrarily
ness these maps super big once those
objects get to be like a neg you start
like really destroying or length
distribution so what I've been
specifically working on that I have a
couple prototypes up is the ability to
split them off so you're going to write
one nap and then it will shard like kind
of auto show it's the key but this is
like purely academic stuff for a paper
not specifically anything
would be in the datastore because what
what's interesting to me is rather than
try to provide like a highly like a
strong consistency multi transaction
over CR dt's where i can say i want
anatomically right an update to a series
of twenty our duties I'm wondering if
there's a more interesting use case
where you can say use bottom values of
the lattices like if you know what the
crdt structure is going to look like it
doesn't matter that maybe you can
observe one of those updates because if
you're going to cause we make a change
based on that and you'll guarantee that
the change converges then maybe you
don't need to observe the whole state in
addition to that what is also
interesting related to that on the
performance side is that if i have a
mega map and I go to write that to the
database and then it auto shards out I
like the ability to query it and say I
only care about one field go to that one
replica and pull it back and don't worry
about reassembling the data structure as
well so that's kind of some of the stuff
that I like personally been working on
that I think make it not necessarily as
research data structure wise but like
super practical because we all know that
like busy disk buffer and like TCP a
pass and stuff like that or like real
problems that cause systems to collapse
and it's crazy of these data structures
that will guarantee that everything for
urges but if you can't use it a large
scale then maybe it's less useful you
know having a stuff off your data in one
key is
sadly the best approach so but yeah the
work is the work is super interesting
some of the other work that we were
doing related to it was exploring if we
could automatically generate the crd
tease from a theorem prover yes I was
yeah so that a good I was going to ask
you both how do you validated do
orientation against the instant paper
but then there's the other thing which
is the paper itself people
implementation structures and papers
combines you get part way through the
other thing I'm just sitting with one
what this has been yeah the original
crdt neighbor has like 20 TRD teased in
it and there's at least four of them
that are incorrect in the paper like the
actual like proof and like semantics of
how to implement it are just incorrect
and if you try and fomenting it it just
won't reproach we see that's why I don't
get it hahahaha realize difference oh no
no but no one is you're looking at those
in the conversion and community of paper
and so the basic ones are okay I forget
there's a couple of them some of the
more advanced ones towards that what's
that just grab since the grass that's it
the graphs are one where the
implementation is not correct I remember
if i remember correctly and whatever the
one they talked about in the same
section is the mass is also wrong the
graphs but yeah the graphs I I know for
a fact that the graphs is one book so
yeah I mean yes it's an interesting
thing we have we've had problems where
we've implemented quick chek suites that
are pretty extensive tests for some of
the crd tease and run the tests for like
30 hours or something and a models have
it wrong and we'll have a customer
but within 10 minutes of trying it and
then we have to go back and like fix it
so yeah it's hard it's ordered on the
papers don't have a lot of proofs and
one of the talks i gave about
implementing some proofs for the pn
counter i use one of the lines in the
paper that was just like the fruits are
left into an exercise to the reader so
yeah I think but it's super fun research
and I love it and I wouldn't trade it
for anything so but yeah you have to her
to be very careful so we're trying to
find mechanisms to make it so we feel a
bit safer about our implementations but
you run into the same problems you have
with everything early yep anything else
it's going to marathon meet up here
that's crystal
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>