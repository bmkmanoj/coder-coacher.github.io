<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scalable ETS: Does Such a Thing Exist? - David Klaftenegger, Kjell Winblad | Coder Coacher - Coaching Coders</title><meta content="Scalable ETS: Does Such a Thing Exist? - David Klaftenegger, Kjell Winblad - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scalable ETS: Does Such a Thing Exist? - David Klaftenegger, Kjell Winblad</b></h2><h5 class="post__date">2013-07-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v0EcMi_gc_0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello
as you just heard we are going to speak
about the scalable ads and what we mean
with that is how when its scales when
several processes access it's on a
multi-core machine we are coming from
Uppsala University and we are working
with our supervisor Costas Saginaw's so
just briefly because I think most of you
already know what it is it stands for
our long-term storage and it's a key
value stores and the keys and the value
or stored in a couple so one element in
this couple is the key and the rest of
this Apple is the value and it supports
operation such as look up insert delete
and queries that return a list of all
tuples that much specified pattern and
the earth is used in necia distribute
the database that is included in them
would be you can look at ed's as the
in-memory database tables and a short
example of how it is used the ex new
call is used to create edge table the
first argument is a item that can be
used as an identifier of the table but
only if you say that it's a name table
in the settings list that is the second
argument and you can specify a table
type and the full table type is set and
then all the keys in the table are
unique and in bag you can
tapos with the same key in the table and
duplicate back then you can even have
duplicates of tuples in the table order
set is the same asset but then there is
also an order between the elements so if
you traverse all elements in the table
you will get them in there our long-term
order of the keith and you can have the
different protection modes so public
means that all processes in the airline
node can access and modify a read from
the table protected mean means that the
owning process can write to it but all
other processes in the system can read
and private then only the opening
process can see it and this key posts
setting is you can specify which element
in at Apple contains the key and this
right concurrency and read concurrency
options is fine tuning options when you
are running online multi-core machine
and have shared tables and are here are
some examples of operations when you
insert you or you call an ex operation
you you give them the function the table
identify and in the inside you can give
it a couple that you are going to insert
and look up the the key that you want to
look up so we think it is important for
our line the searched 180 open source
project and out of them we found that
the
least 41 you share the eight tables and
you can take these numbers with a grain
of salt because yeah for example if a
library use on each table and the source
of the library is not included in the
repository then you will not see it here
but I can just ask you how many of you
are using shared at stables yeah so it's
quite a few almost half of you think
Google's more yeah so why is that so
popular you think there are two major
reasons one is convenience that aids
provider and way to do a frequently use
task in a standard way and it's easy to
use and also for performance reasons it
is implemented in C and make use of new
table data so it will be difficult to
implement something with the same
performance in with a pure line
implementation and it's also scalable on
multi-core systems but we have a
question mark there because we will show
later how scalable it is so when you're
programming with our line you have two
ways of communicating between processes
or two main way with a one is message
processing and that can be a problem for
some applications for example if you
have them a process that has like the
sole purpose is to provide data to other
processes then it can become a
scalability bottleneck because that
process can only serve one other process
of the time and you there are ways to
get around that you can divide the data
between several process
so on but instead you can use a shared
at table to do the communication and
then process it can access it stable in
parallel or they can it we will try to
to investigate this question so what
kind of systems do we want it to scale
on SMP means the symmetric
multiprocessing and we yeah normally
what you mean with that is a single
sheet with multiple cores and then we
have a relatively sheet communication
between the course on the system and
numa non-uniform memory access systems
and the in such a system you have
several processor chips we possibly will
be the many cores in each ship so then
you have sheep communications between
the course in the same ship but a
comparable very expensive communication
between the ships so you have to think
about this when you are designing
scalable at implementation so just
briefly like how f is implemented as you
saw you get a identify from the ettes
new function and this identifier is used
in the operations to get edge table so
you have a metal table containing
mappings from the table identify to the
table data and this in the current
implementation this metal table is
protected by a couple of blocks so each
slot in the metal table have a look
a really writer lock that you need to
take to make sure that the table is not
deleted by another process for example
so there is in this metal table slots
there are also a flag to says that are
these slot is currently alive or depth
so when you have they're taking the red
a lock of the of the corresponding table
metal tables lot look then you can check
if the the table is alive and you can
read the table data and the meta name
table is the corresponding table to me
metal table but for named tables and
other back-end data structure for
ordered set hits AVL tree and the hash
table for set bag and duplicate back the
over / l tree is balanced binary search
tree and that one doesn't have any
fine-grained locking so it's protected
by a single reader-writer lock and the
linear hash table is a hash table with
so every bucket in the hash table is a
linked list containing the elements that
are storing it in this packet and
resizing is done one element at a time
in such a way that it tries to maintain
average bucket length in all 16 b 6 but
all signs only one process can or Fred
can grow or shrink the table at the same
time there might be larger bucket length
and the linear hash table by default it
has a single reader-writer lock to
protect the table for concurrent access
but it also supports fine-grained
locking but there are some operations
such as atomically inserting all
elements in a list that still takes them
the global table lock and are you could
get rid of the global table log by
taking on them the necessary bucket
looks but that will probably not be so
performant so bright concurrency is
activated with this bright concurrency
setting that you specified when you
create a table and this option was
introduced in ODP version or 13 b 02 and
originally it was were initially it was
16 bucket looks but it was increased in
our 16 B we spoke to their OTP team and
then I thought that we have found like
in a benchmark that it worked better
with more bucket locks and then in the
next version we had 64 bucket loads and
I'm going to take you for one example
what is happening when you have
activated right concurrency in a set
table so initially you don't have any
other processes working with any table
so all the locks are open and we want to
access the table with ID 257 so
we take the corresponding metal table
look for reading and we see that this
table is alive and then we can take the
the table look also for reading so we
can allow concurrent rights to the table
and then we find them we are first of
all we we release the metal table look
then we find the bucket and we need to
take the bucket look for writing since
it's yeah modifying operation and then
we insert the element into the bucket
and then finally we can release the
bucket lock and cable lock so now David
is going to take over and speak about
the read concurrency option okay so we
have seen a lot of locks right now so i
will quickly explain how reader writer
locks work on a more abstract level
essentially a writer is taking your
mutual exclusion look as you would
expect and for convenience of thinking
about it you can imagine that a reader
is simply increasing a counter if the
lock is free and decrements that counter
again when it's done with reading and
anyone who writes after locking in the
mutual exclusion lock will have to wait
for that counter to reach zero again
that means it's probably slightly more
expensive to do rights but you can do
reads in parallel the read concurrency
option was introduced in our 14b and it
it is an optimization to these reader
writer looks so it maps instead of all
the readers to a single counter maps
them to reader groups for each group
shares a counter and normally what you
will end up with is that you get one
along scheduler parade
so they just have a counter that counts
0 or 1 yeah you want to do that because
reads should really be fast and a fast
right is the right and modern computers
in the local cache and if you have many
many cores that access the same counter
they will actually invalidate each
other's caches so you have to split that
outer a little bit so to avoid the cost
of invalidating the cache lines all the
time and because you fetch cache lines
you actually have to make the counter 64
bytes so that it occupies the entire
cache line you don't use all of this
memory you just have to Pat it a little
bit so where does this cost come from
modern bigger machines are usually are
not SMP anymore my home computer is an
SMP machine with a single processor chip
but bigger service today have multiple
sockets and many many cores and the
problem there is that if the reader
groups are laid out they will be cached
quite friendly so that every core gets
its own one locally cached now if they
had to share for example between core 0
&amp;amp; Co for the invalidation would not even
be within a single chip but would have
to traverse the interconnect between
several chips and that would happen
every single time one of these cores
doesn't read on a net stable and these
invalidations get really expensive
really quickly so Rita groups helped a
lot in that regard yeah the maximum
amount of reader groups in our 16 B is
64 if you have even more schedule as
running by default you have as many
schedule haces cores in your system then
they will share counters up to 64 course
you should be fine right now and we will
discuss this later when we see some
performance graphs so
let's sum this up again right
concurrency is working only on those
tables that are based on hash tables so
set back and duplicate back and you get
64 bucket looks right now thees bucket
locks do not have reader groups and only
if you also set rate concurrency on the
table then these bucket locks get reader
groups on the other hand the main table
lock always get through the group's no
matter where you specify read
concurrency or not we have right
concurrency you get reader groups on the
mainly table log it is not really useful
to have a main table log and packet
locks and the main table locus than the
bottleneck if you just specify which
concurrency you get the reader groups on
all table types that's what it does yeah
so let's look at some benchmarks for
running all this so the benchmark we use
does some warm up initializing the table
with 1 million inserts on a key range of
4 million and then does 4 million
operations now it's two million key
wrench sorry 4 million operations we
have doing only lookups and doing ninety
percent lookups and ten percent updates
and updates or deletes and inserts with
equal probability so that it stays about
the same size we have bound all the
schedulers to course so that they are
not moved around by the operating system
you can do that by specifying a runtime
option to run and it's just a command
line parameter and we will discuss why
this is useful and for the graphs on the
x-axis you see just the amount of
schedule as we use and on the y-axis
time in seconds so it's time not
throughput so first let's see how much
better it already got good news the
first thing we have is our 11b which was
the first SMP enabled / long runtime
system our
13 b 02 got right concurrency I'll 14
before it concurrency or 16 B because it
is what we currently have so yeah this
is running on Intel sandybridge single
ship of with eight course hyper
threading is not used so these are real
aight course and as you can see our 11 b
is the redline it didn't work very well
initially with our 13b it got better
this is a benchmark only doing lookups
so the right concurrency didn't really
have anything to do with this this is
having a new locking library and
optimizing that a little bit as well and
the current versions perform rubber
similar and looking good on this graph
up to eight cores looks nice so for this
is I only benchmark for all the
benchmarks here so I were set or all
that said unless I say something else I
will say what it is but the set is
representative for bag and duplicate bag
as well in case you're wondering so what
that set looks pretty much the same not
a huge difference at all but once we run
this on more than a single processor
chip the picture looks slightly
different this is still ordered set
still only doing lookups and what we can
see here is that once you have more than
eight schedulers we do no longer find
that to a single chip it spreads out
over multiple chips and this suddenly
has a cost and it gets slower again and
for all of them even the current ones so
that's something we will soon see how
that can happen the good news is for set
the new versions actually do not have
the problem they don't get much faster
anymore but at least they don't get
slower yeah so we had a look into
ordered set what is causing this and
patched it a little bit
avoid some cash line and validations
within the AVL trees search operation in
the tree and as you can see there is
this see online down there which is the
patched or that set with rid concurrency
and it now is even faster than I look up
on a read-write concurrent set this
patch will probably not be in the next
release but a very similar one I hope
will make it into it this one has some
downsides I just quickly patched it up
it's a two line patch it really hurts
when you traverse the entire tree so
that's why there is a little bit more
work required what's also interesting in
my opinion is is that you can see cache
line in validations very quickly bring
you to the level of performance of not
having read concurrency that's always
the same roughly the same amount of
performance you get so adding updates
when operating on set we can see that
not a lot changes the old versions get
even worse and the new versions don't
work as well anymore they get slower
when using multiple processor chips
unfortunately but not by a lot r16 be
pretty much is stable you don't benefit
from using multiple chips unfortunately
but if if we do other things you might
still benefit from it so these
benchmarks are really doing nothing but
its operations it might be not your
average workload ya doing this on
ordered set however is somewhat painful
here we can see that it yeah obviously
on the old versions doesn't work at all
interestingly enough ours are 13 B is
much worse than our 11 b and yeah our 16
B is worse than our 14 B and what we see
here is for I don't exactly know where
this council that gets more expensive
after eight cores is the Numa effect of
having
the processor chips and some things
really have a problem in performance in
order to set when you do parallel
operations that do updates this is
somewhat expected it doesn't have fine
grained looking after all so you do
access one big fat log so let's look at
what really is due to these concurrency
options the options you have is not
setting them or setting read concurrency
setting right concurrency or setting
them both and for our 16b what you get
is a level of performance were yeah the
interesting parts here are the green
line and the red line where you see
without setting these options it's not
as bad as you might expect on a single
chip but as soon as you leave one chip
you absolutely lose any kind of
performance you might have had and that
is exactly why you should use pinning
because then at least if it stays on a
on a single chip you'll gain much better
performance and when the operating
system is free to reschedule your things
you just get more of these reversals in
memory so usually you get the worst case
all the time the blue line with the
circle is ordered set which has the bug
already described so it would be
slightly better if that was fixed
already and the other interesting thing
it's somewhat hard to see here is that
the best line on 32 cores is actually
set with Rick concurrency the benchmark
is doing only lookups so that's the best
thing you can do if you only do lookups
you don't set right concurrency you just
said Rick concurrency it's somewhat
obvious but here you can actually see
the cost of enabling these bucket locks
and then you have to take them for
reading as well of course
yeah with updates things hurt just
setting read concurrency and using
updates and reads in parallel results in
the top two lines for set and ordered
set so as soon as you get updates into
the picture and you don't only read just
setting wit concurrency will hurt your
performance this is something you should
consider even if you have very little
updates ten percent is a lot but even if
it's much less you will see this effect
and if you really think I have so few
updates you should measure it's really
even with less than one percent updates
quite measurable with no locking you
gain better performance than that and
with READ&amp;amp;WRITE concurrent sets or just
write concurrent sets you gain fairly
similar performance now that's
interesting because only one of them has
video groups on the bucket looks right
so reader groups can be you can set a
limit on the command line the default is
64 and the real maximum number of reader
groups you get is the number of
schedulers or the number you specify on
the command line or 64 so you can
override the 64 but if you only have
eight schedule as you get a trader
groups there is no sense in having more
than that and it's yeah an optimization
for reads so we just ran it with
different amounts of frida groups and
what we can see is on 32 cores well
having a single reader group hurts
having to reader groups is already
relatively good this is nice this is
running on read/write concurrent sets
and between 4 and 64 reader groups there
is not a huge difference so
this isn't fully investigated but the
current theory behind it is that this is
a four socket machine and so with four
reader groups you're already gained most
of the benefit still there is quite a
large difference it's just on this scale
not really that visible you do gain
benefits from having more reader groups
they're just not as visible as just
blowing up your runtime a lot it could
still be arguable whether we need 64
really for all upset it looks pretty
much the same just that we have
additional performance problems in what
upset as discussed previously and these
low outliers here are pretty much the
randomness because the cache line
invalidations in audits that are not
deterministic so when adding updates
again here we can finally see what the
cost of having more reader groups is if
you have them you actually have to check
them and here we see that if you have to
check them and they are on multiple
chips you really use a lot of
performance and this sequential case
three seconds benchmark you can take
more than 17 seconds that's not the kind
of thing you want to happen when
parallelizing your applications I hope
yeah that's reader groups so the default
should be fairly sensible but you can
play around with it if you really have
to save memory something like that the
number of bucket looks as we said
earlier was recently changed and it's a
compile time constant so you cannot
specify it on the command line it's 64
now so let's hope it was really
important right on ten percent updates
again we see that with a single log it
stops scaling even on 84
pretty quickly this is 8 cores again and
we see that it gets better and better
and with sixteen lakhs we pretty much
have the performance we can get right so
as one would expect by now using
multiple chips it starts hurting and we
need more of these and that's in the end
the reason why we wanted this to be
increased and this is just using ten
percent updates on the operations ninety
percent of your operation still are
lookups if you now use more updates if
you just batch right to a net stable
from some parallel application this is
much much more problematic and 64 is
maybe not even enough to gain the best
of performance but on the other hand you
can't just use actually there's a
problem the code you can use more than
256 and also as you use memory for these
locks and for every one of these buckets
locks you might have reader groups you
quickly can blow into the megabytes
range for just locks on your rights
tables so it's somewhat to be negotiated
what the perfect number is yeah so maybe
this is a problem with the backends use
why are we using these weird hash tables
that need these kind of pocket looks why
do we need why do we use ordered sets
that have problems we can fix them but
these are not exactly new data
structures from nineteen sixty-two and
nineteen eighty so we tested a
concurrent skip list it's not lock free
it's not optimized it's highly
experimental and what we got on 32 cores
is just lookups we see it's slow but it
scales and at 16 course it starts being
faster than the current ordered set and
a skip list is in all the data
structures so it absolutely could serve
as a replacement if it wasn't that slow
now the interesting thing is of course
not for look
spot for updates ten percent updates
ninety percent lookups you can see
ordered set is not a good choice whereas
the Skip list based heads back and just
keep scaling and if we start to enable
hyperthreading on this benchmark we are
faster than set now so using different
backends is definitely an option and
will be investigated further and of
course one could argue that just this
hash table could be improved as well but
we haven't looked into that too deeply
yet and this skip list will not end up
in your ads it's far too slow in the
sequential case no worries about that
okay so the question of this talk is the
scaling Ed's exist you will be the judge
of that in the end I guess um we have
identified a few problems like ordered
set that can be fixed or maybe in
replaced with something else locking is
still a big problem but it got much
better recent versions of the runtime
system deal with this much better Numa
is a problem and I think it hasn't been
on the radar too much previously and it
is probably the biggest problem that
scalable apps will have to deal with in
the near future and reader groups may
not be as important as one would have
thought when just seeing them they get
some benefits but on these graphs the
difference wasn't that huge so what are
lessons we learned from really looking
into this on Numa systems you want to
use pinning the memory communication
costs otherwise is just too high on all
of the tests I did without pinning I
just ended up in the worst case for any
number of schedulers when running on a
server where you have other applications
you might want to scale down the number
of schedulers and just dedicate half
your machine too early something like
that it's probably more
work for the admin but it works much
better so you should consider that when
only doing lookups you can use read
concurrency and as soon as you have
updates you should consider using right
concurrency instead if you have a set
bag or duplicate back and if you really
want to combine them you should measure
what's the best option for you I do not
have any kind of boilerplate
recommendation on that measuring is
possible start with just by concurrency
and yeah so what can we do increasing
the number of pocket looks can help
scalability you can change that in the
code it's a single line more than 256
will cause some breakage in the code so
we certainly cannot go much higher than
what we currently have it will not scale
to several thousands of course with that
if we are lucky we don't get machines
like that so I don't know how anyone
here with a thousand for machine yeah
okay so currently not a problem but it
might be in the future and we still have
too many yeah new data structure
backends well it's certainly a hot
research topic and it wasn't that hard
to plug in something new so that's
definitely something even we will look
at so it also helps at getting rid of
these many many locks there is locks on
the meta table then those can maybe
really be replaced by atomic
instructions we have a patch for that
it's not fully tested but possible there
are locks on the table there are locks
other pockets this can be improved ivor
by better locking schemes are or just by
data structures that don't need as many
locks and an interesting observation was
that a good scaling ordered set
implementation can outperform set and
the assumption that hash tables are
always so incredibly fast is also
something that might be intuitive but is
not always true
so sometimes you might want to consider
switching over to whatever works best in
your code and not what you think should
work this yeah any questions
yes
when these benchmarks insert a very very
small topic so we don't think we will
have that problem but we have so we
didn't do any measure how costly it is
but we think that if the tapal is very
small then the copying might not be so
expensive it's these all these
benchmarks use a couple with a single
integer value in it so for scalability
it will not be such a problem because
you can copy the chapel before you take
any locks or something like this when
when inserting this is currently not
done by the code but it's an
optimization that will at some point
happen I guess you can do the copying
procession for the insert outside of the
critical section you can copy tuples
into the edge table in parallel that's
actually not a big problem for copying
out again you only read the edge table
so you can again do it in parallel it
should not be a huge bottleneck unless
of course your bottleneck is your memory
bandwidth but then you have an entire
different problem then you don't care
about any of this because really your
memory bound in your application but
yeah it's a interesting thing to look at
as well later
yes yeah the complexity class of skip
lists what do you have to keep in mind
if you want to use them efficiently so
what things are expensive what
determines the cost of fusion well this
particular skip list besides that it is
an experimental data structure that
didn't get a lot of attention for
optimization the problem of skip lists
usually is you have a fixed amount of
levels you have to go through so if the
data structure is not really big there
are quite significant overheads for
maintaining all these levels that you
could potentially have for faster
skipping so that you get logarithmic
access to the final elements that's I
think the biggest problem you
immediately get on any skip list but the
real problems depend on the Skip list
algorithm you use and there are quite
many by now we chose a simple one to
implement them not one that would
necessarily be a good choice and when
you implement concurrent they distracted
this skip list for example the search
doesn't take any looks but then to make
sure that it doesn't read memory that
has been freed by another process it
need to communicate what is currently
reading somehow and to do this it in
such memory barriers to make sure that
other cores can see the right and the in
it might be possible to improve but in
in this implementation you have a lot of
these memory barriers that can be
expensive
okay any more questions everyone
exhausted I think everyone just wants to
go out and drink beer something yeah
okay let us say that everyone is
exhausted then we can take questions
later some way out in the bar so give it
up for these two guys I think we're</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>