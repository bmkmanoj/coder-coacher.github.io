<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Kovas Boguta - Machine Learning Models: A New Kind of Software Artifact | Coder Coacher - Coaching Coders</title><meta content="Kovas Boguta - Machine Learning Models: A New Kind of Software Artifact - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Kovas Boguta - Machine Learning Models: A New Kind of Software Artifact</b></h2><h5 class="post__date">2016-11-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bY32GICYG68" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today I'd like to talk about models
machine learning models and in
particular their role as a software
artifacts and how that how they affect
our software development process so a
little bit about me right now I'm at
Twitter on the cortex team which is sort
of the deep learning infrastructure
engineering and model development team
at Twitter previously I was at Weebly
heading up analytics sort of more
traditional kind of big data and
analytics type stuff and before then I
was at Wolfram research working at
Mathematica which if you're not familiar
is signed piece of software for
scientific computing and mathematics and
other kinds of modeling than machine
learning type modeling as well as
working on Wolfram Alpha so the
motivation for this talk is really
pretty simple
these are ml machine learning models on
the whiteboard and these are machine
learning models and prod watching you
deal with sort of the mess that they've
made you know popping popping the
popcorn so for those of you who have or
are about to go through this experience
you know sort of this talk you can think
of this talk as a way to sort of come to
terms with or you know you know sort of
come to understanding of that experience
or for people who are inclined in the in
a different direction you might think of
this talk is kind of a call to action or
a call to arms for sort of fixing fixing
the problems underlying some of the mess
that I'm about to describe so to kind of
give you give you a taste for what we're
going to talk about here here are some
gremlin interview questions so we have
component X component X is a pure
function
over time it starts giving incorrect
results what happened or you know given
component X you have no valid
specification of the valid inputs or the
valid outputs right at estimate so these
questions seem a little bit odd because
you know they're just so
counterintuitive they're kind of against
kind of our way of thinking about
programs and part of the reason why
they're there coming up is because
although you can think of models as
programs in many ways they are programs
you know they take your input they
produce an output
there's also important ways in which
they are substantially different from
programs and in the way that we use them
and the properties that they have in
particular I want to focus on kind of
the differences that matter and
ultimately I think the differences that
matter most have to do with the software
development process sort of how we
create software and kind of expectations
we have about that process and sort of
the results of that process and at the
foundation of that process is sort of
the fundamental properties of the
artifacts themselves the primitive units
that were manipulating during software
development ok so software artifacts
look what kinds of things are we talking
about here so we're going to so
ultimately what I want to do is kind of
introduce models as a new kind of
software artifact so what are software
artifacts in the first place where are
the existing ones that we kind of want
to talk about right now so ultimately
artifacts are persistent and valuable
pieces of information that typically
have a name and address and kind of
structure our software over the long
term scale so for instance you know the
code you know the source code of your
programs yeah you know typically you
know sits on disk and eventually gets
stuffed into a data artifact which might
be the git repository or you know
probably the program artifacts that are
generated from the code
you know stuffed in some sort of package
manager or artifact manager or whatever
and then get called upon you know over
time to sort of implement other programs
and make things run so these are sort of
the fundamental pieces that fit
everything together and kind of kind of
just to be very clear when I'm talking
about programs and code or sort of data
I'm talking about these things and
they're sort of static artifact form for
the purposes of their talk so of course
you know we know that you know programs
can exist at runtime as processes and so
forth or that you know there's an ocean
of data that's transient you know when
it's passing messages back and forth or
internally within a program but for the
purposes of this talk I'm talking about
these things and they're sort of static
you know binary ion disk formats so how
if we think about you know well if we're
gonna introduce a new thing what is a
thing even mean they exist right you
know how can we distinguish a cookie you
know what is code from what is it
program I mean ultimately these are also
forms of data so what you know how do we
even begin to draw boundaries around
what these primitives are and I think
you know really really the only way you
can begin to do that is by looking at
the properties and relationships that
these different constructs enter into
with each other so you know they may be
fundamentally the same at some level but
in practice they're different in how
they get used and those differences kind
of cascade onto each other because you
start building tools on top of that and
and and and and things fit together in
different ways with each other so we can
kind of build a very simple model of how
these primitives interact you can think
of this as like a exercise and
computational thinking that's going to
help us reason about and discover what
are the differences and how we use these
things and where models fit in so we can
think of you know at the boundary of our
system there's a human that takes a
specification we're not going to explain
exactly what a specification is
and produces code so you know code is
exactly the thing that the human
produces the instructions that are then
consumable by the machine could say
programs are simply things that compute
things that take an input and produce
the output we're not going to define
what those are just yet and then we can
start seeing how you know a lot of the
things that we do in everyday
development can kind of start being
expressed in terms of these like very
simple things so you know get to check
out right so you have a program it's
gonna take a data artifact which is the
repository and it's gonna you know get
the right you know code artifact version
out of that repository or a building and
compiling your code you know that's just
another program that takes code and
produces another program or sort of the
notion of in directed execution which if
you wanted to really build us out into a
full model is very very important where
you have sort of you want to execute
some program but you want to do it
indirectly so you have another program
that takes as its input some program and
input to feed them through that program
and once you have sort of these ABCs you
can start thinking about how you can
construct the primitive sentences or
primitive words of the software
development process so a simple flow
might be okay we have a spec the Human
Rights Code two spec that code goes
through the build program to produce the
program at hand and then if we're going
to take that program feed it through the
test program together with some input
data and the expected output data and
you know that would raise the question
so okay we can think of human applying
to the spec as being at the boundary of
the system so we can kind of let that
slide but where does the test in and
test out come from you know there's sort
of undefined variables so typically what
we do is we bake them in with the
program right so we sort of enter those
tests as source code within the overall
program that that we're making so we're
gonna say so we're gonna write some test
code
and that test code is going to be a
function of either the spec and the code
or just the spec on itself and it's kind
of interesting that we've made this
choice very explicit here just by kind
of tracking how all these pieces come
together right you can imagine writing
tests exclusively based on the
specification of what the thing can do
you may even do that at the very
beginning of your process or you could
write those tests if you follow another
development methodology sort of after
you've written your code so you know
kind of take a spec of what it's
supposed to do and sort of the code of
kind of implementation details and you
may choose to you know write your tests
that way so once you have your test code
you can kind of build it together with
the actual functionality implementing
code and then you have another program
run tests which can extract those from
the artifact the program artifact and
run just the tests and output them
somewhere okay so kind of the point of
building up some of this formalism or
pseudo formalism is to sort of
contrasted with the process of what
happens with machine learning models so
the starting point of this process looks
very similar right the spec human being
writes of some model code to the spec
you build a model program from that code
and now you have an additional thing
that we did not have before so the model
program as soon as you've built it it
can take inputs it can produce outputs
but those are going to be meaningless
because we haven't actually trained it
yet it doesn't know how to do anything
so we can kind of imagine our sort of
sample application is going to be some
sort of pet image classifier right you
know we're gonna have some awesome site
where people are going to upload their
pet images and it's gonna automatically
know that it's a cat or a dog or you
know a llama or whatever and it'll
automatically index and classify them
and you know sort of recommend you sort
of pets you might like or whatever so
okay so we have our r-spec that's gonna
say something like you know please
PLEASE classify you know the pet images
we build a model that we think can do
the job but you know if we compile that
and build that it's just going to give
us random predictions so we put it in a
cat and you know with sort of random
probability you'll give you know the
right answer because it actually doesn't
know how to do that yet so we read the
training code we build the training
program and then the thing that creates
the artifact that we actually want which
is the Train model now the training
program consumes the model program
together with a new artifact which is
the training data right so this is this
is the artifact that we want so you know
does it you know does anyone notice any
differences from the previous step right
so you know you did not have this
training data artifact in the previous
sort of the traditional software process
model so so just with what we've done so
far we can sort of start summarizing
some very important differences between
models and traditional programs so you
know models are a typically large and
expensive to compute which leads to a
significant difference in how you
actually manage those model artifacts
also very importantly models are not
uniquely determined by code unlike most
programs you know most programs you
write the code the code itself more or
less uniquely determines what the thing
does models are not uniquely determined
by code because you have this extra data
dependency and this leads to important
differences in how you test these things
which kind of gets us into the gremlin
problem which I'll explain in a little
bit so let me expand on these points
just a little bit models are expensive
you know I don't know about the people
in this room but when I want to test the
change with my program and they need to
sort of build that program or execute
that program if that doesn't happen
within a few seconds I start getting
angry if you know if it doesn't happen
within a few minutes
you know I get very angry whereas with
models you know training a model that
will take as much computational
resources as you're willing to give it
which for most people tends to be on the
order of one day right when you're sort
of in some kind of iteration mode you're
like please you know let this be done
when I come in tomorrow so you tend to
sort of tune to the parameters to make
sure it can finish within one day and
once you think you're on the right track
maybe you'll run it for a week you know
to get it to optimize all the way down
or you know if you're doing something
really intensive it is not uncommon to
run these models for say a month or in
aggregate across many computers many
many months although you know the wall
clock time will will be will be a lot
lower due to parallelism if the expense
is still very high those things that's
true is you know they are not
necessarily small so some some models
are reasonably small but you can easily
get into the gigabytes range or higher
and then if you run say 50 or 100 models
simultaneously because you're trying
very you know a space of parameters this
accumulates to something that's pretty
big and you know what that means is that
okay you can think of models as programs
but if you actually try to treat them as
programs with the within the tool chain
you're gonna start running into problems
right you know I don't know about you
but I don't trust build programs to run
for a month you know and and make sure I
get my really important result at the
end or you know if you start uploading
your hundreds of megabytes of model
results and to me then you know how you
know that does that you know in the
meantime you know all the other
production systems are getting to
artifacts and you're out of the same
same place that seems like a pretty
pretty risky bet to take and likewise
you know things like continuous
integration I mean if your test run
within a few minutes or you know tens of
minutes then sure you know run all the
tests after every commit but you know if
it takes a day or multiple days you know
just for one test then continuous
integration sort of no longer makes
since as a proposition in the context of
sort of the whole process so this
quantitative difference in expense leads
to a quant a qualitative difference in
the actual development process so the
second point sort of the code is not
enough right the models the Train models
are not uniquely determined by the code
that you wrote so you know the model
behavior is ultimately determined by the
training data that you use which
significantly you know you sort of you
don't know what the code will actually
do at the time that you write it because
of this and it's also another wrench in
the artifact management system right I
mean for good reasons build systems
package managers you know are trying to
converge towards reproducible bills
having everything everything that's in
their domain under the same roof but if
you have this side data that you can't
really bake in or specify ahead of time
or really is just too big to possibly
put into you know something like git and
so forth then then that becomes another
sort of point of difficulty to contend
with so I I just really want to
elaborate on on the testing problem
because I think this kind of really gets
to gets to the core many many of the
engineering we problems we have with
deep learning models so we kind of sort
of work backwards from the example that
we saw before and sort of see what it
expands into and what information is
necessary when so you can imagine our
starting point is okay we're gonna run
tests on our trained model program and
you know let's assume that we have some
test code that's sort of where we would
want to be in sort of where would be in
a traditional kind of software
development step so where does this
train model program so the train model
program sub expands or comes from
applying a training program to the model
program and some training data right so
we want to test this program the
behavior of this program is determined
by training another program with some
specific data and we're gonna test that
guy with whatever test code we supply
okay so where does the test code come
from so as we had before we can say okay
maybe it comes from the human looking at
the spec and looking at the code or
looking at the spec on its own but then
you know we have a problem here right
because you know how you know how can we
test how can we write the test code for
something whose behavior we don't even
know yet right like we haven't when when
we're writing this test code we don't
have the training data yet so how can we
possibly write this yeah how can we
possibly test this thing without knowing
what the outcome of the training program
is going to be so you might say
something like okay maybe we can kind of
abstract this to something where you
know we just write the the code against
the spec and you know the spec will tell
us what the inputs and outputs should be
and you know we don't care about the
code or any other sort of implementation
detail but you know this also doesn't
get us out the problem right because you
know what you know so you know I've
imagined imagine our app right we're
gonna have a thing that's gonna try to
take pictures of pets and like tell you
what kind of pet it is so you know what
is your test code going to do is it
going to like you're gonna type in like
the bytes for like a cat and then type
in the bytes of the expected prediction
for a cat I mean that's gonna you know
it's basically impossible to predict to
that level of detail what this program
is actually going to do because you're
trying to predict the results of this
massive computation that processes all
this data and does all this optimization
and comes over this very specific result
so sort of unit testing in the way that
we think of unit testing you know this
is really not not a tenable proposition
and it actually gets it gets a little
bit worse than that
so sort of wither wither the spec you
know can can we even describe what these
things do anymore so the way the way the
way that we are used to thinking about
these kinds of things is if the input is
valid meaning you know does if the input
conforms to the spec and the program
conforms to the spec then we expect that
the output will conform to the spec this
is sort of the basic kind of you know
reasoning mechanism that we apply to
programs and but as we just saw so we
can't deduce that the program is valid
without more information and
interestingly the city actually the same
thing is true for the input right I mean
we're used to you know having database
schemas or all kinds of schemas that
tell us you know up front is the input
okay if the input is okay then you know
proceed with the rest of the program
because we expect it to work correctly
if the input is now okay you know bail
well we actually cannot do this for the
case of machine learning so and a big
part of this reason is because of this
data dependency in the training step so
model is strained on specific data
so we have a you know data set of like
all the pets that we have that we know
about you know the you know the universe
of possible pets but there's no
guarantee that the input at runtime will
not be different right you may have you
may have you know someone has decided
that they now have a wombat as their own
personal pet you know one that's we're
not in the training set so you know the
wand back comes in to your model and
it's like you know I don't know it's a
dog or something and sort of you know
there's there's really no way for you to
tell that something has gone wrong
because sort of the level at which it
knows what things are are sort of
outside of the formal boundaries of what
we can specify with rules right you know
internally you know it's developed some
representation of like all of these pets
and so
worth you know based on some you know
combination of features but unless you
understand what the model is doing then
there's no way to extract that and apply
it to the data upfront so when the data
starts coming in how do you say that
well now this is no longer matching the
kind of data that we expect so you know
there's no way to kind of formally
characterize even valid input and this
this and you know this is something that
you might call the model decay problem
we're sort of the Train of the model put
it into production you know it's like
performing well it's performing well and
then you knodel's sort of slowly start
to dip and start performing less well
and that's because the underlying
behavior of of the system we're
supplying sort of the input data the
distributions and that input data are
changing as patterns and the real world
are changing you know different users
with different interests or different
you know different specific memes or
different specific data is flowing
through the system even though all the
formal characteristics in the sense of
what we think of as kind of
traditionally correct you know software
engineering stuff remain the same you
know the metrics all look good there's
no sort of actual problem you know with
any code it's just that the the nature
of the content flowing through has
changed so the this is sort of my
reaction to this this this this that
effects you know this is this is really
not how I would normally think of
software behaving and you know these
kinds of situations would well you know
are quick to drive one into statistical
thinking right which you know might be
something along the lines of you know
you're saying there's a chance so the
statistical approach and and you know
there's more or less no escaping the
statistical approach but it but it's no
Savior either so okay you're like okay
there there's obviously no way I can
write an explicit unit test where it's
like this input equals this output but
you can imagine saying okay out of all
the
all the pets I want at least 80% of them
to be classified accurately now which
particular 80% might be subject to
change but you know that's that sort of
what what we're gonna define as
correctness but that's sort of that sort
of only kicks kicks the kicks that can
down the road because where does that
number come from in the first place
right it's sort of like you know if you
just you know show up and throw a number
and say you know we're gonna get 99
percent of you know all the pets right
or whatever that number is you know it
doesn't actually mean anything unless
you know what the empirical performance
is possible you know you're just picking
a number that doesn't mean anything it's
like something that someone who's never
used the Internet you know trying to
specify to you you know the
implementation requirements of a website
you know the accuracy the performance of
these things must be discovered you know
not specified and once you do that you
know once once you discover these
numbers or if you have a strong prior
eye you know you can use them for
regression tests but that doesn't really
serve all the purposes that testing or
other kinds of correctness verification
typically serve so for instance it
doesn't it doesn't help you with
debugging right you know you have some
system out in the field
it starts misbehaving you know one of
the reasons is you have these tests is
then you can kind of go in there and be
like okay you know start reasoning start
pinpointing where exactly the problem is
but you know sort of this kind of really
high level statistical approach is not
there's not necessarily going to help
you with that nor is it going to tell
you sort of what you can achieve in the
first place and you still have the
tooling problem right I mean there's all
you know sort of the nitty-gritty of how
do you actually make these things work
as sort of software artifacts you know
it doesn't doesn't go away okay so how
are we doing on time here
okay okay great
okay so software artifacts yeah so I've
sort of explained or tried to explain
some of the differences between machine
learning models and what we think of as
traditional software artifacts but sort
of so far you know now we just sort of
have a bit of a mess on the floor you
know there's sort of problems there's
not really good solutions and I think so
so sort of the you know to figure out
what the real problem is and sort of how
we might solve it you know we have to go
up go up the hierarchy a little bit and
think in terms of the overall software
development process so you know in other
words we're gonna bother trying to fix
this problem now what are we actually
trying to achieve here you know if we're
going to break out models as sort of a
new software artefact well what kind of
property is trigger should they have and
what kind of purposes should they serve
and I'd say you know the most important
thing is they need to serve an empirical
development process which is you know a
development process based on experiments
and sort of that much closer to the
scientific method and you know sort of
when people talk about the problems
associated with models you know they
often talk about our models are this
black box
you know the complicated we don't
understand what they're doing and you
know because we don't understand what
they're doing you know there's all these
problems right you can't sort of it it's
hard to engineer them it's hard to debug
them it's hard to predict sort of when
when they may apply and what they not
apply and when they're starting to fail
or and so forth and you know I I would
say they're not wrong in the conclusion
that if you don't understand what it's
doing or how it works then it's very
difficult to engineer with that artifact
but I think they are they are completely
wrong in the sense of you know thinking
that thinking that it is a black box by
Nature so you know there's lots of
things that we used to think we're black
boxes you know various aspects
you know the nature of the cosmos or
sort of the physical body the mechanisms
of life so forth lots of things people
use to think I've died like boxes but
what you need to do is you need to open
them up right you need to start poking
at them and see what they actually view
build intuition do experiments and then
sort of continue the process of sort of
building knowledge of how things work so
say open up the box right engineering
requires understanding of properties
these you know its properties are
empirically determined you know this is
same thing for you know structural
engineering right like you got to know
the properties of steel the properties
of concrete the properties of these
basic materials if you want to do
something with them and I'd say this is
actually a major challenge to the
software engineering process because
this is not how we're used to working so
software engineering privileges
deductive over empirical reasoning and
this is really built in built into the
whole field so the thing that we do all
day right pretty much what we do all day
is simulating a computer in our head so
that's deductive reasoning right you're
like okay I have a list I have a
function that you know takes a list and
you know returns like some other type
and then the next function consumes this
type and return some other type and so
forth so it's sort of this chain of
reasoning and inference of you know
saying you know if this and that and
making these deductions from the general
principles - to the specifics of your
program and that's built into our
programming languages into our tooling
and into our human processes around us
engineering these artifacts so yeah sort
of the happy path is the deductive path
right now it's so nice when you're in
your IDE and you know it autocompletes
you know the package import or
autocompletes know yeah sort of does the
type inference for you or you know
quickly runs the tests for you because
so you don't have to sort of manually go
through those like deductive steps and
you know know what kind of catch you
catch you from not being able to deduce
correctly everywhere all that
I'm at the same time now that's that's
when you're on the happy path you know
when you're not on the happy path is
when you have to do an empirical
experiment right if you actually have to
write code you know if you have to write
code that's not part of the actual
programming of the actual program that
you're developing but it is only there
to give you information about the
program that you're developing then then
you're not on the happy path then it's
like oh my god this is like such a pain
you know please you know can I just
write a print statement somewhere to
give me that
minimal extra piece of information to
perform the next deductive step oh the
problem is not in this branch is in this
branch and you know to be fair you know
sort of data and modeling you know it's
long obscure right this is not it's not
been in the mainstream not been in the
mainstream of programming and these
concerns you know simply just did not
exist for for many programmers so you
know for a variety of reasons you know
software engineering has privileges
deductive reasoning over empirical
reasoning and just to give you sort of a
simple piece of evidence that that this
is the case you know think about IDs or
your favorite you know development
environment know festooned with your
type inference you know step you buggers
integrative test all these tools help
automate the deductive reasoning that
you're doing all the simulating of the
computer in your head where you're like
oh I wish I could just automate this and
then the IDE just does it for you lots
of that stuff but you know where's the
support for data analysis or running
systematic experiments right I mean how
hard is it to just like run some
benchmarks on different versions of your
code and just get a plot from that is
that something that's integrated into
IDs or you know sort of something people
just whip can whip out with willy-nilly
generally not the case let alone sort of
more complex type reasoning analysis so
so a somewhat more involved example
which I think kind of gets to some some
more core issues is something I might
call the
the linear successor model of software
engineering so the linear successor
model so this is sort of you know the
notion that a product is a linear
sequence of programs and program n plus
one subsumes program and in its
functionality and what the linear
successor model does is it solves for
the problem of the concurrent
construction of this of this sort of
product sequence right so you have like
a team of people you want to push out
versions of the product as fast as
possible and the linear successor model
tells you how to stage that work in
order to pump out these versions and the
way it does that is you have a high
branching factor at the leads now sort
of the individual branches of the
developers followed by a rapid
convergence to do the production
artifact so let me just kind of just
step through an example so you can
imagine you have one product right
that's sort of the product of your
company you may have three classes of
artifacts
you know dev staging and prod you know
prod is ultimately the product but you
have many individual core branches where
you know features are being developed
bugs are being fixed kind of conflicts
between other people's branches are
resolved and so forth by very quickly
all those branches merge into master and
then you know for instance CI build runs
test puts in staging and then eventually
somewhere presses a button or or or
something decides that this is now the
new product so where'd it work so this
is more or less there are many different
versions of this process but it's more
or less kind of the standard way that
things are done these days so what are
the assumptions and the consequences of
this of this model and and how do they
compare to sort of the needs of an
empirical development process so the
first assumption is art artifacts are
not valuable in and of themselves right
sort of you know the thing that's
ultimately about ultimately valuable is
the product and anytime someone needs
the product they can just ask for it and
they basically get the latest version
but the artifacts themselves in and of
themselves are not valuable and you know
this conflicts with our deed with
experimental results which are in fact
very valuable and we can't afford to
just sort of toss them to the side
another assumption is that branches can
be isolated right sort of ideally work
work on in done futures can happen
independently and then ideally merged
with as little sort of conflict or
overlap as possible whereas in an
experimental workflow a comparison is
sort of one of the essential essences of
what you need to do right and you know
sort of sort of the this may sound like
a simple thing but you know it's
actually an incredible pain right if you
may imagine having to yeah so some well
you know realistic case might be you
know you're trying to perform its test
your code and you're trying to make
improvements to make you go faster so
what do you do are you gonna have like
one brand you know are you gonna sort of
make a branch for each version of your
test where you're like okay I'm gonna
try this optimization try that
optimization you know sort of possibly
branch from there you know collect
collect all the data points from each
branch and then make an Excel
spreadsheet with like each branch you
know sha and then you know the
corresponding performance and then and
then what happens when you realize that
you set up your experiment incorrectly
and you need to generate a DNA in a
different way then you're gonna go and
like check out each branch you know fix
the thing you know run the thing over
again I mean it's it's a total mess
right so you know the alternative to
that is well you can have it all in one
branch and you can have like inversions
of that piece of code in one branch and
then once you figure out which one is
the most optimal you know throw it throw
the rest of the way and just merge the
winner into master and sort of all that
work you did this sort of lost in the
sands of time so
sort of you know the branching and
branches in the branching model is all
about isolating things it's not about
being able to easily compare things and
sort of when you get out of and when
your need is to compare things that it's
sort of not a very good fit so yes it so
the way that we develop software sort of
assumes that you know the kind of the
goal here is to have simple and
repeatable comparisons of all these
possible variations sort of at the kind
of unit test level for instance where as
experiments require comparisons and
complex and ad hoc ways so one of the
consequences of this is is bit rot so
sort of any code any code that's not
attached to the head sort of to the head
of the product is it ultimately going to
rot away due to underlying changes that
are happening all the time but
experiments are typically not attached
to the product head right they're sort
of things that generate knowledge and
information which then will generate new
models among which one of them will
eventually be integrated into the
product but all this other stuff still
needs to still needs to hang around
because when you develop the next model
and you're trying to figure out what it
does and how its performance compares
with other ways of doing things you need
to continue to be able to load all those
previous models that you developed right
this is sort of the empirical process
need to be able to have access to those
old experiments that not have them rot
away from underneath you there's sort of
another thing that you might call sort
of prod coupling or other people might
call it just bad design but you know
definitely it's pretty pretty prevalent
which is any code that is not directly
related to the product execution is
marginalized right if you're if you're
not on the code path if you're not
directly on the code path between the
user making the request and the user
making the response sort of you know
that's sort of the code that gets pushed
over off to the side and kind of creeps
around the edges because there's so you
know people people tend to prioritize
the prod code path at the expense of
everything else so if you need you know
if you need access to
in a certain way it could very well be
that Oh someone implemented that
function together with the message queue
and you know the only way to get that
thing to run is by pushing messages
through the message queue or it's you
know in the database or sort of whatever
there's all these couplings that happen
in prod where you try to do things in an
experimental setting where it's like
let's try all possible parameters and
then you're like wow there's literally
no way to like get the information in to
where it needs to go and then back out
then then you tend to be in a very bad
place experiments these flexible and
direct access to functionality so this
is sort of the the Katniss mode the sort
of Katniss and Peirce this mode here so
empiricism there's a final plea for
empiricism so you know I think this is
something everyone can agree with at
least you know deductive reasoning for
all things for which it works it works
well and empirical reasoning you know
for all things that we cannot deduce you
know it's all we got right and I would
argue that the set of things that
empirical reasoning will apply to in
software engineering and computing in
general is only going to expand as
systems get more complex my systems
become more difficult to predict and
understand your empirical reasoning is
going to be is going to start to become
a more dominant way that we approach
these things and figure out what's going
on and what we need to do but you know
this is gonna be a big project you know
it's not just kind of fixing maven or
you know getting better build systems or
you know having more research as to how
models you know deep learning models
behave you know it also involves you
know how do we communicate results and
you know all these steps of
reproducibility of the scientific
process but you know you know
fortunately that ball is rolling so you
know for instance you know what's an
alternative to IDs or an alternative to
get for storing code so if there's
things like notebooks with
after 30 years to finally become popular
so I Python her jupiter notebooks are
now becoming the de facto way of doing
data science and modeling and basically
any kind of thing that's of an
experimental nature because they're just
so much more clearly capture this flow
of you know I need to do an experiment I
need to see the results I need to
communicate those things to other people
I need to go back and reference these
things between each other and not have
to kind of be in and within the
constraints of you know the the linear
successor model of software engineering
now there's things like FB learner at
Facebook and other companies are
developing similar things which are you
know trying to solve this overall you
know model model bookkeeping model
computation model result analysis
package series of steps so you know sort
of the the the the the series of steps
that we were sort of implementing in our
little pseudo language you know kind of
made it into more of a development
environment in its own right or even
just at the fundamental level you know
things like data flow processing systems
and build systems you know seem to be
converging which I think will definitely
be you know the way things go in the
future and and sort of help solve that
problem for us so you know there there's
a lot of positive things happening
there's still a lot of work left to do
and I hope that this talk kind of
encourages people to either think about
and help help dig into some of those
areas and sort of in particular sort of
you know on the question of you know
what are the actual problems so you know
among things that are happening people
are starting to ask these questions of
you know in which ways our machine
learning is - our machine learning
models different you know how did the
impact a software engineering process
and trying to identify you know the
actual problems that we have so this is
me follow me on Twitter thank you
we can do a few questions hi thank you
for that
you mentioned notebooks and some other
solutions for communicating ideas do you
have any suggestions for how data
science teams can work with their
engineering teams to then integrate
those models I mean it's a hard problem
so you mean integrate the models back
into the product yeah so so that's a
very good question so yeah so because of
this sort of split between these two
worlds you know you know notebooks are
very useful for scientists they're sort
of less good as themselves being
software artifacts sort of sort of
ironically in a presentation about
software artifacts notebooks themselves
are sort of not necessarily great
software artifacts so I you know I mean
babe I have a lot of ideas of how they
can be better I think ultimately people
are going to go back to a more
text-based model at least for authoring
and that might be presented differently
but I think it's just gonna take a long
time to have those things be integrated
in a way that makes sense
I mean yeah yeah so I really make any
predictions as which one will work out
hi you mentioned that testing and
validating models is something that is
not straightforward as as with the way
software which is the testing and
validation process that you normally
follow what your with your team so so so
there's so there's a few I think you
have to think about testing a few
different kinds of things so you know as
I mentioned in the talk you know one
thing that you absolutely can do is sort
of regression test in different ways so
once you've determined that such such
and such performance or accuracy is
possible then you can say you can make a
test that's like okay did we achieve
this accuracy over you know this large
data set you know the other thing that
you can do is you know for a very
specific model you know say you've
already created a model you can have
regression test to make sure that it
continues to perform exactly the same
way so you know this data comes in and
this data comes out oh but you know
that's not something that you would ever
want to build by hand
so what you would you know sort of
typically you know you kind of want this
to be a automated process of you create
a model there's a bunch of analysis that
happens on the model you know sort of
that analysis sits off as its own
artifact and then you know the next time
you create a model that's in that same
lineage or with some regularity when
there's changes and source you sort of
want to run something that's kind of
like a integration test then you take
all these analyses of the different
models and then you compare them with
each other and according to some kind of
policy either say you know accept or
reject what happened so you know if it's
the same model that you're at that
you're testing then you want the results
to be exactly the same if you're testing
a new model that's developed with a new
methodology you know the results cannot
be exactly the same but you know that
they must you know be at least as good
or sort of conform within the boundaries
of the performance so so really it's all
about sort of automating these processes
of analyzing the model putting data
through the model seeing the results and
then sort of come
comparing what those things did okay
let's think to speaker again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>