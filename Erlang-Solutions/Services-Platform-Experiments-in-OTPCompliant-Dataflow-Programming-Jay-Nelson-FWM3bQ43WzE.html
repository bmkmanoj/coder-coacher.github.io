<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Services Platform: Experiments in OTP-Compliant Dataflow Programming - Jay Nelson | Coder Coacher - Coaching Coders</title><meta content="Services Platform: Experiments in OTP-Compliant Dataflow Programming - Jay Nelson - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Services Platform: Experiments in OTP-Compliant Dataflow Programming - Jay Nelson</b></h2><h5 class="post__date">2013-04-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FWM3bQ43WzE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so previously I worked on something
called Jen stream which was a way to add
concurrency to reading files and last
year I got the idea that 10,000 core
cpus are coming and people are
programming with Jen servers which is
one process with one state and somehow
that wasn't going to map well to 10,000
cores so let's see so how do we how do
we anticipate where the hardware's going
and make Erlang work well with that new
hardware so right now it encourages the
server style programming like Jen server
you've got one process and if you look
at the code that's been added to print
out the state when it crashes you know
that people to use very large states and
some other Jen servers and that is
something that cannot be distributed
across many cores when you're running a
large state and mutating it internal to
the gen server and there's no automatic
way easily to break up a gen server like
that you could lots of times the API
might have some read-only functions and
then it might mutate part of the state
in a few functions and part of the state
and a few other functions and you could
actually break the state up by trying to
analyze what gets modified and split it
into multiple processes but so that gets
you three or five or ten processes but
if we're getting a hundred or a thousand
or ten thousand cores we're going to
need ten times that many processes to
keep the cores busy so how are you going
to write code that has 10,000 cores busy
hundred thousand or million processes
how are we going to adapt to this and
there's there's two ways really that are
used for keeping lots of course busy the
simple way is I've got a problem I solve
it on one core and then i just multiply
that problem by a hundred and have a
hundred course do the problem
and that's basically what you get when
you're doing web services that hundreds
of people hit the server you've got lots
of course and you use a virtualization
of the OS but that's not what Erlang
programmers Erling programmers tend to
be writing applications that carefully
architect message flows and processes
and we need them to be writing thousands
of processes where today that writing
tens or hundreds of processes so how do
you divide and conquer your problem and
get it into many many processes
certainly you need to get fine grain
tasks right now functions tend to grow
large state grows large and you end up
with a module that might have 20 or 30
functions to manage that state and what
you would like to do is shrink it down
so that you have 20 or 30 processes each
writing one function just so that you
can get fine grained concurrency to be
possible the cost of that may be that
you're sending messages more than you
are now so the trade-off is going to be
there but if you can reduce the state to
smaller pieces that have fewer functions
touching them then you can distribute
the state across more processes and get
the concurrency but as long as you keep
a large state you're not going to be
able to so what I'm working on is open
source on github it's called Erlang SP
that's for services platform to go hand
in hand with OTP and you can include it
in using a rebar config you'll have to
wait for release version 0.1 to get the
features I'm talking about here which
should be in a week or two and it's
undergoing active development and
evolution that means I'm discovering
issues and changing them constantly and
updating my approach and philosophy to
this but the intent is a library that
you can drop in right next to OTP you
can write gin servers gin fsms and you
can write these new data flow style
constructs and have them interoperate
with OTP they could be supervised by an
OTP supervisor you could use all the OTP
to
for tracing and managing them and you
could gradually migrate your application
if you wanted to to using newer
constructs so we're encouraging services
rather than servers where server is a
single process with a state a service is
a collection of processes that's
providing functionality that's
essentially a subsystem to your larger
system and internal to Erlang SP is a
fundamental data structure called a
co-op it's cooperating processes and and
a co-op is actually a graph of processes
that are working in concert to get high
throughput of data flowing across a
network graph so rather than
representing let's say you have a gin
server that has five possible states
right now you would represent that with
a data structure that could have five
possible values and you would mutate the
data structure to reflect what state
you're in what I want to do is put the
state into a graph so that you're
representing the state in the structure
and the individual processes that are
running so for instance if there are
five possible values for the state you
know if I use an atom ABCDE there's five
possible states I can represent that as
five processes and have a router look at
the pattern and route the data to a
process that represents state B or to a
processor represents state D and now I
no longer need to maintain the state
internal to the process because that
process is dedicated to only that state
and it only needs the functionality that
applies to that state so essentially
you're taking as a state machine
normally has a state and transitions and
you're taking all the possible states
and mapping them to a graph and having
the transitions in the data flow so
that concurrently many instances of data
can flow across and be living
concurrently in different states of the
representation and it's kind of hard to
explain that in word so i'll have a
picture a little bit yes so for an
example I work with tigertext we're
doing the texting to mobile phones and
things like that HIPAA compliant for
hospitals just an example of what i
would consider services and when you're
texting there's presence you know is the
person online or not and we have BOTS
and other servers that we also report
presence with so the presence is
independent of everything else if I
didn't have presence I could still send
messages through the system the user
just might not know whether you're
online or not at the time they send the
message the connection listener accepts
new people you know opening up their
phone and logging in they get a
connection you could be already
connected and the connection listener be
down and you could still send messages
but no new connections can be accepted
so the services are sub system functions
that are independent of each other with
Erlang SP library what I want to do is
make services of first class behavior is
something that you can start and stop
and suspend and interact with so that
you can dynamically reallocate your
resources if you expect a spike in
connections then you can turn on more
connection services and turn down some
of your other services and just have it
adaptively change the system
configuration based on data flow and
traffic and things that are happening in
real time so message routing attachments
you might need an attachment system but
you know far less frequently than you
need presence so the data flow and
demand is going to change from system to
system so the goals of the library are
to simplify and encourage massive
concurrency and I mean I want to be able
to say I need a router that routes to a
thousand processes and set that up and
right
function that looks at the patterns and
decides how to route it to each of those
thousand processes and then once I get
traffic in I've realized that I don't
have enough processes I want to be able
to stamp out ten copies of that network
of a thousand processes and now I've got
distributed load balance 10,000
processes and it should be that easy to
do I don't want to be writing a gen
server and giving it a name and
registering it and doing that 10,000
times because I'm just never going to be
done writing the application that way so
automating the generation of a network
of processes mapping the mutable state
that currently resides in a gen server
to the graph and the way processes are
connected so that the path data takes
determines the state of that particular
computation and that's how we're using
data flow is that if the graph is in
place and data flows from A to G to F
that path determines what state the
computation is in and we want to be able
to incrementally improve an existing
system we want to take a system that's
got OTP constructs and add the new
concurrency and see if it helps on a
particular subsystem but leave
everything else the way it is and if
that works then you can incrementally
migrate your entire system and still be
able to use all the tools that OTP
offers tracy and debugging all that
stuff so a process network is a
collection of processes which are
essentially hardwired so that message
messages flow one way through the
network so the way I'm representing is
with a directed acyclic graph it's a
built-in library in OTP you can make
Daggs and they they actually implement
them using ETS tables so you can have a
giant you know million node graphs and
have them all interconnected with lots
of edges and replicating them and all
that stuff is all available the coop is
taking a graph and assigning processes
to each of the nodes in the graph so
basically what I want to do is declare a
graph structure like a fan out or a fan
in or a particular specific data flow
structure and use that as a template and
then create co-op instances from that
graph where the library automatically
populates every node in the graph with
actually it's about seven or eight
processes right now for each node in the
graph so if I make a graph that's got a
hundred nodes in it and then I
instantiated as a co-op I may end up
with 800 processes just like that
automatically wired up and the way it
wires them up is it each process has a
message loop and it can only send to
downstream pits and the downstream kids
are one of the arguments in the message
loop so i'm not using registered names
which is another synchronization point
and if i have 10,000 processes i'm not
going to come up with ten thousand names
so it's going to be yeah it's difficult
to use some of the techniques that we're
using now and having that automatically
created for me when I inject data it
just finds its way through the network
based on the functions that I've written
ahead of time so I write a module of
functions map those to the graph the
graph populates with processes in each
of the processes are running a specific
function task and the the purposes
receive data manipulated in some way and
pass it on downstream or filter it and
eliminate it so that there is nothing
downstream you can also have side
effects by sending messages out to other
coops or to other pins or two other OTP
constructs and in addition flow the data
downstream
well so if you do a fan out and a fan in
it's like a MapReduce but you could also
reflect data out from the inner layer to
other processes that might be monitoring
the graph and then you glue these
networks together remember i'm using a
cyclic graphs that are directional the
data flows one way but lots of times you
need the data to come back around and
get cleaned again or whatever so you
create a graph and then you create
another graph and then you use whatever
OTP ad-hoc constructs or whatever you
need to wire those graphs together and
build your system out of the subsystem
components and so that's sort of why a
service maps to a co-op or a collection
of coops and then you could turn on and
turn off services and change the data
flow through the through the network
each path through the network is unique
because it has to flow through a
function and get to another you could
get to the same location in a process
through two different paths and if they
did arrive at the same process that
means the patterns that took you to that
path led you to the same state
computationally and so you're now in the
same state you may have just gotten
there in different ways but if a path is
truly unique then you would have
separate processes for every path and
you wouldn't have two processes feed
into one process you were to have a tree
structure but if you need to you can
make graph structures not just tree
structures right now I'm basically just
focused on pipeline and fan out and fan
in because with those you can compose
pretty much any graphs that does a lot
of stuff it right now the hottest thing
everybody's writing pooling constructs
for many reasons unbridled concurrency
or allocating resources whatever but if
you have a graph that has a single entry
point and a fan out that's a pool and
built in the early SP library the
fan-out structure
have round robin random or broadcast
type declarative labels on the
construction of the fan out so you
automatically can get round robin
pooling just by declaring a graph as a
fan out with the round robin now the
trade-off is that we're taking mutable
state which was inside one gen server
and we're converting that to a network
of processes where the state is implicit
in the structure of the network and
we're using messaging to get the data
from one state to another state to
another state so whereas before I might
call functioning then function be than
function see that would have them very
quickly because compiled code executing
in a compositional way is very fast now
I'm executing function a and sending a
message and executing function B and
sending a message so you're going to get
latency because we've introduced the
message passing but if I send a thousand
data items into the network and they
flow across a hundred different paths
every path only has 10 data items so now
I'm getting full concurrency and I'm
doing more things simultaneously add
much finer granularity and I'm also not
having to store state for all 1000
instances so potentially my state memory
size can be less less it all depends on
how much you can compress the data by
knowing implicitly if I'm in a
particular state I don't need to carry
around the data that I normally would
pattern match against to determine what
state I'm in I just know implicitly if
I'm in this process I'm in this state
and only this function applies now so it
is a trade-off and whether it benefits a
particular application or not is really
going to be something to be seen that's
why I say this is experiments in data
flow so here's a simple example of
trying to explain how you could trade
state
for a graph suppose you just have a gin
server that counts and tries to send a
message to a subscriber three times and
if it fails after the third time then it
doesn't try anymore normally you would
have the gens over and you'd have a
counter and you'd increment the counter
every time you send instead you can make
a pipeline of processes where each of
them sends to the subscriber the
function could be exactly the same in
all the cases if it makes it through the
first one and it isn't act by the
subscriber then the second function
takes a shot at it and then the third
process takes a shot at it and then the
receiver of the third process is the
failure process so now it's implicit in
which process is actually executing the
function as to how many times has been
retried and whether failures should
occur there is no state it's just the
arrival of the message means that it's
now at the stage where it needs to
compute whatever that state implies to
make it sensible you would have a much
greater computation than just
incrementing a counter but you get the
idea that you're trading three state at
three valued state data structure for
three processes that are linked into a
graph and the presence on the graph
indicates what state you're in so
programming with Erlang SP is really the
art of disassembling and distributing
state if you're used to writing gin
servers you have to look at the state
structures you have and say how can I
take this apart and how can I map that
data to a graph or structural
representation that is equivalent so
it's a tautological thing and if you if
you look at like parsing code you end up
with a tree right if you look at scoping
of variables it's
stood trees or property lists where
you've pushed onto the front I mean if
you do HTTP header parsing and then body
parsing it's a pipeline you know if you
see certain things then you want to be
doing other things at later stages in
the pipeline so being able to take what
normally is a procedural functional
composition and turn it into a graph
that represents every possible state
simultaneously and uses routing to send
the data to the state that represents
the computational progress that's been
made so far and that's selecting the
network patterns that describe the
problem space so they this is sort of
the key chart of what Erlang SP is
trying to achieve and what your brain
has to do to take advantage of the
library and you need to do functional
decomposition really like boil
everything down to the smallest possible
function like if in the in the example
of pooling where I'm where I have a fan
out and I want to route if I was doing
pattern based routing so I might have
three processes you know one works on
attachments and one works on text
messages and one works on images you
know the the head root function might
just look at the data pattern as a
function with three alternative heads
and if it's if it's an image it routes
it to one process and all it does is
forward the message and if it's a text
it forwards it to the other process so
that that function is just a function
head and a pit selector sending a
message and it does nothing else and as
we found out earlier it generates no
garbage so the garbage collector will
never run on that thing it will run
really fast it'll always be there you
know it it's not going to crash it's the
one thing that we are doing though
because we're making this lattice of
processes or this this network of
processes
we aren't taking an approach where a
supervisor sees one process goes down
and tries to replace it like the hole in
the middle of the graph would probably
be disastrous because it's part of the
state space and we may have lost some
partial computation so a graph is linked
completely if one node goes down the
hole co-op is going to go down the hole
graphs going to go down and it'll just
pull the plug on that one but if I have
a template and I've replicated at 10
times and one of them gets pulled out
the service is still up it's just
degraded and we can replace the whole
service you know the whole graph of
portion of the service so it's a it's a
loser coupling in but at a service level
below the service level it's a much
tighter coupling this is a philosophical
thing in OTP that's that's implied and
nobody really talks about and that's OTP
gives you constructs with with the
registry of naming and supervision trees
and things like that where you don't
need to know the pit that you're talking
to you just need to be able to find the
pit you're talking to so lots of times
you're searching in the name registry or
you're looking through other things that
you might know about and getting a
handle and sending to that we're OTW to
it and the reason they do that is loose
coupling if if the pit crashes they
could spawn a new one and put it back
into the registry under the same name
and you would never know the difference
and your code goes by the name so it's
it's loosely coupled at the process
level actually in OTP and and it's
reflected and the way people write code
they write code that uses the registry
and uses these constructs and does
things one process at a time like
supervision replacement and so forth and
instead here the the state is
represented in the connections so the
connections themselves are extremely
important they can't just come and go
and have be taken away and replaced
without you knowing
happening so I've wired them in so that
they work as a unit and if any piece of
it fails the whole unit will get taken
out and it's not contributing errors to
the rest of the system and that's all
done by actually using the pit inside
the message loop arguments and so you
lose that loose coupling that name
servers and so on and give you but but
you also lose the synchronization and
the serial points that those introduce
the registry introduces a serialization
point that can slow things down and when
we're talking millions and billions you
want to discourage the use of things
like name registries and stuff yeah at
the service level you do need you still
need dynamic and loosely coupled
behavior but you want to do it at
aggregate message flow volume not at
single message or single process volume
so I also say it's OTP compliant what
does that mean that means that the
processes respond to system messages
they can be supervised they reply to
exit messages properly and rel tool can
ask for you know get modules so it can
do upgrades and downgrades and things
like that you want it to be compatible
with all the OTP tools so we can just
slip it in it's no different than OTP
and maybe even migrate some of the OTP
constructs to use some of these
constructs so here's an example just to
show you what OTP compliance looks like
for people that haven't tried to do a
spawn of a proc live and make it talk to
a gen server this is I actually grabbed
this from my co-op head on the in a coop
there's actually a control channel and a
data Channel and each of those are
represented by a process and then they
handoff to a root process which talks to
the root node of the graph and then
things spread out from there and there's
also a separate logging trace reflection
each of those are separate processes so
out of band of the data flow going down
the graph you can reflect
trace data or to the browser so you
could draw graphs of what's happening
live at the same time that the data is
flowing through that's why every node in
the graph has like eight processes and
also because if I've got 10,000 cores I
need to waste the processes as best I
can just throw as many processes away as
you can that's a sign of wealth and you
you should burn whatever you have the
most of if it buys you benefit in other
parts like simplifying the code now my
function only has three clauses and no
body and I'm done it's easy to test so
here I'm receiving in the message loop I
have no state this is this is a case
where I've created a co-op head and I
haven't initialized it with a state it's
not ready to do any work yet because
hasn't been started so it's got no state
it's got its root kid and it's got its
debug options which is the OTP feature
for for using debug and trace if it gets
an exit message at exits with reason
that's so that supervisors can supervise
it if the parent dies it reflects the
same reason if it gets any system
messages and that's debugging and
tracing and those things and if it gets
the call from rel tool it can say this
is the module that I used when I started
up so if that module is changing in the
upgrade change it out for me and then
these are my internal control messages
get handled by this clause so when you
first start it up it can only handle
system messages and control messages and
that's it and the main control message
there just one initializing the state
where I actually replace this new state
and now it jumps to another message loop
where there is a state and it does the
same stuff and then all the things that
it can do once it started as a service
so this is a trick that's in OTP a lot
and it it's in our lying ass P a lot to
you can have to message loops that are
mutually recursive so when you're when
you're not debugging or hibernates a
good example if you have a gin server
that's not hibernating it's got a big
message loop and it can handle all this
messages and then you return from one of
the calls and you say hibernate and it's
supposed to shrink its memory down and
do all this stuff and then it jumps to
another message loop which is much
dollar that only handles messages that
are valid in the hibernate state when
you're in the hibernate state you can
only reply to certain messages and then
one of them is to unhi burnet and so
then you expand to the other full
message loop so you just bounce back and
forth between these two message look
it's a way of changing state and it's
and like I'm representing a graph of
states and the processes are States and
they're doing message loops based on the
data that's coming to them it's a
natural to have this kind of recursion
that jumps between message loops so I
did a talk at Vancouver refractory light
you can look it up it goes into a lot of
depth about how OTP compliance works
with proc live in spawning and the
process dictionary and all this stuff
look that up and you see so a little bit
of details I've added something called
an ESP service which is a behavior
that's the services where you can start
and stop and suspend and then I have a
TCP service and that's a fan out for
doing a concurrent acceptor with with
asynchronous I net acceptance and then
as an example i implemented EPMD which
is the the port mapper demon based on
the Earl PMD there's an open source
github so they did it in a way that uses
state in ETS tables and I did it using
Erlang SP eliminating the ETS table to
show how that might compare so in the
provided behavior for ESP servers you
can create a new service and give it a
receiver so a service just like a graph
a co-op graph has a receiver at the
other end that can accept the data after
it's been processed and that can be
another service another co-op a
particular co op node it can be a bid it
can be about five or six different
things so that's how you glue together
disparate services using sort of ad hoc
code you can start it up and stop it so
when you create one it doesn't start
automatically and that way we can
dynamically create a bunch of them ahead
of time wait for some traffic and then
turn them on as needed or turn them off
as needed and there's also you could you
can create a co-op from a graph and then
turn that co-op into a service and
that's kind of another way to upgrade a
static graph into a service that you can
turn on and control you can link a
service because we might want to be
doing supervision or if this service
goes down take down the database
connection to because security has been
violated or whatever so you link to
something else that you want to come
down because there's inconsistency if
one piece of the sub system isn't
working you can get the status suspend
and resume and set overloads so that you
can route data based on weather services
are overloaded or not so you could
bypass overloaded areas and what you
could have is like to alternate services
one in the normal mode which does a lot
of work and then in the overloaded
situation you have a shrunken down
streamlined service and so when the main
service gets overloaded you just change
the router to switch to the other
service until things aren't overloaded
any more than you could switch back to
full processing so the idea is to have a
dynamic adaptability for for higher
throughput and just have idled processes
waiting in case you need them and act on
is is how you pass data to a service you
give it some as they act on this data so
a TCP service i'm using the async except
kind of the standard undocumented pre my
net approach that everybody uses and
ranch and swarm or two examples of
projects that due to similar thing it's
a fan-out graph there's the listen
socket so you do a listen and then you
create a hundred except ters i just
spawn a process for every acceptor but i
create a graph ahead of time that's a
fan out with however many accept errs
you want and then populate it with co-op
processes and then you just write the
function
code that handles the accept and does
the client module calls so all the other
management and administration stuff is
all there you just have to supply the
functions that go inside the graph nodes
and one thing about this one is I allow
the client module to be changed so after
you launch the acceptor you could launch
a hundred except ters and then say you
know what the system is overloaded I'm
going to switch to a different client
module when I get a connection you can
just send a message to all the acceptor
and because it's a fan out in broadcast
mode all you have to do is send a
message to the listen socket which is
the head of the fan out to say change
the client module to module B and it
broadcasts it to all the acceptor 'he's
automatically and so you know one line
poop everybody knows changes their
internal state concurrently and the next
accepted socket will be using the new
module and because it's a service that
everything's linked so it'll all come
down a few which is something you may
have to work around in your application
if you if you have long lived
connections you don't want them linked
because if one person takes down their
socket in a bad way everybody that's
connected goes down so what I do in the
EPMD module is actually on connect I
migrate the co op node to another graph
so it's sitting there waiting until
there's a connection and then it goes to
another graph and gets managed by that
graph so and then I slab allocate so
instead of one process goes away i
replace it with one process I might have
a thousand of them and wait till a
hundred of them go away then I'll put a
hundred processes back so here's a
diagram of how TCP serves you when you
start it it does a crate socket listen
and that then broadcasts to all the
children in the fan out to start
accepting socket connections and when a
client connects they get one of these
just based on how pure my networks
they get one of the processes and it
executes the client module receive that
you've implemented and in the case of
EPMD this function calls co-op migrate
to another pan out so that I take it out
of this and it doesn't affect any other
connecting except ters so the broadcast
is it means that you can manage you know
thousands of them without any extra
infrastructure it's all just there I
have no downstream receivers you see
these these accepts don't have arrows
connecting to another process it just
ends its expects to end there but in
your client module you could receive the
data and send it to another process and
then that may be the process and you
could actually transfer the socket if
it's on the same vm node and I don't
transfer the socket I try to keep it in
the original process just to cut down on
the work that's done so yeah there's no
socket transfer you can use side effects
in the client module and then on
completion the acceptor process so go
away and do the slab allocation so now
the EPMD demon with EPMD does is
whenever you start an erlang node with a
name it contacts the local socket and
there should be a demon running there
listening for that socket call and it
says hey I'm just starting up an early
node and my name is X and I'm accepting
distributed calls on this socket and so
the local demon maintains a registry of
all the known nodes that share the same
cookie and what socket they're listening
on and it comes with a base distribution
is written in C that's how distributed
Erlang works you can query the port
mapper demon and find out the list of
nodes and what sockets they're connected
on so you know the normal approach you
have Erlang's that connect to the demon
the demon has a registry database and
then clients query the demon
and get information from the registry
and Earl PMD works that way the typical
and then this is kind of how the c1
works but the Earl PMD they've got a
connection listener pool then the ETS
table so whenever you're all nodes
connect it inserts an entry in the HS
table and then when somebody queries it
it does a query against the ETS table it
might do a scan it depends on the query
whether it can go by the index or
whether it has to scan the table but in
the services platform style I use an ESP
TCP service the fan out that we saw
earlier and then the query service is
another fan out and I started out with
no children so it's a fan out that has a
head and no children I start that up and
then you start up the TCP service and if
a query arrives as a connection then I
just reply to the query nobody's
connected right now if if a connection
arrives then the accept process migrates
to the query service so now the query
service has one child which is one live
erling node and if a second one connects
that migrates over there too so now it's
got two children and if I then receive a
query I send a message to the query
service which is a broadcast fan out so
it tells all the connected processes hey
there querying for which socket is
connected with the name foo so if i had
a hundred connected there would be a
hundred processes and in parallel it
would automatically broadcast to all of
them which one of you is named foo when
what socket are you connected on and
most of them would say not me and throw
away the message and not reply and the
one that is foo would reply and i have
no ets table and no state other than the
name and the process ID for my process
so I've taken the ETS table state and
turned it into a graph of live
connections and if somebody disconnects
even if they didn't intend to it's
immediately
disconnected because the process died
for the connection and the queries will
reply with a consistent result whereas
in the ETS case I would have to get the
message that they went down and update
the ETS table before my queries would
respond with a consistent result so yeah
and if somebody just does a query then I
reply and die and eventually more
acceptor scum so there's always acceptor
is just waiting for connections and I
can respond to them quickly and
concurrently and handle lots of queries
without any extra machinery and route my
queries to the service fan out which as
I described earlier yeah so the query
reply now let's say I have 10 or lang
nodes connected on my query service well
that's an asynchronous reply right I I
send a message to the query service and
it sends it to 10 processes which for
whatever reason they may not all reply
at the same speed and if I get a whole
series of queries you know three or four
queries in a row I'll get three or four
messages and they may not flow through
that network at the same speed so I have
to be able to bundle the results of a
query together because the query might
be tell me all nodes that are connected
and so I need a response from every
connected note so the query response
actually spawns a process which sends
the message to the query service and
waits for the reply back in the spawned
process so so the query arrived on my
accept connect and it becomes the
collector that sends the message and
waits for the replies back and by doing
that I can put a timeout on the wait for
collecting and if only eight of them
respond instead of all ten
I can reply with all that data and die
and if the other two responds later the
process is gone and so the message
delivery just disappears so we get a
live connection and we also get a result
based on the liveness of the live
connections and the database query is
handled independently and concurrently
if I get five separate requests then I
have five separate processes waiting for
results and the data is flowing sort of
in March step through my fan out but in
Erlang there's no real timing way to
keep them in March step so they don't
have to be a marched up the receiver is
given it spit to them for the query
response to come back and so each
connected Erlang node is going to reply
to the pit that requested the query and
so I can have several outstanding
requests and wait a little while before
I get a moment and then after the
response to the collector dies so this
is sort of how the query collection you
know the collector sense to the quarry
service it broadcasts maybe this one
gets late the other two come back to the
collector it sends a result this dies
and these end up getting dropped so the
contribution that our Lang SP provided
is that we're trading internal state in
this EPMD example the ets table for the
process graph the database of
connections is actually a graph of
processes the the query service is
routing to all of them and collecting
back as if it was a database query and
the query occurs in parallel naturally
it in eliminate the need for any mutable
state update especially in this case
because you connect and your information
about your name and your socket doesn't
change and it provides common library
patterns that reduce the code to
implement the logic and the full OTP set
can be used now suppose I do a query and
I get a list back of six different or
leg nodes that are connected I can query
that graph using OTP tools using
race and debug and things like that and
get more information if those were gen
servers that had more state and we're
doing more things I could get the status
of the state and do any of the OTP kind
of internal probing things that you can
do with Jen struck constructs so when
services platform provides this higher
level concurrency pattern I want to be
able to create patterns that are
hundreds and thousands of processes at a
whim you know just stamp them out and if
I get overloaded make copies of it and
give me 10 so that I've got a lot more
throughput it avoids state-based single
server models and tries to map what
previously was data structure state into
a graph of concurrency that allows data
to flow and use as many cores as
possible and we can integrate with all
the existing OTP stuff and it supports
incremental migration of a system to a
many core cpu we're right now it might
be running on 16 cores or something so
these are features that will be in
version 0.1 I think it's zero point zero
point two right now up on the github but
I'm working on it in my repository
jaynell but duel mark if you look at
duel mark you'll see or like SP and
you'll see my contributor name so if you
want to see the latest bleeding stuff
that's not into the release you can look
in my local repository and if you want
to contribute you know read through it
and see if there's anything interesting
there yep
well it will so every process in the
ground every node in the graph means
that you arrived there by going through
some path so you are in a particular
state when you arrive at that process
and because you as the architect
designed the graph then you've made
implicit assumptions about what
functions will apply in that state and
so that process will only contain those
functions that have the behavior that
are meaningful in that state well let's
say like like chess is a good example
you know when they were doing chest
there were these huge search trees and
they said its enormous it would take all
the time in the universe to search I
mean but if you took every possible
league legal board position there's only
four thousand and so you represent a
graph with four thousand possible legal
board positions and if the the edges are
moves that take you from a legal board
position to another legal board position
you're now in all the possible states
that a chess game could ever be in and
so you can have functions that apply to
states where there are no rooks left or
you know so you're what you're doing is
is a transformation of the problem space
from one data structure type to a flow
graph representation of all the possible
problems states that you could arrive at
and that way thousands of instances can
all be processed in parallel without
dynamically changing what you've set up
ahead of time
yeah so you I do that with glue outside
of the service and and there are a
couple cheats in there like if you in
the when you create a co-op you can pass
in an options flag that says well let me
see the head of the coop that i'm a
member of because i might need to tell
it to like my great me to another co-op
so there are some little cheats and
holes but in general you want it you
want things to flow one direction so
that they're very orderly if you
introduce cycles like because i was
implementing it for the first time
cyclic could introduce all kinds of
complexity so I just eliminated it and
said I'll do it outside the co-op's but
you get a more orderly flow if you know
it's directed and can go in a certain
way only you can make assumptions about
how you build your graphs yeah so so I
have a co-op instance but I can turn
that into it remember there was a
template and I stamped out one I can
turn that into a hundred instances of a
single node fan out and those can be put
into an ETS table with read concurrency
or a process dictionary and then do the
routing that way so you know I can I can
stick a fan out in front of that that
round robins to those and then it peels
off from there but there is a there is a
serialization point where you need to
have if you're going to do high volume
you need to have like an ETS table with
Rican currency or so that you can have
many processes delivering data in and
having them hit different processes
rather than going through one process
because you'll end up with the same gen
server issue
right right yeah I you know with a
10,000 court system if you've got enough
memory you probably have a billion
processes so I don't even worry about it
like you know what ten years ago we told
the job of people we have four orders of
magnitude more processes than you do
nananana you know you're using the wrong
approach and you would think differently
if you had that and now you can say i'm
using four orders of magnitude processes
more than you Erlang programmers are
using now nan and I could do things that
you can't do and I can do really really
stupid things that seem really stupid to
you but on my system it may turn out to
be really faster even though it's using
a lot of resource it may have much
higher throughput and that's also why i
said experiments I mean it may turn out
that it just uses too much memory and
there's too much latency and it's only
useful in very particular situation it
seems to work well with the TCP service
because it's just the natural with
there's with the async acceptor or set
up any other questions oh no no time
left okay sorry yeah
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>