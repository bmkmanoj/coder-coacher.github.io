<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days - Alex Petrov - What reading 5 scientific papers can do for your business | Coder Coacher - Coaching Coders</title><meta content="Lambda Days - Alex Petrov - What reading 5 scientific papers can do for your business - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lambda Days - Alex Petrov - What reading 5 scientific papers can do for your business</b></h2><h5 class="post__date">2014-04-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ktc5Uo8NS0E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I work for code centric that's a company
based in Germany we do lots of Java lots
of consultancy lots of big data lots of
different kinds of stuff and I also do
open source work and mostly it's all
enclosure and I do it they close your
bags who knows what that is so who can
say what what exactly that is that's the
meaning of life universe and everything
but the problem is that it's an answer
to the question of life universe and
everything but what's even the question
and it seems that somehow what we have
in an analytics is exactly the answer or
like the 42 we have the 42 but we don't
know the question and number somehow
evolved around percentiles clustering
revolves around words and Sam filters
well if you take or open up like four
books available on o'reilly which are
mentioned in clustering and basil that
way all of them are going to explain you
how to use while clustering together
with like tf-idf so it's basically word
house and whenever you read about
regressions they are all about how to
predict the page views and no one even
mentions the combinations for mutations
of the things or pivot tables and so on
and basically that's a general schema of
how the data science is being done so we
have something I'm not allowed to really
name it because I'm being videotaped and
then we have our like the programming
language of the toys and return
money and I would like to say what
reading five papers can yield for your
business and all these papers have been
written well some of them like 20 years
ago some of them just several years ago
but still they give you some idea about
the data that you can give in a with a
very small memory footprint and it's
going to be about mostly minimal to
toolchain tuyo some information how to
get the fields like groups and
permutations how to get count for
categorical values and distributions for
number poets and we in order to do like
some data science not according to the
image that you have seen just now you
should have a good impression of the
data you should understand what it looks
like but somehow whenever you start
reading the books everybody and explain
to you how to process the date or how to
already crunch it but no one explains
you how to understand what even to
crunch so you should know how many you
should be able to ask your software did
you see that one like did we already
have the user with such an idea or by
such an IP what is the distribution of
the nominal value and how different are
two factors or well to value face and
the last one is like does it belong here
so I have well as sacks of round red
ball like a red balls and I have well I
square and I can ask myself or whether
my square belongs to the bolts so
getting the info and his wife's we talk
if I i think was XE guys I always held
measure everything so okay no
president let's just say that you can
efficiently measure locally and merch on
the seven sheets and well person tiles
are cool and everything but they're cool
mostly form agency but there are many
more things that I consider more
important than latency that you can /
should measure in the application and
business and same approaches I'm going
to be mentioning our use in the
databases in high frequency trading
trading software in how scale data
analysis so it should work pretty much
for anyone pokes heard about OLAP okay
that's cool and every time you mentioned
OLAP in the conversation all the
hipsters step oh wow like it's uncool
I'm higher than that where I'm better
than that but the thing is that the
principles of the all out there um well
eternal kind of in a way and one of the
first principles of all app is the
consolidation and in order to
consolidate by time you can use for
instance tumbling windows the tumbling
window for me is kind of like a box when
we discuss it with the several people we
came to conclusion that it looks like
Mona but just because the word itself
represents like too many things at once
for different people I decided to remove
it from the site so for instance we have
a stream of integers and the first
integer we see is one so when whenever
we open our tumbling window we put one
as an identification or identifiers
sorella and we wait on
the difference between the number which
is in the box and the number which is
coming in the stream of data it's going
to be 5 or larger than five and then we
close the window and move on so we'll
get to as an x value which is coming in
the stream and one is in the buffer to
is an x value 2-1 as one still less than
pi so we add 2 to the buffer next I'll
use 3 3 minus 1 is 2 still less than
five then four five and eventually we
get 6 6-1 is 5 so exactly it's the time
to admit so what we do right now instead
of one we put six in the box and we'll
start coming from the six and we're
emitting all the values which we
aggregated until that time or down the
stream so that's kind of a tumbling
window grunts and well it goes on and on
so we get in your buffer of only six
then of right seven a xanax and common
value and accumulate until we meet the
next value whose difference with six
will be larger than five the next way to
group by time as a sliding window so
sliding window also looks kind of like a
box and for instance we have a value of
one which is coming into the stream and
we start well computing until or start
accepting values into the buffer until
we get a difference witcher's larger
than fine so 2 minus 1 is 1 and it's
less than five and we start accumulating
2 3 4 5 and we get 6 and since we bought
six we start dropping the values so we
immediately you values from one to five
that we got so far and then we drop one
we don't flash an entire
so we drop one and append six and seven
we drop two and append seven instead and
that's a window that slides so we don't
draw all the values each time but we
drop a single value and append the next
value and that is very useful to see
last and values etc so grouping could
also look like that so you have a gender
height weight and number of appearances
and you can say that okay we have two
genders male and female and we have
distribution of heights ok we have a man
of the height of 170 centimeters and
there are 15 occurrences of the ways of
73 kilos like 28 of book 75.5 and so on
so that's another way to group and the
joy name is basically a cartesian
product of well two tables and thing is
that whenever like you are using the
grouping you can use while some standard
aggregates for instance you can use
count as an aggregate or you can use
some as an end date you can yield like
standard deviation or medium me all the
percentiles and so on but sometimes all
these things don't yield enough useful
information and in these cases you can
use things like company in ch come in
sketches well a proximity of data
structure which is answering the
question how many for instance you have
a stream of data which is consistent
from the several several different
colors and you should get to know how
many appearances of each color there
were in your stream and the problem is
that whenever you have like millions of
IP addresses which are hitting your
server and you want to know how many
requests the work or the IP
dress in majority of cases you have to
maintain the list of all the IP
addresses that you have ever seen you
can imagine how extensive that would be
right you can keep your Cassandra
cluster or whatever the database of your
preference just to store all the key
addresses with the numbers right or you
have the object allocations for instance
in your Java or scholar program and you
want to count the number of objects
which were allocated so it is very
efficient for the large amount of yeast
or like very large amount piece and it
gives you the approximation of the
minimum amount of the appearances of the
given key in the wall so far in order to
use hung in sketchy or using the anon
cryptographic hash function for instance
murmurs rehash is while sitting very
well and we need n linear hashing
functions for to implement the algorithm
so we take a random seed so it's
basically calling well next random
engine and the update looks pretty much
like that so basically we have K rows
and while m is the width of the table
and we have in the end k rollo que seats
for the hashing function and we start
updating by while finding the hash from
the incoming value and for each of the
seeds of the cash we find the value
which is matching like the vertical or
me the horizontal location and
incremented by one and whenever we need
to look up the value we just well pretty
much do the same thing and find the
minimum of the things that while were
the arrows point and that gives us an
information about the how many times
please the item has been seen in the
stream of data so in order to calculate
well this this is useful you may say but
the problem is that in order to look
something up you have always to know the
key because technically count mean
sketch doesn't hold you key with the
amount of the appearances but rather it
kind of distributes the key along all
the well Rose that you have by the hash
functions and therefore it is only
useful to get to know whenever you have
a key how many times have we seen it so
far but that could be used to calculate
the top k so so called for instance you
have like many many IP addresses which
are heating your machines and you would
like to know which ones are the heavy
hitters for you are selling the product
and on the each ink or on each sale you
can increment the counter and that food
or cupcake could give you the list of
the top selling products the same way so
in order to use the top que algo you
should just hold a conference hatch in
memory and do the updates on each event
occurrence whatever we have just seen
previously on design and do a lookup on
the key in the comments hatch and add
looked up value to the top list if the
topless overflows so it holds more
values then while you have the slots
available you well because still is the
sword and we just dropped small value
and continue the aggregation and this
way you will always be able to see Ali
well for instance five heavy hitters
there is also a certain histogram which
is very useful to understand like the
distribution of your a time
the algorithm or the update algorithm is
well but well described in the paper I
probably won't really go through it
because i only have 30 minutes instead
of I thought I'm 45 so we kind of
skipped fee I mean you can always look
it up in the papers anyways well better
this happen on the slide but the general
concept is that you have the number or
double numbers which are coming in as a
stream and you would like to know okay
what is the distribution like hell how
the values are distributed and whenever
you append or if you get a new value you
added like to the very end for instance
we we got a value which was larger than
every one of the ones that we have seen
so far so we add the 12 well as a very
last one and after that if we have more
bins that but then we have planned so
the vertical thanks ru bins we find the
smallest bin so the narrow is thin and
merge it with the next one and well
there is a formula to calculate how
exactly you merge them so basically well
you get a sum of two bins and this way
you can maintain the histogram of the
values like how exactly they are student
and one of the examples I have taken the
database of the distribution of weight
in Heights and shoe size of the people
on omegle Athena and that is how like
what it looks like for instance that's
the distribution of height so there were
50 people whose height is in between 0
and 1 76 and there are welcome around 45
people who's tied up it is like around
167 and well etc etc so it's basically
sorted by the
creasing amount of occurrences so the
East amount of the appearing systems you
can see is of the people of the round
189 17 years I and while the largest is
176 um the same thing with the other
wait so the largest amount of the people
which have been in the database I've
taking work of around seventy three
kilograms so it's like 45 50 people or
well a little bit more than forty seven
people and so on so the histogram is
initially written for streaming decision
trees it will give you the exact stats
and it's very lightweight since you
always keep only the end bins and it's
extremely useful when you don't know the
value balance for instance like with the
weight and height we can kind of guess
like how tall the people can get or how
fat they could be technically but there
are cases when we can't really know the
values for instance like whenever we are
speaking about latency or like even
blames is predictable but if we're
talking about the prices for instance
it's very hard to say like which town
should we take for our histograms for
instance if we are selling computers it
may be just like us one range of numbers
if we're talking about human organs it's
it completely different day so hyper log
log is another thing i'm not going to go
into the details but you may know that
it may help you with a cardinality
estimation so you will be able to count
the amount of distinct items in the
stream of data for instance and it's an
approximation structure but you will
always know the error in
the ability to count the things so
basically you are able to get the list
of the top hitters or heavy hitters that
you are able to get the histogram or the
distributions of the number number of
health values and then you're able to
get well the histogram or the
distributions of the categorical values
but now you may want to group your data
in some way which you just don't know
yet so you basically want to cluster and
in order to do that you probably want to
take your database or the logs that you
have whatever the data you have process
kind of offline and take something like
a Cannes algorithm which will help you
to understand okay which groups is my
date or can my data be separated and
canes works pretty much like that so you
have several points and you define an
amount k of the clusters you want to
just report on you want to separate your
data into and while the three colored
dots are one yellow one kind of bluish
and third one is orange / red are here
and so if we basically places randomly
and then we start optimizing or so we
split all the dots depending on how
close they are to well be randomly
chosen point and then we optimize until
we know that okay we have the nemo or
that the dots are located or are
actually the minimum distance from the
points and we can convert or we can't
make it more optimal anymore so it's
like the best distribution we can get
for this amount of ones and the problem
with k-means the one is that I've
or seeing is that the majority of pieces
people are describing it not with the
vectors of values would rather they are
well they almost never operate on double
values rather they always operate on key
value hashes kind of and which represent
the term frequencies for instance they
say okay we can do the like filter or
cluster the like books to like different
subjects or whatever else so I would say
that it's way more useful if you use a
canyons implementation that can work
with the double vectors because this way
you can represent any gauges app that
you can possibly imagine as a vector of
doubles and then cluster these doubles
and well get the pretty pictures which
will tell you the story that you have
known until date and the last thing I'd
like to mention is the naive bias which
is kind of for me working together with
the k-means algorithm and you can use
the Beijing algorithm with the
observation that you yo get through the
k-means in order to identify the weather
the new data points that are coming to
your system are belonging to one of the
clusters that you have identified and
well once again make sure that your
patient algorithm operates on the vector
of vector vectors of doubles and while
it basically takes the modal which is
variance in the cluster me and finds the
cluster which is well closest to the
observation okay
yeah I probably actually do that now I
don't want to bore you to death actually
the thing is that I've mentioned the
algorithm that can help you counting
things in the predictably large space
I've mentioned something that can help
you to build histograms of the data also
with the fixed space and in my opinion
with just these two things you can
already gather such an amount of
information that well at least for me
unimaginable before I unfortunately
cannot really mention what we are what
exactly we're using all these things for
but only thing I can say is that it's
never been possible for us to measure so
many things at once for instance
databases like elastic search or
Cassandra they're using well they're
using also bloom filters but while
swimming histogram are also used in
Cassandra and I know that while some of
people use in a database world also the
conference catches and well people like
Google are using them to identify what
we have a hitters and so on so if you
use these approaches you can count and
unimaginably write a large amount of
things and the more you count and the
more while patterns you're trying to
find for instance you would like to know
how many things were or how often the
latency and on my obligation is getting
what larger than a particular value and
before that you should have like logged
your thing to this at sea or whatever
written it to HDFS and so on but like
what you can do with these things you
can just maintain a very lightweight
data structure and you can know or be
absolutely sure that that's been a
minimum of the certain amount of
appearances in the extreme and you can
maintain in the list for well quite
alone
of time and then well-heeled the results
which would have been much more
difficult for you to heal beforehand and
you don't have to have a Hadoop cluster
you don't have to set up anything all
these things are implementable maybe 20
up to 40 minutes of your time and
actually that's very interesting too so
I would say that having an idea about
the algorithms which help you to heal
the info from the days i think the
predictable space is a very promising
thing and it's going to get bigger and
bigger like every day and I highly
advise you to check out all those papers
that I mentioned today and that's it
yeah we have time for persons that's
good yeah just if I thought one is time
use the correct name is using a cm sorry
comments heck yeah it's not like a blue
book which you can count the occurrences
deep Avenue we get like you know how
many girls do that was ok let's that's
really probably I should have taken even
more time for the question or seemed
pretty useful ok so basically for
instance you have an IP address IP
address in this given example is an eye
and a by the like vertical axis the key
like every line or row is um like to
think of it as a hash function well more
were three hash with a random seed right
so basically for instance you have the
IP address of one 1 point 1 point 1
point 1 somehow ok and you take the hash
it's like 3344 and the down is
20 you take modulo of 344 modular clean
and you get for instance number three so
and you increment by 1 to 3rd so first
and third so first line third column and
that's it going to be a minimum of okay
then you do it for each of the hashes
and whenever you do the lookup you find
the minimum of of these things and it's
going to be the minimum amount of times
that this item have been seen in the
stream so most likely the the item have
been seen more times but if that that is
the least number that is possible to
predict it's basically a streaming
histograms for or if this is called
serum a decision trees but it's whenever
I upload slides and make sure to mention
all of them and i will just a tweet
usually you start with some small number
well at least if you know that among the
categories is gonna be well relatively
small like five for instance and if you
see that it's too much then you decrease
it if it's you see it's too little you
increase it but as I mentioned the thing
is that the way at least we think trying
to do that is we never were ending up
with like
gigabytes or 40 terabytes of data that
we have to process so in majority of
cases we just had to query our like
lightweight data structures and used
some reasonable data from them rather
than will have a full Hadoop cluster
just well crunching the numbers and well
then generating the k-means and well we
have to wait for one week just to get
even the basic results so the more like
great things to use the more you can
experiment and the more freedom you have
the less you should wait in the more
creativity you can have in your like
data analysis process but like all the
data should be on top of your fingers it
should take like less than a second for
you to get pretty much any responses
it's like that's right many people at
their time decided to use iPhones not
android phones because like a grid
phones always have that little lab not
anymore though but you marry the times</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>