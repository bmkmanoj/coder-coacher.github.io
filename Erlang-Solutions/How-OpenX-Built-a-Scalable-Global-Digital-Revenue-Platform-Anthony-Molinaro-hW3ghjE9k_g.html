<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>How OpenX Built a Scalable Global Digital Revenue Platform - Anthony Molinaro | Coder Coacher - Coaching Coders</title><meta content="How OpenX Built a Scalable Global Digital Revenue Platform - Anthony Molinaro - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>How OpenX Built a Scalable Global Digital Revenue Platform - Anthony Molinaro</b></h2><h5 class="post__date">2013-04-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hW3ghjE9k_g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">see ya Anthony Molinaro I'm a senior
staff engineer at open axe and I know
you're supposed to traditionally sort of
start with a joke but I couldn't really
come with one up with one so I just sort
of let our marketing guy come up with a
title of my talk so if I can get it to
pop up there it is so I kind of think
it's sort of silly title but you know so
what our marketing guy decided that it
should be so what am I here to talk
about well I want to talk a little bit
about sort of the history of open axe
what it is what we do and sort of how
Erlang fits into this picture and then a
little bit about sort of how I conspired
to get Erlang used in quite a bit of
open axe so open X actually has a sort
of long lineage it actually started out
in 1998 as a PHP ad server called PHP
ads Rabia was a script or who knows and
it sort of went through various name
changes over time became PHP ads knew
became open ads eventually around 2007
it was rebranded as open X and I sort of
went and looked at the download this is
sort of the downloadable product and
it's actually installed on quite a few
servers according to one of these online
top sites things and the way it was
originally built was pretty much
straightforward to tear app PHP and my
sequel so I've got a little picture and
hopefully everything comes through here
but what we had is you know we had an ad
server we had some way to manage the
data data was sort of kept in my sequel
and I sort of keep little boxes to keep
track of the languages that these
components are written in because a big
part of this is well how did we get
Erlang so in 2008 the company was
incorporated
they hired a CEO who was an employee
Yahoo he left Yahoo he lived in Pasadena
the company had been based in Europe but
he didn't really want to move to Europe
so he moved headquarters to Pasadena and
started hiring a lot of sort of X Yahoo
folks but at this point most of the
development was still being done in
Europe mostly instead of outsourced
there and they started developing
the sort of the vision for the company
changed from like a downloadable product
to hey let's sort of focus on like
enterprise customers
software-as-a-service will run out
servers for them as opposed to them
running them themselves and then in
order to make money we should create a
marketplace where you know advertisers
who maybe don't have a direct
relationship with the publishers running
this ad server have a place to go and
you know bid on inventory you know bid
on eyeballs as they're showing up so the
open X is called the market and
development began since it was still the
same people that had been working on the
ad server it was sort of started in PHP
in addition at this point Hadoop was
introduced because you know once you
weblog has only get you so far and
there's usually a lot of information
you'd like so there's a protocol called
the lightweight event system that is
great for sort of logging information
and a bridge was created to sort of dump
that data onto Hadoop and then Hadoop
was used for data processing so we sort
of added a few new pieces at this point
Postgres came into the picture because
market was using it a little bit Hadoop
was introduced and I missing a box but
who Duke was written in Java so so I
actually started in 2009 a little bit
after the the market sort of officially
launched it launched in April and I
started in June and already in June sort
of the fact that the market was this PHP
thing built on top of my sequel and
Postgres was sort of already sort of
creaking under the traffic that was
being driven onto it a lot of this is
because they had made really poor
choices like well PHP but then in
addition there tended to write things to
the database with every request so
instead of having sort of an externalize
logging system and really using the
lightweight event system they were still
just sort of writing data to you know
Postgres and then you know sometimes two
or three pieces of data for every ad
served so as you can imagine this
doesn't really scale well and it also
sort of required since it was PHP and
Apache it required a lot of machines to
deal with the traffic volumes that we
are starting to see
so at this point there was a couple of
people who are sort of Java experts and
they started taking on sort of pulling
out certain key pieces and rewriting
them in Java and so the market itself
got sort of completely rewritten in Java
and part of the ad server specifically
this kind of add selection piece so the
engine that goes and determines what ads
are gonna see at this point was also
rewritten in Java and a few decisions
were made the main one was that like as
we started to kind of tear apart
monolithic systems and make much more
smaller interconnected or loosely
connected components we decided to we
settled on thrift as an interchange
between these because supported a lot of
languages it provided RPC which some of
the other serialization protocols did
not and you know so these choices were
sort of helped early adoption later on
so we were ended up in 2009 with
something like this I should mention we
also introduced kind of no sequel for
our serve time data so instead of having
everything read and write every request
to systems instead we would do things
where the management tier would sort of
read and write to my sequel and then we
had dumps that came from my sequel and
went into Cassandra on some you know
five minute interval and then the
serving systems just read directly from
Cassandra
so this eliminated one of the big
essentially like read all of the reads
for the most part on the database and so
the management system was more
responsive being able to just do writes
we would do these dumps and then we were
serving system was able to load a lot
faster and in 2009 this was right after
Cassandra had open sourced and we were
basically one of the first people to
actually run Cassandra in production to
the point where the developers were
actually kind of a little shocked when I
mentioned it on the mailing list one
time where I'm like oh yeah we're
running yourself in production and we're
seeing these crashes and they're like
what you're running it in production
really no one's doing that and that was
like well we're doing it so
so by 2010 we sort of had kind of the
basics kinks worked out market was
humming along in Java and it was sort of
time you know once everything stabilizes
it's time to add new features so the two
things that we were looking into the
first off was building a real time
bidding system which real-time bidding
is sort of now sort of the norm in the
ad industry but at that point in time
there was really googling it and I think
we were the second people out there to
actually attempt it and what this is is
that when you go and view an ad on a web
page if the publisher of that web page
hasn't sold that ad directly to an
advertiser then it goes to these sort of
marketplaces and the marketplaces
what they do is usually try to figure
out like well does someone you know care
about the user that's looking at this
page and want to show an ad and if so
how much are they willing to pay and the
publisher sort of says well I'd like to
make this much and if you know the
amount being paid is more than the
amount that the publisher wants for it
then basically you can fill it with that
ad but you know nobody a lot of these
guys don't want to upload all their ads
into your market what they'd like to do
is say well I have my ads and I want to
wait till the last possible second
preferably right when you're showing
this ad I want to decide like whether I
want to pay for this or not whether I
want to bid on it or not so it's where
you get real-time bidding and a
real-time bidding system ends up being
request comes in for an ad and you
basically send out a bunch of
information - you know 10 20 30
different people and say here's all the
information that I know about who's
viewing this ad right now what page
they're viewing it on you know their
geographic information from IP lookups
and things you know how much you willing
to bid on them and some number of them
will come back and then you run an
auction based on the bids that come back
and whoever's got that if the highest
price beats your floor you can kind of
fill it and move on
and all this is happening sort of while
the ad is loading in your page so now
you have a better sense of why ad seems
so slow and in a lot of ways it's you
know we give people I think a hundred
and twenty milliseconds something like
that to sort of give us an answer for
their bid
but this sort of like farming out a
bunch of requests to a bunch of third
party things is actually a really good
use case for Erlang because it's mostly
just sort of like you know keeping track
of a bunch of you know HTTP requests and
doing some parsing of the data that goes
out and comes back in so I sort of used
this as a as a first system written in
Erlang at open ax and I sort of probably
started it maybe like December 2009 I
think we launched it about you know late
January early February because you know
it came together very very quickly and
with a few hiccups in terms the HTTP
client eventually it got to to working
great
the other feature that we wanted to add
was sort of server side frequency
especially for the marketplace where you
end up having a lot of ads and people
don't want ads being shown all the time
so everyone has a rule saying hey this
ad is shown
I want this user to see this ad no more
than five times in an hour and what
you'd like to do is show them yeah five
times in an hour and then after an hour
is expired or some fraction of like show
it again because now you can and at
first we've kept all this information in
cookies but eventually the cookies get
to be a little bit large so it was
decided to sort of move this server side
and around this time react could just
sort of come out and they published a
blog post about react core and a first
developer had been looking into just a
pure C database but I was like hey we
should look at this react core thing
because it seems like Erlang would be
great for you know the orchestration
layer and then we can still build the
Lola low-level database in C so this
happened also since we were sort of
already becoming a little unhappy with
Cassandra we I deployed react for some
subset of data so we could play around
with it and then we also sort of
deployed a monitoring system and so you
know we finally see some of the my green
Erlang color here the frequency server
doesn't come out well on this but it's
kind of since it's half C half Erlang I
sort of made it both colors and again I
missed a box so the monitoring system at
this point was mostly Perl
in 2011 this was the big push to like
rewrite sort of everything like the old
PHP code base it was hadn't been really
looked at a lot of folks had been on the
market and no one really wanted to go
near the the sort of awful
object-oriented PHP code so we
essentially did a clean a clean room
rewrite we basically said well what do
we want to build and let's just build it
and let's sort of just not even look at
the old stuff because it's for the most
part you know too ugly to even care
about and since we have had a couple
successes in Erlang a couple of the
components you know we were sort of
splitting apart the model at the GATT
server into many components and a couple
of them were written in Erlang a couple
written in Java in addition they're sort
of management layer which had been a
pure sort of PHP management app was
split up and sort of made you know web
2.0 I guess you'd want to say where it
was Web Services with sort of an HTML
Javascript sort of front-end also pieces
of the monitoring system at this point
were rewritten in Erlang with some couch
TV couch to be for a back-end so sort of
got our architecture to where it looked
like this you know there's a lot more
sort of green popping into the picture
we still had some you know Java
components being written but sort of
more and more Erlang was kind of popping
up as and as more people used it sort of
ended up with more so I'd mentioned we
had started to see problems with
Cassandra and most of the problems were
actually in this sort of development of
Cassandra and the fact that I don't know
about three years ago they made a
decision to make a release that was not
backward-compatible and that pretty much
made a made it a no-go for us we
couldn't actually upgrade because our
system the way it was designed or had
been built originally we were tied very
closely to Cassandra which you know
granted was probably a mistake but what
it meant is that we couldn't really
upgrade Cassandra because they provided
no way to sort of piecemeal upgrade it
it was like well you just shut
everything down and just start it back
up and ad-serving isn't really a
business where you ever get to
shut down so what this prompted me to do
was kind of take a look around and see
what had sort of developed in the year
and a half that we had been running
Cassandra and that's when I came across
react and the react course stuff that
had led to other work but since I by
this point also used react a little bit
I said hey well we can use it as a new
back-end but what we really need to do
is do things right which is we need to
put some layers between us and the data
stores that we use so we developed these
these data services for a different sort
of constituents the users the
advertisers and the publishers and these
are sort of thrift services on the front
end and then on the back end they talk
to one or more data sources depending on
what they need to do and this allowed me
to sort of migrate first the
applications that we're talking to the
user data service which is our biggest
use of Cassandra - I first converted
them from talking to Senator talking to
user data service once they were talking
to that I was able to do things like
dual right to react and Cassandra in the
backend dual read and merge the results
and so that allowed sort of a seamless
migration from one data system to the
other
also the the API which had been
rewritten in PHP because they were still
there's still a few PHP guys hanging
around was determined that that was
really not scalable and so they started
yet another rewrite on that which led to
sort of a two-tier architecture where
they have a front end sort of Erlang
router which routes between different
versions and then the old version the
version one is in PHP and sort of
version two is in Python so that got us
to sort of you know the picture of 2012
and it really hasn't changed much since
then since then the Cassandra box is
almost able to be removed from this
diagram as we've as we get closer and
closer to getting the various pieces off
but as you can see there's you know
there's a lot of green there so there's
a lot of erlang going on where did all
this get sort of open ax well it got us
to 250 billion monthly ad transactions
last year
in 2012 which works out to about 4
trillion ad transactions
and since most bad transactions are both
of sort of ad requests and an impression
looking at about like 8 trillion
requests going through our systems in
addition we had sort of a peak about 12
billion bids per day which works out to
quite a bit we also have sort of
thousands of machines and five coalos
we're up to about 260 employees and we
booked a 150 million plus in revenue in
2012 so but you know we're talking about
Erlang and the languages where did this
get us in terms of our components and
languages well we ended up with about
nine components on my diagram in Erlang
although we've got a few others that are
just not there on the diagram
there's too many boxes a couple
components in Java although we have a
bunch more that I didn't list because
they're all sort of not part of our
serving infrastructure and then we also
have you know some HTML Javascript some
Python they're still about one lone PHP
one so now I want to talk a little bit
about sort of you know how because well
from a company that basically didn't do
any Erlang and was a PHP shop to
basically having a large amount of
Erlang I think the question that most
people have is well how did you how did
you manage that
like did you hire lots of smart Erlang
people do you know what happened so I'm
gonna talk a little bit about that and I
kind of break it think that it sort of
was three things that sort of
contributed to to being able to bring
Erlang on board first off was the
architecture second was sort of tools
and automation and third was evangelism
so shortly before I started they had
kind of decided on you know some
architectural sort of visions of how
they wanted to build things based on
experience at prior companies and things
the systems were running on Amazon ec2
so you know it was obviously gonna be
cloud-based you know we're gonna run
software as a service but we want to
sort of run our software in some sort of
cloud infrastructure you know so what
does this mean well you know we want to
have fairly cheap generic hardware we
want to have a lot of automation we want
to be able to bring up and you know
deploy new servers very quickly
there's something that kind of I brought
in which I call sort of package oriented
development which really just means like
package everything and package it really
early and you need it to be fault
tolerant because things tend to just
fail all the time in the cloud so in
addition we wanted it to be you know
service based like instead of having
these monolithic servers that sort of do
everything you want to kind of split
your problem apart into a lot of
different tiers each tier of which does
sort of one thing and is basically given
you know a full box or you know sizable
VM to do what it needs to do and this
allows you to sort of scale each of
those components separately so obviously
you need to have kind of pools of
components and then certain we also knew
that we needed to sort of support
multiple languages so that's the sort of
early adoption of thrift as a RPC
mechanism and sort of allowed us to like
know that like well okay we're using
Java right now and PHP but maybe we want
to use something else in the future so
just to kind of skip back to the slide
you know each of these sort of boxes
actually represents a whole sort of you
know cluster of machines and some of
them there may be a few machines some of
them there may be hundreds of machines
it just depends on sort of the component
and what it's doing and what its
scalability is and a lot of the
challenges sort of figuring out you know
with what the different sizing of these
things need to be but for the most part
you know the guys that are on the
channels on the side all the guys in the
center talked to them through the
lightweight event system which I'll get
to in a little bit and then up and down
most things are thrift based so when
requests come in they'll come in through
the gateway which will send things off
to you know the market or add selection
or RTB via thrift requests and they may
pull data from one of the data services
via thrift so there's a lot of things
that sort of bounce around in there
so now to skip a little bit to kind of
automation in tools so part of being
able to you know handle the fact that we
have like you know a dozen different
roles or even more than that is you know
being able to automate the installation
of machines with those particular roles
on them so we had a couple guys who came
from Google they liked this system
written at Google called slack so it was
sort of in place when I got there this
was you know 2008 I think when they were
deploying it so I'm not sure if any of
the more modern stuff where it was
around puppet or sharing these things
but they had a tool they knew they so
they used it and so now we're sort of
stuck with it for the most part and it's
a pretty simple tool it really just uses
our sink to just sink a bunch of like
config and scripts onto a box it then
sort of will run those scripts it'll run
a script and then it'll sort of take all
the files that were downloaded and like
overlay them on the root filesystem and
it will run another script and that's
about it but it also provides kind of
like the idea of like roles and sub
roles so we can have a different role
for each of our server types and then
sub roles for different coalos in case
we need Colo specific configuration so
this is our sort of workhorse for doing
bootstrap and deployment of servers we
need to sort of start up a new server of
a new type we assign the role to it boot
the Machine it goes out finds out what
it's supposed to be installs all the
software starts up services does the
configuration all of that sort of stuff
so another component and tooling is sort
of packaging and I like to sort of have
this this thing that I call package
oriented development which is really
about hey packaging things and packaging
really for your OS and the reason you
package for your OS and don't use some
sort of language specific packaging
thing and things like that is sort of
that the barrier entry is lower when
working with an Operations Group if you
hand them a bundle of rpms for their Red
Hat system versus hey can you just go
like install these ruby gems using
something or can you go install this
other stuff which isn't packaged because
you know the packaging system unless you
sort of have a good sense of like
everything that's installed on your
system you're able to sort of remove it
so in order to make packaging so what I
like to do is sort of make the entire
culture of the development organization
about packaging so this means pushing
the responsibility of packaging all the
way down to the developers and
developers actually use the package as
the main medium of exchange between each
other as well as you know the other
parts of the company so if I need a
library that another developer is using
then and I need a fix then they will
create a new package for me and put it
into a testing repository and then I can
install that package if it doesn't quite
fix my problem I can roll back to the
previous version that I had an own you
know bug or I can get them to send me
another package but at every point sort
of the state of what's installed on
everyone's system including the
developers is kind of managed by this
system and in order to do this we sort
of require a semantic versioning for
everyone and the packages that they
create so some way to know like oh
you're releasing a package but it's not
gonna work it's not backward compatible
so you know the packaging is a little
bit you know it can be difficult like if
you've ever had to build a spec file or
you know Deb thing it gets to be a
little tedious and whatnot so that's
where a little tool comes in called
framework and framework really is a way
to basically build packages but it's
also a way to sort of build all of your
code and it also sort of will allow you
to have certain organizational like
requirements on the way code is built
and how it's set up
so it sort of has a couple modes when
you're initially creating a new sort of
component or package you use a tool that
will sort of layout the directory
structure for you it'll lay out the
initial build system and then once
that's all done you have a very common
set of commands that pretty much work
across any language that framework has a
template for and these commands allow
you to do things like build the code run
any unit tests that are on the code
certain languages have special things
like the C template allows you to run
your code your unit tests through Val
grind
and it has it enforces that unit tests
have to pass before you construct a
package when packages are created it
integrates with your versioning system
to basically tag the version of the
released software it sort of builds
along with the package for the runtime
it builds a bill time package which has
all the fixed set of dependencies and
really these are about kind of ensuring
that there is some amount of
reproducibility right like if you're
gonna allow developers to build packages
on the VM on their laptop you want to
make sure that like if they basically
quit tomorrow you know how to build
their package you're not gonna be stuck
having to sort of figure out what's
going on and framework sort of does that
for you if I need to build a particular
version of a package I install the build
version for that version I check out the
tag and my system is basically as close
to the state of that system as it was
when they built it and at that point I
can build it make it run tests do those
sorts of things so another component
which I talked about on the you know
architecture side in terms of cloud
things was fault tolerance and fault
tolerance in this case is sort of means
kind of keeping pools of servers and
once you have a pool of servers well you
now you need to wait to sort of spread
load across those servers and at first
we looked into various kind of sort of
built into the upstream servers pooling
systems but in a lot of ways that was
more we didn't find anything that we
liked at first and so in a lot of cases
we didn't really need the features that
were there but you know we had been
using H a proxy kind of in front of
everything but sort of we had this idea
like well we can actually just use H a
proxy that pool all the back-end things
for one particular service so instead of
having an H a price sort of inverts it -
usually you have an H a proxy say on
port 80 forwarding to your non port 80
service well now if you have some other
services on another box and there's like
ten of them well you have a local H a
proxy and you have it point to these ten
other boxes and now your local service
just connects to localhost and gets
connections through H a proxy and now it
doesn't have to care and also the H a
proxy configuration z-- team completely
separate from your server software
so network topology changes mean usually
like an H a proxy reload of a config
which makes it very easy for the
operations folks because they don't have
to bounce a lot of servers when a
machine goes down plus just allow things
like yes we know we get automatic sort
of failover we get a little bit of you
know H a proxy keeps some statistics so
you can keep track of some things but it
does have sort of drawbacks in it like
managing the machine lists was done by
hand for a long time and and was sort of
error-prone and and overall like very
annoying
luckily within the past like you know
three four months we've finally got it
sort of integrated with our you know
machine management database so we can
easily just regenerate configs with
current lists of machines and we sort of
there's sort of one issue that you have
with using H a proxy in this mode is
that all your H none of your
eighty-eight proxies know about each
other and if they all have the same
machine list and they all are using
round-robin or at least con or any sort
of back-end thing they'll are they're
all going to connect to the first
machine in your pool so under a
relatively high load it doesn't matter
because very quickly the connections
spread out across all your sort of next
tier but under light load or sometimes
if you're doing a restart of the
back-end services you can have sort of
problems but that was solved by a nice
little hack that basically just
randomizes the machine list on each host
so if you have ten front-end hosts that
are talking to ten back-end hosts before
they all had the same you know back in
host in the same order on every box you
restart the backends everyone gets
reconnected to that first one now the
lists are randomized on those front-end
hosts so they hopefully should you know
end up on different different machines
so each a proxy is good for you know
quick connections but sometimes you want
to have persistence and so in you know
you can do persistent connections I mean
you can make a person's connection and
hold on to it but we actually then
needed sort of a pooling mechanism in
Erlang so we wrote something called gen
server pool at that at the moment you
know we were basically most sort of
connections
things are usually a gents River wrapped
around some Network you know Jen TCP or
something and you're usually interacting
with the Jen server so being able to
kind of just have a pool of these that
always stayed connected was sounded
great but we already had most things
just sort of set up to like spawn up Jen
servers and do these things and so
looking into the different pooler's that
existed at the time which I don't know
if there were many because this is
probably have been about 2010 I don't
think pool boy was around or any of
these others but the few that I found
all had this required pretty major
changes because they all were check out
of connection you know do some checks or
something with it check it back in
didn't like a lot of that so Jen server
pool actually allows you to take any Jen
server and just pull it with minimal
effort you really just have to change
your start link from starting the actual
Jen server to just starting calling the
Jen server pool start link and telling
it which Jen sorry use and at that point
it'll actually proxy the calls to the
Jen servers and sort of do auto checkout
auto check-in and it'll grow and shrink
the pool kind of on demand as your your
traffic increases or decreases and while
this does have the sort of like you know
Jen server bottleneck in in most cases
you don't really see that unless you get
pretty high high loads so we actually
use this not just for kind of loading
connections but like maybe you need you
need to have you know eight Jen servers
to use the eight cores on your box you
can use Jen server pool to kind of
spread the load out we have a similar
pool that we use on the Java side but
it's a little less generic it's specific
to to thrift another part of the
architecture and tooling was these sort
of cross language communication things
that I mentioned a few times so I
mentioned thrift you know we use that
for RPC between almost all our
components it supports a lot of
languages which is great gives us lots
of options if we ever decide to move
away from Erlang we use protobuf in a
couple cases to talk to react also our
our TV protocol is protobuf which kind
of bucks the modern trend but we sort of
predate
the modern sort of open RTB stuff so
we'll probably get there eventually and
also the the lightweight event system
which is a udp-based cross language
protocol that's used for logging and
monitoring and so again just to kind of
flashback
you know LS is used to kind of
communicate with our monitoring and our
sort of data collection Hadoop systems
and thrift is mostly used up and down in
our stack except for a few cases where
there's a protobuf so okay so we've got
a lot of components you know I sort of
laid out a little bit about how we've
sort of made deploying these different
components and managing these different
components a little easier but you know
the other thing is well how do you sort
of monitor how do you keep track of
what's up what's down whether things are
working so a couple ways we use munion
for sort of operating system type
monitors and then there's a little
there's a system called mon demand which
uses LS and basically we'll Tate will
allow developers to add just small
little bits in their code where they say
okay increment a counter over here
increment a gauge over here and on the
other end they'll have an R Rd and then
they can use the built-in graphing
components for the RDS and it works with
sort of all the other already based
things that Union is one of the other
thing though that was added that really
helps in this run a big distributed
system is what I call what I called
tracing it's somewhat analogous to the
the Erlang tracing but a little bit
different in that usually what happens
is you know I had our sort of big stack
here and when a request comes in it's
gonna bounce around to two or three
components and look up data and do
various other things so what if
something goes wrong how do you figure
out what what's going wrong and where so
we've created a way to kind of tag a
request as it comes in and as it flows
to the system it'll sort of trace what's
going on so a couple little screenshots
this is kind of one of our mod man stats
dashboards for for react sort of pulling
data from the different react nodes and
showing request rates and late in the
sea and then serve an example of what
the trace output looks like
so you'll see it kind of comes in to the
gateway goes off to our user server and
get some segmentation information and
shows kind of what it got back and then
it makes a request adds to our add
selection engine and passes its various
IDs for what it's looking for
and then eventually it'll get some ads
back and maybe go and call an impression
and I cut out a bunch of stuff because
usually these traces take up pages and
pages and pages when they're fully on
but they've actually ended up being sort
of extremely useful for our customer
support as well as our QA folks in order
to figure out like what's going on where
in the system because usually a customer
will call in and say I'm seeing blanks
on this page and so you know how do you
figure that out when there one customer
and we've got thousands of customers and
it's all shared software and so you want
to have some way to do it so usually we
figure out we reproduce their case and
then set up a trace on that about
reproduction and then we go through and
usually figure out like oh you've got
some rule down here in the add selection
thing that's triggering that's causing
it to not do anything further and so if
we go and change this rule we can fix it
and then sometimes it uncovers bugs and
things like that so there's a little bit
about kind of the tools and the
architecture and through those things so
the last component about how I think we
really got Erlang in heavy uses a lot of
evangelism now the first part of this is
is it's it's it's great and this was I
was able to do this it's if you if it's
possible to like fix the game before
going in you can get it to happen by
fixing the game I mean like look the
architecture choice was like hey use
thrift use basically this like service
based model these sorts of things allow
you to kind of slot in a new language
very easily because well everything's
gonna be its own service and you're
using thrift which supports like 15
languages so if you happen to want to
try out one of those languages and it
fits your problem you can kind of just
slot it in and so that was pretty much
kind of you know I sorta helped some of
that had been set before I got there
some of it I helped sort of set and it
really helped long term because you know
when it came time to it it was really
just like well you need to find a
project that really is gonna showcase
the
nology and i found well I've got this
RTB bidding system Erlang seems to fit
really well with it it's just gonna be
another component and people are gonna
talk to it via thrift so I'm just gonna
do user lying and I think I can do it
very quickly and you know luckily we
also an organization that's a start-up
they're open to new ideas they said okay
go for it try it out but then the next
important bit which is really a hard
boot is like well you need to make sure
that it works if you're going to go out
on a limb and like basically put your
reputation and you know online make sure
you're pretty confident about it and
make sure that it works and you know
yeah you're gonna have some setbacks but
you just really have to stick with it
and like do it and the other thing that
I found really worked was sort of
sharing as I was writing the RTV system
I'm sitting right next to the guy who's
still slogging through you know tons of
Java and my favorite thing to do would
be to come up with some neat way to
solve some problem that Erlang and
functional programming allows you to do
in a very compact elegant way and then
to turn around and be like hey check
this out
and then sort of walk them through
what's happening you know and your 510
lines of code or whatever and then just
watch them get sort of more and more
dismayed and then at the end just be
like man that would have been like 200
lines of Java and I would have needed
like 18 builders and like this whole
thing to do that same thing you're like
okay and the last thing was sort of
making it easy for others and that's
kind of how I want to spend the last
sort of few minutes is sort of how I
tried to make things easier for the
people who were gonna come after me
using Erlang so first off was back to
framework which I talked about sort of
was a way to sort of lay out packages in
a way that you sort of they built across
everything and that also would end up
with an RPM and luckily framework had
actually been developed using and had an
airline template already so I kind of
already had that and the Erlang template
itself like has sort of Erle RC
integration but around when I was doing
a lot of this rebar was I guess it came
out a little later so we used f2v
template Erlang quite a bit but
eventually rebar was sort of taking over
the Erlang world and framework uses you
know auto comp and auto tools
so I made a template that would use sort
of still use the auto comp auto tools
for the packaging layer and then it
would use rebar for the build layer and
then I sort of set it up so that this
allowed me to easily just kind of take
any third party Erlang rebar package and
turn it into an RPM with just pretty
much a single command line and what this
allows me to do is kind of if there's
any third-party dependency that I need
you know ten minutes later I've got an
RPM it's in our repository
I now can install it it integrates with
the rest of the system just makes it
overall very easy if someone says hey I
need this package usually I just package
it for them in a few minutes because
it's just so fast to do or someone
needed an updated version of I think
like react Corps recently and it had
like you know eight new dependencies but
it took me I think all of a half-hour to
get everything packaged up and working
it so and also the fw template Erlang
rebar is also about the only way is a
much easier way to sort of develop ports
and NIF's just because it does a lot of
the compilation and sort of linkage
stuff for you so while most of our
packages are still fw template Erlang a
few that need these nips reports use the
rebar version sort of the second way
that made Erlang a little bit easier for
the developers was using some tools also
developed sort of with some friends at a
previous company one was Earle RC and
this integrates kind of your package
management system with Erlang in such a
way that installing an RPM will run
through your hot code hot code loading
and you know if it fails it'll roll back
the RPM of a succeeds it'll you know
remove the old RPM you can sort of
upgrade downgrade and do a lot of that
stuff and then it also sort of
automatically kind of creates sort of
boot scrap boot scripts and things like
that and really this is because it's
sort of meant to be worked on I'm
running a single Earling node on any box
and I'm installing Erlang applications
into that node where each of those
applications is an RPM package which is
a little different than the sort of like
rebar release style where it's hey I'm
gonna pull all of my dependencies
put them in this little sandbox I'd like
to get to the point where sort of you
can use framework to kind of do both
styles but at the moment it's really
tied to earl RC and then the other two
tools that kind of work with it or earl
start which is kind of like starting
stopping things as well as earl node
which integrates with kind of the Red
Hat stuff so all of this was meant to
basically make it very easy for someone
coming in to get to the point where they
have something that's package' below and
shareable immediately and not have to
worry about a lot of the build system
and on if they worry about that if
they're just learning Erlang they can
focus just on the Erlang pieces and not
on the messy crazy deployment pieces
because it particular new person coming
in like they're not gonna go through the
whole process of creating releases and
all this unless they're sort of really
glutton for punishment have a lot of
time I sort of wanted to make it so that
okay you don't know her lying but I can
teach you you know three commands to
check out build a package and and you
can go and make some bug fixes on this
code and release rpms for it and with a
you know hour an hour of time they can
actually have a package that's ready for
installation on the QA environment or
production box so got out sort of page
of lengths and the slides will be
available online but a lot of the
systems I talked about are open source a
lot of the tools framework slack gen
server pool the lightweight event system
on demands and then the Earl RC start
node so yeah any questions
okay the question was what sort of data
are we storing in react we're in the
process of storing all of our data and
react basically the main data that's
sort of currently live and in production
in react is our user data so this is
like information about it's not
frequency data that's a sort of you know
react or outside of it but we do have
other information that has to do with
like user retargeting or which segments
a user is in so sort of you know
behavioral type of stuff a lot of it is
not even data that we look at because
it's just we give tools to our
publishers and then they're able to tag
people as in a segment but we're also
looking to move all of our advertiser
and publisher data which is the
information about how we you know
publisher data tends to be like which
you know ad units do they have on what
pages and what rules do they have about
matching ads to those ad units and then
advertiser data is for the marketplace
sort of advertisers who involved in the
market so all that data is kind of in
process of being moved in to react for
the delivery systems and then we
actually also are using it for the
backing store for even our sort of
management side of things so we're in
the process of kind of moving off of my
sequel off of Cassandra and sort of
consolidating mostly with react and
actually there's online I think might
talk from recon which goes in detail is
that so if you're interested you can
check out the recon talks from last
October
marketplace
right okay so the answer was in the
real-time bidding system like how does
the money work and do we do anything to
make sure it isn't lost
so the a short answer is that it all
goes through our lightweight event
system and then stuck on to Hadoop and
most of what is being looked at are
extremely small values so we actually
can sort of if there's a small amount of
loss there it's not really a big deal
like we don't do anything special to
make sure that that's like it's not like
a you know banking system or something
like that most of the time what it is is
that advertisers in the in the
marketplace have a budget and the real
thing is you don't want to go over
budget so if you lose a little bit
really it comes out of you know open X's
and because the publisher ends up
knowing that it happened and so you know
we do our best to make sure that that
doesn't happen but we haven't sort of
made that a requirement just because
it's very you know hard and you can
usually it's more cost-effective to
allow a little bit of loss and focus
your engineering effort on other systems
like scaling it out to deal with more
than it is to sort of deal with like you
know a thousandth of a cent that might
get lost here and there so does that
answer your question okay
anyone else so one more so right now
it's done with a cookie so there's a
there's a base leaves sort of place
where where one of our support people
can go it'll generate a cookie in their
browser and this way they can actually
go and trigger it on any of our any of
our publishers sites so because the
publisher is gonna and we can even sort
of allow publishers to generate traces
and then we get to look at the trace but
they can generate it so essentially
there's an interface you go in you set a
cookie you give it sort of some
information about who you are and what
you're tracing and that's the little bit
of information that gets sort of
tunneled through the system and it is
then used at the end to collate the
traces that come from all the different
components so the the general bits of
the tracing are all open source in the
Mon devanne project and so you can and
there's I think really only like C Java
and Erlang there might be a pipe on one
now but there's a command-line tool you
can easily wrap with any other scripting
language and LS supports like a handful
more languages so it's wouldn't be
difficult to make it work with other
languages but it's definitely like a
great thing to have if you're dealing
with any sort of like system that has
you know more than a couple servers that
your request is gonna tunnel through
being able to just figure out what
happened and when and reproduce some
problem is great and I knew it was a
winner when basically every single bug
request now comes with the trace so are
every single bug report from QA and you
can sit there and go down and see you
like and they basically usually decipher
them themselves and say it looks like if
the bug is in this component because I
expect this to happen and it doesn't and
it oftentimes provides the only way for
them to test a really deep in the bowels
backends change is by the trace you know
values changing what's being sent from
that component but it does add a bit of
overhead to the to the calls as you
would expect so they tend to be a little
slower
yeah well the that the cookie is only
valid for a short period of time so if
someone gets the cookie and then does a
bunch of stuff like the cookies only
gonna live for you know a few minutes
and then it's gonna stop tracing
although I think there was one time when
somebody mistakenly you know turned on
tracing for every request instead of
just the one and then we got to watch
CouchDB like balloon up and then crash
horribly so great any other questions
cool well thanks guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>