<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Guy L  Steele   A Cobbler's Child - Code Mesh 2017 | Coder Coacher - Coaching Coders</title><meta content="Guy L  Steele   A Cobbler's Child - Code Mesh 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Guy L  Steele   A Cobbler's Child - Code Mesh 2017</b></h2><h5 class="post__date">2017-12-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qNPlDnX6Mio" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">many people made contributions this
language and among them are Gerhardt
Jensen John Backus Peter Nauert Alonzo
Church but none of these gentlemen had
any idea that they were making
contributions to programming language
this particular one anyway and there are
many others some of whom have been
anonymous they've been there been
incremental changes and I would tell you
the name of this programming language
but it doesn't seem to have one so I
call it computer science met in notation
and by this I mean the language that can
has built-in data types which we take
for granted boolean integer real complex
sets lists and arrays its primitives and
suppressions are those of logic and
mathematics for after all most people
regard it as a mathematical notation
rather than a programming language it
does have user to pair datatypes and you
can think of it as records or abstract
integer symbolic expressions and these
are expressed in the language using BNF
the Bacchus now reformed and every time
you write a BNF production in effect
you're declaring an abstract data type
it's code is in the form of inference
rules the so called again some notation
we'll see examples of that in a minute
and while it has several forms of
conditional the most important one is
rule dispatch the anon deterministic
pattern matching so this is a
non-deterministic language it has
opportunities for parallelism curiously
those opportunities for parallelism have
not been exploited yet there is a
recently developed in the language and
various concise notations for repetition
sometimes iterators but more recently
just the very concise use of over lines
or perhaps ellipsis notations by
ellipsis in this talk I mean the three
dots that were you Stewart to writing
that's the technical term for that and
it has a very unusual operator which is
to capture free substitution within a
symbolic expression and that's that we
can attribute to church so given an
expression you may want to replace all
the vir all instances of some variable
in that expression with a different
copies of a different expression that's
the substitution operation it's a very
interesting language it's a very
beautiful language I think it's
neglected and recently I've seen
evidence that it's getting messed up and
I'm going to talk about that so here's
an example of some data declarations in
computer science meta notation it's in
its
BNF for my took this from a paper and
the principles of programming languages
conference a few years ago and these are
typical we're gonna examine this in more
detail later so I'm going to skip on
here's here's an example of code in this
language this code may be
non-deterministic it has to inference
rules and against an inference rules
this horizontal line with a set of
premises written above and conclusion
written below and essentially states
that you were entitled conclude to
conclude the conclusion provided you can
prove the premises that are above the
line and in this case the author of this
little little program has provided two
rules and very thoughtfully also
provided a function signature for them
so to speak in the box this is the no
conflict set of rules and it takes
several arguments sigh I a bunch of toes
notice that little over line over the
talk indicating they're actually a bunch
of them and J and the author as even
more thoughtfully provided a comment
this is rare in CSM programming but
apparently these rules and or intended
to check for applachian conflict sort of
that means and there are two different
ways they're approved that you can prove
no conflict the authors also helpfully
labeled the rules that doesn't always
happen there's NC apart and NC
compatible and you're entitled to
include no conflict if size of the
indicated form in the first rule and you
can prove that a part is true where a
part is presumably some predicate
defined by some other rule or you can
prove no conflict if so I am sighs a are
compact and that's indicated in a second
rule and mostly in these rules I want
you to see that first of all you've got
a choice of how to prove no conflict and
this program description gives you no
indication of which you should try first
already should try them both once you
and five sources and parallels and
figure out which ones thing finishes
first that's a possibility
and I've highlighted here in very very
pale blue I hope you can see on the
screen that several uses of this over
line notation and notice that some uses
the over line are nested that's a point
which will return later here is a
different program which I took from a
different principles of programming
language conference that was just last
year in 2016 that was about 20 months
ago 22 months and there are seven rules
in this and is actually a subset of the
rules actually presented in the paper
and this is in fact a type checking
algorithm and so roughly speaking that
rule at the upper right says get from
gamma a set of assumptions gamma you can
clue that variable X sub I has typed
ossify provided that X of I plus y
actually appear in gamma and that's an
implicit feature of the notation that
this offer took for granted the the rule
of the upper right may seem for familiar
that says that if from gamma you can
show that M is a function from Sigma to
taw and that n is something of tight
Sigma then M applied to n will be of
type talk and so this is a type checking
algorithm and notice a couple of
interesting features about this one is
that implicitly the offering tends that
we regard gamma and the expression
before the colon is inputs and expects
you to regard the type of coming out as
an output the type of after the colon is
an output now in fact there other ways
rules were used to just as in other
kinds of in constraint languages you
could provide a type and try to find an
expression that matches that type the
interesting thing here is that if you do
req regard both gamma and the expression
before the colon as an input then in
fact the non-deterministic choice of
rules is for it fully deterministic
because the rules happen to have it
written in such a way that each rule has
an expression of a different form so for
any given expression only one of the
rules will match at most one of the
rules will match possibly no rule will
match in which case you have a malformed
expression you can't figure out the type
the other thing to which I want to draw
you to your attention is that ellipses
have been used in this rule in the
middle for two different purposes one is
to indicate an iterator here it says
that I ranges from 1 to the arity of up
whatever up is that's an operator and
its intent is that the premise written
before it which refers to M sub I
actually as many instances 1 for each
value of I on the other hand below in
the cutting the conclusion of the of the
rule it indicates a sequence of M's M
sub 1 through m of arity up and and
instead of saying that there are a bunch
of conclusions there's a single
collusion that happens to have a
sequence of things in it
yeah the correspond to the premises
you've tried to prove up there ok so
these are a couple of concise ways the
over lines and
he's indicating repetition within these
inference rules okay before you get that
graph I just want to say that I have
seen sets of rules much larger this I
personally written such a rules that are
10 or 15 pages long and in this case
they were a specification for the
fortress programming language we then
took those rules and hand coded them at
escala and that became the type checker
fortress language on the other hand I've
seen other attempts to analyze a
compiler where you take whatever
language the compiler was written in and
tried to extract from it what type
system is this compiler really
implementing or enforcing and trying to
reduce it to rules of this form and this
is the language that type theorists use
to talk to each other now I think it's
an interesting irony that the language
that type theorists who care about
carefully specified strongly typed
languages the language they used to
community with each other is not
strongly typed and not well specified
there is no specification of this
language written down in any one place
it has kind of grown as a folk tradition
over the years but I searched long and
hard and could not find any single paper
to say here's how this language works ok
so I hope that have convinced you that's
a programming languages because
substantial programs are written in this
and they are then hand translated into
executable code but now I need to
complete my claim I need to show you
that in fact it is popular and so to do
this I did a survey of all of the
principles of programming languages
conferences I did this work about a year
ago
going into last spring so I didn't have
the 2017 puple but analyzing the 43
conferences from 1973 to 2016
that was a fourteen hundred and one
papers is about 1,700 so I 17,000 pages
of material and I had to look at them
all by hand because one interesting
thing about the notation is that it's
salient feature that horizontal line is
not captured pad today's OCR processes
that are used to OCR conference papers
so you can't search for it you got to
look for it so I used V grep visual grep
and I looked at every page of every
paper in every conference fortunately I
have all those paper proceedings my
library and reduce the data and here's
what I found
which is that the number of papers in
the Papa conference has grown over time
and within each the total height of the
bar is the total number of papers and
this is by five year intervals and the
lighter part of the bar at the bottom is
shows that fraction of papers that
actually use this inference rule
notation and you can see that the
percentage of papers has grown over time
as well as the absolute number of papers
and for about the last 20 years this
notation is used in over half of papers
appearing in the puffle conference which
far outstrips the popularity of scheme
or Lisp or Haskell or any of the things
you might think that type theorist care
about you know a little own Fortran or C
you know those are just there are papers
just cussing those but there are not
nearly as many as use this notation so I
did some cross-checking and I looked at
other ACM conferences specifically
sigplan conferences because I figured
this is a programming language thing
Sigma planned people how to be interests
in this and for the three years in 2014
to 2016 this notation is used in 1/3 of
programming language design and
implementation conferences in the
object-oriented programming systems
languages and applications conference
oops law used in about 1/3 of those
papers in ICF feet of functional
programming conference it was used in
over 2/3 of the papers and in P pop
which is principles and practice of
parallel programming it appeared in all
those no papers and this actually
confirms my theory because P pop tends
to focus on parallelism which is what's
happening at runtime whereas this
notation is usually used to talk about
the kinds of the static type checking
that usually goes on a compile time so
that that's sort of consistent with my
stereotypical idea of what kinds of
papers appear in these different
conferences ok so I believe that these
notations are popular within computer
science
and they are used in a large fraction of
papers more than any other language
therefore the most popular programming
language so that was research that I did
last fall going into last spring since
then this year I've continued this
research and I've got I've gone back and
I'm trying to look at all the signal and
conferences 1971 forward which there are
219 so far I've managed to examine 1996
of them so in my original research I did
the entire top line looking at all the
Papo coverages and then 3 years worth of
the stuff at the
and I looked at six 4p pop because the
data was scant and now going forward in
the 1970s sigplan had a bunch of sort of
miscellaneous conferences on various
topics and then they started coalescing
into specific topics that then became
series so PLD I really got going with a
compiler construction conferences in
1980s and then that got renamed PLD I
1988 they were the Lisp and functional
programming conferences that got started
in 1980 and were held every other year
until about the mid 90s when that became
isi CFP and I've also been looking at s+
and as well as loops Limpy pop and I'm
marching forward in time and once I
reached the 2000s I will then pick up
the Haskell symposium as well and try to
analyze that but so so the dark squares
in this diagram of the conference
newspapers I've actually examined so far
and so that's by way of background the
structure this talk cuz I'm going to
examine the history and the variety of
five aspects of this computer science
meditation not only inference rules but
also variations of BNF the substitution
operator in particular and then these
two repetition indications over line and
ellipses and I'm going to identify some
problems that have arisen with the last
three and actually identify the fact
that despite variation the first two
problems don't seem to arisen
okay so let's first look at the
structure of the inference rules this
was invented in 1935 Gerhardt Jensen
created a new rule notation for a mode
of logic which was called natural
deduction and there a couple of excerpts
in German from his paper in 1935 and in
the in the got the title and then in the
center section he says well a proof
figure is in fact this horizontal line
with premises above and conclusion below
the third box has some examples if you
can prove a and you can prove B then you
are entitled conclude a and B and if you
know a and B you're entitled to the
glued a alternately you can conclude B
and if you know a or either A or B york
entitled to conclude a or B is in fact
true so these are rules of logic rules
which with which we are now reasonably
familiar but he was reorganizing them if
it sort of tabular notation involving
these horizontal lines today's computer
science through inference rule notation
is not much better than that
there are two main differences one is it
nowadays we often label the rules and
the other is we've usually got so many
premises they won't all fit side by side
so we stack them vertically as well okay
big deal no problem so notice we've got
20 premises some some of these type
checking algorithms and you just stack
them up and fit them in wherever they
will fit now we've seen a wide variation
in the styles of the labels sometimes
they're in the lesson is on the right
sometimes they're aligned with that line
sometimes they're centered in another
way sometimes at the upper left
sometimes if they're lower right
sometimes they're right next to the rule
summers are swapped against the margin
you might be capitalized in various ways
sometimes they contain mathematical and
symbols or they're just alphanumeric
they might be in normal font size or
small or even in footnote size they
might be Roma and italic bold face if
there are multiple words they might be
separated by spaces or - two periods or
maybe camelcase is used and you might be
surrounded by nothing or parentheses or
brackets or braces or maybe there's a
following : I've documented no dozens
and dozens of variations and these
variations are not a problem because you
can sort of look at it and people can
figure out oh that's a label it's mostly
not a problem sometimes easy to stick
the label at the upper left and it's
real near the premises you can mistake
it for premise you have to be a little
careful of that so the point is it's
natural for language to change slightly
over time sometimes for good reasons and
sometimes because authors are being
random and it's mostly okay as long as
understandability is not compromised and
so far these variations have not proved
to be a problem okay so that's all I've
got to say about interest rules as such
now let's look at BNF okay let's have
some historical background on grammar is
about 2500 years ago panini wrote a
Sanskrit grammar that contained many
concise technical rules describing
Sanskrit morphology unambiguously and
completely and if you squint and assume
that what he wrote is whole words or
become symbols and BNF non-terminals his
rules look an awful lot like Bacchus in
our notation you know so this is
actually there's an ancient honorable
history here however that's kind of a
one-of-a-kind thing I think in part
because it turns out to the grammar of
Sanskrit language in particular is
amazingly regular and a production if
you use these production productions
this grammar to construct a sentence it
very probably is in fact
a sentence in Sanskrit whereas producing
a grammar for say English is much more
complicated
okay skipping forward to the 20th
century a 1914 axial to studied string
rewriting systems defined by rewrite
rules he's trying to understand their
mathematical properties and could you
make the strings grow infinitely and
what Keys include what could you
conclude about what kinds of strings you
can compute with these rewriting systems
in the next decade a more post studied
of very closely related tag systems in
which symbols were repeatedly replaced
by associated strings although he did
this work in the 20s it wasn't published
until 1943 and I think that's relevant
in 1947 Andrey Markov and amyl post
independently proved that a particular
problem posed by to the word problem for
semi groups is undecidable and this is a
very timely thing because it immediately
tied into the work that church was doing
on the undecidability of lambda calculus
and a Turing was doing on the
undecidability of computing via Turing
machines and in in so these things were
kind of converging in 1956 Noam Chomsky
published his paper three models for the
description of language and this
described grammars with production rules
now what we call the Chomsky and
hierarchy of grammars he indicated that
some kinds of grammars are more powerful
than others and I designated them type 0
through type 3 type 3 or what we now
call something call em regular grammars
they are they the kinds of strings you
can describe with regular expressions
with no cheating
I know cheating I mean I mean UNIX
regular expressions let you bind
variables and kind of refer backwards -
backslash 1 and backslash - you know
that that's cheating but the real
regular expressions where you just got a
choice and the Queeny star and a couple
of other things that's regular
expressions that's type 3 grammars type
2 grammars or the context-free grammars
and that's what you can describe with
BNF and then type 0 and type 1 grammars
or various kinds of context-sensitive
grammars ok so you think that ok now we
understand where BNF comes from now we
understand where our expressions come
from Chomsky said there are four kinds
of grammars two of them got elaborated
by computer science and clearly we've
got a common base and one should be a
superset of the other wrong that's not
how developed at all
they developed completely independently
okay so here's the history of regular
expressions in one slide which doesn't
actually have a lot to do with my
story but it says I discovered this in
the byway since I was researching the
BNs okay so in 1951 Stephen clay nee
developed regular expressions to
describe of all things the McCulloch
Pitts nerve nets he was actually trying
to understand what we now call neural
networks he was doing AI but he's trying
to come up with a formalism to describe
what sequences of signals could come out
of these these nerve nets and he
developed the regular expressions to
described that he used the wedge symbol
for choice and he considered using
asterisk as a postfix operator indicate
zero or more copies than he said now
let's make it a binary operator so X
star Y in his notation means any number
of copies of X then followed by a Y and
the reason he did this was to avoid
empty strings if all your base strings
are non empty then X star Y will also
necessarily be non empty and that made
his mathematical formalism a little bit
easier he didn't publish this work for
another five years from 1956 he had the
journal publication and it includes only
the binary form of the star he doesn't
even talk about the possibility of a
unary star the postfix star that we all
know and love today however in 1958
three other guys copy I'll get and write
formulated Reger expressions using dot
to indicate concatenation the wedge
symbol for alternation and the postfix
star what we now call the cleany star
that was actually something that claimed
himself had rejected in 1960 tubers
ascii used binary plus instead of the
wedge symbol and he introduced the
postfix plus to mean one or more copies
of something
in 1968 the AED system instead used a
slash for alternation because it was on
the keyboard I guess but then the cup
symbol for alternation which wasn't on
the keyboards I don't know where that
came from I'm just kind of guessing and
the postfix star also in 1958 ken
thompson wrote a paper called regular
expression search algorithm
there's published I believe in the
communications the ACM and there we see
the vertical bar used to indicate choice
and so he made this Reger expression
search algorithm it was implemented as a
part of the text editor on the UNIX
system and several years later Doug a
McElroy was trying to analyze a large
text corpus and he was trying to use the
regular expression search in the editor
the problem was his corpus is so large
wouldn't all fit in memory
and the editor was designed so the
entire file your editing had to fit in
main memory I was sort of the style the
1960s and so he begged Ken Thompson to
take just the regular expression search
algorithm and make a separate UNIX
utility out of it and that's how grep
was born and so he extracted that and a
couple of years after that Alfred a ho
created Ygritte which provided a
complete regular expression language
including the possibility brewing if
parentheses he used the vertical bar
that Thompson used he provided star and
plus and apparently it was a ho
introduced a question mark I talked him
about that he said well it was the
obvious thing to do at the time question
mark would have the obvious character to
use so I guess I invented it but he's
not sure he doesn't he was just coding
it up and you know packing around but
there's no as I can tell that's where
question mark came into the game but
it's not necessarily obvious choice
because it three years later at Carnegie
Mellon University the Alfred project
using regular expressions in their
notation and they use Star Plus and the
sharp sign symbol the shark same symbol
meant zero or one one one copies that is
optionality and they used that for about
three years and then around 1981
suddenly all of the computer science
papers at Carnegie Mellon started using
question marks instead of sharp signs I
suspect that was the influence of grep
and Egret I'm not high percent sure I'm
still trying to talk to old researchers
from Carnegie Mellon University and for
1981 on the notation seems have been
pretty much frozen people pretty much
agree that the star means your zero or
more copies plus means one or more
copies and question mark means zero or
one copy but that wasn't the only game
in town as we will see later so that is
about regular expressions now look what
happened to be an F you would think that
BNF might have grown out of the same
roots but it didn't rather it grew out
of post productions in 1958 per listen
Sam Wilson wrote a report on the
International algorithmic language how
many heard of the International Albert
Drake language okay two years later is
renamed L ball okay maybe you've heard
of that right I saw one hand over here
so good for you yes but in 1958 was
called the International algebraic
language and it had forms for various
language features and so here are two
examples with forms it says that that a
function what we would
I'll call a function call has the form
and identify er and that a bunch of
parameters and parenthesis separated by
commas and it can be any number of
parameters and they indicated this with
an ellipsis slight notation except
instead of dots they've got these
Wiggles I have no idea
you know what typesetting system
provided those Wiggles but it's very
clever
and they also give the forms of
arithmetic expressions it says an
expression e can be a number or variable
or a function that is a function call or
it can be any of the other following
things and instead of using what we need
the colon colon equals are now used to
they used a slung - so it says e can be
of the form plus e1 or minus e2 or e1
plus e2 or e1 minus e2 and so this is
tantalizingly close to being be enough
but it's not quite there yet
okay the next year John Backus was asked
to write this up and he chose a slightly
different notation he was influenced by
amyl posts notation for post productions
and so he wrote something like this he
said that a digit can be either a 0 or 1
or 2 or 3 notice that he used a colon
followed by an equivalent sign with
three lines and then to indicate
alternation he wrote the word or with a
line over it and also noticed that these
couldn't be typeset she had to draw
every single one by hand so so those
those equivalent signs are drawn by hand
after the typeset colon and the lines
over the or you know all slightly
different because they were drawn into
the pen okay so when that was then
handed off the next year and peter naur
had to produce the report on Algol 60
for publication he didn't want to do all
that work by hand so he changed the
notation yet again to make it easier to
typeset and to make it slightly prettier
so he now I was one who introduced colon
colon equals he introduced the use of
the vertical bar to indicate alternation
long before actually a regular
expressions had that and he introduced
an interesting stylistic rule he said we
will make the names the non terminals
identical to the equivalent English
phrases used in the text to refer to the
same thing so as to make it easier to
correlate the formal notation with the
informal English description and so he
says an unsigned integer is a digit or
an unsigned digit or follow integer
followed by a digit
and similarly for the description of a
full full engineering okay so that
sounds great and turns out there's a
third notation cooking at about the same
time the people working on COBOL came up
with a completely different notation to
describe their syntax
they used a two dimensional rotation
they stacked choices vertically within
braces if an item was optional they use
square brackets and the ellipses have
used to indicate repetition of the
preceding item and so down here we can
see that the form of a subtract
statement is subtract then a literal one
or or a field name one take your pick
and then optionally
a literal two and a fuel name - and
possibly morph but separated by commas
and fun and the ellipsis indicates any
number you like and then there's some
other option then you have to say what
you're subtracting it from that's not
optional and then there are three more
optional clauses indicated by the square
brackets what I find interesting is you
study the original report the use of
braces and brackets in this manner is
very carefully documented but they took
the meaning of the ellipses entirely for
granted and never explained it in fact
it's not obvious at first sight whether
they mean the three dots to indicate one
or more copies of zero more copies what
proceeds they don't talk about whether
you need to put in more commas to
separate things
they just took that for granted okay so
there it is in 1965 po1 the p1
specification used a combination of B
and F and COBOL Medan hood notation so
here a couple of extracts from that at
the top you can see that some and some
wonder defined using BNF with very badly
chosen line breaks in the second
production there and on the other hand
they did indicate specifically the
ellipsis will indicate a nonzero number
of repetitions of the preceding item and
I've shown a couple of extracts from the
p1 specification down below that uses
something closer to the Cobo minute
efficient okay
Niklaus beard up the ante in 1965 when
he described peel 360 which is kind of a
peel one derivative and he did a very
interesting thing he parameterised the
BNF by putting parameters and the
non-terminals so he said when i referred
to a k register and that expands to say
carrots are identifiers
that's an abbreviation for a set of
rules I can replace K with any valid
type so I can speak of a long real
register I can speak to an integer
register and if I get a bunch of BNF
rules this is put to good use in the
description of Algol 68 by Adrian fun
vine garden when he described Algol 68
using a two-level grammar which nowadays
called Venn Watling garden grammars one
grammar has an infinite set of
productions so that you have an infinite
set of productions that describe what l
ago 68 looks like but that grammar is
then described by another grammar so
you've got a grammar that generates an
infinite set of grammar rules that set
of grammar rules then describes Algol 68
this is very interesting as a
documentation thing I don't know of any
full blown compilers that were actually
generated automatically from this
two-level grammar in 1970 the bliss
language description was described using
BNF but used a right arrow instead of
colon colon equals possibly because
chomps came self used right arrows when
he wrote his original papers on grammars
they took this notation for granted on
the other hand they did borrow the
vertical bars from from BNF on the other
hand when this became an
industrial-strength language the Digital
Equipment Corporation published that
bliss documentation 1980 and the use po1
style syntax descriptions rifle would be
an F maybe that'd be enough was you know
too academic or something and and po1
was an industrial language we're gonna
compete with that again that's an
invented story on my part however that's
true I tried to stick I will tell you
all kinds of things now try to try to
carefully the steam screen things that
I've actually researched literature and
things that I'm guessing on earth made
up but I'd like to speculate okay
yet another notation in version of the
1970s this it looked like these might
actually supplant be enough for a while
which of these so called syntax charts
or railway diagrams both of these were
provided in your original book on Pascal
rewritten a co-author whose name escapes
me at the moment the idea is you can
start at the left hand and play
choo-choo train and follow the tracks
and as long as you're following the
arrows you can wind your way around the
track you can see that if you choose a
digit there's a dozen paths that lead
you back to the front of the ditch again
to go around that loop as many times as
you want and the idea is that if you can
wander through this railway diagram and
finally come out the right hand slide
all the boxes you hit on the way will
have generated a valid string so this is
another way of describing a production
that describes an infinite number of
possible strings that might be in your
language the syntax charts were also
used in the draft specification of
fortran 77 along with pol and style BNF
and the read language used syntax charts
only describing its syntax you probably
never heard of read but it was one of
four color coded languages green
eventually became ADA in a competition
red was the runner-up and then blue and
yellow actually weren't fully fleshed
out weren't real competitors okay by
1977 ecosphere noticed there are a bunch
of different ways of describing syntax
and that a bunch of variations of B and
F it appeared so he said we're gonna
solve this problem by inventing another
okay so here is V R syntax notation and
it has a couple of interesting features
as designed to be easy to typeset he
said we don't need a : clone equal sign
equals is good enough he introduced the
idea of a specific character he used a
period to indicate where one production
ends and the next one begins so the
specific production termination
character and he carefully quotes that
single quotes puts are double quotes
rather around terminals and as a result
this distinction allows him to describe
the syntax of your syntax notation in
your syntax notation this appears to be
the first instance of a BNF like
location being used to describe itself
and this eventually became popular and
it became an ISO standard in 1996
extended BNF which fairly similar lucky
proposed two decades earlier there are
lots of other BNF variants at home
literature the Stanford sale language
1976 was used BNF except that instead of
using vertical bars it simply repeated
the colon colon equals symbol but
without putting repeating the non
terminals to its left the Alfred project
used BNF but also used regular
expression syntax as part of the BNF so
it melded the two and as I indicated use
of the sharp signage get off
melody the ADA specification used BNF
but it used a boldface is in a boldface
or instead of the colon colon equals in
a vertical bar the CMU and I do
projects these regular expressions but
then change over to using the question
mark Sam Harvison Sam Harvison and I
when we were working on C the reference
manual later I was working on the book
Commons language I always seeing you at
the time in the early 1980s and I was
influenced by CMU style and usage to use
regular expressions in the BNF in both
of those books the Python reference
manual in 1995 uses the star and the
plus sign from regular expressions in
their BNF but rather than using question
Recker optional items uses brackets okay
that's another choice you could make the
Haskell 98 report uses BNF but instead
of colon colon equals uses a - greater
than and it also uses ellipses in its
productions and the Ruby reference
manual in 1998 uses star and plus in
what he calls pseudo BNF it looks a
little bit like the root syntax notation
but also uses brackets for optional
items so different authors make
different choices and the possibilities
there multiple ways of expressing the
productions of the language and of
expressing optionality and repetition
and different authors many different
choices and I find that fascinating ok
finally we have C style BNF which is a
whole nother kettle of fish and this
goes back to 1978 the C programming
language which instead of using colon
colon equals just uses a simple colon
instead of using any symbol to separate
the alternatives they are simply stacked
vertically which might see the stacking
vertically in braces and the po1
notation but with a special rule for
consider the colon then you're allowed
to stack things horizontally instead of
vertically and this is used for things
like describing the operators and this
style of BNF was carried forward through
all the c-like languages including C++
Java C sharp and then also F sharp which
uses something like seesaw BNF but also
uses ellipses in the notation which
which the c style does not also notice
then the C style BNF optionality is
indicated by this subscripted opt rather
than any special symbol okay so yet
another variation okay so the point of
all this is that first of all BNF and
expressions grew separately although
they merged in some uses and we've seen
a huge variety of BNF variations in the
last six decades and it mostly seems not
to have been a problem and I'll
speculate that's because these are
mostly differences of punctuation and to
a certain extent choices between
horizontal and vertical layout but they
all pretty much follow the rule that
first you see the non-terminal then you
see some kind of separator don't worry
about what it is and then you see a
bunch of choices and they're separated
by some separator don't worry too much
about it is and you know there are
probably indications when things are
optional or when you can have more than
one thing or zero more and maybe you
have to puzzle out and figure out which
one's the author chose to do that but
things are pretty much laid out the same
way in every grammar no matter which
style of BNF you're using and so you can
usually walk up a paper and puzzle it
out with it with a menergy
it would be nicer if we if we could have
some kind of standard on that now I
promised you that we return to this one
this is the one I this is the example of
BNF that I took from the Popple
conference back in 2014
and you'll see that instead of using
phrases or words for the non terminals
we're using single letters sometimes
Greek letters and we are using the colon
clone equals in the vertical bar to
indicate choices but notice that now our
principle has been violated the non
terminals are not self-describing
and so as a result authors very often
then put comments on the BNF so you sue
us and so you have to remember okay Oh
II stood for expression taw and Sigma
stands for tights and you and also
comments indicating what the on the
right hand indicating what the different
productions do okay
none of the programming language manuals
uses a BNF like this they all use very
wordy non terminals and they there feel
free to use space but this is a kind
that's used in computer science papers I
spent some time trying to figure out why
and I have a conjecture and this is just
a conjecture my part but it's a fairly
obvious conjecture there's an
evolutionary pressure on computer
science authors to use this very concise
form of BNF nets conference page limits
you're only allowed 10 pages your
conference paper or maybe 12 and
it's grown to become maybe 20 at the
Upsilon conference PLD i think is still
at 12 or maybe 14 you buy two extra
pages but as the as the page limit has
remained relatively fixed and as we are
trying to write more and more
complicated programs in this language
something has got to give
ix result is squeezing language down i
think it's an evolutionary pressure that
applies not just to be enough to other
aspects notation as well and we'll see
that a little bit later in fact this
particular version of BNF also uses the
over line notation in it so it says here
for example one form of a type is an F
and as type arguments it takes a bunch
of Tawes and certainly this axiom
equation or bottom all seas over lies do
indicate repetitions of things it's a
very concise notation to indicate the
repetition okay now there are some funny
things about BNF that have not been
talked about because actually the
specification for B and F hasn't written
down in any one place either you know V
wrote down a version of it but the
version we're using today with over
lines and other things has not been
written down and even some properties
that old be enough we're not described
carefully and so I've extracted from
just intuitive how these things are used
to conventions that I want to numerator
explicitly one is what I call the
consistent substitution Convention now
in B and F we're told that you can take
what's on the left hand side of
reduction and replace it by the string
on the right hand side and then you can
repeat that and so on the right hand
side the same symbol might appear
several times and we understand that
each of those things we can be expanded
independently so if we say that an
expression can be an expression plus an
expression we don't intend that the two
expressions being added have to be
identical they can be different
expressions but if we use that rule
literally in our other uses of BNF that
every non-terminal can be replaced by
any string derived from that non
terminal then if we write an English
sentence such as a value of type tah may
be assigned to any variable of type tah
then we could expand it to a variable of
type let's say int may be assigned to
any variable of type let's say bool and
that's nonsense we don't intend to write
that when we're writing English or when
we're using be enough non terminals in
any context other than the right hand
side of it BNF production we actually
intend that if we write the same
non-terminal we wanted we expanded the
same way at every reference so that we
can you
reference in our writing and refer to
the same thing we require a consistent
substitution if an entre was mentioned
more than once when we have to do the
same expansion for each occurrence so my
English has sentence at the top a value
of type tal may be assigned a very
variable type talk that can expand to a
value of type int may be assigned to any
variable of type int and that makes
sense the other one is the decorated non
terminals convention sometimes we do
want to mention more than one expression
in a sentence and we took the definition
literally that every non-terminal
commute is replaced by a string drug
from that terminal we wrote something
like if tall one equals taut ooh then
tall one is a subtype of taut ooh well
ok tazza non-terminal let's choose it to
be int and reeled is if in someone in
sub 1 equals into sub 2 then in sub 1 is
there some type of inter sub - maybe
that's meaningful and some book that's
usually not what we mean to write
instead we have this decorated
non-terminal convention we can decorate
a non-terminal with subscripts or prime
marks or hats or things like that and
the understanding is that different
decorations imply the license for
different expansion but the same
decoration has to it has to be expanded
the same way so I can expand this I
expect the two occurrences a tall one to
be expanded the same way and the two
occurrences are taught to be extends the
same way but tall one and Tosh tooth can
be different okay so with that analysis
of BNF I don't want to press forward and
discuss substitution in 1932 Alonzo
Church created this notation he used a
capital S for substitution presumably
and he wrote then a superscripted
variable and a sub scripted expression
and then another expression U and then a
vertical barge to serve as a limiting
terminator vertical bar okay and that
meant the the result of replacing the
variable X with the formula Y throughout
the formula u in 1940 when he published
his famous calculate lambda conversions
of this monograph that defined lambda
calculus and he added this additional
condition to replace any part of of this
formula by substituting and for X with
an M provided that the bound variables
are a more distinct both from X and for
the three variables of n so this point
here is the idea of capture free
substitution
and this is the distinguishing
characteristic of this operators we use
it today so nowadays we write e where
where V is used to replace X or
something like that I use the letter V
to represent the value and X tripperz on
the variable so we write something like
that result of substituting V for X in E
but there are some other variations in
puter science literature anyone turn to
heat take a guess as to how many
variations there are take a stab at it
44 and 78 okay the median wins it Justin
just in the papal conference from 1973
to 2016 I documented 32 varieties of
substitution notation there they are the
top five are highlighted in green you
can see that the one I cited on the
previous slide is far and away the most
most popular it's used in about half of
all papers this the same notation but
with the brackets put in front of the
expression sort of afterwards or in
about a quarter of the papers but there
are several other strong contenders one
uses braces instead of brackets and two
of them uses of the maps to arrow or
colon equals one reason colon equals is
partly this Barendrecht used it in his
famous monograph on lambda calculus I'm
not quite sure yet where he got that
from I'm still researching that okay
when I went through and looked at the
other conferences after looking at
poeple conferences I identified several
more okay so now I've documented 34
variations of which and and your formal
reason other sig think ever so I've got
34 and the ones highlighted in blue have
been used within the last five years so
it's not just that we keep trying things
in discarding them the number of
variations has grown over time continues
to grow and of these 34:19 are still in
live use in the last five years and this
is actually what got me going on this
line of research to begin with which is
I found that I was having a harder and
harder time serving is a reviewer of
sigplan papers because another
consequence of the conference page limit
squeeze is that authors are no longer
taking the space to describe their their
notations they are taking substitution
notations and against Genson notation
and the even over line notation for
granted
and I found for each paper I approached
I spending a good five or ten minutes
just ringing okay which variation in the
invitation are they using and I thought
well is it just me am I not keeping up
with the times you know am i failing to
understand something am I getting old
I'm getting old but so I thought well
let's gather some data and I've gathered
data showing that yeah I'm still getting
old but there is evidence there is
evidence that the variety in the
notation is increasing over the years
and the documentation the notation is
decreasing on a paper by paper basis
over time and I think this is becoming a
problem and computer science professors
have confirmed this anecdotally with me
he's been saying yeah we're having
trouble explaining this to our students
and I don't have any place I can point
them that defines the notation okay so
this is what I discovered as of a month
ago breaking news last week I was
analyzed in the late 1980s and I found
five more three of them and functional
programming conference had never
appeared in Popple and I found two more
that were in poeple that I just
overlooked during my first pass I've got
a more systematic way of searching the
papers now for these so that makes 39
times so the guess of 40 from over here
was just about right so far I still have
another hundred syncline conferences to
examine we'll see whether any more
turnout and there are some problems with
this as compared with the BNF notation
by far the most popular form is the
forum II with V substituted for X but
once every five years we see E with X /
V and he still means V is substituted
for X he just put them in the other
order you offer got it backwards and
this wasn't a one-shot thing it happens
once every five years and you can't
count on the names of the variables to
tip you off because different offers use
different names so you can't count on
that either there is one paper published
in nineteen in 2016 that actually used
both forms in the same paper I think it
was a typo I'm not sure I reviewed that
paper or you know it's just hard to
figure out a bigger problem is that
these forms using the maps to arrow in
the colon equals as well as the form
using the slash while they are
frequently used for substitution and the
first two forms are used in about one
sixth of all Popple papers these forms
are also widely used for another purpose
which
seems the same on the surface it's
actually a completely different
operation which is function update given
a function change the value returns at
just one point in its domain to be this
other value that seems like substitution
oh we're substituting this value when
you see this argument you know but no
substitution is marching to an
expression replacing in a bunch of
places function up data is changing a
function in a single way and this is
often used for updating environments or
representation of heaps in your
regimentation using the same using both
of these ideas in one paper can make it
very hard to read and in fact many
authors will use the same identical
notation for both notions they're
overloading the operation and you just
have to look and sit and say oh well
he's applying it a Sigma weight was
Sigma a type or was it a store you know
yeah tell you have to think about it and
lately again most authors are taking
these notations are granted and not
explaining their papers what they're
using them to mean this is I think
measurably making paper's harder to
understand in the last five to ten years
so there are three things one could do
to fix this problem one is that I could
pull any cloth sphere here and say well
do it my way
so here are my recommendations and I'll
let you the the slides are posted online
I'll you look at the details this chart
later essentially I've made specific
choices about about notation that I
think are suitable another possibility
is to choose requirement that every time
an offer uses these notations of paper
he must choose what a different notation
that has never been used before because
then he will be forced to explain it
people have to explain you know realize
it the third possibility is to have a
better technology for reading these
papers and I can tell the tool here's my
favorite notation for substitution
substitute that for whatever this clown
used and then I can read the paper using
the notation I'm used to so that's
another possible way out that that
require a higher level technology than
we currently have in place okay that's
why pretty bad as a subset about
substitution notation there's a lot of
variety it's been growing over time
problems are arising I think we need to
do something about it I'm not quite sure
yet how to fix it now so the over line
notation okay we're going back in
history again in 1484 Nicolas chick'
used an underline 2 group method
medical symbols together in 1525
christoph rudolph used dots so any
rights square root symbol the surd then
dot 12 plus surd 140 he meant that you
apply the squirt with 140 then you add
that to the 12 only then you apply the
square root operation to the whole thing
the dot provides some separation in 1556
star Thalia started using parentheses
for mathematical grouping okay so we've
got three different notations who's
gonna win in 1631 ought read used double
dots to indicate grouping why did he use
two dots when he hate grouping because
he wanted to use one dot for something
else okay so two dots for grouping in
1637 it was Rene Descartes who from whom
we get the name Cartesian coordinates
among other things he attached an over
line to that Serge symbols list a group
together all the things assert applied
to that produced our modern square root
symbol matter survived to this day and
to this day that is the main use of the
over line in mathematical notation his
when is attached to that square root
symbol in 1640 yon stumping used all
three together
he added three things together he
slapped parentheses about him he put an
over line over it and put a dot just to
be sure you know he wasn't taking any
chances
didn't what have even understood on
shoten went while editing the works of
the math addition v8 ah chose to use
over line throughout that manuscript for
grouping and Leyden it's picked up on
that Gottfried Leibniz in all his early
work used over lines for grouping
mathematical symbols the starting in
1702 he decided to says he decided to
start using parentheses instead and he
urged everyone else to change over also
and his said it was for mechanical
reason said let's make it easier on the
typesetters the over lines require
vertical stacking of the led type within
the matrix on the printing press and the
parentheses only require horizontal
stacking that's they're just like
letters that's much easier for them and
in fact a major journal of the time octa
Arruda tourim the acts of the erudite
you gotta love that title
okay rooted forum officially adopted the
line it's e'en symbolism and said for
now on reason parentheses this is then
popularized by Euler into of the
Bernoulli's and set starting in 1728 and
a Florian kajori in his history of math
connotation says is almost certainly
because of Euler as
the bernoulliÃ­s that we now use
parentheses today but you couldn't have
taken that for granted the 1700s and
look like parentheses are going to win
but no in 1857 just set the piano the
same guy who gave us piano arithmetic
reintroduce dots no you're into the dots
in spades so use different numbers of
dots to indicate different amounts of
binding weakness so Lennie writes a :
BCD he means the B and the C have no
dots they stick tightly together then
there's only one dot next to the east
that gets joined in next and now the two
dots okay now you can join the a so a
number of dots equals binding weakness
and it might have died with that but
then Russell and Whitehead when they
wrote principia mathematica said okay
we're gonna use pianos dots and that was
the last major work I think to use that
that is still an important mathematical
work and it's hard for us moderns a
century later to read that work because
it doesn't any parentheses we have to
sort out this dot notation so for about
five hundred years there have been three
competing notations for grouping and
right now it looks like parentheses are
winning you know but something could
happen it's hard to know okay now the
the over line also plays a role in
vectors and I need to aggress lightning
two vectors because it plays a role in
in our understanding over lines in
modern times okay in 1813 argon of the
Argand diagram graphed complex numbers
on a cartoon cartesian coordinates and
he spoke of the square root minus one is
inducing a rotation in the plane he was
thinking about of geometrically and he
proposed this notation a B with an arrow
over the two symbols to indicate a
vector where a is the starting point and
B is the ending point notice the use
there's not only over there's an arrow
to indicate vector Ness but there's also
a grouping function there was grouping a
starting point and ending point
okay twenty years later Willian william
rowan hamilton recast the theory of
complex numbers as an algebra on pairs
of reals at this point imaginary numbers
were still somewhat controversial what
does the square root of -1 mean anyway
you know how can i have spurred of -1
bushels of wheat you know all those
arguments he says let's just avoid all
those problems by not worrying about
what the square root of -1 is instead
just saying we've got these these things
that are pairs of reals and we'll define
an algebra on those and we can define
describe how
they add they add element-wise we could
scribe how they multiply there's a
complicated formula and it has all the
same properties as complex numbers but
you're not arguing about what is the
square root of -1 in that way yeah at
least not directly there is this pair of
numbers which if you multiply it
surprised minus 1 comma 0 pops out but
somehow it's a little less threatening
that way so it's very successful with
that he said that's great well let's
let's embark on this program and
generalize it if we can add and multiply
pairs of reals we ought to be able to
add multiply triplets and quadruplets
and so forth let's start with triplets
and he spends the next 10 years of his
life beating his head against a brick
wall trying to figure out how to
multiply triplets the addition is
obvious you do it element wise but how
do you multiply them there's this famous
anecdote about him suddenly having an
insight and carving an equation on a
bridge in Ireland as if she was walking
about he had discovered the quaternions
and his insight was yeah you need three
things instead of having a real number
and then something labeled with I the
squared minus 1 we need 3 square roots
of minus 1 I J and K and we also need a
real number 2 because when when you
multiply by I you're gonna get a real
number out minus 1 but he said this
allows me to focus on IJ and K as
triplets and yeah there's this real
number hanging along for the ride and
then he subsequently reformulated the
quaternions without talking about
individual coordinates he just said he
described a quaternion as a sum of a
scalar and a vector a three-dimensional
vector in this vector had imaginary
properties you know it was sort of the
combination of the I the J and the K and
James Maxwell when he formulated
Maxwell's equations in rotovap actually
described using quaternions and he said
yeah well we can get this to work it's
the notations a little awkward but we
can make do you know so we kind of
dammed it with faint praise in 1881
Hosea Willard Gibbs established the use
of the center dot and the X for what we
now call the dot and cross product for
describing the algebra on quaternions
when we formulated in this way without
the individual ijk coordinates and then
in 1882 a physicists jumped into the
fray Oliver Heaviside and he said well
yes we can use quaternions we find that
these scalars are always getting in the
way and it's the vectors who are
described that are describing the
physical part of what we're actually
went and talked about
things like electrical vectors and
magnetism vectors let's just ditch the
scalars and just use the vectors and
there was a big howl of outrage from the
mathematician they said if you just use
the vectors then the algebra isn't
closed and well the cross-product isn't
really a product operation the dot
product search by itself is not a
product and it doesn't have this
wonderful rich algebraic theory and the
physicists said we don't care about
algebraic theory we're just trying to
describe the real world and vectors do
just fine thank you very much and
between 1890 and 1894 there was a big
fight between the vector ist's and the
quaternion as' and you can find the
evidence of this in the published papers
there were they actually wrote very
vitriolic paragraphs sniping at each
other and impugning each other's
intellectual credibility for being on
the wrong side of this debate and it's
really fascinating to read nowadays so
there was this big fight and the vector
ists eventually won out and that's good
for us computer scientists because
quaternions are kind of a one-hit wonder
quarter nians aren't quite as good as
complex numbers because the
multiplication isn't commutative and
that was actually one of the criticisms
of the quaternion theory it's not a real
multiplication because it's not
commutative and it turns out there are
generalizations of quaternions but they
have to do with doubling the number of
components rather than continually
adding one so you can think of a
quaternion is being just a complex
number as a pair of real numbers you're
getting the quaternion is being a pair
of complex numbers you can repeat the
construction this is called the Cayley
construction if you take quite two
quaternion you get something that has
eight components that's called knock
Tony n' and that has a reasonable Alger
too except it also turns out that the
distributive law goes away and
multiplication isn't even associative
and by the time you get to 16 all the
algebraic properties have fallen apart
it's useless
ok so occasionally use octo nians
resigning funky quaternions are so used
in computer graphics today because a
great way to describe rotations of the
camera that avoids gimbal lock and so
and so they are that the quaternion a
theory of Hamilton is still used today
and has its applications but for the
most part we have decided on vectors and
the nice thing about vectors is they
generalize to any integer number of
dimensions you can have a three
dimensional vector four dimensional
vector a seven dimensional vector a 23
dimensional vector that's great
okay now comes a gap and by the 1950s in
computer scientists are talking about
things like interrupt vectors and dope
vectors to them a vector is just a
sequence of integers in memory you know
our sequence of numbers in memory never
mind that it doesn't transform the way a
physicist thinks of vector transforms
you apply a matrix to it and it rotates
in space and things like that a vector
is just a list of coordinates and I've
had discussions with actual physicists
in the last three or four years a nice
you know he's a visit we're trying to
find some common ground to talk about
vectors he said what are you talking I
said well it's this sort of this I've
got a list of numbers here maybe two
lists of bits because how do you take
the dot product of that so the language
is growing apart this is a language of
balls and and words are borrowed from
fuel to feel but nowadays when we talk
about the vector it means just about the
same thing as an ordered list and if you
look at the evidence in poeple I've
skipped over about 20 years about 50 60
years there rather in the history
because I just haven't had a chance to
research that but but in the 1970s both
a the symbol a with an arrow over at the
city will a with over line over it or
both used to denote something like a
vector or a list or a sequence or
possibly a set and the offer is usually
good about saying what he means that is
enclosed so a arrow means this list of
coordinates with you know inside of
angle brackets but by 1981 this
overlying notation begins to be taken
for granted Nagas are bothering to
document it anymore
by 1989 we see the same symbol but used
for an unenclosed sequence it's just the
a comma B comma C dot on the last one
but it's actually important there be no
parentheses around it because other
symbols are surrounding that and the
parentheses might be there but so far
this notation has a fairly simple
semantic model
which is an over line marks of variable
as representing a sequence and the
obvious syntactic model that you can
make of it is that you just make copies
that over line name and start attaching
subscript starting from one so a with an
over line over it means a sub one comma
a sub two comma a sub three as many as
you like down to a sub n okay simple
model for it in 1990 we find the first
first explicit claim that the elements
may in fact be metasyntactic variables
that you can put over line over for you
a BNF non-terminal and expect to get
copies of that non-terminal which then
in turn can be expanded by BNF
productions also 1990 we see the idea of
what I call an implicit unit replication
this author said when I write em over
bar : Sigma over bar that means yeah
I've got a bunch of M's and a bunch of
and print expressions and a bunch of
type Sigma but I don't mean that this is
an abbreviation for all the NS are read
by commas as a glue : and then a bunch
of Sigma's that really stands for m1
colum Sigma 1 and M 2 : Sigma 2 and so
forth there's a problem here which is
that what is the unit of replication
that's kind of left implicit there in
1993 we see the first claim that over
line may apply to any syntactic object
that you can use the over line to cover
a number of symbols and that is the
group that gets copied that's great
what I find fascinating is that over
line was in traditionally introduced
into mathematics as a grouping symbol
then it kind of mutated through vector
notation got used to group together the
start and end point of a vector then we
kind of generalized and morph the idea
of a vector just be any list of things
and then we used over line to indicate
that list of things and now it's
reacquiring it's grouping function here
in the 1990s so the wheels keep turning
but the problem is when you put put an
over line over a group of singles
symbols now this question exactly where
do you attach the subscripts you make a
bunch of copies of fragment and attach
just one subscript at the end of each
copy well no this author actually one of
these subscripts attached to both the
ends and the Sigma's but not to the
colons why should that be well you and I
can easily invent five different reasons
why that should be true nobody ever
wants to subscript a control : well how
do we know that well that's rich
semantic knowledge it's easy to come
with reasons but so far no one's
actually written them down this is an
undocumented language in 1994 we see the
first use of nested over lines in 1996
we see the first explicit statement of
another implicit convention in the
language implicit until now we
implicitly assume that when we write
this substitution operator a bunch of
Z's substitute for a bunch of why's the
number of Z's number-wise is necessarily
the same
seems like a reasonable thing to assume
but this offer actually took the trouble
to state it and but by 1997 we see a
statement of a principle where this
alliance is trying to document the
behavior part of language by abusive
notation operations on Singleton's will
be implicitly extended point wise to
sequences we immediately run into a
problem because that same author wanted
to write this gamma applied to a bunch
of bees equals the result of
substituting a bunch of T's for a bunch
of X's in a bunch of peas well if we
treat both the Equality operator as up
being point wise extended we seem to
what the author wanted and the
substitution operator then this says
this is an abbreviation for this this
conjunction of claims the gamma of b1
equals t1 substitute for X 1 and P 1 and
so forth and gamma of BN equals T n sub
suited for X n within PN but turns out
that's not what the offer intended if
you read the paper carefully in DP you
see what was really intended was yes
gamma sub e 1 equals the result of
substituting in P 1 and gamma B ended
the rules of substituting in P n but he
actually wanted the same substitution
operation applied to all of them where
that substitution operation itself has a
bunch of T's in a bunch of X's so it's
actually kind of a 2-dimensional kind of
kind of repetition and then a patient
fails to capture that so a possible
solution is to nest the over lines and
some offers would have written that same
equation this way instead to indicate
that the T's and X's share a common
length and there are bunch of them and
then that substitution operation it gets
applied to P and then they're a bunch of
such equations so superficially there
seems natural with a couple problems one
is how do we know that the gamma is not
supposed to be subscript in well we can
do a dimensional analysis and say well
from other considerations in the paper
we know that gamma is daren't a bunch of
gammas you know it's a scalar we know
they're a bunch of using a bunch of keys
because there talked about elsewhere
this requires a deep semantic knowledge
of what's going on it's not something's
in the syntactic notation and in the
last 15 years if you look at other
papers we have found that both of these
usages have become live and they are in
a central contradiction on one hand
authors want to write this nested over
line annotation at the top which is
simplification the previously
to mean a bunch of equations each of
which involves a substitution and each
substitution involves a bunch of
variables and a bunch of replacement
values and the author intends that all
the substitutions be the same the same
ception be applied to all the Q's to get
to all the peas on the other hand they
also want to describe the case statement
and a case statement has an expression
than a bunch of clauses so those clauses
are indicated by an upper over line and
each Clause consists of a constructor
name that a bunch of argument variables
and then an Associated expression for
that case Clause but the author intends
that each constructor can have a
different set of argument variables
possibly different number of variables
and with a purely syntactic theory you
can't have it both ways
you know you offer different authors
trying to use these the same notation to
mean two different things something's
got to give so I sent myself to thinking
how can we cure this problem can we come
up with a notation that will be used
useful by both sets of authors for
describing both things so the question
is what do we want from a formal over
line notation well we want the way to
write an overloan line over any string
that we can get any number of copies of
that string probably separated by commas
we can worry about that later
every copy we want to be expanded
differently typically within each copy
the string multiple occurrence is the
same be enough term will need to be
expanded in the same way as usual on the
other hand if over lines start selves
mentioned more than one say in an
English paragraph then each such
expansion has to be the same because we
want this property of reference also we
like backward compatibility with a
vector notation if we just mention a
variable V that's not a BNF non-terminal
with in string then we want copy I of
the strength to refer to V sub I and
finally all variables occurring in the
string must have the same length we want
the same length notation to be enforced
so what are our principles what
principles explain the set of rules
which may have seen rather arbitrary as
I aluminum rated them on the previous
page so I tried to extract the
principles behind these desiderata and
it seems to me that there are two
principles one is that if a notation
appears explicitly more than once in
more than one place on the paper or on
the screen then we want all such
explicit copies
given the same expansion this preserves
the ability to reference so far right f
of e and e and e that means that all
three arguments really do have to be the
same expression E and there's a way to
override this default by decorating the
non terminals I can say F of e 1 e 2 and
E 3 that means yes they can be three
different expressions on the other hand
if we make a bunch of copies of
something by using the over line
notation then it's convenient to let the
copies be different when we write F of E
bar we really mean F takes a bunch of
arguments freshens and we would like
them all to be possibly different and we
have a way to express this by decorating
the non terminals and this plays nicely
with the backwards compatibility theory
if we say that yes x over R really does
mean X sub 1 X sub 2 X sub 3 then if we
expand f of E bar into f of e 1 e 2 and
E 3 then the decorated on terminal
convention takes care of the rest and
says that yes the expressions can be
decorated differently can be expanded
differently the problem with the
notation so far is that we have no way
to override this particular default this
is the source of the problem
we can if we write under haitian in more
than one place they by default got the
same expansion but we have a way to
override that if the copies come from
the over line notation by default they
have different expansions and we have no
way to override that i perd propose to
introduce a way to override that and
tell us all that I'm gonna borrow an
idea from quasi quoting if you know
scheme you know that when you write back
quote and then a list lambda VARs body
but you put a comma in front of ours in
the body that means make a copy this X
expression but the comma means except
here use the value of the following
expression instead closure o has a
similar notation we uh Danny Hillis may
also use a similar idea for peril has
been common list when we wrote an alpha
ment do the following expression in
parallel that is execute many copies of
it but the bullet means except here the
value of this expression is actually a
vector used one element use different
elements from the vectors in the
different parallel copies the expression
this gave you a way to kind of Express
parallelism so I propose to add under
Alliance the over lines
and yes that may be silly but you'll
there's there's an underlying powerful
principle to choosing that particular
method of override so I propose that
Overland stir means you can have any
numbers what copies of the stir and each
copy the store may be expanded
differently but an underlined means
except here the underlined portion has
to be expanded the same way in each copy
and turns out this is sufficient to get
most of what you want so for examples we
can write this I've got a big over line
saying I want a bunch of copies of this
equation P equals Q with some
substitutions and also there are a bunch
of these things have to do for a bunch
of X's too then I put an underline into
that substitution bracket mean I want
the same substitution bracket in each
copy of the equation on the other hand
for the case expression I don't use an
underline and that means I've got a
bunch of case clauses and each one has a
bunch of wise and those wise those Y
expansions can all be different if I
write something like over line gamma
implies that X has typed aw I can
underline the gamut indicate don't
subscript this guy there's only one of
him just subscript the X isn't macaws
and you can use them in more complicated
ways as shown in the fourth example the
unifying principle is that the
dimensionality of each variable is
simply the number of over lines minus
the number of underlines so I can just
glance at this and say ha gamma is a
scalar there are not multiple copies of
gamma it's got an over line and
underlined there's just one of it but
there's two copies there but X is one
dimensional X in the third equation the
lowercase X is one dimensional because
it's gotten over line no underline the
capital X in the last equation is one
dimensional they've got two over lines
in one underline on the other hand Y is
two dimensional it's got two over lines
and no underline so the dimensionality
is exposed by the notation you don't
have to guess it so there are I'm going
to wrap this up there are one can invent
a Mechanics for formal over line
notation I've been trying to come up
with a formal description of this over
line notation and it's theory and I'm
gonna skip over that a little bit you
can look at the slides online and for
cases that simple over lines can't
handle then you can introduce explicit
subscripting variables and even provide
ranges to describe the bounds on those
on those index variables which is
something is also already common in the
literature I'm just saying that if
simple over lines can't handle
even with the underlines you've got an
escape hatch I'd like to spend about
three minutes talking about the ellipses
Roland Backhouse in 2003 most readers
will encounter this dot dot notation
already it is a notation that is rarely
introduced properly in fact I found very
few papers that you can bother to
explain the dot notation most states use
that explanation the readers intended to
infer what's missing in the past we've
used the ellipsis to explain overland we
frequently find explanations of the form
x over bar means X sub 1 X sub N and
you're supposed to infer that okay ends
an integer and we're gonna fill it in
with consecutive integers and the
distance between the integers is 1 so
it's X 1 X 2 X 3 X 4 and so forth but
there's no explanation what the dot dot
means or what do we mean when we write X
1 comma X 2 comma dot dot is that
different from X 1 X n what if we write
e 1 dot e sub I dot dot dot e n what
does that mean hmm is that binding I
that referring some I have already
mentioned I proposed to explain the
ellipsis notation by providing a formal
transformation to the over line notation
so I tend to invert our order of
definition I think we can come up with a
formal description of the over line
notation it'll completely explain how it
behaves without using ellipses anywhere
in that description and then use over
lines to explain how ellipses behave and
the idea is to provide a set of standard
ellipsis usage patterns perhaps some of
them could even be user-defined and for
each case of ellipses just identify
matching usage pattern that includes one
or more ellipses some number of copies a
separator string and some matchable
strings and then use unification on the
matchable strings and just let me show
you an example when you write X sub 0
dot X sub n minus 1 well there's an
ellipsis there next to the ellipsis is
the obvious separator comma and you have
to pattern master define figure out
where that comma is that separator is a
single symbol in this case it is the
matchable strings are x sub 0 and x sub
n minus 1 if you do unification the
match the common structure is X X sub I
with a substitution 0 for I and n minus
1 for I and then there's associated code
with this rule that says and then you
construct this and what you construct is
and over line a an explicitly subscript
over line notation x sub i where i
ranges from 0 to n minus 1 and that may
seem trivial but we have fruit now
provided a formal definition of what
that dot notation means so to wrap it up
I believe that computer science
meditation is a programming language
it's become the most popular programming
language in computer science it has a
very distinctive syntax semantics and
idioms but I think it's been neglected
the standard proverb is that the
Cobblers child goes barefoot and
computer scientists have had this
programming language growing under the
noses for last 40 years and they're not
treating it as one I think it's time
we've applied all the standard tricks
and tools of the programming language
designers and definers and implementers
trade to this language in order to
better support its use I think it's
being explicit object to study it can be
an explicit object of historical study
it's got roots that go back centuries
specific problems have arisen with
substitution over lines that are causing
us trouble today
these can be fixed I think we should
develop a formal theory the language I
tried to make a start and I think there
are interesting opportunities for
parallel execution of computer science
meditation that might be used to express
parallel algorithms for other
applications not sure yet and I invite
you if you have a favorite parallel
language or favorite functional or pure
language think about how you might map
computer science meditation onto your
language and perhaps thereby provide an
implementation thank you
I've run a bit over I'm afraid but if
you're willing to sit here and listen to
answers I'm willing to take questions is
it correct to say that CSM is actually a
logic language and each rules actually
is very similar to a horn class yes yes
the question draws draws an analogy
between computer science education and
the horn clauses we see in horn Clause
logic programming languages such as
Prolog and yes there is a very close
analogy there I think it is stylist we
used in somewhat different ways on the
other hand as an experiment I did
construct a toy compiler capable of
translating a subset of CSN with the
overline notation into Prolog and then
ran those Prolog you wouldn't you
wouldn't have the same execution model
asprova Lucas Prolog as sequential and
here you have non-deterministic O's it's
more like like concurrent Prolog right
yes
well Prolog made made a specific
commitment to executing in a specific
kind of serial order using backtracking
in order to solve it in order to make
these noises there are other ways it
implemented non-deterministic choices
concurrent Prolog is one you can treat
it as a constraint language is another
possibility but the point is is yes as a
demonstration I have provided a toy
implementation of a compiler and once I
compile them into Prolog you did just
work I don't think I've were able to
compile more than one such program it
was the seven rule program that I showed
you on the slide of interest is the fact
that this compiler was written in Emacs
Lisp on the ground stood on the grounds
that it was the target language Prolog
the source language was my low tech you
know source code for these for these
stuff because I want to be able to say
that yes I compiled the very stuff that
I typed such so so could one way to make
some progress forward on systematizing
this be to develop a low tech package
which operates at a relatively high
semantic level for describing this
notation think think math parts here but
more so and then journals can
individually choose style files which
types that all of the papers within the
journal using a consistent notation yes
that's a great idea
latech has become a very commonly used
language for writing these papers and
for implementing this particular
notation and there are all sorts of
wonderful things we could do with left
tek if only we could get it to do some
extra stuff on the side and
other kinds of metadata for example we
might be able to make these PDF files
searchable for this notation some have
suggested well maybe just go to a
different location
Gervin latech such as math ml or
something that's another possibility -
it's really hard to displace the current
latech infrastructure because it's so
widely used and is proved to be very
valuable but i'm not quite sure how to
push that forward within the latech
community to get other kinds of metadata
out as a side-effect of the processing
partly because what the tech is already
doing being programmed in tech which is
self was never designed to be a
programming language it's got this very
crazy macro expansion semantics of
execution and it sort of kind of hard on
meant that in a way and then get get the
large infrastructure of the tech
libraries recoded to support all that
love to see it happen I think will take
a while yes hi
so I've actually been wondering this
since undergrad why do you have care to
speculate why isn't this taught to
students in particular why do people
saved our professors trivialize it and
leave the students to just figure it out
for themselves I don't know why
professors aren't teaching it it's one
of the things you need to learn just
like there are other parts of leaders
annotation this notation does nothing
new widely taught I see a hand here is
perhaps in response it will ask a
professor why don't you teach we have a
pressure just use it so my theory is CMS
is a little bit like English we don't or
you English speakers we don't teach
English very much you use it it is it
has kind of an obvious meaning or
there's enough exposure in your life so
that you don't need to teach it and
something like that this is happening
here and I have been one I have come
across of course the problems that you
were showing so second point but in most
cases I think we I wonder whether it's
not the case that in most cases we know
enough we have enough semantics
semantics in your head that we can't
disambiguate I think you're right and I
think they're also made just the element
of the Frog and in the pot of water
that's being gradually brought to a boil
you know each little tiny stuff doesn't
seem like a big thing and maybe a loose
mo the reader will understand what I
mean or are surely they read the paper
last year that explained this but we
reached the point where I think it's the
water is boiling and we got a big
problem
lemon I'm kind of raising a stink about
it hoping we can do something about it
two less questions yeah okay not real
question with a follow-up on that one I
have a math background and then I move
into engineering and I never
uncontrollably came to science a lot
so when I read paper now interesting I
do not have this background and
knowledge coming from major information
on the notation and I do not have a
reference on it because there is no no
my friend so it's may be interesting to
maybe teach it or at least produce
something about documents that explain
it not only for teaching before people
that leaving inside of it but for people
that are outside and have to read these
papers sometime so to play the devil's
advocate if we're sort of using this
language that was originally supposed to
be very simple and informal and now it's
grown into something that's very
complicated and we need to make it
formal doesn't that sort of mean that
the purpose of this CSM has turned from
just a simple description that everyone
can understand into something that's
becoming a beast on its own is that is
that what we sort of should be doing yes
well taken maybe we can slay the beast
by increasing conference phase limits to
40 pages you know and then we won't need
to use this very dense notation on the
other hand there's not you made that
when we do these things for conciseness
once we get used to them they improve
readability because we can take more in
at a grasp this is I think to some
extent Iverson's argument when you
you know have you compared your analysis
of computer science met a notation to
PLT read X's pattern matching languages
yes I have
you've mentioned PLT read X and there is
another package called I think the cave
package or something that provides
something like at the K framework that
provide things that are kind of like
inference rules and indeed PLT Redax
does provide an inference rule structure
and provides some of the other aspects
in notation but with a very different
syntax and a rather different grammar it
doesn't ride the overlay notation at all
it uses ellipses in a rather different
way and it's a very useful language in
its own right is a close cousin of this
notation but they're about as close
together as French and Italian or as
pl-1 and Fortran and that's not the
dialect we're actually using computer
science papers right now maybe the
solution probably get everyone use PLT
rhetorics and another notation okay
that's here for a guy Steele thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>