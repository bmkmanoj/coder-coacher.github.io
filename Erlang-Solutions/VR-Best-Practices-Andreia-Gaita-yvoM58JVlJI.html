<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>VR Best Practices- Andreia Gaita | Coder Coacher - Coaching Coders</title><meta content="VR Best Practices- Andreia Gaita - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>VR Best Practices- Andreia Gaita</b></h2><h5 class="post__date">2015-11-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yvoM58JVlJI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah welcome to virtual reality best
practices my name is Andrea gaita I am I
work at github I am a mono developer all
doing too much mono these days but I do
still do see chef so that's okay i
worked at Novell in summer and then I
moved on to unity which is game
development engine platform where I got
kind of bit by the game development bug
because it's fun and so these days I
might github and but this is how I end
up here doing a VR talk which is not a
thing that i usually do so this is going
to be first time I do this and it's VR
is not easy to demo so let's let's see
how this works okay so VR is a platform
like any other platform it's it's a
platform like a console pc or mobile so
what you need to do when you need to
always be aware of is when you're
developing for VR your you have to know
what we are you're developing for so
you're the content creators or you're
the ones that are building the content
or if you are without which VR does not
exist and there is very little content
right now because everybody's
experimenting consumer versions don't
exist or barely exist so we are just
still figuring out how this works you
need to always be aware that you're
developing for certain type of hardware
with certain type of restrictions and
restrictions are various rvr all the our
devices of them other submissions are
specific to the type of year device that
you're going to be developing part so
you it's very important to know what
you're going to have on hands or what
your users are going to have on hands
and I'm not going to although almost of
the arse is oriented towards games it is
certainly not the only type of
experience that you can have on dr
there's a lot of companies building
architecture experiences for vr because
architectures
don't get to see the things that they
built design until they're built and by
then it's too late so they are very
eager bunch to have some way of being
able to visualize their designs in 3d
education and therapy experiences have
been built for VR so there's a lot of
content that that is possible more than
just games but it all depends on what
people have X accessible so we're going
to start by looking at the earth places
so this is this is the oculus rift
that's the consumer version this is the
this is the decay to the development kit
it is basically I this version it is a
galaxy note screen with two lenses where
you can stick your eyes and see things
and HDMI connectors and it's a very
simple the VR itself doesn't do much
beyond having a bunch of sensors to
track the position of your head
basically in that gets fed into the
computer and to do the vo to create the
stereoscopic 3d experience what you have
is two lenses two cameras for your two
eyes so when you're doing vr you are
creating one scene and then rendering
that scene twice one for each are
slightly separated to compensate for the
distance between your eyes and this is
one of the very important factors is the
distance between your eyes everybody has
a different distance and that affects
your the 3d sensation if it affects how
you measure depth and distance to
objects so most oculus has a
configurator where you can configure
your ipd which is the injured popularity
distance between your two peoples so
it's part of your profile so when you
play games you play
with your profile and it's configured
for your eyes and everybody should go
through a configuration for this for
this type of data so if you the device
that you're targeting doesn't get this
data because for some reason they don't
include a configurator in their
standards SDK you should totally get
this this data also height of the player
because you're building experience
that's usually pretending to have a
thing here so if the height of the
character in game doesn't correspond to
the height of the player and that it is
supposed to correspond then you but
you'll notice it's very it's very weird
when you look down and see things that
are not where they're supposed to be so
there's two cameras two eyes the
consumer version is going to have two
displays instead of one and what it
basically does is to cook to give you a
while feel wide field of view the lenses
distort the image to open it up the
problem then is that you get this sort
of distortion basically so you can see
it's it's just distorted in a barrel
distortion because the lenses do a
pincushion distortion to give you a the
field of view and then you have to
inverse distort to be able to get things
back to a proper viewing way basically
the oculus has 110 degrees a field of
view most devices are targeting the same
type of field view which is not as wide
as we have but is wide enough to if your
if your content is then properly you'll
forget that there's areas where you
can't see the resolution on these things
is let me check because I never know
this by hearts the resolution is 2160 by
1200 for the for the consumer version so
that's 1080 x 1200 for each eye and I
just lost my mouth
sorry guys weird so that's um 1080 x
1200 for each eye and it's actually
being rendered slightly bigger because
because of the distortion so it needs to
actually render more than what you end
up seeing which means there's a lot of
pixels being pushed which means
performance is going to be really
important you're not doing one view
you're doing two with a ridiculously
huge resolution and any slowness is
going to be detected by you users your
eyes your eyes can totally see any any
flaws of things not being appropriate so
the most of the devices have three
sensors to be able to track your head so
in in terms of rotation so y'all pitch
and pitch and roll basically so that's
in the sensors that are included with a
with all of them including your phone so
your phone can also do this this type of
tracking but to get full six degrees of
motion which means leaning going lateral
positions with your head without getting
in just with those three sensors is not
possible so you have to have external
data to be able to track these motions
so the dk1 only has the the three
degrees of tracking but the the dk2 has
a little camera which is any favorite
camera which goes in front basically
like this and there's a bunch of
infrared markers on on the faceplate so
when when you move within the view space
of this camera which is not that big but
it's going to be bigger in in the
consumer version it can track which
means you can look under things and you
can look around things which means that
when you're doing content or doing the
game you can't forget that even though
you can't see the thing in front of you
the user is going to be able to look
around even if you don't allow them to
walk beyond crates there's a crate in
front of you there's the thing you can't
walk around that thing but you can
certainly look around
and a lot of GUI tools for making games
optimize the rendering but cutting
surfaces that are not visible on that
view forgetting of course that the view
can go around so there's a when you're
making content there you just making the
scene and putting things on the screen
you need to be aware that the
optimizations that are normal for a game
might might actually break some some of
the objects in the scene so you need to
test these things to make sure that it
doesn't break this is the Crescent Bay
prototype of the oculus you can see the
markers very clearly there are markers
in the front and back because so that
when you can turn it can track your head
when you turn around because if this one
only has markers in the front as soon as
you turn to the side there's no more
tracking and it's very jarring when it
loses tracking it jumps so you want to
not lose tracking something interesting
that is audio is always present in games
not it's never present in in other types
of applications but it is surprisingly
important when you're in versed in an
environment where that space is your
space soon as you get into the oculus it
becomes your space your brain is
completely fooled by the space which
implies that you can send you can send
positional audio to trigger things and
and the years will will know where
things are coming from so there's tools
being built for positional audio where
you can activate sounds in different
areas which will give your user a sense
of if they turn around the audio is
going to change or if something happens
over here something is going to change
it's it's just another area that is just
growing there's not that many music
artists or audio people not even people
that know music but audio people they
can they can do a good audio experience
in the gaming industry so it's it's very
interesting thing too that's growing so
that's the oculus so valve is doing
there on VR thing
and they're building the vibe on with
HTC so it vive is the school the vibe is
the same thing same resolution the
difference with the vibe is the way they
track your position because in VR all
about tracking if if you're moving and
the experience is not moving with you it
you're broken and you're going to be
sick at least or not use it at all of
course so tracking as accurately as
possible is like the most important
thing and those holes that you can see
or sensors so the vibe is covered with
37 sensors that are picked up by two
laser stations that this is the part of
a manual for the the vibe that the valve
released so you can see what how it's
going to be so basically have one sensor
on one side and one sis or in the other
and it's tracking this area so it gives
you space so this immediately tells you
that oculus is going for a more sitting
position in terms of the experience you
have a camera they do have cameras in in
tripods but most of like the experiences
you're sitting down and you have a
you're doing things sitting down usually
valve is going for Mauer more movement
experience I tried the fruit ninja demo
with the vibe where were you you know
slicing fruit it's it's a lot of fun
because everybody that puts of your
thing on basically wants to get up in
two seconds afterwards because the you
want to move and you want to look around
and looking backwards when you're
sitting down is complicated so it's very
natural to just stand up oculus does
some demos where the chairs move around
so it's easier to rotate but this is
very likely going to be a thing that is
going to be very popular then we have
the gear so this is the gear
vr so as you can see it's actually more
polished than the than the oculus it's
kind of nice straps and everything that
are adjustable so the gear is different
because the gear is mobile is all about
mobile experience where you're not
attached to cables that run to your
computer you're attached to this which
is attached to the base plate this is a
samsung s6 it only works with something
like six at least this version I don't
know if what are going to do for the
next version but likely they're going to
select a certain phone that meets their
respects and and they're going to use it
so as soon as you attach it it runs a
home screen that is a 3d home screen
where you basically just select what you
want to do it has a cinema and your
experiences and it has games what it
doesn't do is have tracking on six
degrees in motion because there's it
doesn't have external cameras to track
this so we can only track what the phone
tracks which is the rotations and
acceleration basically just 23 degrees
it works surprisingly well and the
resolution is slightly lower but it's
perfectly fine screen and it has a store
already so can actually buy games and
play things with it and it has a
touchpad so you interact with it by
going by going like this on the side of
your head which gets a bit tiring at
some point because you're banging your
head but it does work well and the
problem with having the the phone here
is that it heats up because it's a phone
and it's rendering 3d so it's it's kind
of work a lot there's a mini USB port
under it so you can actually get a have
it plugged in and charging so I have
my pet rock which is a USB battery you
can have anything it doesn't really it
it's it's a phone that's pulling a lot
of energy so you want to have a battery
that gives it a fairly good power the
other thing because otherwise it's going
to die very soon the other problem is
that if the phone overheats is going to
fog up the lenses because it's going to
because it's going to be very very hot
so the good thing is because you can
connect you can connect it connect the
phone via the the gear with the USB
battery that you can put it in pocket so
you can still be mogul it will take some
of the load out of the phone so the fun
doesn't have to power the whole thing
it's going to use the battery to to give
us some juice which is enough to get the
heat down so it doesn't up the
lenses so the the gears it has a pretty
sure it has a fan in there to make sure
the phone doesn't really really overheat
I never had a crushing or really going
battle as long as it's always connected
to power it can run fine in it doesn't
have any problems I've had six hour
sessions with this thing and it's it's
really comfortable so these are the tips
for at least the this one is definitely
a tip that you should always be aware of
the higher the CPU especially on a
mobile the hotter gets the worse it's
going to be others with functions shut
down or it's going to up keep the
original cable if you play with a gear
VR because the the gear doesn't like any
other cable it will complain so don't
lose it it also has a developer mode and
for you the developer developing for one
of these things this is important and
it's hidden away you can trigger the
phone into developer mode for the gear
which means it will run without being on
the gear itself the phone is going to be
rendering as if it was on the the gear
this is because when it's not running
normally if I don't have my face in
there it's not doing anything there's a
little sensor here there's a little
sensor here that the text that my face
gets close and as soon as I put my
finger on it
turns on the phone on but that's really
annoying for developing because you
don't want to have your face in there
all the time when you're testing so you
can enable developer mode and it will be
able to you don't have the tracking data
of the gear the sensor data but at least
you have the screen so you can see
what's going on you can also this is an
android phone so you can do all of the
things that enter a development allows
and you can there's also a monitor app
so you can see you can connect to it and
get data of rendering speed and latency
from the phone when you're when you're
running wirelessly over the IDB protocol
so let's go through quickly through the
rest of the stuff Sony we are again same
thing slightly lower resolution than the
oculus and the HTC vive a different way
of tracking this time with lights
because they have the move technology
with the light cameras on the
PlayStation so they're basically
tracking the helmet with the same
technology of light you can go around
and it will track you and they were
going to use the Move controllers as for
the same system interestingly enough
this this device is going to be having a
higher refresh rate then then the oculus
the oculus and the vibe for targeting 90
refresh rate this one is chart a 120
which I'm not sure if the PlayStation
can do but refresh frame rate is really
important for a good experiences so
we're going to get to that in a bit I'm
going to just go quickly through all the
other ones that exist this isn't as an
open source VR so there's schematics for
building this there will give you all
the materials and all the schematics and
you can build this in your home or buy
the kit and there's a open source sdk
available for for it so they're planning
on a full open source experience on on
the razor and then there's all these
that exist right now the the oculus the
5 and the V the PlayStation VR going to
be launched announced next GDC 2016 in
March so very likely they're going to
hit the markets by June
and as a consumer version and at least a
bunch of these are also going to hit the
market these already exist some of them
are like you can only order them by
talking to people others are available
so it's it's going to be interesting go
through quickly through AR if you
haven't learned her des bout the whole
lens AR is different in that it's not
about the immersive experiences about
the Augmented experience so while on dr
you're in a fully completely different
space and completely separate from the
real world it's it's a different like it
takes over your senses this this is a
lens in front of your eyes that where
the projector that projects images into
your eyes or which appear to float in
the air in a very very small cone but
they are there and they and they will
you can put points in the world and have
things appear in the world and then
manipulate them and have them interact
with a little things in the world
because it has cameras and sensors that
track the position of the things that
you are looking at so it's a completely
different thing it's augmented reality
where you put stuff you're going to put
stuff here basically and only people
with the goggles are be able to see is
it might be launched later next year it
is going to be way more expensive than
VR and the way of developing for it is
also with unity also with game tools but
things are so under work so we don't
know they're building a whole unit
integration package for all and so you
can totally build do the same type of
content but it's very secretive right
now so just quickly like Google glass
was the first initial try of this but
this is really just a little screen that
renders things it doesn't track anything
about you or the world and then there's
magic leave which is showing a bunch of
amazing videos like this where you can
put things in your hands which is the
premise of they are where you can put
things
in the world world but again it's we
don't know there's no hardware or
anything yeah yes it's magic we'll see
they're they're very hush-hush about it
but they're there it's interesting
videos controllers this is the thing
that's going to come out because of the
VR so right now everybody uses xbox
controllers or playstation controllers
to interact or the keyboard but it's
really hard with a thing on your face to
see where the keyboard is or even use a
mouse so the nor the usual experience is
you have a controller in your hand and
you're using a gamepad xbox or
playstation xbox is is is the most
popular but with the new experiences
that is not enough because because it's
myself so oculus is going to come out
with a touch controller which is this
little thing left moon with a stick and
buttons that is going to track and you
can see the markers there it's going to
also be tracked by the same cameras and
it has sensors on its own so you can
track your the motion of your hand and
it can track your hands and then you can
be also tracked by the cameras so the
the game knows where your hands are and
it can position an OLE hand or a virtual
avatar of whatever you're holding in the
game valve is doing this which is a
stick with things sensors those are
sensors on top and again being tracked
by the laser sensors that they have
positioned these work amazingly well
I'll try them in game you if someone
hands out you're in the game and someone
hands out this to you which is getting
tracked by the sensors you can actually
reach out and grab it because it gives
you all the cues or where it is you can
see it in 3d and it's exactly there and
you can grab it so it's really well done
the accuracy of the tracking is amazing
and it can be whatever you want it to be
in game it can be hand it can be a sword
that can be whatever it's it's kind of
similar to the wii sticks basically
everybody's just going through that the
we stick thing but in different types
configure
shins for accurate tracking you can have
two of these and just do things vr doing
light tracking so with the move
controllers and then there's leap motion
which is separate accessory that you can
buy right now it's available that goes
into the oculus or whatever you want to
track it gives you a wider field of
vision then the face plate actually does
and it can track your hands this is all
about tracking hands so not only can
track you give you positional tracking
of your where you are but it can also
track your hands and make virtual hands
in the in the world so you can interact
with things it also has a base like a
base monitor that can talk to the device
and give you more accurate tracking and
I'll show you an example of what they do
with this in a bit then there's other
players in the system which this is a
reactive grip and there's a where you
grab it there's a rubbery thing that
expands depending on what happens in the
game to give you tactile feedback on the
strength or whatever you want so if
you're pushing against something you
will contract against your hand to give
you tactile feedback on the force that
you're getting back so it gives you
actual which is called the haptics
reactive grip it actually gives you
textile feedback which is something that
nobody else does and it's it's very it's
something that you miss even though if
you can see your hands seeing your hands
or something that you miss and even if
you can't see your hands pushing buttons
on without having feedback or grabbing
things is it clashes with the experience
so they're trying to solve that this way
the razer hydra is another one this is
just basically tracking controllers so
the where you can track your hands again
in game and then virtually so many is
doing this which is a whole thing where
you walk it is tracking your position it
has a rolling those shoes or have
sensors so we can track your feet
and when you walk it it rolls so you can
walk on it and and be in that which is
basically really where are we getting
too okay so let's talk best practices so
when you're building contact scale is
important is suddenly very very
important you have to have one unit one
meter for your whatever tool you're
using your even wanna game using unity
vision real it doesn't matter you have
to have one-to-one mapping of everything
because scales the things are important
is the height of the players important
all of like sizes are important minimal
sizes for finger targets of anything at
a button or anything that you want to be
interactive should be no smaller than 20
min on waivers the way we track scales
in 2d is by comparison you can have
comparison between two things so you can
know which one is bigger which one is
smaller and parallax gives you hints on
sizes because the distance is in depth
sorry so the bigger the thing moves are
like the the if you're if you're going
and you're seeing a mountain and you're
running but the mountain scale doesn't
change its because it's a big object
it's probably very far so that's where
you get the sensation of scale parallax
gives you sensation of scale because
certain objects move faster than others
so it gives you an illusion of scale
even though it's 2d there's no depth
there and then familiar objects you know
where the size of the cocaine is and if
you're in an environment that has a lot
of familiar objects you instinctively
know the sizes of them so it's actually
important to get a size of the door in a
virtual environment correctly you can go
to Wikipedia and see what they average
size of doors in the world so you can
know what the size of the door is in 3d
there's also another way of telling what
the sizes of things are which is when
objects get closer to you you
eyes rotate to keep them in focus and
the brain takes that rotation as a clue
for the distance and depth and the size
and that is dependent on the actual
distance between your eyes because you
have both of them working for this depth
effect right which means that while your
distance between your eyes in the real
world is constant so the brain always
knows what is comparing against in the
virtual role it depends on the
configuration of your cameras so when
you're rendering a scene you I have two
cameras and the two cameras that are
rendering that view for your two wise
need to be separated by the same
distance that your eyes are separated so
that when this effect of the brain
trying to track what an object coming to
it and figuring out the scale of it
actually works an interesting thing is
that if you change in the game if you
change the ipd and the height you can
actually change the scale of the whole
world without changing the world because
that's how your brain is actually
figuring out what sizes of things so if
you want to be a little mouse you just
change you just scale the height where
the cameras are in the distance that
they separated and then just render that
without touching the world and suddenly
your mouse view or a giant view so
because because the brain similar
sickness is is the biggest formal and
this is the focus of the talk i have a
huge sign of tips i don't know if we're
going to go through all of them because
i would like to show some other things
but basically if people get sick in the
VR there's no experience it's very easy
to get sick and some people get sick
more than others it's all about a clash
of information between what your profile
assumption system which is what your
body knows about itself in terms of
motion and
speed and everything we have inherent
knowledge about where are things are in
our body and how we're moving them we
even though we're not looking at them or
might not feel them but we know and this
sense of position itself oh if you're
moving in a VR world and you are not
actually moving it clashes it doesn't it
is not because your eyes are the center
of the world basically the eyes of a
write everything in terms of you can
trick your brain into believing that
you're moving without actually moving
and we rely on our eyes for a lot of
this this information this is why not
everybody gets sick because because you
can make an experience that is
believable that doesn't clash because it
overrides these senses but if you do
something things you will so we're going
to go through these this doesn't have
only vr by the way the apple parallax
gets people sick because the the contour
motion is very it's is one of the
movements that just completely kills
your inner ear destiny has a UI that
this a counter motion thing and in early
versions they were making people second
they had to tell the movement town even
Witcher 3 has a mode when you go into
which your sensors it blurs and distorts
the screen to just like be in the
witcher view where everything is
distorted and then there's double
colored things that pop up and you
usually have to move around in this
environment and this makes a lot of
people sick and it's the same type of
symptoms there's maude to disable this
because it affects some people so much
to minimize this stuff these are the
three things that you need to be aware
of frame where at least device right so
you have to know what you're targeting
you have to have at least 60 years to be
so the eye doesn't in the brain doesn't
can be fooled because if you have lesson
firm right less than 60 Hertz which is
60 frames per second you're not
rendering fast enough to update the data
that you're getting from me moving and
me
doing things so what happens is you have
to jump frames to up to to get ahead
because I move your head but you're not
rendering fast enough to actually show
this show this movement even if you have
all of the data so the slower you going
the lesson less speed video refreshing
the more jaggi and laggy the experiences
and that is like prime trigger for
dizziness and confusion and making
people sick because then the information
is just going all over the place and
your eyes will get extremely tired of
seeing things jump around so minimum 60
frames per second most consumer versions
are going 90 frames per second the
PlayStation VR is going 120 what do you
have to do is have a constant frame
right and hit it all the time if you
have an average frame rate of 60 but
then you're going 28 2080 that's not
good because it's going to still drop
frames and it's going to still go
jittery and when you do that in front of
like right here it's it's going to bring
people so you want to know which device
you're targeting pick a frame weight and
just hit it and you have to hit it which
is why developing for the gear so much
harder because hitting those types of
rendering speeds on a phone is extra
annoying and also this is going to be
hard than the consumer version because
there's a lot of pixels it's huge
screens the resolution is enormous
hitting those frameworks requires beefy
computers which is why oculus is now
getting saying that the computer that
you have to have is it has to be beefy
it has to be big latency 20 milliseconds
this is from sensor two photon which
means 20 milliseconds from the time you
eat the sensor data off of the device to
the two you shoot shooting in photonic
people's eyes there's there's latency
going on in the pipes between all of
this data coming through you can hit it
anything bigger than 20 milliseconds
you're not going to be showing what
people expect so I move my even if I
move my head slowly
if the what I'm seeing happening on in
front of me is not what I expect because
if the latency is bigger because the
sensor data is not being updated to the
screen fashion enough so to keep people
from being sick this is the speed that
you have to hit there's developer tools
and profiling tools that will tell you
what the latest he is and what you need
to do is make sure that all the
calculations in your game loop when you
go like update data and render how as
fast as possible so that you hit this
that's the number the magic number
basically latency from information of
hands can actually be higher we have
less of a feel for the speed at which
our hands were supposed to be where in
on on screen so when you're using for
instance League motion or other things
to track your hands alone it doesn't can
be hired and go up to 28 or 30 but for
eyes 20 is the the mail so no sudden
movements this means I'm standing here
and then suddenly you decide that I my
character is going to be transported
somewhere and then each you just you
just accelerate or remove me somewhere
that's like no because that's going to
kill people if you're not expecting
movement if movement shows up especially
if its abrupt and not initiated by the
user you're going to make people sick
the same thing it's an expected movement
no shaky cam so no oh there's going to
be your earthquake sake sake sake sake
sake that's that again makes people very
very sick most of it because parallel
motion so strapping like doing this is
one of the surest ways of making people
sick a civilization sickness gets
triggered by by parallel motion what's
one of the biggest triggers so strapping
is a problem and that happens a lot in
first person players and you even
probably want to give that option to
people but that's not at
thing that we humans do we turn and move
we don't do this and so any type of
motion that triggers this is bad which
implies also no panic so some games will
when you go into scene they will show
you the thing they will come at wide
view and then maybe pan a bit and show
things that's all not only moving
without the player action which is a no
because the brain is not expecting it so
it's going to be very confusing but also
my can potentially trigger let these
motions that will make people sick again
also no head bobbing so your characters
you the cameras here your characters
here you're walking know like simulating
this no because again avoid lateral
motions keep the speed contents constant
don't accelerate when when people start
running or slowing down I don't do get
gradual acceleration it's very
unnoticeable we don't have proper cues
for for speed for acceleration it's very
hard to do and it will again all these
potentially trigger simulation sickness
so when you people start running you
just go to the speed if they stop you
just go you have two three speeds and
you just immediately transition to to
those speeds no flashing bright lights
it strobe lights it's the same thing as
being in a disco we don't have that much
time I started a little late concentrate
the action in the center of you so
because of the lenses and because of the
way things are getting rendered you're
the way you're looking at is where you
have the most focus and depending on
your interpreter distance and how big it
is it might be even smaller for some
people if they have unusually big or
small separation between the eyes so so
the focus is best in the center
concentrate that action movements on the
sides that are blurry will will will not
work
holy use the elevator don't make people
go upstairs because going upstairs
lateral motion not good just put them in
a little bitter and then go up okay so
we have five minutes so text rendering
is a problem because again focus on the
center of the lenses as a hard keep text
short and close to the center of vision
it can be there but don't make people we
long lines that I'll just put the text
and in and it's like this floating text
when I want to interact with this menu
this is from darknet when I want to
interact I look at it and it will will
come up to me and when I looked at when
I do this motion it will scroll the menu
so I don't actually have to look down it
will just I will just I'm triggering a
scrolling the menu will just float up to
come to me so when I'm looking at texts
i'm always looking in front and it's not
forcing me to do a lot of eye movement
because the the columns are short so
again many small motions know things
that are fixed and follow the eye so we
follow if the menu is here and I look
away the menus to be there it should not
follow me it should not be tied to where
I'm looking at it's there if I want to
look at it I will look at it it's
nothing that in the VR world should be
fixed or not interactive it should
always be a thing that i can look or not
look at it at text contracts is
important and then everything should be
responsive because people are going to
poke everything i did like as soon as
they realize that can poke anything they
will poke anything so this is a demo of
the leap motion hand tracking with a
menu so you can you can interact with it
it's it knows where your hand is and you
can see states they have a whole UI
toolkit for building these widgets and
putting them on with bex practices and
you should go look at what they have
because it's really good a lot of best
practices on how to put which is on the
screen
menus and toolbars and buttons and all
of these things to to work with VR so a
few examples of games that are breaking
the barrier of things that have never
been done before or do it in a different
way keep talking and nobody explodes is
a bomb defusing game where you can
actually play with out of your game of
your device and you should it's really
good so someone is in the VR world and
there they know what the bomb looks like
someone else has a manual and you can
either print it or look it up online and
basically the guy with the manual has to
tell the girl with the VR advise how to
disable it by asking questions and like
what's the color of the wires and how
many batteries it has or what's the
serial number stuff like this and if you
don't do it on time and exports it's
awesome it's an amazing game and it's
very static so you're sitting down you
don't actually move around so you can
play with the dk1 which doesn't require
six degree positional tracking which is
really nice it does everything really
nice the colors are subdued it's not
very bright it doesn't play around with
your senses it's just trying to defuse a
bomb it's it's a good game darknet is
another really nice game it's for the
give you it's a it's you have to hack
networks in the hat and networks or
nodes and you have to go in and spread
viruses which are you can buy them it's
a really good example of a nice UI
that's very slick and very optimized for
running in the VR it runs amazingly well
they did they have blog posts on how
they optimize this for the for the gear
so that they don't have all of the
polygons rendering like they do in the
pc version they just take shortcuts to
make sure things look nice but they
still render in a really good way and
this is the game that i spent six hours
playing in the gear VR which tells you
that it I mean if it gets sorry that
because you know
so I was playing a game that is tiring
but it's it is very very very good and
you it's 360-degree vision it doesn't
track your head it doesn't need to but
it tracks your rotation and the world is
all around you so we can sit on rotating
chair and just rotate it around it's
really nice elite is a fighter
exploration gain on a bit on a vessel
there via experience is really
interesting because everything is
activated by looking at things so when
I'm in a cockpit I want to activate the
menus I look to the side and the menus
activate because they've came in really
early and there was no real good way of
doing interaction so they just base
their on field of you when everything is
just on the cockpit I don't have to do
anything and on and it gives you a
sensation of space but at same time
there's the cockpit there to give you a
sizes of things and scales so it's a
really really good experience yes yes
yes because you're in a cockpit no
because because they did a well you
don't have to be looking all the time
most of the time you're looking front
because you're flying right so I have a
have a juristic with a throttle when we
can using game so you're flying the
joystick and throttle and most of the
time you're fighting or exploring or
doing things and you are doing like this
which is it is most the the best like
the most comfortable position but when
you're for instance going into stations
and and you have to land you have to go
into the menus are you selecting
Michigan or something you have to go
into the menus and to get information
and that's when you look on the side and
the menus activate so they give you all
the information it's there you don't
have to go break the experience to to
get it it but it's it's well done they
use way too many much text on it
but then the thing was done for the pc
so they didn't change those those bits
for vr i would i would prefer not like
that but but otherwise it's pretty good
give me so there's only any other
questions yes yeah yes yes the this is
coming out of of the this is coming out
of the oculus Research Center where
they've been doing all this research
with similar sickness and the problem
optimal experiences and latency times so
they've built a latency Chester you can
buy it which will tell you exactly how
much in the hardware how much latency
there is from the from the sensor to the
screen so you can do that and all of
this research is indicating that 60 60
is the minimum and if you've looked at
if you worked with CRT's you kind of had
this feeling 50 is kind of important
below that it you can see it Sony is
saying 60 is not enough and I think
Valton too they're not enough 90 is the
minimum and you probably want to go
higher and also the more VR you
experience the more tolerance for it you
have so you can actually like you can
start seeing more things that you didn't
see before so you kind of build up a
tolerance for these things which means
your eyes will oh that's not that's not
very accurate and stuff like this so
while in normal games 60 ok you can do
it you can do 30 and do a nice game not
vr + vr if you do less than that it's
not going to it's not going to work out
well we are out of time
and this yes okay so so this is this is
unity I just wanted to show you this
this is unity this is one of the demos
there's a unity integration package
comes with oculus I'm using a Mac you
can't use Mac's anymore for developing
for the oculus because they've cut down
support for a Mac because they want to
they want to target a very specific
hardware configurations so they it's
windows and a PC but unity looks exactly
the same on one side and the other and
when you connect the oculus you can you
can have you can see and you can you can
make a game and see the reaction get the
sensor data off of it so you don't have
to stuff your your face into the oculus
every time you wanted to see anything
and you can build this you can do the
reason I'm pointing at unity is because
not only can you do C sharp on it you
can also do F sharp and any other than
that language on it so and in all I
thought it was a good target for it this
community so anyways this is what you
can do right now for a unity so you can
download and you can use it and you can
set up the unity integration for for VR
it comes there's documentation online is
very easy and then you can just build
experiences and try it out oh thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>