<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2015 -  The Art of powering the Internet's Next Messaging System | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2015 -  The Art of powering the Internet's Next Messaging System - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2015 -  The Art of powering the Internet's Next Messaging System</b></h2><h5 class="post__date">2015-03-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mv2MBYU8Yls" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">oh come all right we're good thanks
everyone for coming on this morning it's
funny because when i was submitting my
talk I'm thinking about names I was like
well there is so many things I want to
talk about and we want to go through we
could possibly talk and and I really
couldn't come up with any cool name and
everything I could come up with was so
super boring right and it's like we do
with all these names no one is going to
go there I'm going to be there by myself
so I better come up with something
interesting and his impressive huh so
now I feel the pressure that we got to
deliver something really interesting so
once you have how it goes yeah I decide
to we decided to call this one the art
of power in the internet next messing
system and as you see that logo by the
way how many of you guys recognize a log
over there no okay that's cute okay some
of them okay so a little bit of
interaction with ourselves my name is
Juan and I work at the lair alongside
with Mubarak which will be co-presenting
with me this talk and we're BOTS out
suffer engineers of the server team and
a little bit of myself I I've been
working on Ellen since 2009 I wrote my
master thesis about a cubic quick check
on taste testing and my colleague
Mubarak has been formally at Apple he's
got a lot of experience with large
distributed systems and inhibition to
that he is committed to the Apache flume
so we'll be a delivery in different
sections today and I will I will kick it
off so this is going to be the agenda
for today we're going to start off by
introducing layer the concept and what
layer does and then we're going to go
through three main components in this
stock the first one is going to be
speeding the second second one is going
to be message routing and
third one that we're going to go through
is data infrastructure after this we
will go through several topics about
performance and problems we faced and
difficulties we solved and lessons
learned of course then we'll open it up
for 10 minutes or whatever is left for
questions and answers but it won't feel
free to stop me anytime with you if you
have any questions or any comments there
are more they're more than welcome so
what is layer layer is building the open
communications platform for the Internet
what that means is that we make it very
very simple for app developers to have
communications features to the mobile
applications whether it's for a message
text message and video messaging
location sharing genetic data
synchronization we make it very very we
make it very simple and we take care of
all the difficult things of
communications weather is a back in back
in storage typing indicators push
notifications anything and how is this
even possible okay so we developed claim
SDKs that make it possible for those
mobile applications to integrate our
system and talk October servers so with
a very little effort you can integrate
message insist machine features and
communication features into your app but
of course like why why should you trust
someone like layer to the communications
what is so hard about communications
everyone can do communications well
pretty much everything plans have a
storage network transport cloud storage
data synchronization of land estates
global infrastructure security push
notifications multiple devices kill
ability policies regulations a lot of
difficult things that you had to you
don't have to take care of if you only
want to focus on the experience to your
customers and obviously make it
available across all the platforms so
the way we see things here is that when
you're an app developer and you're
developing something for your mobile
application in the same way that you
want to
geographical features to your app you're
not going to build a mapping system from
the scratch you can go out there and
integrate things like Google Maps app or
Maps yahoo maps and get it already
integrated right away sin happens with
the payments tax you don't want to build
a payment systems from the scratch you
can hook it up with an available API and
get payment payment features that can
work out of the box so that's where we
want to fit in in the communications and
we want to make it simple for everyone
to our communication features to their
apps without having to take care about
the difficult things in other words you
focus on the customer experience and we
take care of the most difficult thing
okay so a little bit of our subway stack
this is very very simplified and as I
mentioned before we develop tenancy case
that talk to our servers obviously they
are written in their own native
platforms and today we've got a android
and iOS sdk is available we're on the
works to release Windows Phone and
JavaScript for the web and then our own
back-end systems are mainly powered by
airline systems we've a Cassandra as a
data storage solution we also have a
other additional components written in
this color the help us with services
like policies reconciliation and so on
but they are totally out of the scope
for today and the Kerdi the OS of choice
for us is correct that were everything
everything runs on top so the quest the
the question why why have we chosen Earl
and well liked as we saw all of us in
the keynote is is for the main use case
that that is been invented for its
communications right and that's what
we're about to solve we want to wear in
this wall and this is a natural fit for
airline right we also want to achieve
fault tolerance in our
system because we we want to make our
service available to all our users
without having to experience any
downtime and under by using the best
practice of OTP that helped us a lot not
22 well be not to be a cross with any
problem with with the issues in
production especially concurrency model
message passing not being able to share
any data across processes very helpful
it's also it also gives us the ability
to have to handle a large number of
concurrent from currency processes and
especially this is very important to is
the ability to handle binary protocols
especially when you are talking to a
pnas DCM and Kafka for example that we
do it in on a daily basis so how does
the layers back and look like in a
natural well super simple as I said
we've got our clemency case that talk a
SPD HTTP to our system we we have a
cowboy running as a web server speaking
3.1 protocol of SPD and that happens to
be handled by our core engine that is
smart enough to route every request to
the right Q and at the end at the end of
the request that gets delivered somehow
to the right recipient obviously there
is a lot of complexity and logic in
between such as a storage of a lot of
metadata and information about
everything related to the communications
that happens to be storing Cassandra
okay so as I said these are going to be
a three main components we're going to
go through I'm going to go through the
first two ones myself and my work is
going to deliver the infrastructure so
yes to start off with this question how
many of you guys are used in a cowboy in
production okay maybe half of the
audience and how many of you guys have
been
I came with a speedy in cowboy or in any
other web servers just a few okay all
right so we happen to be using both okay
and for those for those of you who don't
know what it speedy speedy is a set of
evolution of HTTP 1.1 and happens to be
the the predecessor for HTTP two it's
not meant to be a replacement but it's
meant to be a upgrade in the way that it
looks at improving some of the issues of
HTTP 1.1 has and such as a lady's
introduction which is the main goal also
the one of the other goals is to improve
the security so we layer rely on speedy
protocol for transferring our content
from clients to service and so on and
the 2 million the two main features that
help us achieve the latency reduction is
a compression of headers defining the
protocol basically a speedy connection
keeps track of all the headers that has
been sent and for those of one having
already sent they they are not sent any
more so that's avoided and also for the
new headers that are being sent they are
compressed and then the multiplexing
that happens to deliver soup resources
of a content to to the plan that is
requesting it within the same connection
so it's also more efficient a little bit
a little bit of history about the a
little bit of history about the protocol
well there's been a few drafts written
everything is started off in 2009 and
then the second version came out the
third version 3.1 and the latest the
version 4 has been on summer 2013
SP vs SF is an open networking protocol
and not not all the versions are
implemented everywhere the most common
implemented are a 3 and a 3.1 and
today's browsers most common browser
Safari Chrome Firefox they are all are
implementing those and as we can see
here the cowboy project added a basic
and experimental support in spring 2013
and we happen to took it to take it from
there so the the whole implementation of
the protocol obviously it's not fully
implemented but what is available is
split up across these two modules if
you're familiar with cowboy cowboy from
back some religious back I don't
remember which one there wasn't a split
up in terms of the common libraries that
were supporting cowboy and Kobo itself
so in the cowboy speedy module we've got
the protocol implementation I know on
basically what it is is the main loop
that handles frames and coordinates all
the replies to the clients and then
we've got a couch pd which which helps
you work out the protocol manipulation
and also the parson and Billy on the
frame settings and streams and so on so
yeah we'll know that cowboy is a pre
well maintained one of probably one of
the most popular LM projects in the
github and Wallace is great to to fill
that is it's got a lot of support and
attention so one of the things that we
discover the protocol that we're
beneficial for us unfortunately weren't
implemented in the in the cowboy at the
stream project so those some of those
features were things like server push
really meeting and flow control server
 is very beneficial because
you can see the ability to stream and
post content from your server without
having to without requiring from the
client a request Pacific request she
becomes more efficient once the speedy
connection is established it's a similar
concept that what you get with
WebSockets then you've got real limiting
and flow control which happened to be
two flavors of limiting resources the
relay Madine option is to limit the
number of concurrent extremes per second
and then and then the flow control which
limits the amount of bias that the clan
is allowed to send so what happened with
that unfortunately those were
interesting features for for as for the
for the performance we wanted to get and
we obviously when when you want to
contribute your German to fork it for
the project implement your self so maybe
pull requests and so on but the pace we
were working at may be pretty difficult
to be on top of things and merge it back
upstream so things got a little bit
delay we are implementing all these
three advanced features on our branch of
cowboy and as of today really limiting
feature is support request has been
submitted hopefully merge soon and we
want to get the other ones merged as
well so that was the first sorry that
was the first part any questions so far
yeah
yeah okay so he and the question is if I
can't talk a little bit more about the
speedy compared to ATT be too and if
cowboy is going to support HTTP to the
second the second the dancer for the
second question is i think is yes but I
think as Lloyd told me that's going to
be released alongside the new version
the new major release of Ellen
distribution so he's waiting for that
and so the spec for the HTTP 2 was
released by Google last month so he's
been already finalized and as far as I
read everything of this pack has been
inspired in what is PDV fine so speedy
basically was a foundation for what hcp
is today I can't really know the fine
details I don't have them out of my head
but that's all i can tell ya
well I mean the thing is that we we have
control over everything over both ends
of the connection because we are
implementing the client sea caves so we
decide what protocol we're talking to
our own servers then since we have
control we decided to go for something
more efficient than regular HTTP 1.1 and
today we're talking is pd from those
devices to our servers yeah we are not
supporting webbed yet so of course when
that happens either will be on HTTP two
or web sockets or something that can be
fully compatible yes which one yeah so
the rate-limiting is is it submitted its
been submitted there for a while I think
everything is good to be ready but he's
super busy so hopefully that will happen
very soon the other two ones are not
very sick that we have to do some well
it's a seller workers we've changed
quite some things and we've got to
isolate those changes and and then put
them on top of current master and send
it up a stream so yeah I send the words
hopefully that will happen soon and I
will go over these later on the lessons
learned section but this has been one of
the most painful experiences because
once you've work it out especially a
very very popular project then you
struggle to come back again to the pen
on the upstream and that's not something
i recommend 21 yeah
well I mean this those are decisions
attached to a roadmap and product
decisions like at the time maybe we were
still planning to the web or whatever
and we were only focusing on mobile
devices and at the time choosing this PV
over what was available was more
convenient for us also as far as I know
maybe more Mubarak has a better answer
support that you get for an iOS and
Android at a time was very good for SPD
an implementation were already pretty
mature so that was a good a good
decision for us more questions okay yeah
so feel free you're welcome to check the
project out unfortunately documentation
is not it's not updated everything is
still as the original with me but yeah
things are working in production for us
words in those features continuously
okay so message routing obviously when
you're building communication systems
running the right message to the right
recipient so one becomes very very
crucial so we've we've designed a very
very trivial and a simple solution that
I suppose is able to scale out and and
also deliver right message to the right
recipients whether they are whether
they're connected or disconnected of all
the times so the main concept here is
that layer messing platform publishes
every single message to a rabbit in
queue broker and and things are things
that I've described as simple as this
when the post post method arrives well
obviously we handle it and then and then
after doing all the parts in and checks
and things like this we we send that
message across to the to the broker and
that's that's received there until is
properly storing the right Q so this how
things look like we've got a a sender
device here is establishing a speedy
connection to our server the server
happens to have a connection and
obviously channels to rub it in queue
and then when a device connects to layer
like let's say a recipient for that
conversation a cue for the device gets
created okay so this how things look
like super super simple again all the
talking I n QP client by by the rabbit
ink you guys so we declare the Q all we
need our channel the Q name arguments
for the record and then at the same time
attached to that Q there is going to be
a process that is going to start
consuming that Q and listen for incoming
messages I'll explain in a second what
that means so this is how things look
like on the on the receivin on the
receiving end we've got a android device
establishing a speedy connection and
then that's going to create a specific
cube for that device there is the second
guy getting connected we've got a second
q and so on so that they kind of scale
up up to well we've got thousands of
cues running our rabbit in Cuba cluster
and when when a device comes in online
and establishes a speedy connection
we've got Q's and then if they got if
they go away the connection closes then
the Q's expired because we've we've set
those rules when we've got no messages
inside so there is a process / connected
device that consumes message from that
device Q and that's that's what it
happens does that's what happens when we
start consuming the Q so with define a
behavior call out already okay well we
gotta go faster sorry
yeah we've got this process listening
for the messages that are going to be
handled by a OTP behavior that we define
the handles queuing handles the queuing
handles channel connection and so on so
and this how things look like we've got
the amqp message and then we we get that
to whatever handler with God and that's
going to end up in a server push from
the phone server to the receiving
receiving device right and this is how
things look like well this is irrelevant
stuff but basically when we do this PT
post this is a this is basically when I
call what we implementing the server
push feature in cowboy this is going to
send it two hundred the path and then
eventually they push body were standing
will stream into the device so let's go
very quickly through a example let's
assume we have a four device
conversation okay so they got the first
guy sends the first post that gets
delivered by Adele gets handled by
cowboy we sent a n kpt message to the
broker that gets routed through the
routing keys to through the to the right
cues and they get delivered to the guys
participants on the conversation but
what if those guys waiting for the or
participants of the conversation are not
connected obviously we are not going to
have a ques right so what's going to
happen here okay well we need those
those guys need to get notifications
regardless over their connectivity
status right so we've defined also a
what we call platform queues platform
pushes queues were they are they having
defined with a module binding what that
means is that all the messages we will
also be going through that stream and
that freeway is going to end up in
either the APNs server or the GCM based
on and look up in our storage that tells
you whether that device has been reduced
register as an iOS or Android okay
that's pretty cool but how do you manage
connectivity resources
I don't know if you guys have been
yesterday on the Anthony's talk about
pooling libraries on Ellen we I found
that very very interesting because we
are dealing with those things every day
and some of the libraries that he
mentioned we use them in production and
the solution is very simple pools pools
pools and more pools okay so for the
speedy connections is a no-brainer for
in 99 s have created this ranch library
that is a soccer socket connector
receptor pool that defines a gen server
that manages connections listeners and
ports and what that gives you is ability
to control all the established
connections that you've got in your web
server and we says other cool things
you've got this callback that what that
gives you is a number of in this case PD
connections you're running and when you
touch that to a monetary system you get
things like this where you can visualize
all your samples of current connections
at all the time in your production
servers but how about a QP connectivity
right in the NQ p war we've got
connections and channels connections are
TCP connections and channels are
lightweight processes that happen to be
within those TCP connections so they are
all multiplex 21 real TCP connection
right so sorry for being very very fast
but I just okay good news well it's
taught me if you have any questions but
so basically what the problem we have to
solve here is that we wanted to have a
system where we were able to scale out
and a skill a list and skill down alien
QP resources as needed right so your
solar system is dynamically sized and
what we did was to develop a two-layer
pulling system based on connections and
/ every connection will create as many
challenges Navy and we did it with
puller I don't know if any one of you
are using polar in production here you
guys okay because well I'll tell it in
second
and we've got some issues that we
managed to understand but they were
pretty tricky although it's a pretty
pretty well-maintained library and
obviously OTP compliant so yeah I think
it was probably one of the best in the
in the list at Anthony mentioned but
things are super super easy this ad note
callback is what we get when our service
discovery system discovers a new
rabbiting q novena were in our network
and that's going to end up creating a
new pool of connections that well are
pretty finding the in the number by
whatever variable we've got and then
we're going to call a star connection
method that happens to be in the same
module what that gives us I mean you
notice here that we're not obviously we
want to open a connection post but we're
not giving the callback for an ink you
peed yet what we're doing is to do in a
wrapper and what the wrapper does is
that it obviously calls amqp connection
to open the TCP connection but it also
right after calls tells the same gen
server that the connection has already
started and that makes us so pull inside
that connection so we take that
connection Pete that has been returned
by n QP and what we really call at the
end is amqp connection open channel so
that's going to start off with 10
channels per connection and that's going
to be dynamically sized as as long as as
as much as we need this worry was
talking about the cooling feature so
this this is basically a feature that
polar has that helps you to cool as till
connections or still resources whatever
they are and when you use rabbit in
queue or in this case amqp client
together with puller there is a lot of
odd behaviors and because what the
cooling does is that it sends a exit
with a kill reason to the channel and a
and Q P understand understands that or
turns out into internal error with a
specific code so what happens if you end
up killing the connection because that's
what RabbitMQ p does when it gets an
internal
forever and what's going to happen is
that that connection that a lot of
channels depend on are gonna are going
to be killed as well so it's a total
mess and when you're talking about
thousands of channels that you are
multiplexing in your system the mess
becomes huge so of course we had to
disable the feature that didn't help at
all okay so I'm going to hand it over to
a mubarak who is going to deliver the
section of the infrastructure right now
yeah yeah well did you do you mean the
persistency mode we write a rabbit thank
you messages or yeah nonpersistent no no
problem yeah but the problem is that
when you're talking these situations the
mess that you get when you do the
channel closing is huge so I'll go over
the absolute thanks so on hi Mubarak I'm
mubarak I'm going to talk about data
infrastructure layer communication
platform leverage big data and
distributed systems to build internet
scale large volume messaging
infrastructure so we use we use like a
lot of Apache projects like Cassandra
Kafka and we use or EKF which is a long
Kafka client to produce data zookeeper
for service discovery conflict
management and apache spark for the
in-memory data analytics and we hope we
have a homegrown scholar consumers as
well as spark streaming jobs apache
cassandra is one of the critical
component of our data infrastructure
meaning we store all messaging events as
well as
all the messages inside the Cassandra
whenever the new app developer comes on
board we create apps management
credentials policies certificates on all
other application related stuff and
store them in Cassandra as a more of a
authentication scheme and whenever the
messaging comes online whenever like you
send messages these store messages and
even says a source of protein Cassandra
we are using Cassandra for two different
things one for online real-time services
and second thing for offline analytics
data store this is how our data
infrastructure look like if you see the
left hand side the all the services are
built using air lung and the right
middle and right side or data
infrastructure wind and we are
leveraging lot of Apache projects as I
said so services internally uses EKF a
line client whenever the service
encountered an event it published the
metric took Apache Kafka then the Apache
Kafka as internally has lot of brokers
topics and partitions then the homegrown
consumers can read the data do real-time
computation and push the aggregated
results to Cassandra cluster such that
the in-house dashboard as well as the
developer customer-facing dashboards can
view the data in near real time so we
use we used two different consumers one
is written in Scala and another one is a
spark streaming job we play around with
a spark streaming job now but we have
been using scholar based consumers since
the since it went into production now I
am going to talk about how we use EK
which is a l on client for Kafka so B we
did research on several along based
Kafka clients and we are happy with the
hip curve because it is working pretty
well we load tested it is I performance
it is very lightweight it creates
workers based on need so whenever new
calf car broker comes online it
automatically creates the worker for you
it internally uses gin fsm and jen
server modules to do the work on
management you don't need to query the
zookeeper to get the metadata about all
the kafka brokers if you supply the only
one broker address the EKF Cain takes
care of fetching the metadata
information about all the kafka brokers
and it does very smart way of connection
pooling all other stuff we can either we
can use synchronous way of sending
matrix or asynchronous way even if kefka
broker goes offline EKF takes care of
buffering your data inside the service
and whenever the broker comes online it
transfers the events from client to
broker via streaming messaging matrix
such as how many message messages have
been sent for a nap about message
delivered how many message read for an
you say some like for application
management how many active users are
there for an app like authentication
related stuff like how many author has
happened how many authentication
successfully happened related to push
notification how many pushes are sent
from our infrastructure to devices how
many push notification others and how
people are using our SDK versions app
related matrix and user related matrix
we have around around 150 matrix we are
storing sorry I can't put everything
here as I said kefka is our backbone
data back phone
and we are using is a data freeway it is
a distributed partitioned replicated
commit log service and it provides
pub/sub messaging model which is from a
linkedin so we can store all the
messaging Givens and matrix inside a
topic and each topic can have multiple
partitions we are having a set of
brokers which is called Kafka cluster we
are have we are basically using
replication factor of three to basically
address all over I availability concerns
and single point of failures we are
playing around with transferring all our
application level logs private logs
public logs search infrastructure logs
to Kafka such that we can write our own
consumers to fetch the data from broker
and process it then so I
are we are using like a calf cow only
for fuelly data analytics okay is that
your constant and we are leveraging
apache spark which is from amp lab for
lightning fast data processing we have a
lot of in-memory aggregation jobs like
monthly active user calculation daily
active user if you want to find how
people many devices being involved in
the claim to serve a messaging pattern
if you want to run summary jobs like
daily hourly weekly kind of a app based
stuff and they are creating a data
pipeline to build data service on top of
existing data we are working on spark
streaming to get a near real-time events
and aggregation of job jobs so that we
can give near the real-time visibility
to our developers we are working on
spark SQL to give a dog Corey's for
external customers as well as our
business team we are we have returns so
many like a data validation on migration
tools to leverage spark any questions
related to data infrastructure all right
so like we believe that rabbitmq for
pure messaging and we believe that
apache kefka for log based analytics
infrastructure and these those two are
very like a vertical I don't think like
both of them is going to solve our all
the problems okay thanks i will give it
to all can you hear me all right all
right I'll try to be fast I know within
a little bit delayed but i think this is
bri interesting i also want to hear your
feedback in terms you guys have been
through the same things because it's all
about learning here right well before
before you start I don't know what how
many of you guys are following the
mailing questions least of yellen
questions list because of course it
always happens the same thing you sign
up they start following and then after a
while it just gets a lot of fun read
messages as you never follow but
specifically this one in january of this
year roberto stinellis started up you
know explaining what the behavior will
he was seen in his production server and
memory accumulation he didn't really
know where to look for and another
leading into a huge threat that uh i
consider is very very interesting to
learn about how the vm works how the
garbage collector works and and how to
dig into memory analysis so i definitely
encourage you to check it out because
you are going to learn a lot if if you
if if you know already hello okay yeah
so some of the challenges are building a
communication system like these are
always the same you deal with a memory
accumulation memory weird patterns that
you can understand especially when you
are monitoring them we particularly
found problems with our Cassandra
driver we use a CQ URL anyone using
Cassandra here in production with
everyone no you guys what huh yeah what
driver do you guys use you guys just
wrote your own one is it open source
okay well we didn't have time for that
so we went to github and we grab the
best one we thought it was and yeah you
know obviously it talks well the the the
Cassandra binary protocol does paper
statements a lot of cool features but
we've found some bottlenecks and issues
that we amended we forked out so is
again in our repo not of not all of the
fixes are a merge upstream but we only
works some of interesting things that we
found out was the famous sat down and by
the llvm when you get to reach the max
restore intensity and I think well there
are a few reasons there but I don't know
if any one of you are using supervisor
to how many of you know what supervisor
to is okay well supervisor to is
basically the main OTP supervisor that
it's been written by rabbiting q that
adds a few features on top of that and
it makes it more secure such as when you
get to make a mistake like this one
where do you do template copying of the
max or a Mac Steve parameters for your
supervisor do you end up you know making
these mistakes word you the whole thing
goes down just because you didn't really
think about the right strategy or then
write numbers not obviously all the
supervisor on trees to serve the same
parameters so that's that's that's a
very common problem that you have to
study well rebar and reproducible builds
does been a pain for us we have a great
hope with River three fortunately that's
going to help a lot and as I said
earlier trade-off
so when for cannot popular github
projects if you don't want to be
maintaining your the project by yourself
forever you should usually be careful
and be merging up up a stream very fast
so some of the lessons learn as i said
the supervisor to is very very helpful
for when you're not very confident when
with your supervisor strategy some of
the issues we face the word trigger was
the abnormal heap growth that I thing is
called then in the next session Lucas
Larsen is going to talk about the Earl
MVM and especially some of his studies
about the DC that's very very cool and
I'm gonna explain in a second what we
saw in production and one of the most
important lessons we've learned
definitely was was the fact of
monitoring everything you can especially
when you're running in production
anything that you can imagine that can
be that can lead to a issue or a memory
leak or a problem or a bottleneck
monitor builder graphs build your gauges
get your dashboards on and then what's
your out then based on that you will get
a lot of output it can be helpful for
understand your your problems problems
with we had with Cassandra eventual
consistency and not being able to
synchronize the cluster times that's
been a pain but well I mean I'm gonna
skip that causes more develops work and
i said before the rabbit in queue
together with the polar behavior when
you're using a colon feature that has
been a pain and last but not least i
think this is a very interesting one
lagger as you probably know has this
feature where you can define a high
watermark threshold that where you get
to speed rate of log message higher than
that it just gets ignore which is a
thing is a very very dangerous thing to
enable in production especially when
you're debugging because you're going to
obviously miss out a lot of problems
that happen and they're going to get in
north so my
recommendation is be very careful with
that and something else that i found
that is that lagger implements this
feature I've seen that by default is
enabled so be careful with that because
if if you want to get if you gotta get
it disabled the only way to that is
politically site and define by the Foley
setting is a fine 250 or something okay
so i want to show you this this a
specific problem that to me was
extremely particular this is the fact
I'm running I'm running we're running
production I'm monitoring my system and
all of a sudden things seems to be
healthy things seem to be reasonable all
of a sudden I see the memory
distribution of the of the vm changed a
lot especially for the airline processes
okay so I see a huge spike you spike
then next thing to do is to understand
what process or what processes are
gathering all that garbage or memory
right so you you list the number of most
memory consumers processes in your vm
and then you found out that the arrow
logo has a huge heap and this is my face
I don't know what to do there and well I
mean the first thing that I can come up
with is disc right it's like oh my god
it's gonna how are you going to solve
this why why I mean I don't understand
why our loggers in this to me so less
force to run the garbage collector okay
so if you do that well shrinks to
nothing so problem solved right but yeah
the one this to happen what's the reason
behind this like any of these yeah ah
binaries no no no it wasn't binaries it
was related with something that I
explained before rabbit amqp together
with polar a lot of a lot of cascade
issues channel is giving killed a lot of
error messages if you're talking about
thousands you multiply and what happens
is that this I my work this is our
hypothesis after talking with Lucas
frenetic zone let's say that you have a
large burst of messages on the process
all of a sudden 100,000 so and where
those messages probably come from well
in our case is because we have a cascade
issue where one channel is been killed
by polar and that provokes a huge
disaster and all the channels get killed
of course that print that sends a lot of
messages to every logger right so Erinn
lager is going to sit there receive all
the messes in the mailbox and is the
middle box is going to grow very fast
right so if if the process kicks a
garbage collector at a time as an
optimization apparently those messages
that are happen to be in the queue are
we going to be copied to the hip ok so
the obviously the side effect of these
that hip is going to have to grow as
much to accommodate all those messages
in this case as we saw here we're
talking about 60k bites all over salmon
and yeah so then the GC will will be
triggered which is going to release the
messages and but the hip is going to
remain very very large right even though
the messes are having flash and
everything because the hips rincon
algorithm 11 happen until the second DC
kicks and even in what happens this
rincon algorithm is going to happen on a
25-percent basic so it's not going to be
gone completely the error lager hip is
going to be still huge so what is the
outcome do you end up with a normally
large heap forever
okay and this is all the things that
happen they're always with our lager and
so what solutions can you come up with
well my recommendation is pro monitor
very carefully the processes that are
sensitive to this issue in this case
very possibly forever lager and you can
obviously trigger a manual or based on
whatever monitor system a garbage
collector whenever you want because the
garbage collector is going to be more
aggressive than whatever is kicked by
default because it's going to compress
yangon and the regular hip and if you
want to read more about these there is
this paper that Lucas Larson wrote sort
of a draft and he goes through these
issues with more detail and it's very
very interesting so this is what
happened do you kick the garbage
collector and you're good you're good
again but obviously we've got obviously
monitoring system in the OS level where
when your memory consumption goes over a
threshold then we shoot it on the head
and then of course then because of the
security and toward other risk so you
don't want this to happen all of a
sudden without without any reasons and
yeah as I said monitor your system as
much as you miss you can HTTP web
servers methods code Layton sees yeah we
we are the we monitor a lot of things we
use 60 meter well today examiner core
because Venus split up and then we
collect things with another library we
wrote and then we send them over to
graphite and so on so as a final note
some of the open source contributions
we've done well some of them are here as
I said some of them have been merged
upstream a lot of our Forks have a lot
of interesting features that you're
welcome to check out and I'll hope you
have enjoyed this talk
questions ah sorry yeah we do have
customers we've been running ah yeah so
the question is if we do have customers
we do have customers we've been running
in production for number of weeks and we
are boarding customers as up now and I'm
afraid I can't disclose the number of
customers or their names so and I don't
think it's part of my scope of this but
yeah you you you wanna you might find
more information later calm about
services or customers or or any other
info yeah you mean for example when you
do a number system delivering
yeah so the question is how do you
ensure when you have problems with your
cues as I mentioned when channel gets
killed by chance and you lose obviously
all those cues and so on that's a very
good question that is only a that's not
it's not it's not a problem to lose all
that data because first of all we store
all the video and cassandra and the
message in the routing message system
that i explained is the infrastructure
that we use to deliver messages when
both ends are connected that means when
there is an established SPD connection
so when we're both speaking and there is
a live stream setup then we're going to
have my messages are going to be sent
through a rabbit ink infrastructure if
for some reason something goes wrong
right the decline ack is implement a
sinking protocol that are going to talk
to our web web framework api and they're
going to retweet the latest message
based on the restful interface and
that's like the other alternative that
we can't have to fax messages but of
course i didn't go through that one too
I didn't have much time but yeah
obviously if your message is no problem
because you can fetch them and some
other way yeah
what kind of scale did you cast what
what is the number of maximum number of
clients well as we speaking we're doing
a lot of load testing too because we
were looking at seen how much we can
grow in terms of kids we've been working
on the numbers of half a million million
cues per note and then in product well
when it comes to real connections we're
still working on our load we've built a
in-house load testing tool so we're
still working on a scale and adapters
fortunately or unfortunately they're
that tool is written in Java so we're
also having to scale that up in order to
establish the right amount of
connections with our server but yeah
when it comes to scaling out the rabbit
to make it possible to do all the
connections and and they play all the
cues we're talking about half a million
that's that's what I got so far and of
course that you can take it out and
let's get it out by adding more notes no
problem because with with class to rub
it in queue okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>