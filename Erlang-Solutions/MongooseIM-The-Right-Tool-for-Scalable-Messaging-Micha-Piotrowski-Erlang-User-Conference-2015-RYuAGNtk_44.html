<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MongooseIM - The Right Tool for Scalable Messaging - Michał Piotrowski - Erlang User Conference 2015 | Coder Coacher - Coaching Coders</title><meta content="MongooseIM - The Right Tool for Scalable Messaging - Michał Piotrowski - Erlang User Conference 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>MongooseIM - The Right Tool for Scalable Messaging - Michał Piotrowski - Erlang User Conference 2015</b></h2><h5 class="post__date">2015-07-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RYuAGNtk_44" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm in a petrosky and as it was said I'm
working utter lack solutions for more
than three years already and most of the
time I'm doing Mongoose IM and systems
based on Mongoose I am and XMPP during
my talk today i will say about what
Mongoose is about motivations behind
version 1.5 and the story about upcoming
release 11.6 so how many of you know
about XMPP almost everyone okay so just
for the record XMPP extensible messaging
and presence protocol which is used in
message oriented solutions and Mongoose
IM is one of the XMPP servers it is
based on a jeopardy but the codebase was
heavily modified since we forked and we
are building XMPP solutions with
Mongoose I am in various domains mainly
in social-media gaming and
telecommunications all these solutions
are designed for high volume and
scalability scalability is achieved
mainly because of the airline vm and
partially by the architecture of
Mongoose and the XMPP itself and
Mongoose is also highly customizable
platform thanks to the XMPP protocol and
open-source technology we we use the key
target domains we already address our
social-media gaming and telecoms the
common part of this domains are
messaging it can be multi-user charts or
one-to-one shots but any of these
domains are based on messaging and
that's why they use XMPP and Mongoose I
am I also put here IOT which stands for
Internet of Things
it is smaller than the rest one it may
look like I put it here only because
it's a buzzword but not I we believe
that our next the main domain will be
Internet of Things where we can connect
many machines or small devices and
together via XMPP and Mongoose i am and
i hope this will be soon feature for for
mangoes and next the domains and key
features of mongoose IM is the
horizontal linear cluster scalability
what i mean by that is that you can put
more hardware to to serve more users and
serve more traffic in your system so it
is always good to scale your software by
adding more hardware or replacing
hardware with some more powerful
machines mongoose has configured
configurable databases the simplest the
configuration works tunisia only but for
more scalability you can use my sequel
postgres or any other relational
database which can be connected over
odbc layer and you can also use readies
for transient data like sessions or some
other caches it supports web sockets it
has multi-user chat feature and a lot of
pluggable modules which implements
certain ex MVP extensions or can work
with different authentication backends
and our Mongoose team thinks that there
is no such thing like one size fits all
so that's why we are always open to to
build some custom extensions for our
customers because sometimes it may be
when
official to have something which is very
custom for specific reasons and we can
customize Mongoose for customers needs
not so long ago like half a year ago we
released the Mongoose version 1.5 and
the motivation behind this version was
meeting the challenges of modern mobile
messaging with XMPP we have a very good
article about this on our wiki page here
is the link in April we released some
small bug-fix release and now i'm going
to tell about the challenges for modern
mobile messaging the first one is stream
management in mobile networks it happens
very often that you lose your network
connectivity either you go to a tunnel
or elevator and your wife I've
disappears or your mobile network
disappears and during that time some
messages can be sent to your device and
you don't want to lose these messages
and you all so may want to resume your
previous session pretty quickly and this
issue is addressed by the XMPP extension
called three management which Mongoose
implements here is some small simple
sequence diagram describing how this may
work the next challenge for modern
mobile messaging is complete engagement
across devices in systems with multi
login you can have many devices
connected to your account and you want
to have them in sync with each other and
this topic is addressed by another XMPP
extension called carbon copies and with
which if a message is sent to one of
your devices it
and we copied by the server to all your
other devices which enabled the future
it goes to a when you send a message to
someone it will be copied to your other
devices and when someone sends a message
to you this message will also be copied
to your other devices which are online
at the time when the message is sent the
third challenge form over for mobile
messaging we we said is the message
history and synchronization with mobile
devices you probably don't want to store
the whole history on the device and to
you you want to sync your message
archive when you come online after some
longer period of time and this is
addressed by the except called the
message archive management and currently
Mongoose I am provides back ends for
archives in my sequel posters and other
relational databases and also we have
back end for Apache Cassandra we are
going to support ry'ac 20 in the next
release and some time later we will add
the back end for dynamo dB and that
would be quick story about 1.5 and I
think here is the most interesting part
of the talk which is the version 1.6
which will be released soon and the
motivation behind one point one point
six are to a first is DevOps
friendliness and the other motivation is
integration with ryuk 20 what I mean by
DevOps friendliness is the fact that
there is no such thing like too much
matrix and two to be able to observe
your
system correctly and to see what's going
inside you want to have many metrics
exposed by your system and in version
1.6 we change the existing matrix buggin
from folsom 2x a meter and we also put a
lot of new matrix some examples are
standard size histograms counting plain
xml compressed and encrypted data size
so you can see how how big standards are
sent by your clients and there are
metrics for rank distributed network
traffic separate matrix for odbc workers
network traffic and there is also a lot
of a module specific metrics counting
time operation of some specific actions
like right or look up from message
archive or roster get roster set and
various other metrics and why XO meter
before 1.6 we hit our matrix based on
folsom but we decided to change it to
exam eter the first factor is better
performance honestly I didn't observe
foursome as a bottleneck in our system
but from previous talks this year we
heard that foursome is not that
performance and also some of my
colleagues observed that foursome can be
a bottleneck but for Mongoose the key
factor to change from false on two legs
a meter is the flexibility which exam
eater gives us for example it allows to
create matrix calling a function when
it's needed so in Mongoose we have an
API function for getting the total
number of active sessions and we can
define simply this metric in XO meter in
two lines actually
where we call the relevant function and
then we parse the output to fit into XO
meter internal data structures which can
be then reported to some external matrix
services easily and these are some
examples of the matrix visualizations
here we have the histogram and actually
some percentiles of the mum archive time
and here is the median size of plain
XMPP standard size of a system under
load the next the motivation is the
right 20 integration we already
implemented the user base back end in
ryuk message archive and private storage
and I'm going to describe in more detail
the user and the message archive how it
was implemented as you may know in XMPP
we have GT and GTS consists of two parts
which is username and hostname of a
given user and the host part is put into
bucket so every virtual host served
waimangu siam has its own buckets and
the username is the key inside the
bucket so we organized users that way
and any other data for example the
creation time or some passwords or other
data used to authentication are put into
ry'ac map thanks to this you can use
your kosuna to search through your user
base easily and either to search for a
given user name or hostname and this was
relatively easy task comparing to the
message archive management and
it's ryuk back end first approach was to
organize the archive into weekly buckets
which makes the cleaning easier because
you can take the whole old bucket and
remove all keys from this bucket and
also archiving is easy because you are
writing to the specific back end it is
pretty straight straight forward and
every message is put into new objects by
objects I mean that we write the same
message for sender and receiver so it is
then later easier to read the archive
and in this approach objects key
contains local and remote sheets and
also timestamp of the message when it
was sent the value is simple turn to
binary and the the most difficult part
of this approach is the reading because
in the worst case you need to fold over
all possible buckets weeks you already
have and to filter keys in my first
approach i used the MapReduce key
filtering to obtain relevant messages
from the archive and unfortunately it
proved to be to have poor performance
pity i started the load testing after
the implementation I should have done it
easier area so i will probably not have
to implement this approach but when i
started my load tested load testing i
set up some scenario in which single
user connect to the server reach last 10
messages from the archive and write some
messages to other user the load testing
tool i used is a mock it is our own load
testing tool which we used widely to
test the XMPP
installations that the tested
environmental or single Mongoose I am
NOT and the riot cluster consisting of
three nodes the goal was to connect 10k
users sending 500 messages which
produces 1k objects in ry'ac and I put
some limit on the archive sync which
should be less than 30 seconds they
achieved result with the first approach
was pretty poor I was able to connect
only 300 successful users and the
average archive lookup time was 60
seconds which was not acceptable and I
had to optimize something so I come up
with the second approach when I
organized the archive into buckets per
user so every registered user can have
its own bucket in which all these all
his messages output and this gives us
less keys to filter the archiving is
still easy it is simpler to read the
archive because you don't have to fold
over all weeks and all the weekly
buckets but it is harder to clean all
the archive from every user bucket I
mean the key and the object is the same
as in the previous approach and they
achieved results were better than
previously I was able to connect the
around 1000 users the average archive
lookup was less than one second which
was good but only for first couple of
hours when we produce the significant
amount of keys in every user bucket and
we started to observe the same scenario
as in the first approach so the
MapReduce key filter.inc started
taking longer and longer and the
conclusion from my my two first
approaches is that MapReduce kefir drink
cannot be used to read the archive in
real time here is the results we can see
that for the first like eight hours the
MapReduce filtering was pretty fast but
after that is start growing and would
grow infinitely if I didn't stop the
test so this is still not acceptable
approach and that's why i started to
implementing archive based on react 20
search mechanism i returned to weekly
buckets and use the ryuk search to read
the archive and query for specific time
response or filtering by owner and the
remote user the key is the same as
previously but i had to change the
object and now in this approach it is
rya'c 20 map with to kiss one is the
message ID and the second is the message
itself and to search i'm using two
indexes first one is the default index
which is raya key it is used to filter
by the archive owner and also by the
user with which we talk and the second
key is message ID which contains the
timestamp and the ID itself with this
approach i finally achieved my goal
which was 10k connected users and 500
sent messages the average archive time
was around 150 milliseconds
and the 9 99 percentile archive time was
two seconds which i think is quite okay
in big system and the lookup time by
local time I mean searching keys and
reading all the messages and was less
than three seconds here we can see some
graphs of the archive time and it's
percentiles the lookup time in all skies
it was three seconds but usually it was
around 11 and a half and the time to
delivery the message time today lee
delivery from one user to the other
loser was filled 400 milliseconds which
is quite a big value but may be
acceptable during my implementations and
the load tests I discovered some
limitations of the Yokozuna search
mechanism and how it's done I discovered
that without indexing and the Yokozuna
engine I was able to write messages with
average time between five and six
milliseconds but when I enabled to
Yokozuna the average writing time was
around to 150 milliseconds and you can
see that the overhead put by indexing is
very very big i I didn't discover yet
why we have such big overhead here but
probably there are some ways to optimize
this the second lesson I learned is to
use react to
on one where they optimize the write
operation heavily and these test results
I show was done with react to 11 with
204 the results were not acceptable and
the next lesson I learned is to not put
term to binary as the value to react map
or any other react object which can
which will be indexed by yokozuna
because even if you set your schema and
disable some fields with this binary
data it will break the indexing I saw
some crashes in nyack console
complaining about the field with binary
data about some utf-8 formatting or
something like that and that's why I had
to put the raw XML in the message field
in the object and this implementation is
not done yet I still have to make some
optimizations and probably I will try
some other approaches without yokozuna
because of the big overhead it puts into
the system but so far the implementation
is quite stable and will probably be
merged sewn into master branch and that
would be all from me any questions
budgets any thoughts on rya'c vs hey
Cassandra have you had a chance to you
not yet I didn't compare AG versus
Cassandra ok but such such comparison
will be interesting definitely so read
provides like three backcountry like
memory bit can t be have you tried using
different for different type of data as
you know I I didn't try it because you
can set the whole arraya cluster to use
only single engine so I configured it to
use leveldb but in my next test I will
probably try bit cask in case of
archiving is fine by the user base I
feel it's better to use you think it
will be better to use bit casco ok how
only one key alright so by default we
use level DBZ but i'm not sure i'm
asking you yeah I didn't try bit casket
but I will definitely try it and
probably we will recommend to use bit
cask for some part of the data and lever
dv4 for others but still I have to do
some more load tests and the comparisons
between different engines
any more questions I just got the
journal crushin I mean Mongoose IM is a
massively scalable messaging system and
I mean with the integration with react
how would you see the you know the
future development what's on the road
map what's going to come in the future
date and we currently have road not for
for 11.6 we didn't define another
versions yet but as i mentioned i think
the iot will be our next key domain and
we we just started developing the XE
extension of XMPP which is efficient XML
compression and used in the iot domain
so this will be probably part of the 1.7
release and the US of 1.6 and ryuk 20
integration and the devil friendliness
is the key features right thanks very
much for mungu see i am and react to
point out and please thank our speaker
for
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>