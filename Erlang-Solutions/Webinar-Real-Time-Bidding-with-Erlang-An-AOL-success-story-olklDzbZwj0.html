<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Webinar: Real Time Bidding with Erlang - An AOL success story | Coder Coacher - Coaching Coders</title><meta content="Webinar: Real Time Bidding with Erlang - An AOL success story - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Webinar: Real Time Bidding with Erlang - An AOL success story</b></h2><h5 class="post__date">2015-05-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/olklDzbZwj0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you
hello and welcome to the erlang
solutions monthly webinar
my name is vladimir llyich church and
i'm the vp for the AMIA region here at
erlang solutions today's webinar
represents a continuation of a series of
webinars we have been organizing across
topics of interest in the world of
Erlang and dealing with solutions based
on the airline programming language our
webinar today will feature a no lying
solutions customer who has used airline
to implement real-time bidding or RTB
and by doing so have revolutionized
online advertising through offering
unparalleled targeting and cost
efficiency to buyers and helping website
publishers maximize revenue I'm speaking
of AOL of course and in 2013 AOL have
launched marketplace by ad tech a new
RVT system that serves billions of
online advertisements every day the team
at ad tech / AOL have traditionally been
using C++ for the ad serving
infrastructure however the introduction
of our TV brought along its very own
unique challenges we're a highly
concurrent and the fault-tolerant system
was a key requirement which obviously
put Erlang firmly into the picture now
as with any live events please do excuse
any technical issues that we may face
today but to start by telling you a bit
about erlang solutions we are a products
and services orientated business we are
completely devoted to the align
programming language and since we
started in 1999 we have worked with
organizations and individuals using our
line helping evolve the language and
supporting people and businesses using
it today we have just over 100 people
across our offices in London Stockholm
Krakow Budapest Seattle and most
recently Buenos Aires and we work on
projects across the globe involving
Erlang and Erlang based technologies we
provide services around the Erlang
language such as code in architecture
reviews online language training online
consultancy be data on customer site or
remote and we obviously support
aligned based systems on the product
side we develop solutions such as
Mongoose I am which is our open source
messaging platform the react distributed
database and wombat OEM which is our
monitoring and management technology as
well as other technologies applicable
across sectors and problem areas where
our line makes sense now I'm really
pleased to say that our speakers today
are Philip Locke a senior developer at
AOL Ronen or Rafferty the AOL technical
manager and Ken Wilson the chief
architect with the EdTech advertising
division of the AOL group please allow
me to finish by saying you are welcome
to post questions throughout the
duration of the webinar by using the
chat facility on the webinars interface
our speakers Philip Ronan and Ken will
answer as many questions as time allows
at the end of the webinar if any
questions do go unanswered you're
welcome to send us a mail and ask the
questions that way you can use the
following email address which is webinar
at our line - solution comm if you are
interested in learning more about Erlang
as a language or wish to establish
whether we could solve some of the
problems your own business may be facing
by all means feel free to contact us and
myself directly my email address will be
displayed in one of the final slides of
the presentation we will share with you
today the same goes for any other
questions you may have
feel free to contact us I would now like
to hand over to our speakers who will be
glad to start us off hi I'm for me a
well we're part of the a web platforms
group about eight years ago a well
acquired ad tech a digital marketing
company providing publisher agency and
ad serving products to manage server and
evaluation online advertising campaigns
including video display and mobile
formats
we've been working directly with our
tech glass eight years extending their
product range in today's presentation we
will deep dive into real-time bidding
RTB which is a huge service for our tech
which now is provide for our customer
base
but before we delve into real-time
bidding let me take this opportunity to
explain some of the online advertising
terminology a publisher is someone or a
company who is in the process of
producing and distributing content such
as literature music or information here
in a while we're focused on providing
publisher products for the internet and
these digital publishers to creation
provide content on websites blogs games
apps under stuff like that an advertiser
uses online advertising which uses the
Internet to deliver promotional
marketing messages to consume respective
products or services a placement is a
snippet of HTML or JavaScript code that
is inserted on publishers content or web
page wonder what paper is when rendered
and to the user this snippet makes an ad
request using HTTP to receive a creative
to be displayed on the web page an
impression which sometimes is called a
view or an ad view is a term that refers
to a point in which an ad is viewed once
by visitor or displayed once by web page
the number of impressions for a
particular advertisement is determined
by the number of times a particular
pages viewed or are loaded an ad is
almost always a banner a graphical image
like designated pixel size and a set of
ads for a campaign is often referred to
as creatives an SSP
or a supply-side platform is technology
platform with a single mission of an
enabling publishers to manage their
impressions and maximize revenue for the
websites markup AIDS the product that we
developed is an SSP which uses real-time
bidding to provide this functionality a
DSP demand-side platform is a system
that allows buyers their advertisers to
buy impressions to manage multiple
publishers through one interface
real-time bidding or more commonly known
RTB is a means by which publishers
impressions are bought and sold on a per
impression base to advertisers via
instantaneous of auctions similar to
financials markets with real-time
bidding avatars as bid on impression
and if the bid is one the advertisers ad
is instantly displayed under publish
society open RT b is an open industry
standard for communication between
publishers and advertisers it's as Jason
defined document that has real-time
bidding auctions requests and responses
to be made easily between third parties
and finally a second price auction is a
type of auction which better who
submitted the highest bid is the winner
but pays the second price or the second
bidders price or a floor price depending
on the particular configuration here are
some of the terminologies that you be
used in this presentation and hopefully
make it easy to understand as we go
through it
traditionally a check has been a price a
premium ad serving for publishers this
means the publisher would have an actual
team selling ad impressions via phone
calls emails using human relationships
signing contracts tracts with
advertisers creatives would have to be
distributed uploaded and hosted on
publishers of websites to display under
content as you can imagine this process
is a very timely costly and inefficient
way of selling ad impressions hence to
rise a real-time bidding a method which
improves revenue lower costs speed up
the selling process and provide an
overall experience better for a
publisher a browser under customers
ITEX tech stock has been traditionally a
java-based one in the front end where
the publishers can configure their
system and in the air and C++ in the
backend for fast ad delivery when we're
looking to build a new bed this new
piece of infrastructure RTB we took a
step back to see was there any better
development methods or technologies
available that we could take advantage
of the key teams are really important to
a real-time bidding system our fault
tolerance and concurrency
after numerous brown bears investigation
spikes we decided a prototype in Erlang
was suitable we took three developers
who were well versed in C++ and with
some functional language knowledge to
get a better understanding of the
language
after which they implemented our TB
prototype which was reviewed by a group
in the in terms of scheme until
maintainability load and deployment and
we made a decision to actually use her
line for building this solution and
we're going to deep a deeper detail
about how that experience for - later on
in presentation in this diagram that you
can see in front of you in the webinar
and you can see here's the publishers
page a knowledge is a is a placement or
a HTML snippet and it gets downloaded to
the users browsers as it's been rendered
it makes a an ad request or a HTTP HTTP
request to the ad server now since our
system is a as a merger of C++ Erlang
and Java
we've only decide on showing the the
mainly airline components there are
other components that happen as well so
this our component here the answer is
written in C++ when a request comes in
it takes information about the
impression takes key value information
takes user segment information and so
forth on the request to the ad server
it makes requests off to our
infrastructure for direct sales which is
a C++ component but for the purposes of
this presentation it's probably relevant
but it makes a in parallel it also makes
requests off to our RTV selectors and it
uses as 0 mq over drift which we're
going to detail later about further on
the presentation as this request is sent
over to our Artemis selector it finds
that information but the purpose of
picking this user and the different DSPs
which will happen in the auction so one
user will have many many mappings
sifters many DSP is used we fetch that
out of our USS the USS itself is a code
space closer storing user information
about these mappings and it's stored as
an XML file this information is loaded
into the into the selector information
in relation to the impression and how
the action will be executed all this
information is pulled together and
executed what happens is that we make
open RTP requests to the various
different DSPs using HTTP using
jiae's use no open are to be Jason
standard and get these bid responses
back and there's a surveys that need to
be taken in consideration 120
millisecond SLA SLA that needs to be
considered depending on the different
DSPs that take part and also depending
on the publishers configuration this can
be altered and what we do is that we do
a second price auction so as explained
earlier on and the second price auction
will occur highest bidders wins the
auction second price pay he pays the
second price of the floor price and that
information is then returned yet ad
server we we log all the information
have happened in the auction into a
reporting infrastructure which allows us
to analysis of why certain auctions were
one yeah and also to analyze how big
patterns happen into our system for
yield optimization purposes and what we
do is then return the snippet of the
winning ad to the browser just
displaying the ad on the publishers page
anytime that we are dealing with any
DSPs for we don't have any user
information we also inject pixels to
ensure that we going forward for the
auction that we do have these users
mapped it's very important for an
auction that the DSPs or the people who
are buying into the auction understand
the attributes of the user so having
these kind of mappings is essential for
the revenue and correct bidding process
to happen and so that's the kind of an
overview of how our system kind of works
what will happen is that you can't
explain how we got there little and then
we've deep dive in it into the
technology and as I said three years ago
we got a prototype together and we
rolled out an alpha I think was
january/february years ago and that
alpha consistent really connecting to
one demand partner and I want one demand
partner was only a couple of hundred
placements and making a only doesn't
request per minute so it was a very
small alpha solution over the duration
of time what we found that because the
team was learning we rebuilt it into a
more modular scalar scalable solution
and filled with dig deeper into that and
we evolved the system
over the three-year period one thing
that our rssp hired initially from the
get-go it was it wasn't feature party
with a lot of the competitors so we only
had one demand partner with a very few
requests into a system with only a
hundred different places over over for
over a few different publisher pages
today it has extended we have many
emplacements
over many many different publishers
sites and websites those requests come
into our system generate up to 40
billion requests per day to tear DSPs we
approximately we've integrated with 60
demand partners which is very important
what we've noticed is that as every
demand partner that we've integrated
into the auction introduce more
economics into the into the system and
it does increase in revenue for the
publisher we are the data centers we has
extended a east coast and west coast of
the US for the auction part of our
system
we've got 64 nodes each and we're
extending that and also for in Frankfurt
and in Amsterdam and we're fully fledged
SSP now so we support all the
technologies in relation to brand
protection ensuring that when when an ad
comes into a system that it's displayed
for the right industry and we also do
blocking of advertisers depending on
publishers span rights with with certain
advertisers we also have functionalities
in relation to malware scanning and so
forth so between data and yield
optimization this is a full-fledged
platform that we've developed over the
last three years using the airline as a
core solution to help us develop their
product and in the next part of the
presentation
what I'll do is hand it over the Philip
who would deep deep dig deeper into the
technology decisions that we made and
also talk about how we handle this in
production and steps that we use to
debug problems
so if I have my name is Philip and I'm
gonna take a look through our system
from the perspective of an engineer so
how did we get from a brand new
prototype system which went into
production just a few years ago to a
system which is nice sending over 40
billion requests to take part in an
auction each day I will look at the
challenges and how we overcame them our
airline helped us to get our product out
onto high we were able to manage the
system in production first then how did
we connect to the mossad service written
in C++ the architecture is a bit like
this the RTB exchange nodes are here on
the right hand side connected over 0 mq
to the add servers the add servers sent
to thrift encoded requests to 0 mq and
points on the RTP exchange QA can also
use a serum cue endpoint for their test
harness which is written in Python and
we also have a 0 m mq plugin for a Sun
for testing load on the system by in
reality the architecture is a little bit
more like this where every ad server
connects to every RTP node this gives
system a lot of redundancy it also
allows your on cue to balance out the
load across all RTP nodes for us in an
RTP note there is 1 0 mq process running
every request received by the 0 mq
process gets immediately spawned into a
new process called an auction process in
the system we will have many many
auction processes running concurrently
initially our code it is quite well
until we had some high load dan has
started to show up some problems for us
under some very high load we observed
the message queue started to grow and
grow
in the 0nq process requests for coming
in at a much higher rate than what the 0
mq process could take and i'm turn into
auction processes we discovered that
this was mostly due to a selective
receive statement in a 0 mq code this
has been fixed now and also we've done
some other major improvements on that
interface so far we have been able to
prove that the 0p new process can take
requests and spawn them into auction
processes at a rate of tens of thousands
per second when a request gets received
in the auction process we then validate
the request and select relevant campaign
data the campaign data is cached locally
and X tables these ads tables are loaded
when a system starts up and then they're
updated periodically during the lifetime
of the system typical campaign
information would include auction floor
prices protection settings currency
exchange rates and so forth the campaign
data which actually gets used can't
depend on the request so for example we
have to look at the geolocation of the
request it was from a mobile app or a
web page and if the originating web page
came from a secure its TPS connection we
also need to contact the car GPS cluster
the connection to cart space is handled
via process called Earlham seen
initially there was only one LMC process
handling all the requests for all the
concurrent auction processes this again
worked quite well until we had a certain
load and then we discovered if the
message queue started to grow also when
the LMC process to fix this issue we
created a pool of those LMC processes
and the queue sport is the application
we use to manage the pooling of
resources amongst the auction processes
when we have exam configuration data we
then determine which turn-on partners
are going to take part in the auction
for each domain partner we will spawn a
DSP process or DSP here stands for
demand side partner rather than digital
signal processor the DSP processes build
Jason messages for the main partners
depending on the protocol and the
version of the protocol used by that
particular DSP DSP processes use HTTP to
send the GSM messages over to the mount
partners on a long-lived
HTTP connection the sockets for the
connections are actually stored in a
pool and this pool is managed by l HT PC
and for each each TB has to be sent the
NHD socket rule allocates a socket for
that request and when the request is
finished the socket can go back into the
pool for further use the DSP processes
will then either timeouts or they will
receive a bid in the form of a TS
message from the DSP these will get
decoded and sent back to the auction
process the auction process will then
choose a winner and send the response
back to the ad server
so I said that there is a lot of
configuration data in the system
I'll not talk about how we started off
and configuration data and then how we
change our architecture initially the
configuration theater arrived into a
master config node each configuration is
is included in a JSON document each
document got parsed then stored into
amnesia before being pushed using
darling messaging to each RTD node in
the cluster under this mechanism to work
each RTB node had to be in the cluster
however if there was ever a network
issue or perhaps if a master config node
was restarted all the RTP nodes would
immediately reattach and ask a process
in that master config node to send it
all the configuration data that was
sitting there in a meteor this worked
well when we just had some configuration
data and a few nodes but his
configuration data grew and as the
number of nodes in the cluster continued
to grow the time to download the
configuration theater became
unmanageable another dark we had in our
mind was that we didn't really know how
big the cluster could grow or scale as
we continue to add new machines to it
this is the curve this is the
architecture we use today the
configuration nodes are now independent
so we've removed this master standby
concept any configuration service node
can have the legitimate when adjacent
document is received it is nice stored
in my sequel rather than an amnesia the
RTP nodes pull the my sequel database at
regular intervals to get updates for the
configuration the nodes are still in a
cluster here but they don't depend to be
in the cluster so if any node was to
leave the cluster it could still perform
the delivery of auctions
one thing we did have to do was migrate
all the data in amnesia into my sequel
but amnesia gave us a very nice API to
do this and we wrote a small East script
which converted an EZ theatre into my
sequel statements one requirement we had
in our system was the Bill features and
get them out into production fast the
RTB nodes are not just a transport
mechanism to transport requests from the
ad server to a DSP there is a lot of
business logic included in there as well
one of the main benefits of our line is
that it's easy to develop new features
fast this is also a brand new component
we were designing from scratch so we
decided we want to use best principle
design practices during development we
try to use tester and development as
much as possible we use a unit for
writing unit tests on Mac for marking
art external dependencies in addition to
this we use dialyzer for doing static
code analysis I sometimes adding specs
to our code for dialyzer to use can seem
a bit tedious but we do find that it
pairs back a lot for us by finding bugs
even bugs that we weren't able to find
using unit tests in the team refactoring
code is a way of life so new iterations
of features usually change something in
the existing design but having unit
tests and dialyzer means we can refactor
code with a very high degree of
confidence we do deployments every week
either we will release a new iteration
of a feature or it'll release some
improvement which gives the system some
extra hand scalability but didn't you
organize
Jenkins is used for building both the
C++ and airline components for the
airline
Jenkins jobs Jenkins will run our unit
tests it will perform code coverage
through the static code analysis build
an RPM for us and just recently we've
got Jenkins to auto deploy our RTP
exchange nodes into production the RPMs
which are built contain a full airline
release which means it has all the code
required to start the airline VM on a
bare-bones Linux box that means when we
install the RPM we do not have to have
airline install system-wide on that host
another challenge for our system was to
handle third party didn't in our system
the third party theater can come from
responses received back from the DSPs
and also the data in kircheis originates
from third parties they say when you
handle third party data that you usually
have to expect the unexpected this of
course is not possible and but airline
has his vice letter crash philosophy
which we were able to use originally
when we started to write the code we
were doing some checking and parsing the
data you received we soon learned the
best way of her process to handle krupp
data was to just crush one DSP process
crashing could not affect the auction in
any way and I cannot affect any other
ongoing auctions either it makes the
code handling third party data much much
shorter to write it also gives you a
very robust system
so I talked a little bit about some of
the problems we had and with scaling and
taking up more traffic and I'll talk a
little bit about the different tools
that airline has given us to monitor
this mr. eat up we found to be very
useful to monitor the system and find
out which processes were using the most
reductions and which processes were
having message queues building up this
usually highlighted some single process
such as a chance server acting as a
bottleneck in the highly concurrent
system as a result the single processes
usually got replaced by EDS tables are
we replaced them by a pool of processes
usually managed by something like a
queue sport F prof. is another really
useful tool we used with F prof we could
profile our system while it was live and
production without having to do any
special rebuild of the code and without
affecting any of the traffic running a
profile for by 10 minutes in a
production system gives you very very
useful insights for example we find that
we spend most of our time processing
convicted and and we were able to change
handling some long lists of advertisers
from oh and lookup times to constantly
cups by using ads tables again we also
find that encoding/decoding Jason was
putting a lot of CPU load on a system so
we replaced our airline implementation
with an F which is written in C for
statistics gathering we use fulsome and
we export a lot of statistics for the
system both for the application and the
VM statistics
exported every 10 seconds to Carbon
server and we've excellent visibility on
things like percentile response times
another request data finally the trail
system is something we have been using a
lot I can give a quick example of how
really useful that is in a production
system so in production we have a lot of
timers which we monitor which is
recently we notice a spike appearing in
one of these timers you can see from
this graph with the spike appeared
approximately every twenty to thirty
minutes
and the magnitude of the spike was about
five times the normal timer value we
were really unsure of what could cause
the spike was not obvious to us looking
at the code and it didn't really seem to
have any relationship to high load on
the system either by if you consider if
this was not an airline system why would
you find out what caused this delay for
one auction in about 1 million auctions
we probably have to rewrite the code so
at every event we get timestamp didn't
store that in an in-memory database and
then have some kind of alerting system
which can alert us when the timer
threshold was exceeded but we using an
airline clear system we didn't have to
deploy anything new to production we
just traced every new process spawn this
flow diagram shows all the steps that we
took there for every new process spawned
in the system we check to see we to call
a particular function unaffected we
recognized that that was going to be an
auction process when the auction
process was identified we activated
tracing with timestamps we recorded when
the messages only received by the
process at what times the process got
scheduled in a locked by the Erlang
scheduler recorded the linked processes
at the function which was logging the
timer value when the timer value was
seen to be over a certain threshold we
dumped all this information into a file
after a couple of hours we had all the
information logged in the file and we
were able to find the root cause of the
issue and fix it other highlights that
we find in the system would be including
the moving from 16 core machines 24 core
machines when we did this we did no
changes at all in our code and we find
that the load was balanced equally
across all cores in the system lastly
one of the really great things that our
line gives us is this pre-emptive
scheduler the pretty under scheduler
prevents long-running processes from
taking up too much CPU time this enabled
us to keep adding new features without
worrying much about blocking the RTB
options I hope this short overview of
our system has been useful for you and
I'm gonna hand over to Ronan he will
talk about the people and our
organization hi my name is Roman and
I'll talk to you a bit about the
adoption of Erlang and how it has worked
for our organization and how we even
introduced our people to Erlang so prior
to the start of this project we had
extensive experience building and
operating reliable and scalable
platforms in C++ and Java to a lesser
extent at this time AOL CTO was
evangelizing Erlang but also the CTO of
our business unit at Tech proposed that
the engineering team should examine or
as an alternative for new projects at
this time this marketplace this new
project was presented and presented new
challenges and the problem space looked
like a good fit for Erlang this led to a
successful proof-of-concept as has been
mentioned and this selection of Erlang
and so success led to success and we
eventually released the product all the
while the team involved were encouraged
to promote Erlang within the company for
example through simple demos and we also
did a demo for several hundred
developers within the company some of
our team was involved in a cross company
hackathon in a face-off between Scylla
and Erlang and we also found that when
you want to promote our line it's very
important to know and quote your best
metrics so of course in this case we
this relates to the number of
transactions that we process in terms of
ramping up the team and the knowledge at
the start of the proof-of-concept we had
a couple of developers with no
experience of Erlang he went into a room
for a couple of weeks after the
selection of Erlang we had a number of
training sessions with Erlang solutions
using follow-up sessions and advanced
training for a wider group of engineers
putting the product in into production
helped develop our knowledge about
operating out or lying in a live
environment troubleshooting identifying
weaknesses on bottlenecks and dealing
with those issues this was really useful
for the learning process subsequently we
enhanced our team through consultancy
with the help of could help on stability
scalability and also new feature
development
as the product launch was successful
this led to need to deliver more
features the ability to quickly quickly
onboard new airline developers was a
challenge that we did not anticipate but
we considered the challenge that we
could take on finding and hiring skilled
engineers is a challenge no matter what
your preferred programming language is
and so we continued with the approach in
AOL which is to look for smart people
this time we look for people with
awareness of Erlang our interest in our
lying
awareness of functional languages we
also looked to cross trained developers
in Erlang particularly to hands-on
experience and we now have critical mass
of engineers that can help that process
we've recently cross trained people from
Java into Erlang as well as C++ into
early finally we've also revolved two
years ago in hosting an Erlang factory
light in Dublin pretty much all of these
approaches have paid off today we
continue have a relatively small team
but we have a team of nine experienced
Erlang developers roughly six of those
are engaged full-time in Erlang
development we continue to leverage
consultancy to supplement our teams and
we find that our developers are our best
advocates for Erlang for example
speaking at events at our universities
and another point is that this year is
the first time we've brought on an
intern that has been working on in
Erlang so this is all progress so where
is the future life for us well we
continue to scale and optimize refine
and continue with feature development of
our platform we we can see that we'll be
moving towards a cloud adoption over the
next short while we continue to choose
Erlang where it makes sense and of
course we're continuing to look to hire
smart people and that's it from
presentation so I think we're open to
questions well firstly to say Thank You
Philip Ronnie McCann for what was a
inspiring talk on your implementation of
Erlang in real-time bidding and in
general and as you just said I'm glad to
say that had a lot of questions come in
during your talk I just want to thank
all members of the audience for posting
the questions and just to say that
whereas we do have a very limited time
frame to answer these questions
typically we do commit to answering
questions in the order they will receive
so let's start with the first question
I'd like to pose to yourselves Phillip
can and Ronan so first of all we have a
member of our audience Eugene asking are
you presently using physical hosting for
airline costs or do you use virtual
machines so any one of you is welcome to
pick up the question yes so I can talk a
little bit about that and you know
tradition a well we have our own data
centers so these are to be exchange
notes that I've talked about a lot are
running on real Hardware using 24 cores
mostly in in our data centers a except
in Asia where we use arms and web
services for that particular data center
we can see ourselves moving more and
more towards moving these machines to
the cloud whether it be Amazon or some
other provider thank you for that let's
move on to the next question I we can
already see we're receiving questions at
a faster rate and we can respond to them
I just like to assure all the members of
the audience that any questions that
have not been answered during the
webinar we will answer separately and
send you answers via email so let's
question coming from Kyoto Kyoto is
asking why did you choose my sequel over
other sequel databases yes so I I think
in our organization that the choice of
my sequels was was driven by the fact
that we do have an ops teams with a
dedicated DBA and working there so my
sequel was all was very familiar
technology to that particular ops team
and to minimize the risk it was just
better to choose a sequel database that
our team was familiar with rather than
to go and investigate a similar sequel
database of similar features that maybe
had some hidden gotchas that we weren't
all that familiar with thank you for
that and I think the next question ties
into that neatly Eugene is asking do you
have issues are you experiencing issues
with my sequel as a single point of
failure no we actually have a our my
sequel database is actually a number of
hosts running in a cluster so yeah we
have a fairly large cluster there any
any number of those machines can go down
and we can still pull the my sequel
cluster and get results part from it if
there was any particular issue where the
whole my school cluster would go down
and would be very unlikely but the only
impact would be that we wouldn't be able
to update our configuration data during
during the the time of those my sequel
databases would be done it's not going
to affect delivery in any way but but
generally our our cluster scheme is very
robust we haven't had any issues so far
where the whole cluster would go down
thank you for that moving straight on to
the next question and again apologies
for all the questions we won't be able
to answer which is quite obvious now
we're getting about five six questions
whilst answering one so again we will
answer in writing now Michael is asking
you mentioned in the present
you were using 24 core machines is there
a specific reason why you chose 24 core
machines yeah well we decided that 24
was better than 16 I think when I came
and when we were getting more and more
traffic into the system and the choice
was do we add more smaller machines or
do we had fewer larger machines so at
this point you know the development team
was also doing all the deployments as
well we didn't have them streamlined
very well either
so sometimes during the deployments
could be a tricky operation so it made a
lot more sense for us to go with much
fewer machines and to work with which
were more powerful rather than taking
smaller machines thank you for that that
that does make sense
so moving right on to the next question
in terms of debugging the tracing effect
well significantly affect performance on
production so just repeat the question
did tracing affect performance on
production during debugging yeah that's
a good question and so so tracing should
never affect performance on production
because I would always recommend you try
out your trace on a on a local host
before you actually do it on a
production machine and in general if you
do your trace properly you are not going
to see any any increase in CPU at all if
you use aircraft which use tracing
underneath it yeah we did notice maybe
up to about a five percent CPU load
increase but I mean that's that's
extremely small considering that the F
profits tracing every single function
you call in the system so yeah there's
there's various different applications
of wraparound the airline tree system
and such as maybe red button recon but
we actually chose here just to use
creasing provided by airline Chris or
else dbg module and you do have to be
very careful if you're working on a
console and you put on a Chris and by
mistake that traces every function call
you're going to find that every single
function gets output to your console and
probably that's going to increase the
CPU load and very high and the best
thing to your system at that point is
maybe to that it just crashes and
restarts so tracing is something you to
you have to be careful off when you
enable it but if you enable it properly
it's not going to affect your traffic in
any way at all and it's gonna have a
very minimal impact on the CPU thank you
for that and I just like to thank the
audience for all the questions we're
receiving at the moment we are getting
some fantastic insights and again we are
committed to responding to all of these
if not now at the webinar and in writing
straight on to the next question
Richard is asking can you give us some
idea of the size of your again six in
each cluster and we were small one - yes
what we do is that every every six
months eight months we do a forecasting
of highly product this is growing and
your traffic is gone and we do
predictions and that and that it can it
you know a big case or cut it how we
wonder our cluster notes the extender
data stands fantastic thank you for that
to try and honor as many as many
questions as we can we'll move straight
on to the next one so Oleg is asking why
are you using 0nq
instead of native airline RabbitMQ so
what's the reason for your use of 0 and
Q instead of something like Rob thank
you well rather MQ is just a it's a very
big application with many different
routing patterns that just it's just too
much to fit our needs and we in 0nq we
use the simple dealer Rooter pattern and
it's it's very lightweight but its
although it's lightweight it more than
meets our needs which is just to
transport a message from from one our
server to the RTD node so I haven't
dealt a lot with rabbit and queue myself
but I believe it's just a much bigger
fool featured application and we don't
need any of those features for this
thank you for that I think that makes
perfect sense now the next question
asked by a member of our audience called
Radu
is quite interesting because it's a
question that we ourselves get asked all
the time and you know we haven't found
the magical answer but I can try and see
whether you have a good answer for this
so the question is what was the biggest
hurdle in convincing your organization
to use a new language that it had very
little experience with both in
development and operations so how did
you sort of approach that problem so I
think the first thing is that it helped
that the CTO of was promoting our
alliance so I mean that introduces it
makes the path a bit easier so you've
less people to convince but I guess the
business unit owners and the technology
leaders in the business units have to
make these decisions and they were also
looking towards earn I'm given the match
in the problem space with the solution
that one can offer so we had to take
very
delivered considered approach to
evaluating our line and get all the
stakeholders involved the the architects
team leads the CTO involved you know
look at the results of our proof of
concept consider as many aspects as you
can and you know and then really it came
down to in the CTO making a choice and
that was I don't think it's an easy
decision obviously it's it's quite a big
jump but all those factors played
towards us choosing around I know what
suggest that understand the presentation
it's all about matrix it's all about
knowing your data and know what you're
trying to explain what by using a
technology where it's Erlang or anything
else and I think when we did the
prototyping with were working towards
the alpha everything's that we had when
we did we spend a lot of time being able
to understand what was happening to CPUs
understand the law understand how the
concurrent threads are being handled and
explained that and how the system would
grow as features were being developed so
everyone was involved you need people to
sign off at every level but understand
your data thank you for that and I think
you've mentioned a couple of points that
are definitely relevant from our own
experience first of all you know
involving old stakeholders communication
within the organization but also another
key word from our perspective and
experience is running the proof of
concept so we you know often get
approached by organizations keen to get
started in our line to any member of the
audience if you are keen on introducing
our line to your organization and if you
have ideas on a small proof of concept
that could deliver tangible value then
by all means feel free to drop me an
email which will be on the final slide
if it's not displayed already and we'll
be yeah it's right there we cannot
happily discuss with you your ideas give
you some guidance guidance
point
and so on so just to move on to the next
question and try and answer as many of
them as we can we only have a couple of
minutes remaining
David is asking what is your HTTP
request format get or post yep it's it's
it's post it's for post for I think
majority of DSPs it can be can be get as
well actually and when we integrate in
the DSP and the integration takes some
days complete and sometimes you get
requests on DSPs to integrate with us
most of them will use open RTP protocol
and then some will even have a room
protocol that we need to integrate with
so at the moment mostly posts but also
also get methods can be used when we
communicate to those DSPs and here's
another really interesting question from
the same member of the audience so David
is asking how do you control back
pressure in case you get more requests
than you can actually handle yeah and
this is actually quite an important part
of the system because at one point we
did add add to our form of 24 core
machines from virtual machines and they
could only work when it added some back
pressure to the system so what we what
we do is each request that comes in to
the / 0 and Q and we decide shall they
spawn the auction process or not and we
don't spawn the auction process if we
are handling requests over a certain
threshold instead of spawning the
auction process and doing the actual
auction itself we can send back to the
ad server or a valid response but just
tell you ad server that there were no
bids and as no one won in this auction
thank you for that and sorry please go
ahead
when imaged a request sir so a it's not
only protecting our system those we
integrate with other third parties as
well we need to protect your assistance
to do so different demand partners have
different rates that they can handle for
a second depending on the region and we
have that configured in our inner ocean
as well to ensure that we don't reach
any kind of any limits that downstream
infrastructures thanks again the
questions keep flying in so again I must
apologize we're not going to be able to
answer all of them we will answer we
will try and answer at least three more
questions and we'll answer all the other
ones in writing as I mentioned so to
move on to the next question this one is
really interesting Oleg is asking what
was or what were the bottleneck issues
with Jen service yeah so so it was a
pattern we find throughout our system
and we have lots of concurrent auction
process is running and then each auction
process can spawn multiple concurrent
TSP processes so if many thousands of
concurrent processes running in your
system so if all those concurrent
processes want to use a resource and
this resource is guarded by one single
process that single process is only it's
going to be scheduled in fairly by the
airline scheduler so it just means that
that one single process and system
cannot possibly get scheduled in enough
times by their own scheduler actually do
all the processing that it needs to do
you know what one another thing that we
did try once as well was Ria's the
priority level of a single process is
the high priority which did actually
give a process more time being scheduled
and that's that did have a certain
amount of success on it but I think the
principle is in a concurrent system you
want everything to be concurrent and
never depend on one single process in
the system so
as I said before you can either turn
that single process into a pool of
processes or else quite often you can
find that that information of that
process has can be represented by an ads
table with read concurrency enabled on
it thank you for that we have time for
two more questions so we'll go straight
to to the first Michael is asking can
you tell us a bit more about user
nothing to DMP data so basically what we
have is that we use the vacuums in our
system we have maps and we know the user
so it's a very good user experience ESP
are involved in the auction they have
their own representations of these users
they would have their own segments and
wrong way of looking at that across
their infrastructures so what normally
happens is that when a request comes
into our system and a DSP is to be used
if we don't have a mapping for that user
in our DSP or in our infrastructure we
know mean pixelated that specific DSP
and what happens there is that when and
I get silhouettes user a pixel to the
DSP is also embedded in that makes a
request to be its VSP given the
opportunity for the DSP interview
recognize that user and then to
boomerang back to us sending information
which we didn't have as a key reference
for that specific SP for the Pacific is
important I'm going forward then that
user community our system will be
recognized by us
we barely use that key for that PSP and
a PSP will be able to recognize the
benefit for out for the DSP is that he
can vary new frequency Kathy he can make
the experience better for for the user
who's using their system and also for
the publisher properly price T be the
infantry thank you for that obviously
it's all about the user experience so
apologists are audience but we have time
for one final question
and the question comes from Peter who's
asking what tool do you use at the
moment for automatic deployments in
terms of something like share for
puppets and what are your experiences in
a nutshell yes so we use ansible
actually for doing the deployments and I
haven't working on it very much
personally myself but I suppose from
what I've seen and ansible is a very
nice solution because you we don't have
to have anything special and stuff
called on the host for deploying to like
a chef server for example and also be
you know looking at the scripts we need
to do the deployments you know it's
looking much more like configuration
data than an actual program so yeah I
can't give much more information about
ansel versus other technologies but so
far it seems to be a really good
solution working not for us fine well
that was the final question we could
accommodate thank you so much to Philip
Brown and then Ken for a great talk on
AOL's
implementation of our line many thanks
to everyone in the audience who has
joined us for the webinar please join us
again for our next monthly webinar and
following today we will be sending you a
short survey to make sure we capture
your feedback of today's webinar please
also note that the recording of this
webinar and the presentation share today
will be available for you to collect on
our corporate website at airline Python
solution com I would also like to
mention that our online user conference
in Stockholm is coming up and will take
place on the 11th to 12th of June you
see as we call it is the oldest and
largest airline conference in Europe and
is a perfect place to hear compelling
bleeding-edge line talks and simply
mingle with like-minded langurs the
early bird tickets sign-up ends on May
15 so if you'd like you can register for
the comp
following the link on our website or
simply by going to the airline factory
website alternativa just search for the
airline user conference in Stockholm
online we look forward to meeting all of
you who can make it to Stockholm I'd
very much like to mention that AOL is a
platinum sponsor of the airline user
conference in Stockholm and it's also
worth noting that AOL as a company are a
member of the industrial airline user
groups of supporting Erlang in that way
as well thank you all once again and we
very much look forward to seeing you on
our next webinar</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>