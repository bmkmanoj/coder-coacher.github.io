<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tech Mesh 2012 - Deterministic Parallel Programming with Haskell - Duncan Coutts | Coder Coacher - Coaching Coders</title><meta content="Tech Mesh 2012 - Deterministic Parallel Programming with Haskell - Duncan Coutts - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tech Mesh 2012 - Deterministic Parallel Programming with Haskell - Duncan Coutts</b></h2><h5 class="post__date">2013-08-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/d6XigsnIk0s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yes so just briefly a little bit about
myself before I go and talk about powers
and concurrency so I have a background
in in academia and then more recently in
industry
I will founder this Haskell consultancy
company along with some colleagues of
mine and so we do we help companies that
use Haskell that's that's that's basic
what we do to do training we do
development support planning we help
people who are thinking of using it or
people already using it we support the
compiler and tools and all sorts of
things to do with Haskell ok parallelism
concurrency then or maybe I should
really say parallelism is not
concurrency we'll see we'll see what
what I mean by that ok so what's the
point we want to make programs faster
right we always want to make very
everyone let's make programs faster for
whatever reason you know it might be
because you want to deal with more data
or because you want do you need to use
less hardware a cheaper save money or
you might want to get the answer quicker
you know there lots of reasons you want
to make program everybody wants to make
their programs faster so parallelism is
about making fragrance faster by using
more hardware like CPU cores or GPU
cores or whatever big vectors whatever
ok so the parallelism is is hard and if
you have written or tried to write many
parallel programs you'll know it's hard
getting real speed ups and making the
program still give you the same correct
answer is hard and there's various
reasons for that we actually have to
understand what our programs are doing
you've got to understand our programs
better or mr. when you try to make them
run in parallel we need to know what
their what they're doing how they're
what they're doing at runtime
not just statically what the code is
calculating we need to know which bits
of the program are taking a lot of time
you actually to start profiling our
programs and then we have you have what
particular you have got understand
dependencies data dependencies within
the program to be able to know what
could be paralyzed and then you have
these killer issues like the parallel
work granularity versus the overheads
you know is it worth spoiling off
another time
don't go and do this or is the overhead
of that make it just not worth it
alright that that's something that comes
up again and again with with trying to
get real performance improvements with
parallel code and then you have these
horror things threads and shared
variables and locks and non-determinism
and that all makes life difficult so
some of those I think are are intrinsic
so in particular parallel work
granularity and having to understand our
programs I don't think you can get away
from that when you're trying to make
them run in parallel but some of these
issues that make it hard depend on the
program model that we choose and I'll
come back to that
so concurrency on the other hand is a
way of structuring programs and it's
structuring programs to use like
multiple threads or activities that work
independent of each other and then
sometimes communicate or synchronize so
for example threads shared variables and
locks one to one model or Erlang style
in a lightweight processes message
passing that's another concurrency style
and you're typically with you're dealing
with concurrency are typically reacting
to events from the outside world and
that's really sort of what makes it
inherently non-deterministic is that you
know you're getting events coming in in
any all and you have to deal with them
you don't get to choose when what
happens next the rest of the world
determines you know what what happens
next so you've got to react to things
incoming so it's inherently
non-deterministic and the concurrency is
also hard
I mean programming is hard but
concurrency is particularly hard and so
you've got you know deadlocks and data
races you know if you're accessing data
without taking the lock you getting onto
tenant suit behavior you always fit down
to tell us about you it's inherently
part of the concurrency model that there
can be non determinism you might try and
reduce it or eliminate it but it's it's
there testing becomes very difficult
with concurrency because there's all
these possible interleavings and and you
know on a raid on on a blue moon you
might get a different interleaving than
one do normally test and that makes it
crash and that's really really hard to
test and concurrency makes parallelism
even harder so a lot of those
difficulties and
and a moment ago to do with parallelism
there's a really difficulties to do with
concurrency so in particular all the all
the problems that you have with shared
variables and locks and threads non -
sounds and data locks data races these
are all things that come from
concurrency that we also often have to
deal isn't in parallelism that makes
doing that difficult right and one of
the most horrible things to do is take a
nice sequential program that you
understood and then try and paralyze it
using concurrency and suddenly it
becomes horrible it becomes much more
complicated and it's much harder to read
it's harder to maintain it's harder to
test it it makes us unhappy except it
might run faster right and that was the
reason we did it but everything else
gets worse okay I've been bashing
concurrency but obviously you know
concurrency has its uses when you're
using concurrency right it makes your
programs better it makes them easier to
understand if you're writing something
which naturally has to deal with events
from the outside world it's very often
much nicer to write a threaded program
than a big complicated event machine so
servers are the obvious example of that
sort of thing you know you've got an
interaction with a single client you can
think about that kind of in isolation
and then just have lots of those in
threads simultaneously rather than
having to deal think at once about your
interaction with all the clients
simultaneously you can think about one
and concurrency lets you structure your
programming that way which makes it
nicer easier to understand okay so I've
said parallelism is about making
programs run faster and concurrency is
ways of structuring our programs into
these multiple activities and these are
really kind of orthogonal things so
related but there are there are four
g''l ideas so one is about performance
and the other is about how do we write
the program that the structure of the
program so why am I making this song and
dance about distinguishing between these
two things because I very often you
people just sort of use them somewhat
synonymously right and it's so that we
can think about what it is where
trying to do right if if we're trying to
make our program run faster by making it
run in parallel then do we really need
we really want all those things that
come with concurrency and very often you
don't we'd really rather not if we could
avoid it right now so I'm gonna claim
and I'll show you you I think convince
you that all the combinations of
parallelism and concurrency make sense
right
certainly no concurrency no power makes
sense because most programs that we
write are not parallel and they're not
concurrent they're ordinary ordinary
programs concurrency with no parallelism
is what we've been doing for years with
single core computers and running
multiple you know offering system
processes or multiple threads on a
single single processor right next
perfect sense and then similarly running
multiple threads on multiple CPUs that
also make sense for parallelism with
concurrency those those bits names so
we're going to be concentrating on on
that what does it mean to have no
concurrency and yet to have parallelism
now obviously that I talk is gonna be
also related is how do we all do all
this stuff in Haskell and so Haskell
supports both concurrency and
parallelism and it has pretty good
support for it for both we have a
relatively traditional approach to
concurrency we have threads and mutable
variables and threads synchronized by
communicating using mutable variables
and on top of that you can build other
libraries like message passing style
actor libraries and other high level
things on top asynchronous futures or
all that sort of nice stuff is all built
on a substrate of the basics of locks
and what mutable variables would locks
and threads but we also offer a bunch of
different styles for doing parallel
programming and all of these are
parallel programming without any
explicit concurrency and I will spend
rest the talk explaining how that what
that really means how that works so
we've got a number different styles
we've got expression style a data flow
style data parallel style we we have
these multiple styles because not every
problem fits the same not a very
problems are different and some just
work better with one style of
parallelism and and others work better
with it with others so it's not with
concurrency there's fewer models maybe
with parallelism there's a bit more
variety so ok each of these in haskell
is implemented as a library with some
underlying support from the runtime
system and as I say yeah these different
styles are suitable for four different
sorts problems and I hope it'll as we go
through become slightly clearer become
clear what which styles make sense which
sorts of problems now I can't help but
say something about concurrency in
haskell because we actually have a very
nice story about concurrent somehow
school if you know anything about high
school you'll have heard of it io monad
concurrency in Haskell uses the IMO node
that means that where it's this sort of
imperative sub language of house gold
where we can interact with the outside
world we can send messages use a network
file IO whereas the rest of Haskell is
is pure like pure functions but the IO
sort of imperative sub language lets us
do all of traditional imperative
programming we have lightweight threads
that's the critical thing very
lightweight threads we have a slightly
nicer story about locks and mutable
variables where we combine the lock with
the mutable variable so you can never
accidentally access the mutual variable
without taking the lock so that that
eliminates some bugs but it's
fundamentally it's still locked so you
can still get deadlock software
transactional memory is a thing that we
have in Haskell it was developed partly
in Haskell and this gives us rather than
locks which are non compositional you
know you can get deadlock by taking
locks in the wrong order and that means
programs you can't compose programs when
you've got this issue of locked with
with software transactional memory you
can you can compose transactions and
that's really nice I'm not I'm talking
all about that but if you're interested
STL look it up this it's great stuff and
there's a really nice thing is that we
have traditional style blocking i/o
which i think is under appreciated I saw
the same about a second
yes so Haskell threads are very cheap a
light like in Erlang like Erlang as
lightweight processes you can have tens
of thousands of them and that's no
problem whatsoever right these are
clearly not using operating system
threads right these these lightweight
threads are scheduled across cores by
the runtime system and the blocking for
i/o in these slight wave threads works
as you would hope which is that it just
blocks the one lightweight thread if you
if you have a thread in Haskell and it
reads from the from a file or writes to
the network and it blocks that just
blocks that one thread and all the other
threads carry on right just as you would
hope it to work and there is a way also
to do to make calls to foreign C code
for example and have that not block
every thread in the runtime system as
well so I think it is true to say that
IO in Haskell is kind of i/o done right
right
you've probably heard about this you
know events versus threads debate when
it comes to how should we do high
performance high concurrency IO events
faster or threads nicer right we can do
both and we can get the performance of
event based IO we have these you know
tens of thousands of threads all running
on one course a or two cores and you
know it's using a pole underneath and so
we get all this nice event style the
underlying mechanism to the runtime
system really look like what you were
getting an event based server but the
programming model that we have on top is
traditional blocking i/o which is just a
much nicer model for writing code you
don't have these well if you've ever
written any node.js stuff it's horrible
the callbacks become really really
complicated and even in in.net where
you've got these async future things if
you look at most of their examples
mostly what they're doing is actually
traditional sequential i/o they're not
taking advantage of any asynchronous in
the sense of running two things at once
they're just you know you when when this
file has finished reading then do the
next thing and when that finishes then
do the next thing and then when that
finishes then do the next thing
that's just sequential standard
imperative bio-programming why why do we
have this I think stuff
just because dotnet and that has
heavyweight threads and so you're forced
into doing all these other things if you
have lightweight threads and you do
traditional blocking i/o it's really
simple and it's really nice and it
scales really well and it runs on lots
of cores it's I know that's a rant I
think people are you know especially
there's no junior stuff it's just seen
so crazy okay pile isn't without
concurrency right that's that's really
what I want to spend most my time
talking about and that's that's the sort
of maybe slightly mysterious thing if
you've not seen it before so we often
call this deterministic parallelism or a
pure parallelism right and and it means
that we write a program that's not
explicitly concurrent so this this is
fundamentally a declarative notion we're
describing what we want and then we have
some way of executing that program in
parallel okay now let me give you an
example so SQL queries SQL is a
programming language in a sense okay so
I think you'd agree with me that a SQL
query doesn't express any concurrency it
doesn't say do this do that
synchronize past messages right there's
none of that it's just a pure
declarative description of what we want
the result what the result should be and
the queries are deterministic right
given the state of the database
yeah there's no there's no non
determinism in there but the database
execution engine has the opportunity if
you by Oracle or of something that it
can it can actually use multiple cores
you can use multiple arrests threads
running on multiple cores to execute a
query so that gives you I think the
notion of how you can have a declarative
non-concurrent description of some sort
of a problem and then an execution
engine which itself execute in parallel
and to make matter so confusing that
execution engine may itself the using
concurrency to achieve parallelism but
the program you wrote the SQL query has
no concurrency so that is parallelism
without comparing see because
concurrency remember is not about the
execution concurrency is about the
structure of our program what are we
expressing in the program
okay so determinism deterministic this
is deterministic just in the usual sense
of the word right it always gives you
the same answer given the same inputs
and this is what most programs are like
most of the programs we write most of
the time are deterministic and that's
good because I think is a much easier to
test and reason about and you know ones
are using random number generators yeah
yeah yeah but most of our programs are
basically sequential deterministic and
of course it is deterministic then we
get all these lovely properties it
doesn't depend on the number of cores we
weren't on doesn't depend on the
scheduler will always get the same
answer it can't depend on the scheduler
because we have to get the same answer
so whatever the scheduler does whatever
the OS is doing we have to get the same
answer there can't be any data races
because that would give you non
determinism and there's no dead logs
sounds lovely
so how do we do that so I give you the
example of SQL but how do we do this
sort of stuff in a more general what you
would more recognize as a programming
more general programming language okay
so one big class of of this is
parallelism that's available within just
pure expressions that you get in a
functional language so this is a really
trivial example but I hope I hope it
gets the point so if we have two
computations call F 1 X and G on Y and
if those are relatively expensive take a
long time to evaluate then if we take
the sum of F X and G Y then we have an
opportunity to run these two things in
parallel and then do the addition at the
end right and I think you'd agree that
like the SQL this expression does not
express any concurrency but we can and
evaluate this in parallel if we have a
mechanism for for doing and you can see
how that might work right so if we have
a mechanism yeah so we can use the
paralyze and get the result sooner so
that we've got parallelism without
concurrency so that's that's the sort of
basic idea and that's what works nicely
in a functional language particularly in
pure functional language and this idea
we can push a very long way so this is a
really really trivial example but this
idea goes a long way I mentioned parity
this works particularly with nicely in a
pure
program language like Haskell because we
know that evaluating FX and evaluating
GUI doesn't have any side effects so
there can't be any interference between
if we evaluate in this order or that
order or in parallel it doesn't matter
it will always get the same answer which
isn't always true in a hybrid functional
language which allows an imperative side
effects as well we're at a school you
can guarantee this so it doesn't mean
you can't do this in other functional
languages but we get a safety guarantee
that you can always do it if you if
you're doing it in a pure functional
language like Haskell so closely related
to this idea of parallelism within
expressions is parallelism within
expressions when you're dealing with big
arrays or collections and this this we
call data parallelism so it's all about
doing the same operation in bulk on on a
large number of elements in some
collection tip it typically arrays and
because the idea is that each
computation on each element is
independent of all the others then we
can do the whole lot in parallel in one
go
so yeah the typical examples are things
like big vectors matrices arrays and
operations in terms of those and I will
come back and talk a bit about that
later another library we have doing that
now maybe to convince you that this is
real I think I'll tell you a little bit
about how its implemented behind the
scenes because parallelism there's after
all about performance right so we need
we need something that's going to make
this work well not just something that
you know is nice in Syria so behind this
you know beautiful veneer of pure
parallelism we do need something to
really if I use in parallel and do it
well and so it's unlevel has to use
operating system threads because that's
the only thing the operating system
gives us to make use of course multiple
cores and the implementation mechanism
has to guarantee the deterministic pure
property of the high-level language
right go back to the database example if
you've got you know your database
evaluation engine is using threads and
locks and it's actually quite a hard job
the people writing that to guarantee
that you know it always gives you the
correct answer that they have all the
problems of deadlocks and locks and all
the rest of it so your implementational
mechanism has to has to guarantee these
deterministic properties but you can do
that once and then we use it again and
again and it's obviously vital that the
implementation of that is is of a high
quality so that you get the performance
that you're after and so that it's
correct right
unfortunately for us we have Simon
Marlowe who who run who has written the
juicy runtime systems been working on
that for years and years and it's a very
good quality implementation so this okay
the juicy runtime system GHC if you
don't know is the standard haskell
compiler optimizing native code compiler
so it has a runtime system links in with
your programs and that runtime system
provides this lightweight provides these
threats lightweight concurrency and
that's for the for the concurrency
support but it does it by using a single
operating system thread per core more or
less there's only one running at once
anyway and these lightweight threads are
then schedules across there's multiple
core so it has it has a thread scheduler
in there and it's got a garbage
collector which is relatively good
generational all that sort of stuff and
interacts reasonably well with running
running on multiple cores so how might
we evaluate this in parallel so the
roofing system has some special support
for evaluating for evaluating individual
expressions in parallel with whatever
else it's doing at the time so we can
take some unavailability expression like
f of X here and we spark it off and what
that means is that expression becomes
available for the same core or maybe
another core to take that under valuated
expression and evaluate it and do if you
do that on another core then you've
achieved some parallelism right so we
might spark off f of X onto another core
and have that evaluated elsewhere and
then on the core that we were running on
already continuing evaluating G Y and
there are there we've achieved some pair
ilysm so let's talk about the
spark notion and what and how this is
implemented heck just stands for Haskell
execution context you can think of it as
a core so each of these things are two
so here we got two cores those squares
over there are threads running on those
cores so that threat to core zero has
got two threads running on it and heck
one's got nothing running on to the
moment and then we've got this spark cue
so each core has a cue of the spark sort
of tasks and tasks are created by
already running threads using this
primitive called par which I will come
back to in a moment but that that takes
an expression and says spark that off
and so what that does is it sticks a new
spark into the spark pool or cue in fact
and now the other thread sorry the other
core notice it because it's not doing
anything it was idle there were no
threads running on us at the time it
notices that it's not doing anything and
says can I steal a spark from either my
own cue well that was empty or I can
steal a spark from another from another
cores Q so it stole that spark from the
first core and native started a new
thread to start evaluating it it
actually only makes two thread the first
time when that thread is done it will go
and look for more spark so we only pay
the cost of a thread startup once and
also that's also a lightweight Haspel
thread so this is really very low
overhead
speaking of low overheads so the sparked
pool is a lock free work-stealing Q
that's you know they're basically they
deserve concurrent data structure for
this sort of thing it's about as fast as
you can go each spark is just actually a
pointer to these expressions which is in
fact just a function pointer so
evaluating is just calling that function
calling that calling through that
function pointer and as I mentioned that
thread and it has to get started up the
first time after that it hangs around
and looks for more sparks so this is
actually very very low overheads
evaluation mechanism it's about as low
as you can imagine going really for this
sort of thing
yeah this is just just pointers and
jumping to things and because it were
this low overhead this lets us take
advantage of more fine grained
parallelism that we might be able to
otherwise if we had if you're using
operating system threads you know
there's a much higher overheads there in
the startup and the switching costs and
all that sort of thing so for that you
have to have much larger chunks of work
to be able to profitably take advantage
of it doing in parallel here we can
actually take advantage of quiet
relatively small bits of work but it's
still overhead it's not free yes
granularity is still important okay so
let's go back so that was implementation
let's move back to the the high level of
view what the programmer sees so we said
it's always safe to evaluate both parts
of affects the gy in parallel but that
doesn't actually mean that it's going to
be faster it depends on this parallel of
granularity issue if the amount of work
done overcomes overheads then then it's
profitable if it's not then it isn't and
it's pretty hard to determine that sort
of thing automatically so people
initially had this idea or in a
functional language it's all pure we can
just evaluate everything in parallel and
it'll be brilliant
it doesn't really work overheads will
kill you because there's a huge amount
of very fine grained parallelism but you
can't really take advantage of that
small grain so fully automatic pilot ISM
we're not going to get that really so
you as the program we have to decide
what it's worth evaluating parallel and
in case you when you start making a
program parallel you've got often got a
restructure with any way to to expose
more parallelism so this is this is a
programmer directive thing you don't we
don't get it totally for free so as I
mentioned before we have this low-level
primitives called par which is a thing
that sparks off these lot Sparks and
yeah has it slightly strange if you've
seen Haskell types you this might look
strange to you if you've noticed in
Haskell types don't worry about it
what it what it does is it says who is
take it takes two arguments and it says
when there is all
this par is required then we're going to
start evaluating the first argument and
then so start with alginate in parallel
you know create that spark let it let it
go after another core and then on the
same this core will evaluate the other
argument and return it okay it seems
like perhaps a slightly odd but that's
the primitive and all the other things
we can express on top of that so let's
do that with this example and again so
this is where this is very low level
this isn't what I would normally write
in real programs I just want to show you
how the low level implementation
mechanism connects to what you might do
is an expression level so here is we're
the programmer and the tating what we
want to be evaluated in parallel so this
very much makes it uglier but we'll come
to higher level ways of doing this sort
of thing in a moment so using the power
primitive so okay we have to say will
will bind X prime to the first bit of
work bind Y prime to the second bit of
work crucially that doesn't evaluate
them yet that just gives them names so
that we can talk about them but it
doesn't strictly evaluate them that's
crucial right because it's no good we
have to we have to spark off bits of
unev al you ated work right that's
that's really important so then we say
power off X so spark that off and then
start evaluating the next bit now piece
SEC it turns out we also need in
addition to be able to saying evaluates
in parallel we also need to be able to
say evaluate this serially sequentially
here and now rather than doing another
core so we're okay so we say bark off
the evaluation of that of f of X and
then in this core here now do the
evaluation of G of Y and then we'll
return the son right and peace AK is
another one of these primitives and has
a type that looks the same as as par but
all it does is it evaluates the first
argument now and then evaluates the
second argument and returns that and
that's turns out to be important because
remember I said both of those are X both
the X prime Y prime are unev a lead if
we didn't have the P SEC
there's no ordering on which of these to
be evaluated first so the compiler might
decide to start evaluating in this core
X Prime and that would that would be a
bad thing because X prime is supposed to
being being evaluated another core we
want to evaluate y prime on this core so
we have to say that all right so this is
this is low-level okay but I think you
connects the what we're doing in pure
experiments this is all still pure
expressions and it connects that with
what was the underlying implementation
mechanism evaluation mechanism okay so
on top of this we have this notion
called strategies parallel evaluation
strategies which is a library
implemented on top of these para mystic
primitives and a strategy describes how
to evaluate something in parallel and
something maybe like big and complex
complicated data structure rather than
just something a single expression so we
have this thing called as a strategy
type and the a is is the type of the
strategy for evaluating things of type a
okay and then we using is there is a
Combinator that says write given given
this value and unevaluated an expression
I am going to evaluate it using this
strategy and that will give me back the
evaluated result of the same type but it
was evaluated in parallel using the
strategy which described how to do the
parallel evaluation so a bunch of basic
strategies for a sort of single thing
single expressions no evaluation
evaluation or a virus in parallel so you
can imagine that our power is basically
just a thin wrapper around that power
out there but it's but it's as a
strategy and then on top of this we can
build strategies we can compose
strategies and build strategies for
working over data structures for example
so here's a there's one there's one
called power list which is a strategy
for evaluating lists and it takes as an
argument a strategy for evaluating the
elements so what it does is it evaluates
all of those elements in parallel and it
uses that strategy or
each element so for example if we had a
list of more complicated structures and
there was parallelism available within
the elements in the list then we would
say power list applied to our strategy
for for the element if our elements are
small we might just be using our sack as
the sort of just evaluate it it's
sequential serially no no further
parallelism within within the elements
now going in arity I keep mentioning
granularity it's it's very common that
that the data structure you have doesn't
naturally map to the sort of chunks of
work which we'd like to evaluate in one
go right we might have a data structure
well sometimes you just split into into
more independent bits of work sometimes
you've got many many independent bits
need to coalesce them you've got maybe
too much parallelism available and the
other heads would kill you and you need
to coalesce work into into bigger chunks
of work and so we can do that again in
this strategy style by making additional
strategy Combinator's which modify the
data structure before or yeah but before
before no I just talked about this this
example so power lists chunk is what we
do is that we've got it we've got a long
list of things we want to evaluate and
we can evaluate them all in parallel but
they're too small to evaluate
individually so what we want to do is
say let's take the first 10 and evaluate
those as one block and in the next 10
and the next term etc etc right so maybe
maybe if we have 10 of them together
that's big enough to be worth doing in
parallel whereas one individually was
too small so we're chunking them up and
evaluating those as a single parallel
task so yes what that's doing is its
increasing the granularity by a factor
of whatever integer you you plug in
there so let me give you a very small
example from a real program that I wrote
for a client last year this was part of
a bigger program that was dealing with
portfolios
and generating reports based on
portfolios and we have like you know a
few hundred portfolios and and though
things are changing with them all the
time and we want to evaluate these
reports and send them off to someone
else and we have lots a lot of them so
we can profitably make advance take
advantage of parallelism and this was
this using parlous chunk ten our seek
was the only change I had to make to the
entire program to then suddenly make it
run on four cores perfectly and that's
that's really nice when that happens yes
it scales scaled perfectly so what does
this is using that pilots chunk
dimension a second ago so we're saying
reports which is the unev alyou ators as
yet reports based on analyzing some
portfolios and that's a list of of these
things and then we're going to say
evaluate that using this parlous chunk
strategy and i determined by a little
bit of empirical experiment that
chunking them into ten meant that it was
enough to overcome the their heads when
it was one then you know you got some
speed up but it wasn't the other heads
were too high and a hundred or you know
a thousand was too much because I only
had you know seven hundred portfolios
but ten was about right and there's
usually a medium range in the middle
that will mean it'll work reasonably
well for you know two to 16 cores which
is this sort of you know the realistic
range at the moment if you had much
bigger SMP machines you would have to do
a bit more tuning but you don't know you
just have to change the parameters there
and so we can't get really good results
like that but parallelism is still hard
okay I don't wanna give you a false
impression that it suddenly magically
everything is easy there are still there
are still difficulties and you can see
already that you know I've had to think
about the granularity here to get to get
a good performance but once I did then
no I'm gonna skip that bit I was just
trying you can do MapReduce and and all
that yeah I mentioned granularity and
and this being still difficult what you
really need to solve these issues is is
good profiling to us and we've got some
some reasonably good profiling tools
because there's quite a few pitfalls
that you can run into you particularly
to do with this granularity issue you
know your work is too big you get
imbalances between you know you might
have if you've got you know two cores
and you split your work up into two
things and they're not perfectly the
same amount of work on each then you
know you're wasting one core by the time
you get to the end so you want to have
smaller sized units of work so that you
can balance them between cause better or
in the worst case you have fewer bits of
work thing of course you know that's
clearly too too little too large then
you get killed by the overheads and
there's a few other details that to do
with sparks the evaluation mechanism
spark tools might be full or you might
be sparking something which had already
been evaluated there's things that can
go wrong so we need to use a profiler to
to work out and we have this profiler
the runtime system does event logging as
this binary event log format and it's
quite low overhead so it doesn't
interfere with evaluation into a program
too much and then thread scope is this
graphical tool which looks at these
event logs and gives you a visualization
of what's going on so what's are things
can we see with yet so the the main
screen shows us the overall utilization
on all chorus that's the that's the top
one so we can see you know that's four
cores is the top bar up there and so we
can see that you know in this section of
the program we're using weaving for
cause mostly and then after that you
know we're only using one core and then
if we break it down by what's happening
on each core you can see that indeed
it's you know all claws being used at
beginning and then and then something
goes wrong because after that it's clear
that we're only using one core so that
might give us a hint about what was
going on it also shows us the garbage
collection these orange bits are garbage
questions so you can see here we're or
here we're probably doing a major GC so
we're pausing evaluation and doing doing
GC there and and GC as in parallel
programs can often be an issue because
we generally we typically when you're
doing things in parallel you often end
up with just more data structures in
memory you're retaining more stuff quite
often and and garbage collection doesn't
scale perfectly so
GC can be an issue sometimes and we also
have intrascope something specific to do
this with this spark evaluation
mechanism because there's a particular
problems that can occur with that and
we've got some new graphs that help you
identify some of those problems so we
have a graph that shows you how many
sparks are being created and converted
that means put into the spark pool and
then taken out and actually evaluated so
these red ones here are overflowing and
we also see the size of its particles so
we can see here this corresponds to the
program from the previous slide so we
can see that the spark pool is full for
this bit and after that it's empty so
that that's another symptom of the
there's no parallelism available once
the spot pool is empty there's nothing
to evaluate in parallel anymore
and that's why only one core was doing
rust the work there was no more there
was no more parallelism so then then
your next step would be oh why is it why
is that why are there no more sparks
after and it turns out it's because
we're overflowing the sparkle we're
generating 100,000 sparks right in the
beginning of the program and our spark
pool is only 4,000 big or something and
so you know that's that's not the right
strategy you have to generate sparks as
you go along and not just generate you
know a thousand a hundred thousand
upfront so those are some of issues and
you can you can see what's going on with
these sorts of grass a particularly nice
one that I like to do with granularity
I've not got a picture of it but we have
a histogram of the spark evaluation time
so you can see sort of how many sparks
you have that are really tiny and how
many sparks that are really big and that
can help you to see if you've got lots
of ones that are sort of below the
threshold of being worth it or if you've
got some that are just so enormous that
you maybe are getting imbalance problems
and that and that's quite handy for that
granularity issue now how much time do I
have
I may just have to skip repper which is
sad because it's rather cool maybe I'll
just tell you briefly about it but not
but not go into the code so repper is a
library for this data parallel idea
right this idea of expressing algorithms
in terms of arrays and then getting
parallelism semi-automatically
or mostly automatically
by by this method so you right yes the
idea is that you write your algorithm in
terms of bulk operations on arrays like
transform and compute this array based
on that array and and these sorts of
operations and you you compose those all
together to make your to make your
algorithm and then the library
automatically takes your array of you
trying to evaluate in parallel and it
splits it up based on how many cores
you've actually got and then runs you
know one core for each chunk and so that
deals with the current granularity issue
quite nicely and basically automatically
with with the the planner stuff I was
showing you earlier in the strategies
you have to think about the granularity
all the time with with data parallelism
because you because of the nature of
your you're expressing you know that you
have an array every single element of
which could be evaluated in parallel
there's a huge amount of parallelism
available and so the appropriate thing
to do is just automatically you know if
you've got two cores split the array in
half send one half to each core and have
those evaluate those separate parts of
the array and that can be done fully
automatically once once you've described
your algorithm that splitting up and
evaluating using a gang of threads can
be done automatically and that's that's
really really nice and there's lots of
algorithms particularly kind of
scientific and simulation sort of
algorithms which can be expressed quite
naturally quite short programs in in
this English style and this takes
advantage of lots of haskell cool
features type system thing as to to make
this all work nicely which I'm afraid I
don't have time to go into Oh No let me
just show you this demo because it's
very cool this is been little meyer
showing off wrapper so he this is he's
doing some video processing live live
video processing he's got on his mac
book 8 called well it's four cores
hyper-threaded i think so is eight
apparent cores and he's got this canny
edge detection
so it's a it's an image processing
Malcolm so each frame that's coming into
the video camera
you've only got you don't have long to
process it obviously and you're writing
you know the the edge detection
algorithm is specified in terms of like
a function the the function at each
pixel it's a function of the other
pixels nearby it in the in the original
frame and so you can play with the
thresholds and da-da-da-da-da but just
the nice thing the code is quite small
the code is quite easy to understand
it's quite similar to that sort of
mathematical specification of the thing
so you have this nice short code you
never have to say how it split up
it just gets done automatically and as
you can see it runs really nicely across
lots of cores and so that when that
works that's that's really cool okay I
know I can't tell you about the types of
the stuff but that's another time except
the key idea is that you have this
distinction between describing the array
that we want to compute as the final
result and then compute it in parallel
and there's this we make a it's called a
delayed array that describes the thing
we want to compute so declarative style
and then separately now we'll evaluate
that thing in parallel that's that's the
key idea that its resting on okay so
I've talked about a few different styles
so expression style I focused on very
briefly talked about data parallel style
in this array business and also yes
concurrency you can also write parallel
programs and house cookies in
concurrency and you know that's that's
okay and these are all pretty mature of
things now the parallel runtime system
has been being worked on you know since
since multi-core started getting hot and
every release it's been getting better
and better so the scaling now is really
pretty good there's a couple others
worth mentioning data flow style
parallelism if you come across that
before it's all to do is like fork/join
style and we can do that again in a pure
which is really a little interesting
nested data parallelism is like the
array parallels and they're talking
about but instead of just having dense
arrays dense multi-dimensional arrays
and matrices and whatnot we can have
arrays that can contain arrays which is
not the same thing as multi-dimensional
arrays right arrays it may be containers
and that allows you to do things like
sparse array sparse matrices because you
can represent a sparse matrix as
matrices sorry sparse matrices as arrays
that can contain descriptors of pointers
into indexes other parts and so you can
you can represent those things and then
the clever part of it is that the
compiler automatically flattens out all
those arrays into one great big array
and then does the same old thing of
chunking it up and send it off to
different quarters so that massively
increases the expressiveness of this
array style so it extends it into sparse
there's one example but there's many
other examples of nested data plasm
allowing you to express things in this
array parallel style and finally GPU
programming and the GPU programming is
actually very much like repper
this array parallel programming style
but with the extra complexity of having
to say whether this thing whether this
array lives in GPU memory or lives in
cpu memory so there's an extra type that
says whether it's in the GPU memory or
the CPU memory and makes sure that you
don't accidentally or that you're you're
explicit about when things move back and
forth but it's very similar style and
that it's all this array there's nest
you're writing bulk operations on the
rows and then we generate on-the-fly GPU
code and send that off to the it goes by
the cooter compiler and sends off the
GPU but the parallel programming style
is very much like this array like the
record stuff also some mentioned that
you know that this is you know mature
technology you can you can go and use
sign in PJ was he he was speaking in the
keynote this morning wasn't he
I was still finishing my slides so we
were working with him to do a this
two-year project to to show that the
parallel haskell really does work that
it is you know mature technology now so
what his idea was to get a bunch of
commercial industrial users and say we
will support you to write the parallel
code that you want to write so that hire
hired us basically as consultants to
help those groups so we had people from
from Los Alamos laboratory is a small
fisheries company in New Zealand that
looks that looks at models of fisheries
for the New Zealand government it does
lots and lots of simulations and these
simulations take a long time and you
know if they could do that quicker
they'd be able to do more iterations of
their modeling and you know that would
be a benefit to them and a few other
groups I put in particular one one guy
who is working on highly concurrent web
servers for ISP in Japan
and most of these groups were not
existing Haskell Expos so we had to do a
little bit of they new Haskell a bit
they were beginners basically and so we
had to do some so part of what we were
doing was teaching and then you know
pointing them to the right techniques
and tools but basically the one really
any significant problems yes the people
had to spend a little time learning how
school but they got that relatively
quickly I was quite impressed because
functional programming is often a big
leap for people you know in a
particularly pure functional programming
but they did they did pretty well we had
to develop a few missing libraries that
they needed the los alamos group who
need is a high-performance parallel
random number generator that was
statistically good enough to use in
Monte Carlo or some sort of simulations
I don't even know who's I I don't
understand the physics of it and we
extended this parallel perfect profiling
tools the thread scope I showed you
earlier we added the spark profiling
into that but basically they did they
did okay so in particular the the los
alamos guys so they were writing this
physics simulation high-energy particle
physics something to do with black holes
and astrophysics
and they had an existing which are
single threaded
sea version of their simulation which
they'd tuned and it was fast and we
never got the Haskell version to be as
fast as the C version we got it to be
about half the speed which was we were
we were happy with that they were they
still thought they could do better and
maybe they can but that was on one core
and the hospital version well the
hospital was
nicer code to start with and it's scaled
really nicely it was quite easy to have
parallelism to it and it and it scaled
very nicely up to the number of cores
that they had available and actually
interesting thing this isn't really so
much Apparel ISM thing but just
programming in a nice language is nicer
thing is that they eventually the the
reference implementation became the
Haskell one because after working
without them for a while they realized
that it was much clearer and nicer and
easier and it was a much easier to tell
that it was giving the right answers I
mean it's hard sometimes in simulations
to tell if it's giving you the right
answer because you don't really it is
hard to specify in some of these
simulations what the right answer should
be so eventually that the C version was
adjusted to match the the new reference
implementation which was the house got
one and they also wrote and I've been
talking about SNP concurrency here but
on this project we also did some
distributed concurrency we wrote this
cloud Haskell thing which if you ever
hear a silent talking about is rather
fun and they also they also wrote some
versions using MPI and Haskell MPI and
Haskell is still MPI is still ugly but
the cloud Haskell ones not too bad and I
think what the best maybe the best
result is that the programmers were
actually doing it were much happier with
it there they were happy with having a
parallel version you know it was a
parallel version that didn't wasn't
painful yeah so I think that's that's a
good result and and the performance was
good right the performance they were
beating the C version because they were
using multiple cores I'd say they got
good performance the code was nice
everybody was happy so that's it thank
you
customs yes that power thing does rely
on the lazy evaluation yes I imagine you
could come up with similar things that
would work in an eager language but
you'd have to do things instead of being
a single expression it would have to be
like a function applied to an argument
for example yeah see you part you'd say
power of F and X rather than just saying
power of F applied to X that would be
the distinction granularity yeah yeah
yeah I have actually done that yeah what
I what I really want to do is on that
histogram of spark evaluation x put like
a red line in there to say anything
below this line is like below your a
threshold of overhead and it's like you
know this is now you're spending half
the time and the other head now it's not
worth it I haven't done that experiment
but it wouldn't be that hard to do right
it's just a it's just a benchmark you
could write and and yeah you're right
it'll vary from one machine to another
of course although it'll probably
broadly similar because it's all the
same you know it'll it'll be a mean it's
like cash and that sort of thing yeah
it's a good idea yeah no questions yeah
okay so the question is has anyone tried
sort of more automatically working out
what the right parallelization version
switch is so then the nearest thing that
people have tried is the did an
experiment where they took a whole bunch
of programs had not been written for
parallelism and they said how much
parallelism is are really available in
this let's let's pretend that you could
evaluate everything with no overhead no
overhead to doing it things in parallel
how much parallelism is really available
and they found that in most programs it
was only about two or three so and that
was very fine grained parallelism as
well so you might not apply the same so
I guess the way you might go about that
would be to write a strategy which
followed the full structure of the data
so you know a list of this of that of
yet so you write a throughout the
general strategy which looked everything
and do some vlogging while you're doing
that and then and then analyze that
later and go okay you know we're gonna
have to cut it off here and it's even
worse than that rosanoff too chunky it's
a good idea I don't think anyone has
really tried doing it because you can do
it as a programmer yourself because you
can change the because you're writing
the strategy separately from the
algorithm a lot of the time right that
was maybe the thing I didn't really know
clear to the skipper gate was it very
often you can separate these things so
changing the strategy can then become
just a matter of fiddling with it
recompiling run your benchmark okay no
it's still overheads
you know it's other thing let's chunk it
here or increase the factor there so
it's not it's not that hard to do once
once you've got into that once you
started attacking it modifying your
optimizing your strategy is not too bad
but yeah you could you possibly could do
more automatically the other question
that's a that's a research problem okay
so the question was can you extend this
idea of data parallel you know
computations of arrays that we've got
working in an SMP multi-core setting
couldn't you extend that doing it you
know in the cloud and the answer is that
lot those people who really really want
to do that and there's a company I know
that it's working on that but it's it's
I think that's quite a kind of tricky
problem improvable it seems like it was
totally possible but I think there's
some some tricky engineering challenges
that would be really cool I'd love to
see that because I've been working on
this cloud Haskell thing and I've been
working with the people who've been
working on ratha and if you love day to
combine the two yeah because the the
model in principle works for that and
and Simon Peter if you listen to him
says that data parallelism is really the
only way that we're gonna have really
large scale powers and dealing with you
know thousands of cores may be on one
machine or across hundreds of machines
this the strategy stuff you know you can
scale up to maybe you know 16 64 I don't
know how many quarters but they'll be
some limit of that it's not going to
scale into the hundreds of thousands the
data parallelism that really could if
you've got a lot of data that there's no
principle reason why that well that
shouldn't scale really well because
you're you really spit they're really
independent you're really just splitting
it off and evaluating that section in
separately on each on each node I think
we better better wind it up</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>