<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2016 - Jamshid Mahdavi - An Erlang based Philosophy for Service Reliability 1 | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2016 - Jamshid Mahdavi - An Erlang based Philosophy for Service Reliability 1 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2016 - Jamshid Mahdavi - An Erlang based Philosophy for Service Reliability 1</b></h2><h5 class="post__date">2016-03-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tW49z8HqsNw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm really excited to be here talking to
everyone at Erlang factory this is the
first time I've had the opportunity to
give a talk here and it looks like
hopefully it's gonna be a fun talk but
it's called an Erlang based philosophy
for service reliability which maybe
you're not even sure what that means
that's probably okay I'm not sure I
understand why I picked all those words
but I'm gonna talk a little bit about
Erlang and a little bit about our
service and a little bit about
reliability and hopefully this will be
an interesting talk a little about me I
have primarily been doing networking
since sometime in the 90s and have
worked on TCP and I did a bunch of work
on products that do high performance
proxy services and I came to whatsapp in
2013 this is my first first experience
with Erlang and have been really excited
to work with Erlang and don't really
want to turn back at this point I work
on the media servers and media delivery
within whatsapp there's actually two of
us that do all the media delivery and
yeah that's that's what I do there's
actually a talk going on downstairs
right now about media servers which I'd
probably really listen to if I was not
speaking here okay a little bit about
whatsapp for folks that don't know we're
a messaging app that does chat immediate
delivery
we had voice calling last year we got
bought by Facebook in 2014 and we've
been fortunate to still be able to keep
most of our sort of separate whatsapp
culture and not being completely
absorbed into the larger company so
we're still a small team we actually
have about 25 engineers on our server
team that work inner lining out of a
total of something like 57 engineers and
we do continue to have a growing user
base I think earlier this year we
crossed a billion users which is pretty
exciting for us and a lot of what we do
at whatsapp is is about scale and
scaling I have a couple slides here that
just kind of talk about
our systems and and what it is that we
use this slide if it looks familiar it's
because it was one that Rick gave and a
talk a couple years ago and I
shamelessly lifted it to include in this
light here the our system is you know
primarily a chat system that runs on
people's mobile phones there there are
seven supported phone platforms that
connect into our service we have a
front-end tier of servers there's the
chat servers which speak the chat
protocol the media servers MMS which do
all our media delivery there's a web
tier that's also not shown that does our
web client and so basically all the
clients come in through one of the
front-end tiers and then on the backend
we have a private network and all of all
the different systems communicate with
each other over disk or another protocol
that we call Landis which is basically a
protocol for communication between
clusters of Erlang nodes and in the
backend each of the individual services
shown there is implemented as a separate
disk cluster and implements sort of a
one function capability and so for
example the the group servers that are
shown in their store essentially the
list of all the whatsapp groups in the
world and who is members of those groups
and if you look at like the profile
servers those store each person and each
groups profile picture and so we have
sort of a cluster of servers that each
implements an individual back-end
function there's about 20 of those
different clusters that build up to make
the syrup make up the service and
deliver all the things at the front end
servers do our server specs themselves
we we don't run our own data centers we
use a hosting provider and we get
bare-metal servers that primarily are
off-the-shelf tool cpu servers 128 is
typical but we'll go all the way up to
some of our server 768 if we need a lot
of memory primarily the thing that
consumes memory is our databases so most
of those back-end services are amnesia
amnesia based services
that are you know either running dis
copies or running RAM copies and some of
them use a lot of memory our platform is
FreeBSD where our new servers were doing
FreeBSD 10 now we still got some that
are doing FreeBSD 9 and we're running
Erlang R 16 B which is getting a little
long in the tooth so we're probably due
for a major version upgrade sometime
soon so that is kind of the intro now
maybe a little bit about what I'm
actually talk about so this is not
really a technical talk it's it's kind
of a it's kind of a strange talk we've
done other talks in the past about
scalability and performance and Anton
data talked a couple years ago about our
analytic system that's that was all
written in Erlang I was approached last
year by some folks at Facebook and some
some how it came to pass that we got a
reputation within the folks at Facebook
for delivering a very reliable service
and they asked me you know kind of can
you come down and talk to us about how
how whatsapp thinks about reliability
and and how you make a reliable service
and I said sure I'd be happy to come
down and talk to you I thought about it
and I kind of didn't actually know what
it was that we did that was special like
I think there isn't really anything that
we do that's special to make our service
reliable but I did I did kind of think
about it a bunch and put together a talk
and that talk went pretty well and it
turned out much of the talk was about
our line so I thought this would be a
great audience
to to also show this talk to you this is
not really intended to be a recipe or
like you should go out and do it this
way this is really what we've done
within whatsapp
obviously we've had some success and we
managed to stay small and so hopefully
some of the insights people will find
interesting or used
okay so the first thing if you want to
build a reliable service you should
probably decide what you what your goals
are in talking about reliability when I
started the other thing I should say is
I'm a relative newcomer everything that
I'm going to talk about here was built
and invented by people smarter than me
who came before me and so I learned
everything from from a bunch of a bunch
of experts one of the things that
happened when I first started I think on
my second day at whatsapp I went to an
All Hands meeting and yon started out
the meeting and said we had an outage
this weekend just the day before I
started and it was big outage and people
lost messages and he gave a pretty
compelling speech about how important it
is that we don't lose messages and every
one of those messages is important to
our users some of you guys are probably
WhatsApp users you you probably send
stuff that is important and timely and
you know you I'm downstairs in the lobby
please meet me for breakfast or I am at
the airport please pick me up we take
this very seriously and so number one in
our in our goals and reliability is
don't lose messages right behind that is
don't delay them either if you know
you've got a mobile phone obviously your
mobile phone may go off the network it
may be offline but as soon as you step
out of that elevator and have network
access again we want you to have access
to the service and be able to get your
messages and so these are sort of our
our top two things that we are striving
for when we run the service and then the
other thing that comes in as we as we
develop new features and look at bugs
and work on things is we never we never
want to have a sort of mediocre service
right we want things to work and to work
reliably and consistently and so we kind
of set ourselves a bar of
we would like things to you know fail
less than one in a million if we're
gonna go out and measure something
that's that's our goal that's our target
this is not a prescription
we don't tell ourselves oh if we want to
deliver a six nine service and we want
to have an SLA that SLA would requires
to have thirty one seconds of Dom kind
of years so we're going to absolutely
require that everything we do results in
no more than thirty one seconds of
downtime year it's you know we don't
look at it that way this is really a
this is our aspiration is to always view
things consistently and reliably okay so
the rest of talk is basically kind of
what do we do in our development process
deployment processes operationally
running the service and to some extent
how does that tie back to the fact that
we ran Erlang shop and and we think like
our liners okay so development number
one thing that we do is we keep
everything as simple as possible and so
we have a minimal very minimal footprint
in what we put on to our servers the
next slide I have is is actually a slide
that shows what runs on our servers you
may be surprised at how minimal minimal
actually is and so this is a a PS tree
that I did on one of our running chat
nodes and that is every process that's
running on that chat server except one
thing I took out which is the actual
command that I ran PS tree so other
other than that that's it that's all of
it you can see the big the big honking
thing in the middle is being MP and the
rest of it is a few few services that
are needed for logging there's an NTP
service and a couple of different
monitoring services that we run that's
it there's there's nothing else really
installed on the server it's like a
super minimal freebsd installed
sometimes when we're working with the
folks at our data center they will ask
me to
mmm can you can you go in and run an LS
PCI and usually my answer to any of
those sorts of questions are no I can't
do it because we don't have that
installed we take sort of this minimal
keep it serious
keep it simple approach very seriously
in everything that we do okay the second
thing for development and this is this
is a very weird point this was you know
when I when the Facebook guys were
talking about like what do you guys do I
was like well mostly mostly really
there's just a whole lot of stuff that
we don't do and so you know we don't in
in in our day-to-day operations and
day-to-day running of things we don't we
don't do any like test automation we
don't do code reviews first I think the
first the first day if I had when I was
working at whatsapp you know I asked
Rick you know if you want to review this
he's like nope just checking in I'll
push it out that's the it's it's it's
sort of a strange environment I worked
like previous jobs I worked in
developments which were pretty pretty
serious Waterfall shops you know we we
didn't like big functional spec
processes we had a like a development
phase we had a big separate QA team
where the QA lab that had a QA phase
really complicated release processes
shipped a lot of stuff that way stuff
still gets out the door I also worked in
teams that did agile scrum there was one
team that I worked with that was like we
were we were we really drank all the
cool to kool-aid and we did all the all
the process stuff we also did all the
development stuff so we were doing TDD
and we were doing emergent design and
pair of programming to to like really
different ways to look at development I
think the strange thing about whatsapp
is like we don't really subscribe to
either of those extremes and we do some
third thing that is
really really none of the above and it
is a different environment and different
way of working which honestly when I
started and I think other people that I
talk to me started what's up after me
look at it like how can this possibly
work how can you possibly run a
development organization this way but it
turn it turns out it does it works and
it works really well for us the things
the things that I said how do you not do
this we don't we don't even serve for
our servers we don't even have build
servers we actually just check stuff in
deploy to the servers and and then run
it and load it so it's it's it's a sort
of strange in different environment to
work in yeah I guess and I guess the
thing that the thing that we do I didn't
actually say here is that in place of
all that stuff what do we do we'd be
careful right we know that when we're
deploying we're deploying to a billion
users and it had better work and so
that's that's kind of the trade-off is
care instead of safety net I guess is a
good way to say it okay so last thing
another development side that we do
that's that's kind of interesting and
maybe different is we we have this sort
of investigate every bug philosophy we
don't you know obviously with the the
the size of our user base we don't hear
about every bug but we do the ones that
we do hear about we take pretty
seriously and try to get to the bottom
of them we we really want to have a to
the extent that we can euro bugs service
that we're running and so anytime a bug
comes in we'll kind of drop things take
a look at it I got a I got a message on
on the weekend on a Saturday one day
from someone I was working with up at
Facebook saying you know I was trying to
communicate with someone on whatsapp and
the image or meat or media I was trying
to send it and go through and I said can
you send me your logs and he sent me his
logs and then I immediately started
looking at it
Saturday night and he was like no no you
really don't you don't really have to do
this just for me but it is kind of
ingrained into our culture that we do
when we hear about sup like that
whenever we hear about them we we drill
down into them and find out what's going
on and if there's about we we want to go
out and fix it
we didn't have a bug tracking system
when I started working at whatsapp we
did have one we wasn't used by the
server team and I kind of wondered about
that too but then I realized that you
know when we find a bug
instead of putting into a bug tracking
system we just fix the bug and that's
that's a pretty cool place to be since
we moved to Facebook Facebook has a
great bug tracking system and one of the
things that we started doing is now
tracking our bugs and in some ways you
could say that this is resulted in bugs
getting put into the here's a bug list
and then not getting fixed and so it's
an interesting trade-off of what you get
when you if you have a tool
beware of the tool because you might use
the tool okay so what about er Lang er
Lang er Lang really means a lot to us we
we couldn't do the things that we do
when we develop our product if it
weren't for our line just like though
the way that we do everything works
because of our lying the the top line
bullet here is our system our system is
really small compared with anything else
I've worked on and I know that this is
one thing that probably most airliners
would say about whatever they're working
on we we have my code base that I work
on from me serves is about 20,000 lines
of code the other other components of
our systems all those different backend
systems are similarly are similarly
small some of them are much smaller or
even this is this is partly the
compactness of Erlang and partly I think
because it's really well
to what we're doing so so this is this
is a big thing for us another thing that
is really and I don't know I would love
to hear feedback from people on this
that architecture that I showed with a
front-end tier and then individual
clusters of back-end services I don't
know if this is how everyone implements
things that are lying if I go out and
pull people in here about their services
I would find that yeah that's just that
just how everyone does it or if it's
just something that Erlang makes this
really easy to do and it's turned out to
be a great architecture for for our
service because each of those back-end
services it's running like we don't
deploy out the whole codebase every
everywhere we have one common shared
library with tools that everyone needs
that goes on all the servers and then
each of those individual servers just
has its its little slice of codebase
that needed to run that service and
that's what runs in the Biman on that
server so this been it's good for us
sort of operationally it's also good for
us from a reliability and failure point
of view because it tends to be when
there's a bug in one of those systems it
only exposes itself as some issue with
that one feature of the system and it
doesn't generally show up as something
that's particularly visible to users and
so this is a good benefit that
architecture but the other thing I kind
of as I was thinking about like what do
we do what we do this different than
other companies in the two years that I
was at whatsapp when I when I put these
slides together I'd seen a bunch of a
bunch of cases where someone had gone
into one of these modules and just
completely rewritten it from the top to
the bottom which is something that I
know a lot of developers always want to
do previous play side work you know
there's like this this code is terrible
it needs to be rewritten we need to do a
reorg texture but seldom does it ever
get done in fact there's there's
probably a whole group within Facebook
internally there where they talk about
like what can we do to promote more
hygiene and code cleanup and this is one
of the few places where I seen this
actually get done a lot and I think I
kind of went around and asked people
well why why do we why are we more
successful doing this here and this this
bottom bullet was the answer that I got
back from folks is that you know every
time we need to scale up we have these
clusters of 16 servers they are charted
using p hash too often time like based
on phone numbers into into a set of
partitions and we want to grow that what
we typically do is say ok we have 16 now
we will have 32 well split those
partitions you know partitions from one
box will now be split over to new boxes
to scale up and as we do that
splitting there's a migration process
that happens and during that migration
process is very natural time to go ahead
and update all the code to clean out any
Kroft fix any things that are that have
been needing to be fixed and so this is
kind of a kind of a good thing if I
guess if anyone has any burning
questions for each of these topics I
have to like the three more topics here
and each of them is sort of a separate
topic so if anybody has any burning
questions they want to jump in with
happy to take those there we go
so it I think well I can speak best for
my stuff I think you know Luo and I work
on the media servers and we have less
than five sort of bug bugs we have we
have our to-do list things you know and
we have things in those sort of code
cleanup category things that we'd like
to clean up or fix but things that are
actual actual bugs that are affecting
users it's really it's small number it
is usually when we find a bug we fix it
and then and then we push it out yeah do
we use distributed Erlang not sure
exactly what you mean by the streams so
we use yes so so all these all that
architecture diagram that I showed is
those are all those or clusters so the
so the front end nose the front end
nodes the chat servers and the media
servers tend to be sort of they do their
job by themselves they just accept
connections one thread per connection
they then talk over disks or wendice to
the backend nodes and then each of those
back-end nodes is a cluster and so we
might have a cluster say of the group
the servers that keep all the group
information there are 16 of those and
and they're a cluster and they're
started probably a byte phone number
I don't remember details on all of them
but yeah but is yeah they oh they
communicate over disks when we do these
scaling up we just add more servers we
take heavy advantage of the fact that
Erlang is a built-in distributed system
that that is that's huge for us
in terms of building a scaling system
okay yeah
right okay so the question is how do we
do how do we deal with server failures
within the cluster so all the services
are and I'm going to actually in a
couple slides down the road here I have
a little more detail on this but all the
services have basically a primary and a
backup and so if the primary host is
unavailable there's there's always a
second copy on on a alternate host a
failover host and basically in that in
that sort of shared library of goodies
that we've got one of the things that's
built in there basically and all of the
internode communications is automatic
understanding of that failover process
okay let me let's do one more
right so for debugging so questions like
what are what are the debugging tools
that we use for tackling different
problems so we do have we did a pretty
extensive logging that we use throughout
the system and that's almost always our
first our first line of defense is to go
you know look in the logs and see what's
been logged as far as reproducing bugs
typically I'm just trying to think from
recent history on the server side
typically we are able to reproduce
things pretty easily the other thing
that I think this is I don't know this
is an Erlang thing or our application
thing but typically it is if especially
we can reproduce the bug it is not
usually hard to figure out what the bug
is like there is very little analysis
required to go from we have a bug we've
reproduced the bug oh yes this is this
is where the root cause of the problem
was if if we do have something that is
more difficult to analyze typically
where I think most of us do is we have
the ability to set up a cluster on our
Mac and then we can reproduce it in the
cluster on the Mac go in and do
additional log and additional tracing we
do do a lot of things live in terms of
it so the next topic I'm gonna talk
about is how we deploy things we do
deploy and test a lot of things live and
so a lot of times if it is something
that's sort of more tricky to figure out
we'll put in additional logging on like
one server life to gather more data on
how the bug is
okay one more um yes we have not had a
perfect track record we so the question
was have we ever lost messages and the
answer is yes but what we really we
really don't like it when it happens
okay so playing price so after we're
done kind of development phase writing
code then of course we get to how do we
deploy things out what do we do for
deployment how many guys have like mmm
some sort of automated deployment system
that kind of goes out if you're an agile
team you may subscribe to the you check
stuff in and then it actually goes all
the way from check-in into yeah into
production we we we have a really sort
of different way of dealing with this
we're big believers on automation like
we don't want people spending a lot of
time doing manual work that's sort of
busy work but we also are big believers
in changes to the system are sort of
controlled by humans and so when we do
our deploys we do our employees manual
but usually sort of at the touch of a
button it's usually two buttons so
there's one one to push the code and
then one to load the code and Rick calls
this friction by design that you know we
don't we don't want stuff just going out
in the middle of the night at odd hours
and and just like you know impact in the
system when we're not you know we we we
try to do deploys when we're in the
office and when you know if something's
going to go wrong we're there and we
know what's going wrong the other thing
that we do which is again kind of you
know sort of comparing with the some of
the agile philosophies is we do a lot of
deploys and we may and we do a lot of
small deploys instead of like on the
chat side
things the the guys that run the chat
servers they actually deploy usually
three times a day and it's they'll pick
up each of those will have a handful of
changes in it one of the nice things
with with Erlang is the fact that we can
do hot code load and so it makes it
really easy to do this kind of just
there's almost you know no effort
between something is checked in and
something is deployed and running and
there's no impact to users because
there's no drop connections and things
just keep running you could do in like a
server running in C++ you could do a you
try to implement something that was kind
of the equivalent a hot code loading
where you set up a second server and the
second server listens for connections
and then when the first one all its
connections are idle then you can shut
that one down and do this sort of a be
switching between servers and simulate
kind of the same thing that does that
our line does well that's that is
basically what our line does
automatically for you in language under
the covers and so we take advantage of
that pretty much every day when one
we're pushing code which is it's it's
hard to haven't lived in that world
think about going to some other world
where you can't actually do that okay
so let's talk about failures
how do things fail we have these are
things that we typically see in our in
our environment you know we sell
hardware failures RAM disks Nick's we
have I think we're we're over a thousand
servers now in our server infrastructure
and so obviously the more servers that
you get the more likelihood are the more
frequency of these failures that you're
gonna have one of the things is okay so
then we have like you have like network
network failures we work a lot on
keeping a really clean network
underneath of us because we're so
dependent especially with the
distributed system aspect of Erlang or
so dependent on the network being good
and we've got a pretty good track record
there a few years ago I think it was in
2014 we had a big outage that turned out
to be a result of some underlying
network issues where the cluster could
not connect to itself anymore over disk
and that was that was a major major fire
drill for us and a big outage so
networking is important for us but at
some level we do expect networks to just
work like networks are supposed there's
they're supposed to connect things
they're supposed to have redundancy
built into them they're supposed to
failover and we we kind of hold that as
a standard that we're not gonna try to
rebuild that another layer above that
and our app we're gonna expect that to
be there and working and then of course
the last category of failures that
happen is is software failures and those
do happen and as I mentioned earlier a
lot of times once we once we see a bug
the scope is apparent and and we're
usually able to get at the bottom of
pretty quickly yet another question
so okay so the question is how do we how
do we deal with net splits do they
happen and especially for amnesia how
does how do we deal with it with amnesia
we so we do get would you get little
blips where connectivity between hosts
will drop most most of our databases we
have tried to set up so that they can
return to consistency relatively easily
and so typically in fact relatively
recently we've actually just made a
policy that anytime there's an end user
and need to disconnect we automatically
reconnect and we have I am NOT the best
person to answer this question in more
detail because my servers are not amici
based but yeah it is it is definitely
concerned we have I mean we've pushed
really hard on we just don't want that
to happen and and for the most part
we've been successful in keeping things
pretty clean and so it doesn't happen
very often
okay so how many of you have like your
monitoring monitoring scripts paging all
these guys alright yeah yeah so okay so
we do this too we have actually we have
a sort of strangely simple simple
monitoring system we have this one
gigantic shell script it's actually not
even it's not even that gigantic it's
called Mon dot SAH runs on every server
and just runs out of cron every minute
and it just checks a bunch of sort of
system health things is is the system
running out of network capacity is it
running is it running low on memory is
it running out of CPU it also checks for
a bunch of Erlang sort of more specific
issues have are my processes running
with too much backlog have there been
mmm max restart intensity events on a
gen server or we have things that we use
internally called gen factories which
are scaled up to n servers and so
basically all of these things kind of
run every minute and Mon dies a sh and
anytime there's any time there's an
issue on this on a server it will send
out appropriate alarms there's also some
external health check scripts so mando
that I say checks everything from the
inside these external health checks
check things that you can't necessarily
see from the inside things that are like
overall service health and we have those
for for media we have them for our dub
dub dub czar website etc we have I
mentioned a couple times about that
where we're pretty adamant about one a
really clean Network underneath of us
one thing that we've invested a fair
amount is is this sort of special full
mesh monitoring system that we call
PanAm which does it just is doing
constant network monitoring between
every host and every other host and then
we have detailed records if there's ever
a path failure anywhere within that
private network we know about it and we
can go back and try to track down track
down what caused it and make sure it
up again and then the other thing that
we're using is trend based lured spaced
on you know sort of overall traffic
errors that sort of thing okay so how
many you guys have like on-call groups
something like that so these Mon dot Sh
right you get a I think Facebook has an
impressive infrastructure built up for
doing on-call groups you can you can go
in and just say I want to create a new
on-call group with all the people and
it'll do automatically do a rotation
everyone gets a week and it'll female
your manager if you're on vacation
during the week when you're supposed to
be on call you can probably guess the
next thing I'm going to say is we don't
do any of that so our the way that we
handle alerts is anytime one of those
one of those errors fires in that Monde
Ice Age script we have a broadcast group
on whatsapp called server server talk or
server alerts and we send a message to
that group and everyone in the group
gets the message and it will wake people
up in the middle of the night and we
sort of have a I guess an honor system
or you know the guy who's the
appropriate guy to fix that problem gets
up takes care of it tries to silence the
alarm within one minute so that it
doesn't keep waking people up every
minute and that's that's how we do
things this has been this scaled
actually the scaled really well when we
were ten people when I started now that
we have something like 25 people on a
team it's starting to get to the edge of
how much a group broadcast of alerts can
scale up but one of things one of the
things I like about this is that we like
we take every alarm seriously and the
fact that if an alarm is going out it's
going to a lot of people means it
realises the make sure that your
works
do things reliably and consistently and
don't don't get the alarms in the first
place and so
I think that's that's a plus side of the
cost of the arms going to so many people
come okay so how many of you love let it
crash that's like number one my favorite
thing about hurling why why is let it
crash a good thing anyone why why is
labor force you to fix the air right so
in fact it also the other half of it is
you don't have to spend a lot of time
writing error code past that you hope
never get executed and so like this is
things I've seen in other code bases is
we spent a lot of time handling every
air condition and then we really hope
that we never got those but then because
we've written all that code we have to
find a way to test the code and let it
crash just lets you kind of sidestep all
of that right we just if it's not
supposed to happen don't write the code
for it let it crash we'll do the right
thing it'll tell you in your Erlang dog
that you had a bug and go back and fix
the bug but generally we hope that we're
not spending a lot of time doing sort of
what I might say as non-productive work
yeah question
yes that's true so the point is if you
do a lot of this defensive programming
then you may not even you may be losing
information whereas if you let it crash
you get the stack trace right there you
know exactly what went wrong yeah so we
kind of take that same philosophy to our
infrastructure is if something's going
to break we let it break we get the
alarms and then we have this sort of fix
fast philosophy and so we go in and we
fix things when they happen if there are
things that are systemic we fix them
like we fixed the immediate thing and
then we also try to fix the systemic
problem but we don't build we haven't
built in a sort of deep deep redundancy
we can build everything you know triple
redundant but then we'd have 50% more
for servers so our general philosophy is
we build things to be resilient to a
single fault and we don't build for
resilience beyond that keep our server
footprint small keep errors to the
minimum and yeah okay so that's that's
pretty much the end of what I had to say
this is as best as I could summarize it
what what are the things that we do that
are interesting around reliability keep
it simple let it crash and keep your
bugs to a minimum fix them fast so yeah
we have time for a few more questions or
yeah all right we'll take maybe three
more questions if there are more yes
how so the question is how do you the
server how did the backend servers
communicate among each other um yeah
they do that's right they do share
information obviously they're not so
when I said silos I meant more that
there I was thinking more in terms of
like if you talk about object or any
design you might talk about single
responsibility principle like each of
those each of those clusters it has one
job that it does and then it delivers
that service to anyone else in the
cluster that needs it
so within within the within the system
we have this second protocol that we
call Landis which is basically it was
originally intended to be a dist
substitute to use over the LAN between
data centers but as it's actually worked
its way into our architecture what has
tended to happen is we've we are taking
the cluster the individual disk clusters
splitting those across data centers so
that all of the nodes that serve one
function are in the same disk cluster
and then we use a Landis cluster to
communicate between clusters so it's a
it's basically a second layer of disk
type communication that lets us scale up
our cluster size larger I think in the
and the Erlang talk earlier they
mentioned something about scaling disk
clusters to a hundred nodes I think our
largest disk cluster is probably
something like 700 nodes so it was like
well we're way above that already and we
but but then going forward the way
forward is then we have the disk
clusters of enrolled individual things
that are then connected together with
these Windows pipes which are also full
mesh like they work the same way so it's
if if you're if your media server
cluster is connected to your media
server database cluster it still is a
full mesh of connections between every
one of those nodes and every one of
these nodes but it's it's no longer N
squared across the whole system it's
only in the N by M to do the
connectivity
so the question is what did we take in
from Facebook and Facebook processes as
I mentioned we've been so Facebook's has
been actually an awesome partner for us
we've had the basically the full
resources and capabilities of all the
things that Facebook has done and but
then we've also had been like had the
complete flexibility to take the parts
that that we like so there's some
components that we've picked up and and
really jumped on to and have made a big
difference for us so in the media domain
we're using the Facebook CDN to deliver
media content to people get content
closer to users we're also using there's
a storage system that they have that we
have taken all of our video storage
which was filling up a whole bunch of
hard disks which I failed a lot and so
this was a great way to get a bunch of
failures out of the system and and moved
in that system there's another thing
that we did with the voice calling is we
took advantage of some Facebook
resources to help do more local server
infrastructure from voice calling
because obviously full voice calling is
very latency sensitive and so those are
sorts of things that we really tapped
into Facebook to take advantage of
resources that they have to help us make
service better okay all right well this
probably great place Tom thank you all
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>