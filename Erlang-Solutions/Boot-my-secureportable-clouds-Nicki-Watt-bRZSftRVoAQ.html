<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Boot my (secure)-(portable) clouds! - Nicki Watt | Coder Coacher - Coaching Coders</title><meta content="Boot my (secure)-(portable) clouds! - Nicki Watt - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Boot my (secure)-(portable) clouds! - Nicki Watt</b></h2><h5 class="post__date">2015-11-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bRZSftRVoAQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">right well welcome everybody not sure
who the track coast is but i'll
introduce myself so my name is nikki wat
and i'm going to be talking to you today
about the sort of tools and approaches
that are used to automate the creation
of self-contained secure environments in
different cards so just a little bit
about me really quickly I'm a hands-on
lead consultant at a company called open
credo co-author of the book near for
Jane action which is about graph
databases but I'm currently working with
the department in the UK government on
their sort of cloud automation projects
and a lot of what I'm going to be
talking to you about today is kind of
based on that I was asked by Francisco
to kind of talk on infrastructure
related stuff and I think the stalk is
probably going to be quite different to
some of the talks you've kind of been to
already it's not doing a sort of deep
dive into one kind of particular
technology but rather we're going to
look at sort of overall sort of
approaching tools that we use to solve
this problem and that's my twitter
handle if you want to tweet about this
or get hold of me later so the agenda
for the talk is that we're going to have
a look at what is the actual problem
that we're trying to solve what are the
options out there to do this kind of
stuff and we're going to look at how
we've gone about doing that and finish
off with a conclusion so first up what
is the actual problem that we're trying
to address here so the cloud market has
evolved quite a bit over time you
started off with Amazon is the major
kind of player but now you have the lacs
of google and azure and smaller players
in sort of OpenStack space as well and
many organizations have successfully
managed to embrace cloud computing you
look at the likes of Netflix and their
use of Amazon is a good example a lot of
this was initially to sort of drive down
costs get out of your data center take
advantage of commodity computing but
nowadays when a sort of big driving
factors also to really be able to sort
of innovate and gain a competitive edge
so you know people can react faster and
experiments in the cloud much quicker
than they can obviously if they have to
have their own machines also with the
elastic nature of cloud it's easier if
your business has got a sort of spiky
kind of load it's easier for you to
scale up resources and scale them
down as and when you kind of need them
so many have done this successfully but
for many sort of traditional sort of
enterprises and more traditional
organizations they found this very
difficult and as I said there's lots of
different clouds out there now and what
people find is that you have developers
doing one thing in Amazon and a set of
tools there and they're doing something
else in OpenStack and something else in
Google and for the sort of traditional
sort of IT operations it's very unclear
to them how is the stuff actually being
created in the cloud is it secure does
it address all of the kind of things
that they're interested in there's a
great sort of unease here to the extent
where the thought well maybe we'll kind
of steer clear of is cloud computing
thing but instead of throwing the baby
after the bath water many organizations
and the government is is one of these is
realized that actually if we can do this
process in a more sort of efficient
manner at least as far as they concerned
you can still reap the benefits but
still have the level of confidence that
the way these these sort of machines and
things are created addresses all the
concerns that they're interested in so
when a dev team comes along rather than
simply arbitrary spinning machines up we
want they want to make sure they have a
repeatable verifiable and maybe an
auditable way in order to create these
environments and I'm talking about cloud
computing in a broad sense here so it's
both public and private cloud so first
and foremost we want to create a secure
repeatable verifiable environments but
besides is basic premise we also want to
be able to move cloud providers and the
reasons for this can weave can vary they
can be strategic or financial they could
be sort of technical maybe one cloud
offers better latency or I ops
guarantees then then another one or you
may have work load requirements so you
may say that I'm very happy to run
certain development workloads in the
public cloud because there's no content
that's sensitive but if I'm going to run
in production I want to run somewhere
else because I need more guarantees
around that code and whatever the reason
avoiding vendor locking is becoming a
very popular request nowadays so the
summary of what we're trying to solve is
that we want to see how can we create
fast repeatable secure environments that
can run into
from clouds and we're really going to
try and solve this from two angles as
well as well want to make sure that it's
for the from the devs perspective it's
done fast and obviously not going to
held back by bureaucracy but from a
management perspective in a way that
they're going to be happy about this so
this is not a small undertaking there's
a lot to sort of consider here and
although we kind of keeping an eye on
public and private what I'm going to be
talking to you about today is how we've
done this specifically in the public
cloud sector so whether you're a team in
india whether a team in the UK if you
want to request a new environment you
should be able to get that environment
and have it spun up in a appropriate
cloud provider for you to use so for the
rest of this talk I'm going to use a
sort of concrete example this to
describe and illustrate the points that
I'm talking about so let's say that your
team won maybe you are in the talk as I
think it was about to talks before this
where Martin was talking about
concurrent programming in Java and
taking advantage of new sort of latency
stuff maybe you want to build the next
big sort of trading platform that in
itself would be one environments that
you might want to spin up in the cloud
but for the purpose of this talk let's
say that you also need a tooling
environment somewhere where you can
build the sort of micro services or
whatever it is that's going to be
involved in your next big sort of
trading platform so you might identify I
have some key functional requirements
that i need to be created in my
environment so you might say I'm going
to do continuous integration and
delivery and maybe I want to use Jenkins
for that maybe you decide well actually
I need sort of two I need a few slaves
in there as well because I'm going to be
building lots of things and maybe you
also want sort of geral something for
project management and you can't make
use of things like Travis or whatever
because you currently you need to be in
a secure environment once the sort of
fundamentals have been established as to
what your environment looks like you as
a team just want to come along and say
I'm team one I want one of these
environments I went three slaves this is
the dns entry i expect to go to on the
Internet to get to my environments and
by the way these are the ssh keys I want
to be able to log into the box with so
besides the functional components there
the environments has there are also some
sort of supporting services which are
very handy irrespective of what kind of
environment has been spun up in the
cloud so this for example is things like
DNS so you can use IPS everywhere but if
you have a portable DNS service that can
be used within your environment that's
handy irrespective whether it's a
tooling environments or some kind of
trading platform and we want to make
sure this is completely sort of secure
so it's going to be private but we will
need some kind of VPN access or VPN
service to be able to get into that so
what are the options if we want to do
this what are the options out there
already so you want to define this
template and we want to be able to spin
it up in kind of different cloud
providers as soon as somebody says the
word sort of multiple or hybrid clouds
you will inevitably kind of run into
what people call the cloud management
platforms and these are various
commercial as well as open source tools
which claimed to be a sort of single
pane of glass and they say that we can
do everything that you need to do in
multiple clouds be that provisioning be
at config management be at monitoring
they can do it all and some are better
than others and there is a place for
them but they all tend to suffer from
the same problems and that is in trying
to be all things to all people they tend
up mastering none of them very well they
also impose their approach and tooling
in workflow on you and with a cloud
market moving so fast if there's sort of
features in the cloud that you want to
take advantage of if it's not in your
CMP that is tough luck for you you can't
make use of it so instead of sort of
opting for this one-size-fits-all
container management platform solution
we've gone more in the sort of code mesh
style which says which promotes looking
at the right tool for the job so is
there a way we can find the right
combination of tools just to still
achieve what we're trying to do and come
up with a solution so this all sounds
fair enough but you can't arbitrarily
try and throw a whole bunch of different
tools together and hope you're going to
get a cohesive solution out of that at
the end you do need to think about how
you're actually going to structure this
and put this stuff together
so they think the key to this for us is
to ensure that we have a set of pen set
of principles which drive the tools that
we choose and also the way that we
choose to integrate them and then
provided we stick to our principles we
can potentially swap out tools as new
options and technologies kind of come
along but we don't sacrifice the overall
goal that we're trying to achieve so
this doesn't mean it's kind going to be
completely seamless when you try and
swap these things out but rather that
you've at least planned for it and it's
more easy to adapt so the principles
that we follow can be described by an
ASAP acronym so in order to do the stuff
we believe you need to automate
everything so the goal here is to have a
transparent process for creating fast
repeatable environments so we don't want
any humans in the loop humans are they
just slow things down manual
configuration is error prone and that
just causes lots of problems so we need
to automate everything one of our goal
is also flexible environments this is
both from the fact that we want to move
cloud providers but also if we have
different tooling that we want to swap
out we want to be able to do that if we
if we if we want as well and in order to
do this we believe you need to separate
your configuration from your code and
you need to do this so that we're
allowing you to swap and change things a
little bit more easily the other thing
is that it might sound a little bit
obvious but you need to have a cloud
provider that is API that provides an
API for you to integrate with as well as
the tools that you use there's a lot of
cloud providers out there that off they
call themself cloud but they aren't
really and you've got to go to portals
and push buttons and all kinds of things
and that really just doesn't work in
this type of scenario so both the tools
and the cloud provider must have an API
to integrate with and although we
certainly don't discount any commercial
tools out there our preference is for
modular open source tooling if if it's
available so what I've described is sort
of there's two broad areas there's sort
of the self-service functionality so if
a team comes along and they say this is
my environment and I
to be able to choose this cloud and put
this information in that's the
self-service part which we're not going
to look at we can assume all of that
done we have the inputs as to what they
want now we're going to look sort of
under the hood at what the tools and
approaches are in order to spend the
stuff up so on to the sort of actual
meat of the talk where we're going to
have a look at the challenges that we
encountered some of the sort of lessons
learned and the tools that we learn the
tools that we used in our journey along
the way we're going to cover four areas
we're going to look at infrastructure
provisioning config management and then
securing all of this stuff and then
moving it so the first challenge is
automated infrastructure provisioning so
this is about creating the various
different resources in the cloud and
what I mean by that is things like
networks your firewall rules your
security groups so all of this stuff can
be automated by a cloud provider and
given our sort of example CI scenario
maybe we decide I want two networks I
want one to have the sort of supporting
services and I want another one to have
all of my stuff in it we also want to
bring our own VPN software in this case
we want to make it sort of portable
across clouds so instead of using a
cloud provider we will bring your own
VPN software and that will be in the
supporting service and depending on
whether you land up doing this in sort
of Amazon or an OpenStack provider what
you'll find is that conceptually they
have very similar components but their
API is a fundamentally different you
cannot code to an Amazon API in the same
way that you code to an OpenStack or as
your API and the lesson here is that
there is no such thing as a single
common cloud API there are libraries out
there that try to abstract this for you
and give you a common sort of API into
the different clouds this is things like
jaleb and sorry look cloud and Jake loud
but all that that tends to do is it
lands up confining you to whatever the
lowest common denominator denominator is
that can be found across all of the
clouds we didn't really want to confine
ourselves to this if there are some
features in a cloud that you want to
take advantage of we want to be able to
do that if we so
choose so we want to really sort of deal
and embrace the fact that there may be
different api's out there but we still
want to be able to to move stuff around
so we want a tool that can look to
handle this and the tool that we've
chosen in this space is a tool called
terraform but just a show of hands has
anybody heard of terraform used it okay
that's just good cool so this is an open
source product from other guys at
Hershey Corp you probably know them from
makers of vagrants and terraform is
really there to sort of create manage
and manipulate your infrastructure
resources the way you do that is the way
you do this is that you define your
resources in one or more files these
resources define the target
infrastructure that you want to create
in the cloud then have a sort of command
line tool which operates on these these
files and it takes what you've defined
and it lands up creating those things in
the cloud itself so terraform
out-of-the-box supports all of the major
cloud providers you've got things like
sort of Google and Amazon you also have
other sort of smaller players like
digitalocean OpenStack etc and unlike
the libraries that try to make a common
API across all of the cloud providers
Tara forms architecture for the way it
interacts with different classes to have
a plug-in architecture and it expects
the plug-in to be able to fully
understand the API that it's talking to
so if you have a Google plugin it needs
to understand the google google api and
if you have an amazon plugin needs to
understand all the features that amazon
is capable of doing and the commonality
is then this is then brought about and
made available in the dsl for you to
code against if you are only going to do
this in one cloud so so now you were
only operating in amazon the equivalent
tool would be something like cloud
formation that's what you would use for
that if you were only operating in an
OpenStack world the equivalent would be
something like heat the nice thing with
terraform is that allows you to combine
things from different cloud providers in
the same definition so let's have a
little bit of a closer look at what this
dsl looks like so the dsl is customed to
how
shakur and it tries to kind of strike a
balance between being human readable and
editable as well as machine friendly but
if you don't like that there is also a
JSON format so each resource within
terraform represents an individually
controllable entity in the cloud and
this is generally identified by this
resource keyword you can see that we've
got a sort of compute resource at the
top there and then we've also got a pool
of public IPS where we want to extract a
floating IP out of the naming is kind of
important yes so the first part
represents the provider you're going to
in this case we're targeting OpenStack
and the second part of the name is the
type of resource that you're creating in
the cloud so in the top we've got our
computer resource and the bottom one is
a floating point IP and then you can
also give this resource a sort of name
or an alias and we'll see that this
becomes important later because what
terraform allows you to do is for your
resources to refer to one another and
reuse information about itself so this
is essentially the sort of AWS
equivalent to what you just saw in the
previous slide so it has slightly
different variables and it's got maybe
slightly different concepts and that's
because as I said before the api's are
fundamentally different but the nice
thing with terraformers you could put
both of these together in the same
definition if you wanted to so ten
reform is also covers more than just
your traditional computer resources it
can also have provided plugins for
things like passes so Heroku and cloud
foundry there are plugins for that there
is also plugins for software as a
service so things like DNS simple or
mail gun anything with an API in the
infrastructure that you can control you
can potentially have a plug-in for that
and this proves extremely useful when
you want to combine infrastructure from
different providers and this is exactly
the case that we have in our example
scenario here so we've said that our
teams are specified a DNS entry that
they want to say when i go here this is
this is where i expect to go in order to
be able to VPN and get into my cloud and
terraform allows us to create
an entry one for the external dns and
separate infrastructure to create it in
the in whichever cloud has been spun up
so whether it's in sort of amazon or in
an OpenStack it doesn't matter we can we
can combine the two together in one
place and although we're not kind of
handling handling it here in theory you
could have an active active scenario as
well so you may want to load balanced
across clouds so you may create some
infrastructure and Amazon some different
infrastructure and OpenStack and you can
create an entry in your DNS provider
which will allow you to load balance
between two clouds so here we just have
an example of the external dns entry in
this case we've used amazon's route 53
service but you could use dns simple or
whichever sort of provider you're
looking for so terraform also provides
the ability to handle variables so we
have a couple of we have some
information that we want passed into
this process that we can configure
exactly what it is we're creating and
terraform allows you to do this by you
know by having whatever variables you
decide you can you could refer to those
in your resources and within the dsl the
resources can refer to one another and
terraform is clever enough to understand
dependencies and when it understands
those dependencies it will ensure it
creates the physical resources in the
right order as well so the example here
is that in the middle we've said I have
for OpenStack I have out of my public
pool of IPs I want to extract a floating
point IP and I want to use that in other
places so that gets assigned to an
address a variable called address and
you can see that that has now been
referenced in the top section which is
the computer resource which says take
that value and assign that to me as my
IP address and in the same definition at
the bottom we've got our route 53 area
code which is take that same IP and
configure it in my a-list records so
that you can route me to my particular
VPN box in the cloud other kind of stuff
that it can do it can do sort of user
controlled resource scaling
so you can do amazon scaling you know
amazon sort of auto scaling but you have
an ability to do something kind of
similar with terraform where you can
kind of say in our example maybe you
want three or four of these Jenkins
slaves boxes in your environment so you
could pass an account and you could say
I want four of them and terraform would
ensure that it created four of those
resources for you if you change it you'd
have to or whatever it is that you were
looking for terraform is not just for
creating stuff though you can also use
it to maintain your infrastructure
whilst it's there so I kind of call this
the diff for infrastructure and when you
do a terraform apply so that takes your
sort of resources and your definitions
and applies it to the cloud it keeps
track of what's happened in the cloud
provider and it stores the state either
locally or in a remote database whenever
you make changes to that definition
we'll have a look at what you've changed
have a look at the cloud provider and
we'll say this is what I'm going to do
if you execute me the example up here
it's little but you can't really see
that can you it's not great but the top
level is essentially saying I'm going to
create an openvpn box in the setup it
hasn't existed before but in the bottom
one it's saying oh I see that you've
actually changed from a medium-sized
resource to a large resource and in this
case I'm going to have to completely
destroy that machine and recreate it for
you but the nice thing is that you can
run this plan and you completely
understand what it's going to do in the
target cloud provider for you before you
do it and then you can either choose to
act on it or not so the sort of summary
for these sort of infrastructure as a
service provisioning side is that we
found a tool that kind of adheres to our
a set principles we've automated
everything we've managed to separate our
sort of configuration from our codes
that we can code things differently in
different providers it's API driven and
our preference for a sort of modular
open source tooling product has been
adhered to as I said it's ki kind of
selling point is that you can combine
infrastructure from different providers
in
same definition terraform is very new
though it is still evolving so it's only
on version northpoint 66 at the moment
and it does have a few pain points in
its kind of DSL we've managed to work
around those but we did have to jump
through some hoops to kind of make it
happen it has a meta programming model
as well which is quite interesting but
we won't go into that at this point but
there are other alternatives out there
as well so the whole point of this is
that there may be other things which can
do this better and maybe you want to try
them out examples are things like Tosca
and clarify again a very very new still
quite an immature product and when we
looked at it we prefer terraform at this
particular sort of stage so we went with
that and it's worked out really well for
us so before moving on it's worth
mentioning how do we get consistent
images in different clouds so you may
have noticed in the terraform file that
when you create a computer resource one
of the things you tell it is that this
is my base starting image within the
cloud but if you're going to start in
Amazonas sent us image there is
potentially slightly different to a
centos image in some other kind of cloud
and for the purposes of what we're
trying to do we want to ensure that the
starting point is the same it's got the
same operating system the same packages
same everything and in order to do that
we use another hash acog tool called
packer and placards responsibility is
essentially to provide you with a way
where you can have a kickstart file or
whatever the particular mechanism is for
auto creating your images and it can
upload it and into the different clouds
so by doing this we've managed to have a
sort of a single definition of what we
want our start an image to look like and
we upload that into all of the clouds
that we intend to go to and we know then
that moving forward everything beyond
that can be consistently applied so
moving on to the second challenge which
is automated config management so we've
automated our infrastructure this is
great we've tied it together together at
a macro level we've got our consistent
sort of OS images
but now we want to zoom into the actual
kind of boxes themselves and how do we
get the software on there how do we tie
up some of those services that we might
be starting up at a more sort of micro
level and there's two aspects to this
challenge there's how do I do it the
first time around and then how do I do
it for sort of longer term maintenance
are we going to focus on the sort of
initial bootstrapping phase so the
lesson here is that we've made a
conscious decision to decouple some of
you may get the reference to that or not
but we've made a conscious decision to
sort of decouple the initial
bootstrapping process from the actual
conflict management tool that's used to
do the heavy lifting so what I mean by
that is at boot time this sort of
bootstrap process what we expected to do
is the very first things that have to
happen when a machine starts up but then
we expected to kind of act as a bridge
and to be able to install a particular
config tool of choice and then delegate
to it to do the rest of the heavy
lifting whatever that may involve so
there are already a lot of really good
config management tools out there you
have the likes of puppet chef and civil
salt and we're not really going to focus
on those per se there's a lot of detail
out there about that rather I'm going to
focus on something called cloud in it
and the role that it plays in this
bootstrapping process for us so this is
where we left off we have our
infrastructure it's been created and our
different sort of various providers and
the VMS have just been started up thanks
to packer we have plowed in its
installed on all of the images so we
know we have a common starting point on
boot when the image comes up clad in it
is triggered and it obtains and it runs
some automated scripts on that vm the
way that and ignore this but over here
for the moment we'll come back to that
the way it does this is oh sorry just
first of all and so glad in it is
basically designed to work across cloud
providers it is an open source package
originally from the guys at canonical
and a code is on launch pad if you're
looking for it and the way that it works
is that
it tends to work in conjunction with
something called a metadata service so
all cloud providers tend to provide this
for you so when your instance boots up
in a cloud there is this metadata
service that you can programmatically
query and say tell me about my machine
that's just started up so that will tell
you things like what your IP address is
the private IP that may have been
allocated to you but you can also inject
some custom sort of user data
information into that service and say
when my machine starts up give it this
piece of script or whatever and it
should run that in order to do whatever
you wanted to do and that's exactly what
cloud in it is doing for us it's hooking
into this metadata service and it is
getting hold of some configuration
script that we've passed in and said I
want you to use this as the initial
bootstrapping mechanism so what is this
config look like with cladding at you
generally you can pass in either a sort
of a normal script it's bash script or
something or there is something called
cloud config in a sort of y ml format
and this is an example of what a cloud
in it config would look like in yema so
you can see you can do things like set
the hostname you can get it to manage
your initial a TTC host files that you
can maybe get out of the internet and be
able to download stuff you can bootstrap
a config management tools that you can
use that in order to kind of move
forward and you can also run arbitrary
kind of commands and install software if
you want to so that's great how do we
get this thing into our actual instance
so we know that we're using terraform as
the way to do our sort of interaction
with the is layer so one of the things
terraform provides is a template
resource and you can point it at a file
on your disk which will say this is the
cloud config that I want injected once
all the variables and things have been
melted together at runtime I want you to
make that available and pass it into my
instance when it starts up so when it's
when it gets created that will be put
into the metadata service and when your
instance boots up it will take that and
start executing it
so terraform is finished user data is
injected cloud in it has been kicked in
in our setup we have one box which is
the big box which is kind of like a
designated orchestration box and we use
this to install and have some of our
supporting services things like our DNS
service and some of the stuff that we
use to bootstrap the rest of the
environments so it's clad in it process
goes something like this it does it's
basic kind of initialization setting
host names and things like that and as
part of the user data we pass in a
one-time security token this token is
used to access a secrets management
service called vault which I'll talk
about a little bit later and so
essentially what it does it will use
this token it will connect to our
secrets management service and it will
download some of the secrets that it
needs in order to continue its process
from our perspective the next thing it
does is it goes and it connects to our
git repository using the credentials
that are just downloaded from the
secrets management so the credentials to
get to get and it downloads our config
management code that we wanted to run it
then initiates a bootstrap sort of
puppet run on itself so we're using
puppets in this particular case and it
lands up installing two pieces of
software as far as we're concerned that
are important one called free IPA this
is something from red hats and this
provides us with our internal dns and
this is the portable sort of internal
dns service that we use in the different
clouds it also installs a puppet master
and that's the thing that we use to help
us bootstrap or the rest of the machines
into the config management so all the
other boxes have a slightly different
process so once this box is up we have a
DNS service we have a puppet master it
has a DNS name the other boxes are
configured to wait until it recognizes
that the DNS service is actually ready
it will then connect to the DNS service
and say register with it and say give me
give me a domains that I can then start
doing the rest of my stuff it's also
configured to wait
until the puppet master is ready and
then it will ask the puppet master to
initiate a run on the boxes themselves
and that will then go about installing
whatever software it is that you want to
convert your boxes into whatever it is
that you've decided so in our case we've
got a few kind of Jenkins slaves down
there but we also have our openvpn box
up there in our supporting services so
we use openvpn as our sort of portable
way of being able to get into different
cloud providers irrespective of the
cloud provider that you you go to and
and we have our puppets code essentially
configure that box so why puppet there's
a lot of you know there's more modern
tools out there people say well not
answer ball and salt stack and you know
all these kind of things we actually
started with ansible and it was a great
way for us to get up and running it's
really a really good config management
tool but in this case the client had
skills already in the sort of puppet
side of things and so we needed to
change from ansible to using puppets but
because of the way that we kind of set
things up it was and it wasn't a
complete sort of easy thing to swap it
out but the mechanisms that were put in
place by the principles that we had
meant that it was easier to do that so
the summary on this kind of section is
that again we've stuck to our ASAP
principles everything's automatable both
from cloud in its perspective and the
config management tool itself separated
our configuration and our code
everything's API drivable and we have
managed to prefer a sort of open source
software we also have an asynchronous
bootstrap process so what I mean by that
is there is no explicit call required
from from the outside in order to make
the config management stuff happen once
terraform starts up the config
management just happens as and when it's
ready because of cloud in it as they
boot up the services start installing
things and waiting for things to happen
so you just initiate the sort of
environment and it all kind of goes and
takes care of itself as in when things
already by comparison what
could have done is we could have used
their some functionality and terraform
where you can have a provision and you
can say when I'm finished all of my
infrastructure stuff go and call puppets
or go and call sort of chef or something
like that but we we didn't want to have
any kind of coordination logic on the
calling side we just wanted that a sort
itself out in the environment and as I
said this approach has helped us to kind
of swap ansible and pop it out so one of
the sort of lost bigger challenges that
we're going to look at is how do you go
about securing all of this stuff so
security itself is a massive topic and
we're not going to have time to
obviously cover it all we've covered
some aspects and mentioned some aspects
of this flag for example perimeter
security all of our configuration for
the firewall rules the security groups
all that kind of stuff is infrastructure
as code in our terraform config we've
taken care of that from that aspect
we're also bringing our own sort of open
VPN software so we'll ensure and
terraform that the networks and
everything are completely private and
we'll ask the cloud provider to give us
a public IPS that we can assign it to
our openvpn box but we use openvpn is a
way to ensure that only appropriate
people can log in from the user access
control side if people need to ssh onto
the box we only allow them to do that
with keys no username a password and if
we have to manage user accounts that
sort of free IPA software that I talk to
you about does identity management as
well and we are actually doubling up and
using that to do our user management's
in the environment as well but the area
which cuts across all aspects is how do
you manage your secrets so this crops up
everywhere whether you're trying to do
your infrastructure provisioning what
are the credentials that I need to
connect the cloud provider with once
I've done that now I need to install
stuff on a box I need to get to get what
are the credentials to get to get if
I've got database passwords one of the
database passwords there's all of these
secrets that somehow need to be managed
and we've got a few tools in our little
toolbox here so how do we cohesively try
and manage all of this
so the lesson here is that security is
quite hard and there are probably some
of you that are really good at this and
that's great you prolly excluded from
this but for mere muggles out there if
you can leverage tools that you already
trust people have done this before and
the advice is really don't try and roll
your own so the final sort of big tool
we're going to look at is vault again
hash Corp tool very very very new we are
just trying it out at present but it is
actually being used in production by
some big companies its sole goal is
really to sort of securely expose and
provide access to secrets through a
tightly controlled API so this API can
expose either via HTTPS or a
command-line tool that they provide it
does more than just act as a sort of
secure database storage mechanism though
it provides well a pluggable backend so
if you want to store your data on file
you can do that but if you rather want
to plug a database in and have it stored
that way you can do that you can define
policies so you can say who's allowed to
access what secrets in the in the sort
of service and who's not it can also
help you to sort of react to breaches
and compromises because you can audit
exactly who's kind of accessed wats and
and when they accessed it so in our
setup we run a sort of centralized vault
service and we configure all of our
tools to be able to talk to it now each
tool generally has its own way of doing
this for example in the sort of config
management space in puppets you have
encrypted hero chef's you have encrypted
data bags but if you were to use that
you'd have to be duplicating your
secrets in a lot of different places and
it's harder to change in the event of a
compromise and this very much helped us
when we had to swap from ansible to
puppets so if we'd stored all of our
secrets in the ansible vault when we
move to puppet we were have an
unencrypted all of that stuff they can
go and put it in puppets mechanism for
for handling that and kind of move the
stuff around but because it's stored in
a central secrets management database we
just need an API to
able to get it and integrate with the
tool so the integration efforts very
puppets already has a plug-in a ruby gem
that you can use ironically terraform
which is a hash core product does not
have native integration with voltage is
very disappointing to me and it is
apparently coming but it's it's not
actually there yet but it's very very
new and we've had to write some wrappers
around this stuff ourselves but again
because of our principle of we want all
our tools to have an API it means that
people are easily able to write plugins
to integrate with us and we can also
write stuff around it in order for us to
integrate with it so how does this stuff
work so there's a sort of one-off
process where a volt needs to be
initialized and unsealed so that the
rest of the environments can start using
it and the way I tend to describe this
is it's a little bit like the movies
when you see one of these missile
launches and you said there's three
generals and they're in a room and it
says two of you have to put your key in
at the same time and turn the key in
order to in order for the missiles to
launch and volts initialization and
unsealing process works in the same way
technically it's an algorithm called
sharma's secret sharing and an event of
the disaster you can sort of seal the
whole volton and close it down as well
so here's an example of the turning of
the first key so to speak so when volt
is initialized we've said are these are
three keys three possible keys that you
can unlock me with so in this case we've
got somebody is used the first key and
they've essentially unsealed the vault
and you can see it's kind of noted are
you one down one to go so at least one
of those other two keys need to be used
in order to unseal the the rest of the
vault and then somebody can put that key
in and it will say or I'd volts is now
completely unsealed two down zero to go
it's now available for the environments
to be able to use and get their secrets
out of so now that we've got it unsealed
we need the particular environments to
have the information that they need in
order to do what what they need to do so
in our case they need to they do need to
and populate the data in volt itself so
if they have specific database passwords
or whatever that are required they need
to put that in there and if it's going
to change they also need to keep that up
to date so here is an example of the
command line interface of what this
looks like so at the top we are creating
a new mount dedicated to team one you
can list them and it kind of show you
the default sort of mount areas as well
as a special one that you've created and
here we have an example of writing a
particular secrets the cloud provide a
password for team one into their area
and also reading that secret out so it's
unsealed we've got the data in there now
we've got to give it some how to the
tools in order to be able to use this
information as part of its process so
the way this is done is that we get a
policy or we define a policy which says
only team one can get hold of this part
of a vault and we get a token for that
and that's the token that's used and
given to the bootstrapping mechanism so
here we have an example of what a
bootstrap policy looks like it will tell
you that in this case I'm allowing a
read of everything under the sort of
team one structure but writers denied by
default and sort of using the command
line interface we can verify this and
see okay if I create a token and I say
this is the policy this bootstrap policy
and it will give me back a token I can
in authenticate with that token and all
all the sort of actions I perform after
that we'll have the policy applied to it
so here we're able to read the providers
password and the team won because that
was allowed the soon as we try to write
something as says no I'm sorry you can't
do that so the sort of security summary
here is that we really scraped the
surface this is really only focusing on
secrets management but we have again
stuck to our ASAP principles it's
automatable separated our configuration
or secrets data from the actual tools
that we've used that's made us easier
for us to move stuff around its API
driven other people have written tools
we've written tools be able to integrate
with it makes it a lot easier
and it's open source which is great as
well centralized secrets management is
very handy it makes it easier to change
because there's only one place generally
that your passwords are stored and
easier to handle any sort of compromise
and it did make moving from ansible to
puppet sort of easier for our for us and
the mantra don't try and roll your own
somebody's already done a lot of the
hard work you can just kind of hook into
it so very quickly we've got this stuff
it's in a particular card it's all
secure its or created how do we move it
so the lesson here is baby steps so the
initial approach that we're focused on
is more of a disaster recovery type
scenario where you recreate the whole
environment in a different cloud
provider so what we're not doing is
we're not trying to actively migrate
your data across cloud cloud providers
in real time we have eaten our own dog
food in this so for our own sort of
tooling environments we were running in
a cloud provider and we had massive disk
failures completely scuppered us and we
couldn't actually work so we decided
right going to go into a different cloud
provider pointed it there spun it up and
in it was 40 minutes at that stage it's
a bit lower now we had our complete
tooling environment recreated including
our restoring from backups so the
approach with this particular case is
that once the sort of environments been
recreated we did ensure that we had
backup stored outside and then after the
recreation of the environment we then
use the backups to restore our state to
the point that we needed it so the
conclusion here is that you need us then
that this is at least limited to a
recreating scenario not real-time
migrations and it's still working
progress in terms of the stuff that
we're doing the final sort of conclusion
for this talk is we've managed to kind
of bring together a whole variety of
different tools we've got terraform
which has done our infrastructure
provisioning we've got packer which has
created some consistent of OS sort of
images for us make sure that we've got a
consistent starting points we've got
clad in it and puppet which install
stuff on
boxes and ensures that it it stays that
way and we have vault as our mechanism
for storing all of our secrets and
ensuring that all of the tools can hook
into that we have managed to create a
secure secure recreate able environment
in different cloud providers and
hopefully we will have happy devs
because the process is not too long
there's no bureaucracy kind of involved
and management are also happy because
they can see exactly what's happened
everything is automated they can check
the firewall rules they can check the
security groups they can be very
confident that adheres to whatever
security principles are out there so in
summary be true to your principles but
be flexible in your tools and
potentially even your approach that you
take and this will allow you to to
essentially evolve as your as your
protocols and cloud is constantly
changing and I think if you want to take
advantage of some of the kind of newer
stuff that's out there you need to try
and make sure that your tools can adapt
and change as well so experiment with
baby steps really because if you do too
much at once it can be quite sort of
overwhelming and we are by no means done
and dusted this is an ongoing project
and there's a lot of sort of research
and stuff involved in it as well but we
wanted to give insight into you know
where we are thus far and what we've
managed to achieve so thank you very
much i think i am like probably overrun
so i don't know if there's any questions
but i don't think the next week is here
yet so i can take some questions there
are any
okay so there are actually different
approaches to this and it depends on it
depends on a few things so some people
go down the approach of installing lots
of software using Packer so instead of
just having a base operating system
there almost went an immutable kind of
image which may include the software in
our case we're not doing that we're just
having the base image and we make use of
Puppets we would make use of puppet to
install the sort of basic Apache service
and then there would be a they would get
there would probably be as part of the
puppet configuration there would be the
setting of their your files and stuff it
can sometimes be done as a post process
for the actual release cycle of the
developers though some people as well so
the example that I've done here is kind
of using puppet to install native
software on the box but what a lot of
people are looking to do is just install
something like docker and then you can
actually have a separate process which
gives more control back to the
developers which says if you uninstall
there whatever your PHP stack is you
just create a docker image and you just
got to pop that into darker and all as
well so people are moving towards that
model more than trying to create the
actual software and boxes
so we kind of ran into a similar problem
so there are not providers for all of
the infrastructure out there but it is
it there are a lot more kind of coming
on all the time and one of the clouds
that we kind of wanted to talk to as a
VMware based cloud which its proprietary
and whatever but there are lots of
VMware clouds out there and people kind
of want to do it so we've actually
landed up writing a VMware vcd provider
and it's will be in the process of being
open source soon so so in that case for
the case where we didn't actually have a
route we actually literally wrote a
provider for the actual hardware in that
case if it doesn't exist then you can't
use that from that perspective so in
terms of where who we're integrating
with it does cover it does cover our use
cases so maybe I didn't make it clear we
are not actually bootstrapping vault
itself volks is a separate external
service which is managing a completely
separate kind of manner so the
bootstrapping of we don't breach trapped
vault as a service per se we just get
the token and we give that into the
environments and then stuff happens from
there so the moment is manual basically
so you will initialize and unseal unseal
the vault
service discovery that's another whole
area so there are a few ways on on doing
kind of service discovery are talking
about machines or four machines so this
project is evolving and one of the
things that we are going to be looking
at doing is needing to kind of monitor
which machines are in which environments
and all this kind of stuff and to be
honest we will probably be looking at
console as a as a product to do that I
think I being kicked out all right thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>