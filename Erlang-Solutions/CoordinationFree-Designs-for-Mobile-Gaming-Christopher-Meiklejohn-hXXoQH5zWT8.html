<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Coordination-Free Designs for Mobile Gaming - Christopher Meiklejohn | Coder Coacher - Coaching Coders</title><meta content="Coordination-Free Designs for Mobile Gaming - Christopher Meiklejohn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Coordination-Free Designs for Mobile Gaming - Christopher Meiklejohn</b></h2><h5 class="post__date">2015-11-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hXXoQH5zWT8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I will say up front that this talk has a
tremendous amount of overlap on my
strange loop talk so if you've already
seen that go see somebody else's or you
can stay I would love for you to stay so
we can have a conversation but I don't
want you to see too much stuff that
you've already seen so yep I work in
code mesh thank you yep so we're going
to talk about coordination pre designs
for mobile gaming this is largely going
to be presented in the context of my
research which is building an eventually
consistent programming language for
distributed systems so the we're going
to look at three kind of application
designs and all of them will be kind of
written in this but that doesn't mean
that you have to use the language that
we provide the kind of goal is that I'm
doing the research that we want you to
be able to take techniques from from the
things that we've learned during our
work and kind of apply them to your use
cases so so I'll just get right into it
so so consider the case of a distributed
register so a distributed register
allows you to set and get values on the
register so here we have two replicas
replica a and replicas be so if I begin
by setting the value one on replica a
and then I asynchronously propagate this
two replicas be replica be will have
once the messages delivered the value of
one now if I concurrently set replica a
to the value to and replicas be to the
value three I run into a situation where
when these messages eventually get
delivered and I have kind of not
basically in the weakest model have no
guarantees on when they'll get delivered
I have this problem what do i do what do
i what do I take is the value to is the
value three are they just going to flip
flop and I'm going to end up
23 and now the replica isn't as much of
a replica as we'd like it to be and the
idea here is that traditionally we use
synchronization right the big scary word
to kind of get this predictability to
enforce an order and this is what makes
programming easier right this is what
makes programming distributed systems
easier is because we get to say I know
that I followed this set of rules and
I'm I can expect these outcomes and I
know when these events will eventually
be seen by everybody in the system or
when they won't be seen by everybody in
the system and this eliminates
accidental non determinism so we we
think about multiple threads that are
potentially racing to talk to some place
in memory and they're going to write
different values we use synchronization
to get this predictability we know that
these events are going to come in a
particular order or we're going to lock
and enforce that they come at the force
they're applied in a particular order
and finally you know the way we usually
enforce these things and get
synchronization is through the use of
things like locks mutexes semaphores and
monitors so synchronization seems really
good right so why do I want to build
these systems without it and and the
problem is that in we have these kind of
new use cases right so we have this
really buzz-worthy Internet of Things
computers of things and you know they
have low power limited memory and
sometimes are not online right they can
go offline for periods at a time and
same with mobile gaming we want people
to be able to play their games or their
offline so one of our research partners
is rovio entertainment and you know they
would like to have these games where I
can continue making transactions I can
do things while I'm offline I can update
my profile and then when I come online
and synchronize I know that those
updates will be applied correctly so
both of these cases kind of exacerbate
the problem right because
synchronization becomes even harder when
you want to allow things that might
violate invariance or make changes to
state when clients are operating offline
when they're disconnected especially
some of these things like you know
phones might go offline to save power
and and things like this right so the
general idea is we we want to have these
zero synchronization systems but we also
want a program which shared state so now
we kind of have a problem right how do
we do this how can we build these
systems so the goal of a lot of the work
that I've been working on with the rest
of
members of sync free who had earlier
presentations at this conference today
what we're trying to do is explore the
design space see can we minimize
coordination and systems while still
getting correct outcomes and kind of
avoid these concurrency anomalies like
the Amazon dynamo items come back in
your shopping cart anomaly right so to
review just to do a quick like review of
distributed computation when we make
this move from concurrent programming to
distributed programming we've already
had this problem of consistency right we
have cache coherency and all of these
things that we have to use to understand
how values you're changing when you have
multiple actors in a system like threads
but when we move to distributed
computing we run into this additional
problem a partial failure right so how
do I detect failures in a network how do
I know when some subset of nodes have
failed if I have a lot of computers
working together to solve this problem
and one of these nodes happens to fail
or goes away do I wait indefinitely to
determine if the node has actually
failed do I wait forever to see if that
node actually applied the update and all
of this comes back to this idea that
like historically we've always wanted
the single system image right the single
system image is kind of the dream can we
program a ton of computers operating
across the internet and just treat them
like they're single computer I write a
value i read it yeah i get the value i
wrote i just wrote right and the idea
here is that like this single system
image is a very very strong contract
that we make it's a consistency model
basically and we see consistency models
as contracts between the application
developer in the system so we would say
something like linearize ability which
is a very very strong guarantee says
that as soon as an operation completes
you know in a multi-threaded system if i
go to read the result of that operation
and i start after it's completed i'm
guaranteed to see that result so this is
a extremely strong correctness criteria
for multi-threaded programming however
you have models like eventual
consistency they're like way way on the
opposite side of the spectrum right like
we're like no we don't even care when
the updates like we know that the
updates will be there that's the only
guarantee we make we don't care about
the ordering every node could see a
different ordering it could take some
unbounded amount of time whatever like
so eventual consistency is way on the
side this is a very very weak model and
as we kind of explore this spectrum of
consistency models we have to add or
remove synchronization to kind of walk
this so it's not we can't really imagine
them on a line because some of them have
different guarantees I kind of sit
parallel on the line right or concurrent
you would say and so we we kind of have
to walk back and forth by tuning
synchronization how much we have in the
system so again this idea the
consistency model this is a contract
that says if I write these values at
this given time and I follow these rules
of the consistency model then I know
when I'm going to be able to read those
values or potentially I know that I
won't be able to read those values and
we think of this as related to kind of
programming paradigms as well right so a
programming paradigm says that if I
follow these kind of rules I can expect
these kind of outcomes and these are
these are important because this is what
allows us to be successful in building
like real systems of computers right
because computers are really hard so why
is synchronization on desirable it
sounds really great right so we're gonna
talk about the problem of time so so
handling time is really difficult so at
strange loop John Moore who's uh who's
an engineer at Comcast or architect at
comcast gave this talk about how ntp can
massively screw things up you can get
within a certain time window sometimes
the two clock skew gets so far that you
can't resynchronize we have problems
like in Cassandra where if your clock
gets skewed into the future you can
write values that you can't delete until
you wait till that time actually happens
we have all sorts of crazy things
physical time is really really hard and
we kind of see physical time manifests
itself all over computing so one case is
a mutable state so if i have a
sequential system and i'm using an
imperative programming language with the
sequential system if I read X now and
then I read X in the future it's going
to have a different value it's mutable
state if we're changing that state so
mutable state takes different values
over time over physical time in
concurrent system non determinism is the
problem so non determinism we have all
sorts of races between threads that are
trying to synchronize on some state
where if we don't use Lux if we evaluate
a program multiple times and they use
these different schedules which is a
factor of time we c diff
outcomes so non determinism is something
else that we have to use synchronization
so these things we have to apply
synchronization to solve these problems
and finally in terms of distributed
systems we kind of see we kind of see a
time as like a factor of network latency
right how do we how long do we know is
enough time to wait for some operation
to complete or digitech or to detect
some failure or anything like that right
like so network latency is kind of a
problem here because we can't in a real
system wait for an infinite length of
time right to complete something so the
general if we want to generalize time is
is unavoidable because computers talk to
the real world at an actual time
computers exist in time and space right
it's unavoidable but we want to minimize
it right it's essential but we don't
like it so if we draw an analogy to a
car driving down a highway we see that
in this example the car uses friction to
grip the road right this is a
requirement if we don't have this the
car will not make fold progress this is
how this works but inside of the car
itself in the motor we don't want to
have this notion of friction right it's
really bad it's going to wear the system
down it's going to cause the system to
get slower it generally causes all of
the problems with the motor right so can
we can we acknowledge that we cannot
completely eliminate the notion of time
but can we kind of push it out right
push it out to the boundaries where the
users actually interact with the
computation so if we imagine this black
box as a data flow graph of like some
computation maybe it takes two values
and combines them and then spit
something out at the end what we want is
to have time at these interface points
right where we input a value into the
system we want time and when we read it
out we on time because that's essential
right you have to read a value at a time
and what we want is inside of this box
we want to have no notion of time we
want to have like a lot maybe we use a
logical time maybe we virtualize time
somehow but we want to have this inside
of the box not use time physical time so
Peter alpro when I gave him this
practice talk a long time ago he's like
well why do why do I even care like why
it's a black box put time in the box I
don't care and ultimately the answer is
because all of us in this room are the
programmers who write the box right and
that that's kind of unfortunate because
that's the hardest component of the
system
basically build and get correct right so
this is kind of this is where we're kind
of leading this is what we're trying to
do so can we build a system without
synchronization sure let's build a bunch
of computers that never communicate with
the one another that's fine that's a
system without synchronization zero
synchronization but it actually doesn't
get us anywhere we actually can't solve
any like any complex problems with this
right so we can't really get anywhere
without synchronization but maybe we can
use a weak form of synchronization to
get the guarantees that we do need and
so this is a so if you saw mark
Shapiro's talk earlier today or the
tutorial yesterday we have this idea of
strong eventual consistency in
distributed systems this is an
interesting word to parse you kind of
want to parse eventual consistency first
to say that we make no guarantees on
ordering but then you want to look at
this strong words epic because this is a
strong convergence property so it says
that even though the order might not
even though we'll deliver events in any
order we could duplicate things things
could get reordered REE batched whatever
we have a guarantee that anything that
observes all of these values gets at
least all of the messages in the system
will have the same state we guarantee
that it will have the exact same state
across replicas and this is really nice
when we want to distribute computations
to make sure that 24 fault tolerance and
high availability and we because we need
to ensure that there's no non
determinism all of the copies of that
computation will have the exact same
value they'll compute the same result
and strong eventual consistency only has
really one requirement which is eventual
replicate a replica communication and
again it's order insensitive and stupid
get insensitive so these are really nice
properties to have so if we go back to
our example before with the two replicas
and I set it to one and then I
concurrently modified it two and three
if I use something like max which is
monotone function over the naturals I
say well I could rearrange this anyway I
want I can move that arrow way down
there and I can move the two before i
can rearrange any of these distribution
messages and I'm guaranteed that I will
get the same result because max is
deterministic over some subset of inputs
and it's monotone so how can we kind of
succeed with strong eventual consistency
so we say we acknowledge that strong
eventual consistency might be a nice
model to program with how can we
actually make it amenable to the user
experience as a
you know as somebody who wants to
program in this language so we're going
to break this into three components the
first is that we want to build with a
foundation that eliminates accidental
non determinism we want to retain the
properties that we like about functional
programming we believe that function
programming is the right paradigm here
and this is these these ideas of
confluence and referential transparency
and finally we need to have an efficient
way to distribute this across the
network so we can actually build things
so there have been a lot of talks on to
your at EDS today so I'm not going to
really talk about it a lot but the idea
here is that crd T's are distributed
data structures they come in a variety
of different flavors sets counters
registers flags maps and graphs and what
these are our objects that observe the
strong eventual consistency property per
object only for that object not over
composition so we'll kind of look at a
crdt set to see how this works so we're
gonna have three replicas we're going to
model the state as each replica as a
triple sews triple is going to represent
the element that has been added to the
set a set of unique addition so these
are unique constants representing
additions and then the empty set there
is a set of constants that have been
removed so here a is going to add one
it's going to generate some constant we
use a just to make it easier to look at
the graph so it's going to generate this
constant but this has to be unique and
some cases globally unique replica see
will then concurrently add one to the
set and it generates its own unique
identifier see so now as updates have
not been delivered to see yet so if c
wants to apply a remove operation it
will remove its observed additions so
we'll say i observe see the unique
identifier see added by replica see and
so now I Union that into the remove set
and then when I deliver the messages i
see that i have convergence we've all
converged this is deterministic and we
see that one is still in the set because
nobody observed a sedition and then
removed it see only observed its own
addition and removed it so then we
synced eyes we share it and we have nice
little merge functions there's a lot of
math behind the behind this that you can
look at if you're interested but there's
all this math that ensures that these
things converge correctly so there's a
really nice basis to build on top of
because it doesn't have any of that
accident on determine
is one nice property the other nice
property here is that we model mutt we
model state as increasing so we see that
the state is always increasing when i
remove an element i add something to the
object to trap that removal which means
that i can read all that vary over time
things that are not monotonic like a set
cardinality which is going up and down
based on the elements and that are
contained in the set I can model this in
a way where it's totally monotonic and I
can reason about whether some object has
seen some update or not so now that I
have these objects what I'd like to do
is be able to compose them build
programs with them and I'd like those
programs to have that same convergence
property that same property that
eliminates accidental non determinism so
we've built a dataflow variant here
called lattice processing and in this is
a distributed deterministic dataflow
programming model that resembles a
functional programming language and the
primary data abstraction of this
language is the crdt there are no other
variables there are only crd T's in this
language and the goal is not to have our
own language the goal is to kind of
think about our language as being
something embedded in another language
where I need some concurrency or I need
to do something distributed in a safe
way and I don't care about things being
offline you take parts of your
application and you hoist it into this
hosted version of last that basically
does this execution in this nice
distributed way and then when you pull
values out of that last thing that's
when you introduce non determinism
because you're reading a value so if I
put something into the box and remove it
and then I add something based on that
decision I made outside of the box we
can't make guarantees about what happens
outside so this again just to repeat
this enables functional composition of
CR dt's that preserves the strong
eventual consistency property through
this composition so we work in Erlang
but I know that syntax is sometimes hard
for people to follow so I made this this
nice pseudo last language for four
slides so in this case I'm going to and
we don't have a type system in Erlang so
all of this type stuff has to be
explicit which is amazing but so in this
case we declare a set called s1 we're
going to add to s1 elements one two and
three we're going to clear a second set
which is kind of the output set and then
we're going to apply a map
this is going to double the elements so
s2 will contain 24 n 6 so this is a
pretty trivial case but it gets more
complex as you do other operations so
again we have functional and set
theoretic operations so product
intersection Union filter map fold
things like that and it's going to
perform metadata computation is going to
take when I said those triples before
that model all of this state that's not
the user that's not the value that the
user programs with that happens
transparently so when somebody calls
that map operation they don't have to
know I'm getting a triple because i use
the o.r set crdt and i need to map these
other things through no they operate on
the value alone so it resembles a normal
programming language we do this through
realizing replicas of CR DTS as kind of
streams I'm going to show you how this
composition works briefly just for
demonstration purposes and then we have
processes that consume these input
streams and write these output streams
so for the case of a Cartesian product I
would read to input streams and as
either input is changing I'm changing
the results of the Cartesian product
which is an output stream and we have
this notion of these inflationary reads
which are safe reads these reads per say
that under failure conditions where I
may go talk to some out-of-date node
because the system has a partition in it
a network partition I have a read
operation that guarantees i just read
only into the future it's like a safe
read to session guarantee if you have a
database so the way these streams work
we're going to continue building on that
original example I had in the beginning
so we're going to assume a single
replica a and two clients that are
interacting with this so in this case
client one adds a adds one to the set
with unique identifier a just like we
had before and then maybe client to adds
that unique identifier see and what we
see here is that as we looked at the
wrong screen as we advance this we see
that even see is only operates e2 is
only operating over partial state and c1
is only operating over partial state but
when they communicate with replica a
we're always merging the states together
and this says that replica AC is only a
monotonic advancement of state so c1 can
actually operate with only a partial
view of the system if it only needs to
operate on that partial view of the
system replica a serves as a
serialization point so when we go to
to apply this function so replica a is
going to represent the original value
that one two three set and function ra
is going to be the function applied to
it so that map operation we had before
what we do is process one will do a read
operation and say I don't want to read
earlier than this last state I saw which
was the empty set when replica a gets
updated it notifies process one process
one applies the function and then we get
to so we see that our map function
double the element so one becomes two
and it transparently moves that metadata
across and then it will repeat this
process a number of times and what's
really nice about this is if we think
about the ideas of referential
transparency which said I can substitute
a value from it the function returning
that value and confluence which says
that I can have any evaluation order and
basically get the same result that means
that I can eliminate entire steps in
here so this could be with a client's
offline and what I get the guarantee i
get is that when the client comes back
on it just resumes computations correct
so we get to take arbitrary evaluations
here and know that since all of this
states increasing and we can reason
about the state changes we can eliminate
entire kind of evaluations entire
evaluations of this process at being
executed so finally we'll briefly
mention the runtime way there could be a
whole talk on the run time but there's
so we have a runtime system that we've
implemented in Erlang which is based on
a broadcast protocol and epidemic
broadcast protocols can scale to a large
number of clients but provide and are
highly resilient but provide very weak
ordering guarantees because it's kind of
peer-to-peer dissemination some
protocols use an optimized tree base
like bimodal multicast or plum tree use
a tree based approach but which gives
you the appearance of ordering until
there's a problem in the tree and you
have to repair the tree and then the
ordering sacrifice so the general idea
is we think of these protocols is being
unordered so they're only suitable in
certain types of applications but this
is fantastic because even though the
epidemic broadcast protocol has
extremely weak ordering guarantees we
have built a language that has no need
for ordering so these are really good we
can match these together so this allows
us to build a language runtime that
clients communicate with one another to
share state
and participate in a computation
together and finally we can optimize
this runtime by having nodes just not
receive messages so we pre we can't we
have a mechanism for sharing version
vectors and these kind of causality
mechanisms ahead of time to notify that
state is changing and then users can the
runtimes users runtimes instances can
opt in and say well I'm interested in
this value now that I've seen it past
the threshold of three so this is a
really nice way of optimizing efficiency
this sacrifice is some notion of
commutativity when you have notes coming
and joining but that's a that's a
technical detail okay so so I've shown
you kind of like these building blocks
right we want to kind of start with
these data structures that eliminate non
determinism because it's a good building
block it's kind of like the idea of you
know we're trying to compose these
lattices because everything is kind of
formalized in lattices and we kind of
think of it as related to Lisp in that
list basically the primary abstraction
is the list and everything is kind of
built in terms of the list and it has
this nice property with the language is
very self referential so we want to kind
of build on top of that and so we you
know there are competing approaches
we've seen that there's some work being
done by the UC Berkeley folk on doing a
declarative version with lattices we're
taking the functional programming
approach so there's some debate and you
could debate that in the hallway track
as much as you want whether what is the
better paradigm but and finally we want
to have a ficient runtime system so that
we can do peer to peer state
transformations and build these come
from build these nice compositions of
applications across clients so I have
talked to you a lot about this language
now I guess I need to convince you at
least a little bit that we're moving
towards actually building applications
in it because if you can't build
anything in it why does it matter right
so the first application so all of these
applications are actually motivated by
the use cases that came to us from Rovio
through our research group so the first
we're going to talk about is an
eventually consistent leaderboard we're
going to kind of build on top of this
one briefly so we think about an
eventually consistent leaderboard we all
assume that we have some mobile game you
play this game and it tracks your name
and a top scorer and so each client will
have a local leader board which will be
the top K and clients will go offline
and they'll continue to modify that
while they're offline and then
eventually come online you have to
synchronize it so this actually is
extremely trivial to implement right we
look at this kind of graphing we say ok
there's going to be some number of
clients they can exchange the
leaderboards with one another and we'll
just encompass all the logics in the
leaderboard right well build a data type
we can build a top k crdt and this top k
crdt will just encompass the logic of
saying well all i have to do is knock
out the bottom entries right like it's
really straightforward to build a
leaderboard so again what's nice is that
if we build this with TRD T's we we get
this determinism that says that while we
can do these we can formalize this merge
operation and we can just peer to peer
disseminate the leaderboard and every
time I get the leaderboard state coming
in I merge it with mine and then I
transmit it and then eventually once
people stop playing their games and all
the messages get delivered in the system
and everything's quiesce everybody has
the right value right so this is a
eventually consistent leaderboard where
we're moving towards the right value and
the reason and the reason that the
distribution model looks so simple it's
just a bunch of nodes that are
exchanging a set that's bounded in size
based on the based on the score the
reason is because we've moved all of
this complexity from the distribution
into this data type we've just put it
all in this data type right and that's
fine because you know for this use case
it's trivial so what would some code
look like so we actually have this
implemented but and I'm giving it the
pseudocode version but you know we
create a top Casey already T and this
argument here is a you know an
approximation of some dependent typing
in Erlang but and this is it's a top K
that has two entries and then all I do
is when I update when I when a game
completes and I get the score all I do
is just update this I set the value I
say here's the name of the score and
anybody who's lower than the top two
just gets bumped out we just removed
that entry from the list but but I mean
what if we want to change the behavior
of this of this leaderboard and we want
to do it without having to build a new
data type right it's kind of annoying to
say well I want to have this thing be
eventually consistent let me design a
data type right a merge function and
formalize that is lattice and write all
the operations put my whole state into
one object and send that one object
around that's kind of not what we want
to have it's extremely error-prone
to actually build these data structures
correctly as my former colleague would
tell you here in the front row so let's
imagine that we want to extend this into
a per-user leaderboard so we want to
enhance the existing design so that we
only have the top score for each user at
each device so we want a top K
leaderboard but we want to collapse
every all of the entries into one entry
for each person's name and that name
gets disseminated we want to do this
without having to transmit all of the
scores so we want to have an efficient
way where we can transmit this state and
finally what we'd like to do is we'd
like to express this in terms of
composition we don't want to have to
build a new data type we'd like to build
with the existing data types that we
have so we see here that are our example
application has gotten the clients have
gotten a little bit bigger they've got a
little bit more complex but we see the
distribution model is roughly the same
here we still see this peer-to-peer
communication between the nodes so if we
zoom in and look at one of these the way
we do this is that we maintain a list of
scores and this color coding here means
that the shaded objects are objects that
the user interacts with as a programmer
and the rest are transformations that
happen through composition operations so
the user modifies this score and then
what we do is we fold that list of
scores into a local top K so that local
top K will be the you know we'll take
the topmost score for every user and
we'll we'll fold that list together and
then we compute another fold and this is
a special type of fold here that
computes a global top K and as global
top K will be the same thing right we
have to perform that fold operation
again to fold in the remote results into
a into a single global view of this
right and we want the local ones to
slowly contribute to the global view and
finally once we have the global view we
just want to do that same peer-to-peer
style dissemination just have everybody
merge with each other through this
gossip protocol and kind of combine this
state so it's interesting here is what
we've actually come across as something
that's existed a really long time ago
right and Orwell's more used a long time
ago is that we've kind of brought back
the notion of dynamic scoping at a
programming languages right and so what
these are or these this local top K is
really a dynamically
variable that's executing at a
particular node and then what this fold
operation here is actually just a
distributed reduce it's a reduced
operation that is folding that local
variable that's scoped dynamically
scoped per node into an aggregate and
that's a dynamic scope and the way that
would work is that every node or some
subset of the nodes would issue this
fold they would begin the fold with
their their local top-k they would send
the accumulator out so they would
disseminate the accumulator through the
gossip protocol and then pairwise share
that accumulator until the network
computer fix point so this is kind of a
distributed kind of fold operation that
eliminates kind of the you know well
you'll see the code in a minute but so
if we look at our pseudocode again we
declare a global variable called G which
is a top K of ten entries we declare a
local top K which is a dynamically
scoped aapke called l so i'll have a
different value at every node like it
will be completely different / no i mean
there'll be some overlap potentially but
it'll be completely different per node
where the global one will be the same
variable but each no might have a
different view of that variable that's
an important distinction and then we
create a set of scores so this is just
another dynamic variable every device
has a different set of scores we compute
a fold operation so we fold the set of
scores into the local dynamically scoped
local top-k using a max by name function
this is you know that I didn't include
the fold function there because I figure
that's more sicp and less distributed
systems but and then and then full
dynamic here is the dynamically scoped
fold that is going to combine all of the
elves in the system to a G that's
replicated on all of the nodes in the
system and this will compute this fixed
point so this is really nice because
this allows us so for one thing that I
think is really nice about this is that
this is kind of the way I'd like to
write MapReduce not with a bunch of
Hadoop machinery that you know is really
it has a lot of accidental non
determinism and how you write programs
but what's nice about this is that
rather than having to build these data
structures and put all the state in this
data structure and send it around I get
to perform functional composition I get
to write things like I'm a functional
programmer and you know this is enough
other than setting up the network and
the whole
arnis to get all the nodes running this
is not a lot of code to make to do what
what you know we showed on the on the
diagram before so let's look at a larger
application now we'll look at the ad
counter which is our kind of canonical
example because it exploits a lot of the
system and what's notable about this
example as it has visible non Montana
city in the system so it has things that
appear to shrink but we model it in a
way that they're growing so we're going
to assume and eventually consistent
advertisement counter so we're going to
have a mobile game the game is going to
have in game ads and we want to pay
these ads based on a number of
impressions minimum number of
impressions or charge for them rather
and this is like kind of a pessimistic
view of the number of impressions in the
system we want the ability to for
clients to be able to make progress with
offline so we keep counting ads and we
keep showing ads and then eventually
when these clients come online they'll
synchronize back with the server and
we'll kind of update the global view of
that count so this is a example kind of
data flow graph will zoom into it to
make it more legible so we're going to
start by assuming that we have these add
counters and we're Union them into a set
and will then just create a bunch of
them right so this is just taking these
egg counters and joining them into kind
of a set with Union operations we're
going to have this kind of for the
purposes of demonstration this notion of
a contract so a contract will just say
you know it's a record that says an ad
is able to be displayed so what you do
in a normal like post grassy world is
that you would want to do like an inner
join so will represent that in the data
flow graph is a Cartesian product with a
filter even though that would be the
most inefficient way of executing that
that computation and then we send all of
these counters in this set down to the
clients we give the client some copy of
these counters now the client can update
these counters while the system is
offline or wallet online as its
displaying ads and then it will
periodically send the counter back to a
central DC where the value gets merged
in when that value gets merging we have
a trigger condition that basically says
remove this from the set of available
ads as soon as that impression reaches
50,000 so we might not here for the ad
for a while it could be have 40 9998 we
might not here for a while so we might
not disable it until it's greater so
this is kind of a pessimist
you know trigger here that will be based
on how often we synchronize and then
finally so these read triggers kind of
form a barrier synchronization point in
this graph and then we propagate once we
make through one of these triggers when
we remove the ad we propagate the state
back down to the clients so what's nice
about this when we disable things and we
grow and when we add ads we remove them
when remove contracts from move ads add
counters all this is model monotonically
so so we don't have to worry about a
value going back in time each node in
that data flow graph can be arbitrarily
distributed so every node in that graph
Quran different machine or could all run
on two machines doesn't matter because
the data structures guarantee that the
distribution makes the problems of the
distribution go away so we can run it
however we'd like to and finally this
idea of divergence so let's imagine that
we had a global kind of third party
observer who could look at the state of
all the mobile clients that observer
would know that canonical value of every
counter right every node in the system
is going to have some visibility off of
that they're going to diverge from that
number a little bit right so because of
this we can say that divergence is a
factor of the synchronization interval
and this is not something that you have
to establish at the beginning of a
program this can be something that is
runtime configurable we you know you
could just build the system to be safe
when there's no synchronization and if
you need things to be more accurate you
just kind of Jack that value up and say
I want to synchronize more here I want
to synchronize more year right so this
is runtime configurable and divergence
is a factor of that configuration so the
ad counter just to show you i mean you
know there's nothing special in this
code i'm showing you except like that
it's declarative and we're expressing
just kind of how the transformations of
data should be done we're not talking
about distribution so again these
declare operations here are are required
because we have to type things so we
have these do we have these declares
that just say that we're creating these
sets but then you have simple things
like this like Union you know that's a
straightforward thing product is a
pretty simple thing to understand as
well and then filter is pretty simple as
well so these are these are some bits of
earling that I left in by accident so my
apologies but you know but it's pretty
simple it's pretty declared so so if we
take that whole mass
graph I showed you and we collapsed that
whole entire thing and we put it in that
little box right there called server
computation the interesting kind of
thing that you can get from this graph
and the details of this graph don't
matter but the interesting observation
is that we still have this peer-to-peer
synchronization here between all of the
nodes some things have counters some
other things don't have counters and
they all have this like ads list and the
important observation here is that even
though we have all of this the only
difference between the server and the
client there's no difference other than
we've nominated the server to be special
right the idea is that the the variables
are transmitted the same way we
everything regardless of its a client or
server they all use the same runtime
system the state has managed exactly the
same way so we want to think that
servers are appears to clients there's
nothing special about the runtime
there's no distinction there's no
difference really servers are basically
peers to clients and any node in the
system can disable an ad I just said in
this kind of data flow graph I gave you
I just said well the server is going to
do it just to make the example you know
more realistic but but that doesn't
happen if a client had a counter that
was after 2000 it could disable that as
well it doesn't matter so what we think
is that servers are basically trusted
nodes and these servers in the system
that are trusted notes they just run a
little bit of additional application
code and they're there to preserve this
what I'm calling as of yesterday when I
with the slides exactly ones like kind
of side effects right like billing
customers is something that I can't have
every note in the system do I don't want
to express it as part of the last
computation it would be rather
unfortunate everybody be getting
different bills at different prices all
the time and it just be mayhem but so
the important point here that I want to
kind of drive home that I think I've
been driving home for most of the talk
here is that what we've done is we've
built up from zero synchronization we've
shown that you can build applications
and find the places where you need to
synchronize and add it at that place
this is a much more this is a much
better approach to building systems then
attempting to remove synchronization you
don't want to start a system off with
you know a zookeeper instance that you
store all of you stayed in and then say
holy god this doesn't scale what should
I do let me
get out of zookeeper and I'd start
pulling components out of zoo keeper
right that is not the way you want to
build a system it may be faster
initially but it leads to a less
scalable solution so what have we
learned so we've learned that cigarette
ization is expensive locking and other
synchronization mechanisms limit the
scalability obviously to the critical
section you've probably heard that in
other talks today we want to kind of
build up not kind of pick apart although
picking apart is a definitely a viable
approach for systems that are in
production today as we've seen sometimes
secret ization is impossible I I said
this in my strange loop talk and I was
trying to explain this to this this
colleague of mine paul baril who's
really into time and see our duties and
and it was studied physics and I said
well I you know my joke that I had in
the initial version of this talk said
well you can't call it like I you know
how do I get everybody on line to
computer paxos around with them do I
called him and he's like well now you're
just doing consensus over the phone
instead of the computer and I'm like
that's a suit observation to make so so
these are cases where we can't do
synchronization all the time right we
want to have a weaker model that allows
us to continue to make progress and
finally there are some things that you
can't do that synchronization right we
can't these are problems that we won't
be able to express efficiently with CR
DTS and these are like globe some forms
of global and variants that have to be
preserved things like atomic visibility
things like this these are problems
where time and synchronization are
inherent to the actual problem so
they're required they're essential so
these are things that like you're not
going to build like you know a crdt base
zookeeper unless you don't care about
recency write effectively we have built
a co TT Basu keeper with no recency but
so how do you learn more we have some
papers you can check them out and this
is a work in progress it's a we've got a
bunch of code on github there's papers
that talk about the runtime system and
the language and how we transform the
crdt so I welcome to you to check all
that out finally thanks to the EU for
finding a lot of this and especially
funding my PhD as well so that's that's
pretty dope I'm into that and thank you
very much
no questions he has one as well so what
ya will do both yeah so that's a really
good point Russell is a really good
person and talk about this because he
actually implemented a lot of the data
structures that we use in our
implementation but there are ways to
compact leave represent things that have
been like long in the causal past
basically so so in that in this set
example there's a optimized
representation that stores a vector as
long as well as the elements that exist
in that vector so you basically pay like
you know it's 0 n on the number of
elements with some padding for storing
the metadata information for the vector
so yeah there are more compact
representations for doing a lot of this
we use the simpler ones because the
paper the first paper is written in
terms of that we have an addendum that
extends it that is unpublished at the
moment and then it's also easier to
explain the ones that don't minimize
state because you have to reason about
this compaction it's a hard problem so
yeah so the first version of last used a
consistent hashing ring with quorum
intersections for all of the requests
what we've done is we've abstracted over
that so the gossip protocol is used to
communicate with all the clients we
don't implement replication through the
gossip protocol we assume that single
nodes are backed by replicated state
machines so every note and the gossip
graph represents a unique client in the
system and you read 100
it's yeah it's a it's a programming
model implemented as well as a library
yes it is not it is not a language at
this point yes it's absolutely correct
the goal is to probably the goal was
originally to try to compile the
language to beam so that you could like
run it on the Erlang vm but then I don't
know the move recent thinking has been
it's probably serves as a better thing
that you could embed into a language and
then kind of hoist particular
computation so we have this notion that
you can like lift stuff into this last
bang and then extra kind of unlisted
effectively query it out but so yeah
that's kind of the most recent thinking
yeah so it's an unfortunate naming
because we call it a programming model
and then one of the papers we actually
put name into the title and then were
like god man I wish we didn't do that so
yeah I just about inflation we read
probably overly implementation-specific
but just passing around a version back
to them saying you don't give me
anything yes the way the implementation
so the implementation uses blocking
processes because that's efficient for
the implementation that just wait so
they get messages and they just ignore
them and they use a blocking version
vector effectively but in the formalism
the way it's represented as is like
lattice positions with a closure so so
like nodes carry around closures with
like associated lattice positions and
when those positions are triggered they
invoke the closures and then potentially
reregister the closure if they want to
do like kind of stepping through state
but yeah effectively yes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>