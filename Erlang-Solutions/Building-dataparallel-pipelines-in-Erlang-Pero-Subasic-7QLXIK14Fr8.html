<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building data-parallel pipelines in Erlang: Pero Subasic | Coder Coacher - Coaching Coders</title><meta content="Building data-parallel pipelines in Erlang: Pero Subasic - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building data-parallel pipelines in Erlang: Pero Subasic</b></h2><h5 class="post__date">2012-06-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7QLXIK14Fr8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">before we start I'd like to thank al
Alex ignoras and Curtis Brown for
allowing me to come here and give this
talk I actually work for another company
now and I'd like to say thanks to my
current company DoCoMo innovations in
palo alto for letting me come here and
talk about the work that i did i've done
with AOL the title of this work is
building data parallel pipelines in
Erlang and it sort of connects to the
previous stalker i've i've had last year
in our long factor in London it's a
little bit level up in in the sense that
I get to talk about the things I didn't
get to talk about last year and maybe
run some more experiments more in-depth
analysis and go beyond MapReduce to
iterative algorithms and i'd like to
share this architecture and these
results with with everyone here and
probably i possibly get some feedback
from you guys so feel free to stop me
anytime with questions here's the
outline of the talk first I'll make an
introduction to data parallel chances
are if you're here you already know what
data parallel is but there's been
recently a real trend almost the craze
in data parlor computing across many
levels of granularity and I'd like to
emphasize that trend and to see how sort
of put it in the framework where we can
explore our lang and how our line can
contribute to this trend then I'll talk
about some requirements very simple set
of requirements for data analytics for
building parallel flows I will recap on
MapReduce flows I'm coming from hadoop
world where everyone is running
MapReduce I'd like to sort of connect
MapReduce framework because many people
know about it and sort of starting from
my produced make sure that we can do it
in our line of course we can but also we
can go beyond that and run iterative or
incremental
rhythms then we'll have an architecture
overview and flow specification language
and I'll talk about the practical
application of practical instance of an
iterative flow that's a constant rain
flow where a build system that analyzes
Wikipedia graph wikipedia a link to
concepts and assigns a unique rank or
score to each concept in the integral
it's like a page like a google pagerank
algorithm following a show some results
it shows some ganglia monitoring results
as well and talk about the conclusion
and future here very simple requirements
we want to support MapReduce
computational model but we also want to
support iterative models and incremental
models and why I'm saying this is that
of course there are many other
computational models but if you have a
dupe and you run MapReduce you can still
run it iterative algorithms and you can
run graph analysis algorithms but
chances are they're going to be very
slow that's because you have shuffle and
sort phase that spills over to the disk
you have in the reduced phase you right
to the disk then you will have to start
another job for another iteration and if
you have ten iterations with a lot of
lots of data will take a long time to do
that processing models the first
actually the first and the most
important wall in this framework is in
stream stateless maybe there is some
small state that we can keep in memory
but I'm thinking only or primarily about
models that hold most of the data in
memory so if we have a state it should
have a fairly small footprint in stream
stateful I already mentioned that but of
course if you can do these two it will
be fairly simple to do badge and apart
from in memory models we also support
persistent models with with this
resistance as a computation platform
this is very well suited for the cloud
we have the talk related to a cloud and
I'll talk a little bit about
optimization within a flow but the same
the same couple of slides can apply to
cloud and any kind of virtualized
resource you may have a private cloud of
being aware virtualized cloud where you
allocate instances on demand based on a
load on your cluster and this applies
equally well to both and of course it
applies to bare metal I've done all
experiments on bare metal cluster so
let's talk first about the data parallel
trend you can really see today
everywhere and I sort of plucked out a
couple of instead of important instances
that I found out out there and one of
them is for example our language for
scientific computing it's it's open
source r is going parallel so you have a
lot of our packages that enable it to
run are in a parallel environment beat
on multi-core machine or on the cluster
nodes or Hadoop their companies actually
working with our own Hadoop today
mathematica has a cluster version which
is called grid matematika matlab has
parallel to toolbox it runs on a cluster
of nodes so everyone definitely realizes
that there is a big advantage that
people want to analyze data faster and
want to move their way grid Mathematica
actually supports also GPGPU computing
so if you have nvidia GPU card on your
on your instance you can you can make
use of it I actually run it in this
laptop on on the internet for big data
analysis yeah yahoo has s4 framework for
stream analytics google has sort of okay
let's say this is more on at the
language level there it's a very nice
paper about flume Java where you can
write a spec spec out bernal flows in a
language that is vaguely similar to java
it works on parallel structures data
structures and for example Cloudera but
also the whole hadoop community is
Hadoop is coming up with mr to that the
next version of MapReduce it will
support also other ways of doing part of
our computing like MPI for example and
crunch is kind of like long java
open-source version from java home and
and it started inside while there i
already mentioned gpgpu computing our
multi cpu multi-core models and clusters
so when we talk about policies and we
will talk about all these different
levels so what is really data parallel
let's say you know if we have a vector
and we want to apply function f or the f
of X x-2 to that vector in iterative
process you will write the for loop or
something go and apply the function to
all elements in turn probably run on a
single core but let's imagine for a
moment that we have this magic operation
with two bars that says essentially
spread this and if you have a cluster
you have a bunch of CPUs if you have a
multi-core you send these computations
to different cores and compute this
function now many people talk about data
parallelism and task parallelism and you
know in fact sometimes data policy
requires task parallelism because you
have to spawn some processes dispatch
them to to another node or
push them to a core or CPU to do
something for you and very rarely this
is actually implemented in hardware and
it's very expensive so we will talk more
about sort of task ilysm induced by data
parallelism here here's a small short
recap from last year we showed how to
build MapReduce flows or MapReduce
chains of flows using your lung on a
small cluster of 10 nodes and here is
show a typical flow used in advertising
to build user profiles and then to
compute some frequencies of accesses to
certain properties in your advertising
network and these numbers show how many
workers I have in each layer here so I
have a bunch of events coming into the
system a scanner scans those events
builds builds user profile profiles and
aggregates them in a net stable here in
a series of 120 at Staples then another
bunch of scanners start scanning this
user profiles and compute some campaign
frequency metrics and then aggregates
that it puts it in another at stable and
then another scanner aggregates that
across all across only two nodes by the
way this first part runs on all 10 nodes
and the second part runs on only two
nodes we want it just to have result on
two nodes so that consumers can connect
and get the data they need here's
another example flow maybe it's clear
it's a little bit different and it's
more appropriate for the architecture as
we have today in general you have a
bunch of event sources that are outside
your system it can be a source feed
coming from your advertising server it
can be a click stream coming from your
web pages
whatever you want and these sources all
feed into your interior infrastructure
and your infrastructure you have first
the input layer where say you staged
that in ads or debts tables in our land
and then you have a bunch of scanner
processes in neural on that scan through
these tables in parallel across multiple
nodes and push them to some processing
layer where something is done with those
events it could be filtering it could be
some computation some enhancement of
your events and and then we can have an
aggregation layer where we typically
aggregate using a single key or a group
of keys and let's say in this case we
push it to a sink which is amnesia table
distribute across all nodes and then
consumers can access needs your table
and get whatever they they need from it
this is very typical flow in any
industry actually and it leads itself
nicely to cleanse itself nicely to
building actual plugins with fairly
fixed functionality so that you can
actually instead of writing airline code
you can just refer to those plugins
plugins is Earl and airline modules and
functions and that way you can sort of
build on top of our lungs and you can
can create a separate language of flow
specification language and I will talk a
little bit about that idn in next couple
slides so here's the dr katek chur I I
propose a hierarchical architecture
where first we have a flow process that
is implemented as OTP supervisor monitor
and then we have a bunch of layers and
fairly sort of approximately each of
these blocks here would be one layer
what is typical for a layer is that it
has a bunch of airline processes so you
can
in a way you can say that it's it's a
worker pool or a process bowl but
there's that there's something very very
common and characteristic about all
processes that are running in one layer
and that's they're all the same they're
all the same of the same type they only
run with different parameters we use ads
dead ceniza and tcp UDP for sources and
sinks and all these tables that's that's
an easier for intermediaries
intermediaries are really between flows
so if we want to capture some data set
and use it in subsequent flows we can
put it in what we call intermediary
table and then TCP and UDP we can use it
basically as the source or as a sink we
can connect this TCP socket listen to it
whatever comes in the scanner will take
it do something with it and push it
further into the system message passing
between layers can be synchronous or
asynchronous in most of the experiments
I will show here I ran a a synchronous
message passing I already mentioned
plugins this architecture very nicely
lends itself to plugins you can build a
whole library of a plugins and then the
user really doesn't have to be aware
about airline and airline functionality
he can just specify plugins and use them
as they are and some of the example
layer parameters are layers size layer
ID input and output output data format
and connectivity with adjacent layers
and then mapping functions between
layers and that essentially got how you
pass data from one layer to another here
with different colors I indicate that
these partitioning function can be
different it can be round draw in some
uniform random function hashing or
whatever suits you
here is going a little bit deeper in the
structure of the layer layer consists of
a bunch of workers and it spawns and
monitors workers and it's elastic in the
number of workers which is very
important if you run it on a cloud you
can adjust your number of workers based
on demand you have on the platform and
also typically layer is allocated to a
single node to a single erlang or a
single virtual machine although there
are cases where you can allocate it
across multiple nodes as well workers
perform uniform functions and this layer
connects to other layers sources sinks
and intermediaries and it may have
additional resources like for example if
I have to have a resource here in ETS
table that all workers use i will have
any add stable connected to the layer
and workers can connect to the edge
stable and get whatever they need from
it I'll show one example later here's an
example of sources sinks and
intermediaries so let's say one source
can be a bunch of that stables I already
mentioned TCP and UDP and I mentioned
dance and easy as well and then
intermediary has these two arrows in
sense that data can come into the
intermediary can be temporarily or
staged for a longer term and then
subsequent flows can connect it and scan
these tables and get data out of it so
this way with these basic connectivity
elements you can really build very
complex flows worker has basically three
parts the first part is input where it
accepts an event or a bunch of records
in some format can be Jason it can be
plain text or xml and then there is
user-defined function which can be a
plug-in
that acts on that record and it produces
some output and that output is typically
partitioned and sent to the next layer
all workers do the same thing and all of
them have exactly the same functionality
in all three places but as always with
data part of computing the trick is that
each worker works on a different set of
data a couple of words about the flow
language flow language is a
specification language that acts on
different levels one level is
infrastructure so you want to specify
parameters of your cloud or your cluster
your vm cluster you want to specify your
hardware the hardware you're running
your flow on and you also want to
specify platform in this case it's
airline but what is the node
configuration for each rolling node how
many nodes do we need to run the
physical or virtual instance what are
the parameters of each other like node
how many CPU cores do we want to use and
so on and you need to point it to code
repository where where is the framework
where the plugins so just initialize all
these directories and pass them on to
the airline notes so that airline can
use it at the application level we have
application libraries and we have a flow
specification language and I will mainly
talk about these items involved here in
flow specification language we specify
flow infrastructure what are we using
for sinks in sources what are we using
for workers flow structure specifies the
flow graph which is connectivity between
layers and connectivity between sources
and sinks and other layers and then flow
replication which is if you want to
replicate a flow make sure it's more
reliable you can do it and you can
specify replication factor and it will
it will happen
optimization but you can optimize flow
and I will show an example where I
change the number of workers in a layer
in order to increase or decrease network
traffic or CPU utilization and decrease
overall running run time and then
monitoring really how you this is not
really gangly monitoring or systemizing
is more like flow monitoring so you want
to understand what happens when you
change certain parameters in the flow
how much CPU core utilization increases
how much this guy or increases and so on
so in general the the structure i'm
following here is really a structure
very typical for clouds today you have
infrastructure layer infrastructure as a
service platform layer and application
layer platform as a service applications
the service or software as a service so
this is very sort of immediately
transferable to to the cloud let's see
some specific examples of a flaw
language of specification let's say that
we run this on a hardware of eight nodes
these are physical nodes we put them on
eight machines and we put some machine
specs here optimizer a scheduler can use
these machine specs to better optimize
the flow for example if I'm reaching a
limit to 128 gig of ram I can try to
reduce a little bit data and flow do
some load shedding or something like
that then I can also modify our long vm
parameters or reduce the number of
processes so that so that I utilize this
16 cores the best in the best possible
way here are the examples of long note
specs we have no names we have okay
here's the directory where the data
resides here's the configuration file we
can put in all our line parameters in
configuration file or put them
separately here and in this case either
side
basically point all nodes to the same
rail nvm but they can be different
actually you can have multiple VMs for a
single physical node and that will work
as well data sources typically you
define the type is it adds is it that
stable give it some name ID and give the
size which means this is how many tables
I have and typically tables will be will
be named by this name plus integer from
12 size so we will have here at com
events one out commands 2 and so on and
then we define a sink in this case it's
amnezia table and the table is the name
of the table is user profiles now we
specify the processing graph and all
layers and workers so there are three
basic layers one is scanner when it's
processor one is aggregator this roughly
corresponds to the sliders shown I've
shown before and the first layer is of a
type scanner so there is a way to define
a layer type and then all these apply
actually to these layers so these three
layers actually octant act in parallel
adcom scanner adtec scanner and there's
another scanner in the next slide so
they're sort of it at the same level so
when the data hits the system it will
hit all three layers at the same time
this is a scanner and it has predecessor
outcome events that we define it's a
source and the operation mode is
sequential parallel scans so we will
have one process access one table and
scan it at the same time concurrently
the other process will access other
tables in scanning the successor to this
layer is a processor layer and then
there is a partitioning function so each
worker will call this function with
kone parameter and pass the data on to
the next layer by using that function
this is the function that we wrote in
advance it could be a plug-in so it's
it's there on the disk and we can find
it and use it communication in this case
is concurrent in a synchronous another
layer same set of parameters passing on
to processors and communication again
concurrent and as synchronous there's
the next layer processor layer and we
say this is a worker type of layer so it
has workers that actually do something
with your data we specify all
predecessors we can simply say scanner
here as well but we can also put all
individual scanners here and then
successor to this layer is aggregator
and we have a worker function that was
defined in advance it has no parameters
again communication parameters their
distribution local means that this whole
layer is distributed on one node and the
size of this layer is 36 what is
important to note here is the previous
layer needs to take care of that when
for example if you use the hashing
function that function has to know if
you pass data to a layer that has 36
workers it needs to generate an integer
between 0 and 35 or 12 36 so some
interpreter layer interpreter will take
this data and make sense out of it and
the final layer is is the aggregator
which is an easier table and it applies
aggregator function on needs your table
actually it applies the aggregated
function and size 36 but necia is a sink
sony jie will accept these and was
already defined now this is the example
of concept rank and this is an iterative
algorithm that you typically run a large
graphs for search engines we have that
Yahoo whee google has it and many other
search engine have it I will explain
this example this problem space on the
an example of Wikipedia so let's say if
we have a page about San Francisco we
have somewhere on the page reference to
San Jose which is a city nearby and that
creates a link between a concept called
San Francisco and concept called San
Jose California now do this across all
concepts on Wikipedia and you will have
a huge graph with about three 3.1
million concepts and about 42 million
links now we want to run something like
a PageRank algorithm on it so that we
find which concepts are more important
than the others and this is related to a
probabilistic markov model and it sort
of applies to the following process
let's say you're on the web on certain
page and you want to compute the
probability that you will go to any of
the pages that are that are linked to
that page and your choice at any point
doesn't depend on the previous choices
so this is a very similar thing you're
trying to compute here at page rank of
san jose california based on page rank
of all pages at that point to it and
then you take into account all pages
pointed to by san francisco page this is
the count of those pages l and this is
the page rank of paid out of page san
francisco in this case you sum up across
all pages that link to San Jose
California applies some coefficient in
normalization here and you get a page
rank with that page so what's really
important here is that this is a
recursive formula so you have to know
the pages the rank of the pages that
point to your page before you calculate
a new page and another important thing
is that this is iterative algorithm so
you have to run in my case I run about
10 iterations before you find a solution
if you were to do this in my produce it
will take an extremely long amount of
time because of this guy oh here is what
you get at the end and this is just to
say well it makes sense to do this
because for example you can compute how
important wikipedia users or Wikipedia
editors how important are different
programming languages from their
standpoint so for example Java and C
have a rank of 133 139 a score let's say
so seems to be fairly well entrenched
built into this graph there are a lot of
pages pointing to these programming
languages whereas Scala in their lung
still sort of making headways have very
low score python is de up there as well
this is all in Wikipedia so I hope it
doesn't really reflect the real the real
world and we have some long-term
languages like Pascal and forth run
there as well fearing pretty well c
sharp is doing well as well
so here's the flaw that is actually
implemented on each node except from
nijiya is sort of distributed across the
hall nodes this is a compute layer where
have save for workers and these are the
resources and this is forward link map
and backward link map it contains
references to all link pairs in my graph
and my workers actually access workers
access neezy a table and using some
partitioning scheme and i'll talk about
it in more detail in the next slide they
access certain portion of the mesial
table and read from that table compute
concept rank using F map and B map and
push back concept ID and constant drank
to another table to another new table in
Asia so one table is used exclusively
for rights rights in one exclusively for
reads and this is run in ten iterations
so here's the worker what the worker
does is first it gets them talking it
can be a hash or integer and if the
token is equal to B hash of that cid so
steer worker actually scans the table
and gets out the cid and the score for
that cid and then computes the p hash of
that cid and if it's equal to the token
it computes a concentrate for that cid
this is it just the spread load evenly
across all workers and all nodes so in
doing that the worker has to use
backlinks from be map and it has to go
through all backlinks and find the
length of the forward links set for that
backlink and then compute some using
formulation showed in another slide so
what is really important here is that
the structure of the graph impacts your
computation and you can hope that if you
use the hashing function that you will
evenly distribute the load but that's
not always the case it may happen that
the distribution of nodes in the graph
and links is such that it favors certain
nodes so every time you complete an
iteration you have to make sure that all
nodes are done and then finally when the
computation finishes we push it to
another table cr7 so when we finish we
will have ten tables in Asia holding
different iteration results of different
iterations from concept rank here are
some results and I intentionally put
them in this kind of diary form because
there's a lot of quality inn in writing
data in presenting data this way I think
there are a lot of pros and cons and
trade-offs here that need to be taken
into account for example a total memory
use in this computation is about 420 Meg
63 megabytes of resident memory nizza is
distributed across nodes so forward map
is 126 Meg be map is turning 20 meg and
then tables in Asia have about three
million records in about thirty two
words of memory each so yeah this is a
big data session but this is not really
big data you can easily run this on a
mobile phone and that is a scary thought
you know it can have a cluster mobile
phones around this easily but you know
you can expand you can you can think
that you know you can run this on a very
very large graph and I think as long as
you partition it and you have
enough resources should be okay with it
in this case I have 128 gig of ram or
each machine so barely using a couple of
percent percentage percentages of the
total geek of the total run run times if
you run a one worker process on one node
it takes about 15 minutes for 10
iterations this is very close to Java
execution time on a single node I had a
Java version before I started this one
and if you run ten worker processes on
one node it drops down to 18 and so on
there are different combinations here
that have different trade-offs between
CPU utilization and network bandwidth so
for example this one is the fastest four
nodes with five processes per node
8-point 83 minutes which is about six
times faster than one process on one
node so it's kind of a good bang for the
buck but the network traffic is the
highest of all approaches 35 megabytes
per second but it's not high so to
further illustrate this trade-off I ran
all these configurations and looked at
ganglia monitoring charts so this is the
number of nodes the number of processes
per node and we see here this is a total
load on the cluster by the way this
cluster has 10 nodes but I used in this
case between three and four so only a
part of it is really showing some
activity and then here's the cpu load
this part you can ignore this is just
loading the data into resource tables
but these humps are important and
correspondingly you can see the network
traffic for each one and this is the
network traffic for the best best
scenario where we have four nodes and
five processes per node 8.8 83 minutes
so this one is the fastest but roughly
you know when you look at this chart and
this chart you see that network traffic
is very similar to your runtime so make
sure that you look at the network
traffic that's very important now you
think in the following way well if I
know that four different configurations
I have different runtimes different
network traffic and bandwidth
utilization I have to I can try and
optimize this automatically and increase
and decrease layer size so that I get
better response from the system
typically it runs in a typical backward
control loop you have you set certain
flow parameters like layer size you run
your flow monitor pics status like
execution time or memory use and pushes
it into monitor database and optimizer
accesses monitor database and that each
iteration computes the size of your flow
and then resets the flow parameters in
the whole process starts again so here's
a very simple way to do it it's not the
best way just to illustrate the point
but and also to show that it's easily
doable in Erlang you can start a swipe
from one process per node 2 20 process
per node and then whenever you see that
your total execution time decreases you
keep going forward when you see that it
increases you go back so this way this
very simple optimizer finds the minimum
right here it's a local minima and it
keeps bouncing between two and four
processes / no of course it's apparent
that it doesn't discover the five
prosper nodal point which is the
absolute minimum this wipe but still
it's a good point and another point is
that it doesn't it doesn't get trapped
here but it's sort of oscillates between
two
for all the time and that that's why we
have these spikes in CP utilization
traffic of course you can say best so
far and then it will be trapped here and
execute and have a very flat curve here
now you can say well I want to globally
optimal point and they want to try more
nodes maybe eight nodes and this is what
you get this is the time I think this is
time for one iteration and this is a
swipe from one to twenty processes per
node on four nodes so you get about the
best point here number 5 50 seconds per
iteration in eighth note scenario in
general you get faster response times
but the fastest one is I think 38 38
seconds with five process per node so in
general in both cases if you look at
this point we note polynomial
extrapolation interpolation you get some
kind of area where where your system
performs in optimal or sub optimal way
and those are the areas that you should
target with your application looking at
the swipe from ganglion monitoring
perspective of course as you increase
the number of processes the load on the
cluster increases on the cpu increases
and the load increases and then cash
network is interest network bandwidth is
interesting because if initial increases
it then starts to decrease because I I
think processes spend a lot of time
computing and actually send send out
results Tunisia much slower
this is the conclusion slide so yeah I
think airline is very convenient and in
the property to use for platform
computing cloud computing and data
parallel flows I didn't even think about
using some other language like Java or
maybe syphilis plus but I think you know
it was very smooth and as long as you
know what you're doing after about a
week I was able to build that sea air
flow and it is good to have people in
general especially in big data you have
you have a job called data scientist
data scientists are not used to knowing
programming languages or learning some
extreme programming languages like like
airline so it would be great to isolate
them from Erlang by building some
platforms or spec languages that they
can use and make use of this enormous
power of distributed processing with
airline golfers small airline team can
actually do wonders you can implement a
bunch of plugins and build a fairly
sophisticated system in a very short
amount of time if you have two or three
good good airline programmers so I hope
there is a great future for rolling in
in this area and certainly I'm looking
forward to it any questions
yeah are you from a oil no okay that's
yeah that's a very intricate problem and
we've been working on it for a while
first in a large organization there's
very often and understanding that data
should be should be put first on on this
storage in some kind of disk storage
beat nos or son or whatever and then we
had this storage built report we have a
repository disk repository where we
attach certain processes and we have
clusters for running some jobs some
pre-processing on data before it's
actually sent out to for example R&amp;amp;D
where I work so you're right it's a big
shift in thinking to say now I want this
data in real time piped from advertising
servers to to my platform and I want to
i want that data to hit the platform in
five seconds but if you realize what
kind of business advantage you're
gaining from that i think that will get
the people working and do the right
thing there so we looked at different
platforms we looked at flown for example
there are some other solutions now being
developed and i think pretty soon it
will it will happen
yeah in this case I had in the previous
flaws i had that case where I had to
build a boundary and
what it is is basically it's purely
experimental so you run the first flow
here and say this is my boundaries so i
have to wait across all nodes until this
user profiles is ready you can run your
flows on all nodes using say data flows
from a week or a month and measure and
build a distribution curve and say okay
no in most cases these flows complete
within certain time and then you can
build a timer here that syncs up and go
to say mean plus minus 3 Sigma and
everything beyond that especially if I
don't notice any messaging here because
aggregators user profiling and sending
messages to aggregator so if I don't see
any messages in a certain amount of time
the probability that my flow is done is
very high that's the point where I say
trigger the next flow
yeah this this simple formula is all in
Erlang directly I don't know where this
but you know</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>