<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Analyzing Erlang with Big Data Techniques - Ying Li | Coder Coacher - Coaching Coders</title><meta content="Analyzing Erlang with Big Data Techniques - Ying Li - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Analyzing Erlang with Big Data Techniques - Ying Li</b></h2><h5 class="post__date">2013-04-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pFLlu-cUEJI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everybody my name is keenly for this
talk I'm going to have a three parts one
is a quick recap of earlier session that
our CEO Alex gave this morning how many
of you were there almost everybody so in
that case I could be very quickly flip
through the slice land and three only
three of them then I'll introduce some
data mining techniques how many of your
consider yourself familiar with data
mining and no okay so i think i could
get into a little bit detail about that
and then after that i will show some
examples of applying those data mining
techniques to data that about Erlang
programs earning applications okay so
clearly you are all mostly there with
earlier talk that can Couric's is
betting on the opportunities that's
provided by the many core machines that
in particular the gap between software
capabilities and the hardware machine
capabilities there so I think just to
point out this study ideal should be a
horizontal flat line in this case
because it's drawing the throughput per
time unit so it should be flat and this
is to publish to study by MIT professors
to show on Linux memcache the throughput
over the core many cores and segregation
of performance so with that we certainly
bet found the opportunity and in this
chart the ideal line is the 45 degree
line straight line so because we're
measuring the total game and we and then
Alex showed this example earlier on this
work so I without going into detail
since most of you are there but to add
we do believe that the red is achievable
there
in particular relevant to what I will be
talking later and if you draw the line
of the 45 degree because of the axis are
somewhat different is the slope is you
know rather steep so the huge
opportunities there and indeed we have
reached the 45 x performance again in
the application we studied in under that
this conditions so moving on and just
quick recap again of visualization this
memory usage message communication
between processes and in this real-time
animated summarize the data between
processes communication okay so now
since you look at the visualization in
particular Alex demo I'm pretty sure
that you most of you have used some
perhaps even all of the data elements in
your studies in your daily development
of Erlang code these are just some
elements have lived it listed there as
of now we continued growth data elements
what's our contribution here is rather I
think if you used any of such or several
of such data elements you would agree
that it's not that easy to just get
things data together so tremendous
amount of work is you know needed
depending on your skill level to get all
the data together so we'll what we have
done here is that you have used them
many appliques to pull the data together
in a very easy to use manner and in
particularly if you think about studying
programs using trace data this is not
necessarily something new that that's
not that before in particular for
instance microsoft windows Watson dr.
Watson data and that's has been done
in the past and then brought you know
benefits to the business what well what
we have here though is that if you look
at compared to the data we have today is
that really now we have the much faster
gaining of momentum in terms of
instrumenting new data elements compared
to previously or in a different industry
I come from online advertising industry
I did big data for microsoft online
advertising and my services for near 15
years where the data yeah we have a
humongous amount of data but the
feedback loop and the rate of
instrumentation are quite different and
that um since switching to this study
field I have been tremendously pleased
with the ability to instrument something
that that we can just gather data and
the feedback loop as well so that you
can go back to the code and the make
changes to the code and the behavior
would be you can observe the behavior
and that is quite fascinating for
someone you know coming from a different
field okay I'm just to mention this data
also to point out the time dimension so
all the data when you see in our website
that's in real time animated display of
data but all the data actually funneled
through aw ss3 storage as well so in
every two seconds time window we would
have a snapshot so accumulated slept
snapshot over time and then this time
pull it into a data mining analysis
brought further benefit and complexity
as well but exploded the power of data
so when I first started to think about
this space I don't come from our land
programming I'm not even come didn't
even come from programming background
I'm trying the mathematician and trained
to be a data person and coming in to say
I talked with our developers and then
say you know how about we gather some
data to look at at any time at any time
moment to look at all the airline
processes of their states that they are
in so we got that data and then I study
the differential software packages in
our and in different statistical
packages and I proud it this way so I
just explained to you the color code is
a main color code is for the state of
each processes in so green is free and
this brownish is uncreated so that makes
sense in the beginning the x x axis is
the time so in the beginning lots of
process uncreated and then you see some
created about waiting that's at the
light brown and then over time you see
more yellow coming in yellow is running
state the process is running and the red
pink ish is the garbage collection and
then it's done in green so you look at
this this is a percentage and eat at
each time interval the distribution of
the state or over all the processes and
this in this particular code there are
about 4,000 processes in the system and
the scale is 0 to 1 because it's
percentage and you say all this probably
makes sense because you know it's just
like I explained over time you see the
the change of state of different process
and then if I plot it this state over by
each individual process so now you see
over 4,000 processes there
at each time the state is in now you
see--okay a little bit more of the
things are showing up somewhat and you
can see a lot more of the pink red is
showing up and where you say maybe it's
not too bad perhaps because you know
data click garbage collection happens
and actually kick in eventually they all
finish then if we draw the diagram in
just buy a sword so I sorted all the
processes by the number the amount of GC
it's in in their lifetime in the process
its lifetime and now you see this
picture it's the previous same picture
just a different sort other there are
two points will call out and now you see
a good portion of processes spend there
you know continuous entirety time in
garbage collection that's number one
number two you see there's a somewhat a
triangle going down that way if you look
at that region that means such process
which spend majority of time in GC will
grow if you let the code run over time
longer right so that triangle will go
down back right so that of course I I'm
just a date a person so when I look at
this I I don't know what to do about it
and when someone who are more familiar
like Alex who look at this he actually
was before I even say anything he was oh
wow this is fascinating as what is
fascinating Alex before I even gather my
head around that he already was able to
point out where the code that he could
fix and he actually reported earlier
this morning also that we actually get
gain from such insight and that needed
his expertise in understanding the code
as well not just data so i go back to
talk a little bit of data mining now
a typical data mining process and I draw
in that cyclic process flow that
basically between business understanding
and data understanding that is a
business understanding would be in the
online advertising space we would want
to understand the business goal we want
to understand the typical scenario they
expected user behavior versus observe
the behavior and draw the difference
make different models in the in this
space in perhaps understanding of the
program understanding what the code that
needs to be accomplishing and the data
understanding also it's important that
what the semantics of that's in the data
and not only just understanding the
syntax of the data and then a huge
amount of time is actually spent data
preparation there so data preparation
often time you must charge the data into
the form that's minable where we call
the modeling you through the data into
different modeling software statistic
packages your own code you build
different models and you evaluate the
model against the data and possibly
oftentimes you go back until you get
satisfactory evaluation before you can
deploy so this cycle typically is goes
around i would say for any satisfactory
model probably easily in a tense range
probably you know over a hundred
sometimes to get one satisfactory model
so early on there were some question
discussion about whether you actually
you know how much spun up would really
help and then you know how much
memorization you know when you do that
and you know is it sometimes automatic
just to do it automatically may not
really you know it could be having
searching in fact as undesirable
that basically I might my thought would
be you usually you observe a date have
many many rounds and you iterate on the
model as well oh so I'm going to with
this I'm just this is just a process
view then I'm going to start going to
drill into a few specific examples of
data mining techniques any questions so
far okay and I want a particular cut out
is oftentimes now I made a point of here
saying discovery of non obvious well
this one oftentimes it is you were you
will see this definition in you know in
all the you know you search what is data
mining what is machine learning you
often will say well discovered long long
obvious things that's the power but as a
data scientist working on data actually
a lot of time your discovery is what's
called confirmative you're actually
going to confirm the code is doing what
it's supposed to do or the user is you
know the predictive model of the users
click behavior is you know it's doing
what it's supposed to do it's often
confirmative so therefore I just want to
make a distinction of that because I
certainly have encountered a lot of that
when looking at the data I show chart I
thought the Charter was quite you know
interesting to me and I show it to our
engineer or engineer would say yep
that's hot that's yeah it does says the
code is doing what it's supposed to do
and to me that's actually good news so
you know don't just cross out any data
work to be shy you didn't tell me
anything that I did not know it's it's
the confirmation from data that's also
very important okay so first of all when
working with data with any data mining
you know analysis a ability to measure
similarity it's very important so this
measure the ability to measure
similarity it's just you because you
will encounter different objects you
will encounter data that's
in you know different form a different
shape it's a vector it's a matrix is you
know have many attributes complicated
data structures and representing a real
word object and how do you measure the
similarities and you know typical you
know if you line up the data elements
into a vector of a fixed order that you
know so that statement itself means a
lot of work actually because the
processes for instance if you line up
all the processes and then so that you
can create a data vector but the problem
is that the processes actually come in
and out become alive and then die or you
know get to finish so how do you create
that that vector but let's put that
aside so if you're able to line up all
the data points allowing your fixed
other and basically you get a data
vector and then you can do some existing
straightforward mathematics in this case
you measure the angle between the two
vectors and that angle you know often
called the cosine similarity but it's
actually our cosine because it has the
property of a distance metric it's no
longer a it's a much easier metric to
operate and with that lots of the
wonderful world is simple measure lost
wonderful work has been done in the few
different industry different field in
our case one application was done on
benchmarking repeatability so when when
we first had our instrument of data and
the data you know just amount is we want
to stress the system so therefore we
cannot just say you know run and run it
just let it run for two minutes and
measure it and you know just be done but
because we want to stress the system we
want to now we get a data now we say
well is this representative in
enough of the behavior of the system
under stress meaning that stress you
know of system state is that a
repeatable that measurement is
repeatable of course you want them to be
repeatable in the sense that you know
your measurement makes sense right so
this one in this case we line up the
data and measure the similarity as the
data observation for each observation
and then you calculate the cosine
similarity and it turned out this bench
bench when we call the bench run this
one the third one has one two and forth
are more similar to each other and the
third one is slightly different and you
know over time if you gather enough
observations you can set a threshold
where is a acceptable observation
acceptable repeatability all right
that's one measuring similarity and this
one will be used in our subsequent
studies so clustering many of you
probably heard a clustering ray so in
this it's you know just very simple
group similar things together and then
put these similar things separately into
different groups and assuming a existing
metric very simple algorithm here a gift
one is that this is the typical k-means
clustering you probably heard the
k-means as well so you basically start
with the initial set of centers and then
you measure all the points assigned a
pointer to the cluster that's closest to
the center right so this is a explained
that the p is the point to the Center
for I is less than P to the center of
tea for any Center j4 nak for any year J
that's less than K within the center so
you have assignment of P to the cluster
and then with a bad assignment you
update the center because you shuffle
basically you shuffle things around a
little bit you have to update the center
which is T plus 1
you get to the center from center of tea
and calculate that and then you repeat
until this process converge or until
certain measurement of change is within
a small sigma one of the application of
such clustering is to run this on the
earlier garbage collection machine state
process state you probably you recognize
those color that to apply a clustering
unto the four thousand processes and in
this case after applying running a
clustering model on to it with some
distance measure and the sequences we
actually achieve the five cluster this
is actually of importance in the sense
that now you can treat there are four
thousand things in five different ways
so you have a way of organizing them or
you can treat them accordingly and of
course Matt matched with knowledge about
the actual code about the scheduler you
can do some magic things over there
clear enough on this one clustering ok
moving on time series analysis because
we've started particular interest is the
state of the process changes over time
of their behavior their different work
done over time their consumption of
resources over time so the study of time
series is of particular use here there
are you know again many things you could
do is time series and the same applies
to the previous example of clustering
there are just about unions hundreds of
thousands of algorithm that you can do
clustering and in this case one
particular useful algorithm that we
found is easy to understand and also
easy to imply
news is there was called a time warping
distance measure on time series and I
use that as a a distance metric to
cluster all the time series because
again you think about each this by the
way I show those time series it's
intentionally kind of in as a service
background but each of this little
swegle is one process of that process
memory usage over time so you can think
about other things you know the number
of reflections of that process over time
number of messages over time so and then
it would be for each measure and for
each process you could get one time such
time series so therefore x here is
clustering is of use here because again
for this application that we ran we
actually have a word 9000 process and
how would you do with this 9000
processes I'll give you more details on
this example but think about time hoping
that hopefully that graph demonstrate
quite well basically you kind of you
work the time and and I provide a
comparison with as you know as if you're
doing the diff you have a insertion you
basically think about shifting the time
out and delete you think about shifting
the time into compared to two time
series the top serious and the bottom
series there's you how you measure the
similarity ok so moving on I I'm still
in the second part of my talk
introducing the data mining techniques
and this is very high-level very quick
introductions and hopefully if this is
not clear actually you should just shout
out I can give you more details more
explaining all right so now as you see
from our earlier demo the visualization
of the communication message passing
between the process that's naturally a
network and there is a big body of study
already done and sulfur exists to
analyze network you know very well long
in a social network analysis area that
has been very popular lately so there's
multiple things again you can do in
network analysis in this one I'm
introducing what's called centrality
centrality is a measure of importance of
not so you have this this one I think
this network has about I think it's
about 60 some notes in there each node
representing one process and the edge
represent a message passing between two
processes and there is a direction and
also often times you can represent the
number of messages or the account word
count of messages in the edge and in
this case I did not draw how do we know
which knows meaning which process is
important in this network without
knowing the code and just by observing
the data and then applying data mining
word unto the data we could study the
code and then help identify the
importance of the processes several
measures to help define the centrality
this one the first one is very
straightforward degrees the number of of
edges connected to the node right so you
could have an in degree out degree or
non-directional degree right and you can
wait them also by the number of messages
on each edge so the second closeness
degree centrality is measuring the the
its measure how close a node is to the
rest of the nodes in the network so
that's kind of a measuring certain kind
of import
in terms of influencing other notes this
is particularly in the social network
analysis part basically about ability to
influence you know how are you closest
to the rest of the network so in this
case measuring the the universe of the
close the short the shortest path that
this node is on to the rest of the
network so measuring the centrality of
me to all the shortest distance between
you and be the rescue belongs to the
rest of the node and I'm able to find
this interesting enough or boring it up
all right she going all right go okay
all right so i get vector sensuality is
another one basically it's it's
measuring the importance of a note by
the neighbor how many important
neighbors the node has that's in a
nutshell and you think about this is you
can get into a recursive definition now
is the the importance of the node is the
neighbors importance some blood in a way
and you solve this basically solve a you
know an eigenvector value and use
numerical computation to get the degree
for the centrality for each node and
then last I listed here is between this
the between is it's not quite as the
clothes closeness the second one the
closeness is like how close you are to
their the between this is that how much
are you on the path between other
communications so that's another
importance so then this one you say you
find for each for the note that you
calculate the importance of the node by
finding the shortest path from s to T
where
is a versus the total right so you can
you know a fraction what's the
percentage of the sort shortest path
between any two pairs that i am i right
so um we've you know i'll show examples
to applying this important measure our
notes and to share with you that her new
potential application could be maybe it
will help identify the bottlenecks or
identify the manger workers in the
network ok one more introductory
techniques and in a data mining
application area is that community
detection so this one again concept is
very straightforward grouping similar
laws into your groups you can imagine
that potentially you could use the
different measures from from the
centrality to apply to community
detection give one example that you
could use between this as a measure to
compute the community in this case and
potential application I don't know
perhaps you know more you could tell
more that if I give you a network and I
tell you the different groupings of the
processes in the network perhaps you
could do something more useful with it
is it all clear everybody so algorithm a
wise that's a different algorithm you
could rank clustering you could run
minimum cut so think about this how do I
detect community you start to calculate
something and then you start to cut the
edges and then see you know now the
network is cutting into different pieces
and then you just recursively cut and
that gives you a hierarchy of cus and
then you can select where you want to
do the clustering or where you want to
do the community selection okay so with
that I'm going to get into the third
part of my talk is to apply some of
these techniques to the data we have
obtained thanks to our earlier customers
we we have some data of running
applications and in running code of some
sample code and collected data and then
run some analysis on them and once you
just mention that i think i have already
repeating myself that the data is
available in s3 and all the application
of what I'm showing no it's on the data
that's that you would get in you know as
if you downloaded concur Xcode the run
time machine you could get the same data
so there's no you know magic data that
I'm getting outside what we make
available to everybody but in the
meantime it is a offline analysis so
it's not quite in our product yet but
they influenced how we put together our
product offering by doing lots of such
analysis right okay so oh the example
that i'm showing is our our own website
can carry calm and you you saw the demo
earlier this morning and we are for when
we look at this we collected data in
this case we run the longest running the
data we collected was seventeen thousand
nine hundred thirty-nine snapshots so
imagine that you know little sweet goals
that i showed you imagine that times
seventeen thousand times multiple
measures times the number of processes
that's running so it is a big data power
it's fairly rich but very interesting
to analyze but with any data task you
first you get up you know you look at
see if you can get overall picture you
know what is going on by looking at this
it's up to 200 time windows I charted
here and it's all aggregated already so
the purple line is number of total
messages and each time between our
possible processes and then the bring is
total communication pairs and blue I'm
reading it because the the legend may be
too small the blue line is the third
from top is the number of life processes
at that time and then red is the number
of active so live it means they still
there but active actually sending
messages or receiving messages number of
processes so you look at this this makes
sense you're right you say you know well
I have more live process than your
active and I have more pairs our
processes the green than the life and of
course I'm sending more messages there
so make sense but you see there is a
spike there or something happen there
what is that and this by the way I did
the analysis you're just basically just
go to s3 get the data and start
analyzing without knowing you know the
old something happened there and let's
look at the data all right it's not that
I designed some change I planted
something there actually I only see this
after I wrangled with data for two days
so what's in there and in fact I have no
way of knowing because it was already
passed it was something happened on our
website so no one actually could not
couldn't know you know what's happened
on that particular day at that time but
we knew something happen there so but
continued the data exercise see what we
can find what's in there what the data
says there and say let's look at the
reductions over time so perhaps the
number of work you know
and look at the same time window and see
what happens there and this one is kind
of saying well there's something happen
here right but wait a minute this one is
around the time window 50 and this one
is actually clearly not 50 well there is
a little thing there but that's not 50
there's the bigger one we're studying is
around the 96-97 all right so you say
okay the reductions don't tell us too
much now let's look at our memory heap
sizes well clearly some pattern you see
cyclic behavior and something happened
in the beginning and but still around 97
nothing we could spot right so let's
look at then you say what do we do after
this usually as a data person will say
let's let's zoom in you know lets you
know drill down into time window 96 time
window 97 okay what happened there so
you study the heap size first and then
we say okay let's look at all the heap
size and see what we can find out we
actually run the clustering in this case
you know over many thousands of
processes and run through the posturing
we find actually five clusters it
actually naturally map today the you
know MFA structure which actually makes
sense by in this case because we know
the code you could have studied the data
by saying let's start cluster the
process by mfa's but I wanted to do this
exercise so that we know clustering will
help us when we do not know the mfa
structures right so in this case and I
showed you you know this way was that
there's clearly all of these come from
the same brand and the clustering
actually put different processes into
clusters that actually makes sense and I
just want to explode into the big
picture so in this case I show this to
from the top of the two processes of the
rightmost
column and there are two different
processes and you see there are somewhat
different but you can see they're kind
of a similar shape the classroom makes
sense and one can say perhaps we could
you know coded or we could change
certain parts of the machine or system
to do things together on those processes
that have similar structure okay so
let's look at the network sensuality
let's zooming in into node at a time 96
96 time window and when we show this
when we work on this network let's work
on the network sensuality and the top
processed shows up and then for people
who are familiar with Chicago part of
the web server land said HTTP p managed
manager in it Earl prim loader request
OTP ring boss I'm Q and the gin event in
it so these show up by the importance
for the other edge centrality that i
introduced the earlier ok and we can do
that using the betweenness centrality on
the same network alright so you see
there different some shows that same the
top one HTTP see manager in it same then
it's different and it makes sense one is
just purely degree he's talking to me
how many are talking to me whereas
between this is how many shortest path
am I on how critical IM to the rest of
the network and then study the structure
changes of the network so this one is 96
time window and this one is actually
nine 1834 time window there or
structures didn't change too much by the
way sorry about this
not to be confused the color actually
it's just a different color legend it
doesn't mean they are different
community okay now so all the work with
know hopefully we can help to detect the
bottlenecks in this program and what
should we do we look at the overview we
looked at the individual snapshot the
network structure we look at the
clustering what else can we do so we did
even look at the overview of reductions
and heap sizes so basically that says we
need to look at further into individual
process at the individual time window
and see what they do and by that we
tried process with the highest number of
reductions / incoming messages so now
we're locate the individual processes
and then look at a number of reductions
/ incoming messages and we could get the
top processes again so at least is seven
top seven of them basically if do that
calculation you can rent rank the entire
list of processes into a other and then
you can choose however however many you
want to look at look at those from top
seven so again I ask you know our
engineers and so does this make sense
well then you know Alex Alex and Charlie
they all can say well yeah yeah this
makes sense all that is to be expected
yet also you see conkers shows up to our
own you know function shows up there too
so for me to look at this and then to
map it to the individual process so now
I know which one to look at remember
that we have thousands of processes now
i jus mean i am able to zoom in i can
spend as much energy as i have is
because I in the author that I choose so
I can look at the top one and show the
reduction for incoming messages over
time for the number one
process alone yeah yeah yeah so what we
have in the quantized version is very
best job yeah what we have here in our
trying to understand the answer the math
behind this we include all the data
because yeah so we can yep right yeah
hold that sound for hold that thought
for 10 second that you show that bad
ladies at seven show that let's look at
the next one is young that is the
conquer x1 it shows there and you see
well this didn't really pick anything
but where did this show up right that
was a good question right so because you
know you visualize you cannot see that
spike and you look at the next one is as
TD pc handler in it that shows up there
too you see the spec there and next one
as well so what's this you know like or
I don't know what's this but now I know
a reason if you look at the axis here
this number here here if you see they
the height is 4 to the power of 5 all
right so this is in the hundred thousand
range this one is in the thousand ridge
so it's a hundred times magnitude right
it's quite different and this explains
actually why when you look at the
overall picture you actually cannot see
because this guy this this guy actually
influenced a lot of that
this is all I have how you stop here all
right punchline yeah yeah yes sorry
alexsa hopefully we can put this into a
high degree of confidence for automatic
about the neck detection questions yeah
yeah so just repeat for people online
the question was are we using Amazon and
am I using some of the latest data
warehousing that's available in Amazon
or is this offline batch processing or
is this just knock on the door and get
custom person yes we do use Amazon the
data data is in RN s 3 this particular
work here is done on using our in the
batch mode so we hopefully we could you
know production eyes this in to a
fashion that you know acceptable to our
user experience that you can you know
use the insight that's generated in a
timely fashion and in the acceptable
manner yeah yeah the garbage collection
time outside temperature and if you
would have run it in a smaller volume
best memory of a garbage play more often
and hotter yes so babe oils Laurel or
both of integrations because the bulb
oil for pd sr T&amp;amp;T the temperature that's
the percentage of time it's garbage
effectively that's the volume of the
program and P must then things are sort
of number of CPU cycles so for the first
graph you've got all the data if you
actually beg Boyle's law can we apply
our physics you know Boltzmann's
equations and statistical mechanics to
it because I'm always thought with
programs get really big the way to
analyze
so you've got the evidence if it's very
poor slowly statistical mechanics we've
got the evidence there have you looked
for that um so just repeat the question
now like this question from Joe that
have we looking at the garbage
collection data that looks like a map of
the temperature or physics system did we
try to apply some of the physical
formula application thinking into this
data because there is a good similarity
beating that you work we're trying to
apply big data analytics is more
physical laws because on these large
systems the non a data to go
understanding yes this is what Kepler
yeah measuring where the planets were
and then you've got a now you've got the
data sending that money theory explains
it yet and we were just at the beginning
of it that's fantastic we're tragically
running over but hopefully we'll be
around one assumes yes yes you do and
available any questions you might have
and I know I talk about this stuff this
is fascinating and it just kind of
frightening actually so anytime any
questions I look forward loaning from
you I have done big data over 15 years i
have only coated a few code in early i
coded fair bit more in art and other
stuff so looking forward to it thank you
so much thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>