<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Factory SF 2015 - Ransom Richardson - Erlang in the Cloud: The Talko Service Architecture | Coder Coacher - Coaching Coders</title><meta content="Erlang Factory SF 2015 - Ransom Richardson - Erlang in the Cloud: The Talko Service Architecture - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Factory SF 2015 - Ransom Richardson - Erlang in the Cloud: The Talko Service Architecture</b></h2><h5 class="post__date">2015-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NfhD_Ycf3RA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you for coming I'm ransom
Richardson i work at taco so to
introduce myself I'm going to put this
slide this has really nothing to do with
the talk i'm giving but this happens to
be the last time I gave a conference
talk so this was 16 years ago I might be
a little bit rusty it's been a long time
since I've done this but the other
interesting thing here I think is so the
title of the talk was about concurrency
right the concurrent composition of zero
knowledge proofs concurrency is
something I've been interested in a long
time in a bunch of different forms so
about three years ago when we're
starting a new company looking at the
technology stack we wanted we wanted one
that would support the type of
concurrency we thought was important in
building a service so this sort of led
to in some ways and test getting into
Erlang but what I am going to talk about
today is they're lying in the cloud so
they're really two key key sections to
this talk the first is the idea that the
cloud is fundamentally different so some
of you may recognize James Hamilton
who's a VP and distinguished engineer on
AWS he has some great talks going into a
lot of the technology behind AWS this is
a link to one of them but he really gets
gets at this idea that the clouds
different so he covers it from a lot of
different aspects I'm going to look at
three aspects that I think are
particularly relevant to what the
service we've built as well as two
people working with Erlang in those
three areas are reliability deployment
and then dealing with service state and
then then sort of the second large
section of my talk I'm going to talk
about an architectural change we made to
embrace using more Erlang in our service
and why we did that how we did that so
so first taco we're mobile team
communications app you know the example
I think that makes sense to developers
is we use this every day for standup we
have a distributed team we have
developers in san francisco seattle in
boston this
app you know it was often where mobile
we're not always at our desk this lets
us communicate using voice also we often
text on the side if I you know maybe I
stop paying attention partway through
stand up i miss something and I want to
go back later and hear what that was I
can go back to the recording about stand
up and listen to that or I can you know
easily find where somebody who's talking
so that's that's one way we use it but
we find it find it really useful in that
context so behind this is our service
our service does a lot of things voice
is key to it right so we think voice is
key to having effective communication we
have voice traffic going through our
service our services all all hosted in
the cloud so you know people people i
think one concern we had when you're
starting out is is network performance
in the cloud going to be good enough to
support a you know real time service and
the answer I think is it's way better
than mobile networks you know I think we
particularly there's there's you know
the latest networks in AWS have enhanced
networking if you're running on a new
instance you get you get really good
network performance at least good enough
that that you can keep up with mobile
clients so we've got a you know our
service does a lot of other things it's
a it's a fairly large service we have a
team of three people who work on it none
of you know we've all worked on clients
as well not not full-time and this is
the team that does everything from
designing the service to implementing
and building it operating it getting
woken up in the middle of the night if
we have a problem so the question is you
know when we were starting how are we
going to build a big service with a
small team obviously developer
productivity is key so there are a few
key technologies that helped us do this
first of all is the cloud we use Amazon
Web Services we use Microsoft Azure for
this talk I'm really going to focus on
amazon web services because that's what
most people use
so you know the cloud let us build build
our service without having to worry
about provision card we're building a
data center networking all that kind of
thing so you know second key technology
for us is Erlang Erlang has really made
it easy to build code that's scalable
and maintainable for us very reliable we
have you know I say it's a big service
it ends up we have about 25,000 lines of
relying code that we wrote which seems
like a really small number you know I've
worked on I've worked on projects that
have two million lines of C++ code and
what we can get done with 25,000 lines
of airline code is is pretty impressive
and so you find a final key thing for us
has been open source technology we use
for voice processing we use a modified
version of free switch we but a lot of
open source technology and you know
taken together this is let us build with
a really small team our big product so
the last startup I did you know 15 years
ago we were probably at a similar state
we were more than 10 times the size that
we were now just because we were having
to build a lot more ourselves so it's
really it's really great since open
source has been key to us we've looked
for ways to contribute back the primary
library we've contributed to his Earl
cloud so this you know if you're using
Erlang if you're using the cloud from
Erlang there's a good chance you're
using RL cloud for us we started using
DynamoDB which is amazon's hosted
database shortly after it came out and
there was there was an erlang client for
it but it wasn't complete and it wasn't
it didn't meet our needs so we decided
we had to write our own we decided to
write it as part of Earl cloud so we did
that we there are a few other things we
had to had to enhance an oral cloud to
meet our needs and when gleb wanted to
hand off maintaining it he handed off
maintaining this library to me so I
maintain it now but
we have a whole bunch of people who
contributed to it so these are some of
the other top contributors and basically
as people are using as using AWS they
add the api's that they need to this
library so tends to be pretty easy and
it's also a great way if you're trying
to learn in Amazon service I understand
DynamoDB much better now that I've had
to implement every single option on
every single method and understand what
it does so if you really want to dive
deep into something implementing an API
for us would be great all right so I
want to talk about reliability there's
you know the basic the basic lesson of
the cloud is it's unreliable and I think
this is this is cut this should be
should be familiar to Erlang programmers
I mean we deal with crashes as well but
you know the cloud is built on commodity
hardware the goal of the cloud is not to
build any single instance to a point
where it's never going to crash instead
you take advantage of the fact that you
have a lot of instances to build in
build and reliability so to a certain
extent you just want to let things crash
right if you have to accept and embrace
the fact that in the cloud things aren't
going to crash they're not going to go
well at times and in Erlang we have a
similar thing right we have processes
and we have supervisors so if your
process crashes your supervisor will
restart it in AWS you have instances if
your instance crashes you can use an
auto scaling group to restart it now
Garrett was just talking about naming
and the importance of good names this
thing auto scale group is pretty poorly
named because you don't necessarily have
anything to do with scaling so the
simplest way to use an auto scale group
is to say I always want to have two
instances of this type up and make sure
I always have to and it will never
change it will never scale but it'll be
there watching if one of those instances
goes down it'll automatically start
another one for you so that's that's key
to you know building a building reliable
architecture in AWS unfortunately so in
Erlang if your process crashes your
supervisor will typically restart it
very very quickly
in the cloud it takes minutes probably
you know it takes a while for the auto
scale group to realize that it's down
and then start up a new one so it's not
quite as easy to recover immediately
from crashes in the cloud as it is an
erlang a couple more key ideas in AWS in
particular in terms of reliability first
of all if one instance is going to crash
you have to have another instance there
that's going to be able to do whatever
it was doing once you have more than one
instance you have to make sure that the
instances you have are not going to
crash at the same time now the way that
you do this in Amazon Web Services did
you make sure they're in different
availability zones and the concept of
availability zones is so you know you
have regions which are geographical
regions so you have Us East which is in
Virginia us West EU west in Ireland so
those are totally separate and they have
relatively high latency due to do to
speed of light between them so certain
services you really want to have low
latency connections but you still want
that protection from failure and that's
where availability zones come in so
availability zones are located close
enough together that communication
latency is very low between them so one
or two milliseconds but they're far
enough apart that it's really unlikely
that a single natural disaster or
man-made disaster will cause them to
fail at the same time so they do things
like make sure they're in different
floodplains and all kinds of things like
that but they're close enough that you
can still have low latency communication
between them so you need more than one
instance you need them in different
availability zones you have to have
extra capacity right if you have two
instances but they're both at ninety
percent capacity and one goes down
you're not going to be able to to absorb
that on your other instance and it's
also possible you know if one instance
goes down and then that causes in our
case we're handling a lot of connections
if those connections going to the other
instance even if you had enough capacity
to handle them at steady state there's
extra extra work to do to to set up
those connections
you have to be able to absorb that work
so here's here's sort of a simplified
view of our architecture we have clients
that connect to our service so we have
three types of servers in our service we
have media servers so media servers run
free switch you know our modified free
switch as well as our line code that we
have that you know controls the
conference handles recordings and things
like that but effectively that is
real-time traffic right so that's UTP
and RD RTP traffic our clients also
connect to our session servers so this
is a long live connection if it's a web
client it's over web sockets mobile
client is SSL and this is our own on top
of that we've built our own protocol
that's a synchronization based protocol
so you know mobile mobile clients are on
unreliable networks and typically slow
often slow networks we want to make sure
that for our clients to connect and
synchronize their state it's not it's
not too expensive right we're not
pulling down the full client state every
time so we have a we have a
synchronization state where basically
the mobile client will send a high
watermark or a set of high water marks
and the service will then go check our
storage and see has anything changed
since the last time last time you
connected so but then once that state is
established and we have this connection
then we're only sending anything that
changes down to the clients and finally
we have our what we call our bz servers
this i guess is a placeholder name if
you are Garrett's talk it has no no
meaning and intentionally no meaning
this was you know failure to find the
right word so we went with a word that
meant nothing but basically what they do
is they provide the interconnections
between our different services so client
is connected to one session service
needs to connect needs to talk to a
client a different session service that
communication will go through the BZ
service so in terms of reliability
what's going on here though is that each
of each of these types of servers we
have multiple instances they all have
their own autoscale group they all have
instances in different availability
zones
and then our finally our storage back in
we use hosted storage that is reliable
without us having to do much so when we
look the other the other piece to
reliability is getting reliable on our
instances so we've got our service
running in the Erlang vm but when we
were first doing this we were worried
well what if the Erlang vm crashes right
how are we going to restart that so
there's this project open source project
Earl d which is makes the airline vm
look like UNIX Damon so that and and it
will it does heartbeats into the into
the vm make sure the vm is still still
alive so it's it's watching the airline
vm if the Erlang vm crashes or hangs it
will restart it for us on top of that we
have upstart which will restart LD if LD
happens to crash and you know that will
start up when the instance reboots so if
the instance reboots everything starts
if the Erlang vm crashes it restarts it
turns out i think maybe we over
engineered this a little bit i don't
know that you necessarily need to go
quite this far the Erlang vm really has
not crashed very much at all for us
certainly recently so the fact is that
our auto scaling group and actually
working with the elastic load balancer
are probing the taco service and
periodically you know regularly making
sure the tacos service is running if the
Erlang vm did crash it would I think be
acceptable for our application to just
let the Erlang vm crash the auto scale
group would realize our services down
and restart a new one for us so if I was
doing this again I might not build all
these layers but Earl d does does it
work pretty well if you're if for some
reason you're worried about the Erlang
vm crashing all right so that's a little
bit about reliability deployment in the
cloud can also be different and it
doesn't have to be there are two
approaches a lot of people do what you
typically do with your own data center
which is you've got your instances
running and you upgrade you deploy your
new code to those instances the other
approach though that the cloud enables
there would be a lot harder in your own
data center is you just deploy all new
instances so you know the cloud has
effectively infinite capacity so if you
want to run twice as many or three times
as many instances for a short period of
time that's something you can do and
this this enables you to deploy your
service in different ways and that has
some advantages first of all it's easy
to roll back right so you start
deploying your new code you realize
there's a problem you still have all
your old instances running they're still
there ready to take load back the other
thing is as we talked with reliability
your instances can crash and you need to
restart instances from scratch all the
time to be reliable if you're restarting
instances from scratch all the time
every time you deploy new code you know
you're testing that constantly the other
thing is if you have an instance that
you know if your instances up for a few
hours or a few days there's less time
for somebody to get on that instance and
change some configuration that's keeping
things running or introduce some bug or
something else you know your deployment
process is is very repeatable and you
know how your instances are and also you
have you have less of a security attack
surface area because your instances are
constantly cycling the you know you're
always taking down the old ones you're
always putting up new ones it gives
gives attackers less time to actually
find your instances and get into them so
what one way people talk about this is
blue green deployment so you know this
is a diagram from from Martin Fowler's
blog but the basic idea is hey you've
got your service running you've got the
blue service running to start with then
you deploy the green version of your
service so you deploy your service all
new green version once once you think
green green is up and good you switch
the router and that's where this diagram
is the router is now pointing at the
green service and then then once you're
sure that's working and take the blue
service down this sounds okay in
practice I think when you start looking
at the details it gets kind of tricky so
this diagram I look at this diagram and
so it's deploying the whole service and
this is kind of a typical three-tier web
application database server
um I don't really know how you deploy a
whole new version of your database you
know how do you migrate the data to the
new version how do you roll back if
there's a problem with the new version
right that that seems really hard so
that's definitely not something we do I
mean we use hosted databases so it's not
not something we have to do but the
other thing here is that doing you know
this shows doing your web steer in your
application here at the same time so
effectively you deploy your whole
service at once and if you've heard
microservices there's there's sort of a
trend in the industry p or at least a
lot of buzz around the idea that you
want to keep your services separate and
upgrade them separately and reduce this
kind of coupling so I think in practice
this is kind of an inspiration for a way
to do deployment but in practice I think
people do it differently Netflix is well
known for both doing this type of
deployment they call it red black
deployment which I think fits better
with their corporate color scheme but i
think they also do micro services and
they deploy each service individually as
do we so what we do when we deploy is
that we deploy each service individually
so are you know we can upgrade our media
servers without doing anything else the
disadvantage of doing that of course is
that we need to maintain backwards
compatibility so if we're changing the
protocol we need to support both while
we're doing the deployment it makes the
deployment a little a little trickier
than if we could just do an immediate
switchover but the other problem for us
is that we've got state right so people
are we've got free switch there which is
you know running an audio conference
there a bunch of people connected to it
if we if we lose that box users are
going to notice you know it's not it's
not horrible it's probably you know you
might you might have a few second
interruption in your call so not the end
of the world but not what we want to do
based on our everyday deployment so what
we do is we deploy our new instances we
test them make sure they're running and
in particular we all everybody internal
to the company runs a special build that
always connects to the newest instances
so well
while our customers are still on the old
instances we're all running the newest
newest stuff that's out there we can
test it depending on how big the changes
are we might test it for a long period
of time or a short period of time but
once once we're pretty sure that it's
it's stable and good then we close the
old instances to new traffic so we
basically say no no new traffic is going
to go to the old instances any any new
calls will start on the new instances
then we wait for the load to move and
sometimes you know we're watching this
like three hours later there's one call
with 10 people in it and I was like
someone's in a really big conference
call but we wait for them to finish and
then then we'll take those instances
down so that's that's kind of how we do
do deployment it's a little different
for each of our services so like I said
media services we have to wait for the
calls to finish session services often
the clients have long lived connections
we we typically don't wait for all of
the connections to close and we'll we'll
move some traffic over make sure our
metrics look good and then we'll just
just bring the old ones down and we have
a bunch of connections go at once the BZ
services which are the inner connection
we haven't implemented this as well yet
so what's going on there is that we have
a hash ring that allocates objects to
those services when we when we deploy
new ones they start getting traffic
right away so at some point I I would
love to have something that lets us sort
of more gradually move traffic onto
those but it's not not something we've
gotten to so service state is ties in
with those last two topics right
reliability you need to expect your
instances to go away and bring new
instances up when you're doing
deployment one way to deploy string up
new instances that gets hard if you
start having a lot of service state so
in the cloud there's a lot of pressure
or if you you know if you know take an
Amazon course on how to architect for
the cloud or go to their talks they're
really pushing you to build stateless
services right so you build a stateless
service basically your service code
won't store state there it'll get
whatever state it needs to process an
event from a different data store or
other places
this makes it really easy to build
reliable and deployable code because if
you need or scalable as well right if
you need more processing you allocate
more instances you don't have to worry
about migrating state or anything like
that the problem is there's lots of
stuff you can't do without any state and
so in the Erlang world i think when i
think of service i think of gin server
right and a gin server has a code that
runs on the state and the server and
it's actually really great like this is
why the after model is so greater this
is the actor model right you have your
code and you've got a single actor on it
and you can easily reuse it about that
state that system and understand what's
what it's going on but in some ways it
makes it harder to build in sort of the
cloud a reliable and scalable to service
but with taco you know to support really
low latency communication between
clients we needed to embrace the fact
that we have some state we have these
calls that are in progress we have
persistent connections from our clients
you know this synchronization state we
don't want to be constantly having the
clients reconnect and we have state on
where every client is connected so that
we can reach them if somebody wants to
send them a message so the question for
us when we were building our services
how are we going to manage this state so
that we can build our service but you
know still make it reliable into
playable in the cloud and what we end up
doing and when the way I think about our
solution is basically is that we think
of this state is ephemeral right like a
cash in some ways so we're not going to
be too concerned about losing it we
don't want to lose it all the time but
we're not going to add redundancy we're
not going to really try to avoid losing
it instead we're going to brace it it
can be lost and make sure that we can
recover from losing it quickly what that
means for our different servers is kind
of different on the media server if we
lose state we lose your in progress call
session server we lose your state your
client has to reconnect to another
server reestablished synchronization
context
you know you might probably that when a
user wouldn't even notice if if we lose
one of our BZ servers that provides the
interconnect there could be some
messages bound for you that you miss and
then you'll have to notice based on our
synchronization protocol that you missed
something and pick it up so you know
there could be a little a little bit of
flakiness but probably not something
that use it is going to are going to
pick up on it does have challenges you
know the way we build things with the
femoral state obviously user impact if
it gets lost it's required that we can
detect and he'll outages very quickly so
if a server does go down and we start
losing state we need to know that very
soon we we found that we're we're at
about 10 seconds now that'll take us to
realize the machine is down you don't we
haven't we don't want to tune that down
too far because we start getting false
positives if it's you know there's some
just blip in the network or something
like that but we want to be able to
notice hey this machine hasn't hasn't
responded for 10 seconds and it's time
to time to assume it's gone so that's
that's been a challenge for us it takes
some time for load to shift to new
instances depending on you know if we
deploy new media instances they aren't
going to be doing anything for a while
until they're a bunch of new calls so if
we have all of a sudden our service
overloading we can't just employ a bunch
of new instances it's going to take a
little while for load to distribute
there and and finally if you lose the
state like if we lose a synchronization
context and it it may be more expensive
to reestablish that so when we are doing
the scale testing on our service we
tested both you know how many steady
state connections can we handle how many
clients can we handle but also what's
our rate of accepting new connections
because when an instance goes down we're
going to have a lot of reconnects and we
need to make sure we could we could
handle that spike so that's that's sort
of the the end of the first cloud is
different part of my talk
talking about so reliability deployments
and state so I'm going to I'm going to
build a little more on that state thing
and we after having this built we built
this service we launched taco about six
months ago we went public and all of a
sudden we had orders of magnitude more
users right so we've been we've been
planning for this preparing for it think
you know wondering what's going to
happen and it really went smoothly right
our Earl encode scaled up fine our
service aquel define everything was
going great right so of course if
everything's going great when you launch
your service it's time to change the
architecture so our original
architecture we didn't have our bz
server we were using reedus as the
interconnect for for our servers and if
you're not familiar with reedus i've
heard it referred to as a data structure
server people use it for a lot of
different things you can use it as a
cash you can use it for more persistent
data you can make it redundant we are
using it in a pretty simple way to
provide interconnects between our
different services so you know basically
it has built-in support for pub sub so
if you're in a taco call you'd connect
your session server your session server
would subscribe to you to pub sub
channel for everybody interested in that
call and then any changes would go over
that pub sub channel to you we'd also
keep a femoral state there and as an
example of the kind of ephemeral state
we kept I'm going to briefly introduce
user awareness so when you're in a talk
a call you've got a bar across the top
of your screen that shows who else is in
this call so people who have access to
the call but are not in the call are
grayed out you know in black and white
if somebody's in the call their pictures
in color if somebody's typing in the
call if they're texting they have a blue
halo around them while they're texting
if their mics open they have a green
halo but basically this this is state
that we need to update very frequently
like every time you start and stop
typing
so it's not something we want to put in
persistent storage but and it and it's
ephemeral right it's the clients are
constantly sending pings to the server
saying hey I'm still here and we can we
can use those to regenerate this
information if we need to so it fits
into the definition i'm using of
ephemeral state in that if we lose it
it'll get regenerated within 30 seconds
so how we implemented this using reedus
actually the hard part of implementing
this using Rita's is ok everything's
going fine we've got four clients
connected I have four for clients up
there that's that's to represent the the
user awareness state so we're aware of
all four clients you know so what
happens when a client goes away right
they lose their connection the app
crashes something happens they're no
longer there but they didn't they didn't
go down in a in a graceful way they
didn't tell us I'm no longer there well
this is pretty easy right they're
connected to one of our session service
and it can realize hey this client is
gone I haven't heard from them for a
while where the connection went down so
this session server can tell reedus or
update the data and read us take this
guy out and then tell all the other
session servers who are interested in
this hey this change there's new there's
new information go go tell your other
clients so that's pretty easy it's it
gets a little harder when the actual
session server goes down right so now
these two clients are no longer
connected to our service but who's
watching this who notices that this
session server go down well right it's
not an impossible problem we've got a
bunch of other session servers but so
one of these guys will notice hey he's
gone and we'll pick this up so not a
real hard problem but she wouldn't it be
much easier if we had a gen server that
was responsible for this so that instead
of having some distributed protocol
where any one of many processes
distributed throughout our application
needs to be watching for this kind of
failure and do this kind of fix up on
our state so that we can we can get rid
of the lost clients you know we really
will after doing a lot of our line
programming realized you know this fits
the actor model we really want an actor
who's responsible for this state
and this is actually a fairly fairly
easy problem to solve but it was also an
easy problem to explain there were a
number of other sort of related but a
little more complex problems that were
hard to do in a distributed manner right
so using reedus and read us you know it
was a data server so it had the state
but it didn't have code so it really fit
in that sort of more stateless model
where where your that your state is
separated from your code it made it hard
for us to do this so you know we were
doing things like well you've got lots
of systems that maybe are updating the
state at the same time so you need a
transaction and taking out a transaction
on a distributed system you're blocking
the connection and read as well it's
going on you know so a bunch of these
issues and we said we can really
simplify our code if we were to have an
actor so that was that was what drove us
to say we're going to use more Erlang
we're going to replace read a server
with what we what we call the bz server
so we replaced it with an erlang server
it has a gin server / sharaab shared
objects so there's a gin server for the
call it's sitting there watching and it
wakes up on a timer and says oh is there
is there a client that hasn't pinged in
in a while and if so will automatically
remove them so that that made that made
that easy for us and you know that that
example of how do we realize that that
one of our servers crashed and who's
responsible for doing the cleanup now we
had somebody who's responsible for doing
the cleanup and the surprising thing
here so when we were cutting over
obviously we had to keep our service up
while we were making this big
architectural change so for a while we
were running both architectures
effectively we had the Rida server up
and as we started moving functionality
over to our lying server we were
exercising both paths so our data would
come through it would come through
reedus and it would come through Erlang
and we would actually get the data fast
or consistently through the airline
server than the ritas server this this
was surprising to me because you know
reedus is single threaded c code that's
supposedly really fast network so yay
Erlang I'm not sure it's a fair
comparison I I didn't actually dig
deeper into it it could be that the
Erlang reedus client library is kind of
slow and it really wasn't the server but
it was a good sign that that the Erlang
served was was was faster so that's
that's that's the end of that section
the talk so these ideas of state
reliability durability is also something
that Amazon is thinking a lot about and
one of I think the interesting things
that are doing and coming coming out
with is this service called AWS lambda
it's not a service we use and it's not
one that I think is going to solve
problems for us so I'm not interested in
it in in the practical sense at this
time although I do see some applications
for it but I think it's interesting from
the sense of where's the cloud going and
the idea behind lambda is you write a
function so you write a JavaScript
function and you send it to Amazon and
you tell them when they should run it so
maybe if a new object gets uploaded to
your s3 bucket hook it up to your lambda
function your lambda function will run
and whatever output from it will you
know get stored wherever you want to
store that output so you could use it
for audio transcoding you know anytime
somebody uploads a new audio file run my
function i'm going to transcode it I'll
store back in s3 but they're lots of
other things you could theoretically do
with it if you're able to you know boil
your code down into a stateless function
then you don't have to worry about any
of the things I've been talking about
right you don't have to worry about
reliability and deployability because
because they'll handle it for you so so
I think it I think it's interesting to
sort of think about you know what is
this enable from a service standpoint
right as somebody who's responsible for
operations I would love to be able to
write a service without having to worry
about the actual instances so I have a
question before I let you guys start
with questions one thing we haven't used
is distributed Erlang we haven't used
distributor lang
cloud I don't know if we should use it
it's something I ponder if this would
have made things easier for us we were
concerned about it when we started with
Erlang you know coming new to the
language we didn't want to we went to
hedge our bets a little bit on on how
much how easy would be just for us to
switch to other technology so we didn't
want to go here but we had other
concerns that you know certainly if you
know would distribute Erlang work well
across availability zones or as we scale
out to multiple regions how would it
work across regions we you know as I
talked about we're really sensitive to
how quickly we can detect failures in
our networking stack and we didn't know
would distribute Erlang work well enough
for us there plus we were worried about
the full interconnection and how that
would scale so i wish i could say
something really interesting about this
because i think it's an interesting
topic but i have no experience so if you
do i would be interested in hearing
about it and that went reasonably
quickly so hopefully you have a lot of
good questions yeah
yeah so the question is do we do
anything like chaos monkey so probably
most of you are aware of it chaos monkey
is a netflix originated project that
will randomly kill your kill your
instances no we don't it's not something
we've had time to set up I think it's I
think it's valuable so you know we do it
during our deployment process and
particularly so what I described as sort
of our ideal steady state deployment
process certainly in our development
environment we're often much more brutal
about what we do and we certainly done
it by hand a number of times where we'll
kill an instance and make sure it
recovers but we have not automated that
limited time not I think it would be
valuable in the back yeah you yes you
um
we looked at it enough i mean i think
the other concern we had was around
security too we dug into it a little bit
but not to the point where I felt feel
like I can answer those questions so you
know I didn't look at what are the
algorithms for how it detects when a
node goes down for example which i think
would would be really interesting to
know I dunno so in Fred's book and learn
you some Erlang he mentions and about
distributed or lying that it tends to
assume that failures are hardware
failures as opposed to net splits which
is something I'd have a little bit of
concern within the cloud I think you're
much more likely in the cloud you know
to have a net split between your azs or
some other things like that but I don't
have any I don't know what the code
looks like and I don't know what the
algorithms are so I'm speaking from from
other people's experience no Dimitri
so I don't I don't want to speak to sort
of exact numbers it's relatively small
and I think you know are you know I'll
say so our session service for example
we've we've load tested and scale tested
and it's certainly that a single server
for us on a moderate-sized ec2 instance
can handle hundreds of thousands of
connections of clients so you know we're
we're a relatively new startup we're not
a huge scale yet and our Lang our Erlang
instances handle a ton of stuff so we're
not running a ton of instances does yeah
I don't know if that you know in terms
of actual yeah I don't have good numbers
on traffic or breakdown between real
time and you know photo uploads and
other things I don't have that data
sorry it depends on me so we're a small
team so it depends what we're working on
we've certainly on a bet on a bad day
when we deploy some bugs will do 12
deploys a day just to fix things I mean
hopefully hopefully that's in our
development environment in no no
patching it takes us about from code
check in to having that code running a
new instances in our development
environment is about five minutes so
it's it's a pretty quick turnaround what
yeah yeah I mean a couple things there
so the way we do it is we most of what
we have is baked into our am I so all
our dependencies are baked into the the
image that we're booting and then we
down our build we actually store an s3
so we just tore up all the Erlang stuff
pull that down expand that and get it
running so it boots up pretty fast our
times are a lot faster since we switch
to
the new EBS volumes that are SSD based
on those those boot up a lot faster than
and actually we made that switch at the
same time we switched to VPC so one of
those switches resulted in us booting up
really faster i'm not i'm not sure which
one i'm guessing it's the drives i will
say a little digression that you have to
be pretty careful with the gp2 drives
the one outage we've had has been caused
by that but basically basically what
happens is the GP to drive so it's SSD
baked Beth based backed EBS so off
offense and storage and they have a
quota on how many operations you can do
if you accidentally hit that quota
things start behaving really weird and
there's not even a good at this point
they're going to add one but there's not
even a good metric that shows you you've
hit your quota what happened to us was
we were switching to use a different log
host you know upload our logs to it to a
different host and we switched we
accidentally you know we were in so we
were switching how we were collecting
logs we accidentally were started
putting those to EBS instead of two
instance store and after a while of our
logs going to EBS we hit that throughput
and what happened at that point was that
any Erlang process that tried to log
would hang for an indeterminate period
of time but any earling process that was
not logging including the ones we were
using to heartbeat and make sure
everything was running where would
continue to run fine so it was it was an
interesting failure mode so anyway if
you if you do those drives boot really
fast be careful if you're using the unit
you're not writing too much to us
yeah yeah I mean well so we have a
dialyzer that that's surprising I mean
if we build a lot into the PLT and then
our dialyzer run is is 10 seconds 15
seconds so I know that's not a single
instance another question okay so
routings interesting let me let me bring
up the there's not an easy answer and it
depends on which server so this should
let me actually so there are three
different service so our session servers
effectively what the clients will do is
always try to connect to the last one
they connected to if that doesn't work
they actually go through our elastic
load balancer with an HTTP request and
then we have code which which picks a
session server for them so they'll then
reconnect directly to that session
server if a me is you know effectively
when you enter a call you're telling the
session server I just entered this that
call on that session server will pick a
media server for you and assign you to
that media server the bz servers which
are the interconnects that's that's a
little more complicated that that's a
hash ring with consistent hashing that
that assigns objects to individual
servers and that in that group
yeah I mean so basically the you know we
thought the advantages here of doing new
deployments you know making sure we
always knew what state are instances are
in the fact that we needed to to to be
able to deploy quickly anyway were
outweighed the outweighed the downsides
and I you know in particular I think hot
patching is something that I looked at
and thought that looks really
complicated to get right to write a
bunch of code that's going to correctly
mutate the state you know it's hard
enough to get the rest of your code
right without worrying about how to you
know we already have to do enough
backwards compatibility between our
different services that that trying to
get backwards compatibility right within
a service was something we didn't want
to when didn't want to bite off so we
you know we it's useful sometimes when
we're debugging something we want to
change the code we're running or in the
development process it's you know will
load a new version of the model but
we've of the module but we've never done
a app upgrade right now we have one
region so it's it's an easy answer it's
something we've looked into so you know
I think the way we'll do it is you can
use route 53 the you know the Amazon DNS
you can use to do G location aware
routing so it will decide what region is
closest of the ones you support so you
know I think we're going to take
advantage of that feature and use that
to to decide where to send clients but
not something that we've dealt with yet
the penis what I understood was use the
piece of risk are you yeah so the
questions around the busy server and
kind of what exactly that's doing so
it's not exactly watching the other
restarting the other servers but it's
noticing when clients chain you know
state changes when different clients
connect and what's going on so actually
restarting the the server's happens
through the auto scale group in in
Amazon if the server crashes agree thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>