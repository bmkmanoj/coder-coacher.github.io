<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>From NoSQL to Mo' SQL - Adding Structure and Queriability to Riak by Gordon Guthrie | Coder Coacher - Coaching Coders</title><meta content="From NoSQL to Mo' SQL - Adding Structure and Queriability to Riak by Gordon Guthrie - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>From NoSQL to Mo' SQL - Adding Structure and Queriability to Riak by Gordon Guthrie</b></h2><h5 class="post__date">2016-09-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uN0GIPWiRYo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I've saying I'm going to be talking
really about using query-based language
to sit on top of a key value store and
understand getting getting data out and
manipulating in the normal way although
I'm talking I'm talking about work
that's happening on the react I'm
serious system lots of people have
there's the core teams quite small but a
lot of people done some work so it's not
just not just me okay so sequel and no
sequel a match made in hell putting a
sequel query language you ask why would
you never why do people tend not to put
sequel in front of key value stores and
the reason is there is actually a very
solidly supported dialect of sequel that
you can use in key value stores and it's
kind of a Model T version of sequel
because you can write any query you like
as long as it looks like that select key
and value from table where key equals
somebody of a key so it there's a reason
why no sequel is has no has no ability
because typically in a key value store
you're trying to solve a totally
different problem you're trying to solve
problems of availability and living with
partitions and you're not trying to
solve the problem of actually
manipulating the data so you tend to use
a key value store is a fairly dumb
object that you put in a fairly dumb
store that you put in objects that may
have some structure to you and you then
deal with them at the application layer
because it's about talk you're obliged
to have a picture of the ring but the
problem with that the baasha ring is
it's a pain in the neck in slide so my
ring is flat so i thought either you're
all good scientist you can all
understand and conceptualize that rings
may not be round but i'll give you a
picture of Saturn's rings anyway what
I'm going to do is I'm going to talk you
through some of the aspects of the
problem that we're trying to solve
trying to get away from where we need we
need to use the underlying properties of
react to give us the reliability and the
availability that we need and then try
and find ways to then put data in that
in that
structure in a way that allows us to
move the queering and data manipulation
from the application into the into the
database so the classic it's a classic
engineering problem you have you offer a
service to layer you look at what people
are building above that layer they build
all sorts of complicated things and then
you say actually if we take that work
that everyone is reinventing and push it
down below that layer then we increase
the utility of our surface customers and
time-series just kind of kind of part of
that world so this is my poor man's
version of the reactor Inge our kv store
typically works you store more than one
copy of data and we use a we use a hash
function to get there we have a key at
the bottom we push that into the hash
function this is a pretend ring of 12
and it resolves to a value 3 and then I
right Alice to a logical position in
this ring and I write another couple of
issues so I've now got three copies of
my data stored on my ring and I can do
the same thing with Bob and I get three
versions of Bob and I do the same with
Kate and you can see it's a ring now
because my ring has rolled back the
beginning so you were all able to
conceptualize that this this ring is
circular and it's real back so so that I
mean there's your fundamental issue of
how I keep the you nokia value stole
works so what we're trying to do then is
and in this world we have a key that we
put into our partition function and we
have a key that we write on disk and
these keys are the same we use the same
key to hash that we do do to refer so
the key refers directly to the value
that we've got in there so what we do
and this ring is then mapped onto fiscal
nodes this is a 12 node ring it may be
mapped onto a number of servers and the
servers represent the different logical
data things are then mapped endren so
typically you may have a big
installation with 1024 virtual nodes and
maybe 150 physical servers so that's how
you get your high availability and your
linear scalability you get this sort of
logical physical physical mapping and if
you if you get the numbers right then
you your guarantee you guarantee
that the three versions are written on
three separate machines there's a whole
whole world of complication around that
that's kind of not really that relevant
so the fundamental problem with that
model is that you only have effectively
to data access nodes for clearing if you
know what the key is you can go straight
to typically a typical installation you
might write three values of data to
react and then you may read to back
check that they're the same so I'm going
to pretend here that we read two values
and check that they're available so if I
have a key I can then go to two of my
logical vinos which is most cases to
physical servers ask them a question I
get some data back if I don't have the
key then I have to ask the entire ring
because I'm writing three times that
typically means I visit every third
vinodh but if I have a big cluster with
a thousand 24v nodes on 150 machines
that actually means I have to visit
every machine some for a single points
on for a multiple points so I either say
I get the answer or I have to ask the
ring for everything and these big jobs
will you clear the entire ring or a
MapReduce job and they're very very
expensive so you have this to costs of
cheap to get one expensive to get many
and what we're trying to do is infill
between those two those two note those
to query modes so that I can get more
and richer information by not doing as
little as talking to 1v node and not
doing as much as talking to thin tiring
and also if I'm am querying the ring
it's not just going to the physical
servers because when I go to physical
server with a key I also go to a logical
point in my database and get the data
back if I'm asking questions about the
entire ring I'm actually scanning the
key space on every vinodh as well so but
I have more journeys but I have more
work to do at the far end so the cost of
actually doing full query mode is really
really expensive so you have to try and
trade off those two things so um with
with so you know if you have a key to
talk to servers if you don't you must
talk to every server and you must
traverse the entire key space
so what we do with time series is we
first of all we break this link between
the hash key and the local key we still
need a unique local key so that let me
write data its to a unique location and
can be referred back and the typical
thing we first do with with time series
then is um is that one of the one of the
one of the reasons we're doing this for
time series as opposed to general
queryable data at the moment is because
time series is a is a common use case
that has a a fixed data pattern so
you're actually choosing a subset of
data models to implement query ability
for a subset of all data models that
people normally use and those data
models are typically writing data it's
the same thing but moving in time so
what is the you know what is the
temperature of this industrial unit now
and in one minute in two minutes and
three minutes and my blast furnace
reports is temperature every 15 seconds
I then have a history of that machines
operating temperature or a scale
electrics out there reporting its lap
time or a traffic sensor reporting that
it's it's available or signal strength
on a base station there's a whole number
of use cases that represent the same
information traveling in time and so we
effectively looking at query ability for
that data set we're implementing that
for a customer use case but we're trying
to design for a bigger set of world so
in this world what we do is we take we
take a two-part key which has an
identifier and a time component and then
we perform a quantization on the time
and we use that when we feed that into
our hash function we get basically here
I'm saying quantize my time in 10 second
chance so I get a series of readings
that come rolling in and for that
ten-second window I always write that
date at the same place and then I write
the next bit to another place and I
write the next bit of third place so I
now have
collections of related data and they're
related by time slices and they're
written in the same physical place so if
I talked to two servers I can then query
if I've got a 10 second slice I can then
query data for 10 seconds or now I make
the same journey that data is located
next to each other on this so I have a
shorter virtual time and I'm basically
altering the collocation of data to get
to get more richness and I can I can as
I span more and more quanta I linearly
increase the amount of visits have to
make so one cointreau go to vinos to
contra go 2 4 6 8 10 and eventually I
still have to talk to all service so
what I'm trying to do though is it's in
that space between do everything and do
a very small amount I'm trying to
increase the the range of query query
options I've got at the other cheap end
and put some data in there now you are
making trade-offs here because one of
the ways in which a key value store is
highly available is that the hash
function distribute work very very
evenly across your you 150 nodes and
once you start quantizing you start
effectively making your data right
patterns more lumpy and if you get it it
is possible to sit and set up a time
tues database where you quantize
everything to write it on one v node and
then you can access it all but then you
have 150 servers and one of them is busy
and the other ones are just sitting
there so there is a trade-off between
between how I distribute work and how I
get access to my online well we have
spindles anymore do we SSTs but you
there is a trade-off between localizing
the data and and lumpiness in your
cluster and so effectively we're using
time as a mechanism to collocate data
and then having collocated the data we
then have it in one place and it becomes
effective to write query queryable
language to access and operate on that
data
so I say yeah did you have to trade off
the quanta versus the lumpiness okay so
i can we improve that one of the what
one of the other things we're trying to
look at so so we now have a query singer
and i'll talk about the language in a
minute but we still have this this to
access pattern we still have effectively
that the same problem you still need to
know you need to know enough information
to go and get a contest you need to know
what identify am I looking at you need
as a whole does a whole range of stuff
that utility throw and we at the moment
we have inside normal kv we have a thing
called to I indexes where we effectively
have a key space in our ring and then we
have a separate space for an index and
when we write to a key we write to look
up index in the index base and we can
then query that it we can query those
indexes to say I'm you know I'm writing
this is classic as the classic index
approached you you write it under a
primary key and then you have a second
field that your index on one and then
you can say tell me the names of all the
master keys but also matches the index
and that that does work and it gives us
a number of issues but to that the
problem is that we write the key in the
index we write the index value on the
same physical vinos as the key and it's
under a single transaction so if I wish
to get a list of all index lookups I I
have to do a sorry if I wish to get a
list of all the indexes I still have to
go and visit the whole ring I have to go
around the ring and say to every vinodh
can you tell me all the keys that you
have them match the index so I still
have it still it gives me to access
roads into the data but there is still
that there's still an expensive cost of
running in index so and with mobile talk
when I talk about future work we're
talking about ways can we reduce the
cognitive load can reduce the big
cluster load of trying to get index
information so that we can look up data
we've all seen along the movie now we're
going to have the sequel yes I get to
make bad jokes so why sequel the reason
for using standard sequel as a query
language is because everybody knows it
there's a low barrier to entry you can
we just saw further bear doing a
presentation about why we made the right
choice you standard sequels are query
language the barrier to entry for new
developer is really low you can sit down
you can play with the show you can type
stuff in everyone knows how it works
then the secondary advantage is that
there is good tooling so people already
have things that emit and I'm run sequel
so you don't need to worry about that
and sequel is actually a fairly well
Detert so you know it's a well-designed
decorative language with some key and
obvious routes to extend it in a way
that doesn't break it so it makes a lot
of sense to just just try and use very
very standard sequel and it's a Henry
here was saying how I'm a tight Scotsman
all you will know that we love in
Scotland not invented here we love when
someone else has done the work for us we
love not paying for it having someone in
design and complicated language there's
lots people who would say oh great let's
design our own query language that'd be
cool we can spend a year inventing
something that's vaguely sequel ish that
you can kind of looks like but uses
semicolons commas instead of it or
whatever I don't like any of that stuff
somebody's done this good job let's
let's keep moving so um there are good
but sound business reasons to use um use
sequel so I'm going to give you a very
high level architecture of how it fits
into react react is a react is a
complicated Beast at the core of the
react architecture is the is the react
call library which is a library that
manages clusters it does gossiping it
moves information about allows a cluster
to share state and do share
configuration and generally operate as a
as a loose cluster and react kv then is
an implementation of a react core
application it sits underneath react
core and it performs
call key value operations and then at
the very bottom you have a number of
physical instances of leveldb you have
one per physical server and they are C++
code they just accept data in which they
then persisted this can read back what
we did with time series is we tried as
much as possible to not not screw up
that architecture so the the react query
language really sits as a set of
libraries that floats over kV we have to
change kayvyun rack or a bit because in
kv that the key that we hash on and the
key that we write two are identical
whereas in time series they're different
so there's some plumbing there but
substantially time series is a library
on top of react kv and there are more
complications that it's not as easy as
it seems there's I'm because a lot of
things turn out to require to understand
the function of your keys so anti
entropy and things but it's very as much
as we can we try and keep those separate
so we honor that kV stores are literally
very very dumb they don't know anything
about values and but we need to teach
react if we're putting structured data
in in the value onto the disk we need to
have a mechanism to teach the cluster
about the structure of that data so we
we we define structured data with the
data query language it's a create table
function it's not strict sequel and
you'll notice of some things a bit odd
about it but it looks like a
recognizable I'm creating a table
statement it has a number of fields the
fields have tight the name fields they
have types they can be no law not null
the types of you know 64 integer double
time stamp boolean and um I sort of our
child binary one and we have a key
structure that tells us that a two-part
key one says how to partition it and the
other says what to write on disk so you
have a two-part key and what we do with
that is we we have a leak check
pass a compiler structure we pushed that
statement through and it emits an erlang
date structure which is the record
describes a table we distribute that
data structure around the cluster with
react core as we act calm f data so the
entire cluster now understands that a
table has been created and it has this
structure has these fields and then we
use Richard Carson's mill to generate a
dynamic module that can D structure the
data that we're writing so we then load
that module in and it's it's the module
name has the table name in it and so now
magically our cluster now has loaded
into it programmatic Erlang that candy
structure data without the app without
react itself having to change it has a
simple call point and it knows that
depending what table it was it can turn
the table name into a module name and
then call the api on that to get
fieldset keys generate keys and pick out
the fourth field check types all the
information that the that is needed
throughout the cluster is now available
programmatically so we try and make you
know we actually teach react about the
data that we've defined um and we have a
we have a distribute query system so you
you connect to what we call the
coordinator that's whatever the I mean
react is a is a is a is typically
organized as like an agnostic cluster
any any machine can generally no machine
has a special function sometimes for
certain things on react call what one
machine is nominated as that as the
coordinator but for the query system you
can connect to any machine in react
cluster and that machine will run the
query for you and act as a cool note for
that query so when I create a new query
my coordinator will run in init phase it
will take the query and it will exit or
pass it'll validate it will do whatever
requires check that it's a valid query
it will then ask it works out where the
data is based on upon the key that
you're doing it sends its requests off
to some nodes which may be local or
maybe remote they will read some data
maybe process it return it back to
coordinator which will do a finalizing
process of it and return it
the customer so it's very generic
dispute system and I'm now going to talk
you through a bit of that pipeline again
there's nothing there's nothing a final
fantasy in the in the sequel processing
pipeline in this case obviously the
pipeline varies if it's a create table
queries different from a select query
but this is a select query we typically
so many requests sends a query to us in
sequel we then compile it into an
internal destruction and we have to a
series of validation checks we have to
say does the query refer to a table
that's actually in react because if it
doesn't you won't get an answer and if
it does to the fields in the query match
the definition of the table and so we
first we'll check that we use this
function this module to help a module
that we've generated up here we can use
that that has all the functions required
you can say does this field exist in
this table yes or no and if you get an
error you chuck help we then say have
you given us enough information because
we still need to know we're in the ring
the data is so have you given is enough
information to send out our sub queries
to get the data so it's a valid time
series where clause and again we use the
help function for that may be returned
error and then we have to go away and
say if it's a calculation function the
calculations are all typed you can't
take the average of a VAR car so we have
says it tight is is a types of query is
it going to calculate correctly and
again we have an hour and then finally
we have a we validate that we have a
complete query and we then send it off
to a query writer which will actually
execute some xq and run our query and
what we're doing is we're using we using
the decorative properties of sequel to
generate sub-plan queries in sequel so
if we have here a typical time series
query which is a Select Clause with a
funk with a function from a table with a
key that has time quantization and we
take that function
and we typically unroll it into multiple
sub-forums which which roll back so let
me let me talk you through how these
things work my coordinator is going to
send out over the network to two
separate V nodes to separate queries I
have I haven't hear a time period which
crosses two contr so I'm sending I know
when my quanta barriers are so I'm going
to 1v node and saying right give me the
piece of data that you understand that
matches this clause i'm saying i want
the average here so i'm asking you to
some the temperature here and count it
and i'll use those two figures you're
going to give me two figures back which
i'm linking a process here i'm going to
turn that into a computational average
so I've transformed my average function
and I'm going to repeat that twice so
this this says go from the start point
to my quantum boundary and this says go
from my quantum boundary to my end point
so I have a I have a time slice and I've
distribute to my work and to make it
easy to understand and make it easy to
debug we we use effectively we take a a
decorative sequel statement which has
sequel syntax and we rewrite it into our
own runtime statements that have the
same structure they have the same clause
of the same names and they we transform
the syntax to make it executable but we
retain the semantics so it's always pure
it's always pure sequel and typically
what we do is we express all these
clauses with example as what we call in
the trade ya zylle yet another shitty
list we knock out little lisps and we
then have executing things that run
those lisps against data sources at
various places so decorative sequel
which maybe have execution hints and we
turn that into a query plan which is a
series of executable fragments with a
transformative attacks preserve
semantics so if you if you look at if
you look at sequel clauses they all have
they have a very common
and standard signature that operates and
if you imagine that your data on disk is
written it has a series of rows with
values and it has a column description
for each row which has a name and a type
and you typically take a pattern like
this and you operate on that and that
normally obviously starts on disk but
and the sequel query will then return to
you a series of values in columns with a
column name and a type and it will have
transformed it may have increased the
number of columns or it may reduce them
it may have increased the number of rows
it may have reduced it it may have
transformed the types it may have better
but all of them perform to this this
generic this generic structure so what
we're then able to do is is we have an
executor that runs and execute all these
lisps and they all have slightly
different behaviors but we're not quite
here yet but we're moving towards a
world in which we have this distributed
executor that you can just pass in
fragments and it will take the same
thing it takes it takes a tight and
named columns and the set of rows and it
emits type to name columns and a set of
rows are very common structure standard
signature that's basically what we're
trying to trying to reduce all this
process and complexity and you know
select is a row and column operator as
only operates on column type so there's
so there are row operations there are
column and row operations or header
operations but they all have broccoli
the same the same approach so so not
only reusing sequel as a as a way for
end users to understand how the system
works we're using sequel as a way for
developers internally to understand how
how the operation works as well and the
the executable fragments are as I said
shitty Lisp so they tend to be a to pool
what sort of thing it is and then a list
of operations that you operate on it so
that's an and operation with some
operators and you then have a runtime
that takes that and you pass in you
passing your your rows and columns and
then it no
how to just run that execute that
against that so lots of you as walls
which are again we're trying to smooth
our users out made them more
interoperable so um how much sequel do
we have we have select well we have a
group by order by and limit I've been
worked on for the next release and we
have a number of functions average mean
max some counts than the deviation so
you can now start putting data in you
can start having structured keys and you
can start building quite sophisticated
stores of data that you can you can then
all prey on I'm along about 10 minutes
okay cool um and I'm talking about
sequel I'm saying it's a decorative
language but it has execution hints and
what do I mean by execution hints so
straightforward sequel that's decorative
it says get me just data from my table
if I start doing things like limit then
limit kind of looks like a hint on what
you how you should execute it give me
the first thousand first or whatever and
we can extend that by putting by adding
our own custom clauses on the end so if
we have to do things like harvest and
yield and won't specify how we would
like the query to execute what degree of
consistency the data we want comes back
we can simply extend the end of our
query language by adding custom clauses
and typically what you do with that to
make it operational easy for tooling is
that your standard sequel would work in
tools and then the execution hence you
want you would normally set its table
default so you instead of you can just
use normal tooling with it and you can
say on this table I expect you in normal
operation to run it in this style if you
need to overload that and replace the
execution hence then you can do that if
you have higher consistency or low
consistency requirement okay so what
does the future look like obviously I
wasn't allowed to come here on my flying
unicorn and park it outside I had to
come by plane but everyone else but
we're trying to look at a range of
interesting things because the we're
using the time structure of data as a
way to collocate it to give the
this wider range of queries and we're
looking at other mechanisms that we can
use to collocate data and come we then
use the same query structure on top of
that data so the other sort of
collocated data we have is crdt sets
that looks like allocation asset is a a
letter set you put a number of things in
it and it the way they're currently
written in in react is that you are you
have a key that points the set and then
you add and remove elements from that
set and those are executed as at the
moment there they're written onto the
disk as a binary store and then read off
the disk and there's a library that
operates on it in situ so this obviously
has performance issues if I have a if I
have a crdt set with a hundred thousand
elements written to disk at the moment
to do reconciliation whats ER dt's will
do is they will send that entire set to
replicas and they will then do merge
operations on the two sets and they will
to actually add an element or removing
elements that I have to read a hundred
thousands from the disk operate on the
set in memory and write the new version
back so I have a lot of i/o and I have a
lot of network costs and Russell be
talking tomorrow about the work he's
been doing to break that up where the
set is written to a disk and you can
read and write an individual element and
you can you move from state based ops on
CL DTS where the whole object has to be
swell trying to delta based where you
can send a causal fragment this is I
have done this operation on it can you
do that same operation under and you'll
be presenting the work of the tremendous
transformation of that in time in speed
so if we have if we have performance
sets with big sets then we can we can
treat them as um we can start treating
sets as queryable things and we've done
a couple of experiments using existing
crdt libraries which give us non
performance but indicative and I'll we
can show that the mechanism works but it
won't perform for performant for the
customers until we get the big sets in
effectively what we have at the moment
is we have a table schema which is a
series of column definitions and types
and then we have sets of rows those are
are our core data elements inside inside
time series if we use so if we stop
treating crd tease I kind of think crdt
is are going to be like OTP which no
longer stands for open telephony
platform it just means something see
oddities are converging replicated data
types and when you look at them in this
way they stop being data types and they
become mechanisms managing causality
with data underneath it so i think the
dt of CR DTS will just become letters
but if you look at it like this you can
represent you can use crdt maps as a
mechanism for expressing a data schema
and you can use sets of maps as ways of
calculating of handling rows and rows of
data and you can use the sets to
represent that but what's quite
interesting that's very boring um what's
quite interesting about about the crdt
work is when you actually look at the
data that you can model with maps and
says is you can start modeling table
joins you can start looking at much much
richer data structures are eventually
consistent and it turns out there's
quite a lot of application complexity
that now becomes possibly eventually
consistent and you can start using maps
that contain sets that contain maps that
contain sets that contain maps contain
sets to represent multiple table joins
um and at that stage then we think that
using that the sort of time series query
the time series crude language stops
being about time series and it is
designed as an as an architectural query
approach to allow queries to be run
against color code to date and if we can
get the next phases in then we think we
can really move that query language
along in a way that takes us just out
the time space as we said that the
is this his future stuff is not a
company there's not a company roadmap
right this is this is as attempting to
understand what that world looked like
um I've got five minutes left so you
still have you still have the same
problems you either talk to all servers
or you talk to two servers you still
have a data access problem and one of
the things we're thinking about can we
steal some that causality information
that is that is used in delta x crd
tease and use that to build eventually
consistent indexes and that's to say
that that's a research project is
overstating it I have a feeling in my
stomach that it can be done but I have
got no evidence that that's not anything
but but I'm you know I
understand the laws of physics I think
that can work um so we kind of look I
was told I was told about that ending
attending a presentation with us with
the word dunno is probably her it's
probably a is not I I suppose the
difference between Americans and
Scottish people isn't America's oh I end
up with rah rah rah rah and in Scotland
up but no but yes there's quite a lot of
interesting stuff about can we broaden
out this period path can we solve this
problem if all on the the kv problem of
all or nothing querying is what we're
really trying to break down can we have
a bit a bit more than nothing and a lot
less than all this what we'd like to go
any questions
it depends on what they are they're
mixture of at the moment more happens
that the coordinates then we'd like and
we as we each each iteration we're
pushing work to the edge as much as we
can so some some operations we do the
work laws at the edge and then we do
most the other stuff and then they're
coordinator but each release we go
through we we optimize that work to
eliminate it we're trying to push as
much the operations the various as
possible no we don't we don't do that
the data structured so you understand it
so you can access it through sequel we
don't we don't let you run no no oh yeah
oh sorry the question was is it possible
to define your own functions and then
apply them roll up and execute them at
the v node which is not not though yet
it's something we have thought about but
remember i am any other question
well I mean Ross will be talking about
mostest tomorrow that's I'm kind of not
going to pretend that I'm a great expert
on that side of things but but by moving
by breaking up the crdt into individual
components you get um you your date
rights as partners much less and you you
I said I don't know how the AE works um
i have read see Shawn's paper on it but
you probably need to ask rustling I than
that but the performance I mean the
initial performs indicate is a really
good on their the big set stuff is a lot
quicker than the monolithic says
tomorrow on the big stage Russell's got
older they want to steal his glory</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>