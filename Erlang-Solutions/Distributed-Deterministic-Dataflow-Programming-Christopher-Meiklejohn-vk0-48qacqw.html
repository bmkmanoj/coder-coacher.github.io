<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Distributed Deterministic Dataflow Programming -  Christopher Meiklejohn | Coder Coacher - Coaching Coders</title><meta content="Distributed Deterministic Dataflow Programming -  Christopher Meiklejohn - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Distributed Deterministic Dataflow Programming -  Christopher Meiklejohn</b></h2><h5 class="post__date">2014-07-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vk0-48qacqw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">cool um so did I mess up this mike now I
think it's fine um so I'm Chris Mikkel
John I'm an engineer at bosch o
technologies where I work on a bunch of
react stuff multi data center
replication primarily right now so you
can come talk to me about that in
addition to that I also in grad school
at Georgia Tech where I do some computer
science which is fun so today we're
going to talk about distributed
deterministic dataflow programming for
Erlang and this research is kind of a
joint research project here with peter
van roy and two of his grad students at
the University and Livan lative in
Belgium Belgium and me at bachelor
technologies and this is very much a
work in progress support we do have a
prototype that we're kicking around but
there's a bigger goal about what we're
trying to build here and we'll kind of
talk about what the motivations are and
give a kind of an overview of where we
currently stand with the research that
we're working on so we got the will
breakdown of how the talk is going to go
here so we have a just we're going to go
through the introduction why we're
looking into this research why we think
this is important we'll provide some
brief background information on the
programming model we're going to talk
about we'll look at the semantics of the
programming language and then the API
that are our prototype exposes we'll
look at one example there's a lot in
here so I put one example but the code
base is online not the most up-to-date
version with this slide which I will do
I didn't get a chance to do because I
was fixing the slides and then I will
talk about some caveats and and what the
final goal of this research is because
this is like I said part of a bigger
work that we're doing in this research
project so um so bash oh um in addition
to the industry partners rovio and try
fork which I'm sure you've all heard of
in partnership with the University a
series of universities under the seventh
framework programme is this project
called sync free so here you see the
universities we have inria which is a
mark Shapiro's group University and
Lisbon which is where Carlos and nuno
are and then a peter van royce group and
lavon and then some researcher
researchers at Koch University and
Kaiser Sloan so with this project we're
working on here is called sync free it's
focusing on conflict Lee replicated data
types so you may have heard about crd
T's in the last talk if you're at the
database talk and you may hear about
them if you go to the react talk which
is either happening next or the one
after that and what's your duties are
are basically these data structures that
guarantee convergence on a value during
parrotlet concurrent operations so there
are data structures that should be very
similar integer like kind of counting
integers sets maps or something similar
to the Erlang map or a map in JSON their
data structures that regardless of the
order that you apply the operations in
guarantee that they come out with the
correct value so if you imagine if you
imagine a counter you know if you
imagine a set and you know you're doing
some add and remove operations if we
remove tries to remove an element before
you observe the addition into that said
it's not the same as if you reverse
those operations so there's plenty of
talks on CRT T's I've given some fairly
I've given a lot of them some of them
are more advanced and talk about the
implementation we're not going to talk
about that today but that's what the
goal of this research project is and
it's essentially trying to do
large-scale synchronization free
computation so we want to do distributed
computation over a bunch of values a
bunch of databases a bunch of servers
whatever it may be similar to something
like react or something like that and we
want to ensure that we don't have
coordination we want to guarantee the
correct result without coordination so
we don't want to use something like
zookeeper we don't want to rely on
unpack
or any of those things reft we're trying
to get around that and this is why these
crd tees are important so part of this
project that we're working on is to
build a programming model that we can
use for the CR eighties so we want to
have a language that we can express a
computation in and we want this language
to use these lattice variables these CR
DTS that we're going to talk about to
build these programs and ensure that no
matter the execution regardless of
schedule or execution regardless of
processing failures across nodes and
clusters of nodes 1 sure that we always
get the correct result so this is where
we get this name of the thing that we're
working on this deterministic
distributed parallel programming in
Erlang if you're familiar with some of
the academic literature that's in the
same kind of area there's two projects
and these slides are up and the
citations are available so you can look
at them two projects l VARs which is a
deterministic multi-threaded parallel
computation a library for haskell which
was written by Lindsay Cooper at Indiana
University and bloom and bloom is a
disorderly programming language that
allows you to do computations over
values and uses a subset of data log and
it allows you to do non determinism as
well and then it can do analysis using a
logic over your program to determine
where coordination is needed and tell
you hey there's this one spot where
coordinations required so put some
zookeeper in there rather than wrapping
everything into keeper rather than doing
every operation serialized through a
single node that acts as a coordinator
and both of these works are very
interesting bloom is implement a pers
focus on the theory behind it and the
logic around proving these properties
about these programs and L VARs has a
haskell library available rube bloom is
implemented as a DSL in the Ruby
programming language so there have been
attempts to port bloom to Erlang it's
their support called bloomer lang it
hasn't been maintained in a while and
it's hidden somewhere in am your curry
Ulrika somewhere at UC Berkeley so you
can look that up if you're interested in
that but I don't recommend it because it
doesn't it's not if I meant to like the
paper is whatsoever and what we're
looking at here
is we want to provide a similar
programming model to l VAR is in bloom
but we want it to be distributed so we
want it to run across node we want it to
run across no transparently across nodes
transparently in a cluster machines we
want it to be fault tolerant we want the
computation to succeed regardless if you
know one node fails if if three nodes
fails if you're adding removing nodes at
the exact same time we want everything
to work out fine fault-tolerant
scalability and have that near linear
scalability property that we have with
things like react core so um
conflict-free replicated data types I
had to put a slide in because of
probably let me get a survey of the room
how many people have heard of crd tease
all right look that's actually an
impressive number I am very impressed so
you're probably familiar with the work
it's probably due in part to all of the
lovely bash of people I work with you
talk about them non-stop myself included
so crd tease come in two main flavors
and these two flavors are if you look at
the original literature mark Shapiro's
original paper you'll see that it's
proven that both of these models of CR
DTS are equivalent and one is an
optimization on the other that requires
different guarantees from the network
layer so we have these two layers these
two flavors we have state-based and
operation space so state-based crd T's
involve applying an update locally to a
data structure and then shipping it
around and then everybody merges their
changes in you're guaranteed to get the
right result and in analogy to make this
really easy to think of is if you have a
set that you can only insert elements
into it doesn't matter who inserts it
what when we all get the set emergent
together regardless of the order that
with that state is delivered and we all
we always get the correct value because
to merge that is a union and a union has
certain mathematical properties that
allow it to happen in any order in any
batch multiple times the operations
based is based on delivering of
operations and all of those operations
need to commute and they need to be they
need to have a reliable broadcast
channel in many of the cases they also
need to be idempotent as well so we're
going to talk about state-based er DTS
right now this is where a lot of the
work on the new programming models are
is focusing because you can easily
transfer two operations base later so
what we want to have is
again this data structure which assures
convergence in the event of concurrency
and the mathematical theory is that
power all of this stuff is it's based on
the state is actually a bounded join
semi lattice we're not going to talk
about what that is you can google it if
you'd like or you can come talk to me
later and if these lattices grow
monotonically in state and they ensure
that they accumulate all this
information about what events happen and
get the right value and so a vector
clock is an example of one of these so
the vector clock if you think of stores
a bunch of actors and how many times
they've performed in operation the
vector clock is structurally equivalent
to one of the crd teas that counts the
number that can only ever increase so if
i count every time every actor does
something independently and then i merge
all that together as a set and then i
take the maximum across those i
essentially get the count and the count
converges to the correct number so again
we're not going to go into a lot of
detail on this but you can look it up
later and I'm happy to talk about see
already teased all night because this is
basically all I do for fun because yes
so anyway the motivation so again you
know if we look at her laying Erlang
implements this kind of message passing
model of execution you have a bunch of
processes that are executing either in
pay either definitely concurrently but
possibly in parallel and they send each
other these a secret asst messages and
to reason about programs under this
under this setup it's it's inherently
nondeterministic because you're not
going to know the order that the
messages are received and also in Erlang
you have the ability to send messages to
processes that might not even be able to
handle those messages so it makes
proving these programs very hard if you
want to prove them correct and know that
they'll always execute under any
possible message scheduling and there's
a bunch of work Costas has done a bunch
of work in this area as well as John
Hughes but again it's still very very
difficult so I'm just I kind of talked
about correctness if you if you're not
familiar with kind of thinking about
this you know we have every message
delivery to a process we can kind of
treat as a choice that the program is
going to make about how it's going to
handle it and then all of the or the not
handling the message and then a series
of these are basically going to give you
some execution and there's going to be a
ton of execution
of your program based on how all these
things communicate and how the different
schedulers send the messages and and
things like that and what we kind of
want to assure that is that the program
kind of terminates or it can basically
handle all of these messages correctly
distributed Erlang makes us a little bit
more difficult the citation I have here
number 13 is the Erlang pitfalls the
distributed Erlang pitfalls paper which
is interesting to read because there are
all sorts of interesting semantics about
when messages might happy to leave
you're guaranteed that you can get
certain prefixes that are delivered but
under certain wit time windows you might
lose an entire set of messages and it's
retry this term number of times so there
there are a lot of complexities that
distributed Erlang ads so it just makes
the state space even bigger and it
becomes even harder to prove that these
programs execute correctly OTP is
essentially a series of like patterns
you know are the gen fsm and the gen
server and things like this there are
patterns to kind of reduce this
complexity allow us to think about how
to build these things in a way that is
the way that we can ensure that we have
the correct behavior in most
circumstances so what are we going to
look at so we're going to look at kind
of like an alternative approach so we're
start we're looking at programs that so
the original the start of this work is
looking at programs that are
deterministic we're going to look at a
deterministic dataflow programming model
it's implemented as a library for Erlang
a bunch of library calls to create
variables and buying variables and
things like that and the property this
has is that when you don't have this non
determinism you can guarantee that
regardless of execution under any
scheduler and leaving any any
distributed kind of situation we're
going to get the same result in the end
and that's a desirable thing and what
we're going to do is the major kind of
contribution is that we work on
providing fault tolerance for this
programming model which we don't see in
things like Alvarez or blue so a little
background and it is hot up here right
it's hot in there I see people wiping
their brows and things so deterministic
dataflow programming historically we
kind of have a couple papers here from
nine
seventy 1974 we have like common
networks we have a lazy version that
comes along shortly after these papers
are easy to read they underlie a lot of
the stuff that we work on today more
recently we have the ctmc p which is um
the concepts techniques and models of
computer programming which is a
co-written by peter van roy who is
working on this project with us and that
has oz and Mozart oz which is basically
a environment to you basically explore
programming languages and this is they
have an implementation of this
deterministic data flow that you can use
their they're not the distributed but
the deterministic data flow in addition
you see some similar developments
happening in acha aur acha however you
say it the kind of reactive like kind of
actor programming library and finally
Ozma Ozma's ask Allah package which
provides deterministic dataflow
programming in Scala and that's a fairly
new one and i think there's actually a
newer one that just came out as well for
another language but i don't remember
off hand so how does deterministic
dataflow kind of work so um
deterministic dataflow relies on a
single assignment variable store so here
if we look we have a single assignment
store here so we have x1 through xn and
each of these can be in a variety of
state so there it can be in three
possible states so the variable can be
unbound the variable can be partially
bound basically it's bound on another
data flow variable that's waiting to be
bound and then we have where it's bound
to a value so if you look at x3 and x4
you have x3 which is bound to the
integer 5 and X 4 which is bound to the
list containing three atoms a B C ok in
addition for each of these variables we
store some metadata that allows us to
know what's happening with each of these
so the value is either going to be that
bottom value that we don't have a value
for this yet its unbound currently we
have a series of processes that are
waiting to be notified about that
variable being bound and then this bound
variable represents the other variables
have been bound to it that we need to
update when this gets bound it's pretty
easy to read and bouncer it allows us to
kind of build a graph of what the
dependencies are between the values and
we'll look at what the basic primitives
for providing providing this
deterministic data flow is so we have a
declare primitive which creates a new
data flow variable you see here that the
semantics just say we have some
variables we call declare we basically
create a new one and stored in there and
it starts at that unbound state with
bind the bind operation is what allows
us to assign a value to a data flow
variable that is going to notify all of
the waiting processes so any processes
are waiting to compute on that value
will get notified of this thing so we'll
actually trigger those to start
processing and then we also bind any of
the dependence so if i have x 2 which is
bound to x 1 and then x 1 is bound to 3
then wait x 2 is simultaneously also
fountain 23 and then you'll see that
after the execution of this happens we
just have updated the list of variables
in the single assignment store with all
of the new information they're not it's
updated sequentially through a single
process right now it won't send the
propagation to the values until it's
done notifying those processes
um so the final to you basic primitives
that we need our reap so Reed is
basically going to block until the value
that you're attempting to read from has
been bound and finally thread so thread
is a primitive in the original
literature which takes a function and
then execute it in a process in Erlang
we just use the spawn primitives to this
so we provide a wrapper so that you can
use the same API as the original
literature has but it's all it really is
is semantic kind of sugar for the
programming ok ok so streams and so
streams are also so strings are
extension to the bottle if you look at
oz or Ozma and streams were basically
streams of data flow variables they each
point they each store an additional
piece of metadata which points to the
next variable that should be bound based
on the stream of information that's
coming in and then finally the last one
is sent to this undefined value so again
we just have the same metadata you have
value waiting processes bound variables
and then we have a next value we have a
produced primitive here that basically
adds a new data flow variable and then
returns the identifier of these
variables so all these variables have
unique identifiers for all of them and
when we start looking into the
partitioning that identifier is unique
based on where it's stored in the the
ring that will talk about when we talk
about the distribution so produce
basically pretty straightforward extends
the stream binds the tail to a new
variable moves that undefined value to
the next one and produces a new value
for the stream what's consumed very
similar I'm consumed basically is just
going to read the value and then return
the identifier with the next
so with laziness do we also provide a
primitive for non strict evaluation to
additionally updates the metadata to
track something whether that variable
should be processed immediately or not
and then we provide this weight needed
primitive to say for data flow variable
X we don't want it to process the we
don't want it to compute that value
until we needed so it actually won't
compute that value and notify any of the
things that are waiting to hear what
that if that value is bound or not until
somebody explicitly calls read on that
value on that variable and then will
perform that computation so it's exactly
the same it will just stop the
evaluation further down until explicit
read the blocking read operation is
called um for non determinism we provide
a primitive and we're going to talk
about all the problems with this
primitive for providing
non-deterministic execution so what this
primitive allows us to do is inspect if
a data flow variable has been bound or
not and taking two different courses of
action based on whether the variables
been bound or not introduces non
determinism isn't of the program because
under multiple different executions you
won't get the same value from the result
of the computation so there's this
primitive cult is debt that determines
whether a variable is bound so we look
at it whether the variables bound or not
inspecting that value and then return
either a boolean representing whether
it's been bound or whether it has not
been bound so failures obviously
introduce non determinism we could just
wait forever until the variables are
available but given the fact that it is
a single assignment store these
variables only get sent once we can do
better than that so if we look at an
example here process p 0 is supposed to
bind it never it fails for some sort of
reason who knows what it gets killed by
user it could be anything like that and
then we have a series of processes or
waiting on that variable to be bound
whether it happens to be a
eat or some other operation that's going
to compute further down the graph we
basically wait forever and then the
program will never terminate so this
this obviously doesn't ensure progress
and we don't want this way and we can do
better than this behavior so we look at
two classes of failures one being a lot
easier than the other computing process
failures and data flow variable failures
so if we look at this example for a
computing process failure this will be
similar to the previous p reads a
variable and p performs the computation
on that value and it's supposed to bind
that result to x2 so if if we never
actually perform the bind operation we
can restart the computation and allow it
to continue if we do bind it we can we
can when we go to bind it again we can
basically ignore that bind operation and
allow the program to proceed with that
value that's already move down and go to
the next operation to achieve this we
use the Erlang primitives so simple
supervision trees and understanding and
linking processes can can facilitate
this so we just know that this process
failed just restart these processes
because they depend on this and this is
the information that is being stored in
the signal assignments tore through that
list of processes that are waiting on
the other process to finish and compute
a value in terms of data flow variable
failures wow I don't think that's let
has the right information but if we
attempt to compute some value so if for
some reason if we attempt to try to
retrieve it it this slide is completely
wrong so I apologize for that if if we
attempt to retrieve some sort of value
so we you know something gets bound and
that's performing some computation and
attempts to go to read some for read
from some value and that value is not
available so we can't reach it for some
reason whether that node happens to be
offline because we're running the
processes cross node or some sort of
reason like that we can't re execute we
can't reacts acute that
basic process because if it's trying to
perform some connection across the
network and that nodes unreachable or
that notice timing out or the TCP
connections being filtered or anything
like that the reacts acuson will
basically just trigger that same failure
and will be blocking again so we've
looked at extending the model with an on
an on usable value to allow the
computation to compute on that but that
is still something that we're looking
into so this is another part of the
problem that hopefully we attempt to
alleviate with some of the work that we
do on distribution and replicating the
data store across nodes to allow for
fault tolerance so I will kind of look
at what the API looks like so there's a
note at the end this is not super kind
of like OTP ish it should look a little
bit more a little bit more Erlang e
unfortunately it doesn't just given
there were time constraints and some of
the people who get involved in working
on the project weren't super familiar
with OTP so we were working on I am
actively working on cleaning up this API
so keep that in mind when you look at it
but so we have declare which creates a
new unbound data flow variable on the
store so this creates excuse me a new
entry in the datastore and then returns
that unique identifier that comes out we
have a bind operation which binds a
particular value to a data flow variable
and that's done based on the ID that can
be another data flow variable so like we
said that could be the that could be
another data flow so i could say bind x
2 to x 1 or it could just be a term so
any Erlang term to say that i want to
bind like an atom or a list or something
like that we also have a bind
alternative which can bind the result of
evaluating a function so we can call a
function call and have the results of
that function call be assigned to the
value of that data flow variable in
addition to provide that read primitive
which will return back the term or block
until the data flow variable happens to
be found so read is a blocking call in
terms of streams we have what we talked
about that produce and
consume primitives so we can bind the ID
we can find a value in we can basically
do the same thing as find with the
produce that takes the mod phone arc so
we can call a function and have that get
inserted into the stream as well so keep
in mind this is inserting a bunch of new
data flow variables at the end of the
stream and then making the stream longer
extend allows us to find out what the
next value the next ID in the stream is
and the produce and consume primitives
both use that in their implementation
for laziness this is the way need a
primitive that we talked about that
allows us to say that a particular data
flow variable will not be computed so if
you know if you happen to say if i have
to say bind and i say you know whatever
i call some function that computes some
addition of two numbers and then stores
day and then returns that result and we
want to assign that to a data flow
variable that won't be computed until
something else calls that and triggers
it so this is just regular kind of like
sunk ish non strict evaluation and then
the non determinism primitive just
returns a boolean status like we talked
about if the variable happens to be
bound or not okay so to achieve the
distribution so what we need to do is to
get the fault tolerance properties that
we need and to get the high availability
properties that we need to allow us to
progress in the event of like network
failures and scale out the competition's
to multiple nodes what we need to do is
figure out how we want to go about
partitioning the single assignment store
across the nodes so we're going to have
some sort of cluster and then we're
going to store the data in that cluster
so each very so there are three
strategies that we looked at for doing
this so each variable could have a home
process so we could say one process is
responsible for this variable and stores
in itself the information about that
dependent processes that need to be
notified of changes to its bind status
that obviously produces a single point
of failure in that particular executing
process if we could run multiple of them
but it's also inefficient because you go
to one to fan out
many so we don't want to do that the
other one is that every process could
just know all the information about all
of the dependence which you need to be
notified which is a tremendous amount of
housekeeping that needs to be done for
the processes so we don't use that
approach either and finally a approach
that we went with was that we want to
partition the single Simon store across
a number of nodes so we're going to
partition it based on you know some sort
of consistent hashing or something like
that and then we're going to replicate
two partitions that are adjacent on the
ring similar to what we do in react so
in looking at the design so we did look
at leesia and react core for
implementing implementing the single
assignment store nisha has problems
during Network partitions obviously we
know that we had there are ways for the
store to make independent progress yo
ends up being housekeeping about how to
reconcile changes it's actually not a
huge problem with the single assignment
store for dealing with this reconciling
changes because all of the values are
immutable essentially once it's bound
it's bound so we could just it's very
easy to do that conflict resolution if
you're familiar with conflict resolution
like an react or something like that
what you have siblings you have to merge
them this becomes a bit more difficult
we don't need that now because of the
immutability kind of once the variable
set we never change it however when we
move to the model where we have crd
tease we want something that's like a
little bit better fit in addition you
know the replication is is not as fine
grained control as we'd like it to be
and generally we felt that react core is
a better solution for this so with react
or obviously you've probably heard this
feel a ton of times so react core is the
underlying distribution layer for react
that's a run as an independent progress
so the key value stores built on top of
this project it minimizes the
reshuffling of data so it uses all these
fancy things we like to talk about all
the time like consistent hashing and
half space partitioning and sharing the
data out and all sorts of things like
that
in addition it provides a good good
facilities that we anticipate will be
using in the future but we are not using
currently for causality tracking so it
provides an implementation of vector
clocks it provides an implementation of
version vectors and dotted version
vectors so these are three kind of crdt
related causality tracking mechanisms
for allowing us to reason about how
events uh happen in the system and dvds
are actually very critical to how CR DTS
are implemented so we get that for free
when we bring this in obviously we have
anti entropy and hinted handoff so if
you're not familiar with anti entropy
anti entropy is a mechanism that says
well I have a bunch of replicas of
different objects if I only have a
quorum right to a group of them I run it
and I never read I run into a situation
where some of those replicas could be
stale versions of that object so anti
entropy is a mechanism where replicas
are kind of pairwise compared to
determine which differences exist
between them and then kind of bring all
the objects up to date so usually its
run as a background process or
periodically to ensure that your data is
up-to-date across all of your replicas
and this is especially problematic if
you don't do reads that often because
normally normally in a dynamo inspired
system you do that convert you do the
writing back and repairing of missing
replicas on read through a process
called read repair hidden hand off and
sloppy quorums are used for backup nodes
to accept rights in the event that
primaries are offline and finally react
core provides the ability to cluster
nodes and have dynamic membership so it
provides a so there's a concept in react
core called ownership handoff which is
when some of the virtual partitions or
replicas get handed to other nodes and
it goes about how you go about
propagating that state when you move it
around and provides a mechanism for
reasoning about that and implementing
that and we take advantage of this
because we want our system to be able to
be running a computation while the
systems online while people are adding
or removing nodes in the event of node
failures or things like that so we want
the system to be highly available allow
you to configure it during run time so
hold through the brief review of react
core
so the react core is basically a
distributed hash table with a fixed
partition partition size so this is a
ring with 32 partitions what happens
here is that there's three notes
indicated by the three colors on the
ring each of those three nodes grabs a
portion of the rain when they join the
cluster and it try our claim algorithm
tries to ensure that this is evenly
balanced across all the nodes as
possible and what it allows us to do is
in react kV and what we'll see and what
the work that we're doing is that you do
replication over adjacent partitions in
the ring so you write two one and then
you would restore replica of that that
the next two and that allows us to
ensure that there's you know the claim
algorithm ensures that when nodes come
and go and we move things around we try
to do the best to do minimum data
movement between nodes to cause the
quickest amount of transition time so
you have rings that your transition
between and then swapping quorum sloppy
forums are you know in the event that I
go to write to this green partition here
and the replicas are stored on the blue
and yellow if the green happens to be
down all right to the well there's green
again in this case so that one would be
down as well but I walk that rank
adjacently to to determine whether so
the replicas wow that's the first time
I've run into that problem with that
ring and that's amazing incredible no
wonder we use five all the time so so
what do we do to implement the single
assignment store on top of react core
and so we partition the single
assignment store across the cluster so
every time we go to write a new variable
into the datastore we perform the
hashing function on that to determine
where it should be placed and that
allows us to determine which node to
place that on and then what we do is we
write to a strict quorum of the replica
set so we don't use sloppy chorim's in
this implementation because it's hard to
reason about state changes on the fall
backs because you can write to any fall
back and it gets really hard without
looking at every single node to
determine where the state might have
been written so we only rely currently
on strict corn that's ensuring that what
at least a majority of that initial
three that primary is online so as
variables for come by so as we do that
there we basically write that into that
partition of the single assignment store
so it allows us to start spreading out
our variables across the cluster of
nodes distributing of them across the
cluster of nodes and that distribution
will be fairly even based on the
consistent hashing algorithm it'll su
reimplemented and the distribution of
how the claim has handed off those
partitions to your nodes and as we call
the bind operation so as the bind
operations call and some function
executes something it computes some
result and goes to a cyan't we go to the
strict form so we find that replica set
though it's responsible for that value
and we write it back to that replica set
those will basically use a strict quorum
to notify any of the waiting processes
on the remote note so any of those
variables on the remote nodes to inform
them that they need to perform that
computation so we have a custom vinodh
that's been written that handles
notifying for these things to run I so
in the event of a node failure so if we
have missing replicas what we need to do
is use an active anti entropy and anti
entropy mechanism to go find the
replicas that have the state and ensure
that we repair all those values so for
instance if I ever I if I have three
replicas and I say i bind some value and
I store it and I say you know a is now
equal to one and it gets written to two
of them the right succeeds because a
quorum is online and then it does it
doesn't get rid of the third replica
what we do is during this periodic anti
end this periodic anti entropy mekkin is
that we go find those replicas and get
the value and then write that value back
to the other one and then we trigger any
processes that are waiting to go on that
and what's very important is that this
this mechanism where we tell things that
we've bound variables since this is a
single assignment store and we only bind
these variables once consenting that
notification again to say that hey this
thing got bound again is idempotent and
doesn't cause a problem for that one
notification that idempotent all the way
through but eventually it is but so
since we rely on strike or
terms this means that under network
partitions we cannot make progress we
have to pause until a majority is online
in the event of a network partition and
this is to ensure that we get this
determinism during failure scenarios
again this idea of this redundant
recomputation doesn't cause problems
because of the single assignment nature
of the variables as long as those
variables aren't doing anything non
deterministic which will again shortly
and finally this idea of dynamic
membership says that if I'm moving this
data between one node to the other
because of some cluster transition or
I'm adding a bunch of new nodes we what
we have to do is we have to take that
portion of the single assignment store
and send it to the new target replica
and while we're doing that we also need
to re trigger any notifications because
there might have been a situation where
there was a network again network
partitions we do make progress we just
don't make them when a majority is
unavailable so there can be situations
where something may be temporarily
unreachable one particular replica and
that could happen during this dynamic
membership handoff phase or it could
happen during during the normal
operation of the cluster notifying a
variable so we have to ensure that we as
we move the things over to the other
replicas we notify any local processes
that might be waiting or any remote
processes that might be waiting on that
to be bound um so here's just a quick
and dirty concurrent map example so this
is going to basically get a bunch of
values from a stream and spawn processes
that is going to compute the is going to
compute some function over the values
that are coming in the stream and then
it's going to assign these to an output
stream and this might seem very familiar
if you're a JavaScript programmer or a
programmer or a scallop rumor or
somebody who uses futures because
essentially it's very similar to thing
you're you're inserting into the
destination stream the output stream
some variable that will be bound at a
future point but the order that that
gets into that output stream is going to
be deterministic okay I I think I only
got like five minutes left or something
something like that so I what are the
caveats with non determinism so we have
that non determinism
non deterministic primitive that allows
us to make decisions on whether
variables are bound that's not the only
problem right there's any code that
could has a side effect is also
extremely problematic so you can write
into some replicated at stable you could
you know right to amnesia you could
write something to a log it could be a
global log it could be a local log so
there there are a bunch of interesting
things that we have to figure out here
right so um here's an example of one
where we have process that binds one and
then binds another and then process p2
is going to read some variable and then
do something that's non deterministic
and well we'll talk about what those
things might be and then eventually
we're going to have process 3 3 mu doll
and computer value and then p for
compute some value after that so the
possible failure scenarios we have are
so that the three kind of failure cases
we want to think about here are that
execution can fail and in the first two
processes and we should be able to
restart those without a problem if they
don't do anything that's
nondeterministic so in this slide we're
considering just you know any of the
things that I mentioned before or using
that is debt operator again the
execution could fail after we perform
that kind of non deterministic operation
as well so we don't have a way to know
this so if we fail before we can restart
and if we fail after we should have
already bound the variable so we should
be able to restart the things that
happen after the non-deterministic
operation and still make those same
guarantees the problem is is what
happens if we fail in p2 as doing that
computation that has non determinism so
this is a problem and the question
remains unanswered as to how we want to
handle this again you have to think that
you know it is replicated we could have
another node take it over however if you
have no way of classifying whether that
operation is going to be globally side
affecting or locally side affecting it's
very hard to determine what case you
take in that scenario so an example
would be if you know this is we're using
this react or it's a bunch of nodes and
they write a local log entry well in
that case it would be ok to restart the
thing because the nodes probably gone
offline another replica will take over
that part of the key space that other
replicas will take over that part of the
processing and we can
move on again if it's a global side
effect where it's you know it like you
know whatever enters the launch code for
an ICBM then that's probably more
problematic than writing an entry to a
log on the local disk so anyway so
future work so where are we going with
this why don't I talk about CR DTS and
what did I talk about this whole
programming model thing so that the idea
is that we want to generalize the model
to these joint semi lattices we want to
look at this idea that we have a single
variable state and it can either be
bound or unbound and we want to
generalize this to all of the semi
lattices that we have so we want to use
this library of CR DTS that we've
created these I to these counters these
sets things like that that guarantee
convergence in the event of concurrency
and we want to allow we want to provide
a mechanism where you can have a cluster
of notes that are running react core and
you can run a computation inside of it a
cross maybe data that's stored on those
partitions and have the computation be
guaranteed to come out with the same
value regardless of dynamic membership
regardless of failures regardless if you
add another node to make it go faster we
want to have that and what the crd T's
give us is this property that allows us
to insert that if we read some item off
disk at a particular version we know
that it's changes with some item that we
read over here is its changes will
eventually some computation performed
across those will compute to the same
value so what this relies on is having
crd teases all of the base types these
convergent data structures as all of the
base types and then providing
computations that observe those
mathematical properties we need to
preserve across those and that allows us
to write deterministic programs that are
scalable and fault tolerant and so this
is part of one of the goals of the work
package and the research that we're
doing finally we'd like to provide an
analysis tool to detect these non
deterministic points and point them out
in a program so this is very similar to
the work that's been done on Daedalus to
find points of your program where you
need coordination and make you aware of
it so that you can take appropriate
actions to handle that correctly an
additional additional thing that I think
would be kind of cool is to be able to
use dialyzer to provide you know a
different mode where you could say I
want to find all the places where I'm
going unsay stuff and use that tool to
detect those parts by classifying a
bunch of the built-in primitives as
being dangerous to run and then we can
basically look at the transitive
relationship between all the calls and
determine where the problematic points
in the program might be finally having a
better syntax would be great everybody
you know you don't like writing a
library call for every time you want to
sign a sign a variable so we could
easily probably achieve this through
some sort of grammar or parse
transformation or something like that
and finally the library was made very
very quickly by people who knew data so
I was the Erlang guy who knew react and
the other people that I worked with were
the dataflow programming people who had
a lot of experience in other languages
so some of the libraries and as
idiomatic as we'd like it to be but this
is just a matter this is a small matter
of programming right so we can
definitely address that and that's all I
have so thank you very much
questions yeah I'm wondering a little
bit about how easy would it be to
implement something like yeah flow based
programming upon this upon this platform
how easy to write a program with it
right flow based programming if I don't
know if are you familiar with that it's
some kind of subset of dataflow
programming as far as I understand um do
you have an example of what you're
referring to you there's a popular
nodejs framework called no flow for
example made it on the kickstarter ok
i'm not i'd be happy to look at it and
see it probably uses that kind of it
doesn't use streams in node Jesus
bounded buffers ok cuz I was SI b uses
that streams and notes on great I'm not
very familiar with node right so what if
I may be the main question would be is
it possible to write a declarative to
define in a declarative way that kind of
notes and connections and so on and may
have it generated you have the actual
program generated to generate a program
yeah from a text definition for exam um
I mean I would have to see what this
looks like but I I imagined it would be
it seems like it would be straight
forward unless i'm misunderstanding the
question ok we could talk more about it
after sure casas has a question of
course
every time it's across the whole so I
understand that this is working progress
and of course it's the first step and
you know it's very very nice for a first
step but I would like to ask something
you know fundamental so this is suppose
this is titled deterministic and we find
out in the talk that there is a non
deterministic construct in it sure and
you know it's one of the motivations is
to be able to prove things more easily
and then you know you say that okay we
now added this non deterministic thing
and we have to do exame dialyzer or do
other things you know so what do you get
it what do you gain yeah I mean I I
didn't make it perfectly clear but I
think that there is a class of programs
that you probably that you will need
coordination essentially to write and
with the crd tease there's a specific
case where you may need to perform
garbage collection or something like
that when the data structures get too
large that is one thing that we're
actively fighting and trying to find
ways around so I think that in the same
sense is with blum there will be a point
that you'll need to have a
synchronization for everything to agree
on some sort of loss of state where you
gain you gain a programming model that
is fault tolerant and eventually like
you can guarantee any schedule
interleaving any possible execution
across a series of values and guarantee
convergence I don't think that you can
guarantee that with with airline
currently can you define guarantee what
do you mean by guarantee um yes okay so
like let's say that we have I mean how
about the set example a set will you
have parallel parallel executions on
some sort of shared set right in Erlang
so in Erlang let's say that there are
two processes that are across on
operating on different nodes that are
inserting values and removing values
into that set there is no way that you
can guarantee
that and guarantee i know is such a
loaded word but let me get the thought
out at least there's no way that you can
guarantee that additions and removals of
items on those sets regardless of
execution and tcp delay and all of that
stuff without synchronization guarantee
that they end up with the correct value
that you can't prove that right unless
you test every possible interleaving and
you test all the tcp different delays
and things like that right if you have
random operations happening and you have
some set that you want to be
representative of the operations
observed at each node okay maybe no I'm
talking about a data structure that is
shared it's it's shared because it's
occasionally swapped between parties but
it's stored locally like if you have
something here's a better example let's
think of react because the crdt stuff
makes sense it react so react you have
nodes and then there's a piece of data
that's replicated between two nodes
right and then all of a sudden you have
a network partition and then you have
rights coming in to either side and
eventually you go to converge so the way
that's handled now to ensure that we
never lose an update is that when the
nodes come back together we have to
store the information that a user has to
write a program to say about how to
reason about the changes that happen
because you might have conflicting
changes in that set right you can have
an item that's that same item added on
both sides removed on one side how do
you know who wins right because the
Opera because the operations happen
independently and then you want to share
that data structure so we run into that
same problem when we use something like
react pipe that allows us to do
computations over an entire cluster of
Erlang nodes and the way it does that is
it it shards the computation across a
series of notes you'd say that if you're
running a MapReduce you'd have the map
run on all the nodes and you have the
reduced front on three and then the
reduced runs on one you have a problem
where you have you can have failures
happen on some of those on some of those
nodes and then you might rerun on
another node and then you have two
different states potentially a partially
computed state and a fully computed
state and then you want a reason about
the changes there so we can use these
data structures there as well to reason
about the changes to the data structure
and we should we could talk more but I
think the rest of that discussion will
have to take place over Kabul Erlang
beers later on so Chris thank you very
much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>