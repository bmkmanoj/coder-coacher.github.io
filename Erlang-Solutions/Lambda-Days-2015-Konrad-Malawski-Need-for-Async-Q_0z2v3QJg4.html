<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days 2015 - Konrad Malawski - Need for Async(..) | Coder Coacher - Coaching Coders</title><meta content="Lambda Days 2015 - Konrad Malawski - Need for Async(..) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lambda Days 2015 - Konrad Malawski - Need for Async(..)</b></h2><h5 class="post__date">2015-03-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Q_0z2v3QJg4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I hope to talk about things that are
completely different than all the
previous thoughts on this conference so
who am i I'm like I said don't say I'm
on the Aqua team i'm also working on the
erect extreme specification where did be
a test technology compatibility kid and
other than verticals about community
things are running tracker so i found it
for London out which you may have heard
of or scholar user group among other
hackers around the Air Force sitting
here Eden and rotten and of course i was
around the annual geek on conference
with the w's ago so that's me but i
don't need a little bit of information
about you guys so I know what I can
compare stuff with so who is a young guy
we have earning people okay that's kind
of we're that was expecting more however
if I claimed by the don't need this
stuff that's before okay do we have a
scholar people okay much miss Collins
wow that's good it's amazing but finally
we have so many scholar people i mean
when i started doing scholar and track
of it was basically me and I'll never
die so it's good that we have more so
this talk will be about performance and
the not human sympathy part of computer
systems so unlike the previous keynote
happiness is of little concern to me
into his fork and there will be sweaty
there will be blood and there will be
new turbidity
so this is a great quote from martin
thomson performance guy that performance
is not about bit fiddly okay if you go
to a google interview they asked you've
been fiddling questions just do a face
Portman what what are you asking you got
okay performance is not bit fiddly I
mean you can save a bunch milliseconds
sometimes maybe but performance is about
really design principles and thinking
about interactions of stuff and this is
what we're going to talk about here this
is the agenda you may recognize some of
the words not recognize my goal here is
to present a top level overview what
this stuff is why we needed and why even
if you think you're not using it you're
actually using it in your day-to-day
work so the goal is to will inspire you
to look into this week if you feel
sleepy I do highly recommend breathing
in or standing up and waiting around
here whenever you need because this
stuff would be quite heavy and it would
be a waste if you fall asleep or new
strike at someone so feel free to wave
your arms or whatever and not follows me
not wasting any time and here's another
list of stuff we're talking we'll talk
about concurrent access to shared
mutable state side effects knocks no
logs and lots of other horrible stuff
including a santa code and there's got
to be a lot of lots and lots of stigma
illustrating on stuff and the reason
this is here at this conference and
always react track the reason we need
all this stuff is to gain performance
and even if your high-level abstractions
abs whatever purely functional somewhere
down below someone needs to do with
stuff so we'll talk about the internals
and how the sausage is made so firstly i
hope this is no news to you guys but
let's talk about what a circus so in
secular systems you can request you'll
get a reply get a request get a reply
and this is pretty boring and you can
really do much because you'll just
sequentially processing this message of
course I'm talking here and this is
usually you can think of whenever I draw
a stickman it can represent a threat a
process an actor what it is with one
finger computation and synchronous
systems we can say yeah it's a message
and get another message and then reason
before responding to the first message
we can do something with the second one
because we already know it's wrong for
example so i can respond right away and
then maybe her subscribe message and i
replied first to it and then I reply to
my first message because it took the
longest time to process easy alright so
let's talk about internals of scheduling
so what is certainly something means I
have some processing power let's say I
have to cause or to whatever is able to
execute code right it needs to be
allocated to some of these actors to
actually progress to fit right so I
can't do more than in this example two
things in parallel we're not talking
about acting as if i enter learn about
extra parallel execution so in this case
we have three actors if i have made
boxes these things and mailboxes and
actors were click perfect pick a frame
from the mailbox and start processing so
this can fix this processes of this case
process of a process here we got time
right this means that here my sector is
unable to progress because first no
computer power available right easy so
at the end of processing of each message
vers la opportunity for someone to be
shuttered all right so here we have no
CPUs but as we end of each processing
when I'm done processing a message I can
research on some other guy so where's to
potential opportunities venice cali
would get some
your time okay if you got lucky he got a
yellow CPU is happy in processing the
message and the blue guy is continuing
to process of analysis so now the middle
guy cannot progress because we don't
have precision colorful and then maybe
we have certain two interesting things
here the down actor is doing some
synchronous i/o to some data where some
disc and this fred is basically wasted
okay that's a terrible thing to do but
front me a little actors perspective it
doesn't really matter if this is
happening so someone doing something
stupid and blocking a thread or if he's
just unlucky and didn't get cpu time
because the other actor one again and
got her cpu to progress by doing his
things and when yes finally returned and
here's an opportunity for the middle guy
to get some cpu time so what i want to
highlight here is if I'm only the middle
guy I don't really care if someone is
blocking or simply not very lucky with
getting compute both it's the same
effect I'm unable to progress so how
does a sink coming so async comes in
that if I have to do some call instead
of blocking vet fret I can perform a
synchronous i/o when I can give up with
Fred's this guy control dress and then
once with database or disk or whatever
returns I'll get some revenge I get
notified and I can vent hand reverse
event again right so in even based
systems we try to maximize the number of
not wasting time a great fingers wasting
half so instead of wasting time waiting
I can just signal the intent of I will
continue doing stuff if if you are done
right by you and meet the data bits mr.
and give up the thread so someone else
can progress so in general mr. Holmes
aim to minimize me okay what does
latency latency is the time interval
between stimulating and getting some
response right in the broadest sense of
this so never quiz
I will try to interact with you a little
bit so don't fall asleep after lunch so
question is a ten-second latency
acceptable in your application in your
day-to-day work I mean hands up if it is
ok very systems go ten seconds is fine
right there systems were three days is
fine I mean some Bank twins physical
date three days and somehow we think
that's accepted me okay what historical
reasons right he kind of think was
accepted for something but yeah maybe is
200 milliseconds fine go in more hands
okay that's the thing so finally someone
God okay so and so how about mostly
mostly 20 millisecond requests and one
and sometimes like twice a day you get
it one minute before you get the
response maybe some systems it's fine
and do people die one girl agencies
above 200 milliseconds yes so probably
need a real-time system right they'd
probably in type system not a soft
squishy GC basic system right so all
these questions are more or less
detached from any context and if you are
happy here raising oh yeah this is fine
you're probably I don't know as an
answering too quickly so what you
usually would like to say about latency
is ya somos ninety percent has to be
below something ninety-nine percent has
to be below a second and then 99
percentile has to be below two seconds
otherwise we are the business for
example so you always need context about
these things and sometimes big Layton
sees are fine and important thing if you
just measure the average latency and
then if you're better he also measuring
ver and i'll send the deviation and then
someone comes around and asks you yeah
so what's for the ninety-nine percent do
not apply mathematics and statistics to
his fate what because just from maps you
can calculate for ninety-nine percent
when you use normal distribution we draw
a trophy look oh yeah I love this
investigatin see of the 19th person
we're great but this does not apply
because latency is not distributed like
normal distribution it's very spiky very
very weird you cannot apply normal
statistics like book statistics to this
thing if you didn't measure the 99th
percentile you cannot say anything about
the 99th percentile because maybe you
have this horrible huge spikes and the
average is still very good right so
here's a better way to imagine latency
in systems here we have a histogram but
here we have a percent tires and here we
have the latencies and the yellow thing
is some SLA so here you see oh here I'm
over my es la I'm i have trouble here I
don't need to optimize my best cases I
don't need optimize this way and also
you see this system has two kinds of
behaviors work free Cheers okay super
happy cares no one cares about her peers
okay some kind of behavior and here's
some critical point where the system
starts to behave differently so these
are the important points we want to
measure and investigate why exactly at
this point for system is starting to
behave different so usually the
investigate these things trying to match
up with something I do recognize his
pedal it looks like a snake and then
when I to you look if you want to it you
recognize oh yeah I hope that you
remember in our system we have this
elephant thing and this elephant thing
starts somewhere around here I mean the
elephant thing would usually be some
buffer size or some latency to some
external resource but you do have to
correlate profit system is behaving to
whatever the code is due so you need to
investigate your so called Lake Taylor
my tell you we mean bright side of the
graph okay now let's jump to different
topics because her general idea of his
talk is to jump through all the layers
of such a system coming from yeah just
measuring stuff burn some
concurrency stuff which I'm going to
talk about right now and we'll end up
with some clustering things so
concurrent versus lock three doses wait
for you so any of you familiar with wait
for Ryoga reasons kit no one but three
people not good that good but I am
showing this one so a concurrent data
structure is basically anything that can
not do wrong things when accessed by
multiple fronts right this also means
but this is fine but hy so right by a
and B and C any threats here and still
tries to write to their choice rewrite
it somehow not waning so what does it
mean if a little wins does it mean that
the algorithm is not a concurrent al
gore-ism what it is it's just what fred
is a complete loser and this is fine
this is fine because always I mean
someone wins it's still progressing
right and it didn't yield some weird
results so we didn't have a tries to
write B tries to write enough sausage
comes out right it's still some expected
result that's a safe concurrent are
doing what Fred a may never make
progress looks like it at least and this
also could happen but it could take a
look and somehow it took the wrong knock
and it just dead looks for it this is
what not cute but it can happen ok next
Friday still a complete loser because he
took a look no tech locks and so how
does a queue API look like normally in
such a data structure so we can offer
and we can get a information if it was
able to put stuff into the queue or not
so it's going to be a successful failure
it can throw an exception if it was only
able to insert something it in the data
structure or worst of all it can wait
until it's finally inserts for any
interpretation struck this is horrible
yeah but this is still valid
let's talk about not freedom and it's a
luxury data structures look mostly
something like that so we come next term
without any synchronization so first now
red or green light we do want to keep
progressing right no one is really
stopping own is going to stop slowing
down even let's see the data structure
is still progressing even with people so
this is what a lot less programming lab
just looks like and it also means badly
any threat is guaranteed to progress
after some time so it will not spin
forever I mean in the previous example
it could take a log and that never be
woke up again in lovelace it was no
locks it will keep spinning and trying
to write on fill it with these
algorithms looks like I look like that
so we take a cue it's an immutable cube
and we have an atomic reference to and
when I insert my frame and want to swap
it so this operation is called compare
exchange that's actually an assembly
instruction but supported by processors
so this is not something we can
magically invent and simulate this
actually has to be done on the
processing level and the process i will
try to compare a given value so the q
value but if so then I saw this value
right yet when I appended to it and I
expect with no one between these three
lines has edited vacuum it's to thank
you so I want to compare with what is in
this black box is still what I got out
and I want to put in my appended way and
I will continue doing this until I'm
successful ok so we try retry retry
retry at some point I will win this race
because vs effectively racy code and
someone always one Fred will win this
compare exchange yes I small
visualization both take 4q both append
something to you here's a compare and
set that wins right because the blue
ones didn't yet right and here the blue
one loses
race because here the the data structure
was already compared and set and now
it's yellow very very great right so it
loses the race but when it tries again
now no one did another right so it wins
the race with what in this case no one
so this is luxury and luxury I'll go and
the third kind of algorithms is weight
free some weight free let's look at the
formal definition it says but that every
operation has a bound number of steps
after it will make progress this means
in the previous example I could be
always losing her race good could be I
mean in practice it doesn't really
happen you maybe spend four or five
times but in theory it could happen that
you keep spinning like 45 years and you
never win ki aurat could happen in wait
for your burritos we have this bound on
the number of retries I am guaranteed to
activists number three tries to have
committed major search here's a typical
cult which does that an explanation if
you really want to understand it here's
a paper it has 40 pages we're not going
to go into that one I just wanted to
highlight the higher levels of
guarantees when we can give than just
not less okay new topic like I said
we're gonna jump topics are quite a lot
so I oh and now the IO Mona does not
solve this and my luxury what is I oh
and what is niall what is 0 in this
context so we're going to talk about all
of these so here's a nice quote from
havoc who's not working at typesafe and
I was asking for feedback when i was
doing this presentation and yeah random
thoughts about IO in your daily life so
his coat was when he learned about John
enterprise in two thousand tonight he's
a young guy so one of the corner
core committees of genome and his
reaction was what the is this I oh
crap where's the main loop so really I
Oh for this blocking is not the most
natural thing to do for some people and
somehow in service we have grown too I
don't be used to blocking aisle let's
talk about how crazy it is so you guys
know that cpus get two modes use a load
in kernel mode hands up okay 90 people
raise our heads so we have this user
mode and this is the safe mode where we
don't trust you until your computer when
we do things wrong and we have kernel
mode where we can basically sec for the
entire system right so I don't really
want to be in kernel mode unless I
really need high performance thing is
and access to random pieces of memory
directly right so we normally switch
between those these two modes because
he'll manama application is running and
here i need to access some driver name
okay all i need to access some I oh and
in this case so the problem is switching
between V smells is super slow because
this is a hardware thing you don't have
to realize with your processor actually
switches melts it's not a colonel thing
it's really a hardware frame and the CPU
actually changes how it executes stuff
so mistakes were depending on processors
nowadays it's quicker but I found and
the links are you to know that Sydney
don't really skilled and would only
painting for mrs. Fane took a thousand
cycles and that was worth around a 400
nanoseconds this is horribly a lot of
time if you want to do any high
performance this is basically a no go so
how does this come in to play when we
talk about I out so when you wish your
read I want to read this file then the
colonel we will issue with cisco and
issue buried in kernel mode then it will
come back to you get your back control
into your program
then you issue the right because let's
say you're implementing a copy so you
want to read one file and write it its
contents to another thing so then you
should go right to right afterwards so
when you have to issue a cisco again
switch this month again and again you
are switching here from Colonel to user
so we've wasted around 2000 nanoseconds
here not good and what don't worry it's
only getting worse so what about buffets
so Glover's is where basically if you
read something you have to put it
somewhere so where do we actually
perform very well Marissa on disk for
example and they read it into a kernel
buffer but then because we colonel
cannot safely share this buffer with you
it must copy into user space ok so it's
copies for flinging into user space and
when you perform the right but in order
to perform the right this data has to be
in the counter space so we copy the
entire thing here so this is free copies
of the same data not good and then we
finally can write from Cairo to TCP
photo so free copies of the same data
not good so verse limits I 0 which is a
sexual nasai oh and over jtm it's new I
oh and that's been used since 2004 so
now of course if new anymore it's just
niÃ±o so how it works is your issue this
right read request and you do not lose
your friend if I always would say yeah
so when I'm done copying the stuff into
userland I will let you know so when you
get this intro and you can then use this
part but the amount of copying is still
the same right it still has to copy
between kernel space in this case so in
order to make this motor the corona oil
linux kernel at least introduced an
operation called sent five and instead
of actually copying this stuff over and
over again it never leaves kernel space
right I mean if I just want to transfer
data from A to B I turned the corner a
corner transfer everything from A to B
and that's well it does so for snow
copying happening
here it is a broken cord yes but we are
not copying and copying of obviously if
it takes a lot of time so we're saving
time go amazing okay let's talk about
connections anyone heard of the seat
mkay problem ends up okay not me so
Seton k stands for 10,000 concurrent
connections or 10,000 doesn't make me
sound horribly much nowadays but the
problem is made eternal 12 years old
from what I researched at least it was
back in the day when Apache was you know
the new thing httpd server and what I
Patrick's model is one request one front
right and that's not really scaling up
to 10,000 frames well he's back in the
day didn't currently Linux is happy to
schedule a few thousand Linux Fred's but
certainly not 10,000 so what is the
solution to this thing if I really want
to handle 10,000 concurrent at the same
time users hammer in my system what do
you need to go Isaac let's first talk as
me about the word blocking version that
was in in the linux kernel since forever
and then we'll talk about the non-black
non working in a place where which is
resolution of the 10th season capable so
we have a bunch of connections and let's
say these are just something tonight we
have some five descriptive it is
representing the socket then the colonel
we can ask the colonel a colonel so
which socket which connection has stuff
to do because that's basically what we
want to we want to handle a few
connections which actually are sending
in some requests not everyone right so
we pull it we wait you know your kind
operate in this example and then you the
colonel will reply to you with a bitmap
in which it will mark which of these
connections has stuff to do so you need
to scan the entire in fire descriptive
list and check yeah does it have stuff
to do or does it not have stuff to do
the problem with this is versus Owen
right you get to a list and you need the
scandalous easy well it doesn't really
scale you if you have tens of thousands
and you're performing this you're trying
to perform this as fast as you can
because this is only selecting the
collection not even performing work for
it so this really has to be fast so the
solution to this one is even even Paul
this is what the name epochal came from
so people works a bit differently so you
register a handle for it my case coffee
I do need coffee and and you register
this once and then you will register
this socket which you are interested in
so your curve is equal control watch
these sockets so you register them and
then you do the same thing as previously
but it changes slightly so we pass in a
data structure this is basically an
array so array key events and besides
because in c you have to pass around
where the data structure actually ends
right so and then the colonel will call
you back with the data structure filled
in with only these events or basically
marking which sockets are ready to do
stuff and only those this is a huge
difference you do not have to scan and
things but well you're scaling exactly
these sockets which you have worked on
so the algorithmic surface of one it
just gives you buffet
this is already way better and you can
reuse this data structure which is also
good so the lesson from this one is but
ons a no-go for epic scalability I mean
if your scalability is five clients with
some skeleton right we want epic
scalability and if you want to go into
epic scalability things suddenly oh and
it's not enough this is a show in berlin
at scheduling systems where the initial
scheduler was also oh and because it had
to it had a list of freds and it was
looking here should i be scheduling
viscal and now maybe not to the key
scheduling risk I know maybe not it was
the same problem wasn't least when there
was a heuristic based or one scheduler
which would just pick stop were big
threats to the schedule and currently
worsley completely fair scheduler which
actually selecting the threat to run is
a one but it's sometimes it has to
modify this list or it's a 3
investigates so modifying the list of
threads is om in business and here in
socket selection we also went from 0 n 2
or one so we're trying to make the
things that are really used by everyone
namely the corner and force operates or
one whenever we can yeah that's one more
rule on this one and let's talk about a
higher high level face so distributed
systems I'm sure you are familiar with
what at least you be the system is but
here's the definition you really like
but leslie van cott and it's actually a
very very good one because it
distributed system is a system in which
you have many computers and a failure of
a computer that you didn't even know
existed can render your computer using
which is a good what is a good example
I'm doing some core to some remote
server need for remote servers down well
I won't get the reply that's easy
and this is not the definition but a
very good thing to have in mind but in
this regard systems have a latency
profile and the predictability of his
fame is yeah I wouldn't say random it
has some may be predictable part but it
it is certainly more random than just
one host what do I mean by that I mean
if you have 10 computers the probability
that at some point in time everybody
will GC and just not no one is
responding is growing with the number of
service you have so the more of service
you have the more unpredictable
interplay comes in right maybe everybody
is waiting maybe everybody crashes maybe
five people crash maybe six people crash
you don't know so it's getting more and
more complex and more service again so
let's talk about two techniques which
are completely opposite to one each
other so the first technique I want to
talk about is back of requests so let's
say we have a system by this plague by
not nice tail latency and take late and
see if you have a big system when you
have a menu requests per second but a
latency in folks we mean the ninety-nine
percent right and the figures maybe you
think 99th percentile if that's a crappy
response time maybe I don't care right
it's just I don't know how many requests
but when you do the math and you have a
few thousand requests per second the
99th percentile it actually means oh so
every every guy on his set every second
request has a crappy user experience so
this 99 percent it becomes more and more
important than more requests you have
because statistics right he will
eventually get the correct response and
we don't do it so one way to fight tail
agencies is to issue duplicated work
this sounds waste or it actually is but
it's gonna hurt a lot with meeting our
sell it so let's hey let's say out of an
actor and let's say most of this review
and he is getting some worry and the
response time has to be below 300
milliseconds or people die so let's keep
it on to it so first I want to ask some
guy I know he has her data hair can you
give me his data and I'm already
starting a timer but after 100
milliseconds I will perform a backup
requested what do I mean by that a
backup request is sending the same query
to other guys which I know we have the
same data I mean they are also able to
surface request this is very easy if you
have a stateless service right scales
naturally you can just ask anyone from
the pool of notes further able to answer
this question so you asked other guys
and you get a response from one of them
maybe you still didn't get a response
from the first one so what this
technique is fighting is if we were
unlucky in the initial request and cat I
server but it was going down maybe on
the heavy load maybe GC yield whatever
it just didn't make it in time and it
turns out and statistically speaking it
helps to issue the second request even
if we are 100 milliseconds in already
because maybe visco is not on the lone
and can respond fast and yeah Fisk i
will also been respond because we should
duplicate it work but i don't really
care i got my response there are
extensions of this algorithm where we
can cancer this guy to tell him yeah
this guy can tell this guy oh my god I
got this covered you don't need to work
on this but these are extensions with
general principle is for sale and here
we've met our SLA so this is a technique
actually described by Jeff Dean and I
don't know how many years ago me before
a soaking iPad and supposedly they used
to this technique environment produce
and big table systems and these are some
of the numbers for response time from
that system it's a quote form achieve
you from from this talk you can look
that up and without fees back up request
the 99
ninety-nine percent I 999 comma nine
percent is getting ridiculous right this
is really not good but even if with
issuing this one backup request you're
fighting the tail latency a lot right so
even though you're increasing load on
for cluster you're making duplicate work
you're kidding yesterday's like like a
champ right and here you can see the
standard deviation I mean that's but
that certainly shows how many those
worst cases we have just fought okay and
now I said I'm going to talk about it
completely opposite technique so
combining request so combining request
is avoiding the completed work by
aggregating of course okay completely
opposite why would I want to increase
latency because in order to aggregate
stuff I'm going to need to wait for
multiple requests to come in and when I
can aggregate those free for example and
send out one why would I want to
increase agency but sadly we are never
world of trade-offs and this trade-off
is trading latency for not killing my
downstream surface perhaps maybe my
downstream service are not as powerful
or maybe it's just one machine whatever
so sometimes you need to trade trade of
agency in order to survive load so this
is how it works let's say you have this
complete collapse step or actor or
system whatever and it gets free request
we want to collect these free requests
to send in this combined curry for
example of a database I mean sometimes
it's as easy as if everybody is asking
from the same thing I will just ask for
the pain once and then give her on
something for you guys sometimes it can
be more complex but these people are
asking for a movie but with different
IDs right but if you are a squirrel
database it's better to issue one for
you but you know we can have the Select
Clause and kill those three things
instead of sending free quotes
it depends on other confident ideas like
that so we collect those three and we
want to issue one the simplest thing to
determine when to send send out with
request is to have a timer so this also
plays nicely if you have a SLA with the
back end and the back and says you can
query me only of five times a second and
you can until this timer to allow
exactly fight these five requests
because festival for back and does
anyway vago request will be just kick so
that's an easy technique and venue send
in a request getresponse you decrease
load for the infant's ladies so that's a
better technique called back row back
pressure so back pressure means that the
downstream system over let's call it
downstream system has to notify the
upstream so in this case the collapse
that whenever it's ready or how it is
ready to consume requests and this is
actually a protocol that we have
standardized in reactive streams if you
want to get all the details about it go
ahead or ask a question section but in
general it is about the downstream says
I am able to consume five requests and
since this is a synchronous
communication it can keep telling you I
can progress five more process five more
we never just consumed five more so it
because I found buffer of how many
requests it can expect at any given time
so then here we are waiting for verbeken
to let us know it's ready to receive
this request and then we aggregate we
keep aggregating stuff until going until
we get this signal and then we send this
batch so this is the same idea with
aggregating but the trigger is on the
downstream not on the upstream of some
random timer that may or may not be
welcomed here it is tuning itself
automatically well medically but
specifically by knowing how much on the
load it is right but what technique is
were saying we
invent Sandoval goodness so I don't wrap
up with what functional programming is
awesome but none of what I talked about
this function problem so the reason is I
mean somewhere down the line someone has
to bite the bullet and do this mutable
horrible stuff because we are all
running on real machines it is real
hardware and some instructions have
built-in operations in processes and if
you wanted to go fast use the processor
right and the processor doesn't really
care about moment we do the process a
dozen so the trade office someone has to
bite the bullet and it's better for
library does it fall so this is what we
do so acha is a made aware of it applies
all these techniques and then you can
keep your high-level code more pure
right and actually when you think about
a lying or whatnot inver cases for
saying but ven does it for them or in
our case the JVM does many things for us
here right so it's with layers of layers
of here it's more muted broke it's a
little less mutable and when your app
can t kill but someone has to bite the
bullet and sometimes you may need to
understand when you need to bite the
bullet because I and we can squeeze any
more performance out of this moment
maybe you need to drop out of normal and
sometimes I'm not saying you're always
have to one of the great but sometimes
you need here's a great Nicaragua it's
actually from a talk yesterday by young
post a unique and this is sorting
performance so he benchmark ASCO Hanks
has curves functional we sort
implementation and this is the number of
elements in the array and these are the
times how much it took to sorting six so
the three things i want to point out
this the sea craft + + forever immutable
white and let's say let's compare her 1
million case so it's 58 milliseconds and
haskell purely functional this is
millisecond I just saying what
I'm going to complain about Scott more
because here when we use the pure
functional way with moment we got this
is above a second okay it's purely
functional trick so but above a second
to swatting right but in Scala we are
able to just say yeah it and drop
down to infrared and then you see we are
at a hundred milliseconds so having that
option to drop out of purely functional
style is a very important thing we
should just shown it right and very
interesting thing so we were talking
here on the talk with ya so I actually
bent the allocations of the st Monette
version of this algorithm so these are
the allocation rate of we have this
order in performing for sorting and
these are the numbers and numbers of
instances and how many bytes were
allocated as you can see this is a gig
of Ned entries allocated in order to
sort an array of ins ok so pure coding
great awesome but sometimes you need to
drop down and do a neutral holography a
funny story if you have a scar on
YouTube blue vector mu 2 perfecter okay
nice and youtuber functional data
structure the sorting is actually
performed by copying over the thing to
an array performing the mutable sort in
via right then copying but it's back to
the vector so get a new vector bag and
this is still faster than trying to swat
it an immutable data structure and also
this is pure functional I mean it
because from the users of the functions
perspective they don't really care if I
allocated an array and the dipper
sorting in cipher right so these mutable
tricks are still applicable to pure
functional programming because you don't
really care what the function does
inside as long as it's not really key
things outside right and then to this
case we didn't I just allocated opinion
when we drop a ticket
so here's a bunch of links was a lot of
things I did my research on this one
specifically I would you want to
highlight a few specifically the c++
conference talk about juggling razor
blades which is about implementing Q's
very interesting talk and when you
realize that any high performance thing
you want to implement is basically same
asleep we just league see abstractions
all the way up if you need any high
performance that's one thing I wrote the
toilet and learn from shmoocon they talk
about V sittin n problem we talked about
sittin hey so that's 10,000 concurrent
connections and CNN is 10 million
concurrent connections on one box and
this isn't really applicable to move
support eight drops because it involves
bypassing for linux kernel because it's
too slow and horrible things like that
but really interesting and a bunch of
other talks special thanks to a bunch of
people talked on hallways during
conferences and really helped me to get
this info and a little bit of certain
serve vehicle dekh lunga advertisements
so just one line about it so SDK pay is
a Reading Club we read computer science
papers many of which influenced to this
talk on tripods Cara we talked about
Scarlett acha veins and around the track
of Theo not aware of it I thought I mean
this is basically very conference to
announce we have this fing and cracker
and you should definitely drop by so
this is what I got and if you have any
questions we have a little bit of time
alright so we got that right no no but
but it has all comments are for these
awesome
try to check how many connections HTTP
connections I can make on my boat he
goes by super simple cease officer job
to connect and send ok and give any
close connection that would fall and
what was it very interesting on the next
box then there are a couple programs
that i try to do is estimate how many
calls I can height and all of them
behave very weirdly sometimes there were
problems because some of the kinect
connections were not answered so it was
very very weird especially when i go to
about 30,000 connections and we're
looking looking at a pool pool no it
will just stop the socket all night and
then spoke so it was very simple sweet
on three lines program in c but as i
said it behaves very very very great
under load but i don't know the reason
was that let's Colonel already
programs that can estimate only how much
time you need for the restaurant I don't
know that it's just I tried a couple
different and they all hate me three we
agree this is a super great example of
something I omitted or specifically
because we would run out of time so in a
member of a elephant yes the thing about
it if you are measuring any kind of bad
stuff how many how many requests I can
handle whatever you are being implicitly
back pressured so the benchmarking code
whatever you using to measure the thing
it's been flocking it's surf because
it's waiting for the socket closed and
it won't issue another request because
you're ready for so good clothes so
implicitly back pressuring but the thing
that's supposed to measure stuff so when
you measure that kind of low latency
things you usually need to measure
outside so you need I don't know five
load generators and they do not measure
because they are implicitly back
pressured by however the network works
because most TCP buffers and very won't
proceed writing because the buffers for
so you need a guy from who from the
outside looks at the traffic and he can
properly measure them and yeah lots of
interesting techniques for actually
measuring the right thing and not what
you think you're measuring have any
questions please yeah
you mentioned that the latency is not a
run on a bow we are not able to just
describe it innocent it's unpredictable
this is particular to something which is
called reliability territory you can
predict when they gave in part would be
somehow destroyed already exchanged and
you're talking about predicting yeah how
wrong i measured yeah exactly that there
i'm using it quite from a distribution
was always certainly but that's true
yeah and this is the right how you can
how do you could maybe maybe not counter
like you can get something about it i
would say so it's better but still wrong
if you didn't measure at this present
time you you're making up numbers and
making up numbers it is not a real
system because these numbers are usually
the worst performance you have and if
you missed the worst performances
whatever you make up okay and it's
unpredictable it could be five seconds
yeah it could be exponential like night
with the Scala compiler last week yeah I
mean exponential whatever if you can
describe it to its fine but it could be
completely random super spies I mean the
other signing of the number that is
slightly different that they should be
understood in statistics point of views
of more life and yeah but your vet very
sure about it or not sure about it I'm
not a big fan of us you really need to
measure the exact thing we all so tool
for that and what a tool for that it's
not really solving a problem but
allows you to notice you have the
problem and specifically you guys should
check out gentleness this is Azul as CTO
his tool is called HD or histogram and
it's specifically for measuring agencies
and it is able to well detective but
yeah you're probably measuring wrong
it's basically histogram a good one we
just wanted to know yeah but I'm hoping
that basically mean and deviation
useless not even not enough useless
anything it actually quite me basically
what we want to do is to have mean as
long as possible and reduce alligator
right no you don't care about me mean is
meaningless completely meaningless and
you remember we talked about if I'm
above 200 milliseconds people die you
really don't care about me seriously
you're mean can be five but then you
have these spikes and people die ya mean
is useless
kinda random because in what was our
system okay like white uniform is than
those goes this would actually be
exponential just isn't like what we
should aim for or not I mean yesterday
for whatever us la is if your SLA means
on average we need whatever and we
tolerate whatever spikes we get okay but
but that allows for five minutes down
times and you'll still hitless tonight
perhaps because you on your response
rate is five milliseconds you isolate is
500 when you had five minutes down time
and you're still hit first away really
good for you still miss pass away I mean
it depends we are talking about high
performance systems if you're down
you're losing make it don't do if you're
a blog and you're down yeah whatever I'm
not seeing the block is hurting me ok
maybe a little Astros final house I was
going to give myself one more question
had a courageous question but we don't
find after dragging on the corridors now
element this room about how to actually
build a scale of systems without cutting
stuff so you might be interested
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>