<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tech Mesh 2012 - Beyond MapReduce - Dean Wampler | Coder Coacher - Coaching Coders</title><meta content="Tech Mesh 2012 - Beyond MapReduce - Dean Wampler - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Tech Mesh 2012 - Beyond MapReduce - Dean Wampler</b></h2><h5 class="post__date">2013-08-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/W9bXAxqnY90" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm pologize for speaking in this
horrible provincial accent but you know
what can you do and grew up in America
so anyway I work for a basically it's a
Hadoop consultancy called think big
analytics and and this is really a talk
about what's good and bad about hit do
you know just from years of experience
and then dealing with this ecosystem and
and so I'll describe what MapReduce is
for this week they don't know and then
we'll discuss some of the things that
are good and bad about it and then where
I see the future heading and you can get
the slides will be on the conference
side you can also pull them off the last
link there my polyglot programming
website a shameless plug I've written a
couple of books actually this one is
about a tool in the Hadoop ecosystem a
sequel query tool and I think the others
are self-explanatory and if they happen
to be available in the lobby all right
well we had the this slide was of course
created before the panel discussion do
but I wanted to give you my definition
of big data and I'll let you verify that
the math is correct for some definition
of correct this is a town in Colorado
and the EST is established you know when
it was found but so for me I mean this
is a marketing buzz word but the context
for the talk is data that has gotten so
big for some definition a big that it's
either too slow or too expensive to
manage why don't we have two small there
anyway don't know why that's there too
slow too big are too expensive to use
with traditional technologies and so
people went looking for alternatives and
also behind this talk is three trends
that I that are happening right now the
industry some which I think you know
already but maybe a couple aren't quite
as obvious the first obvious one is that
obviously data sizes are getting a lot
bigger for some people like the
Twitter's and the facebooks and the
googles of the world dramatically bigger
data sets in the last you know couple
decades but even for smaller companies
either they are realizing they've got a
lot of data this
just sitting there that might be
valuable for in some sense for mining
purposes or they're interested in
starting to mine social network working
for you know like Twitter streams as we
heard earlier today in this session and
this track and so they need to manage a
lot more data and be able to do
computation over that data so this is an
obvious one the other one I brought this
up in our panel discussion earlier and
that is that the notion of really
carefully defining your schemas is
diminishing and importance just as it's
sort of a response to pressure of the
world in which we live you know it used
to be you would spend a lot of time
building systems and you'd have enough
time to think carefully about the schema
I always thinking earlier about the
classic object-oriented programming
books that came out in the late 80s and
early 90s where they all use the
automated teller machine as an example
application and what's interesting about
that from this context is that you know
that was pretty much a self-contained
system you know it was a private network
you know that they thought very
carefully about it because obviously
there was money on the line so they had
to be very careful about how they work
so they spent a lot of time designing
data schemas and protocols and so forth
whereas you know contrast to today for
these you know typical big data problems
you're absorbing data from all kinds of
sources the schemas you know are
completely all over the map if there is
even kind of a sense of a schema for a
particular data source the schemas are
changing rapidly so we're finding that
we need to be more flexible and handling
the data that's thrown at us and worry
less about adopting formal
representations of the data and not only
does this apply the database level but
it really affects the notion of whether
we should do object modeling in our
class in our applications the other cool
phenomenon that's going on is a lot more
emphasis on data-driven programs and now
here I'm showing a picture of version 2
i think it is of a car named stanley
this was google self-driving car that
one i think actually was the previous
generation that one the DARPA Grand
Challenge DARPA is the Defense
Department in the US is a research
agency that funds Advanced Research and
they created this DARPA challenge which
was to have an autonomous car drive
through a desert you know it wasn't an
obstacle course but there was you know
rough roads navigate that in so many
hours rather than you know you could you
could pretty easily design one they
would creep along a few inches at a time
and very carefully determine where it is
but if you wanted something that could
actually run at normal driving speeds
and not run off the road that was a much
bigger challenge and this team at Google
and with contributors from Stanford
developed this technology which actually
does a lot of very cool stuff and you
can see some of the electronics on the
top for laser imaging and echo location
as well as GPS data for like maps and
they use all this stuff to avoid hitting
pedestrians and the cars in front of
them and stain on the road and so forth
and so that's just one example of how
we're just driving our world more with
data and and making using very generic
algorithms sometimes very specialized
and sophisticated but nevertheless
somewhat generic about the world in
which they're supposed to reason about
and then they're using the data to infer
the right behaviors or information that
they're supposed to gather from that
data and rich Hickey sort of alluded to
this last night when her yesterday
afternoon when he was talking about your
protocols and how we should focus on
like attribute oriented data you know
like key value pairs and stuff very
similar notion so those three trends
kind of begged the question well what
kind of architectures should we you know
build our systems around to support
these needs well this is sort of the
traditional way we've been doing things
the last couple of decades where you
know we might have some business logic
maybe there's a web server on top of
this thing but it's sending it's
triggering some query that goes into our
database maybe the results come back
through an object relational mapping the
query itself goes through there and then
we we used to go through all the
ceremony of converting this to some nice
in-memory domain model as objects and
then that got shoved back into the
domain logic and you know this sort of
endless cycle and once again that kind
of worked pretty well for things like
the the systems we were building 20 30
years ago the atms and so forth but it's
not so flexible for the kind of problems
we're dealing with today where it's
really a
more sensible to instead of trying to
hide these results sets which are
essentially collections of data actually
fully embrace them and really apply our
energies towards using them in the most
effective way and the most effective way
is to have things like our functional
Combinator's that some of the Guru's in
the room here have evangelized very well
things like your folds and your maps and
you know group buys and things like that
make those available as generic
operations on your Maps lists trees
whatever and then combine those together
to build up the domain logic without
embedding the normal object-oriented
style domain logic and the code if I had
to give you one thing that I think is
the way that object oriented programming
got off the rails so to speak it would
be we sort of bought into this view that
it actually is a good idea to move our
domain logic into our code and a very
faithful representation with all the
same nouns and verbs in fact I think
that tended to create somewhat
inflexible bloat objects tend not to be
as reusable as we thought in part
because everybody has their own notion
of a person their own notion of a bank
account and so they don't interoperate
well whereas it's pretty much the same
if it's a map or a you know a list or
something whether I'm getting it from a
Java process or an airline process or
whatever and so that actually turns out
to be a lot more reusable so one of the
big themes I've found that really fits
the big data space is to fully embrace
collections that the things that the
functional programming community has
been telling us to do for a long time
and really build the kind of Combinator
operators on those collections that we
can then very concisely glue together to
construct or domain logic and the other
thing about this is it addresses the
size problem whereas there's just too
much overhead in the old object-oriented
approach there's a whole lot more
scalability in an approach like this
where we're getting a lot of the
boilerplate out of the way in the middle
and the other thing about this will I
get back to this in a second but anyway
really embracing these data structures
as the core representations of our data
and
not so much relying on ad-hoc
abstractions and representations of
domain concepts that change all the time
now just to fill out the details a
little bit there was a really good
reason why we thought it was good to put
object models into our domain or into
our code and that was the communication
side of things where the developer was
talked in the same language as the
stakeholder you know the marketing
person or whoever it was given us
requirements and I think the nice way of
recovering that benefit in a functional
design is to rely on dsl's so we've had
some talks about dsl's here clean from
Leonard earlier I think that's a
wonderful way to bridge that gap between
these sort of low level implementation
but highly efficient constructs highly
productive constructs and the world of
the domain logic that we have to live in
so I'm a big believer in dsl sort of
above visit the top box there anyway
that's a little bit off the topic of
this talk but just to fill out this
notion of which architectures work best
for big data the other thing I've seen
over the years working in in the
trenches is that objects models tend to
build into these big amorphous systems
where all of the data flows through the
system all the use cases of behavior if
you will all tend to flow through one
massive wad of object-oriented code in
the middle and a lot of people find it
very difficult to tease this apart into
separate services and that's really bad
for a lot of reasons in the main one
being that if we want to scale to
massive data sets we need to be able to
scale horizontally as we all know and
the best way to do that is to have these
little single responsible processes in
other words they do one thing and they
do it well and then we just run data
through them and glue the results
together in some way so if we need to
scale then we can replicate the
processes over and over again and we can
have you know different service servers
providing different processes hey and we
can even scale those services for the
particular service they're doing so this
one is i/o bound we can give it better
i/o performance than this one which
needs more CPU or whatever so this is
all kind of hand wavy about where I
think we need to go but just to recap
how
this fits these three trends this does
better address the need of growing data
sizes because it's much easier to
replicate and scale horizontally with
you know a much more fine grain focused
architecture and then the last you
really are very closely related where if
I'm if I have more agnostic processes
that just know how to work with our
fundamental collections and they're more
data-driven then it's a lot easier to
address rapid schema evolution as well
as programs that really are driven by
the data rather than the other way
around and that gets us back towards
MapReduce where the goal of MapReduce
and the distributed file systems that
were built along with it is to kind of
fit this model where we distribute data
over you know a cluster because that's
the only way to virtualize you know
petabytes of data and yet make it look
like one file system it also gives us
things like failover safety and that
sort of thing but also at the MapReduce
level the goal was to find a generic
processing framework that would mostly
handle the boilerplate of this sort of
horizontal scalability across services
and managing what's going on and
collecting data back together but give
us a programming model that then we can
then write our business logic to so how
many of you actually know what MapReduce
azure have ever done anything with my
producer who do okay most of you so I'll
just go through the next few slides
quickly just to make sure we're all in
the same page and what I want to do is
just walk through an example maybe the
simplest MapReduce program called word
count and in fact it's often called the
hello world of this space because it
whenever a developer writes their first
MapReduce program this is usually what
they write because conceptually it's
easy to understand what's going on so
you can just focus on the API
idiosyncrasies the idea behind MapReduce
is that our sorry word count is imagine
I have four documents or some number
some number of documents and a corpus on
the left in this case they're all very
simple documents and one of them is
deliberately left blank although I
didn't actually say this document
deliberate lift blanks that would be
confusing and I want to count all the
word
that appear on this corpus you first
find all the words and then count them
and then you know output a list of all
the words and their counts and this is
more than just a toy problem there's
actually some utility to this for
example if you're building a spell
checker and your your spell checking
English and someone types th e ex where
you can guess that they probably meant
to type either no fourth character or
maybe N or something like that even if
you are trying to infer what language
you're looking at if you see the word a
lot while you probably can guess it's
English for example so there's actually
some usefulness for this kind of data
set but anyway we need to fill in the
magic in the middle to get from the
input documents to these and there'll be
three output documents and this is the
basic structure of MapReduce where we
have to fill in logic for these sort of
rounded bubbles in the mapper and
reducer phase and you functional guys
don't wince too much when you see what
they actually mean when they use the
word map and reduce it's a bit
stretching the definition a little bit
but bear with me and then there's this
magic in the middle called sort and
shuffle that will have to deal with so
in the Hadoop implementation of
MapReduce if these are text files what
will actually be passed into our code
will be key value pairs where the key
will be the position offset into the
file and then each line one of the time
we don't actually care about that offset
we're going to throw that away we just
care about that text that's the value in
this case and all are what a mapper
needs to do is somehow tokenize this
text into words now in this case
splitting a white space is sufficient
often when i teach MapReduce aight we do
word count on the on the place of
Shakespeare and it turns out Shakespeare
used a lot of punctuation so you have to
actually account for some very bizarre
punctuation so it's not as trivial as
just splitting a white space to do a
good job but nevertheless it'll tokenize
the words and then in the most simple
version of this algorithm every time it
sees a word it'll just spit out a new
key value pair which will be the word is
the key and a count of one and you can
probably guess of some optimizations
which would be just remember what words
you've seen and only write out one key
value pair
word with a final count out of each
mapper that's like the interesting
trade-off there is that's actually more
efficient for Network I yo but it's more
complex in the implementation so there's
a distinct trade-off there this is
logically correct but it's a little
inefficient for Network I oh and now you
can see why I have a blank document just
to emphasize the case that this is not
really a real map it's actually more of
a flat map operation because you can
actually output 0 key value pairs or as
you can see I'm outputting one too many
in fact or zero man is what I'm really
outputting so they should have called it
flat MapReduce but I guess we'll let it
go so hopefully it's pretty obvious
what's going on here the first general
output you know Hadoop uses MapReduce
and so forth down the way now what
Hadoop and and also the original Google
version do next is a so-called sort and
shuffle process where they'll sort all
the keys coming out of each mapper and
they do this within each map or not
globally at this point because that
would be very expensive and then they
just they figure out through some
mechanism which reducer task should get
this i should say this that all of these
bubbles are actually separate Java
Virtual Machine processes in the Hadoop
case and by default it's one map or
profile but if the file gets beyond a
big size then it actually spins up
multiple ones and does it in parallel
and so forth but anyway so either on
some other machines or maybe on the same
machine I'll have these reducer
processes running I've arranged for all
of the keys that start with a number of
the letters a through L to go to the
first reducer and through q to this one
and so forth so you can see that the
first three keys that came out of my
first mapper the Hadoop one will go to
the first reducer the MapReduce
keyvaluepair to the next one and uses to
the last one and so forth and obviously
if we're trying to count the words we
want to make sure that all occurrences
of the word MapReduce for example show
up at the same reducer and so forth and
then we have a trivial task of summing
up what we get so what Hadoop does is it
one at a time calls my reduced logic
with the key and then a collection of
all the values for that key and in this
case all I have to do is some the arrays
and
I doubt the results and we're done so
they in the hadoop world and you can
blame google for this the word mapping
is not quite the word that work meaning
we're used to it's actually 0 too many
output but it's a transformation from
some input to some out set of outputs
and then reduction is more what we're
used to where we want to in some logical
sense reduce all those key value pairs
for a given key down into you know one
output whatever that means and of course
this could be another collection of some
kind of doesn't have to be like a number
or whatever so is there any questions
than that pretty clear what's going on
ok now one issue with that though is its
word count is reasonably intuitive how
to do it but once you get to anything
more advanced like joins and sorts it
gets really difficult very fast so one
challenge with MapReduce is that it's
really hard to translate arbitrary
algorithms into something like this and
it's a very coarse grain model to it
when we think of mapping and reducing
and so end of folding and stuff like
that we tend to think of a lightweight
process on a you know collection that
could be big but nevertheless the
process is small these are really big
processes single JVM steps a whole
MapReduce job is a single map phase a
single reduce phase if you have to
string them together you've got a right
to disk and then start over and it's
just kind of a mess and that's sort of
one of the issues we'll talk about all
right briefly where did we get here or
how did we get here well Google had this
problem that they needed to index the
interwebz which are obviously very big
so that when you ask questions like what
is the meaning of life you get an answer
you know look at that 49 million answers
in point 26 seconds which is pretty
impressive and you know we forget how
amazing this is actually that we can ask
you know this web browsers questions
about the world we live in and get these
amazing answers in you know fractions of
a second over billions of web pages well
of course you all know what actually is
going on there actually indexing the web
in advance they have crawlers that are
constantly looking for new pages and
then you're mapping the terms to pages
this is happening advanced and their
particular version
of this algorithm called PageRank is
what made them all billionaires because
it turns out it's so good but you know
early in the 2000s they they realized
that not only page rank but a whole lot
of the data calculations people were
doing all they'll face the same problem
how do we I've got this big data set I
want to run some analytics on how am I
going to do it in parallel I'm going to
have to you invent some mechanism for
distributing the load and monitoring
what's going on and then collecting it
all back together and so forth everyone
was reinventing the wheel so they
decided it was time to build some
generic infrastructure and they started
with the Google file system which is a
virtualized file system over a cluster
you have two petabyte data sizes and it
has things like redundant copies of data
so that you know when you get to a
petabyte size cluster you're going to
lose hard drives every day so that
cannot be a you know a dramatically bad
occurrence it has to be something that's
just routine and then about a year later
they published this paper which laid out
the MapReduce algorithm for computation
that I just described well it turns out
that about this time a guy named Doug
cutting who was the creator of the
leucine search engine was working on an
open source web crawler called nudge and
he read these papers and realized that
boy this is exactly what I need in this
system so we started working on them and
you know cutting to the chase here by
2006 they recognized that these tools
were valuable enough to spin them off as
a separate project under the umbrella
term Hadoop where did that word come
from and why is there an elephant on the
slide well it turns out that he had a
young son at the time it was an owl at
an older son who had this little pet
stuffed yellow elephant named Hadoop and
actually i think they pronounced it ha
Duke and there's sort of a made-up name
well the amusing story about this
elephant is that after Hadoop started to
get famous Doug cutting would carry this
thing around to keynotes is sort of like
a token of the religion I guess an icon
but apparently the family dog aided a
couple of years ago so that's maybe an
ominous sign I don't know but anyway all
right well before I start complaining
about what's bad about this let me just
briefly
what is I think by this point the Sun
was old enough that he didn't care about
the elephant anymore but it's flea I'm
saw Doug cutting in the conference
recently and I asked him if his son
knows he's famous and he said he does
and but he doesn't let it get to his
head so anyway alright so briefly some
of the benefits well as we've said and
we all know this the best way to scale
to really massive data sets is to do it
horizontally you know CPUs of peaked and
even even if your data will fit in a big
expensive system it's still a big
expensive system and doesn't have
failover and so forth one of the
interesting design goals that they
completely dominate everything about
Hadoop works is to absolutely maximize
i/o performance or minimize overhead in
other words there's some things you
simply can't do in Hadoop because they
want to be able to scan data off hard
drives and not have the disk seeking all
over the place which is enormously
expensive and they want to minimize
network io because those are the
bottlenecks in these systems and of
course the other major goal was they
wanted all this stuff to run on server
class commodity hardware although it's
kind of interesting now that we're
starting to sort of creep back up into
expensive customized hardware to make
this run even faster but the normal way
of thinking is just throw more hardware
at the problem and Hadoop will just sort
of adapt to the extra hardware but an
unfortunate side effect of these design
constraints is that it does make a dupe
really good for batch mode processing
but really sucky at real time processing
so if you need to be responding to
events happening in real time this is
not your tool we'll talk about some ways
to get around that but if you know if
offline you're crawling the web to build
up a web index then it's great for that
sort of thing so this is another area
where its MapReduce has important
limitations we have to talk about but I
should say just you know again being a
consultant you have to think about
what's actually going to work in IT
organizations and this does have a
vibrant community that's moving the
platform forward fairly quickly in fact
it's tough to even keep up with what's
going on and also very important for
your typical IT manager is that you know
it you can get good commercial support
and this stuff has been running for a
long
time in places like yahoo so it's pretty
you know it's sort of a safe bet is what
I'm saying even though it may have its
drawbacks and unfortunately as we were
discussing in the panel earlier today a
lot of people will you know think they
need to get into the hadoop world
because they're missing out if they
don't and sometimes they really don't
have enough data to justify it see
sometimes have to deal with those
misunderstanding of where the sweet spot
is for this okay so those are the
advantages but let's talk a little bit
more about the disadvantages and what we
can do to address them well the first
problem I mentioned is that it's
actually really hard to implement
anything other than trivial algorithms
in this MapReduce model in fact the
people who do this all the time have a
very specialized expertise in knowing
what tricks to play with key
construction and so forth to actually
implement joints or implement group buys
and things like that and I think in the
Hadoop case in particular the Java API
is particularly nasty and much harder to
use than it ought to be and it's very
much an assembly language kind of thing
when you work with it so this is
actually the Java code for that word
count program i just described it's
deliberately too small for the most of
you to read maybe a few of you in the
front rows can treat this and I'm
certainly not going to go through all
the details but I do want you to pay
attention to the colors because after
all in ever since we were like 3 years 3
months old we've been paying attention
to colors right so notice that the green
is type information and of course this
is Java so there's no type inference so
the types are just all over the place
and in your face but it's the functions
that actually do the real work right
there little things that do the
transformations and they're in yellow so
you can see there's a little bit of
yellow here and a whole lot of green and
in fact just one thing I will point out
is this is the reduced code that does
that final summation and you can maybe
see that there's a void or rather a map
method and that's what did the
tokenization and all that and the rest
of this is kind of ceremony and
boilerplate to satisfy Hadoop and
satisfy the Java compiler so what do we
do about this so first let's stay within
the MapReduce paradigm let's suppose we
really want to use Hadoop how can we
make our jobs easier maker sell
more productive well we can certainly go
to functional programming as we all know
and one way to do that is to write
sequel because it's you know basically a
functional of paradigm and here in fact
is word count in the hive query language
hive is a dialect of sequel that runs on
top of Hadoop it doesn't give you
transactions or a record level
manipulation but it is great if I just
want to do like you know group by all
these records and some over the counts
and all that stuff it's great for that
sort of thing now in this particular
case I have to use a few built-in
functions and a little bit of special
stuff that's built into hive to actually
implement word count which isn't really
a traditional query but nevertheless it
can be done the first two lines create
this table that are pointing to some
paths in the Hadoop file system and the
cool thing about hive is if I've got
data like column delimited date or
rather comma delimited data you know
that so that actually has a schema it's
in files in the Hadoop file system and
I'm creating this you know table as if
it's a database on top of it and then I
just write queries like I'm used to in
sequel so here's another fact going
going back to industry and what it's
really like out there I work with a lot
of big IT shops where the number of
developers who could write job as you
know you know a few dozen guys and then
there's hundreds of data analysts that's
sort of working on top of them you know
the guys that do the business analysis
that you'll look at what customers are
up to those people can write this they
have no hope of course of writing Java
so this is like one of the most
important tools in the Hadoop ecosystem
is this Facebook project called hive
that gives us a sequel dialect now it
turns out there's a couple of other
options a very curious thing that's
happening is that hive ql is becoming
sort of a de facto sequel dialect it's
now being adopted by other tools in a
minute we'll talk about an alternative
to MapReduce called spark and they're
using they have a port of five called
shark which is a perhaps appropriate
that's almost close to maturity it's
still a little bit rough around the
edges and then there's a brand new
system called Impala that was developed
by a Hadoop vendor that actually
bypasses MapReduce and it's based on a
google project and will actually so on
the next slide called dremel so once
again Google invents something in the
rest
this copy it uses a very fast back end
in C++ and Java and often gives you a
hundred x performance over hive just by
bypassing MapReduce because its
MapReduce is not good for anything real
time it's great for large batch kind of
problem projects okay so if we can write
sequel that's really a great idea but
suppose we need something more flexible
in sequel like a turing-complete
language well maybe we can just raise
the abstraction level and use an API
that gives us four of abstractions like
data flow you'll piping data through
various transformations and hides a lot
of the complexity of the underlying
ecosystem in one of those libraries is
called cascading and just a briefly
terminology for cascading you define a
data flow which is where you connect
source and sink taps of data you
connected in the middle by pipes of
transformations and so word count would
look something like this schematically
in in the cascading it turns out each of
these bubbles is called a pipe I would
probably use a different term and call
the whole thing of pipe and nevertheless
that's the terms that they use so i'm
going to set up a sequence of pipes that
will do the splitting on some regular
expression into words will group by the
word so here's a term we should all know
i hope and then i'll count each of those
words and now you know start at some
source of data in HDFS maybe it doesn't
have to be HDFS though and then write
the results back and this combination of
taps and pipes is called a flow and this
is what the Java code looks like now
this may not look like much of an
improvement at least in terms of size
from the previous example but actually
this is the whole program except for
maybe a few import statements whereas I
actually left off the main routine in
the previous example which does a lot of
boilerplate for setting up Hadoop and so
forth so this is actually more succinct
and even though there's still a lot of
green type stuff and and interesting
things like group by and each and count
that would actually be like you know
functions on collections with you know
taking anonymous functions as the
arguments so some of the type
information is masking what we would
normal language use is just built in
functions and anonymous phone
but nevertheless a little bit less blood
or plate and a lot more of the business
logic is sort of seeping through the
picture here but still a lot of green
and so forth well the nice thing is
there's actually a wonderful dialect of
cascading called scalding which is a
scala api on top of it this is a twitter
project that's actually getting a lot of
traction now and this is the same
program word count again in scalding and
now we're finally getting to something
that we can get our heads around without
you know a lot of heartache so a lot
less green because the skull of course
can infer types and we need less type
information anyway because mostly now
we're recalling yellow functions to do
work and so we're doing things like flat
mapping you know first we read the data
then we flat map over it to extract the
words and here's the logic the anonymous
function to split the words convert them
to lowercase I sorry split the sentences
just split in a white space again and
then we group by each word and then we
count the words and then write the
output so even without understanding the
idiosyncrasies of this API just by
reading the yellow you can get a sense
of what this is doing which is a lot
more like what we really want to do
right and you know a DSL that expresses
the problem and not gets us lost in the
infrastructure we're working in so this
is actually my favorite way to write
MapReduce programs in Hadoop right now
is scalding although I will reach for
hive if it's a query problem first
because it's the easiest way usually to
just ask questions of data yeah so what
it actually does is so it's a layer we
have the MapReduce job api's cascading
is on top of that and what cascading has
built into it are actually generic data
driven map and reduce tasks that it
configures to run the particular
algorithm that you want to do actually
hide works the same way it doesn't
actually synthesize any code like you
know generating Java code it has generic
mapper and reducer zh that are
data-driven to do whatever the query is
and then this scalding is actually just
calling the cascading api behind the
scenes because it's all java
essentially jvm stuff it doesn't have to
do any code generation it just calls
api's that's right so yes what you would
see if the MapReduce layer is somebody
somewhere on this chain submitted a
MapReduce job to this thing that manages
them and you'll see these mapper tasks
and which are the JVM processes the
reducer tasks and those are just
boilerplate java code that's provided by
cascading yeah that's right so typically
the way you run a MapReduce job is you
would put your code in a jar file and
then submit it through is it's actually
a bash script that submits it and it
just starts at the Java Virtual Machine
and submits that job maybe over a
network to the cluster that's running
yeah so it pretty much looks like normal
job at that point I'll just mention for
completeness a couple of alternatives to
cascading and scalding our crunch for
java and a scholar dialect called crunch
and then Scooby and if you get the notes
there's links to all this stuff but
they're easy enough to Google as well
although the word crunch is not
particularly easy to Google unless you
may be say goob crunch or something ok
so those are solutions for working
within MapReduce but be more effective
at it what if we're willing to depart
from MapReduce itself and one of the
best options right now we say what
started as a university of california
berkeley project called spark and is now
a patchy open source project it's really
an alternative to MapReduce it can it
can work with the file system if I have
my data in HDFS but it's actually much
faster for most calculations because
it's it's much smarter about caching
data in memory and also it doesn't have
quite the chunkiness that the MapReduce
paradigm has in the traditional
MapReduce design so the problem with
this though from practical terms is that
it's not it's not a commercially
supported tools so if you're like you
know a middle manager and a giant IT
organization you're probably not going
to feel safe picking this but if you're
you know more cutting-edge like most of
us in this room it's a good alternative
and actually wasn't originally designed
for machine learning algorithms which
are kind of nasty
right in Hadoop alright well just for
completeness here's what the spark code
looks like it's you know conceptually
very similar to the scalding code we saw
a minute ago once again it's very
concise I I gave this talk not long ago
and somebody asks me about well are you
sure that cascading and scalding or you
know projects are going to live and and
and be viable for the long term and my
answer was well actually I think that
they are going to be viable cascading
actually supported by commercial company
and scalding is pretty heavily backed by
Twitter but the fact is if I wrote this
program I would not care about throwing
it away if I had to throw away scalding
or in this case spark because it's so
small and once I know how to write these
I can knock them off test them so
quickly I really don't care about making
that risky bet on something that might
disappear you know maybe in a month or
so and I think this is the case they had
the fewest number of explicit types we
just have three here and one of them is
just the Scala object that wraps the
whole thing all right well the next
problem is it's not so great for real
time event processing so what do we do
if we want to solve this problem well
one of the interesting things it's
emerging is another open source project
called storm it's specifically designed
at real-time event stream processing at
scale with reliable messaging and all
that this is also this is actually
invented by a guy who's now at twitter
name Nathan Mars and if you if you know
anything I haven't been mentioning
closure alternatives to the scalding
api's and it's not i'm not really not
trying to be biased but there's a great
closure api for cascading called Casca
log and he also wrote Casca logs so
storm is is is also getting a lot of
attention and schematically this is sort
of the terminology they use data sources
are called spouts and then processing
logic that you combine together are
called bolts and the output of these
bolts could be files they could be other
services message queues maybe HDFS
whatever you need to write to so this is
one way to do event stream processing
sort of analogous to message queues that
we've had for a long time there are some
database options though so sometimes it
actually makes sense if I need to write
data quickly to some store and get it
back out quickly maybe a record at the
time in a database makes sense and you
might even use sequel or no sequel it
turns out that some of the no sequel
databases have pretty nice integrations
with MapReduce now so if I need to do
MapReduce jobs because I have these
weird analytics that don't really fit a
sequel query but I really want the kind
of transactional behavior for some
definition of transactional I want fast
handling of data then I might use
something like HBase which is built on
top of the Hadoop file system or
cassandra or one of those other no
sequel databases and certainly rioc and
and mongodb are also addressing these
markets so that I get the most of the
best of both worlds that way I think
this is the last issue with MapReduce
and that is it it's really not very good
for graph processing algorithms so for
example page rank is really kind of a
graph traversal problem right and
MapReduce was invented in part to
address it but it's really not that good
for graph algorithms and conceptually
this is the reason why for example one
of the graph algorithms is bulk
synchronous parallel where I'm going to
allow each node to send 0 or 1 messages
to its neighbors and then all of those
in synchronously they'll send messages
at the same time they're asynchronously
i guess but anyway in one iteration and
then each node might update some state
or whatever and then the next iteration
will be the next series of messages and
so forth the problem of doing this in
MapReduce is that either I'm going to
have to start a new MapReduce job for
every one of those iterations which is
very you know heavyweight or I'm just
going to have to just use MapReduce is
kind of a management tool and just
completely subvert the paradigm and have
my mappers and reducers do something
entirely different on the inside which
is what people often do and once it
actually just to make the point clear if
I do a separate MapReduce job for each
of those iterations I actually the way
it works is I have to write all that
data to disk you know save the graph to
disk and then read it back in in the
next iteration so not very fast so why
not use some sort of graph processing
engine well it turns out google wrote
one of their own called prego prego Liz
actually
name of the river and what was
königsberg prussian is now Kaliningrad
I think Ukraine and it was where graph
theory really got its start when Leonard
Euler pondered the problem you know
there's seven bridges over the river
that run the prego river that runs
through this town and there's like an
island in the middle can you actually
cross all seven bridges without
backtracking you know that's sort of
math problem and you proved that you
couldn't do it so in fact that was sort
of the birth of graph theory and graph
traversal algorithms and all that so
that was levy chose this name prego it
uses this bulk synchronous parallel
paradigm the intention was actually to
use prego for all the PageRank but I've
actually heard that they haven't quite
made that transition yet and I don't
really know the reason why some of you
may know graph algorithms especially its
scale are particularly hard to figure
out for example if you have a very
complex graph with a lot of edges where
do you split it to move it to different
clusters and how do you handle the
overhead of sending messages you know
where the edges are crossing a machine
boundaries so there's a lot of issues
there however if you want to pursue your
graph problems there are some open
source alternatives the first two are
also based on BSP and then Arrelious
Titan is a different system that
actually runs on top of Cassandra I
think as its story okay so that's one
possibility though for dealing with
graphs all right well to wrap this up
let me let me express a manifesto the
first is that I think Hadoop is the
enterprise javabeans of our time any of
you actually ever do any hav programming
somebody's smiling okay so you may
remember that that was a bit clunky and
heavyweight and you know you sort of got
your work done but it was kind of
obvious pretty quickly that it was the
really the wrong way to go and I think
this is kind of the same thing with
Hadoop it's kind of a first generation
technology it's been very successful in
the market it's gotten a lot of work
done for people but we've seen it
there's some real issues with Hadoop in
particular and MapReduce in general so
what do we do about this well I think
it's really essential that we stop using
Java for this stuff because this data
problem is really a functional problem
it's mathematics we're applying
mathematical operations to data so we
should be using tools that
really express those ideas on the right
paradigm and I certainly people have
made Java run very fast for particular
problems but it just exposes all the
wrong abstractions I've argued that
objects are really the wrong abstraction
for this and the collections library in
Java are nice in their way but they
don't give us things like folding and
mapping and reducing and all this stuff
we're used to and of course we're only
just now starting to get closures or
it's not even closures actually it's
just lambdas in Java which is coming in
the next release so I really do think
that we should just embrace functional
languages because they're the ways that
which we can most easily express our
paradigm yeah the things we're trying to
do and one of those might be sequel and
also I think it's really important for
us to embrace collections as really the
coin of the realm or the communications
medium whatever term you like and the
operations that are available on them
that lets us do some very sophisticated
transformations and combine them
together to get our work done very
quickly I mentioned that MapReduce is
kind of a very clunky coarse-grain
paradigm and I think the other thing we
need to do is embrace fine grain compute
models and one of the best ones is the
actor based model that's embodied in
erlangen acha so I really there's
actually there's they're sort of an
opportunity in fact MapReduce is pretty
good for this very large data sets where
maybe I don't care that it takes ten
minutes for each reducer to run but what
about these middle sized datasets you
know like the one to ten terabytes where
it's just a little too big for
convenient use just on one machine
especially like a machine learning
algorithm but it's not big enough to
really justify could do so I think that
what we're going to see emerge is more
use of tools that are much lighter
weight have lighter weight abstractions
fine grained controls and Combinator's
you know to build systems that address
the middle market and then sort of eat
their way up into the larger market
meaning larger data sizes as they get
more mature so final thought this was a
tweet that came out recently from a
friend of mine with the new tech mullet
simple mobile interactions up front big
data in the back and with that I'll take
questions
stunned silence yes so you mentioned
that um I do this is great for batch but
not very good for interactive stuff
don't mention MapReduce which among
other things is actually well structured
for answering kind of queries that go
into Google did you know does Google use
MapReduce for that yes oh the question
was and we I forgot we have a microphone
here but this google you actually use
MapReduce now so my understanding for
answering queries so I think the answer
is no but with one of the things I
didn't show that a lot of the no sequel
databases were inspired by yet another
Google paper about the same time early
2000s on their so-called big date a big
table a key value store so I think what
they're doing with the output of the
MapReduce calculations for PageRank is
we're storing that data in BigTable or
some store like that where they can do
fast queries fast distributed queries so
no they don't do it for any sort of
interactive real-time use although I
think their version of MapReduce is a
little bit more responsive and maybe a
little closer to being suitable for real
time than the Hadoop version mostly
because it's like you know c++ and very
lightweight and they've really worked
hard on it to optimize it for
performance I didn't mention the early
in Hadoop it was adopted by yahoo and it
sort of grew internally as a yahoo
project and around that 2006 time frame
i mentioned yahoo announced publicly
that they were running a 10,000 core
cluster with the dupe and that was sort
of when it suddenly became something
that everyone was talking about but once
again it was just doing indexing of the
web in the background not serving up
real-time queries now there was a
question in the back
he was asking that there is a streaming
MapReduce a system that someone has
worked on I haven't had any experience
with that so I don't know how well that
works you know you could imagine there
are ways you could make it a lot faster
like why do we start up a JVM process
every time and there's actually some
options that you can avoid doing that up
for every case but so this there's
actually some work that could still be
done in MapReduce to make it more
responsive which I think will happen
because there's a lot of pressure now on
the community to address this problem
other questions
yes the question was what about industry
adoption for the alternative concepts as
opposed to just going with a stock
Hadoop and the answers pretty much what
you would expect in fact a good analogy
was what happened with spring the spring
framework which was the real answer to
EJ B's back in the day about a decade
ago now and at first it was the more a
leading edge thinkers the more
experimental kind of people who are
willing to take a bet on spring and
question the wisdom of you know the
canonical stuff you're supposed to be
using which was EJ bees and that's
pretty much what's happening now so most
people especially if they're just
testing the waters especially if they're
in larger organizations tend to be more
conservative and want to go with
something that seems to be well tested
and commercially supported so they have
someone to sue you know when things go
wrong but then you'll find organizations
or teams within larger organizations
that will you know they'll experiment
with cascading orcs which is actually
pretty safe or experiment with spark and
some of the other things but certainly
the you know that got people like
Twitter and Facebook and all those guys
that have really been living on the
bleeding edge of a big data have already
you know they still use MapReduce for
what it's good at but they've definitely
got a very heterogeneous environment now
of a lot of different tools to address
the different problems they have anybody
else all right thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>