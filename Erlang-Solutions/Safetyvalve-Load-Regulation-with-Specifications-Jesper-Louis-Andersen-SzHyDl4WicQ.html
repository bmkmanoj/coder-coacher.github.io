<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Safetyvalve - Load Regulation with Specifications - Jesper Louis Andersen | Coder Coacher - Coaching Coders</title><meta content="Safetyvalve - Load Regulation with Specifications - Jesper Louis Andersen - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Safetyvalve - Load Regulation with Specifications - Jesper Louis Andersen</b></h2><h5 class="post__date">2013-07-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SzHyDl4WicQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah so yeah this talk is about
tool you can use for load regulation in
inner line programs called safety valve
safety valve OHS all of its existence to
wolf which is sitting right down here
because he wrote a application called
jobs a couple of years ago and he'll
which I didn't want to use so hence I
tried to write my own no I really we had
to investigate different paths this is
why essentially i wrote safety bell but
good we'll be talking about job snack
right next after this talk so you'll get
to enjoy that as well this talk is a
little bit a little bit about what loke
regulation is in jungle then how you use
the safety valve thing and then how this
safety valve thing is tested because the
main focus of safety valve is to have
proper tests of the load regulation
framework and the reason being that this
becomes a very simple part of your
application so it better has to work
correctly otherwise your systems host so
the basic idea of load regulation is to
protect your Erlang system against the
overload situation so you have a system
it's built to handle say 40,000
simultaneous connections and all of a
sudden due to you being popular and
hagun user something you have 60,000
connections you have to handle all of a
sudden now how do i see your system act
or react in that situation does it
handle the 40,000 and then deny the
extra load of 20,000 connections or does
it just degrade into a point where no
connection at all goes through because
it tries to take more load than what inc
it can handle or does it just crash do
something really horrible so the idea of
Rollo protection is to basically build
us build a system inside your big
application that allows you to limit
load on the system so you can keep at a
certain level of load and you can
certify your system to work at a certain
level right so that's step one and I
tend to call that local regulation the
second part is load normalization that
when you get in requests they might
arrive as spikes it might not be a
uniform distribution where
qwest arrived at uniform rate no they
might arrive all of a sudden a lot of
things in a short time frame and hence
you want to queue your excess spike and
then distribute it Oh a little bit of
time so you normalize the load on the
system and this can have impact because
you might have a process set up inside
there that takes a little bit of more
resources when it's set up but once it's
running it might go back and have a
state of resource consumption so the
spike then works like an amplifies you
might run out of a certain reason was
quickly unless you have some kind of
normalization going on so that's the
basic idea of doing a load protection on
the system safety valve is as I said a
load regulation framework like jobs
there isn't a building or low protector
in the OTP Susan called overload not
sure how many uses it but it's there and
it's kind of like the same idea the
basic idea is that you ask these systems
are whenever you get a request into the
system you ask these systems is this
request allowed or denied and based on
whether or not the overload protector
says that I'm in a low load situation or
it says I'm fine you get to run or
denied the request that's the basic idea
and the basic basic gist of it they're
different in the way the work and they
are built for I think different purposes
so I think both safety valve jobs into a
load have their uses it's not that one
is better than the other and they are
really built for different things safety
well works by tracking essentially three
main things it tracks a concurrency
count which basically means that it
tracks for a given type of task inside
your system how many workers are there
of that test type so basically that's
the first thing it tracks what is my
current concurrency count for this given
task then it tracks a queue for if it
hits the concurrency limit then it can
begin queuing extra work up to a point
there's a queue size and if the queue
size grows for it'll begin denying work
by a tail drop by default so that's the
second kind of metric you have inside
the system next to the concurrency
counter then there's a frequency
rate at which it allows work to DQ from
the queue so the rate defines how fast
it'll be able to do things and this has
to do with the normalization part that
we normalize the dough so we do not if
all of a sudden all of our current
workers there just finished likely to
snap then we do not want all of the cute
workers to come into the system at the
very instant we want them to come in at
a steady rate the way it works
internally is that it is what is called
a token bucket regulator so it has a
gender of a process that basically sits
and adds totems onto a pocket and in
order to get DQ'd you need to pair
yourself with its own the tokens pours
into the system at a steady rate and now
the idea is this is the rate-limiting
but it does allow for small surpluses of
extra tokens if there's nothing in the
queue ready so in the case that we have
no work we allow for small surplus of
terms and that's very typical of token
bucket regulators the idea being that if
the system has been idle for a little
while then we might be able to take a
small spike burst at the very beginning
of initial work so this is why you allow
us more surplus of extra tokens in the
chocobo greater later when the system
starts off so the thing is that the O
Lord thing built into OTP has no queuing
facility it's basically you ask it it
says yes or no based on that you get
whether or not you are to run another
one so essentially overload works with
the queue size of 0 for some workloads
that is exactly what you want you do not
want to queue because if your
expectation is that there will not be a
free resource in a short amount of time
there's no reason to try to queue at
resource if you expect there to be five
minutes before the resource becomes
available then it's better just to deny
him right away and but say you have a
lot of small web requests going into the
system and they are very very short in
this time span they have then it might
make sense to cue them for a little bit
in order to basically take them a little
bit later because the churn rate on the
system
pretty high in that case so whether or
not how much q you need and whether or
not you you need any q can be based upon
the fact of what is your current rate in
the system and that that that
essentially is a difference between say
overload and safety valve sales to valve
allows for this cueing over or doesn't
but for certain workloads it's
completely okay not to have this kind of
queuing what I did in safety valve a
couple of months ago was that I can't
looking at cuddle or control delay so
how many here has heard about the word
puff up load in TCP there's a couple
that's good so if you do not know what
Buffalo deals you should go and look it
up it's important it's very important
it's a problem that TCP hells and is
actually kind of related to what Steve
was just talking about with my TP and
the reason why he got added on top of
the patron protocol or they exchanged
TCP form for my TP in the patron
protocol when we joined system exactly
for this reason the problem has to do
with that in the internet network we
have buffers in front of each router or
switch the system and especially the you
a dsl modem at home has a buffer network
cards has buffers usually a thousand
packets or something like that they can
take a thousand packets on a buffer
because they are 10 geek interfaces
today and all these buffers means that
when you operate tgp you will begin
having what is called standing hughes in
tcp these buffers will be half for all
the time or full all the time off to a
certain level and since TCP is only
using packet loss as a way to determine
whether or not it has troubles it means
that these buffers fill up and that you
will have a standing queue in the buffer
all the time that's the gist of the
problem now these standing huge they do
have a strong x over the q you have to
go through all the queue before you get
DQ'd and go on to the next hop on your
route this means you have just encourage
latency on your link right so that's one
problem and this is what for instance
hurts real-time communication or the
internet because all these standing huge
just adds to the latency all the way
through what is worse is the small adsl
modems you have at home because they
usually only have a 10 packet buffer and
as soon as you have a bit try and
running it'll fill up that bother with
all its packets and now your skype core
cannot go through and now you're in
trouble so let pad is one idea which D
talked about my last session that passes
one idea for tracking this by tracking
the delay on the packets and fan
Jacobson and Kathleen Nichols has this
coral algorithm as a way to solve it and
and the most important thing I think
that that what let jay comes on or God
as we tend to calling onto this thing
was that he figured out that in most TCP
systems packet arrivals are not Poisson
distributed people would think that a
rail rate can be explained by poisson
distribution mode most academics would
use that distribution but it isn't true
data is right at is arriving and is very
dependent and it kind of makes sense
because say you are in an HTTP webserver
and I'd make the first request towards
it on HTML page well write next after
there'll be a request for the JavaScript
for the JavaScript for the CSS for the
JPEG images and so on so this dependency
here going on the first request
essentially spawns other requests that
will go through and that is also true in
TCP I have a window that I can fill up
so when I see that I have a knack back I
can fill up the whole window so things
will not arrive in a general Poisson
distribution and that is a very
important thing when you look at tgp at
least so my idea was to say well this
control delayed queuing is interesting
let me try to apply to the queue inside
safety well because that might actually
be useful
the problem here is that if I have a
large standing human safety valve I have
the same problem that i'm waiting in the
queue until i can get to do work in my
system and that becomes a problem
because then i have to tune the queue
size but i do not know the queue size of
my system bit unless i initiate that is
a problem i want a system that is
self-tuning so coddled is the sales
tuning algorithm hence I thought well
little made me try to use that so what
is the basic auto trick and the basic
coddle trick is that whenever you end up
packet on a queue you take its time
stamp and whenever u DQ the packet you
have a time stamp as well and the
difference between those two is called
this a joint I'm that's the time we
spend in the queue now note that
compared to other systems like a random
early detection and other active queue
management algorithms this tracks the
time in the queue it doesn't track the
size of the queue it doesn't track how
many bytes I'm the queue it tracks the
time we went through the queue and then
very simply if we have a sojourn time in
the queue that has been too high for too
long given by a target we set we begin
dropping packets we begin front dropping
from the queue begin dropping work out
of the queue and that should then pull
down the queue size again so we begin
selectively randomly denying work if the
cube goes too high and that means we can
define a target we can say we want to
spend at most 25 milliseconds in the
queue that's our target and then this
algorithm will basically begin doing
this kind of management on the queue so
if we get as a joint I'm that's longer
for for too long it'll begin dropping
out work until we get down again so this
is the basic idea and the only thing I'm
missing currently and this is why the
experimental is that the producer must
react work rejection by lowering its
initial in st. rate into the system so
it's only useful if you have feedback
mechanism like in TCP where you where
you can essentially turn down your rate
because that is what happens in TCP when
this happens essentially this system is
sitting and watching over the queue size
and when the queue size and TCP grows
too large
fill a packet but that packet loss then
feed backs to the TCP protocol that
cheesy people they will then say well
let me close down the window because
obviously there's loss on the line so it
must be congested and that is exactly
what what fixes the problem with TCP now
in order to use it completely in safety
valve you will need to have some kind of
TTP like additive component with kind of
like a window you want to spawn say if
you have control of the work respond you
have to feed back up and hope that the
guy you feedback to have some way of
stopping the bumping on the system right
so there's a control for everything you
have to do but it can tell you when you
have a too long sojourn time and when
you should when it will begin rejecting
work on the queue so I said this and the
details is in a azm article called
controlling q delay by and jacobson and
katie nichols it's really worth reading
if you're interested in inec anything
with ggp protocols and stuff this is the
paper isn't really to read they
essentially take the survey of all of
what they did in a couple of years and
then have built the essence of it into
into this paper so it's really worth
reading so it's currently in a quiet
experimental feature inside a safety
valve and it's not that used right now
but I hope to build the part that does
the TCP at the additive stuff onto on
the front of it so you can really use
this it ties together with all these
ideas all these ideas of having to when
needing to do something about your q's
tail driving a queue is wrong you'll
need to do something else you need to do
act a few management and use when they
become full and the reason is of course
that that it takes time to walk through
the queue it takes time to adjourn for
the queue when that means that you have
a standing latency by the way this is
what node.js is missing
so how do you use it well this is the
configuration part you would write in
your config file in the organ system it
defines that I want a queue here with a
name and then i have a problem that sets
some settings this is more or less
modeled live from from jobs jobs has
kind of like the same way of setting
cues so I'm totally in depth of tool for
that and you said a raid at which you
want to recognize the tokens in the
token bucket regulator you said how many
tokens you want wrapping is with there's
a limit on the burst so if there's no
work you would at most have 30 tomes
ready to take the initial burst the
queue has a size of 50 in this
configuration and you may add most have
32 concurrent workers at a given point
so this is a very limiting construction
and the way you use it is suppose we're
that we have this with postgresql on a
query phone function and this is
probably not crash safe but please
ignore that and so the idea is I obtain
some connection I run my query fun and
then I put back the connection into a
pool and I want to protect this pool
this in this case I want to protect some
foreign system against an overload
situation so this is there's two ways
you can use a low gradual ater you can
use it to protect your system from the
outside by telling the outside that I'm
overloaded or you can protect one other
system that cannot cope with your load
which is very common in early so in here
I want to protect the the foreign system
by imposing some kind of regulation on
the foreign system and then the then the
second part is that how do you run it
whether you wrap your with PG coral in a
one and then you call safety well run on
it this is again very jobs like and what
you get back is either that you get okay
rest back if you have a result which is
okay in here or you'll get some error
with the reason and the reason could be
that you are denied to do the job
because the system is currently
overloaded or you're denied to do the
job and and and the queue is full
there's no more space in the queue or
you are denied doing the job because
cuddle has told you that it is it has
dropped it has dropped you off he's
injured so you're not allowed to run for
that reason and internally it works by
you having a gin server and you ask it
to work and then when you get an okay
back you're blocking on that and well
then when your process gets an okay back
you can do its work and then you can
tell back that it's done so it's an
ass-ton pairing that you use internally
in order to in order to solve this and
yeah so this is more or less how the
system bronze but that is you can say
one of the more interesting parts and
the reason why I wrote this exactly this
manifesto and this system is more or
less it has a rule when you want to
write programs when you want to write
this system you're writing them
according to this manifesto and it is
expired inspired by idomef'em eclipse
which is a Danish set of movie makers
and they put down some constraints on
themselves in order to produce art and
the things worth for instance that you
all need to use a handheld camera you
may only use natural lighting and you
only have one take passie stuff like
that so they imposed a lot of
constraints on themselves in the hope
that they could produce better art by
doing so so let us do the same for
programming right so the first thing is
in this manifesto you're only allowed to
write code when you know how to test it
that's pretty standard but
and a test is a quick check model a test
is not a unit test it's a quick check
model essentially I want to banish
unitus I hate them with a passion so a
test is a quick check model of what we
are trying to do to achieve and then
there's a rule that says that there's a
duck ruling principle you may only write
a feature when somebody has has the need
for it you're not allowed to write it
before and this is because I see so many
libraries where people just cram stuff
and stuff and all we could also do we
can also do in some wit and the result
is just that you get a blow to miss
nobody can maintain so there's a dog
fooding principle and then the final
thing is never discuss indentation and
this rule actually is stated a bit
different in the manifesto it says that
you're only allowed to discuss
indentation when all other important
decisions has been solved that includes
design algorithm choice data strategy
choice etc right so essentially never so
how do I test it well the basic
philosophy that I use is that many
people who try to do quick check stuff
they start out by saying let me build a
complete specification of the full
system and that usually fails and the
reason it usually fails is because your
full implementation is essentially the
thing you're trying to test so your
model ends up being the same as the
system under test then you're testing
nothing you have to equal and
implementations nothing is gained so
instead what you want to do is you want
to have many many many small very need
quick check models and each model only
tests a single aspect of your system
under test so you're essentially hitting
it from all kinds of directions and the
hope is that one of your small model
tests actually hit something important
in there and then you want to have quick
check and the random model generation of
quick check as an amplifier on your
simple test so you write a simple native
test and then you use quick check as the
amplifier by randomizing this test all
all over and over again and that
randomizes the amplifier that makes you
able to actually do
really interesting stuff and find a lot
of boxing record so always prefer more
tests that's the generic file haces
rather than having a simple one single
one that has all the specifications so
the second trigger is simplify and when
you do this you want to simplify stuff
simplifying stuff means that the first
trick is that one of the things you
could do in quick check is you would say
I do not care about results I do not
care about postconditions i generate
random commands I fire them all through
my system but the system was not crash
the system should be total it should
return in time and it should be crass
free that's your first test that's your
first quick check model totality and
that's very easy to write but it will
check that all these commands I bumped
the system with with random crap will
not crash the system right so that's
that's the first trick you can you can
always write this test almost all the
time even though you try to grab for
something higher later this would force
you to have a model in the specification
and the second trigger use is I make a
degenerate model so in this system we
have concurrency we have queue size and
we have the size of the token bucket
regulator the rate of the token bucket
regulator and how many and then also how
many and surplus tones we can have in
the pocket regulator so these are three
main values so we limit them we say the
queue psychic queue size can be 0 of 1
the concurrency count can be 0 or 1 and
the debarker size can be 0 1 so we limit
them down even though the system in
reality can handle arbitrary national
numbers in these positions we build a
test without that just to keep it simple
and this will probably find errors
anyhow in fact it did so that's the
second trip the first trick is that you
have a queue inside here so you might
think that your model should track what
is inside the q what is not inside the
queue but you can simplify you could say
well let us just track how many elements
thank you we do not care about order we
do not care about when things end up or
when when things leave we just care
about how many are in there that's a
grave simplification but chances are it
will find things and we can write us a
separate test the test that our Q is
actually preserving order and that is
actually a simple test right because we
have a cue module it OTP so we can just
use that as a base that is our model and
then I was just them on the test is the
faster Q and then we just run them
intent them and check that they produce
the same result so it's easier to check
the queue for ordering so why add that
to this more complex specification right
we even skip that so that's another
trick simplify by only counting stuff
you have a dictionary what is the size
of the dictionary chances I it might
find a part the second the third trick
is that you are allowed to hack your
code all that you like so i often add
specific query commands to my systems so
i can query the internal state and get
it out in some simple pre-specified way
that helps postconditions i can ask what
is the state of the system while it's
doing stuff that helps a lot and so feel
free to hack your code in ways that
makes it possible to do more efficient
testing that's essentially what you want
to do here time is usually a problem in
most quick check tests and the reason is
the time is something that happens in
the system wide your test is running and
you have no control that is a problem
because you do not have control of time
and your model wants control of time so
the trick is to inject time you let the
model generate what time it is and then
tell the system what time it is there
are several ways of doing this and one
way is to use a tool like Mick and then
hack the OS timestamp function and
decide what time it is in the running
system but then time won't progress and
so that is one thing to look out for the
other thing you can do which I tend to
do is nowadays I make my systems
actually take a timestamp value as an
extra parameter and then the default is
that you do not supply the parameter so
it takes always timestamp 0
our depending on what it needs and then
it gets that into the call and then
things work but now the test case the
model can will write the time and say oh
the time is by the way this so now I
control time in my model and that means
I can have an advanced time call in my
model that tells now time advanced now
something happened with time that allows
me the freedom to control time from the
model and that simplifies my my system
under test and this again goes hand in
hand with hack your code so it's easy to
test or easy to build a model for it the
way I this system is kind of like I have
processes that I need to go and then you
to hit a jenn suhr and wait for reply
and stuff so I have a very non
deterministic system basically and that
ways to handle that with quick check but
this is the low-level way that would
work basically everywhere what I do is I
issue my commands and then I wait for a
fixed point basically I look at
processing for for certain processes and
I make sure that they're all sleeping
blocked on receives and not doing
anything I track all the reduction
counters for those processes and then I
check again that reduction counters did
not bump in the meantime and if they
bump I'm not at the fixed point so I
keep on running until I hit the fixed
point that's the basic idea so I'm
making sure that when I issue in command
is essentially issue in command issue a
command to the system and then I told my
thumbs until the dust settles when the
dust settles I can issue with the next
command so I'm basically waiting around
until this happens I could wait with the
time and say sleep one second but that
would make my test cases low yeah and
the situation here is that I know
exactly what processes I have so I'm
only interested in the set of processes
that could affect the system in general
you cannot do this you need to know what
set is eligible for changes and you need
to be sure that nobody outside can plot
them because in that case you wouldn't
be able to do this so that's an
important point yes you need to know
what set you have for doing this
creature can do stuff with this as well
so you can actually you
quick check to do some of these things
and that there are other ways around it
but basically you need to know when you
hit the fixed point and then one of the
tricks that I also utilize a lot is that
in Crete check you have these data
models which basically means that I have
a series of commands and the commands
are dependent on each other so the first
command defines what the next command
can be next command defines what the
next command can be that that's the
state and model for the ricotta model
state is stateful track state between
your commands and but it also enforces
you to do complete postcondition checks
you have to check the post condition at
each step of your chain so one of the
tricks you can do is you can actually
say well what I need is I need a system
that is in a valid state and then I do a
single command on in and check that the
single command has a valid observation
so how do i generate an initial valid
state well I can construct it directly
by defining a generator that constructs
that thing I could do that in quick
check so i could say let me build up a
structure which is valid but that's hard
in general and a better trick is
actually to say well let me start with
an initial state and then just product
randomly with commands but ignore the
output that means now I have some state
which is valid because I generated it
only with the commands that I can issue
to the system and now I can do my
observation on my generator gremlins the
thing but I'm not checking all this all
the states up to that point I'm only
checking the last step and that usually
way simpler than checking or every step
on the way so sometimes that works the
coral thing inside is checked by that so
basically it generates a random sequence
of insertions and queuing and then at
the final step we try to DQ and then we
do an observation based on what that TQ
should do
so the main queue is checked by by by
this observation this observation is
that you have three bits of information
namely you have the queue size which can
be 01 you have the concurrency count
that can be 01 and you have the pocket
size that can be 01 so these are the
three possible values you have in there
and that gives that free command you can
do you can tell the time has passed so
the pocket gets recognized with new
tools you can ask for work that is we
can have a worker that goes in an ass
for work or we can have a working work
that can only be one in this case that
says I'm done so that free bits and they
are that gives eight different bit
States and then that free commands that
gives 24 possible cases and then you
call this a bit and figure out what
cases are actually the same case and
you'll figure out that 10 cases to
handle so those 10 cases can then be
done in a static model so here's an
example ck and tears the concurrent
account which can be 01 the queue size
which can be 0 1 and the tone count that
can be 01 so one of the cases is this
there's a concurrent account we don't
care about and we are trying to cue a
new packet that's the queue here and
there's nothing in the queue and we have
no tokens left in that case the only
thing we can do is to cure the packet so
the transition for the model is there's
one thing in the queue there's still no
tokens left and we did not do anything
with the congruence count right this one
is the other typical example you are
done and there were at open so in this
case we have one worker that is working
is saying I'm done we have one in the
queue that is waiting to do work and we
have a token we can pay him up with so
the result of this is that the guy that
were in the queue is now the guy working
so that stays one but there's no one in
the queue anymore so that goes down to
zero and we took the token and paired up
with the guy in the queue so that goes
down to zero so you build these
transition diagrams which defines how
the state transitions in the system are
and that ten of those and then you
define a model for those 10
and and then what you do is you randomly
run this these state transitions and
whenever you do it you query the
underlying system and ask it are you and
sync with what my model says and note
that we do not care about ordering on
the queue in this case it doesn't matter
but we do not care about all these
things we only care about the basic
things and that is the count so now we
have a model we can build up we can
check that our system under test
actually behaves like this model is
saying and and then you generalize now
we have a simple 01 thing so we
generalize we say that can be K elements
in the queue and the term count can be T
and C is arbitrary in that case you the
old one we had this year one case but in
the general case it's basically minus
one and a minus one right we move one of
the work one of the one of the things
from the from the queue or into the set
of concurrent workers and the more
general one is you more because you have
more that can start and then you add
time to the model so now time becomes so
I have a basic model now so I can begin
augmenting the model with more and more
stuff so you have time you add arbitrary
process crashes who shoot random
processes and check that the system
survives and so on so forth so with this
simple 01 model bitwise model I can
start with that and then I can extend it
systematically by doing these things so
what is the outcome of this well it
already found four or five six seven
bucks in there and the code is very
small there's less than four hundred
lines of code in this project and still
it finds six seven bucks and I've run in
30,000 40,000 tests every time I did to
change something like that just for fun
it takes a launch something like that
and the coral stuff also has an equal
see model but it's it is simpler because
there's not that much I can check inside
it but I can check that it doesn't crash
I can do my simple post condition check
which is nothing it's trivial but
nothing must crash in there and I do
check certain simple things with that if
the
if coddle is dropping then actually
there's a drop packet along the way and
stuff like that and I guess I'll take
questions but I can also show code if
needed how are we for time mesh five
minutes then I will just take some
questions and then I'll show code later
Yeah right right right
right right certainly but you need to
come up with a quick model for it yeah
and there's considerable overhead with
the solution I have a single gen server
on which I hinge everything I thought
about trying to avoid that but I wanted
the model first I thought that the
models getting the model for this was
more important than getting the getting
the speed there's ways by which is so
there's always by which you can speed
this up one is that you can go
approximate if but then it's very hard
to define a model point and so you could
do it approximately rather than doing it
explicitly I'm trying to make the fast
path where you are allowed to run very
small so I'm trying to make that fast
path as small as possible inside the
system and there's a reason for that
because this is the common case if you
get queued for some reason you're
probably going to be cube four
milliseconds anyhow so so if you get
cute it's not that much of a problem if
the problem is if you get allowed
because that is actually the fast path
but where I'm putting on latency right
but I haven't really measured up to more
than 5,000 10,000 in the limit something
like that it it works kind of okay up to
that level I do not know if you have the
40,000 it depends on your rate into the
system and if your rate is too high i do
not know what will happen it'll probably
be slow that would be my guess there's
very little tuning in there so one could
chew a lot and now we have the model so
tuning is simpler right you just rerun
the check and if it works it properly
works tom
ah is it's there's no link yet and it's
not on my blog it's on github in the
safety valve repository gay Lewis last
safety bar on kiddo yes oh yeah more
questions yeah right right right in some
situation cheese it depends right some
situations is proper with us yes well 2
q's means to jen's whoever's inside the
system currently there's no
cross-pollination between the two Q so
they act completely independently of
each other but I know jobs can do much
more here it can actually watch over the
note and then do stuff when the node as
a whole becomes overloaded but I do not
have the model for that yet so I haven't
edited this is the last one
the first users of this oh well the
point the point is that you have to have
the model first right so it has to start
with the model ah the dog food thing but
then it isn't that hard right I actually
wrote this because I'm tracking quake
Jules and I need it so then I had the
dog fooding principle I needed some kind
of limitation because apparently a
raspberry pi can kill the quake live
side and so that was the dog fooding
thing and then I needed the model right
so you can starters there's an escape
hatch that makes it possible to start
I'd say so I don't think you found a
park yet yes thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>