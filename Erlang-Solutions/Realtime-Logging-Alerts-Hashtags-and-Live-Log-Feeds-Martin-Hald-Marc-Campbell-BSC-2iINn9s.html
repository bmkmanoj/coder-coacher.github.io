<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Realtime Logging: Alerts, Hashtags and Live Log Feeds - Martin Hald, Marc Campbell | Coder Coacher - Coaching Coders</title><meta content="Realtime Logging: Alerts, Hashtags and Live Log Feeds - Martin Hald, Marc Campbell - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Realtime Logging: Alerts, Hashtags and Live Log Feeds - Martin Hald, Marc Campbell</b></h2><h5 class="post__date">2013-04-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BSC-2iINn9s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and we invented popcorn to help solve
the problem of knowing what's going on
in our in our production systems we'd
also like to give a quick shout-out to
Fernando benavides from Anaka who's been
an early contributor to popcorn popcorns
a fully open source technology so you
guys have access to both contribute to
it and to use it so here's the top view
is one you're probably familiar with
it's a logger output we certainly both
use logger and if you have a small
number of servers hopping on a box and
looking at logs works fine as you scale
up it doesn't work so fine nor will the
system alert you if issues occur so the
the bottom part here we you can see that
that's the popcorn view of an alert in
this case it's a quality of service
alert having to do with too many things
in the queue I'd like to point out a few
things you can see that it comes from a
queue length monitoring system it's
rolled up how many alerts have happened
and you're able to launch into work
flows associated with with the alert and
so what we'll do with this talk is will
present both popcorn as well as is there
like a volume let me see I think might
actually be check check check this one
so let me see I'm not the house AV hmm
continue to see if I just turned this
microphone off yeah that'd be great
thank you so we'd like to show you both
what popcorn is about how we built it
and then share some Erlang sort of tips
and techniques that we used as we went
along so I'd like to share with you
popcorn itself we're going to hop into a
running system here all right is it is
that better well for feedback credit now
let's get this working okay now try
alright so the microphones here when
you're looking that way you're not going
to be able to talk into it so I can move
it over here if you look down and we'll
be great alright alright so what we see
here on the screen is a production
popcorn system running at tigertext
we've had about 4.3 million events flow
through popcorn so scalability in sort
of where we're at with scalability
something we'll address you'll see
occasional flashes on the screen this is
a system that we've built to have live
updates so that is a information be
pushed back out from the Erlang server
back into the browser to tune form that
an event has occurred one of the goals
with popcorn is too yeah did this so
there could be cascading events in
Erlang which is one we just saw but one
of the goals is to have summaries of
data and very quickly be able to see
what what issues are occurring
alright
popcorns been used both the tigertext LP
as well as whisper has used popcorn so
we'd like to just go through some of the
concepts quickly about popcorn and about
how we view logging so first off just
simply what's what's a log entry so we
use lager and lager provides a host of
familiar log items including module
function and line number which is nice
popcorn extends that model by adding
roll node and version so the idea is as
we roll out new software we want to know
eventually if the version we rolled out
has have more issues than previous
versions so we want to be able to triage
information from from roll outs we're
also looking on the roadmap for being
able to look at the caller information
both of our companies deal with custom
clients that our companies build that
are out in the field one of those
versions may have a different call
syntax and others and may cause issues
so that will also help help debug issues
this is a pretty standard logger log
message in this case for rendering and
avatar popcorn is looking to extend this
model by adding information about topics
and identities so this is a Twitter
style sort of log message if you will
custom request is sort of the topic
that's happened and it's for an
individual user called M hauled like
other systems we're very interested in
rolling up log entries that that are
critical that have caused issues on
production systems so what you see here
are by severity and we're looking at
doing identifying topics later those
need to be grouped so we need to be able
to collapse down common strings of text
they're related for that we need an
addressing scheme so a combination of
the popcorn provided data and the logger
provided data we can come up with an
addressing scheme for for log messages
out in the wild so in this case X and P
P is the protocol we use and we have
XMPP servers
the particular version of software the
module that had an issue in the line
number give us a unique scheme for
saying that this error happened in a
particular version and we're inside the
code that runs slightly a foul when you
get crashes because they have low no no
line numbers so we need a system by
which we can collapse down common common
descriptions of errors into into an
addressing scheme it's a little bit
similar to auto associated memory in
this case we generate md5 checksums from
the data the last concept that we wanted
to share with you is that of data
streams so with with these different
building blocks we have the goal of
being able to see large volumes of
traffic coming into the log server and
sort of pinpoint more precisely exactly
what it is you want to see so all right
who here has laptops okay so we wanted
to do a fun interactive demo with you
guys so will I think there's enough
people that will try it in this case the
demo is for pretend company called Acme
calculators and they're going to put up
a rest-based service and we're going to
see see you guys hit that service and
see see well how popcorn behaves so
we're just going to walk through the
process that acne calculators would have
used to to get popcorn out there so
popcorn is just a standard OTP
application it's hosted in github under
mark Campbell's account so one simply
builds it with rebar there is one
additional step here for optimization
purposes we can compile down the the
less dynamic style sheet into a CSS file
so that saves the browser from having to
do that on every request
popcorn right now is mostly configured
through the app config file not through
the browser so in this case the there's
one password for popcorn which can be
set in the config and then we can set
things like outbound notifiers so emails
can be sent if issues are seen so that
was the steps that would be required to
get popcorn up and running popcorn
itself comes with a menia database which
will be auto created when when it's
launched so the next steps for Acme
would be hooking it into their
application so here we can see a quick
update to their rebar config where they
included as dependency and the last step
that they would then need to take would
be to update their Siskin figure app
config so that logger could use it la
Garza a great logging framework and
we've been very happy to extend off of
it you can see here that we've included
the logger popcorn back-end as a handler
to logger itself so logger routes log
messages to it if they meet the criteria
in this case debug level messages and up
are sent to popcorn and then we specify
the host and port and as described
earlier the role and the version are
also important so those are set so I
think what we like to do now is I'll
bring up the popcorn UI for the demo
site and you guys you guys can hit it
and we'll let you guys play with it for
a few minutes then we'll get back to our
slide and look at some more technical
views of popcorn so that the demo site
I'll bring up here but that curl command
which will be on the screen for a few
minutes is going to be what's going to
let you hit hit the site it's just a
reverse polish notation calculator it
only takes two operators and operands we
didn't implement subtractions on purpose
so you could crash it there's probably
other exciting ways you could crash it
if
experiment and I will bring up the demo
popcorn
alright
so it's anybody willing to well hell I I
did promise to bring up the text so let
me let me do that that's that's going to
be slightly helpful I suppose
alright so we can see that popcorn is
sort of live streaming the debug
messages well the calculator application
which is also an OTP application in this
case is a live streaming the popcorn
messages over we've got a couple debug
messages there's a crash that happened
which is which is nice in our use case
here if we come over to popcorn we can
hop over to the main screen and we can
see that we've rolled up those errors
that have happened with the minus the
minus here and at the end of the day
popcorn is about encouraging work flows
within your organization's right it's
about knowing what's going on it's about
being able to remediate what's happened
and it's about sort of tightening the
feedback loop between the application
and developers excellent we have a we
have a very imaginative crash which is
good so because this was actually an
error which was recorded by through a
log error message one of the things that
we can do is we can click on source code
and we can actually jump from the error
message directly into github looking at
the version of software that was
deployed on the servers so I don't know
if you guys deploy very often we find
that the the master git repository that
we work with is often different by the
time we've deployed code and we found
this to be a very useful way of getting
back to the actual source code that was
deployed to see what issues that had so
we can patch it
excellent
so just looking back at this fictitious
customer that we created mackney
calculators their their previous work
flow was development puts out code
customers will happily use their product
at some point if things goes wrong
customers will will talk to the support
folks and that information will get back
to the developers that's a very slow
process coming out and there's already
frustrated customers at that point which
which isn't so good there's some
excellent you know application level
monitoring and sister monitoring tools
pingdom for instance or a host of other
ones could tell them if their whole
application is up or down nagi oh they
can look at the health of individual
servers but by adding popcorn to the
story the idea is that they could more
rapidly identify edge cases that
happened out in production they could
sort of know with good certainty that
their system is running healthily or not
based on the fact that they're getting
feedback from their production or lying
systems they've got real-time alerting
so they don't need to wait for you know
issues to be addressed to them they
already know what issues are happening
and they're able to interactively look
at log streams both in terms of what's
happening in production as well as for
things like development and working with
other groups within their company that
might be using their product so that's
that's my little part here i'm going to
hand it off to mark Campbell who will
talk about popcorn design and show some
of the interesting techniques that we
use to build it
you got that off that's the Micra room
Thanks yeah so I just wanted to talk a
little bit about some of the
technologies that we're using inside
popcorn and what we use to build it how
we're using in some of the problems we
ran into think of my school how we did
in popcorn and it might be beneficial to
anybody else working on similar
technology the three parts that I want
to talk about our our ingesting messages
into popcorn how we're storing them and
how we're rendering and back in the
browser efficiently and effectively with
a focus on scale starting at the
beginning how we're ingesting the
messages into popcorn as Martin
mentioned right now or a custom we're
using a custom back-end a logger which
takes the message the log message inside
lager protobuf uses proto buffers to
encode it and UDP sends it over to the
popcorn server we're doing the same
right now in winston which is a no Jas
logging framework and also we started
working on 14 log4j it can really be
used in any logging framework that's out
there as long as you can have access to
a UDP socket and you can use a protocol
buffers library there's downsides to UDP
packets come in out of order you might
drop some and we've kind of built
popcorn up from that in order to handle
the scale that we need to and by scale
I'm talking about specifically at how
many messages per second we can handle
right now the system that I'm running on
one medium instance on ec2 handles about
500 messages per second pretty much 24 7
and the handles it fine how we're
getting the messages into popcorn once
all the clients on the left here the
protocol buffer encode the message and
send it over the UDP why over the socket
we have a single UDP listener inside
popcorn that receives the message
decodes it really quickly with no hits
to the database if very fast and then
spawns off no finite state machines one
for each node that we're monitoring each
node is responsible for its own triaging
of alerts logging to the database
sending it down to the clients whatever
it needs to do the idea is that the UDP
listener that we have is suppose it has
to stay really short and quick because
we don't recommend out we don't have a
pool of them we just have a single one
once the node finite state machine gets
the log message it needs to
or into the database we designed the
storage system to be pretty modular
right now it's using manisha 500
messages per second works fine there
might be somebody who wants to use react
or register my seagull or anything else
on the back end so what we've done is
we've defined a simple behavior that you
have to implement in a gen server if
you've implemented that behavior you can
swap out the manisha in a config file
and then use any other back-end that you
want you have to meet very specific
requirements so what we've done is it's
a pretty typical gen server if anybody's
written gen servers inter lying we do
pool them we'll start a pool of normally
five of them to start with we have two
different ways that we can create your
gen server one is just to deserialize
any previously saved data that you're
responsible to do and then work your
processes which will start and then
there's some synchronous and
asynchronous calls handle call an 80
cast that you have to implement the
synchronous ones we try to keep pretty
minimal obviously anything that we want
to be able to do 500 messages per second
or more which turns into 40 million
messages a day you know we can't have a
lot of synchronous code running
especially with the way we have it
designed with one finite state machine
per node so the synchronous calls our
stuff like we need the value of this
counter or does the database know of
this node if not create it events like
that these are pooled your gen servers
are for the storage a handler that
you're writing so you can't really cash
inside there for counter values make
sure that you know we actually have
implemented a caching layer and a buffer
in front of this inside popcorn the big
ones on the asynchronous methods are a
new log message every time a new log
message comes in we're going to call in
and you have to store it to the database
and the top one which is expired logs
matching popcorn has like a log
retention policy / severity level you
can tell it you know I want to hold
debug messages for five minutes in
critical messages for two weeks if you
want to or whatever and will call into
your worker processes to tell you to go
delete those when needed with a severity
and time stamps it's kind of how we've
implemented the storage going on to how
we're working on getting the displayed
in the browser
we're using them a technology called
server sent events sse it's a little
lesser-known w3c standard out there it's
similar to web sockets but it's a single
direction web sockets allows the client
and server to communicate server sense
events allows the server to push data
out of the client but the client does
not have a way to talk back to the
server over that same channel so the
client has to open up another HTTP POST
or some out of that out-of-band
communication mechanism once the server
sends messages down to the client / SSC
it can send it in any channel that you
want in any data payload that you want
the examples on the bottom there's a
channel for alert in a channel for a
node and we're choosing to JSON encode
the data payload that we're sending down
but you can really do anything there
this is an example of the JavaScript
client side code to handle the server
sense events in the browser this just
responds to two different channels
update counters a new node parses the
data and then we'll pass it off to
whatever it needs to do on the server
side for the server sense events we're
actually using cowboy for the HTTP
server and the way that that works it's
we have the node finite state machine
running and then when the browser
connects to that log streaming it
creates it calls into cowboy web server
cowboy spawns a process and goes into a
receive loop then it creates a log
stream finite state machine as a
separate machine registers that into
right now we're using an ETS table and
that finite state machine has all of the
parameters for that stream it might be
what severity is you have filtered out
what time stamps you have filtered what
whether it's paused or not it registers
that into the ETS table and then when
log messages come into the nodes the
nodes broadcast them out to use just
using standard Erlang message passing
it'll broadcast them out to all of the
log stream handlers log stream handlers
check to see if it's filtered out if it
if it's relevant data for that connected
client it is it passes it back to the
cowboy handler which is in that receive
loop the cobo handler just protocol or
I'm sorry the proto SSE encodes it and
sends it down over the existing
connection and we're able to do like I
said 500 messages per second and were
able to really keep that up through an
SS each stream over a you know normal
internet connection also without taxing
the browser
too much right now SSC when you're using
it it's it's kind of a different
technology this is like when you
configure it with engine X on the front
end you have to make sure you have proxy
buffering off you can use s tunnel you
can use H a proxy you can use different
technologies on the front end of server
sent events but point is cowboy supports
it and engine X and most of the common
proxies out there do the wire protocol
I've talked a little bit about some of
the standards that we're using from
server send events to UDP to protocol
buffers and I want to focus a little bit
on protocol buffers versus JSON encoding
the messages when we're sending the
messages through protocol buffers if
anybody's not familiar protocol buffers
is just a standard that Google has
created it's a very efficient and
compact representation of data they're
tight and ordered packets but their
binary at the end of the day this is how
this is an example of our Lange encoding
a protocol buffers packet the same
packet in JSON looks like this using
jiffy which is a see extended extension
for JSON parsing in our line both of
those packets converted to the wire
format look like this with the protocol
buffers one on the top and the JSON one
on the bottom the protocol buffers one
as you can see is significantly smaller
the JSON one there's advantages to JSON
it's more extensible it's definitely
more human readable but it's bigger so
what we decided to do is spend some time
figuring out if JSON is good or if we
should go with protocol buffers for this
solution this is using jiffy for JSON
and the Erlang protocol buffers for that
bass show has actually four for the
protocol buffer side we encoded and
decoded a million messages just on a
typical macbook air and you can see the
processing time is really about the same
encoding is almost identical in decoding
protocol buffers is slightly slower than
the JSON ones but the big win was with
the memory overhead per message that we
needed to handle JSON was used a
considerable amount more memory and we
could optimize that a little bit by
shortening key names or doing something
like that inside JSON but I mean
protocol buffers is more efficient just
out of
box and it's going to be because it
doesn't have those key names in there so
we decided to keep protocol buffers and
then the next part was a little bit
about how we're rendering in the browser
what you saw so far was just the log
streaming coming in and that's just Dom
events manipulating inside the browser
that we receive through the server sent
event stream there's another URL which
will show you also actually the next
slide as a preview of it where we're
starting to use some d3 visual
representations of the data so you can
easily look at the screen and say at the
on your dashboard screen on your log
screen and say you know I all my events
are coming from a single server from a
new version of the software or a roll or
I have a lot of critical messages and
not a lot of debug or you know something
out of the ordinary so we're using d3 to
render that in square has a JavaScript
library called cross filter I don't know
if anybody's use that it's um it lets
you take a large data set inside the
browser and basically do reductions in
MapReduce is on site inside that in the
browser so we're using that and what we
do is we can produce a screen that looks
similar to this on the top thus there
there it's a d3 graph where it's just a
log volume over time then you can see
you know this is a beta so we don't have
the UI is not totally done there's
labels missing and stuff but you can see
these severity and the roles and the
nose and as they're coming in you can
the idea is to extend this and allow you
to select a time frame on the top of the
screen and then the graphs down below
that will reflect the message volume
inside that time frame and then down
below that the log messages show you the
law the raw log messages that made up
the stream at that time that you were
looking at with the filters that you
have selected so going back to scaling a
little bit to get to the point where we
are now we have to keep going you know
500 messages per second on a single
server is good we want to keep going and
we need to get it to a distributed state
and be able to handle larger and larger
clusters of Erlang servers before we
could figure out 500 we're happy with
because that hits both of our criteria
very well right now but before we could
even get there we had to figure out
where our bottlenecks were because the
first time we wrote it and we launched
that it wasn't there so we had to
identify some tools and build some even
in
to figure out exactly what the problem
was that we had to solve so one of the
biggest tools that we found is stats d
in graphite this is actually a
screenshot of graphite logging stats d
data we wrote a little module called RPS
which basically is just a request per
second library an application inside our
letting that you can install in your app
and then you can pass messages through
to it and it's just designed to like
basically keep really simple counters
and pass them through the stats d which
graphite can graph in an efficient way
without impacting the over the
performance of the monitored application
too much other tools that we used rd top
d top is built into Erlang where you can
monitor or watch connect to a running
console and then watch the gen server or
Jen fsm queue lengths reductions CPU
memory overhead the current function
that a gen servers running right now how
many messages are in the queue red bug
is another one red bug allows you to
connect and put a trace on a running
system probably wouldn't want to do it
on a production system but you could you
can see the funk you can give it a
function name or a module and ask it to
trace to the console any in real-time
calls that are made into that module or
function with the parameter names and
you can actually it's really helpful to
help track down exactly why something
scratching or where problems are coming
in and some other tools that we used so
at the end of the day you know we found
three big issues that we needed to fix
in order to get to the level that we are
right now our manisha database wasn't
scaling as well as we wanted it to we
had issues with just receiving UDP
messages and the client could after
about five hundred messages chrome
stopped responding and so we had to make
that a little bit more efficient we
liked manisha because it came out of the
box with Erlang know there's no need for
an external database server and nothing
else that anybody would need to set up
we both liked it and we know many as you
can handle the volume so we've had a
troubleshoot and figure out what the
problem was when we started we looked we
were creating indexes on our log message
on every almost every record our every
field in the record it was severity
whose role
it was knowed it was time stamp it was
everything that we thought you might
want to be able to filter from from the
browser as you can see it turns out that
it's not very consistent time inserts as
this graph just shows 5,000 inserts into
the database in a very short period of
time and you can see each insert took
longer and longer so that wasn't going
to work really well for us to get to a
system that can handle four or five
million messages so just to confirm that
indexes were the problem I removed all
the indexes from the database from the
Manisha table and reran the same test
and it became very consistent time
inserts constant time inserts but the
system is no longer usable you know
those indexes were there for a reason so
we had to find a solution adding just
one of them back in which we have the
size added the severity index back in
there's eight possible severity Xin a
logger log message it looks a little bit
better but it still has the same problem
5000 messages the y-axis is a lot
shorter now so they're not taking as
long but it is still growing so that's a
problem taking that severity index out
and adding just an index on time stamp
it becomes very good the difference
between timestamp and severity index is
there's two timestamps are incrementing
and their micro second timestamps so
they're very very distinct severity
indexes are all over the place you know
human there might be a lot as ones and
then a lot of eight and then they're
also a lot with the same value so what
we decided to do is keep it this way and
just kind of re architect the way that
we're rendering the data and how we can
query it on the browser to work with the
timestamp index which actually works
really natural for log messages anyway
so then the last one was the browser um
like I said you know 500 messages would
eventually use up all the memory on the
browser so we did some work we ended up
actually with two SSE streams right now
in the beta version the beta you I we
there's a line that we're trying to
figure out where we can filter on the
server versus filter on the client
filtering on the server obviously you
know there's there's a trade-off it's
more load on the server but i think that
there's the benefit to be able to do
both right now using cross filter and d3
to render the data we're able to get
were past that 500 message limit but we
still need to keep pushing that forward
a little bit more and as moving forward
we want to be able to add filtering on
server and the client and I think that's
where actually the power is you want to
show the beta screen share chrome we
could also say that read on login plans
it or even free to take your application
and configure it to send that data so
just because it's not totally done yet
we kind of put it on a URL with data
equals 1 we'll see if it works
so as log messages come in what happens
is the graph on the top will show up and
then these don't have labels on them but
they're the these are d3 of just pie
charts showing the severity then rolls
and then servers and then down here you
actually still have the log messages
coming in filtered down to that so
that's uh that's it end of the
presentation on popcorn sure yeah we do
we we can we can if it's a log message
that's causing that you can have an
error or an emergency or a critical
handler on it we definitely can lager
can provide us a lot of information
about the line number of the module and
we can group that very easily in
Martin's done some worked on just Erlang
crashes which don't have line numbers in
them yeah yeah we are so right now we
use a simplified model where we sort of
generally know the structure of a crash
will be dynamic and then we have like a
hearing set that we create it and check
some from so yeah that's the checksum we
realized that there's probably machine
learning techniques that will have to
use down the road but you know our ad
hoc method actually works very well
sure so we have a bash script that we
can use and releases called get
underscore SP and I think and what it
does is it uses a fine command to like
rehearse and find all the ER lying
modules and then it looks to see what so
if you look both in the depths folder
for instance as well as your source code
and then it figures out what like get
what get project apart and like which
like what's the latest like log head
checksum and then it builds a submit
file along JSON and then when a server
launches it can simply submit that to
popcorn and that's that's technique has
worked well it basically it didn't know
a priority though like you know what's
that what that is it's out of your code
release version one that you got three
Yeah right we use chef to release source
code and so it's simply just a part of
our like build and deployment process
we're not right now like that's
definitely on the list we want to be
able to use ranch and pull them and like
there's a lot of optimizations that we
can still do there right now you know
we're we're not dropping UDP packets but
i think you know to get it to like we're
probably pushing the limit on what we
can do in a single gen server right now
older than a certain time and with
certain severity levels because you know
like for me i'm sending in production
sending all of my debug messages even
over to it for some of the servers and I
want I don't want there's a lot of debug
message traffic and I only want those to
last for five minutes it's definitely
for debugging stuff um initially ya out
there was there was I mean that I don't
know any tricks um possibly know we
definitely are doing that there's some
parts transforms that are written inside
there to help make it do it if you're
curious i mean the code is just up there
and github definitely take a look it's
all in a single file so yeah
well it's definitely going to use up cpu
to like serialize the data and send it
across the wire we look at our
production systems like we look at like
the number of like reductions on Jen
servers across the system logging is
normally somewhat up there even with
just logger by itself but by no means
does the longer popcorn back in like you
know get any worse than Walker itself
blogger uses Jen event in order to have
many event handlers and then broadcast
to them so they can each handle the
event in the way to Egypt so we haven't
we haven't changed lager would simply
use one of this extension the way the
handlers work you know you can have
debug messages going into lager and you
can just in the handler you can specify
a different minimum severity for each
handler so you can definitely like say I
only care about errors and above for
popcorn time zone
right
yeah we know we can survive 500 we don't
know where it rains right now I mean we
work to get to 500 I mean you know it's
probably not that much above that is
where it would start to need some
additional work yeah I mean github repos
great we're all so happy to hear ya
good but when you remember it yes up
again you know i should say is that
we've been building this for ourselves
you know my first and uses it I got a
cute but it's also fully available for
the community so we would also be
interested in hearing you know how well
does this match your current these cases
and you know what what are the next most
exciting things like
yeah you know this was
okay that's going to definitely idea
well like you know how many requests per
second you're getting getting more to
turn over questions that could be
dropped the number of
another good curious history graduate so
when things are not going well you can
look at both what's happening right now
in terms of a graph that you can as
compared to what this is what the graph
looks like over the past five minutes
and past hour or yesterday and I'm
curious how do you see this stuff
I think this is part of a larger
ecosystem right so I wouldn't look to
replace graphite and graphite in a very
particular use of you know rendering
arbitrary certain types of data the the
way I use this most is I look to see you
know the question is what issues have
happened out there you know what are the
current issues you all most frequently
occurring ones I mean which ones have
happened most recently those are two
questions that I always want to know the
answer to and that often he also sort of
informs if there are issues that
engineering needs to look at the said
that's one of the reasons we're adding
some of those d three graphs into there
because I don't think it's going to ever
replace graphite and that's not our
intention but i think you know if you
can filter the messages down to a
certain time frame and then down to a
certain server or role of servers and
you see the raw log messages in those
graphs it might it might help when
you're looking at graphite we're
still in early technology as well so you
know as it matures having those graphs
on the various pages you know a picture
really can't say a thousand words in
terms of getting a very quick
understanding of wearing shoes are
yeah and one of the things that that's
kind of we haven't even started on the
code really yet but we've been talking
about is adding heartbeats into the
popcorn server so the server can send
gauges kind of same way stats the in
graphite has gauges you can have some of
those events through and that's outside
of lager but it's just a separate TCP
connection that your server through the
lager popcorn back end or a different
module can make send that data up but
when the popcorn server sees the raw log
data it also knows these heartbeat data
and this gauge data it can it can
correlate that but that's just something
that we've been talking about but we
haven't done it yet
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>