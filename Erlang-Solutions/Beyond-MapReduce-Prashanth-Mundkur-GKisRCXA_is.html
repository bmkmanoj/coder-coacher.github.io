<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Beyond MapReduce - Prashanth Mundkur | Coder Coacher - Coaching Coders</title><meta content="Beyond MapReduce - Prashanth Mundkur - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Beyond MapReduce - Prashanth Mundkur</b></h2><h5 class="post__date">2013-05-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GKisRCXA_is" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Thanks so this is going to be about
disco which is normally thought of as a
MapReduce implementation things have
changed with little so i'll be talking
about what's changed and the ideas that
is kind of time to move on beyond
MapReduce and how it disco is trying to
do that so this was a big data platform
but does MapReduce so I'll just kind of
set up the context what we mean by that
well this goes being presented that
airline factory a couple of times but in
case for people who aren't aware of this
coil just go over the architecture of
disquiet a very high level I won't spend
too much time on that but please feel
free to ask questions and then i'll go
into what is going to come in the next
release of discouraged what we call the
pipeline model kind of describe why what
motivated the pipeline model what it is
and what kind of things with sols and
and also what it doesn't solve when what
are the limitations of it and just
conclude with the disco roadmap so big
data's got a very simple definition in
my opinion it's just basically data that
don't fit in one machine at done fit in
one machines ram it don't fit in one
machines disk you need to store data or
analyze data using multiple machines and
the simplest way of doing that was what
Google who came up with the almost a
decade ago now you have chunks of data
this way in a cluster you analyzed
pieces of data and parallel you call
that the map phase you get the
intermediate data together in one place
and you compute the final result in step
code reduce and you use our cluster of
machines to do that and this is where it
kind of looks like
you have parallel maps that analyze
piece of data and then you have reduces
that generate the final output and
there's a hidden step bear called
shuffle which is kind of usually ignored
unless you get under the hood of
MapReduce and that's where a lot of
problems like there will love problems
lie here and disco as well and in fact
that's one of the reasons why we came up
with a shuffle where the pipeline model
will be getting back to that so the
regions of disco our hostel was one of
the originated and in the originator and
it's now just over five years over now
so it's been around for a while and this
is what the architecture looks like you
have master node that kind of controls
the whole MapReduce game and you have
complete nodes that run a line note that
that are basically worker nodes on which
compare computation is done you have a
client that interacts with the cluster
by by the master with simple Jason I
should be protocol
and these are good good on to which you
actually analyze data is the green block
there that that comes on the compute
nodes so what happens is when you submit
a job your client takes your code
compresses it into a zip file submits it
to the master the master sends it to the
compute nodes and the complete nodes
unpack the zip file into a workable
ectly and launch your designated
executable end a zip file that executes
the ones it becomes the green block
there you are executed a is linked
against a library that talks to disco
that's the small blue circle there it
implements something called the disco
worker protocol it's a very simple
protocol can be implemented in any
language fact you can make any language
of first class citizen of discourse so
we have Python which is the supported
one and we also have a no camo
implementation of disco the the user
code accesses the data over HTTP or from
the local file system usually happens
over the local file system so there it
is the code runs local to weather data
is one thing to note is that your user
code accesses the data directly it
doesn't go the access to the data
doesn't go
through the disk architecture that's
almost quite right except in one place I
will come back to that that's one of the
things we fix in the pipeline model this
is the worker protocol the details are
not important it's just simple protocol
it tells your code what task it's
running where to get its inputs and when
you generate output swear you tell the
school where you put your outputs and
then you say you're done or you say you
have some error I mean it's a very
simple control protocol there's not data
going through this one of the things
that you need to provide with any big
data system is a place to store data and
disco comes with its own distributed
cache is with the air live stream where
is it plugged in just need that part
okay check check check check looks like
it's okay here
should I gone ok so the distributed file
system that comes with disco has got a
very simple implementation it's fairly
similar to the compute infrastructure
there is a master node that coordinates
a bunch of storage nodes which have
locally attached storage the client
interacts with the master to get
metadata about where the data is located
and it interacts with the bulk data
directly over HTTP to the storage nodes
no bulk data goes through the master so
this is what the layout of your data
looks like in disco it's not POSIX file
system it's not tree structured than no
concepts of directories it's a tag
structured system and so tags can point
to bulk data which is replicated for
fault tolerance and you can have
multiple tags pointing to the same bulk
data and you can have tags pointing to
tag so you can have a hierarchical
structure that way so it becomes a
directed acyclic graph it's not a
tree-structured Bozek style file system
kind it kind of provides a lot of
flexibility in addressing your data this
is what the metadata looks like in the
file system it's basically a collection
of where your data is located it's very
simple it points to the replicated blobs
where they sit in the file system it
also points to other tags that are
addressed by this stag there are a bunch
of attributes that you can set in your
metadata key value pairs that you can
add additional metadata in addition to
the name and they're all so simple
access control tokens you can have a
read token and a right token so you can
control who can access your data you
need to have the right token to do the
right to perform the corresponding
operation
on the metadata this is the only mutable
data and the system the the bulk data
that is immutable once you push data
into the file system it stays that as it
is it cannot be modified unless you
delete it the tags the green blobs there
are the only mutable state and the file
system so you can add bulk data into a
tag you can remove bulk data from a tag
and you can delete a tag can create a
tag so that's where the mutation happens
it enormously simplifies the
implementation of the file system this
is what disco looks like in terms of
code size it's extremely small here are
the lines of code for javits and order
of magnitude smaller a few thousand
lines of code of eight thousand lines of
code in the MapReduce compute segment
this is for the development branch the
last release actually a little smaller
than this but the next version of disco
is what these numbers refer to around
four point four four point six hundred
lines of code in a four point six
thousand lines of code in the
distributed file system and the Python
library is three thousand lines o camel
library is seventeen hundred lines and
of course one thing that's missing in
this are the external dependencies of
disco so these look very small and you
kind of don't you should be suspicious
of very small numbers like this of
course what's missing is the external
dependencies of disco so it depends on
an external HTTP library which is mochi
web which is in fact almost the size of
all of the rest of disco it's twelve
thousand lines there's a logging library
the logger which is around four point
three thousand lines and of course the
standard libraries of or lang and Python
okay so let's get back to what's new in
disco any questions on disco so far so
so this is the MapReduce flow graph and
the motivation for the new stuff that
I'm going to talk about is a bug in the
scheduler it does in backtrack so what
happens is that currently if you're if
you run your Maps you do your shuffle
and you start running your reduces and
by the time you start running your
reduces one of the nodes on which the
map Rand goes down and your reduce
cannot access so so your reduce cannot
access this data because the node on
which this data is was host it goes down
what this code cannot do right now is
rerun that map somewhere else in the in
the cluster so it cannot backtrack and
fix a missing data earlier in the task
graph so that's a bug that kind of
motivated what I'm going to talk about
another kind of issue that is exists
that currently exists in the stable
release of disco is that the shuffle
step is actually implemented in the
master which kind of is not quite nice
because this is one place where bulk
user data actually goes through our line
and we kind of like to think of erling
as a control as the control plane and no
bulk data goes through the leg portion
of disco but this is one place where
actually it does go and that's not quite
nice in fact this is one of the reasons
why the backtracking scheduler is kind
of complicated to implement so in trying
to address these two issues it took a
look at what's happening here and we
rethought the whole MapReduce idea and
the the certain problems with the whole
MapReduce ideas which is that the job
computation is done in three fixed
stages
these are the status there's a map stage
this hidden shuffle stage and there's a
reduced stage and it's kind of
inflexible and you have to implement all
your computation in this limited
framework it's it's kind of constricted
another problem with this the
conventional presentation of MapReduce
is whole it's tied to key value pairs
and maybe you cannot express your
computation always in terms of key value
pairs now this is actually fixed and
disk already if you look at the the
protocol the job protocol what you get
in your user code actually what what it
gets is input and output there is no
notion of key value pairs that's
actually implemented in the worker
library it's it's a library
implementation it's not core to the the
disco or length portion and another
limitation of MapReduce is that there is
no inter task optimization of network
resources now you're usually you run
into network resource issues with very
large jobs and as machine learning
people who were data analysts you
normally don't think of network resource
consumption but once you actually start
getting in our once you become a power
user of these systems you start running
into network resource issues and it
would be nice to provide the users hooks
in which they can address problems of
network resource consumption so I'll get
into this in a little more detail so
what happens once your MapReduce task
task flow graph actually runs in a
cluster you will have some tasks that
actually run on the same node so that's
this grape box here so you have three
maps that and on the same physical or
virtual node they produce
outputs that are partitions and those
partitions are sent to the reduce stages
now what's actually happening is that
all these partitions are sitting on the
same physical node but your reduce is
running somewhere else on some other
physical node in the cluster and all of
them have to be shipped across the
network and that's one of the reasons
shuffle is kind of expensive in terms of
network resources all these files that
are sitting on the same node have to be
shipped across the network and what
would be nice is that there might be a
way of moving poor a portion of this
reduced into this node to access this
data locally at the node instead of
sending it across the network notice
that this is different from what is
called combined in MapReduce sometimes
you come across this local combiner
combiner runs in the context of a single
task what we are talking of is combining
a cross toss that ran on the same node
so it's slightly different from the
concept of a combined you could think of
it as a node local combined instead of a
task local combined so so this is what
could what you could do is what would be
nice is if the if a if a portion of
these stars were pulled into a physical
node so that they could access the
output files of this previous stage
locally to that node compress generate a
summary of all these intermediate
intermediate data and send the smaller
assumption is the summary smaller than
the input data and send those smaller
compressed summaries across the network
and so what what's actually happening
here if you look at it is you have you
have one stage of tasks that generate
outputs that become the inputs for
another the subsequent stage of tasks
and what
actually happening as your kind of
routing inputs into your grouping
outputs as inputs for the next stage and
the various ways in which you can do
this kind of grouping so what what's
actually happening here if you think
about it is there's a grouping by label
so the color is the label here in the
context of a single node so each each
task they're each colored tasks there is
actually collecting all the is grouping
all the outputs by the label of that
output in the context of a single node
so you could kind of call it as a group
by label per node that's one possible
grouping you could do another very
simple grouping you could do is just
group all the outputs of the previous
task that ran on a single node into one
task process all this intermediate data
and then construct whatever output data
you have and then send it out of the
node so this is another possible output
grouping so what's what's actually I
mean if you step back what's actually
happening is you have stages of tasks
and you have grouping operations
performed inside so this is effectively
you've arrived at the pipeline model you
have pipeline stages of task a pipeline
as a sequence of stages and each stage
perform some grouping of its of the
previous stages outputs and run some
tasks on some computation on those
inputs so that's that's basically the
pipeline model that disco will have in
the next release and so the question is
ok if it sounds very nice but can you
actually implement what you could do
before actually I will get back to that
so here are some other possible grouping
options that you can do you could have
what we call a split which is
for each possible input you run one task
okay so so there is no grouping
effectively as you just each input has
some tasks that's called the split so
that's typically performed in the map
phase you could have a group label kind
of grouping which kind of groups all
outputs across the whole cluster
depending on what label the output has
so this is there's no node locality in
the group label option the group baking
is done across the whole cluster and
similarly a group all grouping book just
consumes all the outputs of the previous
stage across the whole cluster so again
there's no concept of node locality that
so yes so once you have this pipeline
model you can express MapReduce as a
pipeline since it's the pipeline model
is a strict generalization of MapReduce
so MapReduce can be easily expressed in
terms of the pipeline model as a map
stage which performs a split grouping so
there is a map / input chunk there is
one map task / single input and it this
is the case across the whole cluster
there is no node locality there and the
reduced age groups by label all the
intermediate output files and performs a
reduce so so you can express MapReduce
in terms of the pipeline model but the
pipeline model is more flexible so so
what does the pipeline model do it fixes
exists existing issues in Disco are we
using the pipeline model it's easy to
think of a general backtracking
scheduler it can backtrack not just
across the map shuffle reduce faces it
can backtrack across a whole sequence of
stages also by by exposing the shuffle
stage into user code you
you actually avoid any bulk data going
through the online portion of disco so
both of these issues are fixed in this
pipeline model in addition what it
provides is a fairly flexible compute
model you are not restricted to
expressing your computation in just
MapReduce phases with a hidden shuffle
stage which you really don't have access
to this allows you to express express
your computation in a whole bunch of
stages and it allows you to perform node
local optimizations in case you want to
so in case you are interested in
reducing the amount of data that's
shipped across the network and cause a
whole bunch of problems like TCP in cost
and stuff like that what you could do is
you could have explicit node local
stages that compress all the data on a
single node before it shipped across the
network so so it's it's a fairly
flexible compute model also it's a
pretty simple or conservative extension
to MapReduce it so it's a linear stage a
linear sequence of tasks it's not an
arbitrary graph data flow which which
like dryad is pretty interesting
dataflow programming a big data
framework that came out of Microsoft
it's pretty general the problem is that
genera generality it allows you to
express arbitrary data flows a graph
structure data flows but in order to
express your graph you need a DSL it's
kind of complicated you not only are you
writing your data processing code you
also writing code that generator that
computer graph or expresses your graph
so here it's very simple it's very
declarative you have a sequence of
stages there is no need for a DSL to
express your graph
it's probably a more flexible platform
for higher level tools that are normally
built on top of MapReduce like a pig
flume Java which take your computation
and then run a whole bunch of MapReduce
jobs in the background and to to
basically express your high level
computation now won the this pipeline
has limitations because it is
intentionally not very general right now
there are no Forks and joins in the data
flow so that you could kind of implement
a limited amount of forks and joints but
I won't go into them you can kind of use
labels to implement some some restricted
Forks and joints but that's the that's
still has some limitations it's not
general forks and joints and also there
is no iteration or recursion so it's a
linear steel is sequence currently it
doesn't allow you to iterate go back
into some previous stage and iterate or
a curse and usually you do require
things like that for machine learning
computations where you have some
convergence criteria and you keep
iterating until your algorithm or your
computation converges to some result so
this could I mean this could be
implemented in future iterations of the
pipeline model but right now it's does
exist so now that we have this new
pipeline model we don't have any map or
reduce tasks so how exactly do you
program this so you need a new API the
previous version of disco had you just
have to provide a map function and it
reduce function and that that's all you
did and everything else was taken care
of for you here we you need a new API
for your computation you need to provide
a process function that processes your
data in a certain stage that process
function iterates over the data so
you're given an iterator over the input
and you just consume that
and so it is different from the previous
API where you provided a map function
and data was pushed into it either
mapper reduce you had key value pairs
are pushed into your function here you
pull it using an iterator also there's a
simpler handling of processing state you
have in it and done hooks so you can run
you can specify init function that
generates that creator or initial state
for your function it gets passed into
your process function and you and it
returns the process function returns an
updated state it's kind of fairly
functional style API and the done hook
gets called with the final state and you
can do whatever you like you can
generate your final output and there's
also an input hook that allows you to
control the order in which you iterate
over the inputs in map and reduce you
really don't have access to the you
could consume several inputs but you
really don't know how many inputs you're
actually consuming the input hook allows
you to actually control the iteration
order over your various inputs so this
is the what's so this kind of summarizes
what's new in Disco in the next version
of disco so there's a bap backtracking
job coordinator you have a pipeline
model which is more flexible compute
model you have an alternative task API
which allows you to get a little more
control a little more visibility into
what's what your data is and again all
you have the existing MapReduce API
implemented on top of this
infrastructure on top of the pipeline
infrastructure so all you're almost all
your existing disco jobs will run
without modification on the next version
of disco there are few corner cases
which will not work bets if you if you
use some
scheduler options are you kind of
control the num number of reduces
there's a few cases where they won't
work without modification but almost all
your conventional MapReduce jobs in this
Cove will run without modification the
beyond Oh point five we are interested
in involving this pipeline model
depending on how people use it because
it's kind of new we haven't used it
extensively ourselves so users once they
start using it they will probably ask
for features and modifications and it
will be a evolution for the model also
one area that we're kind of interested
in evolving discourse getting the
scheduler little smarter about the
network topology right now it has a very
simple idea of how your cluster is laid
out it in typical physical clusters you
have servers racked in racks and the
network distance between servers in Iraq
is different from servers across racks
oh so there's an implication in terms of
network resource consumption and this
goes simple it doesn't worry about that
but I think it would be nice to get
disco smarter about that so that it can
scale across larger clusters now one
thing that often crops up and for a
valid reason earlier was disco was often
compared with Hadoop because at one time
both were implementations of map and
reduce and it was possible to compare
them and contrast them I think that
those days are over and it would be good
to know that when you try and compare
disco and Hadoop in the same sentence
what exactly are you comparing because
the storage HDFS and DDF s storage are
completely different so you can't really
compare them very easily and
the disco I mean Hadoop last year move
to a new model new generation I forget
what it's called yet another something
so they have a new computer model the
the next version of disco will have a
pipeline compute model which is kind of
different so so in both the compute and
storage aspects disco and Hadoop are
diverging and it would be when people
try and compare it would be good to know
that you're actually comparing apples
and oranges so so that's that's where
I'll kind of conclude if you have any
questions take them
yeah so I have network access this we
meet right bringing up so there's this
company called continuum I 0 which is
Python company let me just bring up they
have a blog on trying to access you know
whether blog is yeah so here they use
disco in there sorry
I want to give you something that I am
NOT cooking up so it's so that it is a
concrete example I think it's I'm not
sure if it's this one so on one of these
articles actually discusses he
implements his computation as a sequence
of disco jobs ok there's a sequence of
MapReduce jobs some of them are map only
some of them are reduced only so in
order to do his computation he had to
write a sequence of like three or four
or disco MapReduce jobs and all of and
some other some other stages were just
map only and reduce early it's so he was
constrained to express his computation
in terms of MapReduce well in this new
pipeline model all he has to do is have
a single job with multiple stages so
that's it's it's flexible in that sense
you can yep
sorry yeah so the question is whether
the pipeline is linear yeah that's one
of the restrictions of the pipeline it's
a linear stage so they may go back to
this so it is a linear sequence of
stages okay but across subsequent stages
that there is I mean there is branching
and joining going on so I'm not sure
what in what context you mean by linear
so there's a linear sequence of stages
so each stage runs the same computation
so it is I would call it a linear it's a
linear pipeline but so you you can't
have you can't have a fork in your
pipeline and a join currently you like I
said I mean if you look under the hood
and if you if you are an advanced user
or you can kind of use labels to you can
kind of use labels to implement forks
and joints but that that's kind of your
really poking under the hood then it's
not nice to have to do that it you could
think of high-level tools like big that
implement for can join using labels and
so it's kind of abstracted away from the
ordinary user but i would say we don't
have forks and joins
well I have like one question so too
yeah so last year DD FS in fact
classroom most of the work was done on d
DFS what we did was we implemented a new
garbage collector what that was kind of
motivated by fixing there was a race
condition in the garbage collector and
so it was kind of to fix that race
condition we kind of read it the whole
garbage collector and it in while fixing
that we also added a new feature where
you can allow nodes in the DD FS cluster
to be scheduled for removal so suppose
you decide that you want to remove a
certain mode from DDF s note from the
cluster now there's a lot of data
setting on that node and you can't just
remove it because it might have the only
replica of a certain data blob and you
would like that data block to be
replicated on other nodes in the cluster
before you remove this node so you so we
allow new we have a new feature where
you can mark that note we mark that node
for future removals so that all existing
data is migrated off that node and you
are given an explicit indication that it
is safe to remove that node so you don't
have to guess and you know do a Hail
Mary pass and then just remove for node
and then just hope your data is actually
replicated elsewhere so you actually
given an indication when it's safe so
that that was the DD FS stuff last year
so it's been around it's in the existing
stable release of Pisco
that that is of it would be a future
work I mean we were looking into that
and we have some ideas on how to do that
but yeah yeah so the problem there is
that this course it's a very
intentionally very simple implementation
of MapReduce and it's the idea of the
and one of the guiding goes of discourse
it should be dead simple to set up and
configure and in when you configure your
disco cluster all you just have to
provide as the names of your notes you
don't have to provide you don't have to
specify which nodes are local in one
rack that that adds some configuration
complexity as a result of that discourse
idea of your cluster layout is that all
nodes are like equally distant from each
other which is not really the case and
physical clusters and not true in
virtual clusters either but so the one
consequence of that is when you're
shipping data across the network disco
doesn't know whether it's shipping data
from one server to another server that's
in the same rack or once over that's
kind of across the cluster in a
different track and their efficiency
implications for these two different
transfers and so it doesn't do the right
thing in certain cases it can for a very
large cluster you have almost all the
data going across course which and your
cluster will not perform well so it
would be nice to fix that and you have
some ideas on how to fix that but that's
the underlying issue
another thing that we have been actually
actively collecting a list of companies
who are in many of you are have been
using this ever considered reduce disco
I mean like yeah right so we have these
companies and if you're not on this list
that's because we don't know about we
haven't heard of you so please join this
elite club alright thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>