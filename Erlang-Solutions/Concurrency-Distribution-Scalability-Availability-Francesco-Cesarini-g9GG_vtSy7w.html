<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Concurrency + Distribution = Scalability + Availability (...) - Francesco Cesarini | Coder Coacher - Coaching Coders</title><meta content="Concurrency + Distribution = Scalability + Availability (...) - Francesco Cesarini - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Concurrency + Distribution = Scalability + Availability (...) - Francesco Cesarini</b></h2><h5 class="post__date">2015-11-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/g9GG_vtSy7w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks I'm yeah happy so many of you
decided to come listen to me I wasn't
supposed to speak at this conference and
then the speaker dropped out so I got a
slot tomorrow and then a speaker got
sick and actually more sick than what I
am right now so I got their slot instead
what I'm going to talk about today so
you know my background is are like I've
been implementing airline systems you
know since 1995 and right now I'll go in
and I'll do things in my sleep almost
and what I've been doing for the last
three years is writing a book I don't
know how many of you have ever written a
book okay pro tip you know if you're
going to become a first-time father make
sure you finished it before your first
child is born because yeah that's one of
the reasons it's taken three years but
one of the you know one of the other
reasons is you know it's you know
chapter 14 14 is never a good number the
first 10 chapters were easy to do I
based it on the training material which
I implemented you know it tells you how
to architect your system it tells you
how to design you know on a single node
level it tells you how what processes
you know behaviors to give your
processes and put everything into
supervision trees and in chapter 11
started becoming harder it was about
release handling which you know trying
to make release handling an airline look
easy because you know the tool chain is
not it's not that good it's there to be
hooked into other tool chains so and its
really badly documented so yeah got that
done and then chapter 12 on software
upgrade incredibly powerful feature and
airline got that done and there worked
well you know despite it being really
badly documented and then I got to
chapter 13 and you know chapter 14 was
completely new booty was actually really
excited by the title it was originally
called node architecture and what I
started doing is you know where for this
how do you architect a single airline
node
that that's what I thought I was going
to write about and so I start writing
and I continue writing and then I write
a bit more and then I start thinking
about the outage you know we had once
when we were on a customer site and you
know what we did to avoid you know
making sure that if that particular case
happened avoid that outage and we just
kept on going and you know before I knew
it I had probably in excess of you know
40 50 pages and you're my editor about
three four five months had gone since we
sent anything to edit their net it was
jumping up and down you know and yours
Grand Francesca what's in your case yeah
so I just went in and you have sent him
everything I had and and explain you
know what I'm trying to do with this
chapter is yeah there's a popular belief
that airline is scalable by magic you
know you just use our long and
everything is going to be scalable and
you use hair long and everything's going
to be reliable when in fact you know a
lot of it it's much easier to write
scalable systems much easier to write
reliable systems because of the
programming model and because of the
semantics of the language and you know I
was trying to give the guidelines and on
how your system would be available and
reliable so I sent everything off to the
editor and he usually responds you know
reads through the chapter and no matter
of length and no response within a day
and but a week later I still hadn't
heard back and finally he does come back
to me and starts organizing all of these
ideas and all of these experiences which
I was trying to formalize so I think you
need four chapters here at least and and
that's where we ended up you know taking
you know 14 and you're breaking it up
into into four more chapters and what
else you know so I started taking it
breaking up into chapters and well it's
doing I said okay why don't we start
trying to formalize the way I go in and
would design an architecture learning
system you know breaking everything up
into 10 easy steps and yeah I had done
that with software upgrade you know you
try to make soft rock great appear easy
in airline and and I managed to break it
down into four
five steps and the same we've released
sounding so that's exactly what I did
and you know in this talk what I'm going
to go is walk you through your the
different steps I would take you know
when going in and architecting a system
and it it doesn't matter if it's a
betting exchange or if it's the other
backend for a for a maximum of users
online game or or even a telco system
you know the concept and the principles
are all the same you know highlighting
things one should think about so the
first step is you know think in terms of
distribution now airline has built-in
distribution in the language so what
what you tend to use this distribution
for is to have loosely coupled nodes so
a node is an instance of an errand and
virtual machine running and what what
you tend to do is split up nodes into
node types I think some people call this
microservices these days or
service-oriented architectures or yeah
whatever buzzword it happens to be it
you know we've probably been doing it
for 20 years now so you know multan know
types are merely a way for us to
describe the overall responsibility of a
particular node so I mean in this case
I've just taken a your typical
three-tier architecture which you use in
the web world you know the front end is
what we call the front end nodes so all
we have here is nodes which are
responsible for managing connections
towards clients so it could be a web
server you could be running a REST API
web sockets you name it and they're
usually stateless and usually considered
unsafe as you know they're facing
Internet we then have what we call logic
nodes and that's where you tend to put
all of your business logic so all of the
computation all of them you're switching
which deals with a request and then
we've got a set of notes called service
nodes no service nodes could be things
such as you know API is towards further
party services it could be a database
you know possibly written airline it
could be it could be just a note which
provides a particular
of service for the rest of the system
allowing it to execute and the goal is
here start you know the first step in
architecting is you need to try to split
your systems functionality into
manageable standalone nodes and you do
that too for a variety of reasons
instead of having one big monolith you
know first of all identify and isolate
failure the second is optimizations you
know you try to you by isolating and
reducing what every snow does it allows
you to you know make sure that you know
your nodes either memory bound or CPU
bound or i/o bound you split your safe
notes versus you're unsafe nodes in this
particular case you know this the web
near the web services front ends you
know the exposed to the Internet you
know you might want to put them behind
the firewall and and then protect your
business logic now once you've split up
your system functionality into knows and
you just need to have a rough idea in
doing it you need to go in and decide
what distributed architecture pattern
you're going to use and what what comes
out of the box with airline is the fully
meshed architecture you've got nodes
which get connected to each other and
they ended up becoming fully connected
so if you've got five nodes you know in
a single cluster they'll all be
connected to each other which has its
pros and cons it pros it's very easy to
do it's it's scalable you can you can
easily you know write a program to run
on a single node and then easily
distribute it because of the semantics
of the distribution model the problem
there is that you've got limitations if
you've got fully mesh nodes once you
start hitting 50 to 70 nodes you know
you start getting a lot of redundant
connections and all of these connections
monitor each other to send tick tocks
here when there's no message passing
going through and that starts creating
overheads so you know that creates a
limit which in the telco space was never
an issue
it was only when you know they started
dealing with kind of whatsapp type of
scale or even your machine zone which is
running currently learning the back and
the largest playing field with the
largest number of users that's when we
started seeing limitations and we
started thinking through you know and
looking at other approaches and the most
common approach is it used right now you
know scaling beyond clusters of 70 nodes
or more is the Dynamo principle and
that's because you know thanks to react
the database there's a library called
react or which provides us a lot of the
implementations which are described in
the dynamo paper a lot of the features
described in the dynamic paper what you
do is you've got your key space and you
divide it into you know into a set of
partitions we're in this in this
particular example let me just go onto
this slide what you do is you have a
special key and what you do is you use
the dynamo principle to redirect your
request to a particular node you you go
in and you hash the session it could be
a session ID for example what you're
doing it could be a unique identifier
you hash it to a particular vinodh now
AV node is a virtual node and what
happens is that your airline nodes will
contain a set of vinos in this example
we split our whole key space into 64 V
nodes and by splitting into 64b nodes we
now go in and no find the V node which
then gets forwarded to that particular
node and using this approach we now get
you know we're now able to add and
remove nodes dynamically we're hashing
but the disruption by adding and
removing those is minimal because it's
just facing I assume we went in and
removed node 1 we would have to remove
for V nodes and move that data you know
across 64 some of the other 64 nodes it
also gives us the ability to have
redundancy because we can incopy V notes
across other knows if one node goes down
we get we get if 10 goes down we can
then your access and the other node
which is which is on standby and the
Dino principle andrea corr is still very
much based on a fully connected network
so you know the way we start scaling
these systems now is using the whole
react node the react or node as a switch
as a giant switch where your requests
are coming in from the front end server
they get forwarded to one of the logical
nodes and then from there they get
handled and managed and then forward to
another logic node within the whole
react or node and passed on to one of
the service nodes and so you know this
allows us to start combining different
distribution methods and styles here's
another example which also uses react or
it's the vent store you might have heard
of it it's a time series database which
uses the record as a giant switch and
then it adds all of the nodes you know
as you're creating a star like shape and
so it can dynamically scale up and down
you're managing your up to millions of
reads per second so what you know real
core gives us is your consistent hashing
so consistent hashing issues that when
notes come and go your whole key space
is not rehashed at one go it gives us
virtual nodes which give us duplication
as well give us gives us redundancy so
that your data can be copied across
nodes it gives us the gossip protocol
which where nodes queer each other for
the health and well-being and in case
notes get out of sync in kit because of
network partitions it will also your go
in and sync you know sync them back into
your what we call eventual consistency
and finally we've got hinted handoffs
well which is if nodes come back up you
know data will be moved from one node to
another and sloppy quorums and I'm not
going to go into these details but it's
these are items where in the space of
distributed computing are almost a must
another tip another typical arc
texas service orchestration service bus
consider very enterprising i guess by
many but you know very popular today and
even there you could have your service
bus your messaging bus and then have
clusters all along so you could once
again you start having clusters with you
know 50 60 nodes you're fully connected
then you're connected together by a
service bus and you know one of the
things to decide on when you when you're
dealing with your distributed
architectural pattern is you need to
start taking into consideration things
such as scalability availability and
reliability which are free things where
you know your final architecture will
consist of a lot of trade-offs in
between them you know you have a system
with a static number of nodes is that
enough and in most cases it is your
typical in your static system will
handle you know maybe 15-20 nodes on you
know mid-sized hardware it will handle
twenty thirty to forty thousand
transactions per second another question
you ask is you really need distributed
frameworks do you really need to add the
complexity of react or you need to scale
up and scale down dynamically another
really cool well distributed
architecture and I've actually not seen
this in an airlink system but it's it's
basically the peer-to-peer approach
where every node is both your client and
a server and and they're connected you
know with connections which come and go
and even there you start using and this
is probably the most scalable
architecture of them all where you start
plugging in front end nodes and service
nodes your two different parts of the
network now once you've distributed to
deciding what distributed pattern you're
going to use next thing you need to
decide is what network protocols are you
going to use to let the nose communicate
with each other and distributed airline
well it comes out of the box but that's
not always the right the right position
in this example you assume you've got
your distributed rare line you know the
front-end web service and your logic
nodes running is
buted airline hackers new again access
to your front end servers you know they
could they basically get access to these
back end machines my favorite command is
RPC colon Cole and then you provide the
node name and then OS which is the OS
library CMD which is a command and then
RM minus RF and so you know that you
have someone who may be your likes neat
hard drives you know we would be happy
whenever that that gets it that way so
what what you do is you need to make
sure that you know with your web servers
you know and their back-end you might
want to go in and have SSL for example
you might want to have a TCP IP
connectivity and they're even cases
where they're even cases where you know
you security is not an issue but you
need to have multiple socket connections
because in a single port you know
becomes a bottleneck this is an example
you know we had an instant messaging
server where we were handling your tens
of thousands of instant messages per
second as soon as we expanded the
service is this gateway to handle email
as well you know a single tcp/ip
connection became the bottleneck and we
had to run about half a dozen TCP IP
connections between the nodes because
emails attachments greatly greatly
increase the size so what so you
basically to go in and decide no do you
use distributed airline do you use your
sockets do you anticipate is based on
tcp/ip you know do you wanna run it over
MPI zeromq UDP SSL do you want to have
rest api is between your interval
between your nose mqp SNMP no there is
no one-size-fits-all and that's one of
the decisions you need to make and
finally once you've decided your nodes
once you've decided you know types once
you decided you distributed
architectures or create your patterns
you need to go in and define the node
interfaces you know what functions do
this particular nodes export and usually
the way to do that is using your stories
you start going through your stories and
seeing what parts what
nodes types you impact in your request
when handling your request so what you
tend to get out of you know running
through the stories is you know the
function your the arguments you need in
that particular function as well as the
data which you need locally on that node
and in that way you start developing
your state and your data model you when
defining your node interfaces things to
keep in mind is you know reducing
communication you want to reduce your
data dependencies make sure you've got a
redundancy as well and wherever possible
standardize the ipi api so you know
these would be the first four steps you
know split your systems functionality
manageable stand-alone node types once
you've done that the side you know the
distributor architectural pattern you're
going to use decide what network
protocols your different nodes node
families and clusters will use and your
most of the time it's the mixture of
these and when you're done you define
your node interfaces the state of you
know you'll have in the individual nodes
and your data model now at this point
you know you'll have something
end-to-end which works end to end and
that's where you start thinking about
availability so the availability it
defines your the uptime of a system over
a certain period of time usually we
refer to highly highly available systems
as system which are very small downtime
and that will include your software
upgrade and maintenance included now
when you're dealing with availability
availability encompass is actually a
large number of terms you need to deal
with the first is fault tolerance and
fault tolerance is the ability of your
system to act predictably under failure
and you know that could mean you're
returning an error as well so assume
your client sends a request to your
front end server that request gets
parsed handle you know converge you to
narrowing term and then sent back to a
logic node now if your logic node
crashes or your logic node is slow in
responding or if there's a network issue
it could be a large variety of items
which go wrong yeah you get back an
error in a timeout you send back an
error to the client and
and you know this is a full tolerant
system it acts in a predictable way on
the failure and in this way in this case
you know the predictable item is
returning an error and when you get back
areas you know you know a predictable
action could mean you're looking going
and looking for other nodes alternative
knows where you could execute your
requested i'm going to give you an
example very soon about that now the
second item availability or brings to
the picture is resilience so resilience
is the ability of a system which or a
node which has failed to quickly recover
and so for example is example here you
know client sends a request yeah the
front-end crashes so the client sends a
new request the front end node has been
restarted quickly or a load balance has
been reconfigured and you know we're
able to handle that particular query and
third is reliability so when you think
about availability reliability comes
into the picture as well and the
reliability of reliability of a system
is the ability to function on the
particular predefined conditions and
that includes things such as errors or
your data becoming inconsistent so you
know in this particular kick you know
case you know we go in we send the
request to a front end server once we
send a request to the front end server
it might send it to a back-end server
the back end server you know for some
reason doesn't respond now the beauty of
dealing with this in airline is if
something goes wrong with the backend
server it doesn't matter if it's the
process in the node which is terminated
or crashed it doesn't matter if the node
is crashed it doesn't matter if the
process or the note is extremely slow at
responding so we timeout it doesn't
matter if there's a network partition
what we do is in between nodes we handle
all of these errors the same you often
get people saying oh the network's
reliable but it won't be reliable but
we're working to make it even more
reliable that's great but you know it
won't make your life as an airline
developer much easier because you still
need to handle the case is where your
network might never go down at least
near even if
customer claims so but the process then
process might go down the node might be
overload and be extremely strong
responding so you handle all of these
errors in exactly the same way you know
which really simplifies the whole model
but you assume a request fails what
happens here is the front end server now
forwards it on to another node this node
response and the client gets to reply
you know completely unable is over all
the drama happening behind the scenes so
you know these are these are the items
you need to think about so your
reliability resilience and and fault
tolerance now together with this you
need to start making a decision on how
you're going to share your data across
nodes and what you need to do is start
thinking in terms of failure when you
share you that you distribute your data
to mitigate failure and that's probably
one of the hardest things you tend to do
when you're dealing with distributed
systems so once again your typical
example of what you know I would maybe
walk through when architecting a system
is we go in you know we send the request
to the front end server could be a login
request the logging request gets forward
your logic node and we create a session
in this logic note stored in this logic
now another client goes in its directed
to this front end server which handles
and stores the session data in the
second node now if this logic know
terminates when a client goes in a good
any you know sends a request it gets
back an unknown session now if you
controlling the client what you would
automatically do if your sessions not
recognizes go in and create a new login
then all of a sudden you're actually
have created a new session on the node
which is running this pretty much it
equates to sharding and share nothing
architecture where you're no data so
that the sure nothing architecture with
no data shared across the nodes you will
have areas where you go and you share
some of the data but not all of it and
once again you're doing a trade-off here
between
in reliability and availability here so
assume you we go in and decide to start
sharing all of the session data across
nodes so if a user logs on you know we
store a session in this logic node and
also in this logic node but decide not
to share the shopping cart so the user
decides to go in and buy a book he buys
a book and it gets put into this logic
note this logic no terminates when he
goes in and you know decides to buy
train set his request is forwarded to
the standby note which also had this
copy of the session and the train set
gets put into a shopping cart over there
but the two are actually separated there
are two separate shopping carts you
won't go in and copy all of that data
that's another of the trade-offs you
need to decide what to do biz it could
you know in the case of copying the
shopping cart it's not that expensive
but it there might be large volumes of
data you know instant messaging you're
you're handling your thousands of
messages per second in a sink sink sink
single system you might want to copy
this session but if you lose a node you
might lose the message go and through
that particular node at that point in
time but that means that your system
will scale much much more and finally
you know the third thing you need to
think of is the whole share everything
that's the third approach and in this
particular case we call this primary
primary replication you go in you buy a
book and the book gets stored in both
nodes you know you lose a node you buy a
train set it gets stored here and then
note comes back up you decide to remove
the book and it gets removed from both
nodes and you know that this is
basically primary primary replication is
the way call and it contrasts with
primary secondary application where a
single primary node is responsible for
the date on that particular node now so
far so good you know one of the biggest
problems when you're dealing with
distributed data is network partitioning
and you know the decisions you need to
make is how do you want if you get
network partition if your nodes are
still up but they're separated from each
other so you get a book in here in a
train set in here when the network comes
back up how do you merge the data back
together and I think in parallel right
now your marks appear is giving a talk
on CR dt's which is one of the
approaches we use so you can have
eventually consistent data where you
know the data is it you know it tries to
merge the data when the nodes come back
up strong consistency where requests
actually fail if you cannot guarantee
the consistency of the data and then
you've got everything there in between
and in this particular example I mean
think of the dynamo paper and Amazon
what happens if you get a network
partition you go in you delete some
items from a shopping cart which are
still in the shopping cart which you've
got you know which is in the node you
can't access when a note comes back up
the shopping carts are merged again so
the item you delete it will reappear in
your shopping cart that's how amazon
deals with it you know in many cases you
know you'd end up using a distributed
database such as react or cassandra
which does a lot of this for you but
it's once again a decision you need to
make and finally you know I've been
describing all of these functions and
whilst describing these functions every
request has a retry strategy we know
when you send a request from one node to
another you can you either go in and
decide you know when you send a request
you can go in and have three approaches
one is the at the most once approach so
you send a request to a node and once
you've done it's just a synchronous and
then you continue executing so it's
almost like a fire-and-forget and you've
got the risk of this request being lost
because you know the network might me
down you might be crashing the note
we're handling it might be crashing but
it's usually the fastest and the most
scalable of them the second requests
retry strategy is that at least once so
you need to guarantee that the request
executes at-least once and that's when
you'd go in and you'd go in and you'd
send a request let's see here here for
example we've got at least once we go in
we send a request we don't know if this
request has actually been executed by
the node before the node terminated all
we know is that we didn't get back an
acknowledgement and the reason we didn't
get back an acknowledgement could be
that this particular node could be
extremely busy so it could still you
know have queued that request and might
still be processing it but that requests
yeah but we happen to time it could be
that the process was the request was
processed but that acknowledgement was
lost that the network went down after it
was processed or it might not have been
processed at all so what we do is we
send it to another logic node and we
continue sending it until one of the
nodes actually acknowledges it and
that's the at least once approach least
ones request and then finally we have
the exactly once the exactly ones
request and that's those are the
requests which you know we need to
guarantee have executed only once anyone
from Greece here okay I'm Italian so I'm
allowed to joke about briefs what if you
were you know Brussels is moving seven
billion euro to Greece as part of a loan
and you know that you need to make sure
happens exactly once there's a problem
there though you cannot guarantee it
exactly once if your in your honor
running everything in a distributed
system because your acknowledgement
might be lost the request might be lost
the request might be received and
handled but you don't you just don't
know and so what happens with these
retry strategies with exactly once is it
is successful or it has failed you know
it's either black or white and if it has
failed you don't know what state you've
left your system in and that's when you
need to start triggering that's when you
need to start triggering scripts or in
some cases even human interaction and
intervention which actually goes in and
tries to
her out what happens and you know if
you're dealing for example with money
with stocks payments you know it's an
exactly once approached and you know
something goes wrong usually the back
office goes in and kicks in and starts
looking what's happening so you know
writing away I basically ticked off five
and six where you know for every
interface function in your nose you need
to pick a retry strategy so you know
exactly once only once and at least once
and that's for every function you need
to actually go in and trace everything
which can go wrong enroute your from
node to node and from process to process
it's tedious it takes time but it has to
be done and finally you know for all of
your state and data you need to pick
your you're sharing strategy cross nodes
and you do that you know keeping in mind
your retry strategy now all of these
choices will have you know trade-offs
involved with them you yeah you'll know
of the cap theorem it's very very
similar principles which are applied to
you're dealing with consistency and
availability so the most consistent
approach is the exactly once approach
but it's also the least available
because you know to guarantee
consistency you might have to take your
note offline while someone's trying to
figure out you know what went wrong at
least once becomes more available but at
least once you need to save store state
and continue sending requests until you
get an acknowledgement so you know it's
consistent and it's available but
there's more risk of failure and then
finally you know the most available
probably the least consistent is at the
most ones that the most once approach
where you fire and forget so you've got
no idea if that SMS soria that instant
message receives it arrived to its
destination and we've consistency it's
exactly the same with right reliability
when you're sharing your data if you
share everything it will be the most
available system you lose a node you'll
have all of your data on this other node
so you know your users will hopefully
not experience any downtime
and then you'll share something becomes
even more reliable and then share
nothing is the most reliable of them all
it sorry it's the most available of
their mold but the least reliable
because you could lose data and as a
result you know when you send the
request you're not getting back to
response you're expecting so you lose a
note you might find an empty shopping
cart and the same applies to scalability
in this case when you're dealing with
scalability the choices you've just made
will have trade-offs between consistency
and availability where ya at the most
once is the most scalable and only once
sorry is the least scalable of our
apologies because yeah you have a lot of
guarantees you need to make it's usually
a two-phase commit across nodes which is
not always a good idea versus the only
ones only once you fire and forget and
you know it will become really scare
birds it's real it's a really it's a
fast approach and the same with
availability you know the share
everything will become the least
scalable to share nothing the most
scalable so you know at this point in
time you know you need to start thinking
of the scalability the choice is your
may you've made you know how scalable
are they do they fit your needs and you
know most of the time the most likely
will now back in the days we had big
centralized servers they used to be the
bottleneck we needed more scalability
with x yeah we wait one and a half years
we'd buy a faster computer and your
system would run twice as fast that's
not the case anymore what we're doing
now is scaling horizontally you know
we're throwing commodity hardware the
problem and and that's where you know
you can read and write on every server
your replicated data and ensuring that
losing a server won't lose any data so
you know these are the trade-offs you
need to think of you know when you're
dealing with with scalability and at
this point in time you might want to go
back and actually reveal
some of the design choices you made and
you you will know you know what your
fruit bit of the system is as soon as
you start load testing it and every
system you should not go into production
unless it's been load testing and
there's several types of load of kind of
stress testing or capacity planning as I
call it which you should be dealing with
first of all is a stress testing you
start increasing the load on your system
to find different breaking points you
know when you break your logic nodes
when you hit bottlenecks in your
database you know when can you your web
server not manage it anymore I mean
there was some great benchmarks over
Phoenix alexia framework where they were
handling something excess of two million
TCP IP connections on a single machine
the problem there is great yeah you've
got the connections but you know what
load can that machine actually withstand
you know it doesn't have enough capacity
to actually manage parcel of the request
and then send them to back and logic
nodes the second part is so you've got
stress testing you got load testing
where you run the system close once
you've discover your breaking point you
run it close to breaking point to ensure
that there's no degradation of service
that you know after five or six hours of
running it you don't start experiencing
I or starvation you don't start started
experiencing network congestion and
other issues and then finally you've got
you've got soaked testing where you let
your system run for weeks and make sure
that you don't get any degradation of
service across weeks now out of your
stress testing you should then go in and
figure out where to put in your back
pressure and your load regulation back
pressure is when your your systems
overload it and start rejecting requests
versus load regulation where it tries to
even out the peaks and troughs by
queuing request so assume you've got a
third-party API right here which can
handle 30 requests per second you start
fra telling these requests ensuring that
you know that you only send further
requests down and so even if your system
sending 60 you will have queued verti
and you'll handle them on a first-come
first-served basis
and this is where you start having
things such as you know at least in
airline you you've got a ready-made
applications such as jobs safety valve
another and another application you can
just plug in and start using and most of
the time you're the back pressures
handled by the tcp/ip stack itself you
know it's your biggest challenge when
your stress testing airlink systems is
actually creating enough load on the
system's themselves I think our first
experience of going like we're really
big large-scale product we started
stress testing and first thing we happen
is we managed to get the firewalls to
crash so we shut them off next thing
which happened was we actually got the
load balancers to crash they just
couldn't handle the load the system was
running at thirty forty percent CPU we
wanted to load it even more you know the
infrastructure just couldn't handle it
so we removed the load balancers we got
f5s which at the time were the most
powerful ones around and they were
actually able to extend the load despite
your crashing maybe you know every other
day you know we get a khordam in that
fives as well but running you're
managing you know having massive load
you know is is going to be your biggest
challenge ensuring that your
infrastructure can handle it and then
finally you know when you're dealing
with capacity planning you need to think
of your holes no single point of failure
so make sure that you know if your
network goes down or you've got a power
outage or you lose a node or notice slow
at responding you know make sure in your
end-to-end requests that alternative
routes can be taken you need to think
about load well regulation which
recovered and back pressure and then the
final item which I'm not really covered
here is monitoring and pre-emptive
support don't think that just because
you're running errand on your system is
going to you know keep on running day
and night and without any issues that's
not the case you need to constantly
monitor it and
you need to have constantly a preemptive
support you've heard about the
nine-nine's availability which was you
know three milliseconds of downtime in
per year then that's a bit of a myth but
even there you know as soon as a node
failed there was scripts which were
triggered which immediately handled and
managed the request itself is it time
soon yeah and and so yeah you need you
need to hack be very proactive with your
preemptive support with any air link
system you need to monitor everything
which can be monitored we've had cases
where you know we had your hundreds of
nodes deployed and processes were
crashing in these nodes and the only way
for us to find out was to actually log
on to the notes log on to the log on to
the machine log on to the nodes and you
manually browse the crash reports and
the problems we weren't even noticing
because you're the pro so we're crashing
and restarting you need to start
collecting all of the state into one
place and make sure that you know
somewhere out there one client
experience an issue and so you need to
deal with all of these issues one by one
you need to you know try to monitor
trends and monitor spikes and just make
sure all of these items are handled so
you know just just to go through you
know in the end and I think you know
finally if they are sent off the final
chapter to the editor these are the 10
steps I managed to narrow it down to so
you know split your system functionality
and manageable standalone loads decide
what distributed architectural pattern
you're going to use and note this is
something you need to do very very early
on because it will affect your AP is it
will affect your you know your recovery
strategies it will affect how you share
your data it will affect a lot of things
decide what network protocols you know
node families and clusters will use when
communicating with each other this is
something you maybe don't need to do
immediately but you need to have a good
idea and so if your architect in an
end-to-end system you'll know that you
might have congested in certain parts
define your node interfaces so you know
exactly what functions does each node
respond and have your
state and data models for each
particular node and finally you pick
your retry strategy is it for every call
is it you know at the most ones at least
once or exactly once you need to go in
and you know for your date and state you
know picture sharing strategy across the
node so you know what data and the state
you'll have in individual nodes from
part for you now need to decide how you
share it and once you've done that you
need to reiterate through all of your
steps you know steps one through six and
make sure that what you have actually
suits your specification and this is
usually done you know in conjunction
with low testing stress testing and in
your capacity planning and at that point
you know when you're dealing with
capacity planning and your stress
testing identify where you need back
pressure and load regulation and you
know there's no point in having a system
which angels ten thousand transactions
per second when your database or you
know you've heard your party provider
will only handle a fraction of that so
yeah don't over optimized you know
premature optimization is well is the
root of all evil as joe says and then
finally define your operation and
maintenance approach you need to think
in terms of alarms you need to think in
terms of logs and you need to think in
terms of metrics and doing using these
free you know for monitoring and
pre-emptive support you know and your
last word of advice don't over engineer
system you know even if you're writing
the engine from the next generation your
mess with multi use online game or your
building next wats app always start
small and ensure you get something which
is working end to end there's no point
in having something which is massively
scalable but just doesn't work start
small and then start expanding you know
you have your node types you can always
go back and change your distributor
architectural patterns once you've gone
live yeah any questions no
so this is the book I'm sure you can
find it on BitTorrent and I'm not quite
sure yeah exactly i'm probably is more
up to date than what you get in on
safari but you know I've basically a
chapter 13 is out the other ones will
slowly soon be yeah we will soon be
released and if you do like you don't
like it and you'll use the discount code
elf d you know for a fifty percent of
the digital one thing I shouldn't be
telling you but FD also will work on all
the other Riley titles as well so
they're outside don't tell them I told
you but yeah okay any questions okay I'm
around for next two days so yeah feel
free to come up thank you right</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>