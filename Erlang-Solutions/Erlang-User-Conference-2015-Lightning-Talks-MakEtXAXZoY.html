<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang User Conference 2015 - Lightning Talks | Coder Coacher - Coaching Coders</title><meta content="Erlang User Conference 2015 - Lightning Talks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang User Conference 2015 - Lightning Talks</b></h2><h5 class="post__date">2015-08-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MakEtXAXZoY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and we're going to see you all here my
name is URI and I am researcher from
ballman moscow state technical
university i was a speaker at lambda
days in February I have designed my
model for representing graphs in
parallel and distributed system and
today I just would like to show you a
few possible use cases just a few real
desks but first few slides to
familiarize you with the model in the
case you didn't know that so first of
all the model is for parallel and
distributed system so of course it's
parallel and all vertices are
represented as actors second I use along
standard message passing between actors
so the model is a synchronous because we
don't like bottlenecks third is
hierarchical because of isolating error
etc etc and because of Ellen
architecture it's scalable also because
of architecture and faltering because of
a link architecture and it's pretty easy
to add some new functions you know why
so just let's have a look on the top we
can see a master not this is varun node
and we use it for communication its
bounds all other processes and it's his
main function its its main function G's
P stands for global support process or
global service process what is global
level on global level we know for
example which vertex on which not but we
don't know PID of each vertex because
it's local managing level communicator
is the top process on local managing
level it knows only about our vertices
on this computer node but but it
doesn't know about another vertices
about vertices on another note and here
we also have LSP stands for local
service or process and walkers level are
our vertices very just plain workers so
let's move challenge one genome assembly
branch of science bioinformatics you
know it's a modern trend and graphs in
this branch of science are really really
really huge sometimes it could be about
three million of vertices type of graph
deploying graph and in a representation
will be directed graph model represents
most types of graphs as directed task
just such tough to find a chain of genes
from A to B it's pretty simple and I
used a basic algorithm and here we have
some results I implemented if
implemented it I used only 1024 vertices
but it's pretty enough for some
primitive life forms and what do we have
I would like to show you the red line
the red line is a total time of
computing of switching path / a number
of vertices and what can we see it is
predictable so you can see it's about
own we don't have a table but it's about
five microseconds and it's pretty quick
and you know it's predictable and that's
good the model is scalable so you can
say if I have well 1000 of vertices will
be done in probably in four or five
hundred microseconds of a thousand we
have five milliseconds if I have two
thousand in about 10 milliseconds and
where's good values
is predictable let's more challenge to
is knowledge storage bench of science
informatics artificial allergens
databases etc usually they use multi
graphs to show some relationships
between a and B ages usually don't have
their weights instead they have a type
of relation in representations of graph
is directed graph it will be directed
multigraph and task just find relations
between a and B it will be just some
kind of search of puffs searching if you
want to know more I could recommend you
to have a look at this it's very useful
link but probably you should be familiar
with some mathematical stuff like
chances and the third challenge hyper
graph in M dimensional linear space it's
more about pure mathematics major
science math type of graph hyper graph
in a representation of graph connects
graph what's this Oh task just to
allocate hyper graph in n dimensional
linear space without intersections and
self self intersection of edges what is
connects graph here you can see um this
one yep this one is hyper graph
traditional there x1 x2 x3 and x4 are
vertices you to you free and you won our
edges and now you can see that both
edges and vertices represented in a form
of a onyx graph as you know some kind of
vertices and that's pretty simple and of
a model card and special mechanism for
it and how should we saw it how to
represent linear spaces well we know
what's the functional spaces and we have
fun evaluations for it lambda
calculus to be exact so we can implement
a functional space as a global service
process using fun evaluation and so we
can implement an M dimensional linear
space and that's pretty well and
vertices and spaces or one space are
sending messages to each hour just to
check can are the voters with all its
edges can be allocated without
intersections and self intersections so
let's move to vent you can ask me some
questions give your suggestions or even
more challenges I'll be very grateful
here's my mail my Twitter and my full
speech at lambda days if you are
interested in this model thank you yeah
so I'm just going to present something
I'll call spell one it's a simple ll 1
parser generators railing and the
question first question is of course why
another parser generator I mean we've
got yak why not you Zeke and yak is lalr
1 so it can handle much more grammars
they'll wonder can so why bother right
and there are a couple of problems with
the echo it's one is it's not re-enter
that means i have to give the exact
number of tokens it needs to pass
something if i give it to few tokens it
gets an error if i give with too many
tokens it gets an error that's fine for
things like our language we got the doc
terminator so you know exactly how many
tokens you're going to your past but if
you're doing something else it doesn't
the reason I'm doing this was because
i'm doing my Lisp implementation and I
have a handwritten piles of a much nicer
if I could just generate the passer
instead and there there is no way of
exactly knowing which right bracket
which right parenthesis is the ending
for it and I don't want to read the
whole file so that's that's the problem
another problem with ya kid well one of
the reasons that this is a problem with
the heck it's
partially the way yech is implemented
and also think it's a way that the
grammars will handling the grammars work
you need some form as something to say
some token something to say this is the
end of the input keep going right this
is what so generous the actual parsing
of it all another problem with the egg
is is locked to Ali yes it's written in
our line but the code is just one big
mess so it's pretty hard to separate and
break it out so that's why I did spell
one it can only handle ll 1 grammars so
I can handle much a much simpler branch
of grammars it's fully reentrant in the
sense that if you give it to few
characters or too few token sorry you'll
get back more with a continuation asking
for more tokens then you can call with
more tokens and we'll keep on passing
until it's done returns it ok it will
turn what the passes and will turn any
remaining tokens so it's perfect for you
to save something you were entering
expressions from a command line or
something like this and just keep on
going as much as you want to yeah it
supports multiple interfaces so it can
ok it's written in our line but it can
it can handle input in different
languages so there is an in there is a
one module putt that's part of it that
can pass grammars written describe the
now line and can input files now like an
output files and having from these
grammars there's another one that can
input your passes in input LFE and
generate LFE output from it so this
works sorry this works in the same ways
yet you get you have your grammar file
you run the grammar file through it and
you'll get a parcel file and you can buy
that so you could actually plug other
languages and if you want to the way
they split up and do that just add
another module with the inputs and
outputs of that specific language the a
line interface just borrows yek syntax
there's really no reason not to it's
perfectly okay for this anyway
so for example just all here is the
other Thea grammar written as an ally in
the NEX syntax so oh we have a pen do we
does this thing that light up as well
yeah we do yeah so so we borrowed
borrowing the X in tax here so here are
all the terminals which we can get from
our part from our tokenizer and here are
the non-terminal forms and here is a
grammar description so it's a it's a
format which is an ex expression next
expression is one of these things here
and if it starts with the parenthesis is
a list there's a list in square brackets
etc etc this thing is well it's the
vectors it's the vector syntax from from
common list which i use for tuples and
that contains a proper list this is the
binary syntax etc and here we're just
calling function for and within the same
way as in yak we're using this or the
the atoms dollar one dollar to dollar
three etc for saying which the symbols
here we're extracting the values from
and here is remaining bits from for list
it's a nice expression with a list tail
or empty and in this tale here is not it
is an ex expression with another list
tail or is the dot minus s expression
the dot should be quoted or it's an
empty I think about this form it's just
a very simple grammar form and here is
exactly the same grammar written in LFA
so if you push the other one into the
air line interface you'll get an
hour-long file which is then passed if
you push this one in you'll get nelly
three-part finalize output you and you
can compile that as well too so here's
exactly the same grammar this is much
easier to read because it's lot less
syntax for them I don't actually need
any specific syntax for just for just
reading lists so here we have the
terminals again here we have the non
terminals and here we just have each
rule if you don't like this syntax you
could just change it just as long as you
put the input input to it
to do that and here are the remaining
bits as well so yep it's a simple tool
it works it generates the same result of
my handwritten compiler so I'm happy
which seems to do the right thing and
it's on github of course the
documentation is rather lacking to be
honest but I'll get around it's a
feature yeah you have to work it up
there are a couple of things it doesn't
do it doesn't try and handle ref left
recursion which you can get another 1
grammars it just it happily warns you or
tells you that you've got one and
generates an error and the same thing if
you and if you have ambiguous some rules
they're relatively simple ways doing
left recursion or probably around to
adding that there's not something I need
myself so yeah that's basically it so I
mean why at the moment no right it's
just I have to fix up the compiling to
make file for the LFE so we'll do that
there's 11 slightly practical problem
you can't always depend that you
actually have this in your path when you
could when you're generating LFE so what
I'll do is I'll generate the a-line file
the pacifier for which I just would just
leave there if you want to that's just a
practical detail but it does work
12 so hello I'm I'm talking about hard
real-time Ellen again I've like headed
as last slides of several other talks
but since I have a little bit more
slides in the meantime I try to like
keep it in your mind that I'm I'm
working on it so what is hard real-time
it's actually actually a pretty soft
defined term it's like there's no hard
hard real-time definition yeah it's like
basically that's the one of the oldest
definitions the other thing is it's
usually an era when it's late it's not
basically a late answer is the wrong
answer that's one other way to look at
it like basically if it's too late it
blows up like sync nuclear reactor so if
you want hard real-time in the air long
we need underlying hardware time
operating system otherwise the operating
system would mess everything up we do
hard real-time nalang because yeah for
obvious reason I did this part already
it's like i potted them along to a small
hard real-time operating system for
embedded systems and it will be
published pretty soon it's it's not done
yet I mean it's done yet but it's still
entangled with customer code so i have
to disentangle it and rebase it to their
current yelling version so are tempers
are pretty small it's a system that can
be pretty small I'm not saying I'm
running Ellen on these small systems but
it's not like the operating system is
not getting in a way of getting on small
computers it's it's also pretty simple
so we don't have virtual memory we only
have basically one process that runs
that so that's the whole thing we have
threads so
everything shares memory with everything
and but it runs basically on everything
that has 32 bits and on some 16 bits but
that's not relevant for airline so let's
look at how this looks so we have our
airline code we have their long run time
and the box below is basically the
operating system executed which gets
linked to their long vm basically what
you get out of it is you get one
executable then which you can put on a
flash just put it in memory and run it
and on the site basically you have you
can have like see application parts
which like use normal API of the of the
operating system and you can use
LinkedIn drivers and and and nifs lifts
are possible of recent because somebody
in ericsson needed also nifs that are
not dynamically loaded and dynamically
linked so I can reuse the static link
nifs and got this for free thank you by
the way so how would this look if we
also have hard real-time processes in
the Allen basically what you need is you
have like to have a second kind of VM
running on a higher hard real-time
priority than all the rest so the hard
work the Allen hard real-time processes
run on a higher priority and preempt
everything else basically so you have
basically the thing duplicated and the
rest is the same so how could how could
this work in a dynamically in a dynamic
language which has garbage collection
and like Ellen and message passing and
everything what does hard real-time mean
in this context because the context is
we have messages so we get the message
at the beginning that starts our
timeline basically time flows from left
right and in the green area there we do
all our actions and we have deadlines
where we have to like
produce results the first deadline is
that TD is the deadline by which we have
to make all actions like send out every
message that is caused by the incoming
message or right values to ports to
devices to whatever and because we have
a garbage collected language we need a
second time frame basically that's a
cover recovery deadline that's basically
where we do our garbage collection and
we probably do it every time just in
case doing it not every time is like an
optimization performance but hard
real-time costs your performance it's
not that hard real-time is faster than
non hard real-time it's just guaranteed
on the same time it's not faster it
costs you performance always so after
the recovery time we can receive another
message and this is pretty much
translate very easily into like the
design space of hard real-time systems
because the recovery time interval and
basically defines the maximum message
right we can get and that's a thing you
can work with so how would you schedule
this for message passing I think the
best scheduler that that fits this is
earliest deadline first so basically
they're the default scheduler in in
Hardware time is priority based you have
fixed priorities and higher priorities
prayer and lower priorities and then you
get priority inversions in all the fun
starts to happen if you have messages
and actions happening I think it's it's
better fitting if you have earlier state
line first which means who you define
deadlines in your system I mean we have
to define deadlines that I showed in
last site and whoever has the next
deadline runs at the highest priority
basically runs so if you have like two
two processes the two timelines we have
a message coming in we do our actions we
have the first deadline the left TD on
the on the top so we run so what what
happens next
in the meantime while we process our
stuff the second process also got a
message so so the process the process to
on the second level basically has the
next deadline so this process runs so
garbage collection is basically having a
later date line of course so it runs
afterwards and and then the garbage
collection of the next process so how
you design usually a system like this
that the normal reaction time like the
TD is pretty short and the recovery time
is a larger value but yeah so how would
we make the garbage collection be can
Kemp can have a fixed runtime like a
limited run time what we have to avoid
at all costs is allocating a larger heap
because this this is not bounded that
this can like take time so we have a
fixed size heap and we are located twice
so we have a fixed size heap with two
parts the island garbage collector can
be reused because it's a copying garbage
collector so we always copy by at each
each round we copy from one heap to the
other if we run out of meat if you run
out of heap and then you crash mailbox
size needs to be limited like the number
of messages and the amount number of
memory can you can have in the mailbox
because otherwise you have the same
allocation problem and so we can you
reuse the garbage collector and we crash
on everything that doesn't work
basically I mean with Ellen and we can
add another thing since we already have
our heap and have a definite starting
point we could have a faster restart by
not relying on the supervisor for
restarting but just jumping to the
beginning throw away everything and jump
to the beginning of the process it's
very quick recovery and we can put a
counter on it and if you do too many of
those we really creation let let the
supervisor handle it because then we
like need to
restart a subsystem there's also an
interesting mailbox behavior that needs
to be configurable I would like to make
everything of this configurable at spawn
you spawn the hard real-time process you
say how much heap you have how much
message box you have and how you want to
handle if anything goes wrong so what
happens if you get too many messages
somebody sent you too many messages you
can basically have these four behaviors
you can crash the sender because he is
the bad guy sent you too much not a lot
you can crash the receiver because let's
crash or you can keep the old message
and keep the new message keep the old
message means it's actually makes in a
real-time context it makes the least
sense keep the old man just keep the new
messages this is most sensor let's say
if its sensor data like the new
temperature reading I'm so if I get a
newer temperature reading before I
process by old one who cares about the
old temperature so keep the new one keep
the old one I don't know really maybe if
you have some i don't know i would put
in it all for because we could questions
yes
but but when you when you have a crush
you don't observe your hard real-time
stuff you're not supposed to crash to
button yeah but if you break the promise
it's an arrow that's why we crash that
that's how it works I mean I mean in
real world hard real-time systems are
built like you you test it and then you
put ten percent extra space in it and if
it works with ten percent more extra
space then you ship it and then your
boss comes and once more features and
that's why you use up to ten percent and
then stuff breaks in the car that's a
real world example like shrunk a little
bit happens at several car manufacturers
all the time actually I think Toyota has
something documented like this and what
I very it was a very costly
documentation yes supervisors of course
if you crash it these crushes then then
then then you probably have to crash
other other processes also like the
supervisor has to take them down and we
start the whole sub system that you can
recover maybe hopefully before the
atomic reactor blows up or something
like this but basically crashing is
something is wrong I have to get rid of
my state and for example crashing this
end that makes total sense for like if
if non hard real-time process ends our
stuff and they misbehave and send us too
much we crash the sender because the
sender's supervisor you need more
configuration space for this and then
find out what what's missing and edit I
mean we have to like half the running
system first before we learn how to how
to use it actually more questions
oh you have you have to specify the
whole system so the whole hard real-time
systems all you need you need to have
like a static description at startup
time who communicates to whom who sends
messages to whom what all the deadlines
so you know how many processes you have
you don't generate them like arbitrary
genomic and actually with all these
input you can actually do a static
analysis of for example heap size and
prove that the heap never overflows if
you if you don't get too much messages
in so you can augment the type system to
do a static analysis and this is
actually planned but somebody else is
doing the static analysis it's not much
I'm a I'm a practical guy I want I want
to build things at work yeah yeah I mean
in theory in us we should use static
analysis for for their hard work time
stuff where they don't do either so why
should I now actually we plan to the
static analysis and it's very helpful
and actually industry does static
analysis are very primitive one because
for example in a car you have a very
fixed schedule that what they call
scheduler is basically a area function
pointers with with with run times and
you and you do a static schedule you
need a fixed static schedule so you can
you can know at compile time that you
over on your schedule and that's they
solve the problem by getting in more
primitive and if you over on your
deadline as a single process which is
actually just a callback function then
then you get an error and this you test
and for this they don't do static
analysis I know they don't I I was
describing I was describing how to do is
do knock in like in the car ECU and see
here
you should find out what's the right
time you need to have enough time to
recover otherwise you can't and this and
this you can actually you can measure
and you can when you test your system
and if you're always crash when when
something goes wrong then you will
notice during testing so it's there's a
way to find out during testing that it
doesn't work that you didn't put enough
time you always specified your system or
you're on the play you didn't give him
give it enough cpu power not incremental
that would be an optimization it's I I
would start with a very primitive run
the normally Allen garbage collector
copy everything over you don't have to
copy much stuff because usually the
hardware time stuff is small has a small
heap I mean it's doing this in your
longest mainly has has a use the
positive use that you can interact very
easily with other airline code because
most hard real-time systems you have
like tiny hard real-time requirement
code like ten percent of the chorus
hardware a time requirement and the rest
is dynamic stuff like a web server and
you want to have to have to have it in
one system and interacted well and
that's that's the reason so it's
actually it should be very simple to
verify it and to because it's like you
build simple systems if you have our do
real time you don't you don't be
complicated hard real-time system at
least none that work
yeah haha yeah actually I'm funding this
because I member of the Planetary
Society yeah well they had the problem
here they have for everybody who didn't
know they had a learning system and
overflew the the hottest can had to wait
for a strike cosmic ray resetting the
processor so they can talk to the CPU
again and actually came earlier than
they expected though they were lucky and
it was not the actual light sale it was
only the the demo lights I like the they
were not planning the next one should
sail this one is just unfolding the
selling crashing okay thank you hi I'm
so this probably won't be as exciting as
the previous ones this is something that
literally came to my mind about two
hours ago I discussed this with a couple
of people here and they suggested to
submit a lightning talk so there we go
so I don't know how many people here
build big data systems shipping large
volumes of data so for such systems
there are two main components currently
available there is Apache kafka which is
a massively scalable message queue it
runs in JVM basically what it means is
that what means massively scalable is
there is a single topic which is
basically like a queue and it's
distributed across machines in some
partitions and for processing the data
that you put in Sansa and you use
another Apache project project which is
called Sansa yeah and basically and it's
written in Scala and it runs a single
thread per partition so this is
basically what it works like so the
little stripes on the left those are the
partitions separate machines and then
you have sams the jobs that are executed
most likely on yarn and they process the
messages of the queue and they do it on
a per message basis so there's basically
process one message after another in a
single thread and the outcome of that
computation is then put back in Kafka or
this car that if it doesn't match a
certain condition now if the process
that is running on Samsa takes a little
bit longer than you would expect this
will basically block your processing
from that partition so one idea that I
had today was why not apply the concept
where we basically have like a single
component which is the green component
which is what a sams our job would be
and this will be consuming the messages
from the partition and then we have a
predefined set of workers which
basically receive which are populated by
the green component and then the blue
messages are executed in peril and then
the green component is waiting for those
messages to come back and the result is
then submit in the batch back to Kafka
how would that help in this scenario in
the previous scenario here you read the
messages and you basically process them
in also a batch so you will process and
say one hundred a thousand messages and
then you will mark the segment on CAF
cada that was actually read in that
batch in here we could process those
messages in peril they would execute
much faster which means that we could
process the batch much quicker than in
the previous example where we could take
this one step further would be that if
the calf kaku is growing much quicker
than expected if we need additional
processing power we could just spin up
at
nodes that could utilize the airline
nodes functionality and just spin up
more workers and process even bigger
batches at one time and then shut down
the machines when they are no longer
necessary so as I said this is a very
very laughs idea it could be possibly a
candidate for elixir and that was my
lightning talk so if this sounds
interesting to anybody who would be in
this big data processing stuff yeah just
get in touch on Twitter and maybe there
is something out there thank you
beautiful view even more beautiful wow
what a fantastic day in conference and I
can't believe I'm the last person
between you and the bear right my name
is froot I work for the industrial on
user group I like to talk to you about
the alarm central the community website
for airline we are this is our website
we're also on Twitter as well if you
subscribe to our news feed you'll
probably see some of the picture
pictures up today and it's a community
website of news of jobs resources
tutorials yeah anything around the alarm
and its ecosystem we have registered
users around the world and plenty of
traffic from the USA Poland UK and
Sweden how many of you are registered on
the central hold of Alessandro great job
how many of you are listed here on the
map nice and right how many of you are
looking for a long job right this is the
place to go and how many of our you are
recruiting Atlanta Belle affairs
this is the place I got em right so we
have the job board and just stay tuned
and check it out everyone's once a month
once a week and we put our along jobs of
different kinds up there regularly now
we also have a tutorial sections it's a
wiki and it's free and it's open for
anyone to you know contribute and then
yeah check it out the allen central
website is Saban's sponsored by the
ellen industrial and user group its
organization nonprofit organization of a
list of airline users of enterprise
online users and together we have one go
dice to secure the future house of a
well-being of the alarm programming
language and then if you care to help
and if they sounds interested in you and
please talk to me afterwards and we are
looking for new members currently right
so that was part and a long central
community website i hope to see you on
hope to see you there</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>