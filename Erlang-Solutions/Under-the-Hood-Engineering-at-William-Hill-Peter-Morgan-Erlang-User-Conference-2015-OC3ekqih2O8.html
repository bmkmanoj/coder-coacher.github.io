<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Under the Hood: Engineering at William Hill - Peter Morgan - Erlang User Conference 2015 | Coder Coacher - Coaching Coders</title><meta content="Under the Hood: Engineering at William Hill - Peter Morgan - Erlang User Conference 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Under the Hood: Engineering at William Hill - Peter Morgan - Erlang User Conference 2015</b></h2><h5 class="post__date">2015-06-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OC3ekqih2O8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so welcome thank you very much and a
great conference as well I am really
enjoying it so i wanted to talk a little
bit about fate because i was having a
discussion with a colleague recently and
talking about erlang and he's a he's a
big scala fan and I wanted to give some
of the background as to the reason why I
in particular have chosen I'll anchor
for some of these systems so I'm going
to take you back to nineteen
ninety-three don't worry thurs not one
for every year but essentially my career
started working for company called
Rachel communication systems and the
radio that you see in front of you is
written in a real-time operating system
called OAC which is supported by by a
line was written in a real-time see so a
bit of a link to twirling there but it
was also written in NC and it was
written in military standard 2167 a
anybody had the excitement of writing
code in military started to on 67 a we
calculated that i think we had 47 lines
of comments for every single line of
code that we had it was it was an
awesome project so it this was kind of
my background to kind of small systems I
then moved to her to a company called
Iona only technologies were rebuilding
corporal orbs anybody still remember
cawber um but I also wrote an EJB
container and I was massively into java
at the time and the way i did that i
liked the contract getting a getting a
contract to do some some java work and
fell in love with the JVM so i actually
got addicted to bytecode compilation and
wrote over the weekend with a with a
good friend son called j coverage which
got into the curvature which did
line coverage for for java and i really
enjoyed that i really enjoy getting into
that kind of depth of detail within the
vm
and I know it's been way way too much
time worrying about getting things on
and off the wire marshalling things he's
using I up and I got way to detail into
into the power then went to Disney I
spent wear to work today Disney World
for probably three years as a consultant
and worked on the Disney Magic Band so
they'd be the pitcher is somebody
holding an RFID ogre against the reader
and that was a billion-dollar project or
is it was calculated to be a
billion-dollar project I think they
ended up spending about 3 3 billion
dollars in total probably the biggest
project I'm ever going to end up working
on it had lots and lots of crazy things
we had a a thousand a hundred thousand
square feet of a of a soundstage in
disney hollywood studios as our demo
space i think we spent ten million
dollars on demos the demo space included
planes real planes buses rights from the
park and these pictures that when you
walk past they changed and they would
show your your face it was it was cool
stuff but very Disney this was me Disney
and I don't particularly get on but I'm
on the left just in case it's not so
this was probably the first time that we
were we were in the park and my handler
or our handler took us into the park and
disappeared off and came back with these
these wonderful Disney is and it had
them beautifully inscribed with a with a
name with our names so my name is Peter
my colleagues name anybody guess what my
colleagues name is it well done
but where we were working was actually
in Hollywood studios so this is the the
Hollywood Hollywood Tower it's a it's a
great ride I used to park my car just
below that that building so my trip into
work would be listening to people
screaming literally not much has changed
actually so but I distinctly remember on
this on this huge project particularly
dark dark meeting three billion dollar
project every consultancy company
involved and just realizing that what we
were doing was pretty futile so myself
and another colleague we went on my
Bollywood tower hotel right ride random
drop ride it was the most in control
have been that day it was just superb
and it was it was a nice way of sort of
getting away from that project but
around that time this book came out I
love this book this book really
resonated with what we were doing so we
were doing Park simulations we were
looking at RFID throughout the park
eleven thousand readers within the park
28,000 hotel rooms each one with an RFID
46 square miles of Florida 200,000
people in the parks in one time you
can't simulate those kind of things in
Java you can't create 200,000 processes
so what I was looking to do is basically
build simulations of what we were doing
within the park before we actually build
heart build the actual hardware and we
ended up building simulations in Erlang
that were trivial in comparison Stu to
what we would have had to have built in
a in Java so um hello those are kind of
background to me so now I work for
William Hill and I'm hoping that this
this will link my seater to some of the
issues that we have work in a in a
large-scale gambling company so I
represent the
the head of engineering and we have a
small team that are building interesting
and cool stuff for for William Hill why
William Hill well we deal with extreme
scalability and that's that's what I
think of when I think of extreme
scalability our scalability is 464 bits
per second in 2014 we also have issues
with massive concurrency so this is this
is our website on a Saturday three
o'clock when all the football pitch
matches are kicking off we have about
five million price changes a day so
we're amazon but we're changing our
prices continuously all the sort of
things that you would ordinarily cash
you just can't cash because they're
changing we're also highly available
this isn't a picture from our data
center mainly because nobody in our
operations team would ever wear
something quite trendy assoc shoes and
trousers but that's typical data center
third always 24 by 7 a lot of sports
events during public holidays so that I
system has to be available 24 7 365 days
a year needs to be fault tolerant you
can never assume the system is going to
be running all the time and I love this
guy it's probably my favorite picture
that he's worked out that he can just
sit in the back and he's he's
sufficiently weighty enough to make sure
that the car doesn't fall apart but I'm
surprised he's not kind of leaning
forward trying to steal the car at the
same time so it unmet works are dealing
with about 160 terabytes of data a day
what does that look like so this is this
is our what we call our slipstream so
these are not live bets it's a recording
but of if I finish on time I'll
I'll show you a live stream as it's
actually running now but what you
actually see is customers their bets and
then what we call our event hierarchy so
what we do is we organize our sports
into football international football
premier league champions league what it
is is essentially is a is a graph a lot
of what we're doing is dealing with
graphs so we have here the edges of the
graph are already the ends of the graph
are individual customers their bets and
then that that actual hierarchy and that
the system we're looking at here is is
reading that bad stream and dealing with
our bed stream so what are we building
at the moment we have a big program of
work at the moment called Trafalgar and
trafalgar is taking the the front end of
our of our system i'm rebuilding it away
from a platform called open bet which
which we use so a couple of systems what
we're building there is a product
catalog and the other thing that we're
building is a recommendation system and
their two really key components so we're
building this year and then underneath
that we are building some other
components and we are looking at
particularly in the areas of settlement
liability and back capture and they're
kind of the core of what a gambling
company is doing or at least hippo
sports betting company and then the
other piece of work that we're doing at
the moment is around our trading
platform our trading platform is is huge
it's it's it's automating the pricing of
all the the events on the markets that
we have so on an average Saturday we
have typically around 100,000 markets
that are available we're looking to
seriously increase that number of
markets as are all the gambling
companies so we're looking to keep on
stretching that and building out more
and more and more and more so
the way that we do that is we automate
that so we take feeds from third parties
those feeds provide state and that state
information goes into mathematical
models that are living the prices so at
the moment we are we're building a
trading platform and that's the kind of
areas that we're looking at there are in
the feed handlers looking at match
coverage the actual pricing models that
we build the publishing rules behind
that so based on what's going on in the
match is well not that that particular
market is available and then the actual
resulting which feeds into settlement in
terms of the elling bits well we use
react we use react quite a lot have been
using it for for a little while now we
use it in our in our product catalogue
so that's currently delivering to our to
our website right now and the other
thing that we're building at the moment
is our our trading platform so a trading
platform is also built on top of of
react
but kind of the real interesting bits
not that they are not interesting but
the real interesting bits where we are
actually looking to use Erlang and
Erlang in anger is a piece called a
recommendations platform that I'll demo
to you we're looking at some work around
settlement that we're also doing an
ailing and we're also looking at as our
as our trading platform the Varks
building that trading platform inner
links so we're currently building feed
handlers I'm currently building other
elements with it within that platform
with it within I but another key
technology that we see look that we
really like is a Kafka is anybody using
Khafre at the moment is that we get
really really good results out of Kafka
and what we're what we're looking at
using it for is our kind of fire hose so
we have our fire hose of bets and we
seek Africa as a way of being able to
persist those bets and be able to replay
those bets on demand so the kind of
architecture that we're looking at is
very much with with streams of activity
that are coming into the system and
those streams can be many many different
things so stream of bets streams of
liability stream of prices and we're
looking at Kafka as a way of being able
to store that information to be able to
persist that information and then in
cases of recovery that we can then go
back to Caprica to actually ask Kafka
for there for the offset and categories
is something that we're we're actively
looking at as that kind of mechanism so
we're looking at this as a general model
of activity into Kafka and then
producing product from that so things a
little bit more specific for example on
our settlement system we're looking at
delivering our stream of bats so these
are bats that are struck within the
platform delivering that is a stream
into into Kafka
delivering a match State so what is the
current school and also delivering the
opinion so what's our what's our value
of that of that current market what is
our competitors value of that market and
we're looking at taking those streams
into Kafka using them for a variety of
different of uses one of these uses
might be settlement the nice thing about
putting it to Kafka it allows us to then
switch that usage into other places and
other uses so again a little bit more
specific we can start thinking this as
hierarchies a lot of what we do a lot of
sports is the way that we view sports is
a hierarchy no graph so here we have our
bets on match state I match opinion
going into Kafka and then being
essentially building a graph and that
graph consists of things at the top
level which are the individual events so
leads vs liverpool then the actual
market and we offer a number of Marcus's
as I said so that Mike it might be the
90-minute market and then within those
markets we might have a number of
selections now those are the outcomes
that you can bet on so then one of the
outcomes might be leads the Liverpool
and the other draw and then you can see
the actual bets that are there on those
selections and what you end up with is
it is a graph
so the kind of place that we've been
thinking about in terms of this problem
was near the pretty obvious problem
pretty obvious answer from from from
airline would be treat everything as a
process so we we see this as a possibly
a process graph with messages happening
between those graphs between the notes
on those graph on that graph or between
the processes in that graph so making it
a bit more concrete here we have a an
event 330 we have a wind market we have
a horse called feta and we are sending a
message to the fatso to process to say
that that particular section as one and
where we're thinking about ways that we
can build that within within the earth
the oiling system and one of the ways
that we're thinking about doing that is
how can you both horizontally and
vertically scale that solution so we're
looking obviously a doctor as a
mechanism of being able to do that so
we're looking at a way that we can scale
that solution both horizontally and also
vertically so I'm following him for
Garrett he did an awesome talk and I've
just changed one of my slides to will it
be evolvable I busy he was he was
talking about no longer calling things
scalable so will it be available
available and I think a lot of the
testing that we've done so far suggests
that it will so this is this is a simple
system that we've taken it's a very very
small system currently and we put it on
a on a 32 CPU box drove it with it with
it with a large metal load and we saw
all 32 CPUs lighting up beautifully and
that's kind of some of the stuff that we
were expecting from this platform be
able to scale as much as we want to be
able to scale some of the ways that
we're doing that is thinking about how
we have a link these applications
together how we link them with it within
containers so another product that we
really like is elastic search and we're
using both elasticsearch and cabana and
we're building dashboards and again
going back to this idea of having
khafre's as a central point that we can
then drive these events from what we've
built is a dashboarding system that is
showing life bets on the system so we
have to within a minute and it's
probably a lot better than that every
bet that's placed in the system going
into this elastic search instance what
that allows us to do is to see the
number bets it also allows us to query
directly into that back Raph so it
allows us to select an individual bet
see how many bets have been updated see
how many bets have been placed on that
individual but essentially start
building queries into this stream and we
find Kathy avere Eve airy powerful for
that it's also it seems to be very very
performant there that's real time with
probably the last six months of bats
from from our system like it loaded into
that NEC their the queries are simple
click and it allows you to see the
different types of bets that I mean
place different markets that those bets
have been placed on we can build
dashboards that allowed out that data to
be updated and it's it's all essentially
JSON objects that are being fed into
into that kafka system so you can see
that we we started off with X thousand
bets we've ended up with I think three
bets for for a single customer we've
sent you gone all the way down to a
single customer we're going to leave
that customer selected and instead of
looking at the the last 15 minutes we're
going to look at the last seven days so
you can see how many bets that customers
placed in seven days so what this kind
of tool allows us to do is to start
really at the top level drill down into
the individual
components within the system and then
build out the timeline and look at what
what customers been doing drink during
that period so how are we linking these
things together with it with in Dhaka if
we imagine elasticsearch instance
running in in a docker container
elasticsearch uses port 9200 to expose
it it's query API Cabana needs to be
able to discover that that host port the
machine is running on and if we were to
be writing an application that is
wanting to also log that information
into into elasticsearch hacking that
application discover that that from
Dhaka so if we were to be building an
application first thing that we would do
if we wish to dog arise that application
is to build a doc file so we another
thing that we like and this is turning
into a conversation of what do we like
we like we like LMK I like that NK makes
the release process really really simple
so we use LMK and this is essentially
what one of the dakka files would look
like for what I've our containers so
from whichever distribution that you use
in this case centos copy from the the
underscore release directory which is
the release that's being built for this
application this application exposes a
HTTP interface so you expose a port and
then we have an entry point and that
entry point is essentially running the
release but running that release for the
console that's that's how we distribute
our applications one two three four
lines we used to build that that that
application but how do we then link
those applications together so at the
moment in development we're using a tool
called docker compose no it's not a
production tool you can use yet what it
allows you to do is to have a really
simple yamel file that describes how you
wish to link these
applications together so for the
application that we're building so we've
got elasticsearch cabana and our
application how do we connect those
applications together where you use a
docker compose file and you can use this
natively within dokie you just set the
links up with it within docker itself
but for example we are here saying
creamy elasticsearch instance we're
using the elastic search instance that
is a way available from elastic we're
telling the system what ports
elasticsearch exposes in this case 9200
we're also using Cabana so we're having
a package version of Cabana but that
we're using and then the the important
thing in in this in this picture is is
the link the link is telling you that
Cabana wants to be able to link to
elasticsearch and Kentucky compose uses
that in order to be able to set the
communication parts between those
machines and so Cabana exposes itself on
port 56 open and then for application
very very similar so our app you'll have
your your provider that you've that
you're the name of your app and the
provider that you've chosen you store
those in Dhaka hub or your local da curb
instance you provide some hints to to
igp that it's not running quite as it
normally expect so it's not a TTY this
stops things like OTP when the service
starts up stops it um immediately
stopping the vm because it doesn't have
a TTY that is that it's expecting but
we're also telling docker what ports
this application is exposing and we're
also telling it about the the links of
that application once to be able to
exploit what doctor actually does is set
it's really really simple it sets to
environment variables that's all dr.
linking is really really doing so
there's two environment variables that
are being set we're linking to
elasticsearch so in this case we've got
elasticsearch port 9200 tcp address and
i'll ask
search TCP port as well what that is and
what ducker we'll do is we'll actually
set the locations the IP address of
where elasticsearch is running which
container it's running in and the port
that has actually been used by Daka
because doctor uses ephemeral ports and
you need to be able to link to those
those ephemeral ports and then the other
thing that you do in your application is
essentially set a little bit of
configuration and you can see in the in
the environment section within this
application that we've we've set two
variables here which are the lowercase
version of the environment variables
that have been set the docker Xbox i'm
going to set some defaults and that's
purely because defaults are usually a
good thing and we're going to set some
defaults that basically say TTB
underscore address is 127 and the port
is 9 200 which is which is our standard
port and we've got an index prefix that
we can use and we essentially get log
where we get our Erlang system to
pretend to be log stash when it's
actually pushing the day trim and then
in the actual application and again this
is another thing that we like we really
like g proc so we're using a little g
proc utility here where we're telling g
prague in that first line in in the in
the get environment call we're asking
Deepak to when and when we get an
environment to go and look in the the OS
environments to start off with that
allows us to to override it and that's
essentially the way that docker will
actually set these variables and then
the other mechanism is then to fall back
to the application environment so the
idea being that if you're not running in
this in the doctor environment you can
then just start this application up and
it will do the sensible thing and the
sensible thing will hopefully be talking
to the variables have been certainly in
the local environment and then you can
see that the TTP address and the TCP
port are just making calls to that to
their get out of call so we can
configure Jeep Rock Jeep Rock uses the
environment if he can otherwise we'll
look up in the application environment
now we have a small library that shows
that linking the basic it allows
to see how you connect elasticsearch
using docker in this way and it's
available on github on on that
particular URL issue if you're
interested in the code it is a tiny
amount of code the other piece that
we're looking at is sports betting
recommendations so very much the way
that Amazon does their recommendations
we're also looking at doing and what
we're looking at building is essentially
based off that graph that whole graph we
store for a period of time and then what
we do is we then go and look at what
other customers have done in that graph
remember this graph is essentially
everything that you're seeing in that
graph is a no long process this is
stored within a vm and what we're doing
is we're looking at the relationships
between the nodes on that graph so when
a customer asks for recommendation on
sports betting what we do is we go
transit reverse the graph traverse the
process tree and go and ask the process
tree for recommendations so the way that
we're doing that and here's some his'n
math we're using something called the
jacquard coefficient and the jacker
coefficient is is really quite simple
it's essentially taking the cardinality
of the intersection of two sets and
dividing it by the cardinality of the
union of two sets so it's actually very
very simple operation to to do in order
to get recommendations so what does that
look like so if we if we imagine a
customer and the customer who's placed
three bats and place three bets on three
different selections so three different
outcomes so think of a selection as your
team and they've placed three to three
different bets on three teams what
recommendations would we offer to that
customer in this graph
so in order to be able to give a
recommendation we need a customer that's
connected to that to that customer so in
this case we have another customer
called be and that customer has a have
also placed a bet on the select on a
selection and this case they've bet on
selection w what we do is we navigate
the graph and again the graph is
processes throughout the roller leg
processes and we navigate the graph and
we go and look at everybody else he's
placed a bet on that selection so the
jacquard coefficient in this case is the
cardinality of the intersection and in
this case the the intersection is is
just one divided by the cardinality of
the Union in this case for so the wet
the jacquard coefficient that we get for
this customer for the recommendation is
one-quarter another customer same
customer ray but a customer see in this
case in this case we have an overlap of
two so the jacquard coefficient here is
two quarters one-half so the cardinality
of the intersection is too and the
cardinality of the union is is for so
what the system does is using processes
and exploiting that the Erlang vm sends
messages to all these nodes all these
processes and what we essentially end up
with is a recommendation for for our
customer and it's a weighted
recommendation and this is a
simplification obviously of the
algorithm that we're using but it's the
basic principles of how this system is
working we essentially were then offer a
recommendation for for that customer ray
and the weight of that recommendation
would be half selection Z and point two
five four for selection fee
we're also doing a work on sports
mapping so sports mapping is where we
take feeds from external suppliers the
external suppliers have IDs that they
represent so there's a an ID for the
leads of a liverpool match we obviously
represent that differently in our system
it has a different ID and we also trade
matches from different providers
depending on their availability and
other factors that were interested in so
this is this is an example of another
Erlang system it's a it's a mapping
system we really like fizz Jess as well
most of these graphs have come from
Vista and what it's essentially doing is
building the graph and building the the
relationships between nodes that have
been created by the feed provider this
is knows they're being created by us so
games that we wish to trade versus games
that would vs. gave is that we have
courage for what happens here is and
luckily we're on the Swedish I'll
svenska league i do hi both not
completely mispronounced that but where
you can see in yellow is is some data
that we've got from a feed provider and
in blue is the data that we've actually
built ourselves and what we're doing is
we're linking the data between both the
data that's come from the feed provider
and the data this come from from
ourselves so we can then translate a
dangerous attack from a from a few
provider for a particular match into a
dangerous attack in the NR in our system
now forget why I put this slide in but I
like this slide
I've also stolen most of the good
pitches from Francesco yeah so this is
what we're trying to avoid this has been
a very Erlang specific talk we do lots
of tech we do lots of taken in different
different platforms so we we build lots
of stuff in Java we've got some Skylar
going on we do a lot of anger at the
moment a lot of spring we use puppet
from that for a number of our
deployments as well we're also looking a
lot at spark and no jess is also in our
platform so by no means are we exclusive
you using our line but we are seeing in
certain platforms it really hits the
spot it really is scaling to the kind of
levels that we want and it's it's
working well so far in their kind of
applications that we have so a little
note from my employer we're hiring if
any of this looks interesting come catch
me be very interested to talk to you
about it and if there are any questions
now is the time to start and what I'll
do is I'll run the slipstream now so
it's kind of in the background and then
maybe if there are no questions and we
can talk through that
this is life now I don't tend to run
this on really busy days because as you
can see it tends to freak out but it's
more the JavaScript bit than anything
else and what will happen is that this
graph will build and it will it will
organize itself or maybe it won't hoping
that's not gonna do that for too long it
must be quite busy at the moment but
essentially every bit as it's struck
goes on to this system so every online
bit on William Hill is currently
learning my laptop and it basically
connects for 20 seconds and then stop it
it is navigating graph is it's a memory
because of the speed of the
recommendation that we need to be able
to deliver so we're planning to put this
into a part of the system where is very
very time critical so what we want to do
is build a graph structure in memory to
be able to do that if we were to look at
other structures like a more tabular
toilet type of structure and business
that today's I think that would that
would affect their the kind of i/o that
issues that I would slow down that
process does that that's so obviously a
question or is it it's it's more to do
with the way that when we ask your
recommendation what we can do is we can
get each of the connected processes to
go and do the work as part of that
recommendation so that allows us to make
that recommendation very very power very
very very parallel that's that's the
principal reason the example I gave you
where there were just three that can be
30,000 for the recommendation system do
you only consider
the same selection or like different
selections on the same market we do lots
that's we do lots of variants of of what
I described we will we're trying out
different algorithms at the moment so
the algemeiner scribed is is one of the
herbs that we're looking at I guess it
depends is the answer to that question
so the way that we do that is this this
graph here this is a customer the yellow
circle is a customer these the bet so
that customer is placed and these are
the selections that I customer has
placed those bats on so these these
darker blue not sure can you see that
from the back okay yeah so that's
probably a bit better so here you go so
here's a customer here's this selection
so Leeds Liverpool whatever your
favorite team sorry they're the bats
these are the selections here so we're
in tennis here and what we do is we we
look at customers who are connected to
that customer so we use the connectivity
of that graph in order to be able to
build the recommendations so you can see
here here's his a customer they are
connected to this customer because
they're both placed a bet on this same
selection so essentially the graph
builds up bets customers selections on
them as the whole hierarchy of events
that that we have it's in memory and
they're all airline processes so we send
a message to that customer say give me
the recommendations then causes a
cascade of messages from that customer
to all the connected notes
hundreds of thousands lots of hundreds
of thousands okay we have time for one
more question or let's just thank the
speaker thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>