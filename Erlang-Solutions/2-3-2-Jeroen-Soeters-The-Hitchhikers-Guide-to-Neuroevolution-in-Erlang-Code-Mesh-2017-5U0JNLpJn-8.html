<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>2 3 2  Jeroen Soeters   The Hitchhiker's Guide to Neuroevolution in Erlang - Code Mesh 2017 | Coder Coacher - Coaching Coders</title><meta content="2 3 2  Jeroen Soeters   The Hitchhiker's Guide to Neuroevolution in Erlang - Code Mesh 2017 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>2 3 2  Jeroen Soeters   The Hitchhiker's Guide to Neuroevolution in Erlang - Code Mesh 2017</b></h2><h5 class="post__date">2018-02-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5U0JNLpJn-8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning guys thanks for coming so
my name is your ruin
suitors I work for a company called
thought works I'm Dutch
hence the funny name as well as defining
accent I give you guys a little bit of
help how to pronounce my name because
everyone is miserably failing at that so
I'm gonna talk about neural evolution
which is a branch of AI that uses
evolutionary computation to generate
neural nets so let's first take a look
at neural nets all right is a neural net
right so neural net can be defined as a
a model of reasoning based on the human
brain so our brain is this densely
interconnected set of neurons which are
effectively just nerve cells we got
about 10 billion of them and 60 trillion
interconnections between them and every
one of these nerve cells is like a
little information processing unit now
the interesting thing about our brain is
that it exhibits plasticity so that
means those connections between those
neurons the strength of them can get
stronger or weaker and and typically
like connections that lead to the right
answer they get stronger and connections
that lead to the wrong answer they get
weaker or completely disappear that
doesn't always work if you're late at
the conference party and have to decide
rather to take another beer or to go
back to your hotel room and work on the
slides that might fail but generally
that seems to be it seems to work so
let's take a look at these neurons like
how they actually do their work right so
neuron has a very simple structure so
they have a cell body called the soma
and then they have these arms reaching
out of the soma which we call dendrites
and these arms are there to bring
signals into the soma so electrochemical
signals come into the soma and they
build up a potential now when they that
potential hits a certain threshold the
neuron becomes what we call activated
and the neuron will propagate that
signal via the axon to other neurons and
then the synapse is these other arms
that we see there that's where
communication takes place to synapse is
of one neuron communicate with dendrites
of another neuron now if we were to do
this to simulators in a computer right
how would that look like
so first we're gonna change the
terminology a little bit as in computer
science we don't speak about dendrites
and synapses
we talk about input signals weights and
output signals now this model was
proposed in 1943 by Warren McCulloch and
Walter Pitts and it's still you is still
the basis for all artificial neural
networks these days so how does this
work so basically this neuron here
calculates its output by taking the
weighted sum of its inputs so basically
it does a dot product there of x1 times
w1 plus x2 times w2 as those all up in
the neuron it compares that with some
threshold value if it's higher than the
threshold value the output will be 1 so
the neuron is activated otherwise it's
minus 1 so in other words we just apply
a sine function there that sine function
is what we call a activation function
and that by no means needs to be a sine
function we're going to see an example
where I actually use the step function
in deep learning popular functions are
like a sigmoid or a hyperbolic tangent
you can use anything almost anything for
that so now we know like how this neuron
calculates his output we don't know
right how it learns right so in 1958
this gentleman Frank Rose the blood came
up with the algorithm to train one of
these neurons and the algorithm is
called the perceptron learning rule and
it works as follows so we start with a
training set where we have a set of
inputs for the neuron and expected
outputs for that input and we're going
to show the neuron those inputs and for
every example P we're gonna calculate
the output now here we know the desired
output which is yd and we know the
actual output with this YP and we're
gonna first calculate the error right so
it says desired minus the actual output
and what we're gonna do then is we're
gonna change these weights so in this
case if the error is positive so that
means the desired output is higher than
the actual output so we need to increase
the weights a little bit otherwise we
need to decrease the weights and that's
what this rule is doing right so we add
some constant factor alpha times the
current weight times the error to the
current weight
this looks something like this so we set
this random
use and at threshold as we set their
weights and threshold to some random
values now we activate the perceptron
basically we show it an input and we see
what the output is and then we're going
to update those weights until those
weights converge so let's see a little
bit of example I think you most of you
guys knows that know the or gate or the
the logic gates right so here after a
truth table for and or an ax or we're
going to try to train a neuron to
function as a and a function or and gate
so here I have this in a tabular form so
we're first gonna pick some random
values so we set a threshold to point to
we set that learning rate alpha to point
one that's not random by the way and we
set our initial weights to some random
values and now we're going to calculate
the output right so we have to do the
dot product so as zero times point 3 is
0 plus 0 times minus point 1 is 0 trash
of all this point 2 so it's less than
the threshold so the actual output is 0
but for 0 and 0 we expect 0 so the error
is also 0 so we don't do anything with
the weights next one so 0 times point 3
so it is minus point 1 also less than
point to actual output again 0 desired
output is 0 so the error is 0 again we
don't do anything now the next one so
now we have point 3 plus 0 so now the
actual output is 1 right because poetry
is higher than the threshold but the
inputs are 1 and 0 right and for the and
function 1 and 0 should give us a 1
should give us a 0 right so we have an
error now so the error is minus 1 so
this means we update the weights and
this basically we keep on doing every
time we show complete training set we
call it an epoch and I'm gonna just walk
to a couple of these epochs now it's 2/3
see sometimes two ways change sometimes
they don't and now we see in the fifth
epoch that everywhere our error is zero
so this means now for every like set of
inputs our neuron is actually giving the
right
the correct output right so we have
trained this neuron now how to like act
as an ant gate know what we are actually
doing here basically this neuron is what
we call a binary classifier so you can
only solve like a limited number of
problems with this and you can only
solve problems that are linearly
separable so that means like that
the output space needs to be you need to
be able to divide that by a line and you
see if we highlight points where the
output is one as black and the output is
zero as these open dots you see for the
and and or we can cross a line to
separate that output space but for XOR
that's not possible so it took people
quite a while to figure out like how to
solve this problem and turns out if you
connect a bunch of these neurons and
what we nowadays know as a neural
network then you can actually solve
arbitrary complex problems if you have
one of these head and layer so basically
there's a few layers first layer doesn't
really do anything it just takes two
inputs and forwards that to every neuron
in the next layer and so on and so forth
it's a fully connected network and turns
out with one of these hidden layers you
can represent any continuous function
and with more hidden layers you can also
represent non continuous functions they
also call this oscillating to higher
dimensions if you can't separate with a
line you try it with a plane or like
higher dimensional structures training
this works much same like the perceptron
or the neuron we saw before as in like
you show it some inputs you calculate
the output and then you go back and you
update the weights until you have found
a configuration that gives you the right
output for any given set of inputs and
then you hope that that also works if
you show it examples that are not in a
training set all right so this algorithm
is called back propagation is a very
compute intensive algorithm that
actually dates back to the 60s I think
and it's still like the algorithm used
to Train like neural nets these days but
there are some limitations to is back
propagation algorithm first you need a
training set of sufficient size and
sufficient variation if you look gue GLE
and
Facebook they actually like augmented
images a little bit like change to color
rotate a little bit and show it again to
the network to train it to recognize
your face you need to know that topology
in advance so the number of hidden
layers and the neurons per layer they
also call that hyper parameter tuning
and then deep learning no recurrent
connections are allowed so you cannot
connect from a layer and to any neuron
and layer and have a smaller than them
and the activation function that we saw
before it needs to be differentiable
because that's how the back propagation
algorithm works right it uses the four
it differentiates the activation
function and tries to minimize the error
rate it really only works well for
supervised learning problems right when
you have that training set available and
even then nowadays - good question right
we saw yesterday and frats talked a
turtle disguised as a gun I think and a
couple of weeks ago there was a paper
came out the one pixel attack anyways so
it's roughly 200 years ago I think that
young Charles Darwin went out on this
HMS Beagle initially actually to a to
find evidence for the theory of creation
that was laid out and like Genesis right
the first book of the Old Testament but
as he was on this journey as we all know
and he was gathering more date more and
more data in like true like Lean Startup
style he pivoted slightly and that's
what ultimately led to like what the
classical theory of evolution right and
this this theory of evolution together
with Wiseman's
natural selection and mandles genetics
is what we now know as neo-darwinism a
new Darwinism is based on processes of
rapid reproduction mutation selection
and competition now in the early 70s
there was a gentleman mr. john holland i
was wondering if he could emulate a
process in a computer so what John did
was the following he described a
solution to a problem that he wanted to
solve as a series
of bits a bit string of zeros and ones
and then he came up with an algorithm to
move from one population of these
chromosomes as he called them to the
next generation of these chromosomes
either that by defining a fitness
function that we tell them what is the
health of one of these chromosomes how
well does that perform and then he had
genetics inspired operations crossover
and mutation to like select parents from
a population cross them over and then
with a very small chance do some
mutation on them and put them in a new
population so crossover would look
something like that he would based on
like the fitness score it would take two
parents it would cut them in half and
like join the halves and mutation if we
had a child with a very very small
chance very now and then would like make
a small modification just like nature
does it as well so let's look at a more
real-life example to make this a bit
more concrete so for doing one of these
algorithms we need to walk to sort of a
checklist so we're going to do that for
a one time in schematic form basically
what we're going to do is this we're
going to generate a population all right
we're going to calculate for every
candidate in that population we're going
to calculate the Fitness and then until
some termination criteria is satisfied
we're gonna do the following right we're
gonna select a pair for mating based on
Fitness based on like a crossover and
mutation probability we're gonna cross
them over and we're going to do a slight
mutation on the child then parents and
child go into the new population and we
keep on doing that until we have a new
population of the same size as the
original population and we keep on doing
that until some termination criteria and
that's typically either you run it for a
number of generations or you have some
target fitness in mind that you wanna
achieve so traveling salesman problem so
a classical computer science problem
right given a salesman a list of cities
and the distance between each pair of
cities now what's the shortest route
this gentleman can take to visit all the
cities and return to the origin city so
this will be typically go away and be
hard problem right if you add cities
this
quickly like brute force is not an
option because it's very soon taking
years or more to solve this problem so
let's use a genetic algorithm right so
first we're gonna represent the
candidate solution as a chromosome
that's pretty easy here right we can
just list our cities in our chromosome
like a list of characters and we're done
so the genetic operators these are a bit
more tricky because remember john was
cutting those two in in half like that
doesn't really fly here right because we
might end up with duplicate cities or we
might miss cities so we need to be a
little bit more creative here to solve
that problem so what we do is following
we cut some part of their original
parent we place that in the offspring at
that part and then we start taking from
the second parent and every time we see
a city that we already have we just skip
that alright so we take B we take E a we
already have so we skip so we take F and
so on and so forth so that's crossover a
mutation is again pretty simple we can
do same thing like John did we pick two
random genes and swap them around and
now we need to have a fitness function
well Fitness function is also easy right
because we know here what's the distance
here right so it's basically the
distance between C and B between B and G
until the distance F to C right so we
can just add all these distances and if
we want a fitness higher for like a
shorter distance we can do one over the
total distance and that will be your
fitness and now we need to only choose
our initial population size crossover
and mutation probability it's not really
as far as I know like hard science to
get to these numbers typically numbers
that work well are like a point seven
crossover probability and a very small
like point 0.005 even for the mutation
probability so now I'm going to show how
that actually looks like so what we will
see here say we have 250 cities now and
what we'll see here on the screen as the
the solutions that the algorithm is
generating right so this is every time
the best
solution we find in every generation and
we see here that very soon this
algorithm converges to pretty decent
solution right yeah there are some
optimizations right here there is still
like a weird loop and top-left corner
and it will improve that over time and
not all of this and I have it running
four thousand generations now but really
the key takeaway is here that the
genetic algorithm will give you a
solution that is good enough in a very
very like reasonable amount of time
right where you'll be calculating for
many months or I don't know even how
long you'd be calculating with 250
cities right so it's quite like a nice
solution so it does not guarantee you
the optimal solution right so here we
still see that's not optimal right but
it's good enough for most cases so this
works well for vehicle routing resource
allocation scheduling problems all these
type of problems okay now we've seen
neural nets and we've seen evolutionary
computation now let's see how we can
combine those two right and this
technique is called neuro evolution so
research tuner evolution has been
happening since the early 90s but some
like groundbreaking work has been done
and started 2011 2012 I guess by this
gentleman Jean sure who created the xnn
which sets for Deus Ex neural network
and the xnn is a TWA e8m which stands
for topology and weight evolving
artificial neural network and basically
dx9 is a it's a general purpose platform
to generate neural nets so that means
like the the platform to generate these
networks is decoupled from the actual
simulations that you use to train these
networks in right so you can use it for
many purposes so let's take a quick look
like what does that mean if we want to
plug in a stimulation to dx9 but what
all do we need so we need three things
we need some sensors and sensors they
provide input from the simulation or the
environment to the neural network we
need some actuators actuators say
they take the output of the network and
they make some change in the environment
and then again we need to define a
fitness function to basically score a
run of a neural network in that
environment so give some more concrete
examples right say rumba right I've got
one of these in my living room so the
sensors on this room bar it has like a
bumper sensor there's like a bunch of IR
sensors on the side and on the bottom
and it has a battery sensor right
actuator as well it just has motors
right like service or whatnot to like
drive that guy around and a fitness
function you could do something like
percentage of the environment cleans
plus the time it's active minus the
number of furniture bumps or whatever
something like that right so another
example zoo forex trading right so
foreign exchange of currency trading so
you could say the sensors are the market
data right so for the different
currencies like the current price the
actuators is like buy sell or hold right
and the fitness function is but a number
of correct predictions minus the number
of incorrect predictions and you can use
it for many more things right you could
use data compression and pcs all kind of
stuff so now let's go back to our
genetic algorithm right because what
does it mean to like
generate a network using evolutionary
computation so we go back to our
checklist so first again we need to
represent the candidate solution as a
chromosome
now you can imagine this fixed line bit
string is not going to work anymore
right because there can be arbitrary
amount of information in what we call
the genotype of a neural network and so
it's very simple actually the in the
chromosome here it's basically a data
structure that contains all the
information of the structure of that
network right so it we'll see in a bit
this is written in Erlang so it's just
like a massive like list of nested
Erlang records that contain like with
sensors we have which neurons we have
per neuron like the inputs and the
weights and activation function we use
which actuators we have and so on and so
forth
so now we need to define the genetic
operators well now if you think about
that crossover operator right like
having these two and like crossing them
over if like a little bit of intuition
will tell you that won't work if you
have two like massive neural networks of
millions and millions of neurons if you
just pick two random branches and swap
them around like chances of that ending
up with a better performing neural
network that's close to zero right so
that's not really gonna work so we're
gonna Park that thought a little bit and
we're gonna come back with that to this
later mutation as again it's very easy
there's actually a whole bunch of
mutations that we can do to a neural
network we can mutate the activation
function right that doesn't need to be
differentiable anymore we can have like
every neuron has a different activation
function so we can change that one we
can add connections we can add neurons
we can something called out splicing so
and if removing a line and adding like a
neuron in between and neck tons and tons
more right you can imagine like how
these things would look like important
thing to note here that these things I
just so showed they're like complexify
operators most of these have an inverse
which are simplifying operators because
the algorithm that we'll see in a bit it
will refer smaller networks over larger
networks to to prevent Network bloat
right so it will like optimize for
pruning as well now we need to define a
fitness function well that we've already
seen right because the fitness function
is just a performance of a network in a
specific solution so it really depends
write the fitness function for that
robot was different for the feinted
function of the of the forex trading
agent and now we get to this we need the
population size crossover probability
and mutation probability right so now we
need to do something with that crossover
right we need to solve that the way the
X and then solve that is it basically
doesn't do crossover and it combines
those two in one so instead of doing
crossover it does this thing called
random intensity mutation so instead of
picking two parents it really picks one
parents based on like a fitness
Corrigan what it does and it clones that
parent and then it applies a random
number between one and a square would
have the network size of mutations to
that agent right and so what what this
gives us is that it allows for like very
small improvements right if we have
already a well performing Network to
like improve on that but it also allows
for pretty large mutations right to jump
out of local optima yeah so the reason
why the exelon works so well as it does
is because of the core algorithm and the
core algorithm as we call the mimetic
algorithm and a mathematical realism
algorithm that works in two time skills
so what dxn and does it separates what
calls the global search which is
basically searching for the topology of
the network from the local search which
is searching for the optimal weights for
each of these networks so for the global
search it still uses like a
population-based algorithm like we saw a
genetic compute of evolutionary
computation but for the local search it
uses stochastic operators and we're
gonna see what that means so first we
look at a topology service here so what
we do is we generate a population right
of completely random network so we start
off very small we're just one neuron
there may be a different activation
function and then we're going to apply
these to a problem and what that means
is we're going to apply this network in
a simulation and we're going to see what
is the best fitness we can achieve in a
reasonable amount of time with this
configuration right with this topology
we keep on doing that and I'll explain
an algorithm in a bit until all these
networks are done and then what we do is
we calculate the effective fitness so we
those networks reports back to fitness
or the simulations report back to
fitness
we like take into account this network
size first thing we're gonna do is we're
gonna just drop the bottom 50% of that
population immediately and then of the
top 50% we're going to do it's called
mutant allotment that means like every
agent there gets assigned a number which
is the number of clones it's going to
produce so that means if something has a
very high fitness compared to the rest
the rest of the population it'll get
more offspring if it has a very low
fitness it can still drop out of the
population there now then we do this
offspring creation we do the random
intensity mutation on the topology and
we start again now this a local
optimization is very easy actually what
happens here basically we're going to
apply the network to a problem and we
see okay the Fitness that we have here
is this better than the best fitness so
far if this is the case we back up those
weights because that's apparently a
configuration that we want to keep we
change the weights again again like this
is also random intensity mutation now
for the weights and we're going to try
that again right if it's not better then
we go back to our previously past
weights and we keep on doing that until
some stopping condition is reached and
stopping condition is if we didn't
improve say 50 iterations in a row then
we just stop and we report the best
fitness so far it's working on that that
yeah last night late so Erlang right so
I guess most people here are familiar
with with Erlang so I don't need to
explain much so for everyone who is only
running iterate and production and
doesn't like touch Erlang at all
Erlang is a concurrency Ori and the
programming language right build around
the actor models who you have like many
like small processes that send messages
to each other and nvm right and if you
read Joyce thesis he emphasizes on this
concept of having an isomorphism between
the problem and the program and having
this one to one mapping kind of of your
your problem to the to the program in a
concurrency oriented programming
language it makes it much easier to
reason about your your program and
that's exactly what we get with the xnn
because that genotype as we saw we can
map that literally one-to-one on to
Erlang processes right so every one of
these neurons these sensors and these
actuators they'll just become an airline
process and they start sending messages
to each other
now this message sending that's kind of
the core in Erlang right so what I want
to do is I want to explain you guys the
architecture of DX NN doing that by
showing you the messages that flow back
and forth in the system and we're going
to start by zooming all the way into the
deepest level of DX none and that's the
level of one of those neural nets
so most affair should look familiar by
now right so we have sensors you have
actuators we have neurons the scape
that's really the simulation and then we
have an element cortex and the cortex is
really only there to synchronize between
the network and the simulation because
it needs to know like when actually to
get the input and when to process the
other question so what happens is this
so the cortex sends a message sync to
the sensor and the sensor then sends a
message sense asking for data to the
scape the scape sends back percept and
percept also contains the input vector
and then like an every neural network
right the sensor will just forward that
stuff to the next layer to the next
layer and ultimately that will all
aggregate in the actuator and the
actuator will send an action to this
Cape upon weight escaped does something
right it buys a currency or it like
moves the robot and then it returns back
Fitness that we have so far and a halt
flag so basically are we done right do
we not have anymore historic market data
did a robot run out of battery or fall
of the stairs and the actuator sends
back a sync signal to the cortex now
we're going to zoom out one level and
now we're going to see one extra element
and that's called the x2 itself and what
the XSL fir is doing a the XS Elvis the
component as responsible for this local
search right the stochastic hill
climbing algorithm so that so as the
ones gonna see what's the best fitness
this one can achieve in a reasonable
amount of time so the X is how sense or
gets actually from the cortex okay I'm
done I come this is my fitness and the
extra self will then say okay is this
better than our best fitness so far yes
it is so we're gonna back up these
weights right so we send them a message
to all these neurons to back up their
weights and now this random intensity
mutation happens and to some random
neurons we say okay change your weights
please and then it says to the cortex
reactivate and basically we drop back
into the previous loop right on the
previous screen and now we're going to
zoom one more level out and are we
really on the highest level of DX NN
right so here we have the population
monitor and here we'll see this global
search right looking for the best
apology of this network so it happens
this population monitor spawns a random
population of networks all that like
different different topologies and it's
going to say then like okay guys go for
it please let me know what's best
fitness you guys can can get so at this
point all these little acts ourselves
keep on car starting to crunch right I'm
going to see okay what's the best
fitness we can achieve here and once
they're done like one by one they'll
send back to the population monitor okay
I'm done and they'll report back their
fitness what happens Dennis population
monitor drops the bottom 50% of the
population clones too and here does run
the random intensity mutation and it was
too late last night to get that
animation going so you just have to
imagine that these now changed the
policy over there and then it basically
starts all those guys again
so that looks all very easy but
unfortunately it's not so there's a lot
of like little details to actually make
this work and that you need to think
about all right so I'm gonna go over a
few we don't know how much time we have
ten minutes okay whoo so recurrent
connections so here we don't have this
limitation of like connecting back to a
layer a previous layer all right because
we're not doing back propagation but you
need to think very carefully right so
for those of you who took a hardware
class right which element has a
recurrent connection in the circuit
right
that's the flip-flop right or a latch
and what does a flip-flop do well it
creates a memory right and that's
exactly the same thing that also happens
in the neural network when you make a
recurrent connection you create memory
so that means if you're like training a
neural network to to solve the XOR
problem well you better make sure that
you show the examples in random order
every time because if you show it in the
same order the network might start
understanding and what order you're
showing that and then if you show an
example outside of training set it
doesn't know what to do with that so the
next one is kind of you know when you
learn how to bike that doesn't affect
like your ability to walk right you
might like damage some sensors or
actuators along the way but like the
part of your brain that's responsible
for walking
that doesn't change right and this is
the same with here right if you add
elements to the network you want them to
work well together with you're ready
like well performing network and you
don't want to change like things
randomly all over the place so that
really what that thing does right so
every element gets added to the network
gets a generation number and elements
with a higher generation number have a
higher chance on getting mutated neural
plasticity basically these neurons can
evolve things like oh yes rule or happy
and learning public state steady state
evolution so that's actually an
interesting one because what we saw is
just a one network and one simulation
but you can actually have multiple of
these networks working in a simulation
and you can do sort of a life
simulations right so you have a
population of predators and priests and
they're like bad battling against each
other and then you don't have that you
create a new population right but you
keep the population in there at some
stable number ones like candidates drop
out of the population
so last I want to look at a little quick
benchmark so this is a typical benchmark
for neural nets so the idea is following
right so it's called pole balancing
benchmark so we have a little track and
we have a card on that track and on that
card is a pool and that pool is
connected with a hinge so it can tip
over now the idea is the network gets as
input and there's different variations
to this one it gets like the position of
the card on the track the velocity of
the track velocity of the card the angle
of the pole and the velocity of the pool
and it needs to produce a force either
positive or negative such to keep the
pool balanced and that's the that's the
benchmark we're running here so here I
just recorded this in my Erlang shell
just to show you guys that it actually
also works okay I'm just starting up
there the excellent platform and I'm
starting the benchmark now and you
should see like it should look pretty
familiar what we're going to see now all
right so you'll see you see these agents
reporting right and the population
monitor will tell okay this is how much
agents I have left right six left five
left for left and then every like when
it has all the tenant like you see this
like blow-up of an attack generates a
new population basically right and these
agents they report back their Fitness
evaluations basically like how many like
like variations did I try before I got
to this fitness and this just keeps on
going I think not going to wait until
this thing completes but I'm just going
to show you two benchmarks results so we
have to like do a little bit of
explanation here because we see some of
these benchmarks they have this little
star and this little star means they're
actually not twee and ends so they don't
evolve topology and weights they have a
fixed apology they only evolve the
weights so it's not really like fair to
compare with those guys and state
cheating a little bit so we really have
to compare with need and as we see here
like we're more than three times as far
as neat right and the second version is
almost four times as fast as neat so
this second version actually is the one
you built in this book so if you wonder
like how do I know all this stuff well I
wanted to learn some erlang a couple of
years ago and I wanted to build
something non-trivial in Erlang but it
seems pretty non-trivial to me so I
found this book yeah and in this book
you're really like are developing the
second version of the excellent so
you're really building a
state-of-the-art neuro-evolutionary
platform throughout this book it's a
really great book it's a full of like
great ideas I would recommend if you're
going to go to it like write a code and
start introducing some kind of
engineering practices from the start as
the the code is like there's like zero
tests and there's like gaps between the
chapter so there's pieces of code
missing that you're going to need to
fill in yourself so if you don't like
rewrite it and start like typing the
examples like you go absolutely mad but
yeah that's all I had for you guys so I
have to take some questions
this is a proprietary effort or do you
release anything open-source no so for
me it was just like a learning efforts
to just figure out how this stuff works
the original stuff is as open-source
like my implementation as also on my my
github yeah it's really like learning
and not really using this for any
practical things by the way today the
guy himself like he is using this so
this works particularly well in like
robotics right where you doing have that
training set and also may be less useful
but you know you have these programs
like learning how to play Atari games
like it works like very well for those
things as well as that like the network
learns what information is important and
what action to take but the guy himself
in fortunately like is not an academia
anymore
so the excellent two is the latest
version as open source and the excellent
three is all proprietary and he is like
using this stuff for training unmanned
aerial vehicles right now but
unfortunately that's all like
proprietary
so it's noise so it's all pure Erlang
because actually if you think about like
neural nets and you take away
backpropagation all that's left is
really message passing right so every
one of these neurons in a layer all it
needs to do is it needs to wait for the
neurons in the upstream layer right so
it just has a list of bits that it
expects a message from and then when it
has liked the information it applies an
activation function and force it to the
next layer
that's why Erlang is such a like nice a
very elegant solution to this to this
problem and you really need you know if
you do back propagation then becomes
very compute intensive and that's why
people are running that on GPUs and
stuff like this but for this that's
really all message passing might be if
the messages are very large I guess yeah
I maybe we can rewrite it in Pune but
cool thank you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>