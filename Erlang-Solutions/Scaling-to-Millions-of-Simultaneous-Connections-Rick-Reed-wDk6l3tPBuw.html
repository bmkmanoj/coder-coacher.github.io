<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scaling to Millions of Simultaneous Connections: Rick Reed | Coder Coacher - Coaching Coders</title><meta content="Scaling to Millions of Simultaneous Connections: Rick Reed - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scaling to Millions of Simultaneous Connections: Rick Reed</b></h2><h5 class="post__date">2012-06-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wDk6l3tPBuw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for coming I was afraid being in
the biggest room that it was gonna be me
and our host and the website guys and
that's about it but if you're not
familiar with whatsapp we have a
multi-platform smartphone messaging app
and we're at this conference because
most of our server systems are back in
that does all the message routing is
done in Erlang and so a little bit about
me to start I joined last year I'm I'm
new to Erlang so if at some point during
this talk you're the thought pops in
your mind well that's obvious or why'd
you do that
there's this and bear with me a little
bit we're still learning some of the
intricacies of using Erlang effectively
but I think we've made good progress and
we want to share some of the things that
we've discovered and some of the results
because we feel like it's been a pretty
solid and performant platform and it
just continues to kind of amaze me my
background is in mostly C I was
previously at Yahoo I spent 12 years
there working on a lot of scalability
and data caching types of problems there
network routing things of that nature so
let's get into what we're going to talk
about today so first of all I'd like to
start with just kind of a problem
statement some of the performance goals
that we set for for ourselves and have
been we've been trying to push the
envelope there some of the tools and and
techniques that were instrumental in
helping us improve the scape about
scalability of our systems some of the
results to date and then some general
findings about scalability of Erlang in
our environment and then some of the I'm
gonna go through some of the specific
scalability
fix's that well we've implemented in the
code that we're running so the problem
unfortunately my labels are the wrong
color here so there's sort of three
fundamental problems here we've got our
user growth so if you look at this this
graph on the Left that's that's our
messaging traffic over the past four
weeks so you can see it continues to
climb which is a good thing we're all
we're very excited about that but of
course that makes the people who write
the checks nervous because that's gonna
eventually mean buying more hardware
which drives up our costs and it also
creates more operational complexity the
more systems that we have to operate and
keep running so there's just sort of the
organic growth that we're that we're
going through but then we also have to
build into the system enough resiliency
to deal with unexpected thing so you
know I just happen to be grabbing this
this graph the other day when I was
putting these slides together and I
noticed this little bump there on the
left-hand side and I thought oh that's
kind of strange so I went went through
our country by country graphs and there
was a big bump in Mexico and so when we
see bumps like this we go to the news
you know we just go to Google and we
search for news for Mexico and of course
that was an earthquake off often these
these little bumps that we see are one
of two things they're either earthquakes
or they're soccer games so the one on
the right is a soccer game in Spain so
you can see we have a halftime here is
this first bump and here's a goal here's
a goal and there's fulltime right there
so it's it's kind of interesting but of
course it creates a challenge especially
when these things happen close to our
peaks we have to make sure that we have
enough extra capacity to deal with those
those types of incidents so the problem
our initial server loading was about 200
simultaneous connections per server
which when we extrapolated out where we
hoped we were going was gonna be pretty
discouraging prognosis for growth in
terms of how much
money and operations time we were going
to be spending on server hosts and
further the cluster was a little brittle
in the face of those those kind of
overloads where we got close to or
exceeded certain limits or we had issue
you know we'd have little networking
glitches or things like that we we
needed to put in you we needed to figure
out how to decouple these things a
little bit so that things weren't so
brittle when we were running them at a
pretty high capacity I mean well
obviously we could run with you know 75%
over capacity and we'd probably not run
into these issues but but that wasn't
the kind of efficiency we're we're
looking for so that was the initial
problem so sort of the initial goal was
and and I kind of was in a little bit of
disbelief when I was first presented
with this goal of million connections
per server because despite having worked
at Yahoo on scalability you know one of
the you know probably top five certainly
top 10 traffic sites on the internet you
know we didn't talk in terms of millions
of connections per server we have many
many servers to deal with the traffic
there now it's a little bit of a
difference because we kind of have a
very homogeneous traffic load at
whatsapp where is JA who has many
different types of properties that
they're that they're serving and so the
characteristics are a little bit
different but still millions seem pretty
ambitious especially when we were
running at about 200k at that point but
then obviously in conjunction with that
if we're running our servers with you
know a certain amount of headroom to
allow for you know world events or
hardware barriers or other types of
glitches i'm going to need enough
resilience when we're running at those
fairly high usage levels to deal with
these other things that are going to go
on
so so this is the standard configuration
we use for our for our service that we
do our our connections with these are
the base of the user facing server hosts
they're their dual Westmere hex cores so
that's 12 real CPUs with hyper threads
meaning 24 logical CPUs we load them up
with RAM and we run them with SSDs they
have dual NICs so the user the user
traffic comes in on a public interface
and then all the backend that all the
distribution runs on a on a private
network that's on the back on a separate
NIC
we're running FreeBSD 8 3 and currently
we're running on our 14 p0 3 so now I'm
gonna just talk a bit about some of the
tools and techniques that I've been
using to to increase our scalability and
I'm sure you can't read that at all
the first tool but basically the first
thing I did when I got to whatsapp was I
wrote this tool which was pretty similar
to a tool I wrote at Yahoo it's a system
activity recorder so basically allows us
to record system stats across the system
including the OS staffs the hardware
stats we beam stats it's modular so we
so if we want to add a certain we want
to monitor metrics from a certain you
know from beam or from virtual memory
system or from the network we write a
module for that and and so it's it's
easy to plug in new stats so I'm gonna
zoom this in a little bit it was
yeah go back there we go
not the best resolution but basically up
here are some of the OS level stats that
we track CPU utilization so it's got
overall utilization user time system
time interrupt time and there's the init
and there's different levels that these
can be displayed so it's actually for
CPU it's also tracking things like
context which is system calls traps but
this is a reduced set of things that
it's displaying right now for TCP we've
got packets sent packets receive
retransmit rates
listen queue overflows connections per
second PCB counts we also track things
like sync clash overflows all those
types of things
we've got virtual memory over there so
overall utilization active inactive
cached and so wired and then down below
I've got the beam staff so my first Earl
my first er line code was the the module
that exports these beam stats so that W
SAR can can can extract them so we look
at things like what's the total count of
messages and all the Q's across the
processes are there any you know there
may be a disk any busy of port events
for the for either the Dysport or normal
ports what's the disk message traffic
raid bytes going in and out of also
coops operator error here I've also got
some scheduling stats here percent
utilization in the schedulers context
switches weights per second sleeps per
second reductions get killer reductions
per second how many garbage collections
are happening how many words are getting
collected and so on so originally we
this this tool was run once a minute and
basically would take a snapshot but as
we got further and further trying to
drive these systems harder it became
clear that we really needed one second
resolution because we have events which
would happen inside the space of a
minute and we wanted we couldn't really
tell what was going on during that so
so rather than pulling on one minute now
we run it on a one
second polling interval so we get really
fine staffs around where we have issues
that we can go back and look at how
everything's performing another thing
that was that's been invaluable here has
been the hardware performance counters
that are in the in the CPU so on freebsd
there's pmc stack which allows you to
look at you know for instance this is
looking at the clock on halted count
cycle count in the in the cpu so it's
basically where is the cpu at it for
percentage of time so you can see in
this example it's well you probably
can't read it but at the top thing up
there says 16 percent it's in process
main which is the big emulator loop
which is which is interesting right
because that tells us that in our
particular application only 16 percent
of the time is actually executing the
emulated code so even if we were able to
take you know a super-hype which
eliminated all of the execution time of
all the Erlang code it would only take
16 percent out of our total run time so
we've been focusing on other areas
obviously to to improve the efficiency
of what our system is doing but it was
but this this allowed us a view of
things that were happening both in terms
of insight beam originally earlier on we
saw a lot more time in like the garbage
collection routines they're still on
there but they're further down we saw
some issues with the networking stack
which we needed to tune away and things
of that nature Jenna we've done a little
bit of DTrace although that's mostly for
debugging not really for performance a
little bit of lock Colonel lock counting
but we haven't really found any issues
that we needed to deal with in and
freebsd that we didn't find out through
pmc stat and then of course looking at
our code itself
we used f prof now the freebsd build of
being doesn't support cpu timestamp
so I actually passed it so that we could
get that as well
and then it may be user error on my part
but I couldn't figure out how to get an
aggregated look at at the call or the
what they're called the the out the the
analyzed output aggregated across all
the proc so I run a few scripts to
basically boil it all down aggregate it
all down give us you know here are the
routines where we're spending all our
time and the biggest thing probably was
the the lot counting you know compiling
the emulator with lock counting turned
on because it turns out that most of our
issues were with contention in the
emulator which shows pretty strongly in
the in the output of the lock counting
and we'll talk a little bit about that
more in the future slide so the way that
we actually measured what was going on
here a couple different ways so
originally we were looking at trying to
do synthetic workload against against
our systems which is good for the
subsystems we have which are have really
simple interfaces because we it's pretty
easy to just drive them hard like you
know say like a user table being able to
inserts and and reads against that table
as quickly as possible from a small
number of hosts and generate a
tremendous amount of load but that has
really limited value for our user facing
systems because first of all if we're
talking about supporting a million users
on a server we're gonna need at least 30
hosts or 30 hosts or at least 30 IPS
just to generate enough IP ports to be
able to open that many connections and
that's just a test one server and that
and that's just for a million
connections if we go higher than that
then we're talking you know 60 hosts and
it just becomes really troublesome to
come up with the right kind of scale
further actually generating the type of
traffic that we're actually going to see
in production is very difficult
obviously
you know we can we can guess at what the
normal workload is like and you know
create some behaviors with lots of
processes that that are gonna emulate
what what our user is going to do but
but in actuality what happens is we see
you know networking events we see world
events we see all these different things
we see we have we have since we're a
multi-platform we have very different
behavior between our clients between
countries where the people are located
and so the synthetic workload just was a
very limited value for at least tuning
our user facing systems another type of
workload testing we did was with
basically with the TD workloads we take
the normal production traffic excuse me
and pipe it off to a separate system and
this was very useful for the places that
we could contain the side effects so
obviously we don't want it we couldn't t
traffic and do things that work actually
gonna affect you know the permanent
state of a user or what's gonna result
in multiple messages going to users
things of that nature
but in the cases where we could contain
those side effects then this turned out
to be extremely useful for being able to
tune things especially with Erlang and
being able to hot load code because we
could be running under full production
load have an idea compile it loaded it
in as it's running and instantly see
well does that better or is that worse
and or and some of the things I did also
were to add some knobs where there
weren't knobs before so the sort on the
fly we under production load we could we
could twiddle the knobs and see how it
would affect you know we basically be
tailing the SAR output and you know
looking at things like CPU usage or VM
utilization or dropped listing queue
overflows things like that
and turn knobs and and see see how the
the system reacted but of course the the
the ultimate test here is to actually
give it true production loads so that
it's doing not only the
in put work which would which is what it
would get if it was doing the teed case
but also doing all the output work as
well and we did this via a couple
different ways one is just put the
server into DNS a couple times so that
would give it you know double or triple
excuse me double or triple the Tritton
normal traffic but that creates TTL
issues with TTL s because even even if
because there are a lot of DNS resolvers
and clients out there that don't respect
the TTL very well and there's a you know
there's a delay so if we're trying to
push systems to their limit we get to
their limit it's hard to quickly react
to to the fact that we're we're actually
getting more traffic than we can deal
with so we tried another approach which
was use IPFW in freebsd and and
basically forward traffic from one of
the production servers to another
production server so so the traffic
would be coming in to two servers but
one of them would forward to the other
one and so so we could we can exactly
give it the number of additional client
connections that we wanted to to run and
that work that works great except that
we actually panicked our kernels a
couple times for there's a bug in there
somewhere
that that occurred a couple times so
we're we're just trying away from that a
little bit at this point okay so start
talking about results so as we started
to ramp up from that 200k number in
terms of connections
the first bottleneck started to show at
about 425 Kay and and basically what
we'd see is that the the system would
just start it was it would stop and it
was just obviously running into a lot of
contention and and you could see it I
don't have the I don't have the I don't
think I do I don't have the data here
for out of sorrow that we're showing
that but basically I'm jumping ahead a
little bit in terms of one of the fixes
that I was going to talk about but one
of the things I did was instrument the
the scheduler and I think it's similar
to what's
gone into hour 15 with that scheduler
wall clock but but basically measuring
the time that a schedule is actually
executing process code or executing
system code and then the sum of those is
how much time it's actually doing useful
work versus sleeping or spinning and in
the in in the case where we start to see
contention that number starts to go way
up relative to how much CPU is being
used because when it starts to run in
contention it's hitting locks at least
in our 14 most of those locks are
sleeping locks and so the the products
go to sleep so they're not using any CPU
but yet they're still counting against
the schedulers utilization so we'd see
35 40 % CPU across the system but the
schedulers are at 95 percent utilization
so obviously they can't they can't be
driven any harder they're just they're
running into each other too much so the
first round of fixes got us to cause to
over a million connections and here's
what yeah the resolution is just
horrible here but you can see here where
we're at the end here open files on that
top one is so there's a few hundred
files that are open you know across the
system just for you know normal
operations but the rest are sockets that
are open to our clients so you can see
there's we're over a million there our
virtual memory usage is at 76% and CPU
is running at 73 percent or so and then
and then what beam looks like here you
can see that the emulator is is running
at 45% whereas it's in the middle here
on the bottom one forty-five percent
utilization in the emulator itself which
matches pretty closely to the user time
user percentage so that's good
that means because all the time in the
emulator is user time and so if it
matches up well with what the system
thinks is being used in user time than
that that's and I forgot to mention one
thing there's a caveat which is that the
schedulers actually by to fall
do a lot of spinning the SP schedulers
when they don't have work to do and I
think the reason for that is so that
they when they do get work there they're
immediately ready to pick it up and
start a skew getting it they out the
wait for the CPU disk or I'm sorry to
the OS to schedule them I'm free BC it
doesn't turn out to make much of a
difference especially the way we run it
so we've got a scheduler per CPU this
the system's not doing anything else so
it's either gonna be in the schedulers I
don't thread or it's gonna be executing
beam so it makes no delay imperceptible
difference between spinning and sleeping
and it makes more sense for us to be
able to to actually put them into sleep
because it makes our cpu stats look
better it allows the system to power
down those cpu so they're not they're
not chewing up a bunch of electricity
for no reason so ordinarily CPU
utilization isn't necessarily a good
measure of how busy the system is I
think the OTP teams said that a couple
times here but we tweaked it so they
don't spin basically so that so those
things should match up so we continued
to attack similar bottlenecks and about
a month later we actually got to two
million which was which was amazing
because that goes like I said originally
a million seemed like a long shot and we
pretty easily got to two million and
just quickly you can see what it looks
like here so we've got open files here
over on the right-hand side which is two
million our VM utilizations now up to
about eighty percent so we're getting
close to where freebsd is going to start
thinking about I need to reclaim pages
maybe start hitting us with some paging
CPUs about the same as it was with a
million connections so we've we've
improved things in terms of overall so
we've doubled our connections CPU still
running hot but it's but but it's still
within the realm of what we can do now
if you look at this what the scheduler
is doing the scheduler is running at the
same as the system's running now so
we're starting to get you know some more
contention but it but otherwise the
system still
running pretty well so so we got to two
million and that seemed like a pretty
good place to stop for that for the
holidays so we put that stuff on the
back burner and then after the new year
started looking at it wasn't clear that
there was any more low-hanging fruit to
get us past two million so we started
looking at the application code itself
the Erlang stuff and started doing some
profiling there and found some some
things do we did one of the things we
did was and I think it shows you yeah so
originally we had two processes per
connection we cut that in half so
there's only a single process per
connection that helped a lot we did some
things with timers and I'll go into more
specifics in a little bit but we had
kind of an we were playing around with
the DNS T you know doing the double DNS
and we ran into the issue with the the
long TTL and we actually had an
unintentional you know we were trying to
get to about 2.3 2.4 and this this
particular box actually ended up keeping
going and eventually peaked at about 2.8
before we kind of intervene and say okay
that's enough we need to need to figure
out what's going on at that point it was
doing you know it was doing 571 thousand
packets in + out per second
within that with you know 200k of those
were we're distribution messages so it's
just and let's see some of these other
stats CPU is running at 86% our VM load
is that is down to 70% now so we made
some memory optimizations and otherwise
it's it's keeping up which was which was
really markable so so then it seemed
like well heck if we can do 2.8 we must
be able to do 3 right but haven't gone
there yet tried to get there on st.
Patrick's Day but it didn't it didn't
work we kind of aborted that task as
well it was Saturday and the system's
started to show a little bit of backlog
so one of the things you know basically
what we see in our systems
when they when they fall behind as I'm
sure a lot of people see is you get you
start to get message cues that they
start getting along so our one of our
biggest measures of the system getting
into trouble is at look it's you know
either a single message queue getting
long or a sum of message queues getting
long and then in this particular case
let's well I didn't it I don't have it
in the SAR output but this thing below
it is oops is another Oh how'd it go
back so far pardon me is another another
tool we use so we so one of the things
that I added to beam was instrumenting
the the message the message queue
activity per process so how many
messages are being sent how many
messages being received how fast they're
being sent how fast are being received
and that allows us to do things like
this which is look at a process by
process basis so here's sort of every
ten seconds so it so this so our logging
notices that there's message backlog it
starts logging every ten seconds showing
us which process what the backlog is so
this particular process has six hundred
thousand messages in the in the message
queue its DQ rate out of that queue is
only forty K so that means they've got a
queue delay of 15 seconds which is we'd
like zero or you know almost zero but
it's it is draining I mean it is
draining it's not filling faster than
it's draining so so the time the
estimated time to drain it's 41 seconds
so ten seconds later the DQ rates gone
up to 46 K the message queues down to
360 K and the time to drain in seventeen
seconds ten seconds later it's almost
drained it's down to 160 KS draining at
48 K and or actually it's draining at 22
K the DQ rate is is 48 but there's an NQ
rate so the the difference between those
two is 22 so it's draining 22 K
messages per second we're down to seven
seconds of drain so anyway so this was
this was a little blip while we were
running this test did it recovered from
so we probably could have run it pushed
it a little harder but we it wasn't
clear that we were gonna get to three
million so we Bandhan it and and in this
case this test got a stupid about 2.7
all right so onto some general findings
I'm gonna do on time good obviously
Erlang has awesome SPE scalability I'm
just blown away so this is you know this
is a 24 way box and we can run this
system at 85% CPU utilization and it's
it's keeping up I mean it's doing
production load it's not it's not you
know blowing up its we cannot we can run
it like this all day so the combination
of you know our code beam and FreeBSD is
looking really really good for for SMP
and here's a little graph of that
experiment that week so what this is is
a number of connections on the x-axis
CPU utilization on the on the y-axis and
you can see there's a couple different
kind of parallel lines there so what
that is is our daily traffic goes up and
down and after we restart a server it
starts to accumulate some long connected
clients some of our clients are short
connected some of them are longer
connected so the longer service been up
the more the higher percentage of long
connected clients it has which are kind
of idle so we can actually handle more
connections because it's not as busy on
each connection so those so those
parallel lines are basically day by day
as it's accumulating more and more of
these long connected connections it has
to do less work so that's why they're
shifting out to the right but overall
you can see that kind of looks linear
right you know upper corner is 100% CPU
lower corners you know so we start out
with no connections at a you know a
little under 10% and and when we get up
to out to like 2.5 million were you know
over up above 90% but you know it's
pretty linear in there so you know if
you see a lot if you look at a lot of
software
it's not as good at scaling across
multi-core that curves going to look
like that you know it's gonna start
going straight up at some point when you
start to get a lot of contention across
across the processor so this is just
remarkable I think and it's a real
testament to you know the the
programming model in in Erlang and just
a fabulous emulator and then and then a
few fixes that we found along the way so
the biggest thing obviously was
contention that's what got us from 200k
to 2 million those were all context
contention fixes some of them were
internal to being some of which were
addressable with app changes so there
were things we discovered about the way
beam would deal with a certain Erlang
usage and so we could change that in our
code without having to patch beam but
some of them required actually having to
patch beam and then other ones required
some app changes you know partitioning
workload so that it's not crossing
processors a lot or or hosts and then
avoiding some some common things that
that come at a price and have more
specifics so here's here's some of the
specifics so freebsd quickly we we back
ported at esc based time counter out of
previously 9 into FreeBSD 8 the TSC is a
is a Zomboid the processor and it ticks
once for each cycle and it's very cheap
to read so it's very easy to do get cut
it's very fast to get time of day calls
much less expensive than going to one of
the i/o chips to get the time of day so
that that helps a lot we back ported the
igb driver from FreeBSD 9 because we
were having issues with the with the
multiple queues interrupts queues on the
driver excuse me on the cards blocking
up and then just some obvious sis ETL
tuning for you know increasing the
number of files and sockets but we the
one thing that we noticed that a PMC
stat was spending a lot of time looking
at PCBs in the networking stack so
bumping up the size of the hash table
for the TCP PCBs was was a win as far as
reducing that time in the kernel
okay so on to some of the bean patches
so I mentioned that I instrumented the
scheduler to get utilization in terms of
process execution time system execution
time number context which is number
waist number of sleeps added the
statistics for message queues so that
was that's across all the processes so
it's you know you can do that in or you
can do that in Erlang code by you know
doing process info and adding them all
up but it's tremendously slow and when
we've got millions of connections it
just it grinds the system to a halt if
you try to do that so there's actually
loops in the in in beam which are highly
optimized for getting you know the list
of processing so I added one to actually
get us the message stats the message
queue stats out quickly like that so
that we can actually do it in production
when we have you know three million or
you know two and half million
connections going without busting the
system process info I mentioned the the
message queue stats in qdq send rates
counts and rates and it has three
different decay intervals so it'll show
you the 1 second interval and these are
approximate cuz it just does some very
simple math to decay these but one
second ten second hundred second
intervals so that allows us to kind of
if we're having an issue we can go in
and see like okay this process should be
running in this speed or it's running
this speed this minute and it's raining
this speed this minute there's something
going on there and then statistic
message counts which is just an
aggregation of all the message queue
stats in terms of the rates and then I
fixed the the lack of CPU time stamping
F proffer for FreeBSD
I had to make lock counting work for
larger async thread counts cuz it
because it blew up once you went above
64 async threads without that and then
added a couple options to the oats debug
lock counters because if we try it if
you try to do lock counting with a whole
bunch of let's see one of them so there
you can turn off process locks counting
but you can't turn off port locks and
since users are our processes has a port
that the TCP port it didn't do us any
good so we needed to be able to turn off
port locks
- because otherwise you'd get millions
of these lock counting statistics
running and what we're and we know those
aren't going to be hot because those are
just individual connections so that
there's no contention on those the locks
that we care about are the the sort of
the system locks so we need to be able
to turn those off and then just fix them
missing counting for outbound despite
there's a couple missing increments
there as far as tuning one of the first
things we did was set the scheduler
wakeup threshold to low because on
FreeBSD we're having an issue where
schedulers would go to sleep and they'd
never wake up and then another one go to
sleep and eventually you know you'd have
two schedulers running at 100% and the
system falling behind but SWT low solve
that we bumped up the EM say maximums so
that the allocators the memory
alligators would prefer em sag over
malloc and that's important for another
reason I'll turn in a sec we wanted an
alligator per instance per scheduler
which I think is the default and hour 15
now and then we configure the carrier
sizes so that they start out bag and get
bigger so they start at 2 Meg and they
go to 32 Meg and the reason we want that
is we want to mega lined big chunks of
to make a line malloc memory so that
freebsd will do automatic promotion to
super pages so that reduces our TLB
thrash rate and improves our our
throughput for the same cpu we run our
schedulers we run beam at real-time
priority so that other things that we do
on the Box
cron jobs and things like that don't
interrupt the scheduler because we we
did have issues with that we'd be
running along smoothly and all of a
sudden you know for a few seconds we'd
glitch that the system we get a bunch of
a huge backlog that it could work
through but we we want though we want
the schedulers to get top top priority
on execution resources so we run them at
real-time priority and then all the cron
jobs and everything just get scheduled
as as possible and then I mentioned
dialing down the spin counts so that the
schedulers wouldn't spend that was a
patch as far as being contention and I
mentioned contention was a big thing one
of the big ones is the time of day
and I hate and especially time of day
delivery so I believe as I recall every
time a message is delivered from a port
it looks to update the time of day which
means and that's a single lock across
all schedulers which means that all the
cpus are hitting one lock that's bad so
there was a little bit of optimization
there on the timer wheel itself I did a
little bit of optimization there there's
two levels of timer tracking there's the
timer wheel and then there's the Biff
timers where if you're actually create
so the timer wheels used for timers
fifth timers and four like receive
timeouts things like that and then
there's a separate table for just the
Biff timers and that's a pretty it's not
a very wide hash table so we had
multiple timers per connection which
meant that that hash table was just not
wide enough so we made it wider it was
still a source of contention so we ended
up getting rid of those Biff timers and
moving everything to just receive
timeouts because that just uses the
timer wheel and remove the the Biff
timer lock out of the equation all
together there's the thing called that
check IO table which grows
arithmetically which is bad for us
created lots of VM hash VM thrashing
problems because it would it would
allocate until this big then this big
then this big and this big this big so
move improve that to just do a sort of a
geometric allocation of that table get
rid of that we added a a prim file write
file that takes a port so we don't so
there so that you can write a file
atomically with a port that you already
have opened instead of having to open
the port every time and that reduce
contention on port creation which was
the big issue and then there's just a
minor thing there's a an EM say every
time you allocates an M saying it has to
check to see if it's over the maximum
number of M seg allocated and that's a
single point of contention across all
the processors across all the allocators
now I think in our 15m say gala caters
our per scheduler so that's not much of
an issue there and then there were some
issues with lots of port
port transactions when setting up a when
accepting a connection so so there were
a couple changes to setting the options
on that so that there weren't as many
port interactions which are expensive in
terms of contention Oh in terms of OTP
throughput so we were running into
issues where when the message queue in
these situations where we are dealing
with a with a with a backlog
since GC runs through the message queue
the longer the message queue take is the
longer the GC takes and so we could get
into these destabilizing these unstable
situations where it could not recover so
we add in some code so that it would
back off on the GC if the message queue
got long and then once the message queue
resolved itself then go back to normal
GC parameters we had an issue with it to
see the distress Eve buffers how when
it's receiving over the dis channels is
reading 4k at a time which for some of
our high throughput systems is just not
big enough there's too many reads going
on so making that configurable and
setting the default to reasonable number
was good for us one of the big one the
issues that we had with amnesia was
we're using async dirty so no
transactions just as simple as you can
get but we'd run into a backlog on the
when we're doing remote replication it
still goes through immediate TM on the
remote and that's a single process so we
parallelized that by basically launching
a separate process for each table that's
getting async dirty updates and that
that allows it paralyze the updates the
underlying gets tables and that created
it much more throughput for us
what's it okay
I think let's see where am I let's see
there's it I'll just quickly name these
and then you can ask me questions issue
with PG two we increased the size of the
and M seg cache we moved all our code
from using Erlang now which is uses it
takes a single lock to using OS
timestamp which is very fast we got rid
of our normal gen server calls because
they craw we're going when we're going
because that causes an extra message to
go for the link and then partitioning
let's see yeah yeah if you're super
interested you can ask me after or look
at the slides after but questions
quickly
I probably absolute time for one or two
yeah yes they are absolutely there there
there are they're up on github in right
now we're we don't have a whatsapp
account yet they're in my account which
is read are but they're all they're both
against our fifth 14 and our 15 not now
a lot of these have had no testing
outside of FreeBSD so you know I don't
know how soon we could possibly get them
into OTP but they're at least they're
available there for people to look at
I'm sorry yes yeah yeah anything else
all right
thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>