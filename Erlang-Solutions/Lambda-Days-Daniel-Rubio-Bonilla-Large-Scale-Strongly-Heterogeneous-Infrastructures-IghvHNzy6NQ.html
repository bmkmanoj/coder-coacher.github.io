<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lambda Days -  Daniel Rubio Bonilla - (...) Large Scale, Strongly Heterogeneous Infrastructures | Coder Coacher - Coaching Coders</title><meta content="Lambda Days -  Daniel Rubio Bonilla - (...) Large Scale, Strongly Heterogeneous Infrastructures - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lambda Days -  Daniel Rubio Bonilla - (...) Large Scale, Strongly Heterogeneous Infrastructures</b></h2><h5 class="post__date">2014-04-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IghvHNzy6NQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so welcome everybody I'd see a lot
of people here so that makes me very
happy well my name is Daniel you see
there and I come from a teller SLRs is
the high-performance computing center of
the University of Stuttgart and I'm
going to talk to you about what problems
in scalability we have been hvc hpc its
high-performance computing I don't know
if there are many people that are that
no out hpc and what kinds of machines we
have so I'm going to start talking about
them how they have involved in time what
Poland's we are having with then how we
program them and how what ideas we have
to to solve these issues and yeah we are
realizing that it's very important if
you want to scale to solve all your data
dependencies if you want your execution
to be right so well this morning we were
already talking about houses views of
the vault over time in this graph we can
see the evolution of the of the
frequency of CPUs over time so in the
last recent years we are not able to
increase the frequency any longer and
Intel plan like 10 years ago with a
pentium 4 to reach over 10 g guards and
well advanced in its NT guards machine
around and well the problem is also that
there are also no new ideas on how to
speed up the cpus by putting new
hardware inside them all the DI all the
ideas they were developed quite early in
the 60s and 70s they were difficult to
implement because they need a lot of
transistors with develop technology we
were able to implement them inside the
CPUs and now there are no new ideas of
how to make a cpu core fast
there we can just make a larger cache
and larger data structures inside but
that doesn't really make them a lot
faster so how do we try to make much
interested we just had more and more
goals so in this graph we can see the
last 20 years of evolution of hpc on the
left column we have the numbers of
course that the most performant the HPC
machine had on the right cool which is
the correspond to the orange line is how
many teraflops per second that much he
was able to deliver so right now the
biggest machine has around three million
course and gives around I think was like
30 petaflop a flop itself a floating
point operation so yeah I mean this
morning I saw discussion some people
were talking about scaling to a few 10
hundreds so we want to scale right now
23 million threats at least and we spent
that we will just be increasing and
increasing the numbers of course so we
are talking about I don't know the
number that we will get in the nest
years at the problem is that and we'll
see it here this graph is similar I just
replaced one of them the number of
course with how much perform an is a
core and you can see didn't change much
even sometimes it goes down it goes down
because it's easier to build a machine
with smaller course and just put a lot
more of them but this is not usually
good and we will see the haven't later
wine so I'm going to talk about the
machines we have at the hrs right now we
have a module we got in 2011 which has
over
thousand course and now we are going to
get a new one and when it's finished it
will have almost 200,000 course and this
half of the you making the calls are a
path reddit course so if you want to get
the maximum performance of them probably
you have to schedule two threads per
core that depends on the on the
application and on the amount of memory
you have work or so I don't know do you
are familiar with I'm download and it
basically explains like if you have an
application and your application has a
part that is a strictly sequential you
have no way of paralyzing it or run
concurrently with anything else of the
prom and another part that you can
paralyze depending of what percentage of
your application you can paralyze what
is the speed up you gain by adding more
cores to it to by increasing the
presentation so for example here if you
have an application that were fifty
percent of the your coat is going to be
yes sequential and the other fifty
percent you can really paralyzed if you
increase the number of course to an
infinite number you will be still
limited to speed up of ha like half of
your time execution time because of this
sequential part of course if the station
is much higher that you can do it will
be better but you will still get the
party you cannot realize and that you
cannot go speed it up the application
independently of the number of course
that you up but the problem is is that
this is a very theoretical and gruff
what that learning happens is something
like this and in that there is some
point where the more course you are the
more threads you create the slower your
application becomes and that's because
in reality you have to assess memory and
you have to communicate and exchange
data so there is a moment we're adding
more threads just creates a worse
communication much more overhead and
your application performs worse and
worse over time that's why we will
really like to have one single huge
score to program instead of see one
single circuit or one for one single
core one single for that was able to
deliver us three teraflops with area
with a single thread then you will not
get to provide anything you just put the
code that runs on no problems but that's
impossible to make that's completely
possible to make otherwise we will test
what that is what we will be using right
now yeah and well the plan you have this
also is because most compilers there all
day they are trying to do is reduce the
when they use optimizations your call
they try to reduce the complexity of the
algorithm that you are using and to
reduce the number of operations but they
almost never take into consideration
what is going to be the cost of
communication they assume that the
memory is there for free and that you
can access it at any moment that they
have 0 lighting latency but it's not
like this yeah I'm one of the problems
also is that it gets worse over time
because memory we were not able to
evolve memorias the same speed at the
CPUs they are able to deliver a lot more
data but the latency that is like the
time since you requested that you start
getting them is still stream lehigh even
if we are adding more and more cash the
problem with cash isn't when you
increase the sides to get to be able to
have more than near the cpu that's
something you can do now with smaller
technology it also becomes slower it
also increases the
address it so now inside any modern CPU
duka have even up to five levels of
cache you saw you people know about l1
l2 l3 zero that yeah they're very sound
0 which is usually they decode it
already instructions I'm all for that
you have in some Intel CPUs now that
these 64 128 megabytes yeah that's a bit
it doesn't have a lot more fun with that
we are run but you cannot see it quite
faster does the minute Vantage with ram
so how do we run it in Timothy's we
usually use a C or C++ or Fortran which
is basically programming in assembler
these ideas and wow yeah it's basically
assemblers we saw in application earlier
this this morning just a basic
translation from one tuna inside the
node well I forgot to say that when you
have so many cores to divide your 8pc
matches into note each note has maybe
one or two CPUs each CPU has a lot of
course inside and they usually share the
memory I'm at least address space of the
memory so they can each thread connected
all the memory within this node and all
the other so you set up a lot of nodes
and they are in an internal network so
if you want to communicate with inside a
no usually use of something like OpenMP
which basically is just normal threads
and if you want to communicate between
the different nodes you use message
passing interfaces so that's how we are
doing it till now and we are not using
up to now functional languages because
they have some disadvantages for these
machines and maybe I will be a bit phone
troubles you're not so the plan is that
they used too much memory that's one of
them learn usually when you measure it
they tend have a tendency to use to
allocate much more memory and you have
no control
over it and the memory is really
important usually there are some
applications in hpc machines that where
you reduce the number of course you are
using so that a threat can have more
from and usually you have our own 40
bytes of RAM for work or in some cases
and another thing is that you have no
control over the hardware where you're
running in with functional languages
like you cannot optimize for example to
fit your data structures into your cache
of the CPU you have no control or if it
fits it's not fit how many cache misses
you are having so that's the problems
that they have right now and for us is
really important performance we consider
it's really good if you are able to
obtain a two percent faster performance
that can be a lot of mine at the end
today ok so how applications are
executed coming back to the topic of
brown so if we have this nice
application which has like four let's
say four tests we can run in
sequentially one after the other but if
we give the information about what are
the dependencies between this test and
we have more cores we cool say like well
things that where the dependencies have
already been resolved with all running
concurrently like in this case within
the red and the orange change part so if
you have a good scheduler and you have
no the dependencies you can improve your
execution if you have even more
information about when the data is a
place you can further improve it for
simple it is in this case and there are
you right and we reached two front two
variables so if we know when we write
and we can start the the
that is going to consume this data and
as you can see the total execution time
is a bit reduce we can do it even better
if we know when is going to read we can
stop the application the dust when is
about to read wait till the data is
written and then continue and with this
we can improve and improve their desa
cution time with having this data flow
but it has some problems and is usually
we don't have enough information to know
if what we're doing is good if it's
really worth to run sequential it or
come correctly so for example these
these are the same dance as before that
we have divided them into two smaller
tasks depending on when they read and
write but what happens if the workload
is too small like the cost of creating
the thread is much higher than just
executing that part of code or the time
to communicate the data to another node
or even inside annatto and our core is
going to be more expensive and well I
don't know if you know but if you ever
try to measure how much linux need how
much time limit needs to create a thread
yes call this pthread library from boxes
and it's around 20 or 30,000 cpu cycles
when there's nothing else running in the
system so that's a lot of time so if you
want to paralyze come for a great
threads that run concurrently and are
very small that's a small computations
probably weighs much more time in
creating them that just running so how
do you insert that a flow into things
like see basically automatically you
cannot do it you have all these scope of
variables arrives pointers I don't know
how many people here how to create the
rice up
to an array of pointers of pointers to
functions I did that once so it's
horrible you never know anything and
there is no to everything is going to
depend into your data you have so you
cannot make a static analysis of the
cold so basically no compiler will
support in doing this kind of thing so
the things they do is are very basic so
what people do right now is we have some
models there's family or foreign
stenches that's called star SS and where
do with pragmas define what are the
inputs and outputs of your functions
like each task what that is going to
need and what that is going to touch and
then the smart scheduler that when sees
that the dependencies are resolved we'll
start scheduling the does that can start
security but this is not nice at all so
I want to talk to you now do you also
think about this about mathematical
programming and in hpc we are basically
executing mathematical problems we do
simulation physics simulations of
molecular dynamics of how of structures
of chemistry and all of these have nice
very nice physical equations behind them
so basically if you are going to if you
have some pyramidal that where you can
just put the mathematics inside that
will be perfect you don't have to
convert any algorithms in to see which
is just a set of instructions on how to
secure things into a cpu for example
this is a one dimensional heat
dissipation function so this is like if
I think you can imagine it because i'm
going to splain a bit more we are going
to
with it right now about a stream that is
hot and how they keep spreads around it
so for example if at some point is very
fun in the leader when you move over
time the heat will start spreading
around so how do you compute these you
make discritization so here at the end
we have these formula which is what
basically it's like the the computer so
this is for it for its cell so if we
divide our string for example in 10 pcs
that are the one of the top with the
regional value the boundaries that are
this one and this one are always with
the fix a heat for example 0 so we are
all circulating the ones in the middle
so you can see very well with our
dependencies and the dependencies point
each time step that posts from t 0 to t
3 in this case are very well defined by
clear lights I need my position so to
know what is going to be making in the
picture i need my current heat and the
heat of my neighbors which are this one
and this one I myself and that's what we
have here to know my heat at this point
I need the ones of my neighborhood so
yeah it's what I explained before so if
we are doing mathematics with on wheels
right mathematical some kind of
mathematical language into hpc machines
and this is all the process of
communication and optimizations a table
stuck in before why we don't do it like
this so we try to make an algorithm in C
or Fortran and put it inside there but
with mathematical with mathematics we
can do more things we can reason with
them and we can transfer the algorithms
and this something we are trying to do
for example we have can have a horrible
alike this on top and we drew a
mathematical transformation we can go to
the second
one they are completely valid but if we
represent them we will see now that they
are completely friend and they have a
complete impact on how they can be
executed for example this is the first
one so if you take this this is this
species this formula like the first
class or this one it doesn't matter it's
a here we have B and then we make the
multiplication of amb to to get this
cell in the next time step but if we use
the second version of the formula
equivalent formula we have a complete
different graph with the two operations
only before we had three now we know
Polly too but at some cost and is when
we have three operations for example in
this version this one's can't run
completely independent from each other
so you can make this group for this read
this operation and one in one thread in
the second version they have these
dependency here so they will have to run
soup when Sally over these so one thread
will be waiting for the other that's not
good but we will see that we can solve
this in some situation so
transformations are all as to change the
computational load while they are still
the same result we are going to achieve
so the thing is that when we program we
always think like in natural time and
the for mathematical formulas we have
they're always like thinking over time
like I calculate this for this time then
I move to the next time step then I
calculate everything for the next time
step and it doesn't necessarily need to
be like that because the only limiting
factor to calculate every of this note
is that the dependencies are resolved
so we can iterate the kind of entire
gonna like this is one iteration with
the other iteration here with the color
if the defendant is our resolve you can
just do it the problem is for example in
this case and unless you see is that
this iteration you cannot compute it in
a in parallel because there is is that a
dependency again that you didn't have at
the beginning but if if you term it a
bit more is this case you will see that
there are no other dependencies anymore
so we call running in from another you
could change the iteration vector we
could turn it around and this allows for
example to this auto for my fault we saw
before that only uses two operations but
have this data dependency and is if we
turn it around now there are no data
dependencies from this group between
this group in this group we have with
the previous one but this was result in
an iteration before so this all these
data are already ready to go into
seclusion so we have been able to
improve the algorithm now we have less
operations while at the same time we
have been able to maintain the
parallelization of it so i hope i don't
know how many people are lost all right
but you can watch the video later again
yeah so we yes instead of operating over
time as before which are represented by
these lines you we have a new iteration
vector called power now
so what is the impact of the code so it
before we interpolate it's like this
this is the sea pearl for this version
it's very clear that this looks very
nice because you have these four loops
like from 0 to 100 from 0 to 5 and now
it gets more complex and that's why it's
difficult to reason over it if you want
to do it yourself advantages that we can
have a complete prowl for that runs all
the iterations of it in different
threats I mean all I wrote it by hand
because the bridge still very new stuff
in the instep yeah the idea is yeah
exactly so now what's sucky what we want
to do in vodka so these were the
concepts and now it's exactly yeah what
do we want to do so we want to bring all
these transformations all the risk all
this reasoning over space and time and
extend see with it so that we can hit
this information and notate this
information in a sipper and that it can
be done automatically the tools can
transform it think about it and see how
it will work better depending on the
platform you have so how do we plan to
stand see and here is where we come with
functional languages again in the
approach we're taking right now we are
using haskell or a subset of haskell so
we have a program in C with its
algorithm written with skull and we as
want to say like we just write the
equivalency of it in haskell because we
are able to what please one partners
were able to
make this kind of reasoning of
transformations using Haskell as it
doesn't give you the really go that
later much darker the tour we say don't
worry here you have it it's already
written in C or is your written in
assembly so just play with it arrange it
andressa QT that's using is whether we
want to dress all kinds of architectures
that are used in hvc right now
especially CPUs well CPUs continue to be
the main part and you p use in theory
too and especially we want to address
FPS and do you know our people can we
are here with FPS some some are but the
FPS are basically however where you just
define the logic gates inside and for
example our apartment develop one thing
called one system called clash clash is
a subset of haskell that you can
directly compared to VHDL so you can and
this VHDL if the language usually used
to pour on FPS so you can directly put
your high school program into hardware
without needing it to execute on a cpu
and that can be very very impressive
sometimes day the gains in performance
you have an instance the retina
translated so we want to address for
example PDS without with removing the
lexical because you already have it
there written in Haskell and what we
will provide at the end of the project
will be well a premium model
specification the toolchain and some
reference use cases so that we are
analyzing right now lots of bargains in
the period from different countries and
even some companies there and what the
people who helped me to do the slides
because dealing with all these diagrams
can be sometimes tricky so that's all
for my part thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>