<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Contemporary Approaches to Data at Scale - Ben Stopford | Coder Coacher - Coaching Coders</title><meta content="Contemporary Approaches to Data at Scale - Ben Stopford - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Contemporary Approaches to Data at Scale - Ben Stopford</b></h2><h5 class="post__date">2015-11-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LMPw1o1uR-I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay thanks coming my name is Ben
Stopford I work at a confluent inc which
is the company behind a patch of kafka
so cafe is like a distributed message
broker essentially that's get to use
quite a lot in big data use cases and so
we're going to be talking today about
contemporary approaches to data a scale
and so this talk isn't about Kafka and
we kind of predates my working a
component but the year so the ideas here
definitely my own I'm not necessarily
those are those are compliments although
I think there's a line there which is
probably yeah this is some yeah
confluence in it no pun intended um yeah
so if we when we talk about data
technology um what we really tend to be
trying to achieve is some sense of
locality and that's kind of a fairly
abstract way of looking at it but it
just means really that we've got a bunch
of data its spread around some media it
might be disc in might be memory and we
want to get it into some some shape
that's that has a degree of locality so
that we can do sequential addressing and
what the reason we want to do that is
that if we can address any kind of
workloads sequentially we can take you
we can take advantage of many many other
layers that the computers have to make
our operations efficient so obviously
the caching CPU has held levels of
caching the page cache dis perfect etc
so if we're accessing for accessing data
with a sequential workload the computer
has a chance of actually predicting what
we're going to do next it's very very
simple idea and that kind that makes
things very very fast so to take the
extreme which is a a sort of standard
magnetic hard drive if we address if we
if we stream data sequentially will get
maybe 200 megabytes per second that's a
pretty standard throughput for a
magnetic disk if we try and address it
at random we'll get maybe 300
per second so for 100 bite row that's
about seven thousand times faster so you
probably get this right because most
people have some sort of intuition that
discs are kind of slow but the
intuitions I see a little bit more
fine-grained than that because it's
that's discs or slow if we ask them to
do something that's effectively a random
workload whereas if we ask them to do
something they're sequential there
really quick and this is this property
is ubiquitous it's got nothing to do
with discs per se if we saw an
interesting fact is actually if you try
and if you were randomly address the
objects around your address space like
many garbage collected languages tend to
do you'll actually get performance which
is pretty much exactly the same as
sequentially addressing a magnetic hard
drive so rat so the intuition that Ram
is always fast and disk is always slow
is it's not a particularly good one much
better to think about whether or not a
workload is addressing data randomly or
sequentially so the point here is really
just what we want to do is obviously we
just want to make our workloads as
sequential as possible so let's just
this just to examine this a little bit
further let's let's think about a
rudimentary database and a rudimentary
database is nothing more than a file so
if we want to add data to our little
database we can just append it to the
end of the file and that will work
really well because we're that's a
sequential workload writes a sequential
write work out so we'll get the
throughput of our disk media and if we
want to read data efficiently we can
also scan right so we could do what is
the equivalent of a table scan within a
database or we could jump to a
particular offset and we could scan from
there both of those would be very
efficient were closed because of the
sequential access so that's quite nice
obviously what we're trying to do is
avoid scattergun in the file system
while we're reading data or writing data
but both of those passes would work
quite well so if you think about kind of
writing trade-offs just appending data
to the end of a file has some
limitations so if we if we if we let's
say we will have some workload where we
actually want to change a value within
our database if we just append it to the
end of the file
we'll end up with an orphaned version so
that's kind of okay if we table scan the
entire thing and G duplicate on the way
that would be all right but and
obviously we keep on my sequential write
performance but obviously that means
that we have to scan the whole thing
alternatively we could do what a
traditional database would do which is
to update in place and that this kind of
assumes that we've got fixed width we
fix with feels like a traditional
relational database would do and that
works very efficiently from a read and
write perspective but of course we can
end up scattergun in the file system as
updates come in because we can't just
append them to the end of this nice file
but it's so these two these two patterns
are quite important for understanding
how storage engines work in different
databases both sort of old-school and
contemporary so scanning the entire file
is of some use for some workloads um but
it's obviously relatively limited in
terms of objective terms in terms of
it's sort of general applicability the
next thing we tend to want to do is
support some form of look up so not
unsurprisingly we would do this with an
index so the oxys lot sort of types of
index this would be an example of a
b-tree and we might use a hash index and
we might use a bitmap index we might
just use a set of values with pointers
and in a sorted file and that would work
equally well it doesn't it doesn't
really matter but but the traditional
way of doing this is to have some kind
of index structure and have some kind of
heat foil underneath and have the values
in your index with pointers associated
with them that tell you where the
underlying data is so again all fairly
rudimentary stuff so the problem with an
index is that it provides this an
overarching order which is read
optimized the reason we added this is so
that we can get very quickly to the
value Dave but as soon as we add that
index we've if we if we had this the
nice append only structure which
leverage sequential I oh then we'd lose
all the nice benefits of that because as
values so if we update the value Dave we
can append it to the end of the file and
that'll be very fast but we still going
to have to
jump to the position within the index
where Dave lives to update the value to
say that they've value has changed or
has appeared for the first time so we've
gone out of this nice sequential
workload and we've gone back to scatter
coming gunning our file system or
address based on your what kind of data
type of database is so that's kind of a
bit annoying um so there are a whole
that this is kind of a fundamental
problem for databases so anyone who's
used a database will be aware that if
you had lots of indexes your inserts
start getting really slow and it's for
this reason so that there are kind of
variety a whole range of different
approaches for making or forget for
getting around this problem and we're
just going to talk about three of them
and those essentially are the three that
sit at three extremes I guess of the
year of the spectrum and there are some
which I won't discuss which sit in
between so the first one is really
simple right this is like really
intuitively kind of obvious so what you
do is you stick your random workload
problem your interim so you put your
index in RAM and that's that's what many
databases do and it works very very well
under certain use cases so my my problem
of random I ervice is only storing man
that by all of my data sits on disk and
I can append to the end of it and
effectively I've kind of solved my
problem well I've solved my problem
actually only in the only in the context
that I have enough around to fit on my
indexes in that's actually specifically
what I've done so it turns out that
that's actually not always that
beneficial for a number of other use
cases so one is is a kind of big data
workout so if you think of a big data
work out the prom with the big data
workload is that we're likely to get
into a situation where we don't want to
bear up we don't want to provision
enough around for all of our indexes we
need we want something that's a bit more
adventure effectively disk efficient so
there's and there's another way of kind
of reframing this problem so that we can
we can get more efficiency which is
quite interesting so in this approach
what we do is we take our rights and we
batch them up and
we sort them and then we write them out
to a little index file and typically
this little index file will actually
just be a sorted array of effectively
key value pairs but it could be another
form of index if we wanted it to be it
doesn't really matter but the key point
is is that instead of having one over
instead of having one heap file and one
overarching index what we have is a set
of small index files each of which is is
navigable in itself quickly but we've
but we lack that overarching structure
that we had with it in the previous
example where we just had one big index
so that's kind of an interesting idea
and you might think that what does that
gain what did you gain from doing that
well certainly one of the problems with
doing with going from one big index to
lots of little indexes is that when you
need to look something up instead of
having to just go to the index find the
value you get the value of the heat
vault and off you go you have to consult
all of these files and in physical
implementations of this the number of
these files will actually get pretty
large you'll get into the hundreds if
not thousands so you sort of have to
navigate all of these different files in
order to read something so intuitively
you might think that well what if I
actually gain by doing that well the
answer to that problem is that what
we've actually done is we've shifted the
problem of random bio on right to a
problem of random I Iran read so now in
this model we can sequentially stream
data to a media very quickly because
everything sequential I oh but we've now
we now got this how do I now have to
read a whole bunch of files effectively
at random and because we've shifted from
the right concern to the breed concern
we can use some clever tricks to
optimize read so it turns out the reads
are a lot easier to optimize the rights
that's basically the crux of it so we
can use things like the most common
approach here is a thing called a bloom
filter who's familiar with the bloom
filter good audience that's a great
start okay cool well for these of you I
won't go into too much too much detail
but it's a very simple probabilistic
data structure it sounds kind of
complicated you can write one pretty
quickly
but it just allows you to answer the
question in a very small amount of with
a very small memory overhead of whether
whether the data that you want might be
in one of these files so mascots this is
kind of an interesting and so we've got
another approach which is effectively to
push the the problem of random writes to
a problem of random reads so that we can
optimize with clever index structure and
of course all of those levels of
clashing that we had before the cashier
within the CPU the caching on the desk
etc and this thing is termed a log
structured merge tree and it's the basis
of many storage engines thats it
particularly in a big data space so if
you've used HBase or cassandra level of
DVD blocks TV etc they all use this this
structure and its best so it's a based
of a collection of small immutable
indexes and always it's always append
only on desk we have this still have
that problem that we had with the
original file where we're going to end
up with duplicates so we end up with
this with garbage collection process
that runs in the background to
deduplicate things but that's actually
okay because again that's sequential
access and it's only the random access
that's going to really kill us from from
the performance perspective and then the
nice thing is also we can use sort of
low low memory tricks to improve the
performance of our reads so it's
shifting a problem from random random
access for right to random access of
breed okay so those are two approaches
stick sticking indexing RAM and secondly
use something like a log structure merge
free which is going to push pushes to a
random read workload the third approach
is kind of completely different and this
is the this is this is effectively just
the brute force approach and so if we if
we imagine that we want to we have a
workload where we actually need to get
at all of the data and the most common
or lease a large amount of the data and
most common use case for this is some
kind of aggregation okay so you kind of
have to visit it anyway so if you asked
a database a traditional database to do
it it's good a table scan so the
kilometer approach is basically saying
well I've got to do all this scanning
anyway so what can I do to optimize it
optimize these scans
to be the most efficient thing the most
efficient sort of most efficient
strategy that I can get and whilst
obviously retaining retaining the
removing the data that i need to get my
result so it's very simple idea we so in
a this this is a tabular form you can do
it in a in any put in a place where I
should form as well but we take a row a
B and C and we split it up into three
different columns so each of these will
be individual files so we have a file
with all of column a file with all of
Columbia and file with all of column C
and the reason that's quite a good idea
is that if we want to do an aggregation
we can literally only access those files
which we need and that's quite typical
in sort of analytics work workloads so
we've already reduced the amount of data
that we need to remove from disk so
that's cut that's quite a quite a nice
idea the thing that's really important
with it with this approach is that we
have to keep the same order in each one
of these files so it will be really nice
to be able to sort each one of these
individually but that wouldn't work
because we're actually using the
effectively the row ID to bind these
things back together again and the
reason for that is that whenever
whenever a columnar database ever does
any work it is always doing a merge join
because it's all that unless you had
some strange used case where you
literally only wanted to access one
column you're always going to be doing a
join so it's all about basically
optimizing for the best efficiency you
can within a year a brute force workload
and the other quite nice thing that
comes out of this pattern is that column
cons have a tendency to compress really
well so for example if you has better
you had a column full of country codes
that's got really low entropy right so
that's just going to compress
beautifully even with this or a very low
cost compression mechanism alternatively
you might have them just incrementing ID
which would delta compress beautifully
so there's some really neat tricks you
can do on this kind of with this kind of
set this this kind of data to decree
decrease the amount of i/o that you need
to do with in the storage engine
so there's kind of like there's like
three approaches to to building storage
engines and so brute force less i/o by
column use compression always you need
to hold them in row order unfortunately
it would be nice if you could you could
sort them all but it's it's just doesn't
work out to be efficient the reason is
that you have to do merge joint right by
a row ID and then this mother stuff like
you can add practice on to compress data
and you can do things to make these
things very efficient so which of these
approaches technology uses is a useful
tool to sort of work out which workload
is going to be good for so kafka so this
is the the the product of the company I
work for belton support so calc is
actually very efficient for the dis
perspective because it's a message queue
and it doesn't have a lot of the
overhead that you will get us you tend
to get associated with sort of
traditional messaging products which
have to implement less a JMS or am
keeping or one of these or one of the
standard protocols and so it's very much
based on the idea of just scanning to an
offset reading some values giving some
data to someone and then going away so
the user goes away they come back again
they go back to the offset with a lot
but they lost the they were last at and
they scan a little bit more it's very
simple idea but it's very very efficient
because it's all about sequentially oh
so many of the no SQL is that you use
react Mongo pretty much as a large
proportion of them will take this
approach of effects of it effectively
just a very simple approach of memory
optimizing their indexes and that that
works really well for general-purpose
workloads where it's been get it will
always become problematic is when these
these wear these index based structures
start overflow onto disk so very fast on
memory centric workloads not so good
when the proportion of memory available
is much lower when compared with the
amount of data you need to store so
that's kind of think that the Rio go etc
and the big data tools tend to take this
lsm approach so HBase Cassandra Vox gb
etc and if you use these tools that the
one thing that's very obvious about them
is that they can bacon map they have
very good scalability as the data set
grows and the the vm size is relatively
small so that's why these these storage
engines suit big data workloads Mongo
actually now has one of these as though
right there as a because it has a
pluggable storage engine model which is
quite nice and then any kind of calumny
beta base be it an old one like sybase
IQ or a sort of more contemporary one
like or any of the Hadoop stack that
uses parquet in parlor etc they all used
it they'll take this approach of
chopping data into columns and
compressing it effectively so that was a
bit on storage engines now we're going
to talk about parallelism so in any any
workload that sort of goes beyond the
size of a single machine you end up
getting into some use of parallelism and
I'm not going to talk about this too
much but we're just going to talk about
a couple of simple and salient points um
so one of the common patterns which is
fairly well known is of see the idea of
consistent hashing he's familiar with
consistent hashing okay so it's very
similar to what you do inside a hashmap
effectively so it's simply to say that
look that I can use a hashing a
well-known hashing algorithm to get data
specifically from the machine that it
sits on because I know that it's been
sharded by its primary key and this is
actually a wonderful pattern i mean if
you've got a if you've got a year a
problem that happens to fit into this
space you have a wonderfully scalable
solution the downside is that it's
relatively simplistic you can only
access data by key but it does give you
this very scalable pattern at this very
scalable mechanism for getting into a
sort of distributed shared that shared
nothing architecture
the other person is kind of the opposite
and that's that's broadcast so this is
this is what most analytics engines will
do this is what I dupe does and that's
very simply to run a process or run a
query analytics etc on a whole bunch of
nodes at the same time so you're
exercising the ball and your return
results at the end so that's the
broadcast brooch so the interesting bit
or the bit to watch is the middle ground
between these two so any any sort of
consistent hashing based approach will
give you very good scalability any
broadcast method will allow allow you to
exercise a large number of machines to
solve a big problem and there are quite
a lot of workloads which sort of sits
somewhere in between and probably the
best example of this is secondary
indexes in no SQL store so many you know
us girls stores Mongo Cassandra HBase if
you use the add on will provide a
secondary index function and you might
actually be asking for something that's
relatively unique right so you might be
asking for I don't know users by their
name where their name is not the Chardon
key in that kind of instance then you
just need to be aware that you're
obviously going to exercise all of the
machines at the same time and that's
going to limit the scalability so those
the two patterns work really well we
just have to be aware of this list of
this middle of this middle ground so the
final thing when we're talking about
parallelism is replication so actually
to build any single store that truly
scales you have to have both of these
things you can't just you can't do it
just with sharding and the reason you
can't do it with just four sharding is
because of that broadcast problem if
you're broadcasting to all the machines
and you're exercising on the machines
and that that won't scale from a
concurrency perspective so replication
is easy is your friend replication is
actually used in many stores for other
issues or other reasons for redundancy
etc that's probably its most common use
but where it's really so where it's
really useful from a performance
perspective is just simply for scaling
out a very
simple in a very simple way so we used
to do this back in the 90s with
relational databases you just have like
a reporting replica it's the same
concept as you might do inside something
like Mongo or aunt Cassandra just by
having a readable replica so the key
point of replication is that they allow
us they allow us a way to isolate load
that allow us to isolate loaded onto a
bunch of subsets of our of our datastore
and that gives us a different form of
scale out to just using sharding so in
any any sort of any store the needs to
scale linearly you're likely to want to
have both both of these operators to
play with and it provides provides
Brandon redundancy too so the problem
with using replication is that it plays
off against a consistency and this this
kind of concept has been made fairly
famous by the use of the cap theorem
people familiar with the cap theorem
probably fair for you so the I mean this
is very very simple idea but if you if
you have multiple replicas and those
replicas can't talk to one another for a
point in time then when a particular
point in time then you're going to have
a hard time trying to keep them good
make them consistent very simple way of
looking at it which brings us on to this
idea of a tomite City and ordering so 80
meters in an ordering is a very
expensive property in any database so
most relational databases dating back to
the 1970s would have some form of
isolation level like serializable which
basically says I'm going to I'm going to
ensure that it looks like all all
actions on this database have been done
within a single thread and in reality
very few of them these days either even
implement that or certainly it won't be
their default and the reason is that it
even on a single machine is too
expensive particularly in the days of
multi-core processors in a distributed
world it's even more of a penalty so it
it becomes something that we end up
giving like giving up which actually is
a slight side is something I think is a
bit of a shame in in the current no SQL
world because there's actually many ways
that you can get better consistency
concerned or better consistency
guarantees that most of those products
provide but that's really a different
talk so one of the one of the best ways
to solve a consistency problem in a
distributed system is patiently to avoid
is just to avoid it and the simplest
idea that sits behind this is the single
writer principle so if I can if I can
ensure that all my rights just go into
one area and then I effectively have
replicas then at least my sort of
consistency like my right consistency
concerns can be isolated 28 to a to a
single machine which makes my problem
much easier and potentially have read
consistency concerns over my different
replicas but we'll get to that in a bit
so this there's a few kind of names for
this but one one one term in uses is
this this idea of c QR s which is
command query response segregation and
very simply put this is just the idea of
a splitting read reads on the or
splitting your rights from your reads so
if i if i right I my bite some data to
this database and then i might
denormalize it or pre-computer a result
into something which has a more read
optimized so storage engine and then I
can query from that interface that will
give me a better boy certainly a better
response in terms of my ability to it to
to write data and my ability to read it
the problem with this obviously is that
in this architecture you can't
necessarily don't necessarily get to
read your own rights because there's a
no latency associated with this process
here but the key point here is that the
underlying principle of having kind of
multiple storage engines within the same
architecture is quite a nice one and
certainly some certainly something worth
considering
so if you look at it there's a datastore
so many decibels as you do something
like this but just as an example we look
at druid so druid is a analytics fairly
contemporary analytics engine anyone
used dude now yes and no so the so this
is I mean this this is a relatively new
products so they just split spits out
the spirits out real-time storage from
historic storage using effectively
different storage engines light light
very similar to the ones that we were
talking talking about before so
something that's right optimized for
accepting data and something which is
really optimized for providing
effectively analytics and then when it
it just queries both so this is also
transparent to you which is quite nice I
still don't know quite how it deals is
negation but I need to look into that
more so the core point though is that is
that by spice by using multiple storage
engines with a sort of veneer on top we
can kind of get the best of both worlds
so there are other ways that we can take
this pattern a little bit further so
this idea of trying to separate
different different the operation of
different storage engines within an
architecture and so the martin Clemence
did a very good talk maybe a year year
and a half ago about turning the
database inside out did anyone ever seen
that if you had nods yeah so he said but
it really it's a really nice idea and
that idea is it plays out for a number
of reasons there were a number of
reasons that it plays out quite well and
one is that databases are very good at
the jobs that they're they're designed
for but they have some shortcomings and
one of them one of them is that it's
actually quite hard to interact with
with the database and it has sort of
fairly set rules in terms of what you
can do with it it's fairly hard to get
data in and out as well so there are
some advantages to using an approach
which actually explicitly pieces
together different pieces of technology
to achieve a similar goal so in this
pattern what we have is a
we have a layer which is which which
like the pro like the druid example has
the sole responsibility of effectively
accepting data and managing consistency
so that means right consistency so we're
so that's that's the layer in which if
we may we may be transactional so this
is the sort of this is where we'll hold
mutable state behind that we have a
streaming layer and from this point
onwards everything is immutable and
immutability is really a really
important property in this kind of
system because it allows us to
effectively allows us to use replication
without having to worry so so much about
consistency concerns so have a mutable
there at the front we have a stream
processing layer in the middle which and
then which clients can potentially
listen to but at the end we'll have a
set of views and those the important
point here is that those views can be
different things they could be different
technologies or more importantly they
can actually be different cuts of the
data if we want some more data than we
can just replay from our stream and
regenerate the additional information
that we need so this is quite a powerful
pattern simply because it's offloading
as much of the work load but as much of
the analytical workload as we can from
it from the consistency constraints that
we would have on a traditional database
and shifting it into an immutable world
which is easier to optimize the smart
people amongst you will notice that time
is the tricky thing at that end but yeah
they're always around that too so yet
another sort of similar approach which
really addresses actually quite a
fundamentally different problem but is
worth discussing and is the approach
used by the lambda architecture or this
this is sort of batch data flow so
typically these will be Hadoop based
systems and who's familiar with the land
around Asia who definitely a theme and
so that lambda architecture is is really
just an evolution of the batch data flow
pipeline so when people started using
doop they would generally generate a
batch layer which will iterate through
really just sort of a different set of
incarnations of data in some way that's
useful and hopefully come up with some
useful results which they put it into a
serving layer and one of the main
problems with this which you may be
familiar with even if you've never used
to do because you've data into a
relational database and pulled it out
and some something with it and put it
back again and and sort of ended up in
this in this person have been caught in
it in a persistent query paradigm these
tend to be quite they tend to be quite
slow so a chap called Nathan Mars who
wrote storm came up with this idea of
bolting effectively a streaming layer on
the side and that allows you to get a
much faster response to specific to sort
of specific workloads and will
effectively be fast response writer
result of serving layer and then the
bachelor will just overwrite it when
it's got a sore spot on result when it
actually gets around to doing it later
so actually despite this having a name
many companies kind of did this anyway
it's a sort of natural response to the
problems they're associated with batch
batch data processing so the city's
pattern and I guess the downsides of
this is you have to have like two
different layers today there are better
technologies which allow you to do it in
one but fundamentally you're still
separated it's still sort of separated
so you have as analytics working in two
different in two different layers so
then the idea of the stream data
platform so this is quite a nice idea
this is really an evolution I should
save actually so that the big difference
between these patterns and this person
and the previous pattern or the
operation analytic bridge is that all
the data here is just immutable so what
we're saying it's immutable by default
and the reason for that is that many
many data sets that we would use today
have no requirement for consistency as
an if you're collecting logs educating
log files then
an entry a line in a log file never
changes it's a beautiful beautiful by
default but so in the stream data
platform it's very very similar approach
to the previous one the argument is
simply that anything that you can do in
this layer or certainly for a large
number of use cases you would be able to
do in a stream processing layer
supposing that you have a sufficiently
powerful stream post processing engine
this is kind of the same kind of
approach really we're taking data or
putting it but instead of putting it
through a batch engine with a sort of a
shortcut alongside we just put it put it
through a scale-out messaging system
like Kafka and then off the back of that
we can use stream processes to create a
variety of different views and the
really nice thing about this pattern is
that um it's sort of a little bit
different to the traditional approach of
kind of try and consolidate all your
data into one place because you can
leave stating Kafka it means you can be
a bit more flexible about the way you
create these different views and
actually the trick to doing this is as
is the trick really with any any data
centric system is to create something
where something that has a degree of
reproducibility because it's the
reproducibility which allows you to sort
of evolve and change things quickly
because beta is just really kind of
difficult to deal with that's kind of
that that's the idea of through new
platforms which is which is quite a
quite a nice and powerful idea and so
well we can apply that pattern to a so
we can apply this pattern to a sort of
big data analytics workflow with a with
us or a powerful stream processing
engine like Samsa or or storm we can
also can also use it in sort of bigger
data integration approaches so this is
again this sort of combination of the
last three patterns so hit but here we
just have source systems which is at
which are acting really to encapsulate
the consistency concerns within
themselves and then stream out a set of
immutable changes and this is something
that in theory we could have done a long
time ago but actually it's pretty
to do this in practice so change capture
systems have never really been able to
fund all the the day you'll handle the
data flow associated with a large number
of databases but certainly that so
there's there's an approach for doing
this in size effectively a company or a
company integration integration work use
case and then off the back is similar
similar approach again we have we can
have a Hadoop layer and analytics engine
we might have a something that the
supports and inverted index like a
search engine and I guess the other
really nice thing about this pattern is
this idea of we tend to do we tend to
get caught up in the idea of persisting
state and then processing it this
pattern really just tries to keep data
moving all the time so by keeping data
moving by default then our that where we
want to get to it is somewhere where we
we actually we temp to process data
whilst it's in flight and pass on the
result to the next layer rather than
just work trying to work out how to
persist it so there's something else
going query it later and that makes for
much sort of more agile and sort of
temporally efficient workflows certainly
but with it within both big data
pipelines and also within companies so
things we like well we liked routing
state is something which is immutable we
like the idea of being able to just
append things to a log that's a nice
idea we like the idea of listening to
things that are changing right so if
we've got a log then we should just be
able to view that as something
analytical or something that's just just
a stream of changes that we want to
respond to and that this is something
that relational databases have never
been so great at Prince it's a sort of
powerful pattern the idea of replaying
things that have happened before it's
also quite powerful the idea that if we
have a single sort of chronic
chronological stream of state that we
that we can create something that
doesn't have to be all encompassing from
the outset the idea that we can just
create something that's specific to our
workflow just the data
we need to solve this problem today and
then when we need a bit more data we
need some extra columns or to enrich it
in some way we can go back and we can
just replay the extra days that we need
over the top that's a really powerful
idea for actually getting things done
and the idea of trying to split the meet
the mutable from the immutable so we
isolate consistency concerns in the
smallest possible bound that we can find
that's the best way to scale and also
and using both sharding and replication
so replication is actually a very easy
way to scale if you can if you can just
create replicas so I mean the key point
here is that generally your shard at
this layer if you need to and this layer
your your replicate first and chard
later okay and and finally winner yet
within a single store we need both of
these premises so many of the sort of
technologies like ongoing Cassandra to a
certain extent and give us both of these
primitives but we definitely need both
of these primitives in any solution that
doesn't have both of them will be
limited at least in some context and the
final one is being able to raise reason
at that time and this is as yet a a
problem that needs to be solved in a
sort of ubiquitous sent because as soon
as we sit in an asynchronous world we
have to have some form of some mechanism
for coordinating so the way this tends
to be dealt with at the moment is
generally by either avoiding the problem
completely and and not talking to
different replicas that's a very simple
way of solving the problem or the other
one is that replicas actually have some
context of the time of which they on
which they are running and clients
simply have have to solve the problem by
by asking them and then finally it's
just this idea of play basically
blending a different set of tools tools
that are right for the job inside inside
a single platform which allows you to
propagate state
across it so that's it and thank you
very much for listening and other ready
any questions I probably over around
slightly bit hey don't you sing the
writer yeah I well I I look sister Tomic
when it first came out I came out and I
haven't looked at it that much since
then so the question was about de Tomic
which is a database which were built in
the closure ecosystem really i thought
it just used whichever or sony when I
look today use any pluggable storage
engine that you wanted to I imagine it
probably now has one and I don't know
what it is but certainly when I looked
at it you could put it in anything
people were generally using key value
stores but it may have moved on I was a
few years ago good question now but yet
certainly the single writer principle is
smart right simple simple way to solve
the problem any other questions yeah
certainly yes so you have to say yet
because that's a really good question
and it's a really good question because
the temptation to just update the meat
the immutable state in those views is
almost overwhelming if you haven't set
it up in such a way and that's why that
it that's why the trick of having
automated is so important because you
just can't basically have you have to in
any of these patterns you have to be
able to fix in it in this layer
otherwise this stuff just kind of breaks
down that's it that's in my experience
anyway I mean as soon as the start to
diverge from the truth then it just
becomes a nightmare so you literally
it's it the trick to this is automation
actually like in many problems in
technology it's about having something
that's that you can just click a button
and regenerate it and any other
questions all right well thank so much
guys</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>