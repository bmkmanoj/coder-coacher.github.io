<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Erlang Solutions Erlang Writ Large: Adam Kocoloski | Coder Coacher - Coaching Coders</title><meta content="Erlang Solutions Erlang Writ Large: Adam Kocoloski - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Erlang Solutions Erlang Writ Large: Adam Kocoloski</b></h2><h5 class="post__date">2012-06-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iWLudbnB72I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so yeah this is me by way of
introduction i guess i earned my big
data stripes not really an engineering i
have some background in engineering but
i'm a experimental particle physicist by
training and in those experiments so you
take these beams of elementary particles
whether there are electrons positrons
protons etc and you accelerate them to
very high energies and collide them and
when they collide they generate these
very large high-energy collisions and
you try to detect as much as you can
about that event and that'll be a
megabyte let's say two megabytes
something like that it's on that order
of magnitude but you're looking for
needles in haystacks you're looking for
really rare events so we try to collect
those as fast as we possibly can you
know we can build the electronics to do
it every 25 milliseconds we will and
that ends up to collecting lots and lots
and lots and lots of data so in that
field we had developed all these
strategies for you know data collection
calibration manipulation analysis we
were doing all this stuff with batch
computing systems and all the grad
students all the poor grad students
spent all the time bookkeeping and
resubmitting jobs that failed and
worrying about which sets of
calibrations were applied and this that
and the other thing and it was well fun
for someone who was interested in the
software side of things to design more
and more efficient systems but it left
me with a real desire to build something
that was a as bulletproof as I could
make it and be could recover from all
sorts of Byzantine weird scenarios
especially in the realm of data
replication who's one of the things we
were did for technical or political
reasons was keep copies of data sets in
different facilities and universities
for a lot of people to have like local
compute clusters so we had you know
multimaster my sequel replications
across the u.s. and I think I was in
charge one for a while and you know grew
tired of shipping bin logs around and
really wanted to do something different
and so when my colleague and that group
talked about hey let's go you know pitch
Paul Graham with an idea to build a new
database mostly i'm sure it's worth shot
and so here i am today i got into our
line by way of CouchDB because i kind of
knew what i wanted to build and I looked
around this was mid-2008 and CouchDB
he's little blurb on the website
described all the things that I was
looking for us
this guy gets this is what I want to do
and saw that you know the clustering was
like on the roadmap and so I want to
take a shot at that and so I worked with
our company I led the effort to develop
a clustering layer based on distributed
relying that we called big couch and the
company's business models one of them
managed hosting platform as a service
provider we take you know software
that's based on big couch and run
people's databases for them so a little
bit more about that big couch we called
a marriage of Apache couch to be an
Amazon Steinem all right i mean there
are a lot of systems out here that are
based on the fundamental principles that
you talk about consistent hashing
quorums for reads and writes to minimize
latency xand survive you know transient
node failures some anti entry mechanisms
to deal with you no more permanent node
losses or situations where you didn't
write the data that you wanted to right
where you wanted to write it at the time
that you wanted to write it and I felt
that it was really a natural fit for the
core technology that had already existed
in CouchDB right so couchdb is this
restful document store it's HTTP and
json every document that you put into
that database has a hash history
associated with it that lets it track
you know divergent edit histories that
might happen if you were editing a
document on your local machine and
someone else was editing on the server
couch has the you know data structures
in place to track not only that those
two edits happened but it contract if
one was an ancestor of another if they
were two different edit branches and if
so where they diverged and allow you you
know as the client to ultimately employ
some application specific
conflict-resolution strategy to take
care of that so with big couch what we
did was try to leverage as much of that
existing core technology in CouchDB as
possible to build a system that was
horizontally scalable across clusters
and machines connected via distributed
Erlang links so it's you know it's
fundamentally master master because you
have those hash histories to track
what's going on you don't have a
situation where you need to elect a
master and have rights start there and
flow elsewhere and you know it's
certainly capable of storing redundant
copies of information in fact it's the
only way we run our systems at Cloudant
to ensure that you know well data safety
queen service died and it should be able
to restore that replica level as they
come back online or as you replace them
all the buzz words you know no single
points of failure durability fault
tolerance etc etc this talk is not
really about this per se but it is you
know all of my experience of working
with our lying over the past several
years has all been you know working
towards this product and globally
distributed this is not so much a
feature of big couches it is the cloud
and platform what we have big couch
clusters kind of distributed throughout
the globe sometimes spanning multiple
data centers and you know we allow users
to kind of leverage that as best we can
but what I'm talking about here so i
called it Erlang writ large and what i
wanted to do was with a couple of things
but first and foremost i wanted to just
kind of take a step back and think about
how the principles of a good girl lango
to be application and I'll enumerate
them are really oftentimes shared with
the principles behind building a
distributed days or a big you know web
service big networked computing
environment and I think sometimes it's
you know it's it's something that
started in the the network environment
and you know Erlang has borrowed that
same kind of philosophy and sometimes it
flows in the other direction and for the
rest of the talk I wanted to dig deep
into high-throughput distributed airline
we've learned a lot about it and I think
it's something that many people I think
there are a lot of our language out
there who don't use a lot of distributed
Erlang and it's I think it's really
great I think we've been impressed with
how far we've been able to push that
protocol and instead of having to rely
on another out-of-band method for
distributing data across our clusters
but it does take a little bit of work
sometimes a little bit of tuning and
there's certainly pitfalls out there and
just generally share some war stories
about the experiences that we've had
operating our systems in production over
the past several years so the first
principle mutability right we talked
about this in the video in the keynote
at this morning single assignment and
and all of that which seems odd if
you're coming from another language and
personally after years of working in
Erlang I have a hard time imagining in
any other way we solved a bug in our
some of our web stack at one point last
week his you know python django app or
something like this and it all related
back to a mutable global array of load
balancers and sleep no that explains it
you know I think immutability keeps
simple bugs simple you know if you have
a something you don't expect the single
assignment oftentimes protects you it
throws a bad match before things get
really weird and you find out you know
down the line that your data structure
is just not at all what you expected it
to be and on a performance perspective
there are a lot of nice attributes that
you get you know in today's multi-core
many-core high concurrency environment
when you don't have to worry as much
about lock contention whether that's in
memory or on disk because you're not
worried about immutable data structures
and mutable memory so it makes
concurrence easy and the same is really
true when you start looking at a couch
TV for those of you who aren't familiar
with couch can be one of the kind of
core pieces of the design is
everything's based on append only be
trees so the way those work is you have
your tree structure and you go to update
a leaf you know you you update a value
your add a new value somewhere in that
tree you end up rewriting the path to
the root appending those nodes to the
end of the file and so all of the
pointers always point backwards into the
file then you go to read the b-tree you
find the header you know the root of the
b-tree at the end of the file and it all
has back pointers to the current version
of the data right and so it means that
there's some some garbage and you know
there needs to be cleaned up
periodically but I think it goes a long
way towards creating this this you know
perfect ideal we have like a bulletproof
storage system it becomes much much
harder to break if you if you know that
the you know you don't have to do some
sort of repair mechanism after a crash
or something like that I mean we know
we've done it if you crash midway
through updating the tree it's no big
deal because the next time it opens its
going to go to the end of the file walk
backwards till it finds a validator and
use it and it'll just ignore all that in
place stuff that happens you don't have
a corrupt file you just have you know
some data that you didn't write and
hopefully you wrote it on one of your
other copies in your clustered
environment and by the way please
interrupt me if I'm going you know too
fast or making assumptions about the
audience that I shouldn't be making
I'm happy to to delve into these topics
in more detail but so immutability I
think it's it's it's a huge part of what
makes our service operable by you know a
staff of a start-up operations team you
know as it scales to hundreds of servers
so and you know it's it's something that
we borrowed from our lying in a sense
and it's just it's a nice marriage of
the way or allowing expect you to
program in the way these systems are
designed operate on disk so the second
bit is supervision and I think here I
would almost take about it from the flip
side when you're building a big
networked environment with lots of
servers possibly multiple data centers
you're forced into treating these things
as isolated units with their own failure
modes and supervising them and
restarting them or building new ones as
needed right so you've got your process
monitoring tree in the operating system
that restarts a demon if it fails and
you've got you know in our case with a
web service you've got proxy servers
that are checking the health of the back
ends and knocking them out if they fail
and above that I don't know maybe you've
got you know DNS managing multiple data
centers and failing over you know the
detects that one of those goes offline
whatever the case may be is this natural
tree of isolated units you know with
their own failure modes and that's you
know forced upon you by the the physical
reality of the system you're working
with and our Lanyon forces you on I'd
bye-bye just the very nature of how the
programming language in the virtual
machine is designed I mean certainly we
can build things in other you know
paradigms in other languages but Erlang
encourages us into the same sort of
isolated faults and an error handling
that you know that people often claim is
a you know a competitive advantage for
Erlang servers and so I just think again
it's you know it just speaks to kind of
the duality of Erlang applications
inside a virtual machine and larger
networks that's all I want to say about
that I just think it's a it's a nice
parallel that's the time I'd like to
talk about is really about distributed
or lying so just very briefly the Erlang
distribution if you set it up just
normally is going to make a TCP
connection between every node right it's
a pairwise connections you have you know
n squared minus n TCP connections
between all the notes and your cluster
is fully connected mesh
and it's just one tcp connection used
for all communication between each pair
I'm sure their ways to hack around that
but that's the that's the normal way
that it works and so I think most
people's introduction to strip it or
lying wow that's kind of small is you
know the RPC call right so then RPC
module in are lying and you can say RPC
call with a node module function in some
arguments right and that's all well and
good it's you know it's a nice
abstraction it makes you know you think
it kind of looks like you know it's
still sort of a local thing when the
results come back to you you don't have
to think too hard about it unfortunately
it's it has some some weirder properties
sometimes so the way it's actually
implemented if you go and you read the
OTV source code is that there's a named
Jen server living on every node and
you're distributing ring called Rex
right and so when you do an RPC call
what that actually does is a gen server
call to Rex on that other remote node
and with the module function args and an
infinite timeout let's say and if you
dig in the the Rex server loop the the
call handler for that you know message
is this handle call Cole thing which
does a few smart things right it doesn't
you know block the server to do the job
that you wanted it to do it spawns a
local process to get the work done all
right so that's all well and good and so
this is the process that spawned right
here it does some some you know clean up
of exit statuses and that sort of thing
but then when the process that spawned
is done it sends a message back to this
local RPC server with the reply that it
specs expects the RPC server to send
back to you when you made that call
right so you can picture the line of
communication it's it's your client
process on node a talking to Rex on node
B Rex on there be spawns a little worker
worker talks back to Rex Rex talks back
to you ok and yeah this is just Rex
talking back to you so Rex receives this
reply finds out who the original color
was and does a gin serve a reply which
is really just you know it's like the
bang-up right are the you know it's a
regular old or lang send back to you
so that's how that works and it's fine
what everything works well but when
things aren't working well it gets
interesting so in Erlang if you are
trying to send a message to an
overloaded process so you're trying to
communicate with an overloaded port or
an unresponsive node the virtual machine
responds to that condition by suspending
you by blocking your execution for some
period of time and hoping that you know
the other side that you're trying to
communicate with deals with its overload
situation you know after you're
suspended that becomes problematic
because if you remember it was Rex
that's responsible for sending
communications sending messages back to
all these original client processes you
have one process on your airline node
responsible for the remote communication
with all the other nodes in your cluster
let's say one of those notes dies and
you've got this high throughput thing
you know with Rex pumping out messages
to all these other nodes it's going to
get walked eventually trying to
communicate over the socket with the
node that is dying that's overloaded row
and responsive right so let's say no day
just drops off the face of the earth
right it's going to take a while for the
vm to notice that notes gone meanwhile
no would be is you know feverishly
trying to send messages back to know day
for the jobs that were started and
eventually it gets blocked and not just
blocked for communicating with no D
which would be sensible right but block
from communicating with c d e f and g
they're all backed up waiting for Rex to
you know get on with it and figure out
how to send things to know that these
damn sorry
it does it does and there's a thing in
net kernel that will detect that so
there's two conditions right you could
just have an overloaded socket right
where maybe your your TCP link is just
you know to not not bad enough to handle
the traffic you're trying to push
through it right and there's not much
you can do about that it's a socket
really you know the node really does go
unresponsive and the other end is just
going to disconnect yes you're like yin
will notice that but it's it's several
seconds before it actually disconnects
from you and it's configurable but I
mean it's a trade-off right if you said
the time I really short you're gonna get
these spurious kind of disconnects all
the time so you know if you're if you're
running a system that expects to respond
you know maybe it's powering a live web
site and you need to respond in a couple
hundred milliseconds before your users
start complaining that's no good yeah so
yeah so so any under overloaded or
unresponsive node can suspend all those
RPC communications if you're if you're
relying on this you know paradigm for
communicating with remote machines new
classroom by default that's the way the
RPC call mechanism works so you can
think about ways to work around it right
I mean you might think well I'll just
talk directly to my registered gen
server that's not racks and it's like
well okay fine now you know it's not all
communications it's just that server
hopefully that server is not a critical
functioning piece of your infrastructure
because it will get blocked if it's
trying to respond to calls so this can
be problematic but I think there are
good good work arounds so what we've
gone at Cloudant is a couple of things
if you remember the way that it worked
was Rex would spawn the local process
and the local process would communicate
back with Rex it doesn't have to be that
way the local process the local worker
can just as easily do a direct return
back to the original collar right so
that's step one that we did is we wrote
a new server that changed that one piece
of the paradigm so when you asked for
you know a little RBC thing to be
executed the worker would actually
receive the information when the process
was initialized about who the original
caller was on the remote node and would
take care of responding directly right
and so yeah if no day drops off the face
the earth all the workers that were
doing jobs on behalf of no day are still
going to get suspended trying to
communicate with it until it comes back
or not but those workers are cheap
we spawned it just to do this one job
and then die it's not as if there's lots
of other work waiting for this to be
done hey so you can get yourself into
trouble if it really is just never
coming back and you can completely
overloaded I mean there is a finite
number of processes you can spawn in
erlanger virtual machine but it's large
right I mean a millions not that big of
a deal so it buys you time and it
prevents 1 nodes failure from affecting
your entire cluster it's just that link
that that fails and the other thing we
can do if you really really really have
to send a message to a remote machine
from you know the one of your more
important gen server processes in your
application you can do an on walking
send right you can try to send the
message but you can add these options if
you dig into the to the airline
documentation you know the bang operator
is just Erlang send without any operator
options but you can add these options to
say if the nodes not connected don't try
to connect or if you're gonna suspend me
in order to send this message don't send
the message I don't really don't want
that right and then you have to check
the response of that function call and
it'll tell you you know no I couldn't do
it because you'd have to connect or no I
couldn't do it because i was going to
suspend you and then it's up to you to
decide what action to take maybe you
just drop it maybe it wasn't that
important after all you've got other
workers doing the same thing maybe you
spawn a local process to handle sending
the message on your behalf something
like that and so we encapsulated some of
that in a library that we use you know
in lieu of ever using Rex for anything
other than like debugging and you know
inspection of running machines in a
cluster so that was you know something
that we learned through trial and error
when we were first kind of doing serious
load testing of the software about a
couple of years ago and it proved to be
you know a fruitful endeavor so moving
on there's a lot you can do to monitor
the health and and start to change the
behavior of the distribution if you know
where to look there's in the Erlang
module there's a function that will tell
you exactly the ports that are being
used you know the sockets that are being
used to communicate with the other nodes
so you do the system info disk control
and it'll give you this you know this
key value list of nodes and ports that
use the communication and now you can go
wild right these are regular old
like Jeanty effectively ports right
there like jenti TCP sockets and you can
do all of the old inet communications
stuff with them right so you can inspect
you know their buffers you can expect
you know I've never tried it but you
could probably turn on Nagle's algorithm
return off maples out of the room if you
really wanted to you know to try to tune
the performance of an erlang distributed
early networking stack so once you have
these ports you can start to do lots of
stuff one of the things you can do is
get all kinds of detailed statistics on
the port and so you ask you know get
stat on the port and it'll give you all
these these counters and some
measurements of packet sizes and all
that sort of stuff and it even starts to
give you some undocumented things like
the number of pending octets waiting to
be sent out that socket right so this is
like this is clearly no danger zone if
that value is not zero it means
processes are being suspended trying to
talk over the socket right so like hook
up your mod or drink infrastructure to
watch the value of this thing and you
know start raising alarms if it stays
nonzero for any appreciable length of
time because it's not healthy and so we
saw this once and we saw it specifically
over one of these big clusters that
we had that was spanning a couple of
data centers that were nowhere near each
other very high bandwidth link like 20
gigabytes of connectivity between them
but we're talking about like 40
milliseconds RTT between the two data
centers so you know it's not the
situation that Erlang is necessarily
tuned for and you'd hope that Linux
would auto-tune its way out of this
right but for whatever reason it kind of
wasn't doing the best job it so maybe I
should take a step back because that
might not be obvious to anyone if you
have a fat long link like that right
like satellite connection or just you
know a very high bandwidth link between
two very data centers that are very far
apart you have a few things that you can
do to try to maximize the performance of
tcp / that kind of connection one of the
things you can do is if you control the
network you can bump up the size when an
ethernet frame right instead of the
default 1500 bytes you can try to turn
on jumbo frames and reduce the number of
packets that you need to transmit a
certain amount of data that could be
fruitful after that it's a lot of buffer
tuning
you know you try to figure out how you
know you try to keep the linked full the
whole time you try to be able to send as
much data as you can without having to
block and wait for an acknowledgement
you know from the other side and people
disagree about like the exact right way
to do it I think it's kind of a trial
and error Network my network sort of
thing linux tries to do a very good job
and generally does a pretty decent job
of tuning the buffers you know with a
little bit of setting for no parameters
and stuff to provide okay operation over
these these long links but in this
particular case we were finding that it
really just wasn't doing it so this is
this plot I was talking to you about the
send pen value over time and what we
were seeing was and we still don't
actually know exactly why but some
customer at one point had a job that
once a day on one of our clusters would
just try to kill it right we just you
know the amount of data transferred
would climb by 50 or 60 X all right it
was if they were trying to read every
document the database all in ones and
one of our nodes in the remote facility
where we had a minority of nodes for
disaster recovery in this outer
datacenter just couldn't quite keep up
and so it was trying to send packets
back you know to the primary data center
where the client was and failing and its
buffers were filling up one by one by
one and you'd go through sort of the TCP
stack in the kernel and then Erlang bm's
built-in buffers you know that decide
how long to wait before they start
suspending you and then you get up into
application logic that I was talking
about with Rexy so gradually you'd have
more and more processes backed up more
and more memory until the node would
just not do well and so you know we
tried to kind of do the the nice thing
and autotune it away as much as we could
and eventually we just said well you
know what we have the socket right we
can just start choosing these buffers
ourselves and so we just cranked up all
the buffers you know that were
associated with that link so I just kind
of looped over all of the the control
sockets there and started raising the
buffers until we were able to get to a
position where it seems like it was
healthy and it took you know it wasn't
crazy it was like 30 to megs something
like this which was larger than you
would calculate for that given latency
and bandwidth of the link but it wasn't
you know insane and the performance
difference was remarkable so I was
really happy about this
I was happy to see the distributed
Erlang which was really only ever you
know recommended to run over local area
networks could do a fine job of
utilizing these other networking
environments to build a cluster that you
know could survive the loss of a data
center without losing data yeah it was I
forget what the exact numbers where they
were like 800 megs per second and
megabits per second you know so close to
the line rate of sending our Lang terms
you know which point you might say well
why aren't you just using another
protocol to send the data but it was it
was a nice fit and it made sense for our
application of time and you know it was
it was good for us to see that it was
possible to take the airline virtual
machine and extract this kind of
performance out of it well 20 minutes
ever so that's most of what I have mesh
the exact opposite yes yes this was us
you know this was this was the choice of
providers and networks and clients
asking us to do things that you know we
hadn't done in the past and seeing it
assists you know proving that the system
could do it and so I mean I would still
of course not recommend running
distributed Erlang over the public
internet right this was the case where
the customer had you know multiple
datacenters and secure links between
them and everything right and it was
just a matter of whether we could make
TCP you know and distributed allowing
behave nicely over it but yeah of course
we still appeal to http anytime we're
talking about like public network or
really unreliable links this was a good
Network just yeah along that yeah okay
the number of nodes in the cluster we
did 100 node tests on ec2 the bill runs
up quick right I mean we made it work we
build a database we built some views we
did some search indexes and like got
results but we haven't been asked to do
more than 25 nodes in production at this
point so that's that's like the actual
production size that we've reached yeah
at some point at some point they would I
mean I can't remember if I tuned to the
net colonel took time at all at that
point with 100 node cluster but I think
our traffic was still dominating over
the the actual Erlang you know not so
much yeah I mean that's the thing like
we built these systems to more or less
not care if the link is up or not right
with things like Rexy so if it goes away
and comes back like we went on ec2 for
long enough that we had built things to
basically expect that right it's not
that important at that point for us yeah
i mean it's it's only important in the
sense that the longer you go where you
think a node is connected and it's not
you'll still continue to fill up those
buffers and you'll still continue to
have like processes trying to use them
whereas you know once it does that
disconnect everybody just dies right
there's like okay well that's up listen
you know so I wouldn't earn it 22 hi
mm-hmm the Erlang distribution from
scratch hmm that's a good question
that's actually above my pay grade
because I don't spend that much time in
the you know in the the net kernel stack
I would go find you know I I see a
couple guys from erickson around here
and ask them what you know where it
really falls down you mean not forcing
fully connected meshes but being able to
have finer grained control over the
topology and you know routing you know
tables so to speak for the nodes in your
cluster it'd be a nice feature for sure
yeah there's connect all false and
there's groups that you can build right
but it's I've never done it like I said
you know we're talking about 20-30 node
clusters at tops in our production
systems at this point when you can put a
lot of data on that at least for the
types of applications that we're
generally serving so it's been okay for
us
yeah yeah I changed in the usage I mean
yeah so I think i mighta had a backup
slide about sort of the verticals that
we're finding you know traction in and
in big data but yeah I mean as we've
learned more about the system as we've
become you know more adept at extracting
performance out of it I mean we've been
able to you know enter some markets that
previously couchdb would have kept us
out of right we've started to be able to
tackle lower latency requirements than
we had in the past but I think what we
find is you we we do have I think
there's a compelling value proposition
for having the people who build it run
it for you right I just like we
generally catch things before they
become big problems and can fix them
fast so i think we end up with a
situation where there are a lot of
companies who are kind of willing to pay
for that peace of mind right to go
beyond like having a sport contract and
to really just say okay look it's your
problem you know so we do a lot of that
where it's not just like the
multi-tenant sign up for yourself
customers but it becomes more of like
you know a database that really deserves
its own cluster but one they're still
just working with us to have us manage
that became unnecessary because Intel
got fast like yeah we moved off ec2 you
know we still have a presence on ec2 but
it's mostly for providing like Heroku
platform-as-a-service stuff but I mean
when you can get you know dual cpu
technocore machines like you don't need
to dedicate service receivers you just
kind of trust our line to scale right up
with you and for the most part it does
you know so when we're a bit label to
actually choose the right hardware for
the job that that makes you know like
improved our capabilities good fit
no I mean I'd like to see you know going
forward is definitely more applications
that leverage the replication in
interesting ways right because I think
it's you know if you have an application
that needs it like there are relatively
few data stores around that can you know
that have that kind of capability built
into the core so like the you know we do
have some like mobile applications for
instance where they'll have a local data
sore synchronizing with a cloud database
I think that's you know a place where
I'd like to see future development for
sure I mean I don't know ty get back you
know big data means so many things to so
many different people right for some
people it's all about just like let me
build the biggest to do cluster that I
can and crunch my logs as fast as
possible and you know make it one cup of
coffee instead of two but I think that
you know it also moves people to think
about creative ways that you might
reduce that data volume at the source
right do some processing do some
analysis you know before you have to
like a boil a notion to get it all into
one place and crunch it and never throw
anything away it's fundamentally I mean
sensor generated data has can be grow so
fast at some point that we can't
possibly keep building bigger and bigger
data centers to manage at all right we
have to do some are things with it at
least that's my opinion I don't know
maybe the NSA facility somewhere know
about
hmm there are a lot of similarities
aren't there that's an excellent
question we have a lot of bass unity and
say well I think react is a little less
opinionated about the data types that
you put in writing laws you to just
throw binaries in there I haven't kept a
pace with all of the developments and
sort of like to I secondary indexes and
different things that reacts been adding
but one of the things you can do in this
system is you know after the fact after
you've already got a database built you
can upload you know some JavaScript
MapReduce functions whatever to build
and then maintain a secondary index on
that data I think react you might
require like a dump load cycle to build
a new index on data that you already
have in do you have to like read it and
write it back with the headers yeah so
it's not that much different but it's
sort of you have to do that client side
instead of having the server build that
up for you from scratch and then you
know then there's replication I think
reacts started add thing well I
shouldn't speak for them I think I heard
that they were adding things in terms of
like multi-site replication which they
had an enterprise offering and maybe
there was more of that coming rioc
proper to deal with you know the case
where you've got the cluster on east
goes to one on the west coast you want
to keep them somehow synchronized gosh
she be as you know had primitives for
that for a long time but yeah I mean
this is a lot of a lot of common
philosophy there you know this that I
think that the real difference in many
ways is in the business model like
almost all of what we do is you know
like our company makes a living by
managing the software that we write you
know controlling the servers and
providing as a service to people
if you're in the single server I've no
doubt you know it's sort of like a
smaller piece of kit right when you get
like a bill when you're building big
distributed systems it's it's always a
challenge to make it simple
uh yeah we should get a Pearson half now
no it's a touch base you know so
catchphrase was a the mem base
technology which is sort of the memcache
deep protocol right and merging it with
the couches storage engine I think at
one point in time there was like I hope
that they would provide most of the
couch TV interface for that but as my as
I understand it these days it's much
more use your existing members memcache
d interface but now we've got couches
like bulletproof durable storage engine
underneath instead of sequel light right
and it's a storage engine that we've
tuned and written like explicitly to
handle our workloads I think that's kind
of their their pitch these days I think
it's definitely tuned for key value
operations you know micro second Layton
sees as fast as possible and it's a
clustering architecture that is really
geared towards kind of just working over
like local reliable networks like it's a
master slave I think replication
topology for for dealing with redundancy
so you don't have the kind of
flexibility to reconcile edit histories
if you're trying to do you know the sort
of thing we've got your East Coast and
West Coast data centers so I think
there's you know couchbase is a product
these days is much more about like
memcache d than it is about like the
couch TV restful interface</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>