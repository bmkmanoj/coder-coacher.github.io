<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Antidote - a Scalable and Consistent Transactional Data Store by Annette Bieniusa | Coder Coacher - Coaching Coders</title><meta content="Antidote - a Scalable and Consistent Transactional Data Store by Annette Bieniusa - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Antidote - a Scalable and Consistent Transactional Data Store by Annette Bieniusa</b></h2><h5 class="post__date">2016-09-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bGjdnBmX-oY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so welcome to my talk and it's an
antidote and I will introduce you to the
whole platform the idea of project and
what we are doing actually so the
project that just was mentioned is a
swing free project it is funded by the
European Union for three years it will
actually kind of run out in December
2016 and we have a lot of very very cool
partners that help us in doing work on
synchronization free programming you
might know some of these on the slides
so what's our vision what do we actually
try to achieve with our project the big
big goal is that we want to build highly
scalable highly available systems that
are correct so what do I mean with that
scalable we look for systems that run
geo scale Hedgehog cloud out on the
devices maybe part in data centers they
need to be highly available if you have
a customer that is building an app you
don't want the app to be unavailable
just because one node and some data
center somewhere just failed so I'm part
of the story is looking at how we can
build resilient fault tolerant and also
partition tolerant systems correct well
it's very important to actually being
able to implement the application logic
you want to sometimes when you kind of
have to first sketch the first prototype
you actually want to see is this now
doing what it should do I mean I have a
whole stack of you know network layer or
s layer database other things does this
really integrate into a correct system
and can I really actually implement what
I want to do and it gives a given system
so only reset agenda of our project we
were looking for use cases what do
people in a real world to do so we have
some partners that gave us some hints on
that we're building verification tools
for your program we're looking into
security issues
quite a few people are looking into
protocols how do we disseminate
information between different notes that
are involved in a large-scale system how
do we represent data and what we
discovered pretty quickly and it's
actually quite obvious is that a data
store the underlying data store plays an
essential role it gives you an API and
the iPad I has some semantics and this
is what you're stuck with right so if
you have a key value store and you want
to do strong transactions wrongly
consistent transactions and this simply
just won't work and you also I mean it's
it's kind of a contract between the app
developer and the data store seller or
the company or group that provides the
data store and you want to you need to
provide guarantees so if I want to build
an app I want to actually know what can
I do and I really want to know if it's
real I if I can really rely on that
thing and even the usability of the app
might depend on it so if I don't have a
fault or own backbone for my application
my application won't be fault tolerant
right and one of the difficult things in
this picture is consistency so there are
a lot of data stores out there they give
you a lot of different disciplines S&amp;amp;C
options you usually have to pay a price
between high availability and
consistency if you want strong
consistency you won't have high
availability in real world systems that
are partition tolerant so everybody who
is heard about a cap serum.this should
probably ring a bell so what we are with
like what what I'm focusing with my team
on is actually to see what kind of
consistency can we provide to
application developers while still
having a highly available system and
this is all what our data store antidote
is about so if you look at antidote it's
actually a cloud scale research database
we're working still on putting it to yet
but for now it's cloud scale and we have
charts we have parallelism within each
data center such that we can load
balanced load better and
get higher performance you can widely
replicate it also on Geographic leaders
and places availability is score to us
so neither weeds nor updates block if
you configure it in a certain way and
the still enables fast responses so if
latency is kind of small the data store
itself doesn't need to contact all the
other data stores all the other clusters
and you only have to talk to your to
your local or the closest data store and
we'll get answers immediately back and
we still try to make your the developers
live as simple as possible so yes we
want all the performance but we wanted
also to be usable and so what we
developed was actually not only this one
antidote instance I'm focusing on here
but it's actually more or less a
research platform so it allows you to
play around with different options we
implemented different protocols we have
different information dissemination and
what we as academics also want to have
some fair point of comparison so if if
I'm telling you I invented a very cool
new protocol to disseminate information
between different notes you're thinking
well but I have something that does that
for me and it implements a really cool
protocol so which what is actually
better this protocol adjusts or the
implementation right so if we all
implement them in the same platform we
can do a fair comparison
so back in 2013 we kind of started with
this and you know in academia a line is
known but only in very tiny places so
actually why are we using air long well
we have industry partners in our project
and it was actually bit surprising for
me so I have a background as a
functional programmer actually so I knew
a long and when we asked our industry
partners so what's the language of your
choice what should we look for they were
like use Erlang and we were like well
industry Java no but you know partners
like best show partners like our Lang
solution and so on they are a bit biased
or not on that question I guess and it's
actually what I what
back then is true we can't really were
able to develop fast first prototypes
and we were building on or we are
building on very strong packages
applications components that were
available to us something like react or
so if you've been to yesterday's
tutorial on react or it's really not
impossible to build within a certain
time frame yeah
quite impressive product on top of react
or but we our core we had some issues
finding students that were able and
willing to work with us but we had
applicants and you tell them you have to
program along and they all we don't know
any airline so we actually started with
a team with Noel Langer in and to pH how
many were for PhDs to postdocs
and nobody has ever opened an analog
book and I will show you what where we
are kind of now and I can really tell
that knowing about tools helps so if you
use stuff like dialyzer like unit tests
also like distributed tests and this
really can ease out some problems so and
we have two technologies in antidote
that help us achieve this goal of high
availability and the best consistency
you can get for this price
one is CEO duties which were already
mentioned in a previous talk this is
actually something that does industry
major a lot of companies use it in their
in their products I will just give you a
very short glance at your duties and
highly available transactions and I will
go into more detail regarding this so
let's start with CDG's actually who has
ever heard of CEO duties wonderful yeah
so the promotion is kind of working and
so you all know that siracusa actually
just abstract data types that you can
replicate where you can have
decentralized updates while not losing
information
and yeah so you can try to use them kind
of as normal data types and you get a
replication and consistency resolution
for free sorry
geez we have an antidote or actually the
following we have a counter you have an
or set grow only set or set us the
observed remove set we have multi value
and last for the wins register we have
maps and we have a replicated global
array which is actually just
representing a sequence some of these
are marked with star so again for our
first version we were building on the
react datatype library which is very
nice we deviated meanwhile react DT is
offering state-based nudity's where you
usually have to move the whole state
around which has some issues that's why
Russell was also working on a project on
a different part of course is working on
those big sets but we were you know
taking cities as they were meanwhile we
have replaced some of the
implementations from react DT with our
own operation based implementations
where it's not necessary to send a whole
state around but just send the operation
around which helps a bit and so we
seared it is you can already do quite
some nice stuff indeed when you're
having an abstract data type this helps
you to reason about yes state invariants
for a single object so you can you kind
of know what happens if you if you run
an update if you understand the
concurrency semantics of Co duties you
also know what happens in the concurrent
case so internal invariance is something
you can you can use when reasoning about
your application that's running on top
and you get the convergence guarantee
kind of for free the problem turns up
once you start dealing with multiple
objects so very often in your
application you want to relate objects
in some way you have some invariants
that involve more than one object and
there are different types of invariants
in quality invariants for example and
usually require different mechanisms to
make sure that they're running and one
of the mechanisms that we found were
transactions so a lot of people who
switch from old-school sequel databases
want to have some form of transactions
want to do multiple reads multiple
writes in some way grouped together and
so you're wondering how can we get
transactions back into a key value store
and of course while not losing
availability and the nice thing is
there's a concept called highly
available transactions hats for this
notion was coined by Peter Bailey's and
his co-workers and what you actually can
have in highly available systems and are
transactions with weaker isolation
guarantees so you don't get a strong
serviceability that you might want to
have but which might also be too much in
many cases and what you still get with
hats are monotonic reads so if you are
connected data center and you keep
reading you will always get newer
information as time flows
you're never going back in history
similarly for writes so your if you if
you do a writes in a certain order they
don't get reordered somehow by the
database writes follows read reads mean
if you update something based on some
previous knowledge so you retrieve some
information you then calculate your
update based on it and you write it back
this will make sure that this right
happens after this reads and you can
also have atomic writes in a sense
they're all nothing if you see some of
the updates that were atomically
committed you will see all of them right
and so if you look at the add a cap
spectrum on the level between CP and AP
the partition tolerance is something we
need to deal with I guess so
this is kind of a spectrum that that we
end up these highly value transactions
are still on the on a good line so you
get availability and we also have
invested or
investigated other options that are more
on the consistency level where you pay
the availability price but I will show
you in a second what options you have
there so what does this actually mean if
you have monotone agree it's monotonic
rights it's a nice concept but what does
it mean if I we want to program my my
application so going back to this idea
of invariance you get some weak
invariant preservation
meaning there are some invariants that
you can express some you won't be able
to express and so one thing you can
easily express a certain type of foreign
key constraint so assume you have kind
of a social network every user has a set
of friends and your friendship relation
is commutative so if I'm friends with
yours you also must be friends with me
meaning if you have an operation that
updates this friendship information it
needs to make sure that both friend sets
are updated at the same time so does
equality and constraint kind of it's a
kind of equality constraint that you can
do there and you will be guaranteed that
you only see either to users being
befriended or not being befriended now
so it does scenario where you see one
being befriended and the other one does
not happen if you are reading in a
consistent way and well it's it's still
a bit research that we're doing here
on really and I'll give a give give you
a good good rules on what what's
expressible and what not so one thing to
keep in mind is rights q rights use
something you will have in a system if
you want to get rid of rights q you lose
your availability what do you mean with
that so if you try to have an invariant
like this we have two integers X and Y
and you want to make sure that X is
always smaller than Y and you have a
transaction so up there a transactions
running that reads both variables C is
okay X is equal to 0 Y is equal to set
to 5 and I can set Y to to commit my
changes the environment will not be
violated so 0 is still smaller than 2
however if you can currently have
another transaction that also reads both
variables and
decides to set x-23 this transaction has
the impression okay X is 3 y is 5 and
variant is fine you synchronize the
state and what you end up with is the
situation where X is 3 and Y is 2 and
this clearly violates invariant so this
is the stuff that you cannot express in
this context so we were wondering what
kind of how can we make the transactions
available to the programmer and we have
actually two interfaces one is the
interactive transaction API that allows
you to start a transaction update
objects read objects in an arbitrary way
so both update and retake always a list
of objects for your convenience but you
can also just you know first read
something and update something read
something again and in the end when
you're finished you commit and that's it
right you only talk
we actually have similar to what we've
seen in a previous talk we have a
transaction coordinator running in our
database that takes care of the
individual transactions to be to be
executed and it then commits the things
and makes them available to other
readers in this final commit step we
have saw these transactions actually
return a transaction identifier that you
can use then in the later operation so
it's it's down here and you later
operations to identify reads and updates
belonging to a certain transaction and
it also has a parameter here called
snapshot time I will come to this in a
second so keep in mind we have snapshots
here and the other interface that we
have a static transactions disallow you
so this is a bit simpler and but has
more performance in a way because you
don't need to deal with this transaction
coordinator and you can simply update a
set of objects atomically and you can
read set of objects kind of atomically
from a snapshot right so these are our
static transactions here you don't have
the so these are these are kind
decouples right so between the read and
the update you might see updates from
from other sources happening it's not as
transactional as what you get with the
interactive transactions so why do we
have actually the snapshot in there and
this helps us to give you a property
that is actually giving a headache to
many people who start programming under
weekly eventually eventually consistent
systems so here is a probably well-known
example and we have the option so with
these we see snapshot we actually have
the option to either pass a vector clock
or an ignore if we do an ignore we
actually applied the transaction against
the current state of the database note
that we are contacting and you know it
kind of just executes with whatever is
in there if you provide a vector clock
you want to get more guarantees let me
that everything that happened before
this time in a vector clock is available
in the data center all right so you want
the past to be there when you execute
your transaction it might depend on the
past how can this happen well in a in
scenario where you don't have this
causal information the past you might
might have here for example to use us
and his Edison Bob and Alice has two
devices so these are kind of two clients
in a system usually and Ellis decides
that she doesn't no longer trust sorry
that she does no longer trusts Bob so
she sets so she removes Bob from the
access control list of her pictures
photo album and this information is
actually forwarded to the other devices
to the other replicas and a phone and
also on Bob's machine um and what then
happens is afterwards she no she post
some pic
from her phone and this information
again gets relayed to the other devices
or to her home thing but also to Bob and
something that happens there because Bob
suddenly sees the pictures because we're
some weird chance you know his device
was disconnected and reconnected got the
data in some order and the new photos
were delivered before bob was removed
from the access control list somehow all
right so um that's actually not what you
want to have and this disordering here
that you that you kind of have on so on
your replica down here that the photos
are available before the access
information is something you want to
prevent right so if the update here
happened actually kind of before
causally before this read update here
you want it to be on all replicas first
you and then V this is what caused the
dependency or call the consistency gives
you if a system is causally consistent
it will make sure that the order of
causally dependent operations is always
kept and so what theory kind of tells us
is that this causal consistency it's
actually the strongest consistency
guarantee that you can get while being
petition tolerant and available so it
actually doesn't slow slow you down if
you if you are connecting there is a
short small corner corner case if you
switch data centers but essentially you
can be highly available and have
causality okay so this is this is what
antidote allows you to do so you get all
these nice properties and as I said
there are several people working on
antidote and we also want to you know
not just have this one instance of a
database at the platform so we were
looking into different protocol
implementations how can we actually
implement a system like that and
literature is nice so one thing we found
in literature is a protocol called clock
si what it uses our loosely synchronized
physical clocks so we actually only need
synchronization of clocks within a data
center
not across data centers and it allows us
to version data so we actually have
versioned C or DTS you can read CD T's
from a from a snapshot and it allows you
to implement causally consistent
transactions
so using this this clock mechanism we
assign timestamps to operation to
operations and when a transaction starts
it kind of does it first read it gets a
rich timestamp which is kind of the
coordinators concurrent clock and this
clock has a dependency everything that
happened in earlier transactions if a
transaction commits we need to tell the
involved parties about a commit
timestamp here it's not my machine it's
the Beamer overheating
okay
yes yeah
very availability right yeah yeah so
maybe let me just well this is continue
let me just explain so we have if the
transaction coordinator initiates a
transaction and it assigns to the
transaction read time stamp that kind of
marks
that's the snapshot everything that
happened before this time it's the stuff
that the transaction would cause that
you depend on so that's the history
that's important and everything and when
it when the transaction commits all the
updates are miked was the same timestamp
again to make sure when you do atomic
reads you're all including by
construction this protocol also makes
sure that operations in a data center
within one data center are totally
ordered across data centers you don't
have this total orders which would
require coordination and would kill your
availability and so the nice thing is if
we have read-only transactions and
single no transactions so if the
transaction is only reading data from
from one shard we don't need any
coordination within the data center so
it's really just just one node that
keeps your data where where things kind
of happen so this is a short
illustration and if you have here just
one data center true eight data items
and down there you have a transaction
that adds so x and y assets sorry our
integers that adds one and it's two to
these integers when you start the
transaction the snapshot time that you
get as say three and when the
transaction commits all the different
charts that are involved get contacted
so the one that is keeping X gets
contacted that X is supposed to be
updated and the Shred gives back the its
current time say eight
similarly for the other one so it is has
the nine and the transaction then gets
as timestamp the maximum of all the
threads timestamps and this is actually
quite nice because you know introduced e
communication as fast with an ADC clock
skew is also well Drupal I guess better
at least and across the seas and we only
knew need one scalar value for data
center in order to identify a
transaction commit right so this kind of
helps and the information is then
propagated back to the to the charts and
they register the update under a certain
time stamp so this works pretty well if
you only have one data center but you
know geo scale one data center does not
work very well so we worked actually on
a Jew replicated version of this clock
as I have protocol which we call cure so
it cures your availability issues and we
still make sure that things within one
data center totally ordered but across
data centers we use a version vector
that has an entry per DC and but not per
petitions or really just purpose / DC
and updates are made only visible all
partitions synchronized their operations
to the other DC if there are no
operations happening we use heartbeat
messages to guarantee progress so this
is one of the protocol that we developed
kind of from newly and there are other
protocols in literature that allow you
to implement some sort of highly
available transactions cops eager and
gentle rain so if you look at their
papers we have arbitrary seer duties
like the city library I was showing you
and they only provide our work with last
writer winds C or D T's or another winds
conflict resolution strategies and they
give you weaker transaction so we have
readwrite interactive whereas the others
only have these static read static
writes in terms of blocks the price that
you have to pay is of course
they're so one thing we have to pay for
is metadata for example the gentle rain
protocol makes sure that there's really
only one timestamp across all data
centers used so it really needs little
data center it's actually really just
one scala value varies we have a vector
in the science of data centers others
have more issues so vectors in the size
of objects are probably not really
practically usable in large systems and
also you need to be careful on how you
make your updates visible such that you
really can read from snapshots so gentle
rain doesn't wear gentle rain praise the
place the Frye's so it needs to
communicate so the data centers need to
communicate with each other to find out
what they actually times and this can
turn out to be quite expensive
especially if data centers are not
reachable
so one thing we are working on now is
with our current setting you have full
replication so every data center keeps
all data objects and this is something
that is also quite expensive in some
settings so you might want to have you
know if you have for example social
network again if you have Asian users
they are very likely to have Asian
friends whose time line they want to
want to check but maybe not so many in
in the US right or for Europe so what we
want to do is actually configure our
system such that only items of interests
are replicated at certain places not
every data center needs to replicate
everything might also be that you
sometimes have smaller data centers that
just Candle can't handle a big amount of
no-load and the challenge here is
actually to have a genuine protocol
meaning that even node is not replicated
a certain data item it should not be
involved in synchronization messages
regarding this data hide
so every data center is only informed
about updates and also about metadata
changes that only involves the data item
it hosts and so it's it's quite
difficult it turns out to be well
non-trivial and to make sure that this
transitive nature of causality right so
I might have my own read or write might
depend on an operation that was issued
at a data center on an object that I'm
not replicating so does transitive
information and it's very hard to well
maintain when you don't want to spam all
the data centers with messages but we
have a protocol so one of our postdocs
Tyler is working on that and it should
be ready within a month and we'll see
how this thing goes okay how does
antidote not actually look like so the
architecture we have is that we have
clusters so within a you see we have
several nodes that that will come
through in a second clients are
connecting to disease we assume that
clients are not switching too often sure
if one data centers not reachable client
can contact another data center but most
of the times we expect sessions to to
run a bit longer within the data center
we have physical nodes and we distribute
our virtual nodes across these physical
nodes in a ring and ring comes of course
from the reoccurring and we have some
more processes that deal with
introducing messaging where we use some
types of interface and metadata handling
that we need to have for exchanging
timing information and within each of
these V nodes we actually run
transaction coordinators and we have
processes that don't read the data that
make it available and we have
transaction handling which helps to all
the data's run the clock as a protocol
within the data center
stems and so on but there's some
processes doing this we have an
in-memory materialization lock sorry an
in-memory materialization process that
is actually just a cache and we are
writing our information to disk such
that in a case of failure we can
actually restart we have the inter
deceive replication to propagate updates
to check all the dependencies and only
make up its visible once all the color
dependencies are fulfilled and and we
also support things like special kinds
of seer duties that have special
abilities like bounded counters and
commit talks so there are some more
processes on that so how do we actually
run regarding benchmarks so I said what
we wanted to do was kind of a fair
comparison between things so the
workload that we ran on some French
research cluster called with 5000 was
that we were dealing here with a single
item read or read update transactions
where a single key is read and updated
and we use here an aspirator wins
registers for our convenience and we try
to experiment with sets but as soon as
you start dealing with sets and you
start adding things the payload gets
bigger and bigger so in order to not
just compare bandwidth issues we use
your duties that are that have kind of a
fixed size here and we host 50,000
objects per node we have client machines
that connected to our data center nodes
and they are all well distributed they
are all having a concurrency in their in
your system and yeah so um one thing I
should like to say if you want to write
if you ever are in servation of running
things on grid 5000 be prepared it's
actually not so simple and it took us a
bit of a headache to set things up we
academic so
don't have so much money to burn on
Amazon that's why we need to go to
solutions like that and you know there's
so we were able to use all these 130
machines they fail quite often and you
only get time to work on this on a
weekend or in a night which is also very
interesting but we were able to show
that our system skates so what you see
here are kind of evaluation numbers for
this setup we are currently trying to do
also evaluation on Amazon so at some
point we would have something that you
are probably more come comfortable with
comparing but for a moment and so what
you see here are different workloads so
the thing in the brackets is the update
ratio to the to read ratio so these are
kind of well read workloads more or less
here you have 50 percent updates 50
percent writes what you can see here is
by using so this actually shows the
numbers here how many how many servers
we used how many physical nodes we used
with energy C so the first one is a set
up where you only have one DC with five
nodes when DC was 10 nodes when you see
was 25 nodes so adding more nodes per
disease gives us some scalability which
is nice so it actually just means we are
hurting the data better so we get more
options for concurrency in there and
what we did here for the for the last
two columns is not only having one DC
was 25 nodes but 2 &amp;amp; 3 DC's was 25 nodes
and distributing the work over all of
them and you see that scalability is
very nice for this workload right so we
of course worked with so it's different
workloads and there's also something to
be mentioned so it scales very nice with
a smallish amount of these
these and you get into some issues when
the number of DC's grows very large
because then the inter DC communication
well takes more time and sometimes our
queues get then filled up and deceased
just don't have enough power to works
for all of them so this is these are
always the Federation points right so
this is where you where you can go to
and I guess a set up with 3 DC's and 25
nodes
giving you a hundred thousand operations
per second it's not too bad for a
research prototype right so apart from
the from the things I've shown you we
have added some more things on antidote
for your convenience we have for example
a protocol buffer interface that you can
use we currently only have on the on the
client side an Allen implementation that
similar as what you get with react helps
helps you in running the operations on
local objects and in the background
everything is then transformed into
operation sequences that are then
forwarded but it's actually quite nice I
have students working on a Java
implementation next term and we probably
also will have a JavaScript
implementation I heard we have comatose
so we have kind of packets they for a
moment mainly surface namespaces and but
it allows you to add processes or
functions that are executed when you
change data that is in one of that is
there's no specific Packard's before and
after the update and there are some more
things of course in our agenda so
currently because of the dependency on
react or we require you to install a
ancient version of a line and but the
upgrade to a line 19 it's on its way
it's probably ready by next week we
managed also to a move to rebar three so
from rebar which took us a while but you
know you need we are no Allen experts
so we need some time to get used to all
these tools and yes I can agree with
Fred it would be sometimes nice to have
just in a webpage that helps you you
should do X Y and said so we sometimes
ruin the hard way but we are learners
right and we are also providing
different data storage backends like
leveldb maybe also other data stores or
data storages I have a student working
on access control so building the access
control into the data store instead of
having a separate system just to make
sure that the causality of data
information access control information
gets kept and we're also working on
tools that help you to choose the right
consistency level for your own
application so if you have your
application with with these these
transactions
hopefully in soonish we'll have we
actually have already prototype that
will tell you for these and these in
these operations you need stronger
consistency you need to move to
something like snapshot isolation or
even serializable transactions and for
others you don't need to pay the price
of that if you are interested in using
it and you know you think there is a
feature missing I really want to see X Y
or set feedback has welcome at any time
so we are always interested in getting
insight into real-world problems
sometimes at an academia bit separated
by that so if you have any questions
drop me an email speak to me Twitter me
I'm very fine with forwarding things and
answering of course and our code is
openly accessible you will find it on
github we started working on
documentation so does this work in
progress but it already gives you well
instructions on how to install things on
the API and on the soon free website
you'll find more information on the
overall projects also other stuff that
other people are working on there are
some more talks on soon free so Russell
will give a talk on big sets we have
Chris targeting about LASP of our
a different system that has been
developed I was in Zune free this is
actually our team so there are a lot of
people involved and so these four guys
over there have paid a very central part
but we had all kinds of people helping
us getting up to speed as I said if you
are interested in any form of
collaboration you have an application
you don't know how to adapt it to data
store semantics you have strange
inconsistencies that you're tired of
working at and you have a problem where
you think researchers might help contact
us and if you want to go into the
research track yourself we have open
positions and so it seems that there is
the successor project of swing free is
going to be funded we're still in the
process of doing the administration part
so we are looking actually for software
engineers postdocs PhDs who want to work
on just right consistency in databases
you will find the job advertisement for
inria there also some of the other
partners will have open positions
starting in January February so please
feel free to contact us and come back to
us thank you very much
who's got the burning question about all
this
number of data centers now number of
data centers so five data centers five
scanners no matter how many nodes you
have in a middle range on search we
don't have a search index yet so it's
hard right if there is an easy way to
put in some search index thing let us
know it might be interesting right</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>