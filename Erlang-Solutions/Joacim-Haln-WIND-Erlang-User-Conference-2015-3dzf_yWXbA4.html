<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Joacim Halén - WIND(...) - Erlang User Conference 2015 | Coder Coacher - Coaching Coders</title><meta content="Joacim Halén - WIND(...) - Erlang User Conference 2015 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Erlang-Solutions/">Erlang Solutions</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Joacim Halén - WIND(...) - Erlang User Conference 2015</b></h2><h5 class="post__date">2015-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3dzf_yWXbA4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm going to talk about wind it's a
research platform for doing research
about cloud oxidation and cloud
management for distributed heterogenous
clouds and I'm going in this
presentation to tell you what this is
this platform is what you can do and I
will also make some comments about how
we have implemented this thing this is a
like four year work and I haven't been
allowed to present this before this was
done studying 2009-2010 but for
cooperation we weren't allowed to talk
about this but now I'm allowed to do
that so first what was our initial
assumption here here Erickson hadn't
been into this area so we were 10 15
years behind in cloud so what we want to
do was something that we could be what
interesting for us as well and that was
to try to leverage from the network
architecture that we provide and then
having that how can we take that into
the cloud and make something worthwhile
for others to use so doing this we also
wanted to identify all the problems that
we see you can find verify possible
solutions and do that through prototypes
and what we came up with that would
something new what's the distributor
cloud and the digital cloud is when you
connect lots hundreds or thousands of
data centers together big and small to a
one big cloud where you can have really
distributed applications running in a
simple manner so first what is it
distributed hey Trina's Cloudland let me
talk about yeah it's a cloud where you
have big sentient centralized data
centers with tens of thousands of
servers and you have will have some
smaller data centers as well maybe a
couple of hundred data
servers or really tiny ones from one to
10 servers and so that is a huge broad
variety on the size of each data center
and also each data center my might run
different cloud stacks cloud operating
system like OpenStack like love
cloudstack or VMware or sound or it
might even not have a stack at all we
are controlling it directly machine
through a hypervisor so our system has
to support all this so we set up at a
number of requirements and design goals
for this and as we say it's a fully
heterogeneous environment that we have
to support we thought that all api's in
this should be restful and we should
build the system around a set of
separate services that you can use all
together all one just pick one of them
for for you for your solutions and let
application domains drive the
requirements on top of this cloud
platform and then our key word lead
words here will to simplify as much as
possible to be management organizational
clouds and at the same time automate as
much as possible make it easy for the
the tenants to use this so we came up
with a narka texture for this this is
slightly simplified you can see here we
have 22 sorry about that wrong bottom we
have the cloud orchestration domain and
that sits now on top of a cloud
execution domain in the execution domain
we can have different things as I said
OpenStack amazon web services we and
we're directly talking to live weird
whatever we find out in the future there
so we should be able to support that at
the same time we wanted to be able to
support whatever API that
convenient for our terms so that could
be like someone wants to have an open
stick use the OpenStack API we should be
able to support their or vc3 or our own
that we have developed in the API I mean
and now it looks like here that we have
sort of an API connector on top on one
southbound but in reality how you should
view this is that each individual
service that we have have these
northbound API and southbound API that
is fully plug-in supported okay what
kind of service do we have yeah we have
some fundamental service that you would
expect like storage compute Network
image handling this is what you
basically find in any data center and
cloud stack today we have of course for
identity that is important we have built
up around the kernel of other into more
internal things that is needed to manage
an administrative especially in a
dispute one all the data sensors have we
then also a set up what we call
intermediate services this is for
handing more automation of things like
automatic scaling of an application this
is by the ways more for an
infrastructure as a service is providing
infrastructure for a trial we have a
trigger service where you can store sort
of monitor things that you have all the
resources you have available on to that
and trigger if something goes wrong but
also for measuring from different
metrics how is it time for for this
application to scale out or scaling add
more resources or remove resources that
are not used and longer as this it's a
distributed cloud location will become
very important where do I have things
where do you have my resources and also
we have support for this
to build applications if you for
instance one such identification could
be a distributed video conference system
now you have clients coming in attaching
to the system and then the application
can ask the platform okay i have this
client here coming with these ideas
where which is the closest data center
to this club to this client or give me a
location with her I have presents with
within a certain latency from this
client its kind on top of that we have
some higher order services open
streisand service where you can specify
all the infrastructure that you would
like to have that could be order the
compute resources storage networking and
so on as one bundle and you create that
everything and get it get everything the
instead of specifying one by one give me
the this server this server connected to
this network here you can specify
everything as once the relay service
here is even higher thing here we can
have templates of different kind of
applications and a tenant can specify
that okay I need to have be able to
store this amount of video I need to
have a high availability I need to have
backup for for certain things and that
service will then translate that into
the necessary resources that you like
need to have for this particular kind of
application and it will also create how
this application should be able to scale
in order to fulfill all these SLA
parameters so it will do that
automatically the architecture is also
they fully plug-in based we there's a
small core or functionality the rest
almost everything else is plugins that
we can change I said it's a research
platform that means that we would like
to be
which change functionality in it in the
system in the platform we can do that
easily by just adding change different
plugins another good thing when we
design such these kind of system is that
you should be able for application
developers to use the same AP is that we
can do if we have the need for a certain
app API because it will simplify our
task it might be that same API might
simplify the task for others so that we
have also made almost all AP Isis in
such a way so i will now go into some of
these i will not talk about all of these
services but some of them so you can see
in in more detail so first compute and
network services some few things about
that ok compute service that will be the
same as you will see in any cloud that
you have ok you won't have some virtual
machine so I'm a container what it would
be but what we have done here is we have
extended with the concept of location
because that becomes very important now
you want to have this computer resource
somewhere in this disputed cloud so you
want to place it may be here in stock
called in USA or whatever it so and that
is not just your graphical location as
you see here example of region country
city or particular data center in a
particular wreck or hold on a particular
host it's more things than that so we
have extended data to specify sort of in
within a certain latency range from some
other entity and so on so it's more than
just geographical location ok now I'm
going to talk about the networking part
we have made a model of how to network
to networking and we want to make this
very simple
and you most of you know when we are we
are discussing network and you too what
what do you do you drew a line on a
whiteboard or something on a piece of
paper and say this is my network I have
then you attach things to this network
so yeah this is how we do things and
this is very simple and everybody
understands this so what to do next yeah
this is very simply we do it on it on a
whiteboard but if we add some context to
this it becomes way more complicated if
this is a lay to network and it ships
supposed to spread out all over the
world this is not we cannot draw a make
a physical connection between these
easily we have to do something else so
we have to add features to this sum for
to realize that that could be that we
create these local networks in the local
data centers connect between some
tunneling using between some gateways
between these data centers configure
everything then we can have a sort of a
layer to network here and these are
things that you don't want to bother
with if you're an application developer
you also say oh I have this simple
network connect my devices to that and
the platform should take care of all
this and make it happen if we put a
different context like this is a layer 3
network yeah that would be you would
start immediately think okay we will
have some firewalls here we would have
some maybe nothing routers and so on
some will be virtual somewhere others
will be physical we need to configure
them and so on so that but still you
would like to have all this as this very
simple model that we you can just to
tell this is what I want to have make it
happen so that's part of this network
service okay now we have it distributes
loud we have compute resources and
network and storage researcher in it
Conte
ordinary data center this is quite easy
because you can have them all in
separate and talked about them in the
digital cloud this becomes a problem
because you might have computational
resources in a data center if it's a
small one but you have no network into
to that place or you don't have it
enough storage and so on so how do we
deal with that in the normal data center
it's easy we can have a compute service
and a network service they can be
totally independent independent of each
other because we are basing assume the
basic assumption is that you will have
networking to all the service within the
data center you will it's sort of
connected in if in a full mesh and you
can have connectivity and there will be
enough band available of course that is
not entirely true but that's the model
we have and that means that we can
implement the compute service
independent of the Network Service and
that is normal what you do in modular
authorization of things in the digital
cloud in this case we cannot do that
because we have to try to allocate
resources as as one thing so we make
sure that we have this month one thing
the way we solve this here is that we
have separated resource allocation or
placement from the rest of the resource
management so we have a resource manager
that is owning all the resources so the
clouds are or the compute service
doesn't own the virtual machine
resources and similar with networking so
the compute service has to ask when
someone comes in and all okay I want to
create this virtual machine it will then
ask the resource manner okay can you
play a locate the virtual machine for me
for me and it will do that but it will
not create it it will not do anything at
that particular
data center it will ya say okay you will
have this resource similar to sort of it
another another you can think of is
malloc and free in a.c runtime
environment you request for a memory
block and then you get that from a lock
and then you do something with it here
okay of course you can do that with your
allocated memory machine but it's not
sort of created here it will be created
once it assigned this means that we can
have a resource manager now that can
allocate for an entire everything
networking storage compute at the same
time look at that problem at the same
time will come to back to that little
later buddy so we can say okay we can
grant this you can get all these 1000
machines and what this networking you
would like to have or you will not okay
the container service oh just a few
things about that it's something where
you can specify sort of all the
resources you like to have it fully my
parameterize you can have defined
temporal orderings so that you can say
oh I need to create these things first
once these servers are created you can
create this set of servers and so on you
can specify how they have the entire
system should scale in different
situations should we create more workers
here or remove workers and so on and we
can specify how all these should be put
together and a very very simple example
could be that you define the number of
servers and they can here we can see
that that you can specify some candle
location for them where it should be and
then you asked in network say okay
connect these machines to this
particular Network we can have arbitrary
complex system
vacation of this one okay let's press
we're scaling service this is service
scaly service its little bit more
advanced what you normally find in like
in Amazon where you only can specify
that you want to have a certain number
of more themes when certain conditions
it happened or remove someone here it's
based on a set of rules and tell and the
rule is a template for how things should
be done done when it you need to
increase the number of resources for a
particular application that could be
horizontally where you need to create
more VMs or you can increase the amount
of memory that a certain William have
gets or more network capacity and so on
you can specify so the limitation
minimal and maximal amount resource that
allowed for this application and the
application will have full control about
a deactivation of the rules you can use
API calls to the service and say okay
now I want to scale out now I want to
scale in I want to remove something here
you can also have it automatically done
by the platform specifying certain
metrics what should be measured should
it be the network capacity early the
bandwidth used or some some other things
so and we have them a set of use cases I
will just briefly go in so if you don't
specify any scaling no scaling will be
done at all you can a full application
control scaling so that the patient if
measures and decides went to scale or
you can have a fully automatic you tell
the platform exactly under which
conditions they should it it will
monitor and trigger once some limits or
it it will automatically start this
scaling or you can have some combination
in between
example on that this video can have a
combination is that take this i mention
this video conference system before it's
easy to measure when you need to have
more resources you have a set of of sort
of video transcoding servers and so on
and you can measure the load on this and
if certain load exceeds certain load you
will add more media servers or
transcoders and so on but it's very hard
to understand when can I remove as a
server because with today technology we
are encoding and so on you don't know
nothing is sent over a link if no one is
speaking but the session can be still
beyond just the tip the participant in
that conference is quiet so you minimize
bandwidth that and if we start gnashing
on the bandwidth we don't know if it
still exists or not the application
knows that this session is still going
on so it can decide are now they have
disconnected and then we can remove them
so it's easier to measures on sometimes
the platform can do the work sometimes
the application has to do to work and
actually in the system the automatic
part when a platform is doing it's using
exactly the same API as the application
will do to do to initiate this scaling
so scaling rule this is aviation it's
fully parameterize young again so you
can do some initial to the initial
things that you instances that you
create and say oh they should be had
this location so on you can specify to
close these bottles you can specify sort
of this is horizontal scaling this is
vertical scaling if you know this is
vertical scaling then use increase the
amount of resources of existing instance
have the other is when you create more
workers also
here you can specify very advanced on
how to specify triggers with metrics
which you want to know use and you can
specify if you don't measure just on the
work is you can measure on a lot of
different aspects of this here you can
notify here which instance that have you
created or removed that could be to a
load balancer and so on get notified
this I have created these new instances
but the most interesting is the template
describe what kind of instance should
you create and it's not only that you
can specify that you can create a new
server new VM you can hear set or
collections of different instances so
you can specify different kinds of
services or things that you would like
to have in one template and even more
you can specify that you use give the
template could be a scaling rule itself
and we have used this in order to have a
system where you can again I will come
back to this video conference system you
want to set up now to scale out with
setting up entire cluster of things that
should be placed in a certain data
center because you get getting a clients
connecting very close to that data
center but you want it to automatically
be able to scale out and in at that
point place let the platform do that
that the application decides where we
should place this a cluster so so then
it will create an with using parameter
say okay create this cluster at this
particular data center or like a
location and it will do do so and then
it at that site it will run
automatically and increase the number of
resources the more clients coming in
without the application didn't do
anything okay again we have a
distributed system
so we have to in all clouds you haven't
some kind of image service you need to
upload your your image that should be
run on in your version machines the code
they should run be running in a data
center this is easy you just have one
place to put it but in the distribute
cloud it's very important that the image
that you need is there at the time when
it's needed and an image can be quite
large so we have a distributed image
service here called daun where you can
upload an image to a central place and
that will then tell so so you uploaded
it will look at it it will put put chunk
it up in different pieces so it can see
oh this contains the same pieces and you
can see that other images share the same
part and once it has you have uploaded
and so to split this and said okay this
chunks is one part that is common to
wear a lot of images it can notify all
the instances you are all the data
centers that you have now this image is
available now you can have different
schemes for how who should click get
this everyone or just a few ones or you
can specify more intelligently okay now
it's going to come to this particular
data sent you work we are going to start
up a vm here you better get this but
having this once it's notify we can
start using a peer-to-peer technology to
distribute all these chunks and they
will only fetch the ones there they need
and they will also the different agents
out in the different data centers they
will also transfer transfer form these
into this what's needed at that
particular data center with the
particular cloudstack that that or any
dude how they want to have images will
do that automatically
so now go into the implementation of
this so we take we'll take a closer look
what's how have we implemented this for
the different services we look at that
it normally this is most of this it's
about some 100 plus K lines of code this
system about 75 to 80 percent is written
a line this doesn't start out where we
are using of course all this OTP stuff
that you have where the gem servers and
applications on but we didn't start out
like that from beginning it was just a
proof of concept prototype and as it
grow more and more more of the
components will be reused and so on and
then we started to adding using all this
OTP components and so on so of course
you can gain a lot from that and we have
gained a lot for for making all things
more usable for others but for myself I
normally when I should do something I do
it directly just using a line because
it's sort of i can focus just on the
problems at hand and the rest is more
that when you want to have it as a
production system now this is turning
into a more instead of a proof of
concept it's a experimental platform
that we are using that means that we
need to have it more sort of streamlined
oh so looking at a particular service
here i will t have a look at the
computer service but because it sort of
shares the common pattern and what you
see here is now sort of a map or what is
going on inside a line what process do
we have what modules exist in this and
so this is not a sort of an architecture
and you will not see and as supervisors
in this sport
so we see a lot of sort of things there
this one again so the wrong ones is
processes we have some modules that will
be think we have some plugins that we
can see so these are represented this of
course implemented airline modules now
if we start looking at this you oh it
doesn't you can can you see this that
it's actually shared around so i have to
point this these here this is sort of
the northbound interface looks really
see it on screen well anyway the yellow
they that what looks green yellow here
is third-party components we are using
yours as a web service here and what
happens when you get a request from the
from the tenant you also will see that
assign a new worker create a new worker
and then it will we have to for the
different a be northbound api's it will
coming on different porsche and it will
then invoke the right sort of interface
for that api and it will do a lot of
things it will do automatic decode it if
it's yes on XML and so on it will verify
that it's authorized this tunnel user
that senator sue grass is authorized to
do need to perform this operation this
is done in in common code so you don't
have to write that yourself it's already
done here then you have to translate
from the external API into a canonical
language a language we are using
internally in this this makes it may
seem way simpler if you have a canonical
language because we can have different
sort of front ends and internal it looks
completely vers everything every service
translated to it in internal language is
it has good and bad things with that it
sort of freezes what you can do in two
it makes it way more easy to adapt from
from the external point of view well
then of the translation we dispatch to
to the correct resource handler internal
thing once the result come back it with
post process that that could be that it
sort of in different cases in context
you will provide different results so in
sometimes when you create a virtual
machine you will get all the information
about the machine but if you are just
list all the machines you have you get
just very little information and it will
do that depending on the context you do
it will then also translate back from
the canonical format to whatever
external form at the 1080 expecting to
have it in so and for in the service
logic also is it it's a REST API so you
have the normal crowd functionally
create read update and delete and you
can now of course have other functions
in each API as well but it more or less
they should pause the contents in wind
we do parse the content in from
materialist canonical form into some
record format that we are using
internally perform the requested action
and then send back to result to to the
northbound API the next thing is now see
here that is the southbound we it will
be arranged so that it automatically
sort of invokes their correct plugin I
will come back to this so it will be
invoking the right plug in at fault for
this and it will then have to format the
request toward whoever the southbound
execution where it wants it it will get
back to results posted result updating
necessary object so and a lot of these
things are common form most of the
services so everything is wrapped up so
you can easily create a new service
for me it can take from anything from
just a day or two to number of weeks
depending on the service logic that you
need so it can go really fast to
implement new things and the actual sort
of services that you implement where
I've say I have said this with it's a
plug-in technology they don't actually
know what they're creating so if you run
and want to create with the compute
service it doesn't know how to create a
VM instead what is will you is we use
the plug-in manager to learn so it will
say okay use on whatever no we are going
to use on an old ok I want to use the
computer service at that node and I want
to create a server so find a plug-in
that is suitable for this node for this
the compute service the plug-in manager
will find the right plug in and invoke
that functionality and that will create
an and this here we posted the necessary
information needed to create this and I
haven't talked anything about identity
thing here the first line here is just
to create a local token this is how we
sort of the wind system authorizes
towards that particular data center so
it will get an authorization token so we
can verify this is we have allowed to do
this okay plugins it's a sort of simple
behavior it has two main callback
functions load and unload model we can
load them so the air or every function
that you define in the AP external API
has to take sort of a state variable and
the rest what is needed we can specify
that if the plugin should be loaded at
start when you start up the system or
when first use and then we have
stay here lay in the plug-in manager for
finding the right plug in 0 on it's
built on levels so we have a general
basic plug-in manager it's thread safe
by that I mean you everything of the
actual code where you run in this
function in the plug-in is not run in
the plugin manager it's run in the
thread that was our process that was
calling this so if you want more complex
ways to find the right plug in then we
see as i was using this wind plugin
manager that is based on location so
here we can specify sort of the node the
name function or a type for death note
if it have two nodes specify to now the
type it will find the best fit the
technology this we are using for
instance if you want to set up a tunnel
for networking connecting to lay two
networks it will find the technology
that not fits both data centers so we'll
try to find out the best fit for that so
we also have a driver management for
singleton plugins that could be a bit
like a driver for a database something
that you will not depend on the
particular data center using it's more
for the system itself so you can have
drivers okay another thing we wrote for
this was what we call a vet it's an
airline api to leave it it's a fully
one-to-one mapping at the time when i
did this it was 280 + functions so its
support level zero point nine point
three now it's i think is up one point
two point nine so it's lagging behind it
has full support for callback functions
leave it using a lot of callback
functions where you register function c
function
where that it will call it's based on a
another model call HPD so let's let's go
it's a a synchronous asynchronous port
drivers it's based on adding porch but
it simplifies bridging between for
libraries between adding and C or C to a
lie it's very simple to use its support
others like where there's you get with
support for callback functions and you
can actually write a user or register a
fun function anonymous function and be
to be called from the C code directly it
has a library of convenience macros for
writing this code so it makes it much
more simple than the existing ones and
it has support for logging so you can
turn on log on different sides to two to
two things so another comment how is for
testing this I base this the testing is
done in using a unit test test that I've
written in the in different level they
will test that level itself and all the
code in in in the modular doing but it
will also test all the levels that was
involved below and third it's a plug-in
based thing that is that if we are using
H debate client a that the system sees
and we have an HTTP client that emulates
entire openstack cloud of arbitrary size
and so in normal mode it will call
eyebrows to do the request and then it
will send out that however that sent out
but wind doesn't know in test mode it
will just plug in the emulator instead
so it looks like we have an entire huge
disability cloud a lot of data centers
and we and the rest is it doesn't even
know if it's running in test mode
or against the reals cloud so everything
up to this point is tested when you run
this so it's simple technique to test
everything another reflection the final
reflection before closing is the in in a
line it's very easy to to use as
processes or cheap it's very easy to
just use process everywhere in wind we
have carefully been thinking what
process are actually needed and most are
called handing a request will be run in
the process that was created or the
worker from yours and we will just too
small salty to the different other
internal functionality like the resource
manager or plugin manager very short
things to find out the rest thing or a
database and most of these calls are
very short as well so that means that we
have less serialize less things and
having that that that you don't use too
many process will also help you not find
in getting deadlocks so okay what are we
working with now yeah this was done
couple years ago but it's sort of half
active anyhow i'm currently working a
fully disability scheduler where you can
schedule it over entire this huge
disability cloud very efficient not
having it central database having
knowledge of all available resources
it's fully local working on the polish
description language where you can
describe what you would like to have and
all the SH that should be provided and
in an engine for that that can
automatically sort of do whatever what
you would like to have so this is sort
of a prologue approach server service
that just put on top of this and I'm
also working on what if I call ill he
flows it's completely new behavior it
will look like the model behaviors but
is for
having set of tasks where could we
create a network create servers and so
on and in two flows that could then will
be the entire flow without a bear
accepted or it will be rejected okay so
questions I've got a question about your
scheduler do have something for
dynamical load balancing for example in
vsphere varies a mechanism called Diez
dynamic resource sharing and also the
next question do you have some mechanism
for H a hey okay regarding scheduling
the entire scheduler is a plug-in so we
can put in different schedulers testing
that so we are testing different
schedules and we have research going on
where we testing different things the
one that we have been using this system
has been shown for number of times at
Mobile World Congress and we have used a
very simple scheduler but their work
going on for exactly this kind of scale
is that you were talking about regarding
a che we have another research project
working on exactly that it's done in
Montreal so we have a quite advanced
thing so that it's not built into wind
but yes we are working on such things
have you considered the pricing model of
different clouds on how to take that
into account in your provisioning it yes
we we did a project like one and a half
year ago to do that exactly exactly that
it's not built into this sort of but it
was based on the same sort of
assumptions and separate service for
actually doing this kind of thing but we
haven't in to get into the platform but
yes we have done that is it open source
or a planning to visit yes
Michael and wanna call it you will get
this question and I didn't mention it no
it's not open source yet I hope to be
able to do that I haven't even been
allowed to talk about this before so
this is that I can talk about it it is
quite nice some of this like every 10
and the lower things that sort of it
support things that is no business logic
I hope to be able to release as open
source daughters yeah that might be the
way to get them really accepted is that
you use put it out to it start up and
then the air zone will buy back that
startup so I cannot promise anything
because it's not up to me some part I
really really try to get an open source
so that's that is my goal because I want
to bring back to the community okay
thank you for the questions and thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>