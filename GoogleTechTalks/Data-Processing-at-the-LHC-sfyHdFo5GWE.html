<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Processing at the LHC | Coder Coacher - Coaching Coders</title><meta content="Data Processing at the LHC - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Data Processing at the LHC</b></h2><h5 class="post__date">2011-02-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sfyHdFo5GWE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon everybody and thank you
very much this invitation for
representative from CERN to come in to
explain to you a little bit about what
Suns been doing in sort of data
processing for our accelerator which
recently started up the LHC what I'll
trying to explain to you little today
just very briefly put into a minute or
so what is sir
what's it doing what are we there what
does the IT which department do and
we've got quite a few representatives
here today amongst you and sort of what
are the needs in terms of the computing
for LHC our latest accelerator and then
what mod will be put in place for that
over the last decade in fact and sort of
the you know how does that what's the
relevance of it how it going to change
in the future what's the importance for
the computer centers typically CERN and
the other major computer centers around
the world that are participating just
computing model and how it'll impact as
well we think and be relevant for other
scientific disciplines not just okay I
should say of course that it's me
standing here today but this work
represents the work of many people at
CERN many other collaborating institutes
are involved in the LHC program and also
a whole load of people and organizations
around the world have helped develop
grid technology which is a basis of a
lot of what we're doing here so just a
word on CERN first of all and for more
than 50 years now put together in 1954
originally 12 member states in Europe
we've now grown to 20 member states and
they you see the list here the 20 member
states plus we have a whole host of
other candidate countries to join sir
and observers and at this moment in time
certainly is moving from being a
predominantly European center to evolve
to become a global center for physics we
have we have 2300 staffing total the IT
department represents about 230 people
so properly attend to that and there's a
load of other students personnel are in
there as well as associate fellows and
so on and we have a user community lots
more than Google I understand that more
than ten thousand physicists around the
world basically these are the people
that want to get hold of the data that's
coming off our accelerators an annual
budget is
a billion Swiss francs so today's money
that's basically a billion dollars as
well and that's paid by the Member
States we're not a profit-making
organization we rely on money given by
the Member States to complete our
physics program just to be clear we're
doing physics research we don't generate
electricity we don't build bombs we
don't have anything like that we're
looking into physics and the basics of
physics okay so here's our use of
population as well you see the member
states and so on which represent two
thirds of the population people actually
get hold of that data and using it but
you see many other countries and there
as well something about countries all in
all together they're actually making use
of the results of CERN not many people
know it but certainly is a lot of one of
the origins around the the open access
movement as well so one of the founding
partners there so all the scientific
publications come out of CERN are open
as well inside the IT department well
like many other places were structured
in that way as well but the IT
department provides the computing
resources for all the requirements
inside the laboratory and so if we go
around I mean the first most important
one is the security so part of the art
from the management to the
administration have security their
security across all IT style security
not just for the department for around
accelerators for all access to the sites
and so on things like that I'm glad to
say that we have a nice and Antonio
where are you guys stand up say hello
the next area is user and documentation
services now you might say what's what
CERN got to do with this but in fact of
course we've been around for more than
50 years we have one the largest physics
libraries online libraries that exist
and so we also distribute a lot of
software which is used for storing
repositories so the c.d.s system in in
vino which is picked up by lots of the
libraries around the world and used to
manage their own repositories this is
open source software distributed on a
distributed by CERN maintained by CERN
and also all to do in this group not to
do with the audiovisual system so going
around here we see you have your video
conferencing rooms we do some the same
situation at CERN but of course we have
to do it not only within CERN but across
with all our partner organization
around the world as well and so we have
Carmen and Pedro from this group if we
look onto the experiment support now
this is where we're getting close to the
real physics right so for the
accelerators owns the accelerators we
put in place the accelerator program and
then there are individual collaborations
groups of universities and research
interviews come together and they build
the individual experiments which are
accepted to run on those different
accelerators and experiment support
group their job is to make sure that
those experiments can optimize exploit
the IT infrastructure at CERN to
actually take their date and so we have
a fernando and edward move from that
group please guys next is database
services as you can imagine we are off
we we need databases to manage and
organize the science data the physics
data the metadata that's generated or
produced with that as well but all the
other data that you can imagine it in an
organization of 2,200 people pay slips
or documents all that sort of stuff or
the administration stuff all that's done
as well and we have Katharina and
Giacomo from the database group then we
have platform and engineering services
so what this area is actually worrying
about is essentially the massive batch
system that we have a certain where we
do a lot of processing on-site so here
we have a massive cluster of Linux
machines we have LX batch which
everybody knows and loves which is the
interactive system for for where we
process a lot of the physics data and
they do various services like that as
well and we have where are we yes please
set up that's my mistake I put your name
there on them excuse me over I apologize
and then the communication systems well
CERN is a big campus we have many
hundreds of buildings on the CERN site
is several hectares in size and of
course inside that we are wide local
area network we have a fixed network
which is high speed network to nearly
all of the buildings in CERN we have the
Wi-Fi network as well and we also have
the high speed interconnects to many
other physics centers around the world
and Virginie and Jose are here from the
communication
and then in computer facilities as I
said we have rather major computer
center at CERN and computing facilities
group are essentially operating that
computer center for us so they worry
about ensuring that we have enough
electricity inside that that the cooling
works they also worry about the
procurement of all the generations of
equipment that go for that Center as
well and we'll come back to them later I
guess them but first we'll go on to work
let's go into grid technology we have a
distributed system there for processing
of the LHC data a grid system and CERN
we've contributed greatly to an open
software stack which is basically called
the middleware for the grid system this
has been produced through a series of
years different projects run by CERN
also in collaboration with the European
forum the European Commission also with
our colleagues in the US and and Asia
and we have Lawrence and Ricardo from
from the group here today and the other
area of course is the data storage
services so why do we have disks like
you can imagine where we store the data
coming off the off the LHC experiment
but we have a hierarchical management
system where we have tape robots where
they store a vast quantity of this data
as well and we have Lucas and honor from
this group ok and then we finally come
back to the computer center as I
mentioned and we have Eric
so with that we sort of done a tour so
you know everybody here you can ask them
any questions you like we have no
proprietary secrets so you can so I'm
moving on to the accelerator itself so
of course Google promoted thank you very
much as well promoted by the start-up at
the LHC and of course during 10000 2010
was our first major data taking year
here you see an aerial shot of the
Geneva region you see superimposed on
top of that the ring for the LHC which
is 27 kilometers in circumference
next to that you see they're smaller in
the SPS earlier generation of
accelerator which is used as basically
is an injector in towards the LHC on the
far right you can see the the airport
and so on and at the bottom below that
the concentric rings there is the major
part of the CERN
site as I said it's a big campus and you
understand why we have a rather large
local area network and of course over
this side as well we have the other part
of some FSM site where a lot of fixed
targets experiments take place as well
so it's a very large thing and it
spreads the Swiss Swiss French border
which basically goes through here
somewhere número mountains Lac Lamont
things like that so on that accelerator
so it's not actually on the surface you
can't see a painted line when you go
around Geneva it's not really painted on
the roads it's below yeah up to a
hundred metres at some points and it was
actually developed well dug out in 85
was finished 85 86 with a previous
accelerator called the lap we
subsequently dismantled that accelerator
in its lifetime and then built the LHC
in the same tunnel what you see there
would look a bit more in more detail
because even the magnets inside the side
the accelerator and there are four major
experiments which are the smaller ones
perform major experiments sitting on the
LHC today and so here we see them
distributed around the accelerator so
these are the points essentially where
the particle the particles collide beams
particles come together and collide and
that's where we click the data from
these four major experiments they are in
sense looking for many of the same
characteristics physics characteristics
but don't don't underestimate they also
in competition they're all trying to
invite to do the best things they can
they can competing amongst themselves to
see who can come up with these
discoveries as quickly as possible
shot of the of the accelerator itself I
sort of cut away looking at the LHC
accelerator we see some of the blue
parts of the magnets and of course the
magnets go all the way around at 27
kilometers 'm so there's something like
just over a thousand eleven hundred
magnets each one several meters long if
you look inside the magnet you can see
there are two beam beam pipes here so of
course we have two two rings circulating
in opposite directions and then they are
brought to collide inside each of the
four experiments that you saw in the
previous shot so they're accelerating in
that way ten thousand times of the per
second or more they're going all the way
around this whole mass is cooled for
super conductivity reasons in order to
have high enough magnets which we need
to concentrate bend such high-power
beams we need super conductivity I ate a
bit the ability to pass electricity
without any resistance and to do that
the whole mass of this some forty
thousand tons I think of something like
that are cooled using liquid helium
there's a hundred and thirty tons of
liquid helium inside this machine that's
why it takes us several months to call
the Machine down to work on it
we up again so when you go back to your
flat tonight you've been out for dinner
you had a few daiquiris and go back home
and you go in there anything I just have
a nightcap before I go to bed and you go
to the fridge and you open the fridge
it'll take you three months to get that
beer out right so just remember that
think about us when you go home tonight
and then you close the door another
three months yeah it takes that much
time because of the mass of a material
we have to cool down and it takes us we
do it by sectors we do the running
cyclist machining is very long
previously we would run for several
months with the old accelerator run for
several months close down in the winter
when we do maintenance also when the
electricity it's more expensive but now
we have to run for longer periods
because we lose too much time
calling machine down
here's just a little simulation of what
it looks like inside of one of those
four experiments sitting on the LXE LHC
accelerator so what it is basically it's
a big onion ring and each layer that
onion ring is looking at a different
physics characteristic and each one uses
different materials of Technology more
to do that but all of them are producing
data so what you see here is there is an
experiment let's say it's a + or
something like that and here you see the
beam line coming through it and you see
the two beam particles beams of
particles colliding and out of that
produce other types of particles which
are detected inside that machine inside
that detector so it's pretty much like a
very powerful digital camera these are
happening these collisions are happening
at forty forty thousand times per second
and the amount of electronics in here
means that we have a hundred and fifty
million sensors basically billion pixel
camera that is taking forty thousand
photographs per second and there are
four experiments right so you start to
get an idea of how much data we're
talking about which we have to deal with
in real time as it comes off these
machines
in fact this dedicated hardware inside
each of those experimental very very
close to them underground which we call
the triggering and tomato acquisition
system so the special electronics takes
it off the experiment depending on which
experiment you might have one two or
three levels one two or three levels of
trigger eating and we take it off then
at this point we have some dedicated
hardware specialized custom built
hardware sitting in racks down in the
boot and it's going through it can't see
a hole events you can't see a hole
collision you can only see a little part
of it because the amount of time it has
to make a decision is very very short so
you can see a little bit of it like a
keyhole it has to guess if that's the
right sort of get you want to keep or
whether or not we're going to throw it
away it's a bit like spam you throw most
of it away because it's or spam we want
to keep a little bit of it right so
we'll just make a selection decision yes
or no keep it goes through to the higher
levels of the of the triggering and each
time we have a little bit more time
still course up second we have a little
bit more time and we see a bigger part
of the picture until when we get to the
level three trigger where it's the first
time we can run an algorithm which can
look at all the data from an individual
collision okay and with that we then
take it from there and after that it's
the first time it goes out to permanent
storage so we haven't stored it
permanent to disk until that point you
look at this graphic you can see the
accelerator Underground you see the for
experiments this is where some of the
electronics is sitting the level three
three ger is sitting at the top of the
event filter farm and we have a fiber
optic cables taking the data up to here
and then taking the selected event back
to the CERN computer sector of
centralizing there and there from CERN
we can we can record it and we can also
spread it around the world for further
processing and access to those many
thousands of physicists the want to get
hold of that data as quickly as possible
so to do that I said we use a disputed
processing model because the amounts of
CPU we need for this is essentially we
need 100,000 CPUs every day to process
the data coming off the experience
together they're recording about 15
petabytes of data per year off these
four experiments recorded not the data
acquired what's recorded
and so we have a distributed model was
built or designed basically a decade
again now where we have a certain center
known as tears the Europe so this is
where data comes off into the computer
center community center is tier 0 and we
have 11 specialized centers around the
world now each one of these centers have
agreed to participate in the LHC program
they dedicated sufficient resources
significant resources to work on this
and what we have see basic you see
there's one in each of the major major
centers around the world we have grid
carries in cars or in Germany Rutherford
in the UK Sarah Nick F in Amsterdam
Netherlands Fermilab in the u.s. triumph
in Canada we have Bologna in Italy Leon
in France
Barcelona in Spain Taipei Taiwan
Brookhaven in the US and the Nordic
dated facility where the Nordic
countries come together and produced a
distributed Center across the northern
countries so these are what we call 9001
centers and we have high-speed dedicated
links network links to the Bose 11
centers and so the data taken at CERN is
also sent out to those eleven major Tier
one centers they have specific functions
they will also cause store and avec op e
at least one of the copy of the data so
now we have the replication of the data
in the system so if we lose one copy at
CERN or somewhere else we've got a copy
difficult processing capacity as well to
actually churn through all that data and
analyze and they also provide another
function which is to service many other
regional centers which we call tier two
tier threes where the end physicists are
often sitting so that's the guy sitting
here once you get a hold of the data one
analyze byte special services to these
guys and so that is the whole model we
have and there's of course this there's
there's a whole operational mode that
goes with this service level agreement
about what's provided at each level and
so on and it's completely monitored to
follow up all the while 365 days a year
altogether there's something like a
hundred and fifty regular centers that
are working and using a processing and
punch
to this data information system below
that of course the networking is it's
very very important to us much work has
been put in place to work with the the
academic networks around the world so
that the national regional and national
research and education networks in many
of the European countries contribute to
helping make this system work so we have
high-speed links between CERN and the
major Tier one centers we also have back
up links as well and this proved very
important because in 2009 when we were
doing basically dry runs for the LHC
some of the cables were actually cut but
the system didn't stop because of the
the alternative routing we could
continue to process the data at the same
rate so we continued continued
operations that's very important we have
to be very reliable in that way because
we can't switch off the accelerator we
have to keep going so as you've seen
it's been running basically for about a
year now 2007 was a year when we took
our first major data taking year here
the results shown sort of event displays
from the four major experiments they're
all very happy that gathering data far
better rates than they expected with far
better quality than they originally
foreseen at this stage it's a it's a
fantastic machine so it takes time to
tune it get used to it understand it of
course and the detectors themselves so
they're all very pleased at the rate of
which they're doing and the data they
are visualizing here has been processed
and delivered to them via the grid
system that we talked about so if we
look first here during 2010 restored as
I said 15 petabytes of data onto a
permanent storage from the LHC
experiments and we are storing it up to
right at 220 terabytes per day during
the heavy iron run which happened in
sort of November early December at tier
zero so in and out of the computer
center two gigabits gigabytes per second
with Peaks up to 11.5 and the average
out is about six which picks up to 25 so
it's a very continuous system it's not
just short bursts it's continuous
operation of the scheme
in terms of this processing that goes on
inside the inside the grid system and
said the physics model were fortune the
sense that we're talking about
individual events and they can be
processed independent of other events
there's no relationship between the
events themselves so you can separate it
out split it out into individual events
package those up into groups and send
them off to different computer centers
to be processed and that's what happens
on the grid system at the moment of
course they need access to do that and
so the moment we're running roughly
about 1 million executions jobs
executions per day so across those many
sites they're running about 1 million
jobs per day which represents more than
a hundred powers of CPU days per day so
you can estimate the amount of compute
capacity that we're actually using and
any one time this is a number of
concurrent physicists users online so
we've got ran about 2,000 people at any
one time concurrently accessing the data
and the data is distributed around the
world within a matter of hours using
this tiered scheme that I've just
explained
oh now you've got a sort of rough idea
of what it is but let me say how we
actually got there right we have just
gonna happen bought this system before
she couldn't see it in the catalog
anywhere so we had to build the damn
thing and the way it actually really is
it's a distributed computing
infrastructure provide the production
analysis so it's not only analyzing the
data but all the simulation that goes on
beforehand for the for the experiments
and it's really three major things a
collaboration there's the collaboration
of all these resources being pledged by
the different Institute's around the
world they have a long-term commitment
to the LHC they've signed memorandum
understanding and they're giving
resources to Beijing there's no money
changing hands right it's just people
say we want to work on LHC we were
willing to contribute this amount of
computing resources and effort to make
the thing work service I try to explain
to you a little bit the operation of the
system and all these different services
in terms of data and the CPU and the
networking that offers a real service
which runs three or 65 days a year 24
hours a day and there are service level
agreements which are defined we can't
find anybody because they didn't give us
and you can pay them any money but what
we do is pretty much like many other
places now is we we monitor it
continuously and every month we know the
performance of each individual sites and
we compare it to what they pledged and
so on and we compare that and it's
either name and shame or name and glory
right depending where they are on the
list and then of course the
implementation I said it's basically a
distributed grid system a collaborative
grid system contributing resources and
that's a that's a technology we have
today right but as I said it's basically
InDesign ten years ago and he's been put
in place and tuned made sure it could
operate at this rate of course we want
to see now things have moved on how can
we evolve that technology so that we
could change implementation but still
maintain the service and the
collaboration we don't want to lose
so if you look at that structure in
terms of collaboration framework or
service and distributed computing this
is basically what it represents today at
the bottom you've got the the iron
hardware so on down the shooting model
we have a public the various services
that are here and then the collaboration
itself at the top so what we really want
to be able to do somehow is replace this
tissues you concert' involve it's
disputed computing services into
something else moving towards a cloud
style model as well a lot of the cloud
computing the one sees now has a lot of
similarities and origins in what's
happening with the grid computing so we
can see some of evolution we want to go
in that way but still maintain some of
the advantages that we've got with the
grid system as we move towards that sort
of model so how do we have to evolve
this data processing well basically the
thing is to make it sustainable if you
think about the lifetime of the LHC it's
been preparation for 20 years before it
saw data and it will run for 15 20 years
as well so it's a hell of a long time
right and we need the commitment and
make it sustainable there will be many
generations of programmers
operators electricians physicists
working on this machine and there's
going to be a massive amount of data it
could be accumulated in over the
lifetime of a machine as well so we have
to be able to make it operate in a sort
of any efficient manner
that can be sustained so there many data
issues around that the whole data
management and access question how can
we sure guarantee access to this data on
a global scale and over over decades how
can we make it reliable and fault
tolerant systems when essentially the
hardware we're building on is not
reliable and fault tolerant many of the
centers that we work with including
certain we buy commodity systems we buy
low-cost standard not gold-plated discs
or CPUs right and they have failure
rates often higher than the than the
than the manufacturers publish right we
have to make a reliable system still
depending on those unreliable elements
below the data preservation has said
making it for decades and ensuring is
open access and adapt to the changing
technologies when this stuff was
designed it was single CPU servers
people have
now you had are many-many core CPUs and
who knows it might be the predominant
technology might be GPUs for the whole
commodity world in within the next
couple years can our computing model
adapt to that can our algorithms exploit
GPU architectures as efficiently as the
as the singles processor technology it
was designed for this so we have to
adapt to those similar thing with global
file systems all the physicists want to
be able to see all of the data from
their experiment as one big global UNIX
file system that's what they want to see
not that easy to provide in the June mid
2000 years and that sort of scale but
now if you look around they're different
services which are making it possible
things have evolved and if you think
about it logically well things like
Dropbox it's an a global file system
could we not have something like Dropbox
for LHC could that not work across all
the different centers just trying to
think of different ways of doing it its
virtualization many of the centers i've
talked about are already virtualizing
their computing is one of the groups in
IT department it's specifically put in
place virtual machines with with Linux
flavors with Windows servers and so on
but can we can we expand that can we get
it so that the physicists algorithms are
wrapped up as virtually mesee images
which we can then distribute around
these different sensors and allow people
to change and reallocate the resources
more dynamically than they can do today
course the networking infrastructure
when this system was simulated and put
together the SIP networks were not
particularly reliable and if people
assumed in the simulation that they
would fail it turns out there's been
such progress in the networking layer
that it's the most reliable part of the
whole system right so that sort turns on
the head the way you have to think about
it right do we still need this tier
system it's a peer model instead be more
appropriate now as these technology has
changed has the structure and the
networking capabilities have changed as
well so all these things we have to try
and take into account
but I mean we have to remember this
people involved in this as well right
and there's the the point is when we
talk to the physicists then they say
don't touch it it works leave it it runs
leave it right but of course they want
incremental improvements but if we're
going to keep keep the thing going and
maintain it for the decade and so we've
got to see how we can do it and one
important point is the data the biggest
area where things are going to change
are in the data management and data
storage areas but of course data is
century it's the family crown jewels of
of the LHC right think about it somebody
who started working at CERN in the 90s
or 80s when when things like Atlas and
the LHC were being designed they've
probably working 15 20 years at CERN if
they stay for the lifetime of the
experiment expect the whole career on
one experiment so imagine you spent your
whole career on the experiment and at
the end of it somebody said okay we're
not going to look after your date
anywhere else we're gonna give to
somebody else in the cloud right how are
you gonna feel all right you know you've
been spending so much so much of your
life trying to get this data are you
prepared to give it to somebody else at
the moment to manage it right so we have
to get over that issue we've got to
assure them that it is possible that we
can guarantee access to the data and
that it will be around and they will
have access to it okay it's the whole
question of if we go into this cloud
model public versus private cloud you
could argue that what we have today is a
private cloud it's open to the people
inside the LHC community they can share
the data between and they can share the
resources and so on but we're not paying
anybody money we're not paying a
separate an outside company to run that
however in the future you might want to
do that so that we could consider that a
public a public cloud system each one
these situations are a couple of things
we have to think about from a legal
point of view who has access to it
depending where your data is residing it
falls under different national
jurisdictions and different governments
have different rules about what they can
do with that data do have the right to
access it or not that we have to take
into account does it match with the
collaborations being put in place for
physics and so on but in terms of the
public clouds what are the terms and
conditions of the service when you read
the contracting you sign it and you read
the fine print does it actually offer
you all the guarantees you need what's
the service level quality did you
actually go to get for your money
so so the couple of references anything
to the bottom with good good report but
those are things are going on in the
thinking of people at moment trying to
understand how can we profit from these
advances in this technology but still
ensure that we can serve a community in
the best possible way and of course if
if anyway if you do go in this direction
we start using public cloud nobody can
use a single cloud are they like taking
a single disk supplier or a dingle
single CPU supplier you're not going to
take just one because if they collapse
what happens to your data Chloe you're
gonna have several of them if you have
several them what standards exist can
you move your data between one cloud
system in the other if one pump company
goes bust can you take it out and move
it somewhere else
can you play them off and Milwaukee
forces and things like that all those
questions come into play and also what
about identity where at the moment we
have a federated identity system across
all of those centers using x.509
certificates you're given a certificate
by your host organization and with that
you can go and access the resources on
any one of those machines case if we
have different cloud systems around can
you use your same identity across all
these things is it going to be very
important for us now I'm gonna scoot
logging into the different things right
so those are some of the things we were
working on and trying to understand how
we can address at the moment if we come
back now collapse go from the global
view and come back and look just at CERN
the computer center this is sort of I
should point out that the CERN computer
center while relatively big by research
standards it's for 20% of the computing
resources which are consumed by the LHC
much of it is actually from the other
sectors the Tia Sioux centers which
which appeared on that graph the tier
two and Tier three as a small guys at
the end they're actually providing the
majority of the computing resources
sixty percent on average comes from
those small tier two and Tier three
centers and CERN itself has said only
provides twenty percent of it we can't
possibly put it all in one Center
originally because of its a funding
issues
Stern is funded by all his member states
the member states would rather than give
us extra money to build a massive
computer center at CERN they'd rather
keep the money and invest it locally in
their own countries which is easier for
politicians to do than to
some other countries invest it locally
and then use it as well for other
sciences not just at this instrument and
of course so we have to build that into
the model that we use as well if you
look at it now these numbers about three
months old continuously growing we have
about eight thousand service of thirteen
thousand processors in the computer
center there with about fifty thousand
cause the spec inst don't worry about
that that's a certain specific high G
physics specific measurement and it's
about something over fifty thousand
discs there as well
and you see that you know there's
continuous generations different
generations of the hardware we don't buy
all at once of course like any other
sensor we're buying different
generations trying to get the best bang
for your for your buck so to speak in
terms of CPU capacity and so on a
network so of course there many
generations of the hardware in the
computers into one time we have to
manage those points as well
continuous breakdowns of the equipment
disks power supplies the most most
weakest parts we have there in terms of
failures and of course we have it said
tapes we have about one hundred and
fifty hundred and sixty tape drives with
the robots from from IBM and stun
storage tech there's about forty five
petabytes of tape storage there at the
moment and then you see this or the
networking capacity that we have inside
the computer center this computer center
is quite all building it's not a nice
new life it's lovely building today it
dates back from 1972 and expect five
megawatts in power it has a power to
utility efficiency of about one point
seven so we're using almost as much
again in terms of cooling and for
ancillary uses Rudd and actually the
sort of CPU cycles you get out if we if
we want to improve those numbers we need
to have a new set up for the computer
center and CERN itself at the moment is
looking to have a remote to do zero
finding is some other country in Europe
who's willing to provide a new computer
center for us where we can put extra
capacity in as well but if you look at
computer center itself this is roughly
so the numbers game of what's happening
a moment from the experiments looking
about seven hundred megabytes for a
second it comes into the into the caster
system where we have the disk disk pools
which basically the front cache to the
tape systems as well there the clever
Scott caster software decides when it
goes
- the tape service here we're talking
about less than half a second access
time with the tape service we're talking
minutes before going in and getting out
of data but it isn't just archiving the
tape system we're actually using really
as an active an active part of the
hierarchical management system in the
future no we'll see how things evolve
there as well right and then it from the
analysis the algorithms are being run on
the sort of the on the service and so on
then accessing the data here as well and
of course it's read backwards and
forwards here and then of course pumping
data out to the Tier one centers that
those sort of rates
terms of the the sort of data management
layers well from the end physicist user
he's got sir sort of popular package
they use is called route sort of an
object-oriented framework for analyzing
the physics date I think plugging their
algorithms do cuts fix and so on see
graphs and so on after that access the
file systems through a network client
down to the network server the namespace
so it looks like a big file system if
you want to the disks from the disk
staging area and the scheduler down to
the tape systems and offline storage
that's basically what the whole model
looks like inside the cyber tier 0 when
specifically in the data management area
is that likely to to remove well this
can we take an approach where we move
more of the scale of performance and
reliability issues to a software layer
we're counting very much on the hardware
at the moment can we change that between
make it more and more service oriented
software layer in that sense as well
looking the question of scalability we
need to store at least 20 petabytes per
year the physicists are running faster
than we are
the accelerator is getting better every
day the energy the luminosity is going
up the detectors are getting better
they're understanding better their
machines tuning them better
understanding how they can work better
so that's increasingly 20 petabytes per
year he's looking conservative now and
so of course over the lifetime of LHC
we're talking exabyte scale there it see
of course we need fine grain access
control to all the different files where
we store multiple events and so on and
also we need multiple authentication
systems not just x.509 certificates many
different ways of integrating different
systems there as well they're counting
and journaling that's very important we
have munch and auditing systems at
moment but we can't really roll them
back and restart as we would like to
global access accessibility basically
all data has to be accessible to anybody
in the world that has an internet
connection the bottom line and what we
have to the other dude sounds funny to
you guys I'm sure and then the
manageability sort of how can we limit
the interventions are needed in the
computer centers how can we do it the
moment their engineer is going in on
call 24 hours a day and making sure that
the hardware is performing correctly
right can we change that so it can limit
it just to office hours and only using
technician level
people to do it by making better use of
the software layers or so on what can we
do in terms of the multiple various
levels of service quality at the moment
there's one level of quality that's it
for the LHC
right can we have multiple levels can we
vary them according to what the needs
are what we can afford as well then I
said as well we're sort of trying to do
it all this with low cost hardware right
commodity low cost hardware where prices
that is the gaming factor and of course
the power efficiency question everybody
has to be efficient with energy now what
can we do communal me turn off disks and
manage them and one into the power for
them we've met many core CPUs can we
shut down some of those some of the
course what can we do I want to say as
well just finish off the guy did it
everything we're doing here is not just
specifically for physics right as I said
it's a general-purpose grid system
evolving into a cloud system the data
management issues are very very
important and this European Commission
organized high-level group of export
experts put out a vision for a
scientific data where basically don't
see the infrastructure anymore we just
see the data and that's valid for many
of the scientific disciplines at CERN we
have strong relationships with people
working in health systems life sciences
drug discovery is as a big user these
good information great grid systems as
well
similarly in in the environmental
sciences and Earth Sciences processing
the satellite data as well a lot of that
is done with you know staff here at CERN
as well and so all of these Sciences are
looking at this this question as well
and thinking that there might be some
common elements in what we can do here
and so there's a basic model that people
are evolving towards you know this is
very very you could any software system
you can break into three layers we can
order that right but if if you if you
look at what a sixer versus people are
trying to provide here those are sort of
things which you can map on to what we
have today what we want and of course
the whole question of continuous trust
across all those layers in the data
curation as well just finish there say
thank you very much for attention
one from Grants you can't contact me
yesterday from Seattle you read your
2007 paper on data corruption at the LHC
he's wondering if you have any follow up
data corruption we have anybody familiar
with that paper otherwise I will stab at
it
okay well look I'll get at it while
you're doing it so corruption can happen
that many layers in this system first of
all from the experiments of cells in the
in the in the electronics level and
that's why we run a lot of a lot of time
is spent with cosmic cosmic rays when
the clarity isn't running they're
actually testing the electronics in the
in the detectors to see if they are
actually producing the data that's
expected seeing if different parts of
the texture are dead or alive because
when you assemble this thing there are
zillions of miles of cables in then of
course there's a trigger and data
acquisition system itself which may have
our errors it may have software failures
it may have problems in the algorithm it
may be throwing away good data which is
one the issues for the experiment and
one the reasons for example that the
start when they start up they run with
rather loose triggering algorithms
because they want to make sure that
they're not losing anything important
does the the distributed system itself
now in terms of that yes we can we could
potentially lose data but as I explained
we have replication of data that's very
important use a lot of the Oracle
facilities there as well to actually
distribute and have replicas across CERN
and the Tier one systems and in terms of
the the jobs which are running and
processing that data if any one of those
jobs fails they are monitored but the
experiments note about it and the job is
either restarted automatically by our
resource broker or the the experiment
framework software framing itself will
capture that and reschedule it
themselves so those those systems some
fault tolerance built into all those
systems but we can't guarantee
in the diagrams you mentioned people
getting access to the data at the tier
two or whichever tier it was of the
users or the other end of physicists so
physicists asked for dated they asked
for a time slice do they ask for
specific interactions of particles they
asked for specific experiments and do
they get the date or do they simply
request that their algorithm is run
against that data somewhere else
a synchronously and then have the
results sent back to them okay so
there's there's different aspects to
this one is first of all the
collaborations have a body of users the
physicists right they're all working
contributing to that experiment and they
have all have access to that data but
that doesn't mean people from one
experiment and access to another
experiment remember I said they're in
competition so there they're working to
do something together so they have
access control over who can see the
latest data coming out of the
experiments in terms of requesting that
data typically what happens when they're
reprocessing the data they are
generating summary information from the
raw data so they're reconstructing the
tracks the different characteristics of
the of the event different particles and
so on and that data is also written to
storage that's why the amount of data
that she gets written to the storage is
more than the data that we initially
recorded on the disk and so on so when
they're doing their analysis through
things like raw route and so on they're
not actually looking at the raw data
initially they're looking at a basically
a statistical basis they're looking at
the summary information doing fits and
cuts on that summary information yes
sometimes they're looking over a period
and for each run does specific what they
call calibration data from each of the
experiments and that records basically
lots of characteristics about what did
the detector look like when we took that
data how mean the channels were on/off
with the different subsystems working
what's the timing situation across the
whole detector and things like that so
they bring all those pieces together and
if the if the characteristics are
correct along with the beam
characteristics are correct for the type
of phenomenon they're trying to look for
they will bring narrow down the amount
of day
they want and do their analysis based on
that principle so question is so
basically when you're talking about with
layers of processing information
so you mentioned this where's the first
layer responsible for removing like
extra information which is not
interesting okay so for example like if
ten years at an hour physics were like a
new model or something going to look for
new like particles of new properties so
you do need like change this hardware
layer completely are you getting a great
program it's two ways you can look at
them right the triggering layers all the
way from trigger trigger level one two
and one and two
at that point we haven't permanently
recorded the data then is lost forever
yeah that's gone it's gone into into
mass storage we have access to that data
so under certain conditions calibration
data and so on for the for the
experiments what could happen you're
quite right is that later people come
back and say okay I want to look at that
data again because I want to apply some
other algorithm to it they can do that
but you can't change the conditions
under which that data was recorded it's
done that's passed here you can and this
is done dynamically and it's part of the
the online system for each the
experiments uses some CPU time resources
to do this it can monitor in real time
what's happening in the experiment and
make some adjustments to how the
experiment is configured okay but it's
it's short term that they're doing now
in the lifetime of the LHC what's
expected is that we're running basically
that's 7tv now so to beam 23.5 TeV it's
designed to go up to 70 up to 14 TV so
two beams of 7 TeV but there are then
plans as well so up Gradius it LHC to
the super finished so this will imply
improvements in the technology around
them some of the magnets the
radiofrequency cavities and so on and
the kicker systems to improve
the quantity of data and the quality
data we're getting out of the out of the
accelerator consequently it will be
upgrades to some of the experiments for
example CMS and Atlas I'm looking at
upgrading some of their systems as well
at that point we can start looking at
other characteristics which at the
moment may be out of the visible range
of what we consider with the LHC today
ok another question that I was sent was
around network availability stats for
long-haul links the ones that certain
operates particularly um they were
wondering where things are failing if
that isn't 100% okay I'm going
I guess a lot of the faith fairly so
guys with diggers and ships anchors and
earthquakes
yes there's one particular case when
there was the earthquake in Asia which
cut off some links a lot of the links
not just for CERN but around Taiwan but
I said because we have the redundancy
links we can most cases and carry on
processes so you said these experiments
are in competition but you're sort of
distributing the data over all these
different sites to encrypt the data when
you send it out or do you do something
to hide it or no
we have the access control mechanism so
this x.509 based certificate system so
to access the data you need those right
if you ask the physicists they will tell
you we don't need to encrypt any isn't
encrypted because nobody can understand
the damn stuff anyway so it's true
unless you're really working on one of
the experiments being able to interpret
it interpret that data is very difficult
there aren't that many manuals about
about the physics data itself here how
do you handle contention for resources
yeah that's a very good question as I
said this you have to remember the CERN
is responsible for building accelerators
and running the accelerator facilities
right the experiments don't belong to us
built by separate separate groups the
first of all can beat on paper to be
given the right to actually build an
experiment and have access to the to the
accelerator and when it's built they
have a collaboration they have an
agreement between themselves what
they're going to do now many of these
centers that appear in the community in
the grids are serving multiple
experiments you know researchers just
like to like to hedge their bets as well
right so you're going to work on more
than one experiment and they say
themselves how they want to allocate
their resources we do not impose it on
anybody what we do from the cern point
of view is that we monitor it in the
sense of we monitor what they pledge to
verify that what the center's are
providing are adequate for the
experiment so who find that for one
particular experiment that isn't enough
resources we raise alarm nails we can't
force him in to give it but we raise
alarm bells and typically the rigor the
centers themselves will will do
something to try and accommodate that
but the relative priorities between them
is is a is a decision for each
resource centers themselves right
typically they can do it different ways
it most remote got a vac system running
so they will give different cues for
different priorities to different
experiments</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>