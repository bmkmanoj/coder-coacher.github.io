<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>UpSizeR: Synthetically Scaling Up a Given Database State | Coder Coacher - Coaching Coders</title><meta content="UpSizeR: Synthetically Scaling Up a Given Database State - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>UpSizeR: Synthetically Scaling Up a Given Database State</b></h2><h5 class="post__date">2010-05-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sA69JTQpCBo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">dresses performing modeling database
transactions wireless protocols traffic
equilibrium cache misses in performance
modeling and his other interests include
distributed protocols and the
correctness proof is currently at
sabbatical initially and it was honored
to have been one of these former
students in my other buddies thinking
okay thanks man um so uh thank you all
for coming and I everybody's busy so
appreciate the time you guys think
spending time here listen to me talk
about this stuff um my objective here is
that we have this idea of sizer and we
are trying to figure out how to pick it
forward and I'm here to find out whether
these ideas are relevant to Google in
particular whether it's relevant to the
table so I'm hoping to get some feedback
to hear your views I understand that
maybe you're gonna ask some questions
that you might want to go off record or
you need to run off so I'm going to go
through the slides in fast forward fast
first of all it's like I won't fast
forward the presentation file which is
possible with this slide so it will if
they're stuff in the slides that you
want that you see that you want
elaborate upon here stop means you can
do that and in general just just feel
free to do to certain if you think you
want me to elaborate on something so
upsizer do two problems one of them I
call that it is at scale social networks
attribute value correlation problem so
upsizer is a tool that looks at these
two problems so um
we were thinking about big data when we
came up with this idea we're thinking
about big data and big data could mean
astronomy data or genomic data but no
for this talk we focus on stuff like
internet services he communes or social
networks so we started we we were
thinking about how somebody would come
up with application that and they hope
will go viral how they are going to
prepare themselves both for that invent
how to make sure that the system is
beautiful and in order to do that they
they need a bigger deficit then what
have so within the bigger data set as it
is infested and powerful auto cleaned up
CDC is not it we think that for
different applications you want to be
able to scale up the data in a more
application specific way rather than
take as general things money geez so
this is the data set scaling problem you
give me a set of relational tables and a
scale factor s and I will generate a
syntactic version that is s times this
size of course the key thing is that the
scaled up version must be similar to the
old version so our idea for forests or
for for similarity is based on some set
of queries that we use you upside the
user has
because we didn't think that statistical
measure similarity was this statistical
distribution or graph properties is it's
actually what you mean so although we we
started off no as the name suggests we
were thinking about up sizing that means
s bigger than one after we did is and
when you're not talking to people we
discover that actually there's a lot of
interest in the other two cases s equal
to 1 Y is equal to 1 well if you are a
vendor and you go and your customer is
the bank the bank for legal proprietary
reasons with not let you take the data
out of the bank so the bank may want to
just you may want to just run up sizer
on the bank's data to make a synthetic
coffee which means you can then pick out
of the company that's smaller than one
where this guy this is the case that I
imagine would be relevant to you guys
you guys don't need upsizing you guys
need some sizing I imagine you had a
some application prototype application
that you want to test and you want to
test it on the production data set in 1x
so that's where s smaller than one might
come in
so if you trace back upsizer
this idea of benchmarks so the current
the current approach to creating data
based benchmarks is principally CPC and
you trace it back to this constant
benchmark and the approaches entirely
synthetic rather than what up sided us
remember the upsizer takes an empirical
data set and skills it up whereas
wisconsin benchmark at EDC benchmarks
they take they is bothered all the
tables are completely syntactic and they
have three reasons for doing that
Phillip them are not relevant to case
they were worried that the data set that
they had were too small they worried
about not being able to scale the
queries but those are not relevant over
to the current situation where the data
sets are large and the queries we
already have a set of queries a month so
the only thing is the good the only
issue that is how to scale the synthetic
data set so for for the purpose of this
presentation I'm going to have a mini
Flickr database with four tables imagine
you guys understand what the secure data
set is you have users uploading
photographs and other users commenting
on those photographs or typing those
photographs and if you look at the
foreign key if you look at a
relationship between the foreign keys
and the primary keys in this data set
they form a schema graph so here's where
I'm trying to map this into what you
guys are doing so if I'm thinking if
it's ago if we are think it is a big
table then
where is the schema graph so would it be
accurate to say that a big table will to
represent the Flickr dataset then big
table is a join of these four tables
by default it has one key but there are
other ways to have potti practically
speaking with the keys on it but those
are six built on top of the table but in
some sense into those things would
normally be probably separate tables and
it would be practically speaking
enforcing referential integrity and
being in the series if they have any
illnesses in their bid good at a user
library
so if if someone well if the big table
whatever application is you you you
float into the big table this something
like a joining of multiple relational
tables then I think the idea is that I'm
going to describe would be applicable so
let us see how it goes here there are
use there are patterns of use of the
table that people really put into one
big table table seem to be otherwise
they were pretty to different ones
because they can have dynamically their
columns columns of form they put him
yeah okay so we have a bunch of
assumptions I gonna skip this because
I'm going to relax and later anyway so
let me just go so upsizer uses three
ideas the first idea is a degree so
degree is a very simple idea so a user
here X uploads Y here at Lowe's for
photographs then the degree of y so the
photo tables is for and user X makes two
comments then the degree of X to the
comment table is two and Y makes one
comments a degree of y to comment this
one we call it a degree because you'll
see that this is actually a bipartite
graph each five other graph is induced
by an age in a schema schema graph now
if how is this so in the case of big
table you don't have multiple tables you
have just one table but you still have
the same kind of thing embedded within
the victim to make
pencil which we use it I mean people
using both ways sometimes here in one
table sometimes really so so I'm so it
could be that in your table there are
two columns and in a particular column
because it's non-normalized in a
particular column southern attribute
value is repeated
maybe it's the dates that's repeated
maybe is a location that's repeated
whatever and that will define that
degree between the two columns so the
degree from a key value to a table now
will translate into a degree get into
two columns and then the next thing that
join that upsides are users is this
joint degree distribution and this is
also this would be something that would
also appear did you take a join of open
so the idea with joint degree
distribution is that if I'm a user I
upload certain number of photographs and
it makes it a number of comments and the
number of photographs I upload is
correlated with the number of comments
that I make because people like to
comment on their own photographs so
similarly two columns in a non
normalized table might have a
correlation of this time the third idea
that upsizer users is co-clustering so
if you imagine the users to be divided
into gardeners and painters and whatever
so this this custom
when when we thought about this we were
thinking right click girl whatever where
you have a social network and and it's
very natural in a social network that
you have a cluster of gardeners and
custo of painters whatever and you want
a cluster of flowers in a cluster of
cars but I mentioned this this would
also this would happen also for very
traditional databases like you have
clusters like companies in Singapore and
companies in Malaysia and whatever and
then you have clusters that products
being cars or whatever and then there
were certain correlation among among
this such attributes so again even if
you don't have multiple tables you have
just one big table I think you also have
this co-clustering distribution involved
so as long as you have these three
things I think a sizeable might be
relevant to two if you want to upsize
the table so how does outsider work
using the trigger example
so remember Flickr has this schema graph
induced by the foreign key to primary
key relationship and this schema graph
if you have just one table this schema
graph would be not in not a graph among
tables but a graph among the columns
certain column to point to certain other
columns and then my assumption would be
there is a cyclic they can stop the
columns and by sorting the columns that
tells you which columns do syntactically
scale access so the salt salting of the
columns the figure which column to scale
up in this case I scale up the user
table first
so scaling out the user ticker is really
easy because the user table has no
foreign key here
so that's that simple so you give me a
data set with a million users you want
me to double its size Ron I just
generate two million fake identifiers so
that's really easy so once I once I
scale up in in the in this context now
what have to do is fill up the rest of
the user table the rest of the user the
non-key attributes in a table giving
assigning values to the non key
attribute I'm going to call it content
generation so for content generation
that one you have to mine the data to
figure out the distributions for example
the user name or user location here you
just figure out what percentage of
people
it's called John what percentage of
people is called married and then just
assign accordingly so let's for now just
focus on the key attributes rather than
the non-key attributes so that means I
leave out the content generation for the
time being
so now user table is one side once I
have assigned one some generated a two
million user IDs the rest again just
fill in the values and that would never
settle the user table now the next table
would be the photo table so here I use
the degree distribution I know the
degree distribution from user to photo
so I just use a degree distribution to
assign a degree to this user once I
assign a degree to each user I know how
many could tacos to generate per user so
I already had two million versions in
practice you know degrees
solution will of course be a decision
just a large execution with our pliers
and who knows what are legit very code
to do a random number generation with
those weights just to get for each new
case of how many photos for a sample
each user whatever you upside the user
is comfortable with what we can do is
just take a histogram from the so you
give me an empirical data set I think
that histogram I use that is so
contagious and using it as a probability
distribution if the user outside the
user is not comfortable with that
upside the user can supply me the
distribution and our use that
distribution okay so so why might a wide
what might the upside it okay we will
get to that I'll come back to this okay
so now I have two million users twos
minutes in target user to these two
million Internet users
I have a signing degree for each one
under user according to the third degree
according to the TV distribution so for
each user if this user has degree four i
just generate four photographs for that
user
it is degree if this user has degree 11
and genuine 11 photographs for the user
so that was set over the photo table so
for the photo table I'm using the degree
distribution for the comment and tag
tables I will need co-clustering because
the comment and tech tables have to form
teams and these two foreign keys are
correlated so how how do I do that so
now I first
so they remember that the user table has
already been generated so the users have
been generated the photographs have been
generated now I need to generate a
comment so I need to find out the degree
from the user to the comment I need to
find out the degree from the photo to
the comments so I will generate those
from the degree distribution and what I
need to do is be careful about a
correlation I have a joint degree
distribution
I've already generated the degree from
user to photo so when I generate the
degree from user to comment I have to
take the marginal I have to take a
conditional probability based on
whatever has been generated people the
degree from user to comment and degree
from photo to comment this one I have to
use the focus term distribution so now
what I do is focus the first so use an
eco clustering algorithm upsizer
this is an orthogonal problem to upsizer
once at that co-clustering distribution
to generate a comment i first generate
an identifier assigned at a time prior
to a cluster to a co cluster according
to the distribution
they're just assigns to a podcaster it
does not assign to a user and it does
not assign to a photograph so I just do
the obvious thing is it by declaring so
let's say I assign this photograph to
the co class gardener and flower then if
now I have to figure out which gardener
it is so I will sign according to you
know if this gardener uploads a hundred
photograph or know if this gutter
comments on hundred photograph in that
garden or comments on five photograph
then
with higher probability I'm going to
assign the comment to the one to
generate a hundred comments similarly
for the fourth time so that we generate
the T values and then I worry about the
non key values and that would give me
the top oh and then I keep on doing
every time I assign a comment and
generate a comment and assign it I would
recommend degree until I'm done and
similarly for a tactic so how did the
new users get put into clusters original
according to the the new users put into
the concept the new users as assign to
the pastors at what point did I assign
the new sister process
I think that can happen as soon as the
users identify it identified yeah do
remember this balance sorry okay yeah I
I don't remember at which point the
users were assigned to the classes I
have to look at it okay so so I so this
is the upsizer algorithm if if you only
had one big table then I assume first of
all you have to order the columns you
have to solve the columns and then you
generate the columns according to the
order and you would simply easily use
degree
joint degree distributions and
co-clustering to generate this okay our
first assumption is primary key is
doesn't go to an attribute that one is
not relevant to if you are using just
one big table a table has at most two
foreign keys if you have so in the it in
the case where you have just one big
table then the issue here is the cluster
ring is it two columns or is it more
than two columns it's the code is the
correlation among more than two columns
if the correlation is much more than two
columns it is have to find a if it is
among three columns then just find a
three dimensional co-clustering
algorithm doesn't matter to us certain
relationships among the columns might be
cyclic this one we don't handle yet so
what kind of a relationship
it's like manager relationships or
employee
so X may actually be a manager of Y and
x and y are both employees so in some
sense in the database sense this is a
cyclic that's a cyclic schema of course
the instance is not cyclic nobody is a
manager himself so if you have just one
big table and for a particular column
you have that kind of inter the kind of
relationships within the column right
now I'm not sure how to handle it
simply to figure out new user generated
such that what kind of capacity he would
need you could do by something with your
existing distribution that how many of
the different level yeah once you can
figure out the multiplicity is that you
are here and like it's like the manager
has how many under him yeah you can
month you can figure their out the issue
here is what is the order for generating
the couple's so we have to figure it out
I think if the security is we did one
column I think me is doable is we just
have to sit down and think through it
but the security is among three columns
then that might be painful I don't know
whether whether that's meaningful and it
is meaningful how we get it done so
degree distribution is static imagine
why we why we why we would want to do
our sizing so for one scenario is that
you have a data set and you want to make
sure and your data set is going very
rapidly and you worry about whether your
system is going to be able to handle
next month so you want to scale up your
dataset to a size that you think it will
be next month and then take this
synthetic data set and run it on your
system so that you know that next month
your system won't crash now in that
scenario there's a time element involved
when you have a time element involved
then the degree distribution might
change in the case of flicker of course
over time a user will ugly upload more
photographs so suddenly the tale of the
degree distribution will change so I
don't think it's a big issue for us
if the degree distribution changes the
upsizer user can just tell us what the
growth function is and we'll just use
that growth function that's one way to
do it another way to do it will be just
minded data you just go look into your
tables look at the dates for the upload
of photograph and so forth and figure
out from the dates whether what the
growth function is and then just go from
there in what I've said so far the
assumption is that filling in this issue
about content generation meet the means
and generating the non-key values um for
the non key values there could be
correlation among the tuples that can be
hard to handle so to give an example up
to this point I've described how you can
generate the number of comments or the
number of tags that a new surgeon for
for each user I know this user is
supposed of 13 X but what other type
values now I don't see the type values
how do I do that
so currently our idea is that so that
the issue is that you know generate the
type values
there's certain coherence in the in the
tech values if you are birdwatcher then
the tech values are likely to involve
birds if you are car enthusiasts then
your type values are going to clustered
around things about cars
so you need to maintain the kind of
coherence the kind of correlation among
the goals of the table so how do you do
something like that so currently the way
we do it is by similarity so so suppose
I know that this user is supposed to
have thirteen comments 13 tags and in
connect 13 tag values I go into the
empirical data set and a cool up a tag
list from a particular user so let's say
that hey factor that pulip has 20 tag
values and this 20 type values would be
coherent in some sense because it's from
the interval to the semi so yes several
more than I did I just randomly drop
seven of them so that that is one way to
do it now what is the the the vector
their sample is too short
I need 13 type values and the vector
that I pull out is has only ten type
values I need three more so to get a
three more I use don't type distribution
right so I initially process the data
set to look at the joint type value
distribution for example that the purse
and sky are going to be correlated and
Worth and cars and maybe not going to
have less of a correlation I use the
distribution so now if I have 30 that if
I if I needed 13 type values in our
sample factor I have is only 10 I joined
in the three extra based on the 10 that
I already have so you can adjust the
paradise
karati solutions at a moment we do it
hello yes so you can you can already see
that there are there are ways of ending
troop on on this but the hardest hardest
assumption to deal with is this last one
so I formulated this problem all social
networks attribute value correlation
problem I couldn't find a snazzy acronym
for it so it's a mouthful so the
question is if the empirical data set
that you give me recalls some social
interactions how how can I discover
these social interactions from the data
cells if I cannot discover this these
social interactions from the data set
I cannot reverse to any kind of reverse
engineering and and I don't know how
laughs sighs there's no literature on
this so we struggle with this and our
current idea is that friends are similar
or friends you just mean that they
interact in some way in the face of
Flickr they interact in the sense that
they comment on each other's photograph
and they are similar similar in what
sense okay so in the in the context of
this presentation they are similar in
terms of tag links that you do so if two
users have you take the tag list of two
users then if one of them is a car
enthusiast and the other one is a
birdwatcher
the assumption is that it was signed
with it smaller than if both of them are
watches so now
how can i replicate a friendship craft
so suppose the empirical data set that
you give me has a friendship graph as
defined here
which is just a comment on each other's
work suppose I define friends to be
those who interact by commenting on each
other's photographs how can i up size it
so I first of all pull out the users who
comment on your photograph this is
actually a technical requirement and
then I construct the graph from there so
I know the friendship graph on the
original data sets now what one who do
is up size this friendship graph now for
the synthetic data set I can pull out
the users that comment on their own
photos the part of the data has already
been generated what I need now is modify
the data modify the synthetic data so
that I have this friendship graph so the
way I do that the way I still have the
friendship graph is that the original
data original friendship graph size our
scaled it up by the same size this is
not obvious you can argue that the scale
up size to scale up proportional to s
square rather than the friendship graph
the people who comment on each other's
photographs so you know original data
set there are people who comment on each
other's photograph the upsizer algorithm
up to this point does not try to
replicate that I'm trying to replicate
that now okay and I'm trying to
determine how many of this commenting on
each other actually I should do in the
scale of data sets
you can see you can imagine that if I
don't trigger the comments - especially
replicate this graph the chance of two
users commenting on each other's
photographs is pretty much zero mmm now
why scale up with s rather than instead
of scale up with s square well if your
data set is n square because of the
square effect but if it is very
substantial size then yeah so so I look
a little bit at the literature and and
it seems that when social networks row
they don't although the number of users
in a social network grows the number of
friends you grow doesn't grow as fast
right it's pretty constant so that means
that these social network grows by
adding news communities rather than this
community is growing at the same time so
this is the hypothesis here that's why
it's scaling I would mask with ok so now
I know how many how many friends to
generate I just need to figure out who
are going to be the friends so I will
pick candidates to be friends according
to the cosine similarity so x prime and
y prime x prime and y prime arts and
ethic i will target X Prime and Y prime
to be friends according to the cosine
very we are using this fully harmony or
something
unless you are using that for something
like eHarmony when you have like two
very different sets of people with if
poor people are put and put the same
then the cosine makes sense if you
explicitly have two different flavors of
people and looking at their connections
then the person might be too inside here
yeah they did you remember in my my my
first graph I said this this is because
the way I attack this this correlation
problem is only a strong main solution
it's a strong a for you to not power so
I go into the
they just said I have so far and fish
out the ones who comment on their own
photographs and this is easy to tell and
then I swap them
I saw the identity of the commenter when
I swap them so initially I comment on my
photograph when I swap that means we
comment on each other's photograph and
that's how I generate this friendship
okay so okay show you some data
experimented we downloaded Flickr so
here I'm making a synthetic copy so f1
has 1.5 million comments and half a
million photographs and I'm going to up
sized it with still factor 1 that means
I make a completely synthetic copy and
this table shows you compares the the
real table size and the synthetic table
sizes so the photo remember the photo is
table is generated by degree
distribution and you already see that
this it's not very accurate the comment
and the tag tables are generated by
co-clustering so if you can't you can
judge for yourself
how good those those ideas are now so a
bit of notation here F 2.81 just means
that the real data set is 2.8 one time
the F 1 and then upsizer
I'm using the F 1 to upsize it to 2.81
okay so
okay so you can you can see for yourself
whether it is it's good of that the the
one that jumps out is comment table
for nine up side by nine or comment you
see that the error is quite big and we
don't understand why some book for they
suggested to us that maybe comments at
the kind of because if thing bigger
suddenly it grows exponentially blows up
maybe really this one but remember when
I start that I say similarity is known
it's going to be measured by queries so
let me show you how how that goes so we
have four queries here according to the
number of joints that we take and and
you can see the comparison here
for this for queries and if it for F for
let's say you really have to be very
sharp in order to get such a small
number of the millions of comments
they're generated but all these queries
just basically taking joints we don't
query on the non key values so the next
two queries against on the non key value
and you see now the accuracy is not as
good I don't see this as the method not
working I see this as maybe our cosine
similarity is not working very well
maybe we should have a better way of
doing it now so far so no social network
involved now we want to test the social
network idea whether we are able to
replicate that social network so
remember for for this two guys are
friends if the comment on each other's
photograph so we attack this the social
network with the swap idea so we all
know see whether
it will read what we gots water so the
first query is to retrieve X&amp;amp;Y who are
friends and you see that without a soft
you can do it at all without actually
twittering the data to reflect the the
friendship graph then we wanted to test
where the cosine similarity is actually
affair with reflection of the
probability that two guys are friends
and here is I think working reasonably
well except for f9 so I guess that means
that we can scale up up to a factor of
110 and then we are interested in kind
of light transitivity but not quite know
where x and y are friends and y&amp;amp;z are
friends we see whether we can get that
and we can't see that we are pretty far
off here so here is a topology issue the
it turns out the flicker you have the
friendship graph have a star shape with
some hot guy in the middle and a lot of
friends around this hot guy and we are
not able to replicate that topology we
can replicate a number of Ages we cannot
keep the topology some of the problem is
with our similarity measure so for
example right now we take a cosine so if
a time factor says chicken another
vector say is birds hen then cosine 0
so that could be improved and improving
that mind capture some of the topology
but we think we are going to need more
than just that because what if you do a
little analysis would you serve some
kind of genetics we are going to need
some of that but I don't know whether is
enough
I'd have intercourse on its own but it
will probably not help in the shape of
those connections in a sense of village
it's that that equation the fact that if
you just don't have those segments which
represent connections but they have some
shape such as the study mentioned that
probably would be captured by that
because then even if the similarity
between the santur and it was around
there is a reasonably be or the actual
number of connections there on those are
not a lot smaller than right now I don't
know how what what we need to be able to
replicate something like a star shape
it's so it's symmetric we will need to
have captured an asymmetry somehow and
it's not clear to me how the number of
followers of famous people is proportion
to the population so we sort of
understand what what it underlies this
behavior but we the thing is how we so
then let me get to this so you can see
that so we wanted upsizer to work for
any data set and we are far from that I
think if you want to work for every data
set in every application we are going to
need a lot of help
to tailor it for all this kind all these
applications out there so we are
releasing upsizer
with people hoping that we have some
community effort to come
to do to do this
I think CPC is becoming irrelevant in
the pace at which people generates new
datasets just completely make just make
TPC completely hero and what we need is
something that will double up size at
some generic tool for up citing anything
and it is but we are going to need help
the problem here which we are discussing
is of obviously swap techniques it's not
something that will work in general but
what will work I think we need some
understanding of how social name social
interactions are reflected in the data
without understanding how social
interactions are reflected in the data
we can do the reverse engineering some
kind of conditional probability of those
friendships when you had a friendship as
a big thing yeah some kind of
conditional probability there anything
those I talked about data these people
and they take that data based theory
community has not yet looked at social
networks a lot of work is a lot of work
is being done by traditions on social
networks but doesn't stop Fox community
maybe in a machine learning community
NEPA data based community they have not
yet looked into it the database
traditions classical database theory is
good things time a friend of a friend
must be a friend not so good at friend
of a friend is likely to be a friend
because this involving uncertainty
probability they die out there of course
probabilistic databases button but they
don't look at this kind of probabilistic
databases don't look at this kind
relationships so I'm hoping I go around
talking about cider and I'm hoping to
stimulate some work on on this other
person you mentioned that some event
might be using that to give data to
render I would be a bit nervous about
that
I mean if let's say you up sighs your
data by a factor of 10 or 9 the vendor
you give me their data set it's a decent
chance I can identify a couple of your
customers pretty much for sure yeah
so that that is usually the freedom that
I can have enough correlations that it
we do good chance I can figure out some
that it's still an issue yeah so okay so
it is schedule I have we have another
six minutes before we zoom the camera
into Google context especially to scale
and try to keep some of the cache hit
ratios hopefully close to reality or
gender star shape what about the rest of
us have to try to keep the cash if I
have maybe levels of caching then who
have a dataset which is hopefully to
some degree similar to what time you'll
be getting some time we can see in the
case of se code one last I see de
paper about something called
desensitizer where they they are trying
to desensitize anonymize the data
doesn't if you have more than a few
degrees of freedom so there they they
are more there the requirement is more
stringent they actually want to preserve
the query to the the the optimized query
claim so here we don't write anything on
this salt but I don't know whether is
useful enough for them preserving query
plan they they go into a shop they go to
a bank or whatever and then they want to
they find it that the query is not
running well they do but in order to
debug it they have to take a query plan
out and the only way it would be to make
a copy of the year of the data he
sensitizers data so for for us s equal
one year we wish it we will anonymize
the day after we kind of like destroy it
to some device failure you you can still
you can still do something if it is a
freedom is sufficiently that you will
have a whole fleet of information just
because you are measuring some of those
correlations very accurately
once what's the biggest data set
the f9f 9 is 15 million comments but
many of these data says the size
actually doesn't count from the
attribute values and so forth is
actually in globs in the in the
photograph tinsel so once we generate
the photographs for those 5 million
photos that say we will run into
terabytes that's as big as you dare but
even before F knives we've got a
complaint from Flickr multi sorry sorry
we won't do it again
this social interaction I mean I
understand that this arm is actually
quite different from the way the
interaction
I mean this is some others so this is
this is one of the issues we have to
study as in the case of Facebook our
original target was actually Facebook
and in the case of Facebook if you write
on each other's war then that would
correspond to commenting on each other's
photograph but I understand that for
something like Facebook they have
something like birthday events that will
generate a whole set of comments or
whatever and it's a different kind of
social interaction that we have to
capture if we actually want to target
Facebook but we couldn't do any kind of
experiment with it so we haven't
considered because we couldn't download
the Facebook on Twitter Twitter we have
not looked at how its Twitter different
from what goes on at Facebook and so
forth is much open you should all be
downloader stuff yeah I mean there are
people
our issue was for some of these social
networks they do their structure are not
rich enough for us to test our ideas so
although I just the algorithm that I
describe your sins my trivial but
actually the full algorithm itself will
have to partition the foreign keys and
so forth look at equivalent sets and
equivalence classes and so forth
so the full algorithm is somewhat more
complicated than this because of the
rich structure of the schema graph
depending on how rich the schema graph
is so we look at Wikipedia for example
and it's just not interesting it may be
a very big data set but the from the
point of view of the schema graph it was
not interesting so I don't know whether
Twitter has enough structure for to make
the data set interesting for us to test
Twitter there are only a few things that
can happen that makes this is a bad
choice in the sense that if you are
testing how we generate the key values
it's probably a bad choice because it's
not rich enough the schema graph is not
written up but it's probably a good
choice if we are trying to learn how to
scale up the social interaction so right
now we don't know we don't know what's
the right way to hold tender the social
interaction so maybe maybe we should
look at Twitter where the key variable
part of the structure is simpler but the
social network part of that structure is
easier to study yeah it's great
and one time thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>