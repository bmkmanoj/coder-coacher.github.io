<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Research Scientist - Assaf schuster | Coder Coacher - Coaching Coders</title><meta content="Research Scientist - Assaf schuster - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Research Scientist - Assaf schuster</b></h2><h5 class="post__date">2008-06-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/q5tpZcSrMLo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">ok so the talk k i'm going to give TJ is
about the monitoring distributed data
stream and enjoying with the danielle
karen with a colleague of mine from
haifa university and two of my PhD
students it's rock shelf man and guy
soggy and the this talk is based on
several papers but i wanted to mention
that in my group play on recent years we
focus on large-scale distributed the
data processing and data mining with
emphasis on minimization of computer
communication while doing that and the i
also wanted to mention the other areas
in which a work because i originated in
the systems and pollen distributed the
computing domains so i do great
peer-to-peer network distributed shared
memories of a storage in and so on so
forth so so my origins are more in the
systems area and actually build some
systems the in production systems the
production grades in some some nice
stuff alright so back back into this
talk in order to motivate this talk
let's look at an example i'm going to
motivate to talk to several examples
here okay so consider my old website and
what you want to do you want to follow
the accesses on the different pages so
suppose you have a green page a blue
page in the right page and you want to
monitor the frequency of accesses to the
pages so that when the frequency of
accesses to the let's focus on the green
page the frequency of this is on the
green page a cross a certain threshold
you get a you get an alert what is this
a lot a used for maybe to put another
copy of this page on another mirror may
be to eliminate a is this this page from
one of the murals when when it goes
below the threshold but the
the basic thing is that you want to
monitor the average number of accesses a
frequency the global frequency of
accesses where the artists are
distributing okay so this is a one is
the simplest perhaps example of the
problem that we are trying to attack so
here's the first a lot of accesses to
the green page it crosses the threshold
for me Bob then accesses to the blue and
red so it's below and then there are
again accesses to the green so it
crosses again they is Rachel from below
and what we want to have is alerts
precisely when it crosses the slideshow
so let us say say what will in general
what we are going to to solve and then
go back to the example so bowing to
slide system model we can see we
consider set of data sources which we
call streams they are distributed
geographically or in the same room but
on different nodes and they produce new
data items periodically occasionally
sometimes they do not produce just a
data item but a collection of data and
so we call it data vector so a data
vector is collected from each of these
shrimps periodically the streaming
infinite oh we can we also consider very
large data set sometimes because we
treat them the same as as if they will a
shrimp so usually you treat a stream
with the sliding or jumping windows it
will not matter for a four hour the
results in our methods and I will not
get into this specific issue so what is
the problem that we are trying to solve
we are given a function over the average
of the data vector so collect all the
data vectors from all the sources take
the average compute a function on the
average we want to know whether this
function a crosses a specific threshold
that is given so this is actually
continuous query it leaves all the time
the system
and it wants to know when this
collection is is crossed and our goal in
life is to minimize the communication in
the system while doing this all right so
a back to the interesting example so in
how in my home cities as a lot of air
pollution and so what does a mayor do it
distributes n so that all of the city in
the sense of monitors the concentration
of different air pollutants carbon
dioxide sulphur dioxide ozone and so on
and there's a function called the air
quality index function over the
concentration of these pollutants which
is not a simple function it's not a
simple aggregation which determines a
what is a quality and when the air
quality crosses a given threshold it is
out of the law it is the law prohibits
this this function to cross a certain
size show the to be above the threshold
and so what does the mayor do when the
air quality index the average air
quality index over all the sensors that
are disabilities in city crosses the
threshold form below it calls it gives a
call to the refineries and tell them
please switch off switch to a low
pollutant fuel so it they switch to low
pollutant fuel the Equality Index go
down it goes back under the threshold
there's an alert going to the Mayo
against called the refineries and tell
them switch back to a high pollutant
view now why don't they always use low
pollutant fuel is something that I have
lots to say about but it's a different
talk so they're with me but anyway this
is yet another a situation in which we
want to monitor a distributed set off of
a off of streams of streams of
information that are coming in in this
scenario it's not big because only like
ten of those or between seven and ten of
these and and in this specific example
it's not really important but i wanted
to exemplify the problem in a moment
we'll see an example where the traffic
is important okay and okay so there are
some sense or networks examples which we
are going to skip today's a sense if you
don't mind I mean if you want to hear
about them just let me know all right so
example number four which is non
streaming a example a search engine
database so consider a distributed data
center well house with tens of thousands
of of nodes a horizontal partitions of
the data and in such an environment
usually as a monitoring of what is
happening produces a lot of data a
orders of magnitude larger than the
original data itself the monitoring
produces log the logs are enormous and
basically what you want to do you want
to mine these logs in order to
understand what what do the customers
want what is the best way to optimize
the system what is the way to configure
the system so it performs best ok so the
logs anymore enormous you want to mind
them for instance one of the questions
that we want to ask when mining the log
is a compute the pair of keywords for
which the correlation index is high okay
so this we is going to help us in
optimizing the solutions in the system
and when when working with the customers
so a this is an interesting question but
of course what Greg was asking is
correct what is the amount of
communication that we are produced
because it might be that you produce is
that the communication that you produce
is not too high so it turns out that we
are going to
think in such a situation you usually
think about thousands of such processes
that are live together and they consume
a lot of bandwidth and the wind with
really become a scarce resource and
really becomes a bottleneck and so we
really need to think about solutions
that will optimize the operation is the
computation of such a such a task okay
any questions about the it's example
okay okay so so let's this is the last
example that I want to make a to turn
into a running example and in this
example consider the set of
geographically distributed is a mail
service ok and this mail server they
want to collaborate with each other on
identifying spam so we assume that the
mail server receives a stream of
positive and negative examples an
example is an email it is tagged as spam
or tagged as not spam the tagging itself
is also an issue but it's a different
talk ok so we assume that there is a
stream of positive and negative examples
in mail servers many of them are
distributed across states or continent
and they want to collaborate on
identifying spam so how do they do that
they select a set of features which can
be words from the from the emails or
concept in the email and you use these
emails in order to build the spam
classifier a how is a word the selected
they need to say set to be as small as
possible but but they need those words
to be meaningful what does it means that
the feature is meaningful feature is
considered to be good if some function
which is called the information gain is
above a certain ratio ok then the social
this is determined by the system manager
so m so here you can see for instance
stream of documents from zero to eight
hundred this is a you
you know the situation though is a
stream of 800 document and there are
certain and I consider in this graph you
see is lee three features bosnia IPO and
febrile a bosnia starts with a very high
information game and then is the stream
pocket makes progress a bosnian loses
some of its a capability to identify
spam or not spam IPO is always a fairly
good in identifying a spam not spam in
febrile starts very low goes up and goes
down again later we will see why they
behaving in the way that they do but the
idea is that we set a special them in
the city manager sets a special for
instance seal which determines that
Bosnia in febrile a bosnia is going to
be he'll above the threshold Fabo is
going to be here and we want to know
precisely when this feature cause the
threshold okay so this is a running
example so what is this information gain
thing in the information gain is defined
on on a something that we called the
contingency tables the statistics table
and the contingency tables save the
following information how much of the
mail that came into the system was spam
and included the feature how much of the
information that came in and was tagged
as spam does did not include the feature
how much of the information that came is
the data the image that came in was not
spam included a contains a feature and
so on okay so this is a contingency
table and the information gain is
defined like that it does not really
matter very much a on what is the
specific function this is why it is
different okay so what what do we want
to do we want to collect this
contingency table from all of the
different wave servers and on the
average take the come to the information
gain on the Avalanche and decide if
whether it's above or below the session
okay so what is the problem the point is
as follows consider two mail servers one
of them has this contingency table which
a on all the mail that appeared the n1
will spam the feature appealed in on all
the mail that appeared and will not spam
the feature did not appeal so the
feature is very good in identifying spam
so the information gained on this
contingency table is the highest value
of the information here which is one on
the other hand we have the mail server
for which on all the mail that appeared
and was a tabbed is not palm the feature
appeals and on all the mail that
appeared in was a targeted spam the
feature did not appeal so again a in
this is this a serval a render this
feature very good for identifying spam
when the information get a game gets its
highest value but what happens when we
take the average of these the
information gain is zero it's the lowest
value that the information gain can take
and this is the example that I wanted to
put on the table for why the task that
we take a on ourselves here is very hard
because there's no local indication on
what is happening globally and even even
worse when you consider what is
happening locally on general functions a
the local indication can be misleading
for what is happening globally and we
are interested in the global behavior
okay so the problem is is is clear this
is why we I give this example in a lot
of details because I want you to
understand what is the problem that we
are having here locally you have
misleading indication about what is
happening globally all right so what did
people do with this problem there was a
lot of large volume of previous work on
on aggregation functions on on linear
function on sum and average
a one of the early works was the in
infocom by Danny Rizzo sitting hill and
the there's a lot of follow-up works on
this a unfortunately there were no
previous we found no previous solution
for arbitrary function for functions
that are not linear and not aggregates
only naive algorithms and of course the
naive algorithm move all the data to a
central location and process it there
which means that is a lot of overhead in
terms of bandwidth power cycles of the
processors the centralized resources
that you need in order to process all
this data and also privacy issues
because none of you would like his email
to be moved to a central location only
because the server want to collaborate
with other servers on an identifying
spam okay so maybe you think that in
nonlinear functions are not important
maybe they never appeal so so apparently
this is not the case in industry a lot
of the time in many occasions you do
need to process nonlinear functions you
did you do need to in a process linen
nonlinear function on top of a
distributed environment in the i take
this quote from me sigmod that I
attended earlier this week from sweet
aroma Swami to give a keynote talk on
extreme data mining and apparently he
also think that there's no this quote is
says basically that there is no way that
he knows about how to compute this
nonlinear function which is fairly
important for him the link function is a
function that he wants a in order to
compute the usefulness of putting a
certain add a advertisement on a certain
name on a certain page ok so the link
function is very important but it's
nonlinear and apparently
as a hard problem in evaluating in it
while using the massive infrastructure
is at Google put up alright so this
concludes motivation in the example in
the part and now i'm going to 12 into
the solution the geometric approaches we
developed i'm going to then look at some
results analysis and optimizations i'm
going to skip sensor networks today they
do some may shape sensitivity stuff
which is interesting because it's
geometric i must tell you and you're
going to see movies here you're going to
see geometric stuff some drawings for me
which I come from the parallel and
distributed community this was a very
good a very pleasant change to be able
to put a nice system and nice drawings
on a lecture anyway and then a perhaps
we'll we'll do some top k s solutions
all right so what is the geometric
approach so we looked at the problem and
we said because we are tackling a
general function that can behave in any
way there is no it is not good to look
at the target domain let's look at the
origin domain of the function so we
assume that the function is defined on a
dimensional space and on this pace what
we do we color the whole space as
follows we color in white the places
where the function it gets a value which
is below the threshold we color by crazy
places where the function is defined get
a value above the threshold so what we
want to do when we have three local
points of a different streams and we
have this average what we want to do if
this average ever crosses from white to
gray if the changes colors we want to
alert precisely when it changes the
colors okay and the problem is of course
as you can see in this demonstration
that the local values can continue being
white as the stream makes progress the
local values continue being white
enduring with very well in the white but
the global average gets closer and
closer to the threshold until at a
certain point in time it reaches the
threshold crosses it and we want to get
an alert precisely when he does in a
game the local indication in the
different in the different locally
streams are misleading because they
think that the whole thing is white and
they don't know what is going on okay so
our first observation is when we look at
the origin domain of the function is the
average of the local readings of each of
the strange is contained in the convex
hull okay so if you have three different
readings the average would always be in
the convex hull so the so we conclude
that if we somehow know that the convex
hull is monochromatic then we also know
that the color of the average is the
same as the color of the different local
point okay so let's assume for to slide
that we know how to compute the color
that is the color of the convex hull is
monochromatic okay that it's white then
we get an idea well then we get a
solution immediately we know that the
average is also white and each of the
local points know the answer the problem
is that the convex hull will become very
large you see the convex hull inflates
more and more and more and it's very
very large we haven't a way how to
control it but the effect that the
convex hull is inflate be starts to be a
problem especially when the convex hull
stops being monochromatic you see the
convex hull stops me
monochromatic so our method of is it we
do not know where the global averages
but we know that the convex hull stopped
being monochromatic so we issue an alert
but this alert is a false alarm okay so
it's a false alarm because there's a
long time before the average will reach
the special and perhaps it will never it
will never intercession okay so we need
to do something about the size of the
convex hull what can we do the solution
is at times we can compute a global
average we spend some communication we
compute a global average we let
everybody know what the global averages
but since everybody knows the global a a
point the global information the convex
hull is now shrink to zero why is this
the case because consider these three
empty a diamond okay these were they the
values of the data of the different
local a streams at the time that the
global average is a rectangle in the
middle is a squirrel is a blue square in
the middle which is the global average
the global average was computed okay now
a was some communication going on the
computed the global average and now the
stream makes progress the same X focuses
there are drift of the empty a diamonds
in different directions from here to
here from here to there and from this
one to that one and these rifts
correspond directly to dress forms it
correspond 12 13 for defectors which are
dressed from the global average a 22
these points to these red points now
since everything in linear and
everything is in aggregate the average
of the global values is actually also
contain in the convex hull of the local
drift so now instead of monitoring this
a convex hull this large convex hull we
only need to monitor
is a small contracts okay we only need
to monitor it if we know that it's
wonderful mati then we know the
resulting we know that no communication
should they should be done alright so a
old you everything a assuming that we
know how to monitor the convey the Mona
FOMA TCT of convex on wait there's more
to this film so you see once it becomes
non Mon Homme attic once the convex hull
becomes non Mon automatically again
compute we compute again the global
average and again start with drift
vectors they which are small and the
convex hull shrinks again so on and we
can issue alerts precisely when we cross
the session ok so now I need to tell you
how I monitor the monochromatic city of
the convex heart so a what we did was to
prove a theorem and the Syrian says as
follows suppose you have a set suppose
you have a polygon in our dimensional
space in D dimensional space and now
compute a certain and now take a point
the point can be inside the polygon out
of the polygon does not really matter ok
and now take the distance from each of
the vertices of the polygon to this
point and make a sphere whose diameter
is this distance ok so here's once there
is another sphere is another cell and so
on and the theorem says that the convex
hull is bounded by the union of these
fields ok so this sounds like a very
natural very simple and straightforward
the theorem however apparently it was
not known to computational geometry
people we could not find it in any of
the computational geometry books when we
talked with
the stars in it in that area like me
cause she really said that the term did
sensation that it was not really known
when I talked to other people as I said
that maybe maybe Euclid already knew the
two dimensional version of feet but
unfortunately we we don't have him here
to ask so a so people came with several
poofs to this theorem a yet we did not
find it in anywhere so we assume so so
we get the credit you for proving it so
in two dimensional it's really
relatively easy to prove it like high
school a kind of thing a trigonometry in
higher dimensions there were several
ideas how to prove it yeah I'm not going
to give you the proof although we have
so few very easy ones but what's
important to you is that this serum
gives you a tool how to come up with
local constraints local constraint so
each of the vertices here if you
consider it to be a source a data source
each of the of the vertices deal can by
itself compute something which will tell
it whether it needs to trigger some
communication or not if none of them
trigger communications and internal
communication what is this constraint
the constraint it if whether this sphere
is monochromatic or polychromatic ok if
this fear becomes polyfoam attic then it
means that maybe some of the points
inside the convex hull are switched
switch colors and it means that this guy
should trigger communication but if
nobody triggers communication it means
that all the spheres are monochromatic
and the convex hull is also on comedy ok
so this this was the idea in the
algorithm that follows
is they just pick a an estimate vector
compute a global average and now they
take the drift vectors and each of the
each of the origin is to the data
sources will compute whether the current
sphere is one home attic or not and as
long as it monochromatic there's no
communication that needs to be done okay
so it goes as the algorithm goes as
follows you can see that each of the
data sources compute the Mona FOMA tissa
t of fail if all of them are mono for as
long as all of them are monochromatic
the convex hull is monochromatic and
they do not do need to do anything at a
certain point of time they will become
polychromatic and which will trigger
communication event and they will again
compute the global average and again and
again and again so on I always like to
look at those movies and I say hi p no
diving anyway okay so let's look at some
result when applying this algorithm we
looked at the vital scopus with 800,000
new stories between august 96 and august
97 and unfortunately at that time there
was no tagging for spam not spam but the
worst hugging for industrial in
corporate industrial or non cooperative
duster and we decided that this tagging
simulates quite well spam not spam so so
we use this so when I say spammer not by
me mean cooperate industrial or not
cooperate necessarily and vice versa
okay all right so we looked at the three
different features and at that time is
the bone Yugoslavia was just coming to
an end so you can see that at the
beginning the world Bosnia was a very
good discriminator in those door a new
document for whether this is a
industrial
story or whether it's not industrial
story ok whenever come Bosnia appeared
it was probably not industrial story all
right and and if you look at IPO it's
always a relatively good indicator for
industrial stories and if you look at
fabbro at the beginning it is at end in
August half a year before fable it is a
very bad identifier slowly during the
years of this peak here which we do not
know to explain slowly during the it
gains power and finally it becomes in
February it becomes a very good and
discriminative of industrial stories and
then it drops down again and we I don't
know how to explain this I don't know if
you have an explanation it would be
happy to hear it but anyway this is a
wider behave in this way along the along
the strain and if you see you you look
now at it is the amount of messages
amount of communications that we need in
order to a plower method you can see
that the naive algorithm requires the
about nine-tenths of the of the total
amount of documents because we have your
10 nodes okay collaborating ok so so the
naive algorithm need to move the all the
information from may nine of them to one
of them so it does a nine-tenths of the
total communication and if you look at
what happens with our algorithm you'll
see that basically in thresholds that
are barely met the communication drops
down to practically zero okay a so so
this is these are the extreme extreme
threshold in this say to the left and to
the right okay but if you look at the
middle that some places well we can save
a very little lucky three or four fold
in the amount of communication which I
call very little so so why is it the
case so if we look at fabbro and we set
certain threshold it usually causes this
threshold only once or twice or four
times max okay hey and if we look at
Bosnia there's some he'll it crosses
threshold a very rarely but here it
starts crossing this special van Bertie
on the threshold of point zero zero 31
or 32 a lot of the time so you can see
that if you set the special 2.00 three
something bosnia get some some high
values view of communication because
every time we will cross we will cause
the threshold there's some communication
going on several times because they both
will the spells will stop being
monochromatic okay and the time with IPO
IPO is very good on sonic streams I shot
but when you get to the threshold of
point zero zero three around he'll then
it vibrating crosses the threshold all
the time and so it produces a lot of the
algorithm produces a lot of
communication okay so so how can we help
you of course one way to help is to set
some to use a trade-off of accuracy vs
performance give give away some accuracy
and get a more efficiency by setting a
narrow margin along the threshold values
the error margin is determined by the
dashed lines the white dish line and the
grade esh line and now we are going to
issue alert only when the we are going
to a to say that this fair becomes non
Mon automatic only when it crosses the
dashed line okay so we get an alert when
the global average crosses the dashed
line he is a desk or a line so this is a
high watermark in low water mark okay
and you can see that for the a threshold
point zero zero three IPO gains much
more than the answers a by activating
this method precisely because it
vibrates all the time
a on point zero zero three and when you
set our margins it gains a lot of profit
okay in the other game less okay so it
goes as follows you can count the number
of times is first becomes more home
attic and you'll get seven and when you
set a low margins you can't and you'll
see that it's it's only four times I
think this just to show you once again
this movie okay another way way to
stabilize IPO on such threshold and by
the way this is not a real problem
because you always want money to
function on extreme values you don't
want to monitor function on thresholds
that always cost it's irrational okay
but but if you do nevertheless if you do
then what something else you can do is
you can aggregate messages aggregate the
data items that are arriving at the data
source in a large window size and you
can see that when the window size grows
IPO and IP or gets a more profit than
the other feature once again okay
because it stabilizes and when stabilize
it causes a threshold a less a number of
times with respect to scalability it's
of course different between the
different threshold this is point zero
zero three which is very hard on a
Bosnian IPO so you can see but but
basically what you see here is that the
amount of communication that is required
in our a mechanism is also linear it is
best activated on thresholds that are
not met a lot of times
if you look at the performance analysis
of course is the performance analysis is
the first thing that performance
analysis depends on is a function itself
so it see if the function look in this
way it is much worse than if it looks
this way of course and there's nothing
you can do okay so this should should be
clear up front however the efficiency
also depends on on two or three other
two or three other factors the first of
them is the distance between the global
average and the surface the zero surface
of the function okay if this is large
then the probability of violation is
smaller and when this is small the
probability of law relation is larger it
also depends on how a what is the
variance of the local data streams a how
do they depart from the from the global
average when the variance is high then
the spheres are large and there's a
higher power a probability that this
field will become known one automatic
and vice effects okay so if you want if
you like mathematics you can look at
this bound and see that indeed the
probability of a violation increases the
number of the month which show itself in
the amount of communication increases
with the variance of the a of the
different of the different sources and
they is reduced by the distance from a
of the cool with the diff with the
increase of the distance of the global
average from the 0 surface and is in a
is also decreased by something else
which is the size of the window that you
think
one other thing one other nice features
of this nice a nice things about this
methodology is that you do not really
need to go global communication okay I
suppose you have some trouble maker heel
that is that ends a large variance and
he is becoming polychromatic every now
and then all you need to do is to find
somebody else who's behaving very well
in average disease to make sure that the
one that behaves well averages the is
the the one that is not behaving well
okay and when the average there is a
values together you get this point they
both know about this point now now the
convex hull is is this convex hull and
the two others feels remains the same
they did not head they did not heal
about what was going on but still you
get something which is monochromatic and
no global communication needs to be to
be take the sum issue is a computational
complexity of calculating monica mattos
city of spheres it's a painful issue and
we have many different ideas how to
solve it some functions that you can
compute a closed-form solution some
numerical methods some offline
computation and caching that you can do
in I don't want to get into this in this
mine mine field something else that you
can do is to consider monitoring many
features until now I was talking about
monitoring a single feature what happens
if you go into monitor many many feature
if you're going to monitor for 4,500
features already you meet the
performance of the naive algorithm
because in I've algorithm it doesn't it
doesn't
for the naive algorithm how many feature
you you monitor ok so you somehow want
to be able to monitor together all the
features and not one by one ok so what
can you do in this example we we look at
risk well but the risk well is is a
different function that sometimes they
used a to do the same as the information
gained but it doesn't really matter the
important thing is it ok so let's look
again at the contingency table ok it's a
contain certain values and now what we
can do we can apply a certain
transformation and the transformation
leaves X for the same as it is but
instead of x1 x2 x3 defines three new
values 1 y 1 y 2 and y 3 ok we each of
them is the sum of two previous values
and do you they can one of you see why
this transformation is advantageous for
following many many different many
different features instead of just one
at a time ok look at this at this one y
1 is the sum of X 1 and X 3 so basically
y 1 becomes the amount of spam in the
system so when you look at the data in
this coordinate system y 1 the term
means how a much spam is coming into the
system and sperm turns out to come in
bursts so it might be that a certain
feature a has this coordinate grow a
grow its fail and become non
monochromatic but if this coordinate
grows large it means that it will grow
large for all other features so when we
have the communication event when you
add a violation we can look at this
feature see if this was the one that
caused the sphere to be non
monochromatic then we take care of all
other features at the same time it looks
the following i'm going to show you now
how
a is a strain the Reuters swim behaves
with a new coordinate system you can see
this is a the manifold of some threshold
in the some some of the features go a
equals the manifold they need to be a
alarm alerted in the go in in a y 2 and
y 3 the elections but in the Y one axis
which is a z-axis in what you have you
is the amount of spamming you can see
that the amount of spam grows and
becomes in bursts all of the sudden does
a large amount of spam in the industry
so everybody go up and of course this
fear grows very very large for all of
them together and then they go down and
again this field was very very large for
all of them together so you can in this
respect evaluations it comes from the
fact that they move in the Y one axis
you can treat all of them together and
reduce the amount of communication a by
loud by great deal okay so we move
through the results analysis
optimizations we move through the sensor
networks and we get a shape sensitivity
and talking how much time do I have I
don't see any seven minutes okay so okay
so there's all kind of geometric games
that you can do a one things that you
can do is to feed the cover to the data
okay so you have the the data coming in
so it far the variance we already talked
about the variance it goes in in
different direction and this is a global
average so probably that the election
well the different sources move is along
this axis along the PCA okay so what you
can do is to use a covariance matrix in
order to align the data along the axis
axis you are going to inflate the data
again by using the Quran
and matrix so that all access have the
same influence and now you can compute
the cover to what you got what you
received and then you're going to apply
the infant's the inverse transformation
squeezing back is a the inflated
creature and we're in turn it again to
is originally a regional state and now
you can see that you have a new cover to
the convex hull which is much much more
compact than the original cover okay the
regional cover had a lot of area which
results in a lot of false alarm okay and
this is a much more compact a solution
okay so this is one thing that you do
you can do you can fit the cover to the
data and we can prove some optimality
results concerning this concerning the
ellipsoid za'tari resulted a something
else that you can do if you have to a
local a values and you have the global
average sometimes this field resulting
spheres are non monochromatic why put
the glove white choose a global average
he'll maybe we can choose another global
average for instance this one's a green
wine thus fields become larger but they
are now Mon 4matic okay what did we do
here we pushed the global average away
from the surface okay what does it mean
to push the global average away from the
sources a the safe zone is defined as a
geometric place of all the points from
which if you draw is a s feel with the
diameter is the distance between this
point and the blue point you will get us
feels it does not cause it is named on
automatic okay so the second is all the
biometric points and you can see that
when we a move away from the from the
threshold a
safe zone becomes larger okay this is a
safe zone of the blue point this is a
safe zone of the greenpoint and as you
go into the middle somehow we need to
define a middle the the safe zones grow
larger how much larger can they grow we
need to push the point the globally the
global reference away from the threshold
and deal the distance form the threshold
starts to be smaller because it starts
to to to come here okay so we push it
away until the point well is a distance
from the social starts to be smaller so
how what is this what are all these
locations that are equidistant from a
from the threshold from from all the
from all the elections from this ratio
they are usually called the skeleton or
medial axis if you look at the threshold
and you take all equidistance a places
and you paint them in the same way you
paint them in the same color you clearly
see the medial axis heal in the central
in the so-called center of the of these
creatures that this function defined now
if you apply this to a optimization in
fitting the cover to the data and
fitting the cover to the to the function
the original performance that you had
this is amount of communication that we
had before with field becomes much
lawyer loyal in lower all the way to the
blue curve hill by two or three orders
of magnitude and and so the question
remains how far can we do how better can
we do we can show a theoretical the
optimal lower bound on this method which
is red line so ethical optimally lower
bound is a sabbatical optimal lower
bound a
relates to all the local methodologies
that use convex hull okay so if you use
convex hull you will always bump into
this lower them and it goes as follows
whenever the saw is a convex hull course
as a social you you you have
communication if it okay so you can't
all communication events that are
produced by the a by the convex are
causing the threshold and you get this
lower bound so you see that we are
fairly close not a lot of space left for
info for 40 min for further optimization
regarding the top K M so there are some
motivating example here is a example of
of the you have many stores that are
distributed geographically and at the
end of these days are all these
purchases that were done in the stores
and each of the purchases is a
transaction contains a lot of items and
the four interns at the end of each day
the manager the manager would like to
know the top case all product page or
the top k pairs of products that
correlate to each other the most okay
another example is if you have a destroy
this is a previous example that we had
at the beginning of the talk where you
have a original maintains local
statistics of the queries on a search
engine a from the previous day and every
day you want to know whether what are
the words that imply each other the most
in the query that appeared in the day in
the previous day okay in a distributed
setup you have the count of word and you
want to know the pairs of words that
imply each other inquiries the call it
to each other the highest in in the
queries okay so we use the correlation
coefficient equation a function which is
defined here and has a many nice
properties
in general it takes a value and eat it
in it is it has a value in the range of
minus 1 to 1 and if the if the value is
higher than 0 it indicated the term time
to appear with each other together with
each other and if the the the output is
0 if the value is 0 it indicates there's
no correlation between the terms and if
the value is less than zero it indicates
that the term term to exclude each other
ok so this is precisely what we want to
know and it also is a nice property
which helps us say later that if we
restrict it to the domain well if a and
B appears a well the region will a and B
appear together is less than or equal to
any of the region's well AAPL alone will
be a paranoid so if we were restricted
to this region it becomes more a
monotonic it's monotonically decreases
with the amount of the appearance of a
and the amount of pls MP and
monotonically increases with the amount
of appearances of both of a and B
together ok so there's some may example
here if you have the two nodes thousand
of quiz on each and on both of them in
both a and B appeal hundred times on the
first node and both a and B appeal four
hundred times on the second node then
they get the same value for the
correlation function in each of these
nodes but when you look at a the average
when you look at the average of these
you get a very different value which
maybe is above the threshold so again
you can see local indications that are
miss lady in this function it's not
linear it's not aggregate it's not in
frequent items ok so so we get a a
four-phase approach I don't want to get
into the whole algorithm the whole
things that you need to know the only
thing that you need to know is that
a the algorithm which is a reflecting on
on on similar algorithms for top K that
we know in the literature is done as
follow is it can you compute a lower
bound on the value of the top k and then
locally prunes out filters out all those
objects that don't even have a chance to
pause this lower bound that will never
be able to cross this lower okay so that
first two phases compute lower bounds
and then the next phase the third phase
go through all the objects locally in
each of the node and locally prunes out
those objects that don't even have a
chance to cause the global / how do we
do that with a nonlinear and on
aggregate function we can do it so the
geometric in geometric approach and we
also use some domination relation i will
not get into this and the first phase is
also not a very important so what do we
do suppose a is these are the values the
local values of a certain object in each
of the nodes and what we want to know we
want to know that the value that the
value of this object when everything
will be collected to a central location
the global average when you operate on
it with a function will not cause the
threshold what is the special lititz the
lower bound on the top cake okay so how
can we do that what we do is we get a
global reference point a global
reference point is Oh engine in this
case and it works well I don't have time
to go into this but it works well is a
legend in this case and now each of the
in each of the local data sources we
compute a all the values the maximal
value of the correlation coefficient on
this is phil and here we conclude the
maximum value of the collection of
coefficient on this that field and so on
so forth so if in one of the note the
correlation coefficient or
on any of these points crosses the slash
is the lower bound that we have on the
top k then it sends an alert about this
object and this object needs to be
gathered together and considered to be a
candidate a viable candidate for the top
K okay but if not then we also know that
the global average was covered and we do
not need to do any communication to
produce any communication okay so this
is yet another usage of this geometrical
interpretation and of course we can go
so all kind of optimizations I don't
have time to get into this in the
domination relation allow us also not to
check all of the nomination relation
result from me the fact that the
function was the he was monotonic in the
region of interest okay so they said
that if you have a very high value of
for an object which is dominating
another object it dominates another
object because all the coordinates are
larger than these objects okay so within
a monotone function or a functions it is
a monotone and on the region of interest
this object is one is the dominating
that object and we do not have to look
even in this object so the filtering the
local filtering in itself process itself
is also very a highly efficient and then
and so we tested it on only queries form
a search engine from AOL search engine
several billions of queries you can see
here the sample correlations for certain
a pairs of words and the communication
drops down it doesn't it's not important
to look at the graph itself and
basically with a large data with few
billions of queries we converge to a
savings three orders of magnitude in the
amount of communications that is needed
and we think that when the data is
larger than
the data the more saving we can do and
we can also save on high-cost with this
amount of data we the local polling in
each of the nodes considered only a
small fraction of the total number of a
of the CEO of keyword pets okay and the
only only tens I think and with a larger
data I think we we will be able to save
even more and basically that is it
there's a lot of current and future work
going on on the foundations of all this
the testing is a complexity of the
testing game on a home olicity some
ideas about how to extend this to appear
to peel kind of networks logical
networks in how to do non threshold
computations a top km streams is
straightforward extension of this work
is some swim fusions a problems and in
the implementations and so on and then
okay so that's it any questions I wish
so one open one of your first night you
talked about averages the information
from which you will later complete the
quality of of the indicator so I was
wondering whether one could consider
provide more information about the
distribution of the of the input maybe
compute higher moments or so
you can perhaps get a a more more
accurate shape of the of the of the
input and then you can say how how well
it is whether more accurately whether
it's a it is a good in indicator I
actually I mean this is a very good
question and although we were focused on
on solving the global average case the
distribution which gives a which would
you you would like to know locally what
is going on in many in in many
situations and you would like to know
that if certain stove kmart is doing a
completely different in is an outlier in
the in the in the story you would like
to know of course this results in a very
different set of algorithms very
different approaches which which are
interesting by themselves and
unfortunately and don't know of a lot of
work in this direction
as a question
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>