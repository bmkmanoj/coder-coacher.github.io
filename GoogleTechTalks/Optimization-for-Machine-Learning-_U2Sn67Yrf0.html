<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Optimization for Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Optimization for Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Optimization for Machine Learning</b></h2><h5 class="post__date">2008-03-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_U2Sn67Yrf0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so my name is Sen Vishwanathan a lot of
you know me as fishy and today what I'm
going to talk about this joint work that
I did with a bunch of people Simon
Doonan term is my postdoc Alex - my
colleague Nick sort of my colleague
Genuity and Jen you are my PhD students
and so let's get started
typically what you want to do in machine
learning is is very simple things like
say for instance you have these blue
dots and these red dots and you want to
find a classifier which can somehow
classify these two points all for
instance say you're given a bunch of
digits and along with the labels and you
want to figure out a classifier which
can go and given some unknown digits can
predict for you the labels on these
students simple tasks so the the core
idea here is that in a machine learning
context your key your what you really
want to do is to build a model which
predicts well on data so this is all
that you care about and the way you go
and quantify how well or how badly a
model performs on the data is by using a
loss function so think of a loss
function as some kind of a sophisticated
discrepancy score which tells you if you
were supposed to predict this and you
predicted something else
how much lost you incur or how much how
much problems you incur because of them
so it's just a simple sophisticated
discrepancy score and depending on the
problem domain you have designing these
discrepancies calls is a is a whole
branch of research onto itself
the key challenge the key difference
between optimization per se and machine
learning is the fact that you're not
just interested in optimizing some kind
of a function and getting to the minimum
your challenge is that your model that
you generate must generalize to unseen
data so this is the key challenge as
compared to just optimization where
everybody says oh I have a objective
function and I just want to go to the
minimum of the objective function so
typically in a machine
in context you would find things where
people say I want to use a regularizer
which is just some kind of penalty on my
model and again designing regular risers
is either active branch of research
people have their own favorite regular
risers I'll try not to get into that
argument but think of a regularizer as
some kind of a penalty which says that
these kind of models are more complex
than the others and so you want to
choose from among your all possible
models which could fit your data you
want to choose one which has low penalty
which has which is in some sense simple
you can put this whole thing what I gave
you an intuition in a more formal set up
which is the set up that will work with
so typically you assume that you're
given some training data so some
instances for instance you might be
given the digits and you are given some
corresponding labels so again you might
be given the labels on these on these
digits and the key goal that you have is
that you need to find a model F so in
our case the model F takes input data X
and predicts a label Y so that's what is
this whole job of the model and a goal
is to learn a simple model so typically
you'd have this regularizer where omega
of f could be any function which you
think penalizes complicated models and
lambda is some kind of a trade-off
parameter which tells you how much you
want to minimize the loss on your data
points versus how much do you believe in
your regular Iser and your loss on your
data points is simply the prediction of
the model on X I and compared to the are
true label y I and you just aggregate
and an average over all possible all
possible data points that you've seen so
this is the this is a typical
regularized risk minimization framework
and this is something that will work
with for most part okay now you might
ask Oh what kind of loss functions can I
use a variety of loss functions that you
could use a variety of discrepancy
scores that you could use one possible
discrepancy scold that you might use and
it's quite popular in CRFs and and such
structured estimation problems is to
find the Maximizer of a log likelihood
so you assume that you're modeling a
density in the exponential family and
you just go ahead and you try to
maximize the parameters of the model
whether you try to find the best model
which maximizes your log likelihood this
is very popular and a lot of people do
this from optimization viewpoint this is
again very interesting because this
gives rise to reasonably smooth
optimization problems which in some
sense are easier to to handle another
model which is become quite popular
especially again in this structured
margin case although I do refer to
structured margin in this talk I'm not
going to talk about it further I'm just
using it as some kind of a motivating
example so here the the model under
which the the the scenario is that you
at every point of time you take the
prediction of the model on all possible
y prime so you say what is this score so
you think of now F as assigning some
kind of a score to each one of the
labels so you take each one of the
possible labels and you evaluate this
score of F on that labels and you
predict with the label which gives you
the maximum score so you just have an
Arg max over the possible labels and if
you predicted Y star and why was the
true label that you wanted to predict
with then you incur a loss which is
Delta of y y star again they're very
sophisticated ways in which you can
design this Delta function this label
loss and again sophisticated ways in
which you can come up with a model the
problem is working with this scenario is
that the problem it is non convex so it
is non convex in F and if you want to
estimate if you run into all kinds of
problems we don't want to move too far
away from convexity because
things become ugly so what you could do
instead is that you could go ahead and
try to upper-bound this loss by some
kind of a nice convex loss so to see
that this is an upper bound is very easy
because you just take Y you substitute Y
star in here so you get f of X Y star
minus f of X Y but this is always
greater than or equal to zero because by
definition Y star is the arc max and
then here you get a delta of y y star so
you get Delta Phi Y star plus something
which is non-negative so therefore this
is an upper bound and this is convex and
it is nice to optimize so let us see how
this all works out in the case of binary
classification just a simple example I
generally tend to work well when there
are some simple examples to see what's
happening so if you if you use this
veneer take well model for binary
classification then your labels you so
you could encode D blue dots in the red
dot says plus 1 and minus 1 say labels
are plus 1 and minus 1 and let's just
assume that the model is y over to f of
X this is just the Y over 2 is nothing
special it's just ensures that all the
math works out nicely in the end but
you're just saying that f of X Y is just
the label times the prediction of the
model on the data and to output a label
y star you simply go and look at the arc
max over your labels of y prime over to
f of X which is what we were doing
before but so you need to see that this
arc max is at mean when you set Y prime
Y star to be the sine of f of X so if
you match the signs then you're going to
you're going to get the arc max so this
model is simply going to take your data
compute f of X and predict with the sine
of f of X simple model the Delta laws
that you might desire to use in this
case there's nothing sacrosanct about
this loves but this is just one loss
that you might want to design and use
and which is which seems quite
reasonable to work with is it if you get
the label right then you incur no loss
if you get the label wrong then you
incur unit loss so what I'm plotting
here
is y times f of X and if y times f of X
is greater than zero then I incur
settlers because I got the label right
and if y times f of X is less than zero
then I incur unit loss clearly you can
see that this loss is not convex so what
I can do is that I can upper bound this
with a loss which looks like this so the
zero term comes because when y equals y
prime then I incur zero loss when y is
not equal to y prime then I incurred
elta minus y over 2 f of X and minus y
over 2 f of X so these are that's why
the viable two was coming in and you
recover the well-known hinge loss so you
have a convex upper bound and this
losses is convex and nice ok you can
extend the same idea to multi-class
these losses they are suppose you assume
that you have some K labels and again
the same kind of Delta loss so the Delta
loss is 0 if you got the label right and
is 1 otherwise then if you write out the
convex upper bound again you get a max
over Y prime not equal to Y and then the
plus just indicates it's a max over 0 so
it's always lower bounded by 0 and then
you get D Delta f of X Y prime and then
f of X 1 so it's this is the mineral
take all model one thing that I did not
talk about yet was how do you design the
model again there are many sophisticated
ways that people come up with to design
models and this is one part that I will
not go down on for this talk I just
simply assume a very very simple linear
model
so I'll just assume that given data and
label pairs x and y there is some
function phi so this function phi you
can think of as some kind of a feature
extractor so it extracts or asked
questions about the XY pairs and gives
you answers simple very very simple way
to think about it and theta is a
parameter and what now you can see is
that f of XY is completely specified by
theta and phi so what i need to optimize
over is
I just need to get theta there are two
possible streams of thought here people
look at Phi of X Y some kind of an
infinite dimensional feature mapping and
reproducing kernel Hilbert space and
then you have to optimize theta which is
a function in an arc HS but that's not
the route we are going to go down again
we're going to keep it very very simple
and assume that this Phi of X Y is a
finite dimensional vector and so what we
are optimizing for is a finite
dimensional parameter vector now turning
to the regularizer so now you have to
penalize models right so and we saw that
our model is completely parametrized by
theta so in some sense if you
parameterize t tights if you if you
regular eyes theta it's a it has the
same effect as regularizing your model
and so one possible regularizer which is
very popular in especially in the kernel
community is required norm regularizer
so you just you say i want to get that
theta which incurs the least loss on my
examples yet has the smallest length and
so this is the notion of Euclidean
length if suppose you want sparsity in
your solution so in case you want to
have sparse solutions you want to have
most of the pair of the components of
theta to be 0 and this is something that
you might care about especially if you
care about how fast it how fast
prediction is then you might want to do
NL 1 now if you're working with
probability distributions one possible
and very popular regularizer is the
entropic regularizer
again to keep things simple for this
talk mostly I'll talk about square nom
regular Reiser's lot of what I'm talking
is applicable to other regular risers as
well but just to keep things simple and
nice now I turn to the question that we
are interested in so what's your
optimization strategy for doing this
whole thing to solve this rigorous
minimization problem first of all all
the regular risers that I'm working with
are convex so they are convex in theta
and
the losses because of the way the
winner-take-all model is economic supper
barn so I have a convex loss so overall
what this gives me is a convex
optimization problem which I need to
solve in the machine learning context
there are two different philosophies on
how you can go around solving this
convex optimization problem the first
one is to use an online algorithm so in
an online algorithm you use small
subsets of the data and you assume that
the data is streaming through and every
time you use a small subset of the data
compute gradients and the objective
function value and you take a step along
the direction and you just repeatedly
cycle through your data this is some
area that I've done some work in but
again I won't talk about this today and
what I'm going to talk about today's
batch optimization so in batch
optimization you assume that you have
the entire data set available to you and
you can compute the loss on the entire
data set so remember in the regularizes
minimization framework we had the
summation over all possible losses over
all possible data points and so I assume
that all the data points are available
at once and I can compute the loss on
all the data points at once so this is
the this is what I'm going to talk about
today in batch optimization again you
can do simple things like gradient
descent but they generally tend not to
converge very fast especially as you
approach closer to the optimum these
methods don't work very well so you want
to do something smarter than just simple
gradient descent and so but you still
don't want to compute the Hessian
because theta in your case could be
millions of parameters and so you don't
want to compute the Hessian but you
still want to do something smarter with
the aggregation of your gradients one
possible approach to that problem is to
use a bundle method so bundle methods
are something that Alex talked about I
guess a few weeks ago and the key idea
here is that you use all the fast
gradients and you try to lower bound the
function and every time you get a new
gradient you improve your lower bound
and the key thing there is to
figure out where to cut the next
gradient ready evaluate the next
gradient so that your lower bound
improves the most this is the key
challenge there what I'm going to talk
about today is another stream of work
which is based on quasi Newton methods
so what quasi Newton methods do is the
idea here is it and Hessian might be
very very expensive to compute but maybe
I can approximate the Hessian maybe I
can get a good idea of the Hessian by
looking at the gradients and if I have a
good idea of the Hessian then I can
build a quadratic model of the function
and every time I can take a step on the
quadratic model to reduce the objective
value so the key challenge here is that
how do you use the gradients to update
your quadratic model and how do you take
a step so this is the this is the key
issue these four gentlemen Broyden
Fletcher Goldfarb and Shanno
in 1970 came up with the most popular
quasi-newton algorithm so BFGS is is
basically by default the optima a
variant of BFGS is by default the
optimizer that same matlab calls every
time you call optimize and I'm not
saying that the MATLAB optimizer is good
but at least there's one that most of
the people know about and they
independently of each other published
the same essentially the same algorithm
and so it's named in their honor BFGS is
a is a default optimizer for basically
not necessarily convex but smooth
optimization problems
so this is where smoothness is very
important for BFGS to work so let's
first understand what BFGS does and then
we will see how we can apply it to some
of the problems that we are interested
in so the way the the BFGS algorithm
works is that at every point of time it
assumes that you can compute the
gradient of the objective function which
is
to make so if suppose you are at iterate
theta T so they are at a current
parameter theta T and you can get the
gradient of this objective function and
HD is an estimate of the hessian so this
is the important thing that it does not
work with the true hessian it only has
an estimate of the hessian using the
gradient and the hessian you can build a
locally quadratic model so it says just
a second-order Taylor expansion so you
have the value of the function gradient
and the hessian but again this is the
estimate of the hessian to take a step
to update your parameters what you do is
you find the next iterate as the
minimizer of your quadratic objective
function of your quadratic approximation
that you have of the objective function
okay it's not very difficult to see just
take the gradients and set it to zero
you can immediately see that this is how
the updates look like if you start
closely at the updates you'll
immediately see one a couple of big
problems the first big problem is that
you have to do the inverse of a matrix
if your parameters if you have millions
of parameters in your model then you
have a million by million matrix and
you're trying going to invert that
matrix this is of course very expensive
the second problem is that if HT is
ranked deficient so if your if your
function is not strictly convex but it's
only convex so there are flat regions in
your function then the HT inverse might
not be well defined so these are two
problems that exist and the third
problem is that if you completely and
totally believe this model this or this
approximation this is only an
approximation this is not your true
objective function so this is not a true
Newton step so you don't want to take a
full step so first what people do is
that they say okay instead of taking a
full step let me take only a step which
I find by using a line search so a line
search is some kind of guard against
saying oh I don't completely
my model I want to actually check that
the direction that the model gives me is
actually a good direction in this
direction
HT inverse of the gradient is called a
descent direction a descent direction is
one along which if you take a step you
will there there is an infinitesimally
small step such that if you take that
step the objective value is guaranteed
to decrease this seems like a pretty
trivial thing in hindsight but in 1970
when Brian Fletcher called Ferb and
Shanno actually discovered this this was
phenomenal so they said that instead of
maintaining HT the actual Hessian what
if we maintain the inverse of the
Hessian so now you just maintain a
vector BT which is the inverse of the
Hessian which is an approximation to the
inverse of the Hessian and then now all
of a sudden if you look at this update
instead of an order n-cubed update which
would have involved inverting a matrix
it's just order n square because it's a
metric the product and then update then
of course the question or the key thing
in a BFGS algorithm is the B matrix
update so you update B by taking into
account two possible two conflicting
requirements one requirement is that you
don't want to move too far away from
your previous estimate of your Hessian
right so you had some previous estimate
and you got a new gradient you only want
to update it you don't want to move too
far away how do you measure too far away
you measure it by a waited for vinius
norm W is a easy weights on the for
vinius now and this equation is called
the second equation the second equation
comes about because what you desire is
that the gradient of the model must
equal the gradient of the objective
function at the new iterate and at the
previous iterate so if you if you want
to ensure this condition it turns out
the way to do that is to is to maintain
the second equation so the second
equation is in some sense a relation
between movement in the gradient space
so this is how much you move in the
gradient space and this is how much
you displace in the parameter space so
this is some kind of equation which
which relates these to the W is
basically where the black magic in VCS
is and what is amazing to me still is
that all four of them came up with the
same weighted matrix but if you use the
right weighting matrix it turns out that
the update for B is just a rank two
update so you do a projection into a one
small one smaller Y so you deflate into
one smaller dimensional space and you
have a rank one update and a limited
memory variant of BFGS is one which just
uses a low-rank approximation this is
what is called by MATLAB by default so
you just keep a few columns and you
approximate B by a low rank matrix okay
and of course to satisfy the line search
to do a line search typically people use
what is called a Wolfe condition more
than equations a picture gives you a
much better idea so what it says is that
you look at the one dimensional convex
function so you are given a direction
remember that that direction is a
descent direction so if you take a step
along the direction your objective
function will decrease look at that one
dimensional convex function which is
along the direction of descent here is
the gradient of that function this first
order Taylor approximation you push it
up a little bit and now your step size
should not exceed this why does this
make sense this just ensures that you
don't take too large a step size and
actually end up increasing your
objective function or the second
condition simply says that this
condition does not prevent you from
taking two smaller steps so the second
condition simply says you have to take
the gradient and slightly increase it
and you have your step size should not
be smaller than this so the first
condition ensures that you don't take
two largest step the second condition
ensures that you don't take two smaller
steps so you want some reasonable step
size now when we want to apply we have
years to machine learning problems you
have a you have trouble the main problem
is that we have gears has this
assumption of smoothness but if you look
at some of
losses they're clearly not smooth at
these locations these losses are not
small and so well we have a problem
how do you solve this problem this solve
this problem you had understand a little
bit more about gradients and the
generalization which is called sub
gradients and you had to go back to high
school at least to get the intuition so
the intuition so remember that in high
school you are taught that given a one
dimensional convex function I am just
plotting everything in 1d just to give
you an idea but of course things are
more complicated in higher dimensions
but to get the gradient of a
one-dimensional convex function at a
given location you just drew a tangent
line and then that tangent was your
gradient the other key property of a
convex function is that this gradient
line this first order Taylor
approximation is always a lower bound to
the function so you can you can draw
this gradient line and you'll always see
that the function lies above it now
let's try to apply the same intuition to
our favorite hinge loss can you produce
for me a line which is tangential to
this hinge loss at this location and
lower bounds this crease loss well here
is one line it is tangential to the
hinge loss and is lower bounding it at
every location the problem is here is
yet another line and here is a third
line so what happens is that at this
location for the hinge loss
there's not one gradient but there are
many gradients you can actually turns
out formalize this note a notion and
this yields the notion of what is called
a sub gradient so mu is called a sub
gradient at a given location if the
first order Taylor approximation that
you make with mu is always lower than
the actual value of the function at any
location okay and if you collect this
set of sub gradients then you might get
a set and this set is called a sub
differential in fact the sub
differential is a strict generalization
of the notion of gradients
cause when the function is
differentiable this set contains a
single term and that singleton is
exactly the gradient of the function and
the more even more amazing thing about
convex functions especially is that they
are sub differentiable everywhere so I
sub the a in the in the relative
interior of the domain so there's some
small technical conditions but in all
the places where you would care a convex
function is strictly so is sub
differentiable so a sub differential
exists I call this sub differential set
has these three properties which I call
the good bad and the ugly part of the
sub differential set so the good part is
that the sub differential is always
convex at any location if you look at
this set the sub differential set it's a
convex set the bad news is that every
sub different sub gradient is not a
different direction so key property of a
gradient is that if you take a step in
infinitesimally small step in the
negative gradient direction
it's a descent direction which means
that you can strictly decrease your
objective function to see that this is
not the case for sub gradients just
think of a V Channel right so you have a
V function at this location take any sub
gradient and take a step any even an
infinitesimally small step is going to
take you out of out of this point which
means it's not a descent direction this
is the key this is the major problem
when working with sub gradients in fact
the ugly part is that there is only an
implicit way or there's this one
condition that you need to check always
to ensure that some direction that you
produce is a descent direction so D is a
descent direction only if you take the
dot product with D with all the elements
of your sub differential set then it is
less than zero in other words it makes
an obtuse angle with every element in
your in your sub differential cell and
this is the ugly part of working with
some differentials when you work with
sub gradient three things in BFGS break
down the first thing is that your
locally quadratic approximation is not
well-defined okay the second part is
that the
same direction is no longer well-defined
because there's no gradient there's no
single element here so what is a descent
direction and the third problem is that
the line search the Wolfe conditions for
the line search are no longer valid so
you have to you have to modify all three
so I'll show you how to systematically
modify all three to see why the locally
quadratic approximation is no longer
valid let us look at quadratic
approximations with three different sub
gradients so I just for here I just plug
in three different sub gradients and you
can see there are three different
quadratics functions which fit at that
location this is terrible and I don't
want to work with this so what I might
try to do and this is what I'm going to
do is say let me take the envelope of
these quadratic approximations so I take
the soap over all possible sub gradients
of this quadratic approximation so I
each one of these sub gradients is
giving me a quadratic ball and I'm going
to take this soup well this is very nice
except for one problem the problem is if
you look at this approximation this is
no longer quadratic it is piecewise
quadratic but it is not quadratic and so
I just call it a local pseudo quadratic
approximation okay now to take a descent
direction to find a descent direction
you have to actually go and find a
descent direction using this locally
pseudo quadratic approximation so what
you might do is that okay so you have to
do an arc mean over theta of this
because this is your model now of how
the function locally looks like and you
have to find the next iterate one
standard trick that you can do in
optimization is you can take this linear
part add a slack variable and add it
into the constraints so this is a
standard trick that you can do take a
part of your objective function and
shift it into the constraints this is
significant because now I'm I am
changing the problem of finding the next
iterate or the next direction the next
parameter by into an constrained
optimization
problem and even more this is could be a
very large set of constraints because
you had to enforce this for all possible
sub gradients I could be lazy and say
instead of doing it for all possible sub
gradients what if I just enforce it with
respect to a few sub gradients and the
key trick is that I have to be able to
produce these new eyes
this sequence of sub gradients in such a
way that the optimization problem that I
saw this finite dimensional optimization
problem that I solve actually becomes
closer and closer to the actual
optimization problem that I'm interested
in
the the direction finding that I'm
interested in so I'll show you in a
minute how to generate the sequence of
new eyes okay and so my plan or what I
what we do is that we take this PK of D
is an lower bound to the to the
direction finding problem at step K and
every time I make this lower bound
better and better by producing these
sequence of new eyes how do I do that
this is a this is an algorithm which I
call descent direction finding by column
generation so in what you do is you
start off with an arbitrary direction as
your proposed direction of descent you
assume that there is some kind of an
Oracle so especially for hinge losses
and generalizations in machine learning
context it's always easy to produce such
an Oracle so what the Oracle does is a
given this direction it in some sense
gives you the most violating sub
gradient why is this why do I call this
most violating so gradient because if
you look at this this constraint this is
a constant and so the violation happens
only because of here so I'm trying to
get the most violating sub gradient at
every point I ask the Oracle for that
most violating sub gradient if mu K
times DK is less than zero for the most
violating sub gradient then I'm done
because I found a descent direction you
remember the condition for the descent
direction was DK times mu k must be less
than zero for every UK so if I check
this and if it is less than zero
I have a dissing direction I go off on
the other hand if that is not true then
I find my next direction by solving this
PK this approximation this gate
approximation and I keep repeating until
I find it
this interaction this kind of looks
weird because to find a descent
direction you're solving a very hard
optimization problem it's a quadratic
program and every time to find a descent
direction if you solve a QP this kind of
doesn't feel right turns out that you
can replace the QP by just doing a one
dimensional optimization problem in
alpha so this is some kind of line
search between your current iterate and
your next iterate and then you just do a
line search and you take a step along
that the good news is that we can
actually show that this algorithm
converges very fast so it converges very
fast and you can find it it's in
Direction very fast so all that is good
and the last thing that I need to fix is
the sub Wolfe divorce conditions so I
call this the sub Wolfe conditions
because I call my algorithm sub BFGS for
sub gradient enhance B appears again
the picture tells you more the idea is
that at this location if you if your
function is non differentiable then you
have many many
first ordered approximations you choose
the tightest one and then you lift it up
so that's what the first line says you
choose the tightest approximation and
then you lift it up and you take a step
which is known no bigger than that and
again for finding the smaller step you
take the highest approximation and then
you lift it up and then find the point
and so your step is no smaller than this
in fact if you go back to the hinge loss
things are even nicer if you look at any
direction and you plot this objective
function so you take any direction and
you you move along this direction this
is nice and quadratic but actually the
problem is when you
zoom in you see that the ugly parts of
the function show up so you see these
kinks in the function so this is just a
zoomed in version of the objective
function but because it is piecewise
quadratic it's very easy in fact to
characterize and actually compute these
kinks I won't bore you with the details
if you want please talk to me but the
the basic idea is that you can compute
all the kinks along any direction and
since you can compute all the kinks you
can do an exact line search given any
direction so this is a key property of
the hinge loss which we can exploit to
do an exact line search we don't any
exact line search of course satisfies
this above conditions here is a very
very simple problem to illustrate how
sub BFG house of BFGS works on a problem
so it's a particularly evil problem for
BF years so what you do is you have a
channel which looks like this along one
direction there's 100 times more
curvature then along the other one so
it's very steep along one direction and
very shy it has some gradient but it's
very shallow along the other direction
so it's got 100 times more curvature
information along one direct dimension
then along the other if you run BFGS on
it so what I'm plotting here is how does
the iterates go so I'm looking at it
from the top and I'm looking at how the
iterates go so I start from here from
some one corner and I get a gradient
direction so I swing to the other place
I get another gradient direction I swing
to here but since along one this one
direction I'm getting hundred times more
gradient information and the other what
happens is that I swing widely along
that direction
but I make very little progress along
the other one where I have very little
gradient information and so after you
see if you run it stagnates it never
gets to the solution so this is
particularly evil for BFGS you can try
one possible quick fix might be to say
Oh what if I modify my wolf candy
or I modify my line search to ensure
that if I end up close to the hinge to
these channels to these valleys I don't
actually I push my parameter away from
that so I still try to satisfy my wolf
conditions but I push my parameter away
from these hinges if I try to do that it
turns out that BFGS does convert but
very very slowly on the other hand if
you run our sub BFGS yeah it's not you
can still use BFGS as long as you never
end up at one of the hinge points
because it just yeah just BFGS out of
the box so the first one is BFGS out of
the box this that one is BFGS out of the
box this one is where we modified the
line search so we modified the line
search in BFGS 2 to say that consciously
we will try to keep away from the hinges
so when you the optimal point is on the
hinge yep so but you so the thing the
problem happens because you want to you
want to along one direction you're
getting too much gradient information
and so if you get to the hinge along
that direction then you no longer make
any progress
so only along that direction we are
trying to keep it away from the hinges
this is just a line search which says
that if I have a point which can keep me
away from the hinges then I will
preferably go to that point that's all
so it's just a simple line search with
one modification with one extra
condition
so it eventually goes to the solution
but it takes a long time so it just
keeps on so you know if you look at it
it just goes like that and slowly the
oscillations die out and it goes to the
hinge that's what is happening so it's
doing this snaps from here like that
okay we can take this offline yep
and if you do if you use sub behaviors
or algorithm so at the first step you
start from here you do a you get a
gradient direction you do a line search
the line search lands you on the hinge
right at the first iteration at the
hinge our direction finding kicks in and
it kills all components of the of the
day of the descent direction along any
perpendicular direction and points you
straight towards here and again our
second time our line search kicks in and
the second time it goes straight to the
optimum so in two iterations it's
converging so it's a contrived example
but shows how where the problems in BFGS
are and where how we can try and fix
them on some real data well on some
curtin code real data how this performs
so this is again only with the with the
binary hinge loss and so we compared
against BMR MBM aramis or bundle method
solver this is the solver that alex
talked about a few weeks back okis is a
version of be eminem with a line search
so it's a it's basically a bundle method
enhanced with the line search and it's
currently reported to be the best
optimizer for these kind of problems and
sub BFGS is our algorithm and you can
see that occurs for instance has a bug
in its line search so there's a problem
with its line search so I'm talking to
Sauron to fix that but they have a bug
which basically makes the objective
function oscillate and go all over the
place BM RM starts off reasonably well
towards the in the middle game it's not
very good but towards the end it kind of
catches up so you can see this is the
zoom being lured
if you look at this is oh by the way I
must mention that all the values of the
regularization parameter that I report
are just from saurons paper so these are
not something that we tramped up he just
did some cross validation experiments
and came up with these best values and
we just use them here is another example
where we run around the Sun Reuters and
you can see one thing that happens in
most of the experiments we see is that
sub-base years actually in the initial
phases converges very very fast so the
it decreases the objective function very
very fast towards the end as you come
closer and closer to the to the optimum
it it plateaus out it tends to apply
toward a bit more on the other hand if
you look at B M R M it takes a long time
to start making any progress or start
reducing the objective function in any
reasonable way but towards the end in
the end game be M R M is pretty good so
what we are plotting here is what
happens in the end game this is some
behavior that we observe across a lot of
datasets this is one astrophysics
similar kind of stuff and similar things
on color type so to finish the
quasi-newton philosophy is to use the
gradients to build a quadratic
approximation and then take a step on
the quadratic approximation initially
this is a very good fit this is a very
good thing to do right at the beginning
because at a course level your even
though you have hinges and you have non
smooth points in your function a
quadratic approximation a smooth
approximation is not a bad approximation
to work with but as you come closer and
closer to the optimum somehow the hinges
matter so it it seems that BF years the
the sub BF years that we have is not
able to handle or is not able to work
very very well I mean it it still in
asymptotic performance is similar to be
mrm but it's not much more superior than
as it happens in
beginning and we conjectured that the
reason is a near the the optimum the
hinges actually matter they so your your
progress slows down if you're working
with a quadratic model but your actual
underlying function is not smooth and
has hinges then you have trouble the
other thing that is very important in
sub B F years is that it requires a line
search which he needs to fulfill the
wolf conditions B Eminem for instance
does not need any of these line searches
and for a binary hinge loss if you want
to do an exact line search then it's
very cheap you can just because of the
observation that is a piecewise
quadratic function you can get all the
hinge points you can do it very fast in
fact you can do it for any loss with a
finite number of classes so I mean of
all the multi-class multi-label kind of
losses it's very it's easy I mean this
is work in progress we have very
preliminary results but we have we can
manage it the big open question that I
have and I'd really like to talk to
somebody if they have answers is if you
have a structured loss so even in a
simple chain just take a simple chain
graphical model and you want to do a
line search so I give you a direction
and I tell you that you have a parameter
and you have a direction and if I ask
you to do a line search it seems rather
difficult because the aggregation
happens over over many parts of the
chain so if you have some ideas please
talk to me finally to conclude there are
two fundamental ways to use the gradient
the quasi-newton methods or the bundle
methods have done work in both today I
talked to you about quasi Newton methods
and we systematically modify them so
that we can work with non smooth
problems that arise in machine learning
and in general our solvers are more
generic and faster than hand tuned
variants so you know we want to build
more black box solvers rather than
solvers which are very very tuned to the
problem
and even then of course just building a
black box one is not you important is
not useful you have to make it fast and
it can be used in a wide variety of
machine learning applications I must say
that there is still I mean we are just
this is very recent work it's just
submitted to ICML so this is still a lot
of work needs to be done thank you very
much I think it seems a lot there's a
lot of geometry there you there's nary
method cannot exploit well uh-huh
particular you thinking about this
essentially to have some kind of
quadratic you contours on the face can't
you do a lot lot better yeah even the
most so to answer a question what we are
doing here for instance is is some kind
of interpolation between the two right
it's not a completely generic black box
solver where you just plug in and things
come out
for instance the line search knows
exactly what the objective function is
and how the objective function along a
particular direction behaves so we use
that but we are not completely tuning it
only for that particular application
you're right that if you actually had a
completely tuned solver for that I don't
know what what what you would use as
your base algorithm you would have to
start from some base algorithm so what
we are trying to do is somewhere in the
middle saying okay we know something
about the objective function but it's
not completely black box but we don't
want to be completely for that problem
so that's that's the idea we have
it just reduces to plain vanilla BFGS if
your function is smooth this it's
identical if your function is is a
smooth function so you have some log
linear model plus some quadratic
regularizer
identical to here so in that case
there's just one element in your sub
differential set your Oracle just
returns that in you're fine so you don't
have to do anything you don't have to
know that your function is smooth
beforehand so there is an active line of
research especially in optimization to
to use quasi Newton methods for non
smooth optimization typically what they
tend to do is that they of course change
the model in the same way that we do but
one key thing that the Messer one key
thing which is which is important or
good for which is you know which we
exploit a lot is this line that you can
get given a a direction right you can
find a sub gradient which is the most
violating sub gradient
given that direction this is something
that we exploit a lot what other methods
typically tend to do so another way to
think about it is that we sit on the
point of non differentiability and we
repeatedly sample the sub gradients at
that same location most of what other
people do with quasi Newton methods for
non smooth optimization is that they
make small steps outside and the sample
the gradients outside so they you know
you're sitting at a point of non
differentiability but you you sort of
step out a little bit and you try to
build your model based on the gradients
that are that you sample close to where
you are but this in machine learning for
instance tends to be very expensive
because every time you have to evaluate
the gradient at a new location and that
is very expensive here you are just
sampling the sub gradients at the same
point so you are never moving your
parameter out you're staying at the
point of the hinge and sampling the sub
gradients does that answer question oh
it's just being tend to do it it's just
this is really preliminary work I mean
I've just as I said we just submitted a
paper to ICML we are still we are still
understanding a lot more things about
the method we don't talk about some of
this in our in a paper about what the
philosophies are but we haven't done any
experiments yet thanks
so this how does this thing work
or like hinges press out one type of no
it can it can work with em unpleasant
and the last years is this people are
using the orphan twice in the urges yeah
so actually turns out that we can use
our Direction finding algorithm as a
plugin component into orphan twice lvf
years and it actually improves the
algorithm so they have a they have this
well I won't call it ad hoc but they
have one way of doing a Direction
finding
but this loop that we have this this
loop that we have actually if you plug
this into orphan twice l-bfgs it
improves the convergence of the
algorithm so you spend a little bit of
time to find a better direction the same
direction they have one way they have a
deterministic way so they always choose
one direction and just take step along
that but if you spend a little bit time
it turns out that you do much better
so in we have some experiments where if
you just run this direction-finding loop
or ten device l-bfgs improves
yeah you you know when you're when
you're when you are actually
implementing a solver you have to be
careful about all these things I mean on
the flows overflows everything happens
did you somehow evaluate this in case
you don't exactly have a kink but you
just have something that's very sharp
having a function that you think
something with time should you can you
can approach it from yeah I mean you're
right I haven't come across across a
machine learning problem where I would I
would have this kind of a scenario but
you're right that if your function is in
the limit approaching a hinge but it's
always you know you just take a smooth
function which in the limit approaches
say every function then sure BFGS would
still have problems and you would have
to modify it so but I haven't come
across a problem sorry you had a
question</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>