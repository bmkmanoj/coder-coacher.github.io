<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Adaptive Algorithms for Online Optimization | Coder Coacher - Coaching Coders</title><meta content="Adaptive Algorithms for Online Optimization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Adaptive Algorithms for Online Optimization</b></h2><h5 class="post__date">2008-03-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kioqKenKkgE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's these were visiting us from
Princeton and and
these give us is on thank you very much
yeah so please needless stay stop me in
the middle if you have any questions
it'd be better to have questions in the
middle than at the end so yeah this talk
is titled the adaptive algorithms for
online decision problems and this is
joint work with Alaska's on from IBM
Almaden so the basic framework that so
here's you know here's a sort of
motivation for this framework and what
could be more motivating than well
making money and greed so you've got
money on one side in the stock market on
the other so every day what you decide
to do is you partition your money in a
certain way amongst the various stocks
so that's a portfolio and then the stock
market reveals its numbers reveals the
payoffs or the losses on the various
stocks and based on that know let's say
you know there's some sort of a loss
that you have to pay so the loss is g1
so this happens in the first day so the
next day what you decide to do is
rebalance things and construct a new
portfolio and again the stock market
then reveals its values and you have a
payoff g2 and this basically continues
every day so the whole game is you do
something and then there's some agent
that the environment is provided so you
always make the decision before the the
environment or the adversary makes a
decision and usually we use I should
just say that we assume that no this
environment could be adversarial so just
to put this in more sort of concrete
framework suppose this is you know some
convex body and some D dimensional space
in every round you choose a point from
this convex domain so this sort of
represents choosing a particular
portfolio and the adversary then chooses
some function and what you have to pay
is the function evaluated at that point
and in the next round you choose another
point and then the adversary now chooses
another function f2
you pay F 2 X 2 so on and so forth so
the basic idea is this is going to come
from some predetermined family of
functions so this sort of represents the
stock market for example and this
represents your portfolios so in every
round this be acute BS the devil who
represents an adversary is playing some
function and it could be completely
adversarial it could actually depend on
what we've played so far and finally
what we pay is just the sum of losses
and our aim is to minimize this quantity
so note that this the set of functions
comes from some predetermined set of
functions and depending on the setting
it'll be a different set of functions
and this is some convex domain that sort
of represents our options and the total
loss is simply summation of f txt and
the main thing is that the adversary can
choose anything can be all-powerful can
actually depend on whatever our queries
are so this is the framework is this
clear so something that people might be
more familiar with is known as the
expert setting where you have a you have
n experts and the experts are basically
you know predicting some phenomena so in
each round what we do is we select a
certain expert and say you know what we
are going to predict what that expert
predicts then a loss function is
revealed and we pay the loss of that
expert on this function and again so
this is adversarial and we in the end we
just pay the total sum of losses and our
aim is to minimize that and so this
would be the sort of expression for the
loss of our algorithm and the usual way
in which in the online learning setting
in which we measure performance is known
as regret so let's say this process
continued 40 days we've accumulated some
loss and now let's just take so now
we're going to construct some sort of a
benchmark against which we want to
perform perform well
so take the T functions and compute this
this point X star which is basically
known as the fixed optimum in hindsight
it's just summer
all the functions and find the minimum
so in some sense it's saying that if you
are constrained to play the same point
every day then this is the point that
you would play and note that our loss
was simply the summation of F T X T
where XT is the point that we played in
the ti throned regret is the difference
between F DX T and F DX summation of F T
X star so simply this point evaluator
and all those functions summed up note
that X star is the minima for this so
this is a very standard notion of
performance basically it says that you
know we perform as good if we can
minimize this quantity it says that we
sort of perform as good as the fixed
point any fixed point 9 site so you can
think of this as an expert setting
suppose each point was an expert if you
can minimize this you're sort of
comparing against the loss of the best
expert and this convex body represents
some continuum of experts and the online
learning problem is to basically design
efficient algorithms that attain low
regret for these scenarios so any
questions I hope this framework is clear
I'll give a more concrete example
describing all of this but yes the
regret can be negative because we are
allowed to move so I mean so you know so
philosophically what it's saying is that
you know this is no movement but full
knowledge of the future can we with
movement compare with that without
knowledge of the future so you know just
to put it more mathematically so that
summation of F T X T summation F TX star
and the game is usually to get sub
linear regret so capital T represents
the total number of time steps and one
thing I'll just mention is usually we
assume that these functions are bounded
just say between 0 &amp;amp; 1 so why do we want
this well if you just divide this by T
that's the average regret you'll see
that as T tends to infinity the average
regret tends to 0 so what that says is
our average loss is comparative
with the sort of best fixed loss and so
in some sense this represents some sort
of convergence rate how fast we converge
the sort of the optimal loss optimal
fixed loss and note that obviously we
cannot compare with the best sequence of
points because we don't have knowledge
of the future so the best sequence of
points can be much much better and
that's why we there's this notion of
regret is defined to see if we can at
least get you know this performance
guarantee so a sort of general paradigm
or class of algorithms that works pretty
well for regret minimization is called
follow the leader so in follow the
reader what you do is you look at F 1 F
2 up to F T minus 1 at this point you
have to play XT you've seen the t minus
1 days and you play the next point and
what you do is you compute X star t
minus 1 which is just the minima of the
sum of those functions just the optimum
in hindsight just for this section and
since you know these functions you can
compute that and these are all so in the
case of portfolio management these are
all convex functions so it usually ends
up computing some sort of convex program
and you just output this as XT so it
says that you just find the leader for
whatever you've seen so far and just
play that in the next round in this in
this way presenting these methods you
assume you have the whole function yes
yes which could be in some other
settings I've seen ok you just have the
loss like that but you don't have the
fun right that's the bandit sitting as a
gear so so the main purpose of this talk
is to define sort of a stronger notion
of performance than regret and what
we're going to do is we're going to give
algorithms for that much stronger notion
of performance but it seems crucial that
we actually need to know the function
there and it would be interesting to see
for that notion of performance if we can
actually get anything in the bandit
setting and of course so XT is just the
minimum in hindsight for the first t
minus 1 and you play that to the
action ft and what were shown was for
the problem portfolio management which
I'll define in more rigor later but it's
it's sort of some kind of modeling of
the stock market and for actually a
general certain class of loss functions
you can get log t regret and this is
extremely rapid convergence because the
per round convergence is basically
Lochte divided by T so this is extremely
rapid convergence to the optimum because
you mean this is regret know the
follow-the-leader and cotton wouldn't it
what do you mean by the adversary arms
so even the one who is defining the next
function the FT and he has the knowledge
of f13 ft minus one yes so he could
construct a function f e now right that
is totally different on the rows yes but
the thing is I aim units to minimize
regret so and the thing is if the
adversary keeps jumping around a lot
then the optimum in hindsight goes up
and since we're trying to compare with
that we can also compare with that so it
so the whole game here is even though
even though the functions are being
generated by an adversary legend we can
still minimize regret so it's sort of
like the best of both worlds saying that
if the adversary was being very benign
then we could actually make our losses
extremely small and for the more general
setting and I'll describe these problems
for something on his online shortest
path in which the only assumption
usually made is that these functions are
just bounded in convex that's all you
know you get oh of route T regret which
is again it's it's very rapid
convergence but not as rapid as that and
the interesting thing is that these
regrets are actually tight this is the
best you can do so one thing you know
just a small observation is that
follow-the-leader has some sort of
convergence behavior it's not hard to
see that as T increases xt minus the
distance between XT and XT plus one
decreases because suppose you're at the
million step you take the minimum of a
million functions now you add another
function to it which is also which is
just
and you find the minimum for that that's
just going to shift by a little bit
so what follow-the-leader does is it
sort of tends to converge to a certain
solution which in our setting doesn't
really seem like the right thing to do
because you have an adversarial agent
and obviously it seems to imply that
iced tea increases the amount you learn
decreases because you sort of gain
inertia and I'll make this more formal
but essentially the argument we're
making is that follow the leader does
not adapt to the environment
as well as it should and this is not
just follow the leader but actually many
many learning algorithms they tend to
have this behavior this convergence
behavior because they're trying to
minimize regret and therefore they don't
really seem to learn things as well as
they should and let me now give an
example of what exactly I'm talking
about so let me give a concrete problem
consider this to be just the the unit
ball in in two dimensions say so in each
round we choose a point from this convex
domain the adversary the cute bsd devil
chooses a point YT so we choose XT
adversary chooses YT we pay XT minus YT
squared in other words one can just
think of the function that's being
played as X minus y T squared and this
is a sort of family of functions that
the adversary is playing so is this
example clear so it's not if you just
work out the math you can show that the
optimal hindsight happens to be the
centroid so once T points are played the
best thing to do is to play the centroid
and we want to compete with the loss of
this point in regret and so let me just
show you how follow-the-leader works
here so you start off playing an
arbitrary point x1 so obviously plays y1
so you play the average for x2 you play
the average of what you've seen so far
which is just y1 so your X 2 is the same
as Y 1 then the adversary decides to
play y2 so 4 y 4 X 3 for the next
decision you make you display the
average of these two points so you just
play the midpoint and then
you get y3 you play the average of these
points as X 4 and so on and so forth so
as you can see as time increases the
amount you move keeps decreasing it
actually decreases by 1 over T and this
algorithm actually gives all of log t
regret it's it's not very complicated
and it essentially boils down to this
that this point eventually does converge
to X star which is the centroid and it
moves very little as as as it goes as
time progresses so now let me give you a
very simplified version of the square
laws problem and show you why regret is
not a good measure of performance so
consider square laws on the unit
interval so this is just say between
minus 1 and plus 1 and let's say you
decide to play this point X 1 and the
adversary is in a good mood and plays
the same point so you incur no loss
whatsoever you so you have no reason to
play any other point you play X 2 and
the adversary also plays y 2 and let's
say this continues so for the first T
steps you just play the same point the
adversary is playing the same point as
well so note that I'm actually
describing follow-the-leader over here
so follow leader would just do this now
what the adversary then decides for the
remaining T T rounds decides to play the
opposite point say this was minus 1/2
and this is plus 1/2 now if you're
playing follow the leader
yeah before I get the the optimum in
hindsight happens to be the centroid so
just the origin so you start here follow
leader is now going to play X T plus 2
which is just going to be the average of
those two points and one point here so
it's going to move a little bit to the
right X T plus 3 is going to move a
little bit more and this is going to
proceed in this way until finally the T
2 plus one step you converge to the
optimum in hindsight so note that our
total loss is actually Omega of T
because for these d rounds were paying a
constant loss in every round and the
regret is still small if you work out
the math
but it doesn't capture the fact that we
should actually play this and then after
a while actually jump over there I mean
it seems that the right thing for you
know a quote-unquote learning algorithm
would be to play that point realize that
something is going on over there and
then sort of jump and as far as we know
there is no no learning algorithm that
could do this until and this work so
yeah before is any questions about this
oh okay
so let me now define a new measure of
performance called adaptive regret and
the way it's going to work is in the
following way I'll let this be the time
axis so this is basically just time one
time two so on and so forth choose any
contiguous interval of time so for
example J and let's compute the regret
of our algorithm in that interval which
means you sum up our losses in that
interval and then you take the best in
hindsight for that interval which could
be very different from the best in
hindsight overall so has everyone sort
of parsed this expression this is just a
regret for that interval and let's just
take the supremum over all contiguous
intervals of all lengths and let's call
this the adaptive regret and our aim is
to minimize this quantity so note that
this trivially bounds the regret because
the regret is less than the adaptive
regret and what this says it's basically
the the adaptive regret is the maximum
regret over all intervals and note that
our algorithm has to be comparative with
a different optimum for every interval
so this is a much much stronger notion
than regret and this also captures the
movement of the optimist time progresses
because you can take any chunk of time
and if you could minimize this then with
that chunk of time you would be
comparative with the optimum in
hindsight and the game is to get an
adaptive regret which is sublinear what
this means is in any interval of size
little Omega of the adaptive regret so
if the adaptive regret was square root
choose any interval of size greater than
square root T and our algorithm would
converge to the optimum in that interval
so obviously for very small intervals
there's no guarantee that you can give
because you can't predict the future
because so for an interval suppose this
was route T then this just could for any
interval of size smaller than root T
this is a triviality because all of
these functions are just bounded between
0 and 1 but they show us that for larger
intervals you converge to the OP term
furthermore as the as the interval
becomes larger the convergence rate is
even quicker yes
you know they're over but in a cute
event I'm clear the soup is not over the
circular J which is the thighs of the
integral no no interval of length of j
du Faubourg position oh no it's actually
okay the soup is over all continuous
intervals okay I'm asking for a bit more
precise definition to that okay so
starting position of intervals of a
bigger point or over all possible
intervals of always on possible
intervals of Arlen's
isn't there I mean why do you need a
loop I mean yeah you can just like the
max I guess but it's not yeah it's some
function of n by the number of into it
yeah you can though then include the
interval which can be higher time period
then yes every interval of like one
tonight yes
so suppose this quantity was bounded by
say square root T then for all intervals
of size less than square root T it's a
triviality I mean it's it's it doesn't
say anything but for larger intervals
you get convergence to the optimum okay
I guess I should be looking at the
camera so I hope that's clear so yes
there are so there are definitions that
actually do sub sequences as well and
there have been results for that but the
results are not as strong as the results
will give for this furthermore this
seems to capture basically sort of
temporal locality it yeah so that's the
but yeah but there are definitely there
are definitions that work for sure that
takes up sequences as well any other
questions
right so our game is too basic yeah
before right so we're going to give
algorithms for this which minimize this
quantity and just to give you an example
in the square last case if you look at
the interval T plus 1 to 2 T which is
the latter half of time that the cost of
the optimum hindsight is zero all cost
happens to be Omega of T because we keep
playing in that region I mean the cost
of follow-the-leader and therefore the
adaptive regrette here is omega of t
which is terrible so as far as we know
most all learning algorithms which work
for the square last case tend to give
this extremely large regret so an
intuitive way of putting it is to
basically say an evil algorithm that
keeps following the devil so whenever
the devil play is at a certain point we
want to be able to jump over there so so
let me just describe a few motivating
scenarios for adaptive regret and then
I'll give the actual statement of formal
results and the description of the
techniques we use so the first again we
go back to money so portfolio management
so the way portfolio management is
usually modeled as in each round you
play this vector VT of positive
coordinates whose sum is 1 this is
basically saying you split up your money
amongst the various stocks in various
percentages and then let's just say that
I guess it makes sense these days that
we're only talking about losses in this
model so there's a loss vector that
comes out and the LOS vector usually
says that suppose L T of 1 is 0.5 that
means the value of stock 1 has gone down
is now just point 5 of its original
value and so on and so forth so these
numbers will just say bounded between 1
and 0.5 and so actually let me let me
not go through the math but the usual
way this is model as we say the loss is
then minus log of the dot product of
these vectors and I won't get into
exactly why that's the case but this
sort of models
the portfolio management that is if you
were actually to compute all the money
that you made you could model it using
this function and obviously adaptive
regret makes a lot of sense here because
you want to sort of track the progress
of the stock market not just be
competitive with the best fixed
portfolio in hindsight so the stock
market suddenly jumped at some stage you
want to be able to change reasonably
quickly and move to the best portfolio a
more combinatorial problem that's very
interesting is the online shortest path
problem so think of there being like a
planted graph and there's a source s and
a sink T and every day what we do is
choose a path from s to T the adversary
then reveals weights on these edges and
we pay the length of the path so in this
case we pay you know six we'd pay nine
and although they were like many more
shorter paths which which we could have
taken and so here as I mentioned the the
loss is the length of PT and P star is
usually you know denotes the shortest
path in hindsight which is what we want
to be competitive with so after T days
we compute the shortest path in
hindsight and we want to be competitive
with the loss of that also implementing
follow-the-leader here is very efficient
because in each round you basically do a
shortest path computation of all the
weights you've seen so far and that
gives you the new path to play and
another way of sort of looking at this
problem is to think of there being an
expert associated with each path but
there of course exponentially many
experts and of course the game is to
choose between these expert in some
efficient way and so another so in in
the framework of adaptive regret it sort
of makes sense here that suppose you
know our algorithm started playing a
certain path or it start converging to a
path and then the adversary decided to
congest that part of the graph we like
to sort of immediately be able to jump
away from that and if the congestion
followed us we want to keep sort of
jumping away from wherever the
congestion congestion occurs and so it
says that you know adaptive regret means
avoid congestion on every contiguous
time interval and this cannot be done by
follow-the-leader kind of approach
because eventually it starts converging
to a path and then if the congestion
hits that path follow-the-leader will
not be able to move away
no I mean I guess sort of the efficient
algorithms for for this problem will not
be able to do that so we're gonna
describe an algorithm that will call
follow the leading history this is going
to be a low adaptive regret algorithm
it's basically a bootstrapping procedure
so it takes a loaded read algorithm for
that situation runs many copies of it
and then converts that into a low
adaptive read algorithm and so the main
main thing is that we're actually able
to this very efficiently so I'll
describe that more formally but the
interesting thing is that if someone
constructs a very efficient loader great
algorithm for some scenario that
immediately implies through our results
a low adaptive regret for that scenario
any questions so the thing is since it's
a bootstrapping procedure so if you have
suppose someone comes up with a very
efficient loader great algorithm for
portfolio management so we can just plug
that into our thing just as a black box
and we get more efficient and so yeah
now for something completely different
so for the ex-con cave setting this was
basically think of the problem of square
loss for example or even portfolio
management standard algorithms would
give a regret of log T and they would
have a running time let's say of our per
round R is the amount of work that they
do we have a bootstrapping procedure
that gives an adaptive degree of log
squared T so we lose a log there and our
running time per round is now our log T
so we lose a log in both of those
parameters but note that this is a much
much stronger notion of performance than
regret
incidentally we also get regret of log T
so we can ensure that as well and for
the general convex settings so in this
case think online shortest paths so
online shortest paths there are
algorithms whose running time is our R
is basically the time to compute a
shortest path in each round and the
regret is root t
we can get an adaptive regret of route T
log T and we pay a lot T in the running
time as well so we'd be computing log T
shortest paths in every round but we get
this much stronger performance guarantee
and the interesting thing is here we can
do a further optimization so if we're
willing to lose a little bit more in the
adaptive regret we can actually cut the
running time down to R plus log T which
means in each round we're doing
practically the same amount of work as
that
we're just computing a shortest path
again but we get this very strong notion
of conversions so are the results clear
because we use some banded techniques
over there here so so yeah follow the
leading history is the sort of we
actually have different follow leading
histories for both of these cases but
they form a sort of general it's the
same structure and here we do this this
optimization using a banded technique
which I'll if I have time I'll get to
later so any questions okay so I have 20
minutes basically okay so previous work
basically a little stone war move up
store muth and and we'll study this
notion of what is called K shifting
Optima that is instead of being
competitive with a fixed Optima in
hindsight how do you comparator with an
optimum that shifts K times and we use
techniques from there obviously there
are there are similarities and are
guarantees basically are stronger than
their guarantees they imply their
guarantees and also they work for only
the finite expert setting we work for a
sort of continuum of expert or even in
online shortest path where they're sort
of exponentially many experts and as
Rania CADD mentioned like other sort of
regret definitions that choose sub
sequences so those are known as time
selection functions so you can give a
function that selects some subsequence
of time but the results are much weaker
than ours for this case and there's been
a lot of other work that we borrow on
and interestingly we noticed that in the
digital signal processing community
people had people got results very
similar to us but using completely
different techniques so here's the sort
of the base
overall idea let this be the time axis
so this is just time one time two so on
and so forth what we're gonna do is
construct a room of experts and in each
round we're gonna listen to whatever
these experts predict and then make some
decision on the basis of that and the
way this is going to work is at after
time step one function f1 is revealed to
us we're going to construct the expert
II one who basically predicts FTL from
that point from the first location so
what this means is that at any
intermediate time step this expert
simply runs follow leader up to that
time step from time one and predicts
that point it's it could be not even
follow leader but any load of that
algorithm over there and a time step two
function f2 is revealed we construct
this expert e two who's doing follow a
leader from time step two and a time
step three there's gonna be follow a
leader from time step three so on and so
forth so at each time step a new expert
is entering the room and this expert is
simply a load of that algorithm from
that time step so this expert only sees
what's in front not what's behind and
what we do is we sort of combine
everything out put some X T then F T is
revealed and then of course et now walks
into the room so this is a sort of way
it's working it's it's a sort of expert
setting but where the experts keep
increasing and each round and so just to
show you that we always have the right
expert let's go back to our bad example
where we add square law sin minus 1/2
plus 1 remember the first T points were
just that fixed same minus 1/2 the
expert II 1 just does FTL in that whole
realm and keeps just placed that point
just takes the average of all those
points so do all the others because you
could take the average of 1 to T or 2 to
T or 3 to T and you get the same point
but now let's look at what happens after
the T plus 1 point is presented this
expert is going to take the average of 1
to T
the T plus 1 points so it's going to
move a little bit this expert is going
to take the average from 2 to T and then
the T plus 1 so shifts a little bit is
actually moving a little bit more and so
on and so forth
but all of the experts here all of the
first T experts keep playing something
over there at least for the first the
first point for the P plus 1 point
note that let's choose the expert F T
plus 1 e t plus 1 T plus 1 expert this
guy is gonna play this point throughout
t plus 1 - 2 T because he just takes the
average of whatever he sees from time T
plus 1 so he sees everything in front so
he plays that so I hope I've convinced
you that at least in this simple example
there's always some expert that you can
you can listen to I mean we have a lot
of noise but there is some expert who's
telling you the right thing and at this
point when a shift then there would be
another expert that you could be
listening to so now the question is you
know how do you choose from these
experts so you have these T experts and
you want to choose from them standard
approach is to do a multiplicative
weight approach which is to say that you
assign a weight to each expert this is a
probability distribution on the experts
and you just choose according to that
distribution in each step and then once
the th function is revealed you do some
update on this weight so if the loss of
the iith expert on the th step is large
you penalize him very heavily so you
bring down his probability and this is a
sort of and then you renormalize to get
a probability distribution sort of very
standard approach for choosing amongst
experts and so this kind of approach
usually says that you will be
comparative with the best expert in
hindsight but that here doesn't really
make a lot of sense so what we're gonna
do is a slight twist to this so remember
that after t steps you had t experts and
then it's the t sorry before t step i
mean you have the first t minus 1
experts once ft is revealed this new
experts walks into the room so how do
you deal with
new expert what do you what wait do you
assign to this new guy so we have
weights w1 a WT minus one those are the
t minus one experts and expert et enters
what we're going to do is simply do your
multiplicative weights on those experts
and you get some weights to the new guy
give him a head start of one over t so
you're giving this little boost and then
you sort of bring down everyone else so
that it's still a probability
distribution and basically if what we
show is that this head start is enough
and what I mean by that is consider any
contiguous interval of time J let's take
the expert EEI who corresponds to the
first time step in that we can prove and
I'm not going to prove it but that's the
sort of main technical meat that the
loss of our algorithm in that interval
is less than the loss of this expert in
that interval plus say no vlog T so
let's say we're in the square lost case
so gonna pay an additional of log t but
we get that we get this kind of a thing
and remember that E is actually a
follow-the-leader so the loss of VI in
that interval is less than the fixed
optimum of that interval plus an
additional law of log t so you just sort
of put it together and what you get is
that the loss of our algorithm is less
than fixed R plus log T so our regret in
that interval is o of log T and let me
not go through sort of the math here but
so the thing is you know using this you
get an adaptive read evolve log T
problem is you're using too many experts
it's a highly inefficient algorithm
because you need omit you need D experts
in the worst case so the question is you
know how do we actually cut down that
running time and that's where we use
this interesting streaming trick so let
me first show you the best animation
that I've got so okay think of all the
experts lined up over there and we're
sort of streaming through the experts so
there's gonna be a working set of
experts who are listening to before it
happened to be all the experts that
you've seen so far now it's actually
just going to be some subs
of them some experts are not going to be
in this in this set and now for the next
cool animation so we move from time step
T to T plus one we decide to remove an
expert so it's like this expert is is
killed basically once this expert leaves
he can never come back into the room so
we remove them and we might decide to
add the present expert that we see so
you see that I mean the working set now
becomes extremely dynamic there are
people every step there's somebody
coming in there are people leaving and
it keeps changing and because and this
is a little subtle but because we define
adapt or regret the way we define it
we're able to use streaming techniques
and it seems so natural here because
once in a streaming model once you throw
away something that you remember that's
it you've lost it forever so streaming
techniques seem very very relevant over
here so let me let me now just give you
a short description of what is the
working set at time T so working set is
simply a set of experts you've seen so
far a subset of experts so it's just a
subset of indices from 1 to T and here's
here the 3 3 key properties so s T plus
1 which is the working set at T plus 1
minus s T is only T plus 1 what this
means is that at t plus 1 the only thing
you can add is t plus 1 and you can
actually you can say i mean let's say
that we always add the expert that we
see so that's basically what this says
and ii says that essentially the set is
small so it's all of log t the third
well spread out what that means is that
it's sort of well spread out in a
logarithmic scale more precisely suppose
we're at time T so let's take s T take
any interval any contiguous interval
whose right endpoint is T so any
interval that looks like that purple
thing take the midpoint of that and
there's always going to be some expert
who's in the previous half so you take
any interval whose right endpoint is T
take the previous half of that and
have an expert in that interval for any
such purple thing so it basically ends
up looking like this think of there
being one at distance 2 4 8 16 so on and
so forth all powers of 2 and so here's
the interesting thing the definition of
the working set is completely
combinatorial you would think that the
most natural thing to do is to throw
away experts who are not performing
properly but that's actually not we do
that's that's not what we do we just
throw according to this this rule some
some sort of combinatorial rule which
does which has nothing to do with the
weights so I won't get into actually how
this is maintained but there are very
elegant deterministic constructions that
gives you a rule that from s T throw
these people out together s T plus 1 so
we can maintain these properties and
once we maintain that what we can
actually what we can actually show is
that suppose you take interval s T take
the previous half of the interval so
there's some expert et1
in s T that's just by the properties of
s T and what we can show is actually if
you take that interval J 1 note that
that expert sort of was born at the
enter the room at that step and stayed
throughout the whole thing because he
never got he never got thrown out and
because of that you can show that the
loss of our algorithm in that interval
is less than the loss of that expert
plus o of log T because that expert
survived all these survived that whole
interval and you just sort of expand it
out and what you get is that's less than
the minimum loss fixed loss in that
interval plus o of log T and then what
you can do is you can go from s to p1
and just repeat this again so what you
end up with is an expert in the previous
half of s t1 and this guy sort of
survived that whole interval J - so our
algorithm will be competitive with that
expert plus know of log T and so on and
so forth you add it up and what you get
is that the loss of our algorithm
is less than the minimum lost in the
whole interval plus log squared T so
what we're doing is we're taking the
interval were interested in and breaking
it up into sort of logarithmically many
pieces in each piece there's some expert
who survives the whole piece so we're
comparative with that expert and that's
the basic idea so we pay the extra log
because of this splitting up is this
skier roughly so we can actually get
logs quality adaptive regret with sort
of log T copies of the original
algorithm and the same works for online
shortest paths as well the math looks
different but we get like we get o tilde
of root T regret with locti copies so I
mean if you want to maintain this log T
kind of thing the earliest expert will
probably leave out some chunk of the
interval yeah less than half so that's
essentially what's going on I mean the
earliest expert will probably not be the
first point of that interval it'll be
something who misses that so you have no
guarantee on that so for then you have
to analyze that part so in which you
maintain the expertise roughly
exponentially increasing away from your
right hand right how do you you also
guarantee that the intervals that are to
your left are are covered I mean they
have the same property I thought it only
held four intervals which should start
on the right right you're right but now
let's shift the picture twos time step T
one okay and then this property was from
there okay because when we want to
analyze the losses from T one to T then
we look at what happened then we look at
you know ourselves a t1 oh it's gonna
analyze the time what happens from t2 to
t1 then we move to t1 okay so what
actually sort of okay so you don't have
the best expert
at a tea you don't have the best exit of
experts from s2 fever no you don't you
dropped him you drop them Oakland you
have the best excellent from t1d look I
mean the thing is essentially at any
time step if you look at some expert
then you have you know you that expert
survived from when they came in to the
point that you have it's as long as that
thing is more than half of the thing
like recursively you just keep splitting
it so it's it's a sort of cute streaming
drink this essentially and so yeah maybe
I'll just have around five minutes or so
right essentially so yeah I'll just go
through this alone quickly so for online
shortest paths you basically computing
log T shortest path computations in each
round and the interesting thing is you
can cut this down to just a single
shortest path computation so they're
actually going to be over locti copies
of the algorithm but we're not going to
sort of use all of them in each round
and this works for general convex
functions are this ends up giving an
extra additive regret of root T and
that's why it's not good for portfolio
management or for Foursquare loss
because you know this is too much for
those scenarios and the basic idea the
way it works is think of the working set
these are a bunch of experts each
predicting a certain path usually you
choose some expert and you basically
play the path of this expert and then
the edge weights are revealed once the
edge weights are revealed you have to do
this weight update this multiplicative
weight update and for that you actually
need to compute the weight of all of the
other experts as well so you need to
sort of evaluate you need to compute the
shortest path that all of these guys are
predicting as well and that gives you
the log T shortest paths so what we're
going to do is just we're just going to
compute the shortest path that that
expert predicts we're just gonna do FTL
for that interval that expert and we're
going to imagine that the payoff is
actually only revealed to that given
expert which is sort of what you were
talking about like a bandit setting so
even though that's really not the case
here we're just
imagine that that's what happened and
this is exactly what is known as the
bandit setting in which you only
revealed the loss of your chosen expert
and the interesting thing here is it's a
really amazing thing of ours as a
Bianchi flourish appear a that if you
have K experts you can get regret of
route KT even though you don't even know
what other X what the other experts are
getting and here because of our expert
reduction we only have log d experts in
each step
although these experts keep changing in
each round we only have log t and you
push the mat through what we can get is
actually that our regret is also going
to be this o tilde of route T and this
is through only one shortest path
computation so note that the bandit
techniques are used for reducing the
time it's not we really are not in that
setting and so let me just so just to
summarize what we defined was this
notion of adaptive regret it's a
generalization of regret that captures
sort of moving solutions and it means
that we converge to a fixed optimum in
every every interval of sufficiently
large size and it's a bootstrapping
procedure that converts low regret in
the lower apt of regret and the
interesting thing is if you talk to
people who actually run these portfolio
management algorithms one question that
always ask is how much of the history
should you remember to get a good
algorithm and what what we're doing is
you know we're sort of maintaining all
histories together in some efficient way
so it's sort of dealing with the
question of how much history needs to be
remembered and so just to end further
directions I think the first thing that
should be looked at is whether you know
these sort of streaming or sublinear
ideas can be used to get efficient
learning algorithms and it seems like
something that people have not explored
before and I think of you know the
following scenario where you want to
have a learning setting where you have a
cost of movement so makes a lot of sense
in something like portfolio management
the moment you change your portfolio you
pay some cost for that and now it's a
completely like in a non online
algorithms problem which is you know you
want to be competitive not with the best
point you know the best moving point
but rather with the best strategy
overall so can we use these techniques
to get some kind of competitive ratio
bound for those settings where you
induce a cost of changing your solution
from one to another and it seems like a
really interesting thing to do then what
actually we're trying to work on this
but I mean the main thing that III want
to say at the end is the most important
thing here is not the fact that we okay
we got some adaptive regret bound but
rather that we're using sort of some
bunch of techniques to get efficient
algorithms to capture moving solutions
and this seems like something that has a
lot of use in many other places so it'll
be nice to see you know where else we
can use these kind of things and with
that I'll just end the talk</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>