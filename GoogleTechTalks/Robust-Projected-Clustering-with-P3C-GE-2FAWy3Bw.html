<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Robust Projected Clustering with P3C | Coder Coacher - Coaching Coders</title><meta content="Robust Projected Clustering with P3C - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Robust Projected Clustering with P3C</b></h2><h5 class="post__date">2008-03-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GE-2FAWy3Bw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay welcome to the tech talk I am
taboot Rafi I'm visiting scientists in
the search quality team and I am pleased
to introduce York sander who is visiting
us from University of Alberta York has
done some interesting work in the past
on clustering saw there were papers I
mean his papers at the well site at TBE
scan and optics or like well known in
the area today he's going to tell us
what he's up to more recently what one
thing i have to remind is that since
this is an external visit and the talk
is going to go to youtube so you don't
want to discuss the details of your
projects i mean just some edge reminder
okay with that so i don't leave it to
your food okay thank you very much the
hood i'm going to talk today about
projected clustering the way we have
done it recently so let me give you a
kind of an overview of the of the talk
so before i start describing what we
have done which will contain also an
experimental evaluation i briefly will
introduce the problem or give some
motivation what is this projected or
subspace clustering and i will also
discuss some of the prominent approaches
that have been proposed or pick one of
the approaches / kind of category will
briefly describe it so that you can see
how this relates to other work that
people have done before us so my
hopefully only 30 seconds introduction
to clustering i'm assuming you know a
little bit what that is it's about
finding groups and data so it's not a
well-defined problem and there's two
major intuitions if you look at the
literature about clustering you could
categorize them and approaches that
whether your intuition is that
clustering is a petition of a set of
objects
that objects within one class are more
similar or more related to each other
than objects from different classes
typically approaches that fall into this
category try to define an objective
function and try to optimize that
function finding the partition that
optimizes that there's a different
intuition some argue maybe that is not
even clustering I've had discussions
like that I've worked I've done work in
this area of clustering where you assume
that the clusters are have dense areas
in a data space separated by regions of
lower object density so if we look at
this picture here that illustrates in 2d
some of the cases what that means here
you would argue according to the second
definition this would still be pleased
you would still call clusters but not
according to the first definition
because obviously if I allow this to be
a class as obvious no limit on how the
similar objects within a cluster can be
in given applications typical usages
it's kind of related to density
estimation you wanna get some insights
into the data distribution
pre-processing step for other algorithms
can think of this also as a data
compression if you replace the data with
the clusters that you found later to do
follow-up analysis now what is the
problem why do we need something like
projected clustering no I'm sorry I
still have to explain this slide here
which shows that virtually all the
methods even those based on this notion
of point density use some kind of
dissimilarity function similarity or
dissimilarity distance function that
typically considered all the attributes
of your data after maybe some
proprietary method that you've applied
before but once you have a set of data
you assume all these attributes are
relevant for all the clusters that
you're looking for and here's just some
examples of distance functions for
numerical attributes like an LP norm if
for p equal to g euclidean distance
everyone knows that here's something
based on the cosine of the angle of the
two vectors if x and y are Victor's in
this case for categorical attributes and
so is an example here the simple
mismatch coefficient just counting how
often do they disagree or four sets if
your objects are sets you can do
something based on a jacquard measure
intersection over Union as a distance
what you want- that yeah however if we
do this in all dimensions of a data set
there's a problem when the
dimensionality increases one of the
results that was obtained here in the
database area is that as the
dimensionality increases under very
general conditions on data distributions
in high dimensional spaces then in the
context of query data so you have a
career you want to find nearest
neighbours of that Creek for instance it
can be shown that under those very
general conditions the difference
between the maximum of the minimum
distance of objects to that query
vanishes yeah so that means that
clusters become less and less meaningful
the higher dimensionality on an
intuitive level you can also argue well
yeah if things because if i look at
similarity between things and they can
now very in a thousand different aspects
can be similar or dissimilar and
aggregate all of those so what does it
really mean so an object is they may be
more similar with respect this subset
but then dissimilar here another one
it's the other way around and so but if
I aggregate us all over all the
attributes every object is approximately
equal e similar to your query so this is
on an intuitive level there's other
things going on in high demand
spaces that are collectively referred to
as the curse of dimensionality some of
you might be familiar with other aspects
in that you encounter in your work now
and so this is one of the reasons why
people have started looking into
something like projected clustering
especially also in the context of
nowadays the possibility to just collect
data almost blindly because we just can
clustering has been studied in
statistics for a long time and this
wasn't really a big problem because the
data was collected based on an
experimental design that had to put a
specific scientific question in mind so
you were selecting already features that
were with high probability relevant for
your task whereas in many applications
nowadays you just have instruments that
allow you to measure sensors and you're
collecting all this data and afterwards
you want to find groups of data or
whatever and now in a much larger set
where you don't really know what is
meaning what is a relevant attitude what
is it irrelevant attribute yeah so even
if you assume here's one case how
irrelevant attributes if we just for the
moment assume what we know what that
means saying that we have here two
clusters in two dimensions so if I
somehow would know these are the two
dimensions I collect the data a
clustering algorithm may be able to find
those now if I don't know these are
relevant I just managed to get a
superset of those attributes and many of
them that are included in that set are
irrelevant like here is one ad three in
which the values of that attribute for
the objects are kind of random that will
kind of dilute the clustering structure
the more I are the more diluted this
whole situations is in feature
transformation global feature
transformation can often not solve this
problem which is why people have looked
at projected clustering
now my motivating example is not text
data because we haven't been successful
well we haven't done it yet because of
the there's challenges in the data
preprocessing with ticks that the
students and I haven't been avoiding so
far so motivating example is gene
expression data so who knows what gene
expression data are or who doesn't know
gene expression I try to give a very
brief brief explanation what that is so
the cells they produce proteins to do
whatever tasks the cells of your body's
due to produce protein it has to be
assembled from from like chemicals that
are in the cells and the instructions on
how to assemble it I encoded in the DNA
I mean so much this kind of knowledge
about that I'm assuming everyone has so
now to get the information how to
assemble a gene from that from the DNA
to the kind of factories in the cell
that actually make that protein the
sequence that encodes the protein has to
be copied translated and so transcribed
and then these snippets that float
around and to the factory to make that
protein you can measure how many of
those float around and they kind of are
and measure in a sense that's the
Assumption they measure also the
abundance of that protein but it
measures at least how many of these gene
not being coding regions are floating
around of a particular time they easy to
measure in large quantity so you can
measure thousands of genes typically on
these microarray chips and what you what
people are doing is they measure the
levels of different genes under
different conditions and conditions can
be different patients it could be
different tissue types like heart liver
it could be different experimental
conditions that you say take the same
tissue and expose some of it
to certain chemicals others you don't
consecutive time points when you want to
do something like cell cycle etc now the
problem with this is that this very
high-dimensional if we want to cluster
the samples in some sense because the
genes are in the order of thousands
whereas the samples are maha are
difficult to obtain and I was even with
here this order of magnitude optimistic
that you only have that in some of those
cases often you have even less than that
now clustering you can apply clustering
to the rows or the columns in that
matrix but in any case what makes this a
difficult problem why cluster analysis
is difficult you want to find groups but
it's not that simple genes are involved
and more than one of the process of
functions does this the same gene
encodes a protein but that protein is
involved in different functions so
similar expression it doesn't
necessarily mean similar function it
could be expressed for different reasons
I sing a condition what is called the
condition is often defined on a rather
high level they are like a doctor
looking at a patient and saying oh has a
big tumor this one is a small tumor or
it's based on like some phenotype that
may itself already be a condition that
represents several different subtypes
only a small number of genes relative to
the total number is involved in any of
those processes any of these specific
processes and only a small number of
genes could characterize the different
condition so what does that mean it
means that what are interesting entities
to find are actually subsets of genes
and subsets of those samples now that
are related to each other now and this
is what kind of motivates this idea of
projected and sometimes called subspace
clustering in the database community
there is another notion just a one
not something called subspace classing
that it's related but a little bit
different than this approach where he a
subspace means really subset of
dimension subspace could also mean that
you're looking for kind of a manifold a
lower dimensional in the data no okay so
yeah so it's different from full
dimensional clustering that data may
form clusters only in subsets of the
dimensions this is basically looking
trying to get rid of your relevant
dimensions for certain conditions if you
want to find those subspace clusters may
exist in different subspaces that's all
so different from full dimensional class
pencil you may have a group that is
characterized by one subset of
dimensions another subset of dimensions
characterizes which in this example
maybe another set of genes characterizes
the process a different process these
subspaces may overlap in terms of
dimensions and data objects which makes
this actually a very difficult problem
and so there's major approaches that
have been not proposed recently i'm only
going to talk about the first one in
more detail the second one i will
mention as well but not in that detail
first one is simply called projected
clustering and the second one is density
based subspace clustering and then
linear nonlinear correlation subspace
clustering and this finding manifolds in
this place it would also be a category
there so what's the first one in the
first one first intuition a projected
cluster is simply the pair of two
entities were two sets one is a set of
objects the other one is a set of
dimensions and the idea here is that the
points that are in the set C I are
closely related in the subspace formed
by these dimensions in deaja that's
vague and we will see that the different
approaches
have to make decision of what that means
closely located are closely related so
the problem would be to petition our
data set in 2k such projected clusters
or to find a number of K of those
classes here this is kind of the
illustration again I'm showing here in
2d we have this cluster if you go to 3d
there's an irrelevant dimension here and
you only want to find but in this case
to characterize this set as a projected
cluster the sea would be the set of
these objects and there's no other
objects here and D would be only
containing these two dimensions not
awfully now this density based subspace
clustering is an idea that is based on
the second intuition of clusters where
it assumes that points in the clusters
are now located in regions of higher
point density than in the surrounding
area but only in this subspace formed by
the dimensions in DI I try to illustrate
here only the two-dimensional projection
of the 3d and try to illustrate that now
the closely located is no longer really
necessary they can be far away as long
as there is a region that has a kind of
a boundary that you can detect so this
these approaches try to find now all
subspace clusters in all subspaces with
respect to some density parameters and
plus possibly outliers now let me just
go through a few of those approaches so
that you get a feeling for what has been
done I think the first one actually is
this Pro class that has been proposed
it's a key Metroid type algorithm if you
know K means it's it's very it's try
it's it's inspired by a full dimensional
clustering algorithm excuse to eat
rative lira fine
an initial petition of improving some
objective function in this case the the
methods are objects that are kind of
central in a cluster represent the
cluster so what the algorithm does I'm
just giving you a very rough idea i'm
not going into the details too much so
for each cluster so somehow it is in
english initialization so you have
already somehow clusters in the
beginning you have to pick them using
some method and the paper proposes some
some heuristic for doing that you use
the point to select attributes so if you
think of the problem finding projected
classes if you would know the points of
a projected cluster if it's truly one
that satisfies this in this intuition it
would be relatively easy to detect the
dimensions you look at the distributions
and a different dimension you pick the
ones where that satisfy however you
formalize now this idea of being closely
related and others that don't satisfies
you it will declare it's irrelevant now
this is basically assume you have
already points somehow then you can do
this and this paper proposes the ones
that to choose the ones with the smaller
standard deviation as relevant and then
once you do that you have kind of this
subspace and you can now measure
distances in that subspace and you can
recompute distances now of coins to the
central points in those clusters using
their subspace and assign them to the
closest that gives you a new set of
points now and so you can iterate at
this point you could do again well maybe
we have to now refine the attributes
because you have now possibly new points
you have to do you distribution there's
something here goin on that if clusters
have start having too few points because
they don't in their subspace there's
nothing close anymore you throw them out
and replace them with other ones and so
it goes on until it's terminates
according to some criterium there is an
objective function that is trying to up
to be optimized here parameters are the
number of classes in average projected
cluster dimensionality so the problem
when you try to cast this problem in the
way that you optimize a function with is
that in this paper isn't particularly
successful in that the objective
function may have a trivial minimum
that's why you have here this other
parameter it tries to avoid clusters
having only one dimension that's why you
have here not the innocence in addition
the average cluster dimensionality kind
of prevents clusters from from having
too few dimensions and because you can
show with the objective function without
this parameter it would have an optimum
where each cluster has only one
dimension there's another version this
this is basically one approach that this
inspired by this partitioning approach
there is another approach to project a
clustering which formalizes this idea
differently so in this case we the idea
of being close is simply translated into
a threshold on the distance between
points yeah it says that sorry they have
to be contained in a hyper cube of side
length w so that's and it has to be at
least an alpha fraction of the points to
be a protocol to projected cluster and
then there's a quality function which
kind of combines the number of points
and a number of dimensions in some way
to tell if you if you show me what to
that I can tell which one I like better
and then it tries to find one cluster at
a time now the one entity called
projected cluster in this ends at a time
by picking a point picking a set of X
points randomly and then checking for
which dimensions all these points that
I've selected randomly live within
w around that initial query point or
seed point these are my relevant
dimensions and then once I have the
relevant of it I take the intersection
of all these intervals so I take all the
points that of that fall into that cube
in this dimension analogy and if i have
enough then i have already a something
that satisfies this condition and i do
this several times and then I report the
one that has the best quality in this
process there's some guarantees it may
look a little bit weird when you think
about it but the paper shows that there
are some guarantees if there is a
projected cluster in the sense you can
actually find it with high probability
with this approach but so here's other
approaches the one on the bottom
actually recognizes that if you want to
solve that you can actually help a
deterministic algorithm based on if you
cough this as a frequent itemset mining
problem you can find the best one with
respect to this measure directly you
don't have to do this randomized
approach okay so this is was one another
one if you know clustering a little bit
the the landscape is hierarchical
clustering as well now of course you can
try to do something hierarchical this is
the method hop it's doing something that
the all the hierarchy almost all do they
start by the agglomerative algorithms at
least they start by having each object
and in their own cluster and they start
merging and so closest pairs know in
this case the condition for merging is a
little bit different it looks at how
many relevant attributes it has and the
relevance of an attribute is based on
complex function that compares basically
the local and the global means and the
variances of the points in the cluster
versus all the points
and in that subspace then now there is
two parameters so it says well they
merged if there is at least demon many
dimensions and if this relevant score is
it is greater than this Armin and those
parameters change during the iteration
so it starts with various being very
restrictive in the beginning and it
relaxes or at the end and builds builds
a hierarchy until it has a certain
number of clusters now that's another
approach the the one here I mentioned
only because we have it in the
performance evaluation and it performs
among those that have been proposed
pretty much the best it uses the same
objective function or the same relevant
score as harp but is based on an
algorithmic scheme that is rather like
like pro class the one that tries to
find an optimal partition ok i'm
thinking that i will skip the density
based subspace clustering algorithms
here and move on to our own work after i
briefly mentioned here the limitations
and some of the limitations that we try
to overcome in our approach so they
depend on the parameter settings which
well it's it's not necessarily a
limitation sometimes its desired to have
certain parameters that you can tune but
there's parameters that are really hard
to set in some of these algorithms the
Alpha and the better better how do i
combine number way number of attributes
versus number of points in a projective
class that's kind of difficult to know
what to do with these algorithms in the
experiments that turned out that they
really have a hard time finding really
low dimensional cluster clusters with
only a few relevant attributes the
algorithms ideally should be able to
detect the number of clusters I'd
automatically and something that
has to do with the way these algorithms
compute certain measures they make them
only suitable for for numerical data
there's only few works for categorical
data the work that we are proposing can
be extended I don't think I have time to
talk about that extension here though ok
so it's called p-3c I think it stands
for to a projected clustering with
cluster course there's the three C's in
there yeah so what we try to do is
something that is robust with respect to
parameters it has a small number it has
one parameter in essentially it
discovers very low dimensional subspace
clusters embedded in high dimensional
spaces its robust with respect to noise
it's scalable with respect to the
database size unfortunately not so much
with respect to the subspace
dimensionality so small really means
here absolutely small not relatively
small to the overall dimensionality now
it can allow overlapping clusters and is
extensible two categorical data so i
give you the overview first and then
talk about the details later so the way
we try to find projected clusters is we
try to find something like what we call
a cluster core like a big chunk of
objects that hopefully belong to one of
those projected clusters if you can do
that then all the other tasks become
easier you can refine this it's easier
to detect the relevant dimensions etc so
but and we do this in a way that we
start with one dimensional projection so
we look at the distribution in the one
dimensions and we try to find intervals
that somehow could correspond to
projected clusters somewhere in higher
dimensional don't have to be just one
there could be because you're looking at
very low dimensional projections could
to be an interval that contains two
clusters in some higher dimensional
space or something like that but taking
those one-dimensional intervals on the
attributes we trying to determine
cluster course which are sets of
intervals from different dimensions
hopefully then representing only one
cluster at the end and refinement of
these cluster cores can be done then
with a standard clustering algorithm the
expectation-maximization clustering
algorithm outlier detection is optional
can be done after refinement and once
you have clusters identifying the
relevant add dimensions becomes very
easy so first one dimensional clustered
projections what do we do we basically
divide the interval into bins which
we're trying to do a statistical test we
check is other the values and that
attribute randomly distributed or not
and if they're not randomly distributed
we look for the regions of high density
idea being that if they unusually high
they contain possibly projections of
clusters so in this case the outcome
will be the red arrows here so on this
attribute you find these two intervals
on this at will find this in interval
using a simple choice square test now so
how is that done it's done bin by bin so
when we look at non-uniform we look for
the bin with the highest density we mark
it and then we repeat the test with the
remaining attributes to collect all
those bins that in the end have
unusually high support and then we merge
adjacent interval bins into intervals
okay having those intervals we want now
to combine intervals from different
attributes to cut out chunks in higher
dimensional spaces now how can we do
this so if we assume we have already a
set of dimensions of some intervals in
on those dimensions when do we add
another interval
is not yet in there there has to be some
evidence that it represents the same
cluster to measure this type of evidence
we're using the expected support as
computed as following when if we assume
that we know the actual support support
means number of points in that region we
know that now assume that this dis
interval or the dimension where the
where the new interval lies on is
irrelevant irrelevant in our model means
that the values are randomly distributed
that means if we add this interval from
this we would only expect proportionally
to the widths many points to remain in
the set in the support set so that's
what it means here if we take that said
only proportionally many to the width of
that new would fall into the set where
we add this interval no that's the
expected suppose under the assumption
that this is irrelevant and the relevant
means randomly distributed now we can
say that now evidence that s represents
the same cluster exists if the expected
that the actual support is significantly
larger than the expected support how do
what does that mean significantly larger
the way we model this was we say of
course it's larger but then how much
larger now we model this as we assume
that's a that there is a personal
process if you were with this expect
with this expectation now when you have
a personal process yeah so with a
particular expectation this distribution
basically gives you the probability of
seeing this actual support when this is
the density of the process and we give
this is the parameter that we introduced
in the system that we have to set this
to some small value that we want to have
a very low probability of actually
seeing this is the number of points that
we see once we add that you an interval
versus the the
one that we would expect under a uniform
distribution okay so that's how we
measure the the evidence now what is a
cluster core class decor is defined as
the set of intervals that consists of
only and all the intervals representing
the projections of the same cluster but
in the following sense it means not this
there is a way of adding intervals in
one particular order so that I get a set
where I had in each step evidence for
adding it it has to be we define this in
a little bit of a stronger we say that
for every subset of this is it must hold
that there's evidence that adding the
other another one in is that it
represents the same cluster in the sense
as defined on the previous slide by
doing this this is a little bit stronger
than may be necessary but it allows an
algorithm which is a little bit more
efficient than exhaustive search those
of you who know how to find frequent
itemsets now there is this algorithm
called a priori this algorithm can be
used because this satisfies a downward
closure property so if a set does not
satisfy this condition no superset can
satisfy it so I can in a search space
exclude that whole branch now from
search and the second condition is that
for any interval that is not an s so
this is kind of a maximum ality
condition when therefore cluster core it
contains all of those so there is no
interval for theirs for which there is
no evidence that it represents the same
cluster okay so having these
plus the course with an a priori like
algorithm once we have them we can
refine them by simply but let me back up
so once we have cluster course they're
basically defined as by some dimensions
as it's a hypercube in space plus the
points that are in that hypercube so
giving those points we can now assign
other points by estimating a mean and
the covariance matrix of that's set of
points and then we can use Mahalanobis
distance to see which point of the
remaining points where do they fit best
now and once we have that we can use
like an e/m clustering algorithm to just
iteratively refine those cluster course
and afterwards derive either hard or
soft petition harper meaning we assign a
point only to the most likely cluster
and most probable cluster and the soft
petition would allow membership in
several clusters if the probability of
belonging there is greater than 1 over K
after we're done with that we can do
multivariate outlier detection meaning
we can throw out points that have very
low probability of belonging to any of
these clusters and so as I said already
once you have clusters you can basically
throw out irrelevant attributes again by
looking those that look uniform are
irrelevant we only keep those that are
non uniform in the end okay now so the
last part will be just showing some
experiments that we did here on
synthetic data that we've generated
according to a generator that was
proposed in these papers that i
mentioned like pro class etc so we have
10,000 objects we have 100 dimensions we
have an extension for categorical data
that's why this is still on here took
out the slides because
is not enough time to talk about it we
implanted five clusters having fifteen
to twenty-five percent of the points
five percent outliers and the
distribution within the clusters we have
two categories one way we distributed
them uniform in a little box and one
where we distributed them now on a
normal and like a multivariate normal
ellipsoid somehow and we have another
set of another distinction in the day in
the experiment where we have when it's
access parallel versus rotated that's
something that has to do with some of
the algorithms make that distinction and
specialize on these that rotate it
clusters okay so that's it is just the
setup here for the for the catechol for
the synthetic data the total
dimensionality is 100 and the subspace
clusters a good question is not on the
slide not larger than 10 that I remember
I don't know the actually actual and
unless we start bearing there's one
experiment where we actually increase
the dimensionality but for the other
where the subspace cluster
dimensionality it has to be constant
it's probably something like between two
and ten different cluster
dimensionalities okay so we have as
algorithms that we compare ourselves
with the ones that I mentioned here plus
other ones for categorical data it is on
those slides I won't talk about it
because I've decided not to present the
extension to categorical data when we
look at the results of the other
algorithms we have to set their
parameters so what we did was for those
values where we know the supposedly true
value like number of
clusters average subspace cluster
dimensionalities things that we could
compute we set the values to the true
values but there's numerous other
parameters that have to be said and we
tried several based on the heuristic
suggested by the authors etc and and
played a little bit and yeah chose at
the end the best results we could get
for those for the accuracy since we
generate the data ourselves we have kind
of labels and allow us to measure with
an F measure the DD which is the f1
measure lector the harmonic mean between
precision and recall so we first for
each cluster that we find we try to find
which of the one is the best match and
from that we can basically compute
precision recall and then overall the
average there's over all the clusters
and over five runs we get the numbers
that are presented on the following
slides he is the first slide is just
showing that once you set this threshold
for the Poisson probability to a small
value there is no influence that this
parameter would have any more on the
cluster quality that you this shows kind
of the robustness to this parameter and
here we have now this the quality of
these pictures is worse than I was
hoping for we see now results for the
accuracy for different category when you
look at the heading here category
uniform equal parallel means that we
have the points in the clusters
distributed in a uniform way all
subsidies class they have the same
number of dimensions as the same
subspace dimensionality and they access
parallel so we didn't rotate them the
other category at the other extreme
combination where we have normal
distributions and the subspace cluster
dimensionals are different so one
subspace class had only two
the other one might have had 10 and they
have been rotated we can see that like
the overall behavior of many of the
algorithms is similar or let's say they
fall into two categories those that
eventually start working in those that
seem to never really work hours are here
that the blue and the red one one is for
hard partitioning the other one is the
soft petitioning variant and we can see
that those that work start working once
the cluster dimensionality this is f1
measure were measured here for different
average cluster dimensionalities once we
reach here twenty percent of all
dimensions those algorithms that work
well that work work well but this is
also the case twenty percent of the
dimension that's when the full
dimensional subspace cuz has also start
to work now they will be able to
recognize those clusters they in the
full dimensional space as well now I
just want to give you a likely the
overall impression I don't want to
discuss all the details here what what
the point here is that in those
experiments in those cases where we that
we wanted to improve we improved over
the previous work now finding the
substrate casta when the subspace
cluster dimensionality is very low and
this is for categorical data I'm
skipping this its robust to outliers
that's what what these figures show if
we increase the percentage of outline
the result stood here it's somewhat but
not dramatically and it scales well with
respect to the database size but not so
well with respect to average subspace
cluster dimensionality and reason is
that it has this a priori type algorithm
which
like for frequent itemsets you start
combining one element sets and into two
elements etc and you can only exclude
something that does not satisfy this
condition because you have this anti
monotonic property meaning if you have
something that satisfies the condition
that doesn't satisfy the car no
substance that can satisfy but it also
means that if I have something that
satisfies it every subset will satisfy
and I have to work my way all the way up
to this so it's exponential in the
dimensionality of the largest subspace
that I have a class to end the runtime
okay so we did some experiments on some
real data that's often a little bit
difficult to find good real does it
would mean go to your data real data
where you can actually measure the
quality of what you did because you know
you need something like labels so to
compare this to a kind of a ground truth
and so we picked some data sets from
from gene expression data colon cancer
data set where we have two classes two
more and normal colon tissues two
thousand human genes and we discover two
clusters each had a dimensionality of 11
and here's the accuracy of other of all
the algorithms another data set had two
classes we only found one projected
cluster of dimensionality to with an
accuracy of 68 but i think if we look at
the numbers that it's really not much
better than random visits that's about
the percentage of the one class that you
would find rad by by randomly picking
points okay so conclusions well its
robust with the respect to the single
parameter it's suitable for categorical
data we have haven't really discussed
that and it allows us to discover very
low dimensional projected clusters yeah
but is has unfortunately an exponential
running time in the largest cluster
dimensionality so it's suitable only for
low dimensional subspace cluster in an
absolute sense of law okay so i want to
mention that also this joint work with
Gabriella Moises one of my PhD students
and Martin east from Simon Fraser
University and if you want to read more
details and results are in the paper
cited here on the slide ok thank you
very much and if you have any questions
okay okay well I guess I have a last one
question i mean i can see that this is
for multi-dimensional data and one
question is that if i want to take
something like this and apply it to text
like what are the issues i mean are you
expecting i mean this thing to work on
text or if not i mean like why not well
i'm i'm thinking that the ideas of a
projected clusters are relevant for text
clustering the problem that I see he is
rather that the dimensionality it
depends on how you represent the text
yeah but if we say like word frequencies
or the individual words are my dimension
then I have really a huge dimensional
space and even the sub spaces in which I
expect them classes to exist will be
rather large with a suitable
pre-processing if you look at rather
than individual words maybe in grams or
something maybe it's doable I haven't
tried I think it's probably well let's
just put it this way another thing that
might prevent applicability and this is
kind of well something that wasn't
probably very clear in the in the
presentation there is this assumption of
what is the new relevant dimension and
that is assuming that the values are
randomly distributed in irrelevant
dimensions in real data this at least
data sets we looked at this is not often
not the case so in real data what you
find is that the irrelevant dimensions
have some distribution often normal yeah
instead of random and so so that's one
of the things this makes assumptions
about the distribution we are working on
on relaxing that and and that hopefully
will broaden the applicability of this
approach and if it takes I don't know
how these
the frequencies of words are probably
also not random in a way so that answers
the question I hope</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>