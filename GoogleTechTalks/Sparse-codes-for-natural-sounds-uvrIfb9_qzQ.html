<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sparse codes for natural sounds | Coder Coacher - Coaching Coders</title><meta content="Sparse codes for natural sounds - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sparse codes for natural sounds</b></h2><h5 class="post__date">2008-05-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uvrIfb9_qzQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">um maybe today she is at the record
Center both you know neuroscience at UC
Berkeley and also at the ninth brain and
computation Center at Stanford and she
did her PhD with mike libecki at STM you
and also spent two years of her life
racing and mosques for consumption sushi
we have a little seashell yeah thank you
yeah it was hard after spending that
time with with with the snails not
become interested in human cognition
parallels are just obvious I like kelp
so I'm going to talk today about
research a large part of which I did for
my dissertation and then maybe some
teasers on some other bits of stuff and
this includes some work which is
currently unpublished but hopefully
should come out sometime soon psych
science so I you know it's always
difficult for me to gauge my audience so
please like jump in and get a sense of
what's not enough computation what's too
much neuroscience with the flip if
that's your preference so I'm sort of an
auditory researcher with my particular
Bend although everyone at UC Berkeley is
virtually they're all virtually all
vision science people and in fact my lab
is a grad student sort of vision science
and in the world of sort of
computational perception you have a bit
of an ego issue being an auditory
researcher because eyes are sexy and
ears sort of not so much and then you
know you mix ears and science and odd
things happen but the truth is there is
something really fundamentally
interesting going on its sound
and practically interesting as well so
I'm going to talk about a particular
issue having to do with how to deal with
real-world signals in particular this
issue of sense of the color of windowing
your signals and dealing with sort of
known problems in block coding and
proposed a model for dealing with that
particular problem and then if we can
deal with the coding issue maybe we can
even go and actually learn not just say
for a linear basis representation not
just the coefficients but actually learn
the dictionary that you're going to use
to describe natural signals and then
finally I'm going to touch on some
actual behavioral experimental work
which was an outgrowth of this
computational research so let's just
start with coding sounds and you know
working with the time-varying waveform
that is here on the left is difficult
there's a lot of wonderful information
there in fact all the information you're
ever going to get about the sound is
there but some sort of transformation is
almost always in order and in particular
my personal preference is is a
transformation that increases sparsity
for a variety of reasons not the least
of which is that a special distribution
implies that you're going to have a more
compact code but a sparse representation
also is telling you something a little
more interesting hopefully if you've
selected your your dictionary well
something a little more interesting
about the signal itself but there's a
problem there in searching for the white
right representation to work with in
that almost all of them work on this
sort of block coding formulation if I'm
actually going to do something like an
FFT I need to choose a reasonable window
to do it over or else
switchover entirely into a frequency
domain but there's a lot of temporal
information and sound that we actually
want to capture so for example here we
have to triplets of overlapping windows
they're all about 33 milliseconds
overlapping by 11 kind of standard stuff
having windows and this is an actual bit
of speech someone saying Vietnamese with
a little sort of strange under
articulated T right in the middle there
so what you can see if you look and
compare the FFT or the LPC spectrum from
say a1 and b1 the first red curve in the
first blue dotted curve down below is
that they basically overlap they're
really capturing something very simple
and and you get something similar with
a3 and b3 and yet right in the middle
there a 2 + b2 if you look at their
frequency representation you see that
you get something very different in the
higher range and yet these window
alignments here completely arbitrary one
is just as valid as another but there's
something obviously this little T in the
middle here a non-stationary in this
stationarity in the signal which is just
not amenable to a fixed block and
although you could use coding schemes
that use variable block length variable
window size they still there's no way to
capture the real world periodicity of a
natural signal with an arbitrary window
alignment could always do just a simple
convolution same signal here convolve
with three little three little sort of
Gabor filters and clearly they're kind
of capturing something the high
frequency filter is obviously picking up
the little t like element the lower
frequency maybe is picking up the basic
fundamental and yet really it's our
visual
doing all the hard work here and now we
have three times as much data to work
with so how to deal with a problem like
this well not surprisingly given my
education which is largely in oocytes in
psychology I think about how the brain
dead deals with this here's a little bit
of unpublished data from Tony's aiders
lab in which they found that in awake
behaving rat this is using a technique
called patch clamping so they're
actually looking at the voltage
fluctuations inside a cell they found
that for a given a given class of sounds
test sounds very few cells were
significantly active for at any given
time the vast majority of them the
firing rates are very close to zero and
a few of them are capturing most of the
content of the sound so I want to
capture a little bit of that flavor that
sparsity that's represented in the
mammalian cortex and so I'm going to
draw on the spike or better yet the
actual neurobiological spike now for
people that are very serious about their
neural science I'm going to abstract
away to the degree that it's going to
offend someone I don't mind my spikes as
I'm going to use the term have magnitude
they're not action potentials but
there's something about that temporal
sparsity that discrete a temporal
support of an event which I really want
to capture in a representation a second
topology I'm a little bit under the
weather so I hope my goalies hopes up so
how are we going to go about this well
here's a really simple model linear
superposition I'll go through the
graphically a bit more for the sake of
anyone who's not familiar with this sort
of approach but we have a simple
dictionary of kernel functions which can
take any form
so now let's assume we just fill it up
with something we know works well for
sound say gamma tone filters and they
have some waiting a set of coefficient
values will assume Gaussian additive
noise and then we sum both over all of
the kernel functions in our dictionary
but also across a bunch of time
instances so we allow a function to
occur anywhere in time that's
appropriate for describing the signal of
interest that signal being some sound X
of T and we can give this a
probabilistic formulation I'll fairly
straightforward stuff in which we have
something like a likelihood here which
would just make a Gaussian data
likelihood and again stretching a little
terminology a prior here which we want
to be very sparse turns out that the
specific distribution for that prior is
not terribly important in this case but
as long as it's significantly sparser
than calcium so as I said our goal now
is to actually do signal encoding using
this basic model so we can certainly
take the probabilistic model and try to
find the set of coefficient values which
sort of maximizes the data likelihood
some simple busy and arithmetic and we
end up with a log gradient for doing
this estimate if we actually wanted to
do a gradient descent from the millions
and millions and millions of sample
values per kernel function that are
going to come out of any kind of
reasonably sampled sound you know 44.1
kilohertz per second for standard CD
audio is getting a little out of hand
for standard gradient descent so in a
moment I'll describe another way of
going about it but just for the sake of
illustration here again is this
generative model in which our kernel
functions I'm going to assume for now
I'll just a fixed set in this case gamma
tones gamma modulated sine waves but it
could be anything in this case gamma
tones have some implicit frequent
frequency axis time value specifying one
precisely that gamma tone is sort of
laid down to describe content or
structure in the signal and a
coefficient value just to give a
magnitude to its instance then you know
we just kind of some along the
dictionary access here the frequency
axis and you end up with a signal which
while a rather ugly sound is at least
illustrating just generative lee what's
going on in the model here so we can
reuse the same kernel function different
instances give them different
coefficient values i'm very
unconstrained in that respect so how to
do the actual inference well working
with full length sounds it's a little
difficult and I want to stay away from
the windowing as I said originally so my
first year as a grad student I was so
proud to have invented this fantastic
new algorithm you know proof and all the
end of the year Mike gave me this paper
and said you may want to read this and
then a couple years after that I found
out that mathematicians call it von
neumann's method which meant I was
scooped just a little bit but I'm sure
that happens to everybody so matching
pursuit is a relatively straightforward
it's sort of the simple tool for what
can be a hard problem we have again our
sound X of T and we want to decompose it
essentially just in a way be a
regression by projecting the sound on to
our kernel dictionary using that to wait
the kernel of interest and then leaving
a residual left over we choose our
kernel by the one that maximizes some
cost function in this case and in a
product and then finally iterate take
the residual from before
and stick that in as the signal of
interest it's simple it's greedy it's
imperfect but it has some nice
properties so here's just a little
illustration of how it acts in with a
real signal so there's a little sound up
top and again a fixed dictionary of
gamut own functions to the left the
convolution of the signal with the gamma
tones which sort of act as a filter Bank
in this case produces the sort of energy
surface here which is being illustrated
each maxima and that surface is located
the matching kernel function identified
its structure appropriately waited to
subtract it out of the signal and then
down at the bottom the resulting
representation this sparse spike code as
we sort of want to call it is what's
left over plus some residual
so there's a lot of flexibility and what
one might do with matching pursuit the
most straightforward is how you actually
decide to stop the process simplest
threshold at some coefficient level
threshold at an snr for reconstruction
which is what we do and then that gives
you say a variety of possible levels of
encoding so I'll play this out here I
may need a little boost and hopefully
it'll be audio audible in the room but
the upper left corner here is 5 DB
reconstruction than 10 DB 20 and 40 it
just gives you a sense of how the
algorithm progresses roulette candy with
fresh spring water Gillette canteen
fresh spring water so this first one is
really difficult to hear partially just
because there are so few elements that
it doesn't actually have a lot of energy
in the initial estimate of the sound but
the truth is it's actually capturing
most of the structure say in the vowel
of this is the actual representation
here is just can from canteen so it's
capturing actually most of the structure
of the vowel in that can sound in fact
if you kind of mask it with a little
filtered white noise it really pulls
back the whole bit of speech and by 20
DB here it's getting to be a fairly
accurate representation and in you know
most of these cases are using very very
few elements to actually do the sound
representation and I'll get into sort of
quantizing that theta quantifying that
later so here's just a quick comparison
the word up top is pizzerias and then
the three different representations here
the waveform the spike code and then the
spectrogram at the bottom this is really
just to illustrate that a lot of the
same information is represented in both
and that you can kind of see that the
the foreman structure is present in both
and the the sort of basics of the
harmonics and on sets are are there but
if you look more closely and here we're
going back to this word Vietnamese you
can see that there's a meaningful
difference so the spectrogram
representation on the bottom is showing
this big starburst for where the tea is
being articulated not especially well
localized in time although there are
tricks for fixing that or in frequency
but when you look at this sort of spike
representation as highlighted in the
inset box in the middle portion you can
see that really two single events each
of which has a magnitude at a time so
two pairs of values is pretty much
capturing all of that little T like
structure in fact just using the little
elements in the inset box you can
actually reconstruct just that part of
the speech done by hand not sort of
claiming that this is an automatic
process but it's really the compactness
of the representation which is most
interesting but as I said matching
pursuit is certainly not perfect for
example since this is a generative model
we can always just generate test signals
by generating sounds using the actual
dictionary itself so for example making
a whole bunch of clicks with gamma tones
that are in the first third of this
separated by in time by short segments
or in the second set two different
trajectories which cross in frequency
space and on the top is what we inferred
actually doing gradient descent and the
bottom is with matching pursuit and if
you actually close in SE on one of these
cases where the clicks are very close in
time you can see that the gradient
descent approach actually recovers the
original maybe you can't see to be
honest in grey is the great thick line
is essentially the test sound the test
signal which is the addition of two
identical kernel function
shifted slightly in time and the
gradient descent finds those two
functions matching pursuit however
because it's greedy and iterative first
picks the best kernel function to
capture the entire structure once and
then has to go back and do this residual
cleanup which is a messy process and
imperfect fortunately for single
speakers or speakers with very different
pitches and so forth this sort of
problem isn't as serious and so doing
things like speech analysis turns out to
be fairly effective with matching
pursuit so we have a basic coding
algorithm do we have to use a fixed
dictionary to actually describe the
sounds of interest and what could
learning a dictionary actually tell us
about natural sound statistics so again
here's the generative model and the sub
probabilistic formulation and of course
very simply we're just going to take the
derivative of the of the probabilistic
model with respect to the parameters of
our dictionary which is just a set of F
IR filters so essentially it's all of
the the sample points in these vectors
making up a kernel function yes you also
regret in sulphur ciccolo prior which is
just another one penalty as is as is
yeah I want to mathematically you'll
you'll do find it is and actually in the
case of the in the case of the imprint
step that's a bigger concern although
practically it actually works here we're
doing something which one might assume
is even worse we're just assuming that
the coefficient values are independent
of the kernel functions themselves and
so when we do the derivative here that
the prior completely disappears we're
assuming that each is going to be an e/m
type algorithm so we're assuming that at
each step we've already found sort of
the optimal sparse set for doing the
encoding there's much more detailed
works on the
sort of a bit of research on the sort of
work that inspired this approach but
it's a fair point there's a lot of kind
of approximations to what would truly
want for these models but importantly is
that the results sort of demonstrate
that it's robust enough to overcome
those limitations so what we end up with
at the end is this rather simple and
elegant bit of work in which the
gradient estimate for the kernel
functions is simply the residual error
after the olive encodings done weighted
by well the residual error at the
precise time point at which one of these
spike events took place weighted by the
magnitude of the spike that was at that
time point so in other words we can
actually code up an entire signal
several minutes worth and then just go
back and sort of collect the error data
associated with each spike and average
that and amazing enough that's just the
gradient do the estimate for the update
so what does this mean practically well
being that I'm a sort of computational
cognitive neuroscientist actually wanted
to look at some question of interest to
the brain so we took this opportunity to
collect a bunch of natural sounds animal
vocalizations for example environmental
transients like breaking twigs and rocks
dropping on one another
and ambient sounds the winds with the
leaves rain running water and we want to
see what does the if we learn the
dictionary for this what does it look
like what kind of structure is going to
emerge out of it all we did essentially
is some equal levels of power from each
group of sounds and we call this sort of
generically a natural sound database
there's a lot of discussion that's
possible about what a cat would actually
here as it shapes the forest but for
simplicity sake this is exactly what we
did we initialized our dictionary of
kernel functions with simple Gaussian
noise all the same length each of these
little grey bars is a scale bar that I
believe is about a millisecond long so
all of these are the same gaussian i
believe about 80 samples to start with
for 16 kilohertz sampling rate they but
i give a little leeway because we threw
in some simple heuristics by which we
could actually learn kernel functions at
whatever temporal support is necessary
since we have this model in which we're
allowed to just drop our functions
wherever we want them then there's no
real constraint on their size so we
allow them to sort of learn their own
support and then we started playing
sounds and as we did these things kind
of regularized you can see for example
up in the far left top left-hand corner
a fairly high frequency at least by
indicated by its scale bar with a
particular shape and as you look further
down there sort of organized by length
low and lower frequency kernel functions
but they all seem to preserve a certain
basic shape I should have shot
bond set gradual decay kind of classic
sort of decaying resonator look but in
this case inferred from the sounds we
trained on so it has structure but does
it actually mean anything well here's
some data that's wonderfully made freely
available at the e.r lab at boston
coming from laura carney this is cat Rev
core filters so these are simple linear
estimates of the filtering process of
the cat cochlea so each one of these
blue curves indicates a kind of
frequency and bandwidth selectivity for
a given cell as it processes incoming
sounds so what we thought to do was
basically just take one of our learned
kernel functions and find a filter in
the database which with which it was
highest correlated so all we did was a
little bit of phase alignment we
actually didn't do anything else there's
no rescaling or anything and then i'll
show you here just overlaying them so
while the fits certainly aren't perfect
at least for me they're all so
strikingly similar again a very strong
asynchrony sort of temporal he synchrony
asymmetry in which we have a fairly
sharp onset gradual decay sort of
classic gamitin like structure now we
could be fiddling a bit here with our
choices but let's say we just train on
other types of sounds for example up top
we only train on those environmental
transients and ambient sounds so no
vocalizations whatsoever or down below
we only trained on the vocalizations
well you learn something very very
different up top you can see the
functions are extremely short in time
very compact and down below of these
very elongated in some cases 100 or more
milliseconds
very frequency specific in no case is
there really a clear asymmetry to the
structure either we can kind of do a
simple little population comparison to
make certain I'm not cheating since I
picked the best fits for this sort of
thing for example we could do the same
process of finding best fits in the cat
data for just let's start off with the
Gaussian noise kernel functions that we
initiated the system with so in that
case the best fit between our dictionary
and the cat data for each element the
medium was about point for vocalizations
training just on the vocalizations the
best fits were around point six for
environmental sounds better still 7-5
7-6 but when we train just on the
natural sounds or I shouldn't say just
but on that super set collection of
natural sounds I find a significantly
better match to the cat data and in fact
if we just look at those at those rev
core filters which the natural sound
kernel functions fit well we can in turn
fit them with gamma tones which is what
part of what gamma tones are originally
designed to do was to describe auditory
filters and those fits are really
comparable they're not statistically
different from one another so as a
population then we can look at these the
little blue dots here are individual cat
Rev core filters plotted in terms of
their Center frequency the frequency
that sort of gets the best response out
of the cell plotted against their
bandwidth so classic shape here
essentially the low-frequency filters
tend to have tighter bandwidth and as
frequency increases the bandwidth
increases in approximately logarithmic
fashioned with a kind of pseudo linear
portion at the beginning there if we
just look at those environmentally
trained functions you can see that they
do follow this basic logarithmic trend
but they don't lie in the same space as
the CAD data we look at the actual
animal vocalizations well they seem very
different in fact the kind of seem
invariant center frequency is sort of
invariant to bandwidth more of a
harmonic kind of representation but the
actual natural sounds data produces a
sort of distribution in the dictionary
that falls right in the space of the
sort of cat data the union of those two
other sets um there really isn't
anything that it's just those two things
but together well sort of the three but
you're right i mean the combination of
the environmental sensor in the
vocalizations yeah there's there's
nothing else added in there it's just
the environmental of the vocalizations
no so you know it's the this sort of
more detailed sort of look at it that
you can think about for example the
we've played around for example with
different ratios of combinations of the
two so there isn't a lot of that the
ambient environmental sounds don't have
a lot of thought the sort of correlation
time is very very short whereas the
correlation time of the vocalizations
tends to be very very long and if you
kind of very those two you basically in
the set of the kernel functions you sort
of vary the length of or in other words
the bandwidth of the of the functions
themselves and so as you change that
distribution you certainly move off of
this this overlay this is the case where
the sort of picking from that set gave
the best description or the best match
with the biological data
so you know we've looked at a lot of
different sounds so these different
classes of sounds that went into the
training set the kind of mix of sounds
sort of unpublished and not terribly
thorough but we've looked at music we
looked at birdsong there really was only
one of the class of sounds that seemed
to capture this same structure and
that's speech so we took the timid
speech database and we did the exact
same thing that we did before we just
play a sentence we do the coding we take
those coefficients we do learning on the
kernel functions we use that we do the
coding again a basic sort of am process
and although I would argue that these
are sort of different a slightly
different looking set of functions they
also show a really striking
correspondence to the animal data so
again the learn functions are in red and
the cat did is in blue here and we can
again look at this kind of comparison
population here in which speech and
natural sounds don't significantly
differ in terms of their correlation
with the with the cat data but you can
do something simple like just flip
speech around and coded up backwards and
you get a significant decrease in their
correspondence it's not surprising as
you might guess you learn functions
which are flipped backwards in time as
you can see that temporal asymmetry is a
significant part of the of the
description of the cat data and then if
we look at it again it's a population
somewhat hard to pick apart but the
English data also lies on top of cats
and we've also now run Arabic and
Taiwanese a tonal language and get and
we found very similar results so we kind
of summarize then by at least
speculating that the
the mammalian auditory system has
evolved to represent the sort of
statistics of natural sounds and perhaps
our vocal apparatus our generator system
has subsequently evolved to take
advantage of the channel capacity of the
auditory system which it inherited
there's evidence of a little bit of
change in both since we've been speaking
but I don't know I've never noticed my
cats to be particularly good with
English and despite these results and
and it also holds with the this basic
form of the cochlear data holds with
Chihuahuas and and mongolian gerbils and
a variety of other mammal species so
what about actual coding efficiency well
so what are we gain by learning a
dictionary versus just using
off-the-shelf well it's at least show
that we've got some improved sparsity
here including a signal up top I can't
remember what the signal is when i made
this filter years ago so let's assume
it's my favorite team at sentence she
left my dirty suit and greasy wash water
all winter long and and then we'll show
essentially an encoding of that sentence
using a dictionary which has just
started a couple iterations into
learning process midway through and it
finished and i believe it's pretty clear
that the sparsity of the representation
includes increases whether significantly
as you go along with each non-zero event
capturing much more structure so these
are all encoded to the same 15 DB levels
but each nonzero event is encoding much
more and more information about the
signal as training goes on the
dictionary is becoming more coherent
with the signal structure so another way
to look at this then is a sort of
classic rate distortion which I'm going
to flip into a sort of rate fidelity
here so right along the bottom is just
the cost of transmitting a certain
amount of information and then fidelity
in terms
snr is going to be the the benefit you
get for transmitting that so of course
ideally the Green Line is better than
blue more fidelity for less cost so what
we did was we just took a uniform
quantizer and sent different kinds of
codes through and used varying the
quantizer as proxy for changing the rate
here so we did it with a Fourier fft
with a w wavelet with the spike code
using a fixed set of gamma tones and
then finally with the spike code using
the actual adapted learned colonel set
so what we found most clearly is that at
low at very low bit rates the spike
codes are substantially more efficient
than the alternative really running from
very very difficult to understand below
5 DB up into the range where it's more
than interpreta 'but really is quite
clear furthermore thinking about the
spike codes the actual adapted set is
outperforming the fixed gamma tones not
an enormous amount but significantly and
at 15 DB that 3d difference there is is
a noticeable perceptually noticeable
improvement in this case we just used
one just a Jimmy she wavelet which is
also as a compact support like your own
so we can certainly go so clearly
friends here we can certainly go through
and remove you know trivia low value
coefficients from that as well so I
don't know that it's a totally analogous
but that's the nature of the comparison
ideally of
apparently learned and the gamma tones
is to to show that the learning process
actually arrived did something that was
significantly more efficient than a sort
of engineered approach but it's entirely
fair to say that there are better ways
to do encoding using using wavelets or
even using FFT Jackie is hey yeah
they're a little ugly missing the other
night not nearly as good as gamma times
for describing IC boys cells into the
tidal gates any wealth is what I
remember from physics center every
function that has a compact supporting
the time domain is going to have lots of
uncertainty in frequency the man whether
it's the wavelet function or local spike
was so those would have basically very
very broad spectrum no take that a
player Eisenberg was robbing us no no
you're just over at your buddy what he
said it seems to be uncertainty on this
issue so we'll come back to it um okay
so interesting so now I'm going to just
come to this last portion here and
describe an idea which someone brought
to us and we decided to test out and
this idea was fairly intriguing so here
on the left is a spectrogram of a bit of
speech something that most people here
probably recognized maybe speech is a
liberal interpretation of it but we can
remove a lot of the spectral
representation here just save the
temporal component and in doing so we
produce something which is fairly
difficult to understand but still has
these set of speech like qualities and
again forgive me if you can't hear that
well
anybody know what that was oh you got it
it that often difficult for people to
get that does the problem giving a talk
with smart people or maybe people that
just listen to too much Beatles but I'm
guilty so the so why is this aventuras
well there's some interesting perceptual
experiments you can do with with work
like this but in fact this really noisy
ambiguous and spectrally unresolved kind
of sound is a lot like what you get out
of a cochlear implant so those
individuals that have had cell death
particularly inner hair cells in their
cochlea you can actually essentially
wire up a hearing aid directly to their
auditory nerve and transmit this
information by directly stimulating
those cells the process is in some ways
complicated and in some ways not but
this is part of the part that's not
which is that you end up with this
temporary precise but spectrally
washed-out kind of sound so someone came
to us and said well you know we've only
got like six or eight frequency bands to
work with in a cochlear implant could
you actually learn the appropriate bands
to use or the filter bank to front and
such a system so how does they do this
this isn't the most sophisticated way in
which one could go about building a nose
excited vocoder which essentially is
what these systems are but it's kind of
standard for the experimental literature
in which some speech waveform is passed
through a bank of band pass filters in
our case we're looking at six filters
for experimental reasons which I'll
describe in a moment then half wave
rectification envelope extraction with a
low pass filter and now we have the six
envelopes which we excite with Gaussian
white noise we take that same band pass
filters again and sort of band limit to
what the noise now you have
the original sound again but with all of
that sort of spine spectral detail
removed an example of a way to do this
better would be to use for example
Hilbert filters and just extract the
envelope from the using the analytic
signal it can be done other way so would
it be possible to learn a set of filters
to sort of optimize say speech
perception in this context for people
using cochlear implants well let's give
it a try we had 16 participants seen you
undergrads their task was essentially to
take dictation on full sets of sentences
from the timid trading database there
were four conditions sentences or either
passed through a linear vocoders the
vocoder had linear frequency spacing but
six channels coakley topic vocoder so
there were again six channels but sort
of in a cochlea topic arrangement
frequency wise a sort of a pseudo
logarithmic arrangement a vocoder based
on our learned filter bank so we run the
whole system on the timet training set
test should say test set on the timid
training set but assuming you only had
six filters to use sort of like what if
you evolved that way and then finally
they actually took the rotation also on
just normal speech they never heard the
same sentence in the different
conditions and then we just measured how
many words they were actually able to
take dictation on correctly so it we
found initially is that CMU undergrads
are not good at taking dictation and
perhaps not surprisingly no one's good
at hearing things they've been passed
through a linearly space vocoder it just
is not the way our hearing system set up
most intriguingly though when you
compare the cochlear topic versus the
sort of efficient learned filter bank
there's almost a fifty percent increase
and the number of words that individuals
were reporting correctly
and that's non trivial as you might
imagine so what would be great is
knowing why so we basically just reran
the experiment again with another 16
participants we had half the people also
shadow which means they repeated the
words rather than just typing them just
for comparison's sake what we used this
time we're non-word phoning combinations
so ba da da eb in which they had to
essentially like phonetically type out
what they heard and we dropped the
linear voter example and then we just
measured individually which phones which
little parts of speech they reported
correctly for people deeply invested in
language issues i'm not i'm just using
phonemes and phones as stand-ins for
something that i think is more deeply
rooted language so averaging across all
types of speech we found the same basic
trend performance in general was lower
there are not any actual words here but
the there's again this significant
improvement about a forty percent
improvement here and the number of
individual identified elements that were
being reported oh i should say people
are better taking dictation than they
are at shadowing if you give people a
chance to shadow if they hear go to that
that's what they say but if you force
them to type something and finite set of
options then they actually are more
often correct than their then they're
shadowed reports vowels seems to have
hit a sort of a ceiling here there isn't
a clear difference in the accuracy of
reporting vowels in the other the
cochlea topic or the learned frequency
Bank fricatives are like type sounds
very noisy all literature says people
very bad at it and this is just echoing
that there's again no statistical
difference in the performance it appears
that stop consonants like butta drove
most of the
fact here the very significant
improvement in people's perception of
stops and with the learn filter bank
versus the cochlear topic filter Bank
which just didn't appear in the other
parts of speech it's interesting because
here we can sort of look at the
frequency layout of the different sets
that we use so up top is the linear
filter bank then the cochlea topic
filter bank and then down below are two
different ways of working with the learn
filters so full disclosure learn learn
two functions tend to be a little ugly
and noisy so they're smooth in the case
that we used here the second-to-last we
smooth by fitting them with gamma tones
and then the last we smooth a little
more naturally using a spline so what's
most notable here is there seems to be a
very significant waiting towards low
frequencies and in that sense maybe it's
not surprising people do better speech
perception if they really have a lot of
sort of perceptual representation at
lower frequencies although I have to say
I'm not convinced that that fully
explains why people did well on stops
which in many cases for example but vs.
bhavish as pas is more like a voicing
onset distinction which is like on the
order of 20 to 40 milliseconds timing
and so in that case it's possible that
features such as the very precise timing
onset of the learned kernel functions
enhances that the perception of the
localization sound on sets so that's the
quick summary of that little experiment
absolutely you learn to the tillsonburg
it seems like one interpretation is just
that your wife filter bank is giving you
a better tempo
solution right you've got a really wide
filter right for your learn case whereas
the cochlear filter oh it's half the
spectrum it's only four kilohertz and
above where it's wide so what was the
exact shape that you use for the
cochlear man good having fun but it
shouldn't be more shouldn't we have
triangles have triangle let's how I was
grounded or something instead of those
flat top flat fans with the skirt on the
left side light goes out for a long that
might be what makes the difference well
my okay I think that that's very
reasonable so in this case I'm sort of
inheriting the some previous work from
Shannon and others in which they have
very specific sets of filters with
specific roelofs that they want it looks
like they just took all their bad words
bob shannon yeah so it's just cochlear
bandwidth spit the name posed straight
lines on them as opposed to more not for
sure your reality yeah there's there
well yes although that has its problems
also I mean if you really it's sort of
luck that these things ended up being
usable for a vocoder because you really
want to set up a perfect reconstruction
filters which is what they're getting by
having their very constrained sets of
filtrations sorry oh not at all okay so
what did we find well a way to represent
signals which is time relative gets away
from window blocking and extremely
efficient in its representation some
interesting findings about the
statistics of natural sounds and
actually description down at the filter
level for individual cells as well as a
description of sort of population of
cells that goes into the million
auditory system a surprising or not
finding that there's this correspondence
between speech and an auditory system
and finally at least the inspiration of
a way to look or think about auditory /
csis design which focuses more on the
computational task and less on the
the biological implementation so just to
finish up here a couple of little
teasers on things I'm currently working
on one in the upper left corner is a
hierarchical model for spike coding in
which for example here there's a higher
level unit that represents the
probability distribution for interspike
intervals for in this case the
difference between males and females
voicing and ah type sound the little
animation to the right of that is
learned subspaces sparse subspaces so
this is a two dimensional subspace for
log frequency log amplitude spectrograms
just as a way of starting to get a
handle on the process and very
interesting things emerge out of that
such as harmonic stacks that shipped in
time in frequency and formants that
shift in length to allow for coding of
the same basic format structure but with
variable length pardon me we've also
looked at applying the very similar
analysis to cochlea grams and finding a
very different result although it's
almost impossible to see up here but
essentially the the subspace is almost
entirely temporal in nature the picking
up the periodicity zin the cochlea gram
rather than these larger sort of
harmonic and and spectral components and
spectrogram and then finally I'm working
on for those people familiar with it
work by a gardener and Magnus co which
they do a reassignment process to take
spectrograms and turn them into highly
especially sparse representations the
limitation of their approaches that they
don't have a generative model and they
can't specify they don't have a system
for learning the desired filter Bank for
the system that they're using so i'm
working on a an efficient generative
vocoding system
produces very similar results and I
think that's about all I'm going to get
out of this throat so thank you very
much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>