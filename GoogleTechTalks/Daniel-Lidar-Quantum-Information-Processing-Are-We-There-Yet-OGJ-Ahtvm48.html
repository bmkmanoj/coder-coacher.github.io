<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Daniel Lidar: &quot;Quantum Information Processing: Are We There Yet?&quot; | Coder Coacher - Coaching Coders</title><meta content="Daniel Lidar: &quot;Quantum Information Processing: Are We There Yet?&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Daniel Lidar: &quot;Quantum Information Processing: Are We There Yet?&quot;</b></h2><h5 class="post__date">2015-02-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OGJ-Ahtvm48" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's get started it's my pleasure to
introduce Daniel Adar who's our
neighbors so to speak he is a professor
for physics electrical engineering and
chemistry here at USC and actually he in
some ways paved the way for the quantum
AI lab to exist because he was brave
enough to start a project between you
see and Lockheed Martin two buys the
first d-wave chip and elevate sort of
the study of the d-wave chip to academic
levels and then it's an old hand in
quantum information and generally it's
doing it since 20 years he's well known
for his contributions to quantum error
control master equations recently a lot
of work in quantum annealing and he has
his PhD from Hebrew University in
Jerusalem from the physics Institute
which gives me some nostalgic feelings
I've worked here as well and I'm often
joking with Daniels that you know this
area here's a West Side in LA areas
called the silicon beach sometimes and
say hey if we work hard we can may be
upgraded to niobium Beach silicon was
yesterday but yeah let's see how we far
we have gotten in this area maybe last
thing I should mention is that by way of
what I say it's that Daniel is the
director of the Center for quantum
information science and technology at
USC
okay well thanks Hartman and thanks
Massoud for the invitation to speak here
it's it's a real pleasure it's always
great to come here for lunch first
that's better than anywhere else and
it's also great to see that this this
idea that we discussed a long time ago
about looking into the performance of
the d-wave machine has taken off and
it's become rather popular subject
nowadays although also somewhat
polarizing and I'll get to that but this
talk will not be just about d-wave I was
asked to provide a general introduction
to computing in some sense so I've kind
of divided it into two parts where the
first half will be really easygoing and
the second half will be a little bit
more technical about the actual results
that we've obtained using the d-wave
machine so the title of my talk is
quantum information processing are we
there yet and I'd like to suggest that
quantum information processing is in the
process of arriving we have useful
commercially available choreography
there are particular two companies that
I'm aware of that make quantum
cryptographic devices now on
cryptography in theory cannot be hacked
but in practice it has already been
hacked in fact there is a quantum
hacking lab led by Vadim ocurro that at
Waterloo who has a whole page dedicated
to the hacking of these two particular
cryptographic devices so it's it's it's
a fascinating field with lots of promise
but it's it's still in its infancy we
have quantum simulators that are
becoming more and more powerful these
simulators are designed to simulate
quantum systems and they are quantum
systems that simulate other quantum
systems and that is a real burgeoning
field very promising but at this point
we can
still only do small scale problems this
this picture in particular is from a
paper body about Chris Monroe's group
where they simulated a paramagnetic to a
ferromagnetic phase transition using 16
trapped ions and undoubtedly these
systems will become larger but at this
point they're still pretty small then we
have quantum optimization which is what
I'll talk about mostly it faces many
challenges it has reached the largest
scale so far of all quantum information
processing devices but let me not say
more about it right now and finally
everybody would like to build a
large-scale universal chronic computer
the gate model is where this all started
and there we have fantastic progress but
we're still also quite far and what sets
the the bar here is in particular the
constraints imposed by the theory of
quantum fault tolerance what we have to
achieve certain gate Fidelity's and in
some cases like in the example of the
superconducting qubits produced by john
mccain's lab these gate fidelity's have
already exceeded according to to certain
estimates the threshold for fault
tolerance but still we're quite far from
reaching large-scale systems so I would
say that the answer to has caught
information-processing arrived the
answer is we're working on it
so let me however now take a big step
back and in the interest of pleasing the
general audience I would like to tell
tell you a little bit about why we think
we need pong computers in the first
place so why is that an interesting idea
well the first thing we can say is just
interesting as it's finally observed
early on we can ask what happens when we
shrink the size of components down to
the size of atoms and electrons and of
course quantum behavior is then dominant
and so we can ask what happens it's
interesting if you build if you build
computers out of
quantum components quantum bits the next
thing is of course there are some very
powerful computational complexity
motivations there are these these speed
ups that we know or suspect might be
there in particular we can all break all
the key cryptography using Shor's
algorithm and so RSA is out the window
and shor rules but we can also suspect
we suspect that we can simulate quantum
mechanics itself exponentially faster
that goes back to the simulators that I
talked about in the previous slide we
know that we can solve linear systems
exponentially faster or I should say
again we suspect we can even quantum
PageRank we can compute faster than we
can classically that as PageRank can be
compute computed faster using quantum
mechanics than we can classically and in
some cases there are even examples of
provable speed ups the most famous
example is is quantum searching this is
Grover's algorithm where we know that we
actually really know we can prove that
there is a quadratic speed-up now in
addition there is this argument by
inevitability of why we should be
interested in quantum computing and
that's the notion that that classical
chips are going to hit a wall
so Moore's law the scaling of the
density of transistors as function of
times exponential and while Moore's law
is is still holding strong we know that
already
at some level the party's over
Dennard scaling which which has to do
with with essentially with other metrics
is already coming to an end you see the
flattening of various other metrics like
like speed and frequency and and and
power drawn and the response to that it
has been to increase the number of cores
so nowadays when you buy a laptop you
can yeah you will you certainly have
more than one court for maybe even eight
is as possible so this is the kind of
parallelism that the the classical
computer industry has introduced and
parallelism is a good idea
you can continue this parallelism to an
extreme like in Titan here currently the
largest supercomputer in in the US soon
to be eclipsed but a huge machine and we
can also try to instead of going the
classical route we can try to go the
quantum route and that is to exploit the
phenomenon of quantum superpositions in
order to perform quantum information
processing so how does that work a two
minute primer on on qubits so a qubit is
is a quantum generalization of the
classical bits it can be 1 and 0 can be
in a superposition of two basis States
one in the zero States suppose you have
three cubits then in some sense all
eight of these binary combinations or
simultaneously represented and if you
had n qubits you would get two to the n
binary states and in quantum mechanics
we know that according to the rules of
quantum mechanics we can parallel
process all two to the N of these states
they can all be made to evolve
simultaneously with one operation and
what's more the coefficients in front of
these these due to the young States can
be complex in particular they can be
positive and negative so they can
interfere they can cancel out and in at
some level that is that is the magic of
quantum computing but it's not the whole
story because there's there's a big
caveat and that is that measurement
whether it's intentional or not will
take a superposition and will collapse
it at random to let's say one of the
basis States and so if you evolve all 2
to the N possible answers to a problem
in parallel and then you make a
measurement you might not get the one
you wanted you might get a random one
which which is garbage what's more this
this can also happen unintentionally and
the process of decoherence which is the
Nemesis of of quantum computing and is
is trying to fight so what does
decoherence well every real quantum
system interacts with an environment or
a bath and that environment is both
noisy and uncontrollable and that means
that it makes measurements on your
quantum system when you don't want it to
so imagine that you you put your quantum
computer into a superposition of two
states and along comes a photon from
somewhere and interacts with your system
so this photon is now part of the
environment that will that is like a
measurement which collapses the
superposition so the consequence one
element of your superposition will
disappear and you're left with just a
single classical state and that's that's
very bad that happens at random so it
turns out in fact there's a theorem that
a sufficiently decohere at quantum
computer can be efficiently simulated on
a classical computer so too much
decoherence means it's a no-go and you
you have to engineer your system better
or you have to protect it using tricks
from air crush okay so how are qubits
realized there are many different
proposals as many as there are physical
systems that can support two orthogonal
states if you like as basically a qubit
is any well-defined quantum two-level
system and of the long list of picked a
few of the more popular ones trapped
ions for example the two atomic energy
levels represent the qubit their photon
polarization horizontal vertical
polarization could be the two states of
your qubit that can be in superposition
in the domain of superconductivity there
are many different proposals charge flux
fades in particular so here are some
pictures those and I'll talk about the
flux qubits when we get to D wave the
two states when
Tron spin-up and spin-down so whether
the electron spin is spinning clockwise
or counter clockwise if you'd like
saying for a nucleus and the list goes
on and on any any two-level system it's
good that the question was whether it
matters whether they're degenerate
eigenstates degeneracy can be can be
useful it can help you store information
more robustly but it's it's it's neither
it's not necessary Yeah right right but
if they're not degenerate then you can
also drive a a transition between them
more easily so there are advantages here
or there so it's a fact of this business
that in spite of of twenty plus years of
very hard and experimental work and
theoretical work as well we are still at
the level where we just have a handful
of qubits in most of our quantum
computing devices quantum information
processing devices in that sense quantum
computation still hasn't arrived we we
still can't really solve we can't really
capitalize on the promise of quantum
computing yet it's believed an
undisputed fact that you still cannot
factor non-trivial numbers using quantum
mechanics or using Shor's algorithm
faster on a quantum device than you can
classically and so in that sense we're
working hard and we're making great
progress like this chip here which I
already featured earlier from from the
Martinez lab great gate fidelities but
the numbers of qubits are still not at a
point where we we have large-scale so so
from that perspective quant computation
still hasn't arrived however claims to
the contrary have been made
and in particular the company d-wave has
at some point at least I don't know if
this is still on any of their web pages
but it was at some point they claimed
that Wan computing has arrived with the
devices that they have built and these
devices are not cheap
you you can buy yourself one for around
ten fifteen million dollars the
customers are so far lockheed-martin
with the machine that we have installed
at USC and Google with the machine
installed it at NASA Ames some
heavy-duty investors and a lot of money
and raised in venture capital so of
course this raises the interesting
question of whether quant computing in
fact has arrived with the arrival of the
d-wave machines and we at USC and people
here at at Google and NASA and and
Lockheed Martin as well set out to try
to address this question of whether the
d-wave machines have now launched the
era of quantum computing so there are
many questions and basically I think
these are the key questions so if corn
computing has arrived then the d-wave
machine has got to be quantum and if so
is it faster than classical if it's not
faster than classical but it's quantum
then maybe there's something wrong like
decoherence and then maybe we can do
something about that and we can improve
it and ultimately what can we do with it
what is this computer good for so let me
briefly go over some some answers and
then I'll spend some more time in detail
on some of these answers so first
regarding the question of quantum is
that is that is not an easy yes/no type
answer that one can give but at at a
rough level let me say that at this
point I believe that the the question of
quantumness has been answered in the
affirmative various techniques have been
introduced
people have come up with classical
models and those have been put to the
test and essentially reject it
successfully entanglement has been
measured in d-wave devices and multi
qubit tunneling has also been measured
in do you wave devices so these are
these are some pretty strong answers in
particular entanglement is is something
that you can't really argue with if
there's entanglement and then there's
quantum ease so I would I would dare to
put a checkmark next to this is a
quantum question is it quantum enough to
give us a speed-up is a totally
different question and this has to do
with whether the quantum effects are not
supplied by decoherence and we've we've
looked at that carefully and it's it's
work-in-progress
here's one paper on that topic and I'll
say a few words about it later now if
it's not quantum enough then we can try
to use air correction tricks and so
again we have work on that no I'll
definitely spend some time talking about
our corruption results and the the
answer there is yes we can certainly
improve the device we don't know yet
whether we can improve it to the point
that we can actually make it faster than
classical but we can do quite a bit
that's beneficial using our correction
and finally what we can do with it there
are many different applications and I
actually won't spend any time talking
about that oh here's a couple a couple
of pointers to the literature and there
are many more ok so what's in the black
box briefly and so here's the box if you
open it you will find a dilution
refrigerator inside that you'll find
some powerful magnetic shielding so
together they bring the system down and
in our case at a USC 217 millikelvin
operating temperature and 1 nano Tesla
in 3d across the processor these are
these are very good numbers on if you
zoom in more you'll find lots of
classical control lines that feed the
signals to the chip and the chip itself
is is in here
this is actually the chip that little
white square there and and the chip
consists of superconducting flux qubits
so called RF squids so here's a close-up
on one of the chips actually on one of
the unit cells so the chip consists of
of unit cells and each unit cell itself
contains eight qubits and they're
arranged in this vertical and horizontal
pattern each qubit is is an elongated
loop that carries flux and this is
superconducting flux so silicon current
and that current can flow if you'd like
clockwise and counterclockwise
simultaneously and that is the the qubit
that were we're using here so there are
eight qubits per unit cell and then if
you take these unit cells and you tile
them you get the entire chip in this
case this is the the vesuvius d-wave to
chip that we have a u.s. see it it
contains an 8x8 array units so 8 times 8
times 8 that gives you 512 qubits
although not all 550 with ulid and in
fact and the current chip that we have
504 of the 5 and 12 qubits are are
operational now I it's useful to draw a
little diagram to understand the
connectivity of these qubits and so here
are the four vertical and for horizontal
qubits where they intersect you get a
coupling and that's where the qubits
interact and talk to each other and it's
useful to represent that in in this
graphical form where this is just a k4
for a bipartite graph so now the circles
are the qubits previously the qubits
were the lines that's what they are in
real life but in this diagram the qubits
are the circles and and the lines here
in this case for a graph are the
interactions between the qubits so you
see
it's every Cuban on the left is coupled
to every keep it on the right but there
are no qubits there are no couplings
going down on either side okay so how
does the chip itself hook up well this
is the unit cell again alright so for
keep it's on the on the left for qubits
on the right with their couplings and
then the entire chip is a tiling of 8 by
8 unit cells and the connectivity among
the qubits the the inter cell
connectivity among the qubits is
depicted here you can hopefully make out
that qubits in the right column couples
who keep it's on the right column of the
neighboring unit cell and qubits in the
left column coupled down or up to the
qubits in the left column of the
corresponding unit cell so this is the
the so-called chimera coupling graph of
the the d-wave chip the degree of this
graph is is 6 at least in the bulk every
qubit is coupled to to six other qubits
it's obviously not a complete graph
which presents some challenges when you
try to embed real-life problems into it
every real life problem has to be mapped
to this particular architecture but it's
it's a rich structure and it's powerful
enough to support hard computational
problems so so what does the d-wave chip
then beyond this architectural layout
well in fact it's a special-purpose
processor it implements a type of
quantum computing called quantum
annealing it's not Universal on
computing it's designed to solve a
specific classes of problems namely
optimization problems and those
optimization problems are ones that are
captured by the classical Ising model so
what's the classical Isaac model very
briefly it goes under many names
depending on on the field but in physics
this is how we think about it you have
binary variables these are now classical
spins they can be plus or minus 1 we
have n of them and every izing problem
can be represented in terms of a
tongan or an energy function which is a
sum of single spin terms and a sum of of
to spin terms these coefficients H is
the hsm the J's the ages are actually
local magnetic fields that can be
applied to the the qubits in the d-wave
chip if you'd like and the the J's are
the interactions among the you the
qubits in the d-wave chip but here they
are simply the coefficients that define
an izing problem for every izing problem
once you write down the ages and the J's
and you specify the connectivity you've
you've written down an izing problem and
then the problem what's the problem the
problem is to minimize this energy as
Hamiltonian that is to find the
configuration of the spins and I am
sorry I switch switch notation here I
just realized the essence and the
Sigma's mean the same thing ok so the
signals are the binary variables the
Sigma's are in this context again the
same binary variables the problem is to
find the configuration or the values of
these binary variables which minimize
given the ages of the J's the value of
this H Issac and it turns out that this
problem of finding that that minimizing
configuration is is np-hard already if
you just limit yourself to set all the
H's to 0 you could set all the J's to
plus or minus 1 and if you put that on a
non planar graph it turns out this
problem is is np-hard and the fact that
simpie hard means that any problem in NP
can be mapped to this one with at most
polynomial overhead and so there are
many very interesting optimization
problems like traveling salesman and
satisfiability of boolean formulas in
machine learning which is a big deal
here at Google of course can be mapped
to to minimizing this izing Hamiltonian
so it's a very rich class of
optimization problems that that are
captured by by this model so how do you
solve the Ising model how do you
actually find the minimum energy
configuration so here is the one of the
the workhorses of classical techniques
Hosko heuristic techniques to simulate a
kneeling
and in similarly kneeling essentially
you imagine that the there's some energy
landscape and what we're trying to find
is is global minimum that is the the
spin configuration which minimizes H
izing and the way that's done is by a
combination of downward moves plus
upward moves the downward moves are our
dictated by this this acceptance formula
where essentially we're trying to
minimize the the energy difference sorry
we're trying to minimize the energy so
every time we make a move we check the
energy difference that's Delta e and if
the energy is less then we accept and
with probability one and if the energy
is more then we can make a thermal hop
over this barrier with a probability
that's that's given by this Boltzmann
factor this is called the metropolis
rule so essentially we we go downhill
and then when we're stuck we don't have
to be stuck forever there is some
probability that we'll make a hop over
this hill which is dictated by the
energy difference but also by a
parameter which is like a physical
temperature but it's really just a
simulation parameter this this
temperature is typically lowered
gradually so that the probability of
making a thermal hob goes down and if
you're lucky you will find the global
minimum in this manner so lowering the
temperature is called annealing and it
turns off these thermal fluctuations
that allow you to hop over a barrier so
this technique is very powerful it's
very popular simulated annealing okay so
so now let's go back to 2d wave after
this brief introduction and let me start
to talk about some of the the actual
work that we and others have done in
trying to address these questions so
first regarding quantumness tests
okay so let me tell you about the test
that we did which was published in this
paper last year which attempted to
distinguish the d-wave performance from
the performance of classical algorithms
as well as a simulation of quantum
dynamics so the strategy was to find
hard test problems for the machine and
and so these these test problems were
designed this is by the way that the the
chimera stroke graph of the d-wave one
machine the the first of the machines
that we had with a hundred and eight
functional qubits out of 128 so what we
did was we we picked random couplings
let's say plus or minus 1 although we
also considered other values on all the
edges of this chimera graph now if you
have an izing problem that has a random
plus minus 1 couplings on this graph I
already mentioned earlier that this is
this is np-hard because this graph is
non planar so these these are hard
problems these are not the kind of
problems you can easily solve in your
head and we we picked a thousand
different choices of these random
couplings so every time we pick a plus
and minus 1 for for all these couplings
and we did that a thousand times and
then we ran the machine a thousand times
per problem instance ok so we did that
using the D wave one and at the same
time we also tried to solve the same
problem the same izing problem same 1000
of these izing problems using classical
simulated annealing a classical model
called dynamics which I'll describe
momentarily and also quantum model
so-called simulated quantum annealing or
quantum Monte Carlo which is essentially
a description of the the quantum
equilibrium State
as the d-wave machine evolves or so we
believe so let me briefly talk about
this classical spin dynamics so this
this is one of the classical models that
we compared the d-wave machine against
every qubit is represented as a
classical spin and we simply solve the
Newton's equation of motion for that
spin with a magnetic field that mimics
the the d-wave Hamiltonian ok so you
have a spin it evolves according to an
effective field that's essentially the
same as what the d-wave qubits feel all
right so so what's what's the idea here
the idea is to look at the output of the
d-wave machine if it's a quantum machine
then perhaps the output is going to be
distinguished from the output of these
classical models and if the quantum
dynamics model is correct this model
then it will match the output of the
d-wave machine ok so to do that here's
what we we did we pick one of the 1000
specific random instances performed a
thousand annealing run so we ran the
machine a thousand times just to collect
statistics and if we find the correct
ground state the correct answer as times
out of a thousand runs we we call that
the success probability P as and
calculated as s over a thousand then we
repeat that for many instances that's a
thousand different instances and we plot
the histogram of the success
probabilities ok so here this graph is
actually from the classical spin dynamic
simulation but it doesn't matter it's
the same idea you have the number of
instances that had a given success
probability being plotted here and so
for example if you had a certain if you
had a successful bility of 0.4 you you
group together all the instances out of
a thousand that had successfully 0.4
into this bin ok so just a histogram of
the number of instances with a given
success probability now if you had
instances that had low success
probability over here we call them hard
and if you had instances that had high
success probability we we call them easy
all right so over here are the instances
where the ground state was was found
always over here you have instances
where the ground state was never found
they were hard all right so now here are
the experimental results for these
histograms this is from the d-wave one
experiment and you see that it's bimodal
it has a peak at low success probability
so it finds a lot of the instances hard
but it also finds a lot of the instances
easy and most of most of the
intermediate success probabilities are
not populated very much here's the
histogram for the quanta model and it
looks remarkably similar okay so in this
sense the d-wave result matches the
predictions of the quantum model quanta
Montecarlo here is the result from
simulated annealing and you see that it
doesn't match at all and so similar in
annealing which again is the model that
inspires quantum annealing which is what
we believe hypothesis to test that the
d-wave machine implements does not match
submitted annealing classical similarly
annealing does not match do you have
experimental results however the spin
dynamics model right which was this
model that I talked about on this on the
slide here sorry my responding as
quickly as I'd like so this this model
here the so-called Lando Lipschitz
gilbert model spins that are evolving
according to the dynamics that is
similar to that of the d-wave machine
but their classical spins that model
also has a bimodal distribution and so
at least at a qualitative level the fact
that it's bimodal is post a unimodal
that is a match for the the d-wave
results so that's a little disappointing
perhaps and so we're finding that the
d-wave results are inconsistent with the
thermal annealing cycle simulated
annealing model but it's consistent with
both a quantum model
and this other classical model of spin
dynamics but if you look carefully you
see that this this peak here is is way
too big right and so if you look a
little deeper instead of looking at
histograms we could look at correlation
plots which are more detailed so here
where every instance of the 1,000
instances that we ran is plotted on this
correlation plot and the colors is just
a number of instances with a given pair
of probabilities so I'm on this axis
it's the probabilities of running the
d-wave a certain well there's a process
called gauge averaging but you can think
of this as d-wave compared against D
wait
and so if D wave work perfectly stable
and perfectly reproducible then every
instance would have the same probability
whether I ran it this way or that way
but that's not the case we see there is
some noise right so ideally all the all
the instances would have been on this
diagonal meaning that there's perfect
correlation that's not what we're seeing
so you see that there's some scatter
here sometimes we ran the machine and it
produced a probability of 0.5 for the
same instance run a different time it
produced a probability of 0.1 so there's
some scatter okay so this is the the
noise that is intrinsic now if we
compare D wave to simulated quantum
annealing to the quantum model we see
that it's noisy but it's just about as
noisy as as d Webb against itself you
can't expect to be better than the NEAs
correlations so once again we have
confirmation that the quantum model
correlates very well with D wave as
about as well as d wave correlates with
itself
sure yeah and it correlates well but if
you look at at simulated annealing
versus do you wave where we already saw
that the histograms were we're a poor
match we we now see in more detail that
the correlation plots are are also a
poor match so that's no surprise we
already had a rejection of simulated
annealing if you'd like but now here's
the spin dynamics model the one that
also had the bimodal distribution just
like do you wave and you see that it
actually correlates very poorly and you
see that huge peak of low success
probability that it had right here okay
so this there's more detailed analysis
rules on spin dynamics classical spin
dynamics as a good model of the d-wave
machine so what we're left with is a
good match with simulated quantum
annealing there is a quantum model that
correlates well of the d-wave machine
and there are two classical models that
we've been able to rule out using this
technique now of course you can say well
that's not the end of the story it
couldn't be right because there are
infinitely many possible classical
models and that's true and in fact after
we did this work a paper came out which
showed which which proposed a different
classical model which was in a way a
hybrid of classical spin dynamics and
with Monte Carlo updates for the angles
of Oh two rotors or of classical spins
and in the plane and it turned out that
this this model actually was a fantastic
match for the output of the d-wave
machine not only in terms of the
histogram but also in terms of these
correlation plots so the fact that we
found a good match with quantum dynamics
with quantum Monte Carlo does not prove
that that the dynamics of the d-wave
machine it's quantum it just means that
it agrees with quantum right but in in
the standard pop arian way of doing
science we can rule out a hypothesis but
you can never prove anything
you can you can disprove all right so we
disproved simulated annealing and
classical spin dynamics as candidate
models for the new wave machine
we found a match with quantum Monte
Carlo but now there is a classical model
that is also a match so that forced us
to look even deeper and we found and I
don't have the references here
unfortunately but we have a couple of
other papers where we look more deeply
into this into this model and we found
that there are other aspects that don't
match so I won't get into that in the
interest of time but it turns out that
this model too can be ruled out okay so
you can play this game forever
somebody can now come up with yet
another classical model and this this is
a good way of doing science because
these classical models are they capture
certain physics and so by ruling out
that this is the physics that's going on
we are actually learning something very
valuable but there are other ways of of
addressing this question not only do do
we have additional quantum models that
agree with with the d-wave data we also
have and this is things to work by by
people here in particular surgeon boy
show and and the the d-wave team and and
many others some of whom are present in
this room we now have an affirmative
demonstration of the quantumness of the
d-wave machine and that is in terms of
entanglement and multi qubit tunneling
these especially entanglement is very
hard to fake classically the only
downside of these results is that they
are for small numbers of qubits in
particularly the entanglement experiment
is done for up to eight qubits and in
principle I believe the experiments can
be extended to larger numbers of qubits
but but that's very challenging the
advantage of the approach that we
pursued here is is that it allowed us to
address very large numbers of qubits um
more than 100 in fact with the with the
the D way to chip similar types of
experiments have been done and and we're
looking at at 500 cubits okay so so
that's all I wanted to say about the
quantumness testing again I believe that
we can put a checkmark next to it and we
can move on to the question of whether
the machine is faster so this is the the
benchmarking problem now this is a busy
slide and I'm gonna try to explain it to
you as best I can
it's from from this paper that was
published last year and it shows you the
performance in terms of time to solution
for the d-wave machine and simulated
annealing as a function of problem size
so we're plotting square root of the
number of spins here for a technical
reason that has to do with the tree
width of the chimera graph it doesn't
matter so this is log of time to
solution as a function of problem size
the dashed lines are d-wave I'm sorry
our simulated annealing the dashed lines
are simulated annealing the solid lines
are d-wave and the different colors
represent percentiles so you can think
of that as hardness so the hardest
problems are at the highest percentile
the 99th percentile is then the 99th
percentile the hardest problems so 99 is
hard 1% are the easier problems and
moreover you need to know what the
problem parameters were so we set the
local fields to 0 we set the couplings
to plus or minus 1 in this plot at
random plus or minus 1 at random just
like in the in the previous quantumness
testing slides and now what we're
interested in is what is the time to
solution scale like alright so this is
log versus square root or Lin if you'd
like and so a straight line here means
that the time scale that's financially
in this variable root m and we see that
these lines they they tend to become
straight so there's exponential scaling
which is consistent with the hardness of
these problems but the slope is what
matters if there's an advantage in using
the d-wave machine then its slope should
be lower
it means it's it scales better
so what do we actually see well we see
that let's let's compare like colors so
take green for example green as the
median so you see that apart from the
behavior for very small problem sizes
here there's a nice match in the slopes
so it looks like for the median d-wave
and similarly in a kneeling if you want
to extrapolate they scale very similarly
okay now on the other hand if you look
at the hard problems that's the black
line the reason the duis line terminates
here is because it actually couldn't
find the solutions here and I think your
I will tell you that this slope is
somewhat larger than the slope of the
simulated annealing black line and in
fact if you carefully analyze that you
find that that's the case
so do you weigh scales worse here then
simulated annealing on the other hand it
seems to scale better for the the easier
problems the lower percentiles so of we
might be tempted to conclude that we see
a well kind of a break-even performance
for the median percentiles we see worse
performance for the harder problems and
we see an advantage for d-wave on on the
easier problems but we have to be
careful because in fact it turns out
that these lines are a little bit too
flat and what's really going on
apparently is that the annealing time
which is the time to run a single cycle
on the d-wave machine which happens to
be 20 microseconds on the on this DOF to
machine is too long and that means that
it's taking it too long to solve the
small problem small problems are easy
and if you just wait a long time before
you spit out the answer well then you'll
get an artificially flat flow so we have
to be very careful in interpreting this
as as a speed-up and if we go to harder
problems this is where the the J's are
now chosen as multiples of plus or minus
1/6
see first of all if we compare these two
you see that the slopes have all gone up
all right this is the heart of problems
again and now we also it seems like the
advantage we might have had for the
easier problems that advantage seems to
have largely dissipated the slopes now
become kind of similar so overall it
doesn't look like for these randomizing
problems we see an advantage using the
d-wave machine over simulated annealing
and some of the nailing is is not
necessarily the best classical algorithm
out there there are highly optimized
algorithms that are available that will
perform better then simulated annealing
so at this point at least from this
study we were unable to include that
there's a speed-up and there are many
reasons that can come into play as for
why there is no evidence of a speed-up
here one very interesting reason that
was proposed in this paper by Carver at
all is that actually this set of
problems the randomizing problems they
might actually be too easy for simulated
annealing technically it would cost
Gobber at all I pointed out is that
there is no spin glass phase which is
where the problems become hard until the
very end in simulated annealing until
you reduce the temperature all the way
down to zero roughly this means that
these problems are in some sense too
easy for classical algorithms and that's
why you don't expect there to be a
speed-up and then it's it's fair you
have to be very careful in the way that
you choose your problems if you're
looking for Qantas theatres if you pick
a random problem and you fed it to a a
perfectly working circuit model quantum
computer in all likelihood you would not
see speed-up we know that there is speed
up for factoring that's a very carefully
crafted problem random problems don't
necessarily have to be amenable to to a
speed-up so this is one one potential
problem there are other things that
might be going on and to me the most
interesting possibility is that what's
really going on is that the machine is
actually still too noisy to
um decoherent and this would not be
surprising because we know from very
general considerations that quantum
computing is unlikely to work without
air correction so we have to think about
error correction carefully this was
pointed out from the very earliest days
of quantum computing there's always the
coherence of noise going on I've
mentioned in the beginning when I told
you about the a random collapse of the
superposition we know that air
correction is essential and so is it
possible that this low success
probability that we see in the d-wave
output over here this this peak which is
a responsible force for the scaling
partly is it possible that this is
actually correctable maybe we can get
rid of these low success probability
events by introducing error correction
that would be very exciting if we can do
that so I that's the the last thing the
last topic I'd like to talk about it's
the topic of air correction I daresay
form of quantum error correction even
for the d-wave machine and the example
I'd like to start with is is a very
simple one it's a case of anti
ferromagnetic chains let's see how the
machine performs on these so what's an
anti ferromagnetic chain it's simply a a
chain of qubits or spins that are
coupled with a positive coupling
constant and the positive coupling
constant means that they want to anti
aligned so they want to flip-flop that
minimizes the energy because then the
product of Z sub I which is plus or
minus 1 times Z sub I plus 1 will be
negative so in this problem there are
two ground States the ground states of
up down up down up down and the
electrons that is down up down up down
up those are the two ground states of an
anti ferromagnetic chain
okay so this is a trivial problem you've
probably solved it in your head faster
than and it took me to say all these
words and the question is what happens
when we run the deal with machine on
this kind of problem how well does it do
and it turns out that actually it
doesn't do very well so here is the
probability of the correct answer that
is correct answer is finding either one
of the two ground States as a function
of the length of the chain and you see
that this probability drops rather fast
all right so it's not finding for change
of length 80 and above or so the
probability is just about zero it's just
not finding these ground states what's
going on this is a trivial problem I
you might say well if you can't solve a
trivial problem how's it going to solve
a hard problem but actually while this
is a trivial problem for you it's not a
trivial problem for a machine like the
d-wave machine because of the phenomenon
known as domain walls and so let's say
that bit number three here flips
accidentally there was some thermal
fluctuation that flipped it well then
what's going to happen is a is a cascade
energetically it's going to be
favourable for all the other bits to
flip as well and that's an error right
here is where the air is formed this is
this is a kink right here and then the
rest of this is called a domain wall so
this is the kind of state that you would
get which would correspond to an error
in the output it's the wrong answer it's
not the ground state it's in fact
excited state and you can easily see
that these are there's a combinatorial
explosion of these kind of possibilities
so they their entropy is very high okay
so what can we do we can try to air
correct this type of process and the air
correction I'm gonna glue together a
bunch of different concepts here which
hopefully will will be clear
so the first thing we're gonna do is
we're going to use a repetition code
instead of thinking about the the spins
as as individual spins
let's encode or cube it as an individual
cubed let's encode each physical qubit
into three so we're gonna represent a an
encoded qubit using three physical
qubits so this is going to be an example
of a repetition code and I've drawn two
here because I want you to imagine this
as being part of a unit cell and then
the next thing we're gonna do is we're
going to add the fourth qubit here let's
say the red one this is now an entire
unit cell all right eight qubits total
and this fourth qubit is going to be
couples theremin netic elite to the
other three meaning ferromagnetic li
means that it's going to try to keep
these three in check so these three are
going to have to be aligned with what
the fourth same is true for the blue
these three blue ones are going to have
to be aligned with the fourth blue one
because of the thermal kinetic coupling
so this is the the term that's being
added if you'd like to to the the
problem Hamiltonian here this is the
penalty term and then we're going to
combine the problem Hamiltonian which is
now encoded that's why we have bars the
bars represent these logical qubits we
add these two and we stick coefficients
in front of them which we can control so
alpha times the problem encoded problem
Hamiltonian plus beta times this penalty
and this is our total new encoded plus
penalty izing Hamilton so this this is
the way we're going to represent the
problem rather than in its Barenaked
form we're going to replace the original
izing Hamiltonian by by this expression
here which has two advantages there is a
repetition code which we can decode and
there is a penalty term which suppresses
random flips of these three data qubits
over here because they're forced to
align with the fourth one
so so this is the encoded problem that
we're gonna run on on the new wave
machine and everything here is it's
feasible because all we're doing is if
you look carefully you see that we're
just adding Sigma Z times Sigma Z terms
we're just adding interactions which are
available to us on the machine and this
is a big deal because a lot of error
correction strategies that are available
from the literature and quantum error
correction are not implementable on a
machine like the d-wave machine because
they would require higher-order
interactions I'd say three body or more
here everything is two body interactions
so this is actually implementable on the
d-wave machine and and again the fact
that we have a repetition code allows us
in at the end to decode by a majority
vote telling just do a majority vote on
this triple here and see which is the
majority and take that as the the answer
to the problem so what happens when we
apply this technique now what you're
seeing here is the same plot as before
the probability of a correct ground
state of the anti ferromagnetic chain as
a function of chain length and the first
curve to notice is the curve that I
showed you on on that first slide of the
antiferromagnetic chain so this is the
probability as a function of chain
length without doing anything okay where
it dropped very fast but now you see
that all these other curves are higher
and they actually correspond to
different levels of doing the error
correction and there are too many curves
here to explain so I just ask you to
focus on two others this this is the
benchmark the reference this is what we
have to do better than now if you look
at what I'm calling classical which is
these triangles pointing to the right
it's this curve that starts up here and
then it goes like that so what is
classical classical is when all you do
is you just use triples instead of a
single so you encode your qubits but
forget the penalty that's just a pure
classical repetition code
all right so instead of working with a
single qubit we're working with triples
and then we do majority voting that's
that's that curve classical so of course
that does better than doing nothing
because you're getting yourself three
chances in a way and that's better but
if we also add the energy penalty which
gives rise to actually gives rise to a
non commuting term and so it's a quantum
effect this energy penalty we get the
yellow data the Diamonds what I'm
calling Q AC for quantum annealing
correction and you see that while for
short chains it doesn't work better than
the simple classical strategy as the
chains get long enough it starts to win
so this this strategy of not only
encoding but also adding an energy
penalty which locks the qubits into
place and again that there's a quantum
effect going on there because of non
commutativity this strategy is the best
of all the strategies that we've
explored now how good does it go does it
actually work how well does it actually
work so here I'm looking at the
probability of success as a function of
this problem energy parameter alpha
remember that's the coefficient in front
of the encoded heisting Hamiltonian
think of it as temperature think of this
probability as a function of inverse
temperature so temperature is high over
here temperature is low over there and
this is for fixed chain of length 86
that's the longest encoded chain we
could fit on on the chip so this is
these are the hardest problems there the
longest chains so success probability as
a function of effectively temperature
hot temperature means success
probability is low cold temperatures
means success probability is effectively
high but the interesting thing is that
the unprotected chain where we don't do
any encoding or energy penalties is down
here and you see that
when this problem energy skill or the
temperature is sufficiently high the
probability drops to zero yet if we do
our quantum annealing correction we can
actually kind of revive it from zero to
some small value and the the revival
happens everywhere okay at any scale of
the problem energy or at any temperature
effective temperature we get an
improvement by using this quantum
annealing correction and we always do
better than the simple classical
strategy which which are these these
triangles here so adding the energy
penalty in addition to doing the
majority vote on encoded qubits is is
the strategy that always does best and
the the relative improvement gets better
as you decrease the problem energy scale
or as you increase the effective
temperature so we win more as the noise
gets higher and that's that's very good
so the last couple slides I just want to
very briefly show you what happens when
we apply this error correction technique
to not change which are intrinsically
uninteresting as an optimization problem
but what happens when we apply the air
correction technique to hard problems
actual optimization problems so this is
the chimera graph of the d-wave chip
after you encode it using this code we
started from a degree six graph and it
reduces down to a degree three graph and
as it turns out it's not hard to see
that so what we do is very similar to
our benchmarking studies we pick a
thousand random problem instances over
this graph we use couplings that are
multiples of plus or minus six so those
are fairly hard optimization problems
and then we do the same thing as we did
with earlier with benchmarking we run
each of these instances a thousand times
at the lowest annealing time available
of 20 microseconds compute the empirical
success probability infer the time to
solution and now we're going to compare
the results
of the classical repetition code to the
quantum annealing correction strategy on
these problems and what we find is this
is effectively that the time to solution
or if you'd like the number it's
actually the number of repetitions
required how many times you'd have to
run the machine to to succeed with
probability 99% again as a function of
problem size and now what we're
comparing is not simulated annealing but
rather this classical error correction
strategy that's see that's the - to the
solid lines to quantum annealing
correction and what we find is that for
the different percentiles consistently
across the different percentiles the
quantum annealing correction strategy
reduces the required number of
repetitions that is it increases the
success probability of finding the
ground state and the improvement is
better the heart of the problems get in
two ways both in terms of larger
problems get bigger improvement and also
in terms of higher percentiles the
higher percentiles here is the 95th
percentile the improvement is relatively
larger than for the the lower the lowest
percentile all right so so the heart of
the problem is the more this strategy is
effective and what's particularly
important here is that the slope if you
can associate a slope with this rather
discrete looking curve it's clear that
the slope is lowered by the quantum
annealing correction strategy right so
we've actually improved the scaling of
the time to solution using this strategy
and that's exactly the kind of effect
that we're looking for using our
correction to improve scaling okay so
with that
let me summarize there's a few
references here which addressed some of
the work that I talked about and I've
asked a few questions about do you wait
is it a quantum annealer and the
evidence we have suggests that it
probably is in the sense that it
disagrees with all the classical models
that we've tried to test it against and
moreover it exhibits entanglement at
least for small numbers of qubits so
that's good news for quantum annealing
does it actually do what it's advertised
to do that is does it actually solve
optimization problems using quantum
annealing probably yes I mean it
certainly does solve izing like
optimization problems not for all the
reasons I I told you about does it do it
well enough to generate a speed-up too
early to tell but it's likely that using
our correction we'll be able to get the
performance to improve significantly in
fact I've shown you some evidence to
that effect and finally this is where we
started so how far information
processing arrived and I would say that
the answer is undoubtedly not yet we're
working on it okay and let me thank all
the people that I had the pleasure to
collaborate with in no particular order
actually
so listed here are all the people that
contributed to the work that I told you
about here
Josh Jobe is a grad student in my group
g'way Wong was a postdoc who is on her
way to to NASA
Sergio did some of this work while he
was at USC and has been in Google for a
while now trolls were no postdoc of
Matias Troyer down here at Sergey
Astakhov as well now at Google
Dave wacker at Microsoft John Martinez
and what you know meteo story I
mentioned and Kristin Putin's who was a
grad student in my group is now at
Lockheed Martin and she and postdocs I
mean
did the air correction work and also let
me thank all the general sponsors listed
at the bottom here and you for your
attention
yeah so let me start with the first part
so air correction as I understand it
typically involves overhead there's
there's always overhead and most most
quantum error correction involves huge
overhead in fact if you want to go into
into fault tolerance using concatenated
codes then almost all your qubits are
are doing our correction for you and
also most of the time what you're doing
is our correction so a factor of four is
actually not too bad it's pretty good
but the real question is am I better off
using this technique than just running
four copies of the same problem because
that's that's the same resource and
that's what I showed you all right that
was the point of the comparison of the
qac technique to the what I call the C
technique and the C technique we're just
using four copies all right so we want
to compare apples to apples same amount
of resources we're doing better than
than just four copies are we doing
better than no copies at all well of
course we're taking this hit by a factor
of four so it's what is fair to compare
is n qubits unencoded with n
and coded cubits and yes we're doing
better yeah
oh and finally you asked is it possible
to use fewer no we tried it and you get
no improvement yes and that's actually a
very viable prospect the next generation
of the d-wave chip that's already
operational has more than a thousand
cubits the one after that in fact that
is part of a larger chip that's part of
a 2,000 plus qubit chip
so making cubits is actually they're not
as scarce as you might think a factor of
four so every every year or two is not
unreasonable
current
well the phases let's instead of
thinking about complex phase let's just
think about plus minus one that's that's
rich enough it turns out in fact that
all the power of quantum computing is
already in that so that allows you to
create interference so you can imagine
taking a state that is ket 0 plus ket 1
and another state that is kid 0 minus
ket 1 if you add those two up you
interfered away that one the ket 1 right
and so that's that's interference that's
that's what that buys you and that's
considered to be a resource an
indispensable resource in terms of the
power of quantum computing
right right another way to save what
masu just said is you can think of a
quantum computer evolving all candidate
answers in quantum superposition but
again if you measure you're gonna get
possibly random garbage so what you want
to do is amplify out of that see of
different possibility you want to
amplify the right ones you do that using
interference
yeah so so the question was whether the
d-wave machine has built-in air
correction and well it there is a lot of
very clever engineering that goes into
it which you could call our correction
if you'd like this filtering there's
compensation many engineering style
tricks that are without which the the
machine wouldn't have worked at all
so at that level I would say that yes
there is air correction what we're doing
is we're using the machine as it is in
order to introduce yet another level of
error correction which is inspired by
tricks that we've learned from the field
of quantum information and quantum error
correction which allows us to boost
performance even more now there's
nothing in principle preventing the next
design of the d-wave machine from
incorporating this way of doing air
correction inherently so this could be
an object or an option you could choose
on the next generation of the d-wave
devices and I I think that would be a
good idea
yeah so question was why aren't all all
the qubits usable why don't they all
yield and well a qubit as you saw is
actually a macroscopic object it's it's
a loop of current rather it's a metal
loop and and they they're not all
identical when they come out of fab and
there's a lot of control circuitry that
surrounds every qubit I didn't show
those pictures but it's in fact an
incredibly complex engineering a piece
of engineering um and you know things
happen in fab and so not everything
comes out as well as you would have
hoped in fact the yield out of they
produce thousands of chips and only a
few actually are up to specs and that's
fine you can throw a great majority of
your defective chips as long as you you
find one that's really good
I'm sorry the order what
yeah so so the question was whether
lowering the degree of the graph having
as low as six limit the the types of
problems that you can solve and so this
yes and no ok so first no so this graph
is bipartite it's and it's it's non
planar which means that you can you can
put np-hard problems on it you can embed
and be hard problems on it ok so in
terms of the computational complexity
the answer is it doesn't do damage from
a computer pure computer science
perspective however from a practical
perspective if you try to embed a
real-life problem like let's say image
processing for machine learning or
traveling salesman or protein folding or
whatnot then you you take a hit because
of this low degree so typically the
strategy is to to try to map the actual
graph to a effective complete graph on
the complete graph you can embed
anything and and finding that mapping
for a particular problem is itself hard
and there are various heuristics that
that are being used for it so in
practice the degree six and certainly
the degree three that we get on the
encoder graph create problems</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>