<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Multipath TCP | Coder Coacher - Coaching Coders</title><meta content="Multipath TCP - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Multipath TCP</b></h2><h5 class="post__date">2012-06-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/02nBaaIoFWU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm question Rachel from University poll
taking a Bucharest and I'll be
presenting together with Kristoff from
University category one our work on
multiple TCP so this works started quite
a while ago five years ago when I was a
student a PhD student in London in UCL
where I was working with my adviser
McKinley and Amon was Czech and also
this is joint work with Sebastian and
Olivia and many many others to Fabian
and many many others from many
institutions ok I'll start off by giving
a simple observation you know networks
are becoming multiplied we all have
mobile devices these you know either
whether there are smartphones tablets
you know even our cars have computers
and basically what these have they have
multiple wireless connections each of
them with different properties so they
have different coverage they have
different they give you different
throughput and they even have different
energy consumption right so 3G is
generally more power intensive and Wi-Fi
ok so to get a good mobile experience
what mobile devices have to do is
basically use somehow all of these
wireless connections and make the best
of them
oh that didn't work well ok so in a data
center you know we've got tons of
servers many racks and so forth if you
actually open one of the ok so this is
the animation that went wrong if you
open one of these wrecks what you'll see
is well you have servers connected to
one of the top of the rack switches and
you have many racks and between the
servers you say I have a redundant
apology and what happens is that I mean
you guys already I mean you know this
was all too well you have many paths
between any pair of servers in your
datacenter and finally servers
themselves the the ones that are facing
clients they are also multi-home to get
better redundancy to get better
throughput right so we still use these
networks by running TCP over time like
most of these most of the applications
today I mean why very big majority they
still use TCP and what this gives you is
a bi it's a reliable byte stream so you
know is that you'd put bytes here they
come out in the same order
you know without losses on the other
side and the nice thing about TCP is it
also adjusts the load to network
conditions it does congestion
draw but TCP single path right so once
you start a connection it gets placed on
a path there's no way to move that that
connection or not the path and there's
no way to actually move the endpoints of
a connection and this is this is a
symptom of a problem and the problem is
that there's a mismatch between the
multi path transport and the single path
the multi path networks and the single
path transport and this creates problems
and let me tell you what I what I mean
my problems so here's me commuting to to
my University you know listening to
internet radio right I'm using 3G
Costanzo no Wi-Fi coverage on the route
and as I get as I get to my uni finally
to my to my office that's Wi-Fi right at
this point I would like for for for my
radio to switch to Wi-Fi because well it
cost me less money it actually uses less
energy so my mobile will actually make
it to the end of the day with with
enough battery so but trouble is TCP has
bound the the connection to the 3G
interface and the minute I killed this
interface the connection goes down right
so if I want to move this here all of
the ongoing connections must die so what
I have to do is either manually restart
the internet radio or somehow the
application has to adjust right and this
is not great another example is in data
center so this is a fat - data center
it's it's specific redundant topology
that has been proposed a while ago in
the in the research so let's say we have
these two red servers here shown shown
in these two racks and they connect and
they can choose out of the four
available paths between them normally
the the choosing is done randomly by
something called equal cost multi path
so they choose this random path and they
they they get maybe ten gigs or one gig
who knows what the what the links are in
the network okay so so then we have
these black servers also wanting to talk
to each other and then again they will
choose a random path and what happens
here is that the random choice has a you
know non negligible probability of
putting two flows on the same path and
you get a collision so what happens is
that each flow will get half of the
Bender's it should be getting and
there's parts of networks that are
completely idle okay so and how bad is
this well I'm just showing here the
effect the the output of a single
simulation we did so we're simulating a
data center that has a factory
Terk as shown before we've got 8,000
hosts and what every host sends to a
single other host such that everyone is
receiving from a single other host and
sending closing weather so it's called a
permutation traffic matrix so it's very
it's a contrived example right so what
I'm showing here is the throughput in
megabits so maximum is is one gigabit
and then I'm showing the flows in the
individual flows through put they are
ordered by the throughput they are
actually getting in the experiment so in
theory the factory network gives you
enough bandwidth to actually give every
host one gigabit per second collisions
make it so bad that you see the red line
that's what TCP would give you and you
get a lot less than one gigabit so there
will be some flows the lucky flows to
the right that actually get more than
600 megabits but the majority of flows
get something around 400 300 megabits
and there's a few unlike you want to get
even 100 megabits and this is just
because of these collisions okay yeah oh
this these are just unbounded flows so
you just start a TCP connection you CD
you just see the throughput you get if
if if all of your all of your flows are
small you shouldn't and you're not
utilizing your solution you shouldn't
see any effects so it'll be yeah if
exactly so if if all of your flows are
small this should this effect should not
happen well it depends how big they're
right I mean well if they're all big
you'll still see effects the effects
will be smaller right I mean the these
collisions are persistent and in
particular this this traffic matrix is
actually the worst so you have the
smallest number of flows that could fill
the network and they don't fill it right
and this so the effect is this is the
worst you could get so you know this
brings with multiple TCP what we really
need what we think we need to do is to
evolve TCP into something that can use
multiple paths natively right and this
is what multi participe does it it's
just an evolution of TCP that allows a
single connection to spread over
multiple paths right and we built this
from the beginning thinking that in the
applications that do not have to change
to use multi path TCP and the nettle
does not have to change so the
applications use the same socket
interface they actually think
they're using TCP they don't know it's
multipath tcp underneath they use the
same system calls and somehow underneath
the stack just opens multiple
connections if it sees fit
this works over today's networks and we
tested and christoph will tell you a bit
about it later and this is getting to
staging the ITF where it's ready I mean
the standard will be coming out in a few
months so it's a you know it's
reasonably mature no so unmodified
applications will use exactly the same
API there is an extended API that
applications that do know about
multiplies TCP they can use this to
figure out how many sub flows they have
and stuff like that but the unmodified
applications they use exactly the same
TCP that's the same circuit idea I mean
there are if you bind to a specific
interface then the stack will
automatically disable most paths because
they thinks you know it think that you
actually want to use that particular IP
right but in general if you bind to in
order any then yes yes yes
yeah so basically what what you would
get so if your TCP connection is using
the is using it to get the peer address
to get the P address on the first sub
flow so when you first open a multipath
TCP connection that's it's like that's
the first stop flow that's let's say the
first connection you're pointing at the
guy and then you can actually add other
subtle so the the peer the PR IP you'll
get for the first up lowering right I
mean there was a huge argument in the
ITF over this whether if you lose the
first sub flow should the connection go
down or not you know Jo touch was a big
guy in this argument we can talk about
this offline right okay so this is these
are all the components in multipath TCP
I will be a only covering connection set
up sending data over multiple paths and
then congestion control and some
applications okay so how does MPDC start
so we start with a regular TCP handshake
you know send a syn
but the sin has a new option it's called
MP capable and the X there is a
connection identifier that the client
gets to this to this connection right so
the server if it multiplies TCP enabled
it will reply again with the MP capable
option in the syn and again with its
local connection identifier say this is
why and once this handshake goes through
both the client and the server will set
up state and basically the server
remembers that it has a multi path flow
with connection identifier Y and then
for each of the sub flows it will
basically keep more or less the same
state variables as for a regular TCP no
congestion window sequence number for
the sender for the sending side for the
seeding side and so forth
okay so if you get a new a new path for
instance a new Wi-Fi interface and your
3G interface what what the client will
do is actually send a join using that
new address and it's again it looks like
a regular TCP handshake is that except
with new options and the join option
tells the server that this actually is a
part of an existing connection right
that's why it's using a different a
different option right so the server
when it gets this actually looks up the
connection it finds it and then it
replies with the with with a join and
the sub floor setup I mean this is
simplified right in in practice we have
some H max in there and you notice it's
a bit more complicated to get a bit more
security now we don't want to allow
anybody to just send a join and join the
multipath TCP connection right but and
what I'm showing here is just the
conceptual operation right in practice
it's a bit more complicated I'm not
going to cover it here okay so you know
for each stop loss I said there's a
congestion window that you maintain
there's a sequence number and so forth
alright so you know this is a TCP packet
header what we've done so far is we've
synchronized the endpoints they know
they're using multipath tcp they know
the sequence numbers for each of the sub
pros and so forth how does multiply TCP
change this packet header right and the
biggest question is over the sequence
number right so TCP is the sequence
number to allow the receivers to put
packets back in order before it passes
into the application and it allows the
sender to figure out which packets were
lost by looking at the X right okay so
in multiply tcp obviously packets need
to go multiple paths so you have two
things you really need to do first of
all you need
sequence numbers to put packets back in
order before you pass them to the
application but this is what TCP does
and we have exactly the same contract as
TCP and then you also need sequence
numbers to tell whether there was a loss
on a specific path right so you need to
know when you you when packets actually
get through or packets are lost okay so
there are two high level options you
could have a single sequence pace across
all subfloors shared across all
subfloors or you can have one sequence
pace per each path per sub flow and
another sequence pace at the connection
level and it turns out that the second
option is is preferable and there are
two main reasons right first of all
because you have a sub flow sequence
pace for each of the sub flows it's very
easy to tell which packets were lost and
which packets were not lost right also
in the internet firewalls and proxies
don't really like TCP connections with
gaps in the sequence number that these
will not go down well right so basically
the second the second arguments actually
a killer if you have to the policing the
intern the first argument it's more
about getting all the performance rather
than you know you could use SEC to
somehow mitigate that but it's not it
doesn't get you the same things okay so
where do we so this basically means that
the TCP header must must hold the sub
flow sequence numbers right so here's
how the packet header for multi
plasticity looks like right so source
port destination for these obviously
belong to the cell flow the sequence
numbers they also belong to the flow so
what the middle box is really see is
something that looks like a TCP
connection it just has some new options
in it and the new options refer to the
data sequence number and the date ACK
and I'll tell you about that next so
here's how multiple TCP works whenever
an application actually calls send the
segment is assigned a data sequence
number right in this case say it's
10,000 all right now multiply TCP we'll
look over the sub flows and see where it
can send this this segment it typically
chooses if it has multiple choices it
chooses the sub the sub flow with the
smaller strategy okay so say in this
case I can send on the red sub floor
right when it sends on the red stop flow
it will get the segment or get a
sequence number on that sub roll in this
in this case 1000 and it gets sent right
so the 1000 gets put in the TCP header
the
the sequence number gets put in the
options okay all right so basically the
sequence number is used for X on the red
sub flow it's used by the client to tell
which packets are lost and what is mr.
transmitted and so forth right it does
whenever you know it goes into faster
transmitter it is three duplicate acts
and so forth okay when I get another
segment say 11,000 the data sequence
number
this is mapped on the blue sub flow and
it will get a sequence number from the
blue stop flow see 5,000 and again this
is sent sent over right say the red
packet the red packet gets to the
destination okay and the data sequence
number is used by the receiver to
reorder the segment before it passes
them to the application right so this is
ensure it is ensures that you don't get
your during in the in the connection all
right so when this segment finally gets
there there's a neck generated and the
DIAC will also specify the sub flow
sequence number and the data sequence
number okay so what happens if this path
dies okay if this path dies say this
segment never arrives at the Edit server
so the client will timeout on the blue
stop flow waiting for an ACK the minute
it times out
it actually resets the same data
sequence number on the red sub flow
right and you know the connection the
connection can just go on without
without interruption I say takes one RTO
on the small path to cope with with a
path failure for instance yeah no
actually it stays up and it's doing its
it's normal you know backing off and
retransmitting until they until you
reach the max retransmit count and then
it is declared dead right so if at some
point if it's a transient loss - as soon
as one packet get through then it's
completely fine it just it comes back
into into life right
absolutely yeah yeah absolutely yeah you
could end up sending the same data over
multiple paths but this is only a matter
of efficiency not correctness so the
sequence number will make sure that the
they delivered only once to the
application okay basically yeah yep is
the common case going to be trying to
figure out which of your connections are
better for an infant transfer and
sending most of your data along the
better one millage if I've got Wi-Fi
versus 3G in general - and most of my
data over Wi-Fi if the eye becomes of
course I can't know that our priority so
I have a memory problem etc so so I'll
be talking next I'll be talking about
congestion control which alright so the
question was how do you prefer one path
over the other right you know you might
prefer to Wi-Fi or 3G and stuff like
that for mobile for mobile stuff I think
in many cases you end up using a single
path and then you'll ever I mean I guess
it depends on what the user is willing
to pay for like if you really want the
best throughput then for sure the best
throughput is using all the paths you
have available if you want to save
energy then you could you could look at
what energy Wi-Fi or 3G is using and
just switch to that right but yeah I
mean the technology now for mobile seems
seems to point out that you will be
using one path and just doing a handover
when one that tails more or less yeah so
the path selection is transparent but on
mobiles
you could have preferences set by the
user saying you know use Wi-Fi or
something and that gets pushed into the
kernel so kristov actually will be will
be telling you how we how we implement
this and we'll be showing you some
graphs of handovers and stuff so in
terms of policies just really getting
the options from the user and passing
them to the kernel the kernel can do
pretty much anything you know yeah
so I mean we think there are two really
cool apps at this point data centers and
mobiles and we'll be talking about both
of them in the talk so just hold your
horses Elizabeth yeah I was in the 12
years ago and then there was a
constraint and I think the constraint
was still there the constraint is MVD
CPU requires multiple inputs off but
multiple in IP addresses you want to
that's not true for datacenters so in a
data center multipath TCP can work just
fine with multiple sub flows between the
same pair of addresses the let's say you
have equal cost multi path so the
differentiating point between cell
phones will be different port numbers
right so at least one of the ports will
be different yeah so you can have multi
path between the same yeah the same type
ease but yeah right so let's let's move
on to congestion control right so let's
start with a bit of history so back in
the 60s we had circuit switching right
this is the image I have here on the
left so what what you did is when you
wanted to create a connection the
network could reserve a circuit through
the network and then if you have for
instance this example here where you
have two flows each of them I mean
they're the the usage Peaks are are not
aligned right so what's happening in
this in this network is you get a lot of
on to under utilization because of the
reservation of bandwidth right so then
we had packet switching where you have a
single link to flows but the flows can
actually use as much of the link as they
want
as long as nobody else is using it right
so packet switching what it really does
it actually takes circuits and pulls
them together and this increases the
utilization alright now if we go today
this is what we have today we have
separate links each of them with one TCP
connection or many TCP connections and
of course the usage peak usage Peaks
might might be completely misaligned so
you could you could do you can easily
see that this is what you want to do
right and this is actually what
multipath gives you multipath takes a
number of links and puts them together
it creates a pool of links
and the minute you create a pool of
links the question is well how do you
share the pool and the answer for four
for the four packet switching was well
TCP shares to pull the congestion
control and what we need to do when we
create the protocol that can use
multiple links we need to create an
equivalent of TCP congestion control for
multiple paths right to be able to share
the capacity fair and this is what I'll
be talking to you about next and so
actually it turns out that the most
difficult part of figuring out what
congestion control behavior we want was
actually deciding what are the goals we
are trying to achieve once we have the
goals the controller you know came out
pretty quickly the goals were a bit a
bit more tricky okay so the first goal
is the obvious one you know every time
you have a single TCP connection and a
multipath TCP connection with many
subplots going through the same
bottleneck you don't want multipath TCP
to beat up TCP and get more throughput
right so what you really want is to be
shared to regular TCP at bottlenecks and
this is an obvious one the second one is
not that obvious so it's really about
using efficient pads okay and I'm going
to show you a contrived example but this
actually happens in in networks where
they the Pat you using are not the same
length think like a big cube data center
or you know something like that okay so
what I'm showing here is a multi pass
connection that uses a single hop path
and a two hop path for its sub flow
right so the total flow has a single hop
and the the bottom stop flow has two has
two hops and then I'm adding another
multipath TCP connection where the
bottom sub flow uses a single hop path
and the total flow uses two hot pads and
finally the third which is basically the
same it's one of the sub flows using a
single hop path and the other one is
using a tuhoe path now okay the question
is what do we expect the outcome to be
in this in this case right like how much
throughput can these flows get okay and
if you think that everyone will just use
the two hop path it's very easy to see
that they will get 6 megabits per second
ok and the reason for this is that each
each of these will use two resources in
the network
and if you add up the throughput which
is 36 megabits divided by 6 you get 6
megabits right now if every every
connection every multipath TCP
connection actually uses splits the
traffic equally between the 1 hop path
and dual path what you get is 8 megabits
ok this is not great
the links are 12 megabits it should be
you should be able to get 12 megabits
why are you only getting 8 megabits if
you split traffic 2 to 1 you get you get
9 megabits so if you if you prefer the
shorter path you get more throughput 4
to 1 you get 10 megabits finally if you
put all of your traffic over the shorter
path you get 12 megabits which is what
what you should get right so so this is
really saying that when you're using
multiple paths and some of your pets are
congested are more consciously than
others you should really stop avoid
sending more data on the congested paths
right and there are some nice results in
theory that tell you how to do this
basically and in this particular case
the outcome is that you send no data
over the top ok right and the so this is
the the used efficient path of the
second design goal the third design goal
is well what what I said earlier is that
you should put all of your traffic on
the lower loss path really that's the
strategy at the end point but if you
have 3G and Wi-Fi
you know 3G by definition has very low
loss because has a huge RTT and big
buffers right so you end up putting all
of your traffic on 3G according to that
design goal but this doesn't go down
well because maybe your Wi-Fi path is so
good that you get 10 megabits but you're
putting all of your traffic on Wi-Fi on
3G well you get one megabit right so
that's that's not really nice so this
goal makes sure that there is an
incentive to deploy multiplies TCP and
it says whatever you do in aggregate
multiplies TCP should get at least as
much as TCP is getting on the best path
that multi-party is using so you take
all of the multiple the pass multiple
TCP is using look where you get the best
TCP throughput and you should get at
least that right ok so how do we achieve
this first let's talk about TCP
congestion troll to a little bit right
so TCP maintains a congestion window for
each connection and then only
check you increase the window by on one
/ w okay
and whenever you get a drop you have the
window and that's basically multiplied
TCP works in a similar way right you
maintain one congestion window for each
path okay so each path will each suppler
will get its own congestion window on a
loss each you just have the window and
that's up low so if you have a loss on
the subfloor you just hold on thats
applause it's very similar to tcp the
only thing that changes is the increase
rule okay and if you remember for TCP
this was on 1 / w here we have alpha
over the sum of W across all paths right
so let's forget alpha for a little bit
now it's a constant assume it's a
constant what we have there is a total
window across all of the sub flows
multipath TCP right so what this term
really gives you it actually gives more
of the increase to the sub flows that
has have a larger window so if a sub
flow has a larger window that means it
has a lower loss and what multiple TCP
it actually gives more of the increase
to that sub floor so what this does it
actually pushes the traffic away from
the congested links to the uncongested
links okay now the Alpha part so go-to
is actually achieved by using this sum
here the alpha part achieves goals one
and three right so what what it really
does it actually you look at all the all
the path properties you look at the RTT
and the congestion window on all the
pads and then you can figure out what
tcp would do on that path and you just
sum up your throughput and say am i
getting worse or better than tcp and you
just the alpha parameter to get exactly
what tcp would be getting on that path
okay so if you actually solve this
equation they and you generalize it for
any number of paths the real formula
looks like this but i'm not going to be
spending time describing this it i'll
just say that it's very easy to
implement in practice okay
yeah they will use two different windows
yeah
we haven't actually done the work to
make cubic multipath yet we think it's
very straightforward to takes a compound
TCP and make it work with this because
compound TCP really just fold falls back
to Torino when it fills the buffer we
haven't I mean that's that sort of
future work we haven't gotten around to
doing that yeah yeah yeah so what will
happen with alpha is you'll basically
put as much traffic as you can on 3G
while being fair to TCP so you'll
basically max out it's as if you're
putting a full tea speed on 3G you're
using as much as you can on 3G and then
whatever you need to get from Wi-Fi you
get it from Wi-Fi right yeah yeah I mean
of course if the 3G and Wi-Fi are are
both idle and the application can fill
the pipes to just fill them both right
so it's this is assuming you have loss
like you have external loss if you're
just the only one causing the loss on
the Wi-Fi and the 3G you just fill both
pipes and you get the sum of bandwidth
okay I mean the D algorithm is pretty
simple whenever we get a packet from the
application you just ask what path has
has the free space in the congestion
window and just put it there right yes
then you choose the yellow star tt1 yeah
so if you if your app limited you always
put on a low 31
okay so let's talk a bit about the
applications of multipath TCP and I have
told you how the congestion controller
works let me just show you now what it
does okay
and so what I'm showing here is a server
and we have it has two hundred megabit
links I mean that the speed is not
important we have to TCPS using the top
link and for TCP using the bottom link
right and these are just customers right
so say there you have a customer that
uses multipath
and it comes and starts using both of
these links what will happen is that
actually multipath TCP will push all of
its traffic on the top link because it's
less congested okay so the effect is
that all of a sudden that the top TTP's
get thirty three megabits while the
bottom still get twenty five megabits oh
and multiplies TCP gets thirty three
matching the throughput of TCP on the
best path right okay we add another
connection what's happening here is that
all of a sudden all of the connections
in this scenario get 25 megabits right
if you had another one they all get 22
megabit so we've reached the point where
both of these links it's as if they form
a big shared pool right you know we had
another one you get 20 megabits and 4
and so forth right so it's as if we took
200 megabit links put them together
created one big 200 megabit link and
this is shared fairly between all the
all of the connections right so this is
this is the theory the practice is like
this so we have five TCPS on the top
path 15 pcps on the on the bottom path
and then initially there's no multi path
finally at some point multi path you
know one minute and a half multipath
flow start so what you see is the
outcome in practice right so in the
beginning the the top flows
they're getting 18 megabits per second
the bottom TCP are getting around 6
megabits as soon as multipy multiple TCP
arrives you see that roughly equalizes
the the rates on all of these all of
those links but it's not exactly right
so it's not perfect but it's it's pretty
close so you see that all of a sudden
the two links behave like a single link
sharing capacity okay right so let me
just give you an example for datacenter
Network right so there you have the
problem that as soon as you open a TCP
connection you use random choice somehow
either equal cost multi path or us you
know VLANs or whatever you placed on a
path and it just stays there right so
instead of using a single path per
connection what you could do is use many
many sub flows per connection and if
some of them are congested you just
don't send data over that over that sub
flow right
you just don't care about collisions so
this work let's get back to the example
we had earlier right so we had these two
flows and they're colliding on that link
so what will happen is if if you have a
multiplier flow D the black flow is
multi path it will actually open other
sub flow that stuff flow will get much
better throughput than this sub flow
which is sees more congestion which will
mean that the the multi path connection
will actually push less traffic over
that sub flow giving the red sub flow
the same amount of throughput right so
what you have in this case you first
getting more utilization out of your
network and then you're also getting
better fairness and coming back to the
example we saw earlier basically if you
run multiplied over the same network
this is the throughput distribution you
get with multi path flow versus what is
TCP right and again yeah yeah right okay
there's some chance of that in that
millisecond I'm going to collide with
some other flow and some path and not
yeah at the same time with MP TCP it's
going to take me some amount of time to
figure out which of these paths of
course yeah I mean if yeah what's the
number of round trips
what's the crossover points that works
for infinite demand NP TCP is the work
of course here one packet transfers know
for sure yeah actually that's a good
question I mean what we our experiment
so far we had some experience with very
short flows but we also had some long
flows in parallel and we actually saw I
mean the effects what we wanted to see
is that the short flows don't finish
later and that you still get benefits
for the long flows but actually the
crossover point I have I couldn't tell
you it's an interesting thing to look at
so maybe in the future we'll look at
that yeah I mean we can have a chat
afterwards but it's I think it's a it's
a very good question
okay so yeah
I mean that's a configuration question
when you deploy your your OS in that it
isn't that you just tell it you know
force of flow should be enough and then
what what the stack will do is you start
the first you start the first uh flow
and then you know maybe after it exits
slow start you decide okay this is a
long enough flow and then I should start
the three other sub flows if it finishes
within you know a few hundred times then
there's no point because you're wasting
a three-way handshake really so I mean
in general if if you enable mod that's
that's a good question so I don't know
exactly how you how you do that in jail
I mean if you're on a mobile client for
sure you want to enable it because even
if you have a single interface now it
might go away right in a different that
you know beforehand how many we done
that you have you know in your network
and you know what's an appropriate
number of pets and you just you just
started in the general case we have a
single guy sitting at home connecting to
a server with his desktop I don't know
what's the good answer there like
clearly the guy will be single home home
and it depends on the server's I mean I
guess load balancing with the server's
if you load balance you go to different
servers not the same server so that's I
have no idea in that scenario what you
do so as I said for mobile and
datacenters it's not a problem yeah
absolutely yeah so I mean yeah but I
mean this is like getting TCP can any
TCP extension really so it has to be
like you know when you put this in if
you don't put it in the sense it's
pretty difficult enable it later it's
not impossible but I would say it's
pretty difficult it would the solutions
would be ugly alright so what we did is
actually we went on we wanted to try
this in some somewhat
more interesting setting so we use the
Amazon's ec2 data center so this is
infrastructure-as-a-service you you
basically pay for virtual machines and
we could put our own MPTP kernel and it
turns out that quite a few availability
zone so when we did the experiments I
think only one or one or two of the
availability zones had multipath now I
think more or less all of them have them
so I guess they're rolling out new
networks right so they have multi pack
topology something that resembles what I
was showing you earlier I mean I'm not
exactly sure what topologies you just
you can use equal cost multi pack to
access those paths ok so what we did is
we took 40 medium CPU instances running
multipath TCP and for 12 hours
we just had each of these instances I
pouring all of the other ones
sequentially either using TCP multipath
TCP with two sub flows and force
applause and this is what the results
look like so on the first part here
where the flows get less than 500
megabits this is where you had multiple
paths right and you can visually see
that multi path TCP gets better
throughput than than TCP right and on
the right hand right hand side here
basically these had no multipad they
were there were no multi button here the
thing to take away that it doesn't
actually kill throughput you know it's
pretty much doing the same as TCP does
not hurt even if you're using multiple
sub flows for that connection okay
and with this I'll ask Christoph to tell
you how the implementation is is doing
all of this stuff so you want I think
you want this over okay so on hello my
name is Christoph Bosh and I'm working
on implementation in a Linux kernel of
multi pass TCP so on the implementation
started about two to three years ago and
initially by seven cylinder
nowadays while we are actively three
working at the UCL in Belgium on the
implementation and it's about 10,000
lines of code in the Linux kernel and
you can get it at the URL as shown on
slide
so one detail always empties piece
structured so when the communication
starts so the host on the left is
sending the sim with the MP capable
option so he wants to indicate that he
wants to do MP TCP inside the kernel it
looks like this and we have the
application layer who creates a just the
regular TCP socket and inside the kernel
this TCP socket has some variables added
to indicate that it won't do MP TCP and
so later when the server replies with
the cynic and the EPI capable also
included inside the kernel we create a
so-called meter socket which is in fact
delay or between the application and the
different TCP sub flows and the the
application is just talking to this
meter socket which from the application
point of view looks like the regular TCP
socket and so later on when we start
creating new sub flows over other passes
while the additional sub-clause are
being created and the data that is sent
by the application to the meter socket
will be distributed by the scheduler
over the different sub flows and so on
so from the kernel point of view that's
what it looks like how does it performs
we did some measurements by
interconnecting two two hosts with each
one having two interfaces one gigabit
per second and we do 100 simultaneous
HTTP requests to an Apache server by
doing a by using apache benchmark for
total of 100,000 requests and this for
the providing file sizes from one
kilobyte to 300 kilobyte file sizes and
you can see in fact the number of
requests per second is decreasing of
course with the file sizes increasing
because while the file transfer takes
longer so that's for regular TCP because
it can only use a single interface if we
use TCP with link bonding that's how it
looks like as we
have 100 always 100 sammartino's HTTP
requests and each of the same file size
link morning performs in fact pretty
well and does get roughly twice the same
twice more rigorous per second for files
of 300 kilobytes so 4 MP TCP that's what
it looks like
on the left side for very small files up
to 50 kilobytes we get about the same
performance than regular TCP why is this
it is because the first the initial sub
flow will always four MV TCP be
established over the same interface and
so the ants were from the server for the
small fire effect in fact fits into one
single sub flow and so the establishment
of the second sub flow does not buy us
anything and so for very small files we
get the same performance than regular
TCP but it might be improved if we well
distribute the initial sub flow also
over the different interfaces a little
bit likely link bonding would do it as
the file size gets bigger MP TCP
improves and gets close to the link
bonding results and was very big files
very big like 300 kilobytes we even get
better than link bonding because was a
couple congestion control empties piece
is able to better balance the load over
the different links than link bonding
it's able it doesn't link bonding might
put for example 60 60 percent of the
HTTP requests on the left on one
interface and 40 percent on the other
interface and so MB TCP is in fact being
better in that case so now it is a
multi-core architectures MP TCP how does
it perform on these kind of
architectures for regular TCP flow jika
affinity in fact sends all packets from
one TCP flow to the same core it does
this to avoid reordering inside the TCP
stack for example if you have an
application of
using regular TCP running on call number
one and when the packets are coming in
they go to call one and co2
well the packet in on call one might go
through the stack faster and then we
have rear ordering inside the stack and
so flow to call finiti sets all packets
to the same cpu core for regular TCP now
for mpg speed this is pretty much
different because we have the different
TCP sub flows and protocol affinity
issues that each subfloor receives it's
packet on the same call but the
different sub flows will not run on the
same call so for example you may have a
sub floor number one running on coal one
and receiving packets on call one butts
up low number to a receiving packet
packets on call number two and then when
the data is being read by the
application it will be aggregated and we
have to move the data from for example
co2 over to coal one and then we have a
lot of cache misses and these cache
misses kill us the D performance as you
can see in fact in this graph where we
have two hosts interconnected with ten
gig links and we increase in fact the
number of IBM sessions of parallel i/o
sessions and measure the good when we
have a type of sessions the performance
is best however every as we increase the
number of RF sessions the performance is
dropping even below 10 gigabits per
second with MV TCP this is just due to
these layer one and layer two cache
misses and so the solution to this
problem is basically just send all
packets from the same empty speed
session to the same CPU core and we do
this by using the so called receive flow
steering which is an implementation in
Linux done by Google and we extended
this implementation so that it sends all
packets from from all sub flows to the
same CPU call and by doing this we get
in fact a better performance as you can
see the top line it does not decrease
anymore as
for the regular MP TCP case and it stays
pretty stable even if we run 64
parallelepiped sessions
multipass disappear on mobile devices as
Carson already told you like well you
can use multi pass tcp on Wi-Fi and 3g
networks and it will be benefit in terms
of bandwidth but also resilience as we
can
failover traffic from Wi-Fi to 3G so
when he did it in fact in a measurement
in an
immolated environment where we have the
top pass is Wi-Fi with eight minutes per
second and an RTT of 20 milliseconds and
the bottom part is 3G running at two
megabits per second and not ET of 150
milliseconds of course regular TCP of a
Wi-Fi well it will get 8 megabits per
second
regular TCP or 3G gets expected to make
up it's per seconds however MB TCB well
when the buffer sizes are big enough
like 2 millions 2 megabytes then IP TCP
gets in fact a sum of Wi-Fi and 3G but
as the buffer sizes are decreasing like
up to down to 500 kilobytes every TCP
performs very bad in fact and it even
gets worse than regular TCP so we
analyzed it and looked how why is it
behaving like this so imagine the
following setup where we have two hosts
each one establishing one subfloor to
establishing two sub flows the modern
sub flow goes over 3G so a high delay
pass and the tops of flow goes over
Wi-Fi LOD a pass and the receiver
announces a certain receive window and
he allows here up to eight packet number
eight in the receive queue and the
packet number three has been sent on the
high delay pass and it is still on its
way and so basically the receiver when I
was filling up with packets four five
six seven and packet number 8 has been
sent also globally they pass so when it
reached the
destination we are blocked because we
are not allowed to send any more packets
because the receive window is full and
that's why we get the bad performance so
how can we fix this we fixed it by when
we realized that we are blocked by the
receive buffer we inject the sequence
number three on the on the faster pass
and hope well that it will reach the
destination faster and then we resolve
our buffer buffer blocking problem and
additionally to prevent this problem
from happening again
we will slow down the high delay pass by
dividing its congestion window by two to
avoid queuing more more packets in the
in the queues of the of the bottleneck
for example when your 3G the bottleneck
is the 3G interface and typically you're
queuing a lot of packets and so by
dividing the congestion window by two
you reduce the number of packets that
are cute that you reduce the buffer
bloat in fact one of you guys yeah you
know your point by three with your few
pack is because um as you can see packet
number two has rigor reach the receiver
and so the Sentry univer will announce
Effect 3 and so it must be the packet
number three that is blocking us no we
don't need to receive yeah we receive a
sub flow egg for eight but not the data
act doesn't in fact an additional well
the data option in the packet that says
up to how many up to where have we
received
yes yeah that way we know that we have
the priest blocking did you
consider our room to change the receiver
window no basically the buffer is the
receive buffer is will be the sum of the
bandwidths multiplied to meant by the
maximum RTT and then this allows
basically to run all passes at full
speed but this does not solve the
problem
because when we don't have enough buffer
space while and we cannot fix the the
blocking so at the receiver side that
cannot be done a lot at this point there
were solutions that have been looked
into basically scheduling the packets
differently on the sender side too so
kind of estimating the time when the
packet will reach the destination to
avoid a buffer for a blocking problem
but basically then you're just moving a
problem from the receive buffer blocking
to the send buffer blocking because when
we schedule the packets in such a way
that they reach the destination on the
same time well then basically the send
buffer will be used much more and not a
receive buffer so you're assuming that
you have a real constraint of memory as
well yes it's the memory constrained yes
did you look at how here's the problem
that you had yeah as in the graph if the
memory is big enough if the receive
buffer is maximized to 2 megabyte then
we don't have any problems so kind of
related question here it seems like
you're assuming in this analysis that a
jitter free 3G right that it's a fairly
consistent round-trip time where because
of the way the 3G net links hide under
underlying radio loss by doing
retransmission at the the underlying
layers you tend to get much more jitter
did you model that at all no we didn't
investigate it but which would be
interesting to have a father until
nothing well so it does make a
difference in your strategy for how you
cut back right so if you're looking at
the strategy he's looking at describe
what you do when 3 doesn't derive is
presuming that the link round-trip time
is stable because then cutting it in
half it actually makes sense but if it's
unstable but you're getting a lot of
jitter relative to the radio
characteristic
so that maybe may turn out to be their
own optimization okay so yeah rejecting
the Sigmund and hopping down the
congestion window improves gives us dead
in space we need in our c4 window and we
can continue the low delay pass can
continue sanding even if the on the high
delay part is the packet with number
three is still not at the destination
the official guideline by working cool
now this is how you have to abide by you
will not be over the question phase you
can say hey I inject packing all haha
yeah what is a guideline the guideline
in the working room they are not
defining any specific guidelines well I
think that the the next steps in the
Charter to find heuristics and so on -
yeah how to schedule the family what is
it off the bandwidth you can before blue
bottom of a bandwidth another bond to
the jitter question do you review
analyze where like I mean I know that
the design of the protocol is basically
to try to schedule most of the packets
on like a high identify little bit low
latency like you possible you look
situation where we had to tunnel planted
and all someone just becomes basically
totally worthless Devon to you and
basically yeah we're modeling bread gets
very bad the couple congestion control
will that cousin explain we'll then move
to traffic away from this link and will
prevent a congestion with them from
increasing it probably would make sense
but you we then we are we were still
looking into heuristics you'll find sort
of things like that you know and so yeah
as explained the algorithm works pretty
well we don't have the degradation when
the buffer sizes are decreasing anymore
when we are always better than better or
same performance as regular TCP over
wife so next point Wi-Fi to 3G handover
with multi pass TCP so when you are for
example s cousin already explained the
scenario when you are like listening to
a web radio and you're moving moving
with with your mobile phone you may lose
your Wi-Fi connection and regular TCP
will just break it you will have to
restart the radio there are applications
that might support is like HTTP you have
the hero hero range extension which
allows to recover from a lost TCP
session but it is not supported by all
applications and it only works for
static files so like if you're listening
to web radio it will not work however
every TCP thanks to remove address
option it just works seamlessly from the
application point of view and so how
does the remove errors works imagine two
hosts establishing an apt speed session
over two paths and sending traffic over
book both passes and at a certain moment
in time the host on the right side will
lose for example its Wi-Fi connection
and the packet with sequence number data
sequence number 1001
didn't reach the destination so what
else will do he will say the remove
address and this remove address
indicates hey I lost my my Wi-Fi
interface please tear down and resend
the traffic and that's what the what the
host on the left will do he will
immediately reject traffic from the
bottom pass to the upper pass and
destroy the sub flow over the bottom and
that way we can hand over traffic from
Wi-Fi to 3G or 3G in a very fast way and
so we did a little evaluation of this
over real networks over the real
internet we have an MP TCP server and an
empty speak lined the client is
connecting to public residential
broadband over Wi-Fi and to a 3G access
provider and by the way connecting
really access provider or residential
broadband you have a lot of net or
firewalls in this and then which is
peace just able to work across of these
without any problems so in our
experiment we created a modified HTTP
client and so dysgraphia is running
regular TCP and we do the application
handover with the HTTP range extension
and you see the good put in the
beginning it is just using modify so you
can put is about 8 megabits per second
and then when we cut the Wi-Fi good will
drop down to 0 for about 2 to 3 seconds
and why is it working like this it's
because the 3G interface is in standby
mode so in energy-saving mode and it
takes up to 2 seconds to bring it back
up again and so you have here an outage
of 2 to 3 seconds before a regular TCP
can recover from lost Wi-Fi interface
that's regular TCP yes yeah
this is regular TCP application handle
for doing using the HTTP heater
extension so the our application is
monitoring the interface when the
interface goes down it then starts
creating a new TCP session over the
other interface that's okay yeah that's
slot now it's empty TCP okay so am
participe well as it used to remove
address option it will just read reject
the traffic directly and we don't lose
have this three seconds where nothing is
passing by yeah yeah so it looks like um
is that you're actually not allowing the
radio to go into an idle state there in
order to keep enough of nection open to
do this this hand over that correct and
DISA narrow it was M participe it's not
going in the Iowa State yes okay so that
there there's some issues with that in
addition to pulling a lot of energy from
that it actually consumes radio
resources on the part of the carrier's
Network to maintaining an amount of non
idle state and money resources yeah so
well so the money resources as long as
what you're dealing with is is
essentially just TCP heartbeats to keep
the traffic there you're not going to be
hitting something in money resources
because the way they they collect the
money from you is the throughput right
how many gigabytes did you do but it is
going to be a serious impact on radio
resources from the carrier's point of
view and
yeah but we have an extension to MPT CP
that basically allows to only use one
single sub flow at any moment and so
that way we would only use the 3G when
we lost Wi-Fi as soon as the colonel
realizes that the Wi-Fi interface or the
world the IP address has been lost it
will create a new sub flow after the
session has that the connection has
broken and it will create a new sub flow
over 3G and continue the data stream
from the application then the curve will
look like the application handover but
then it developed in realised from the
HTTP specific ones yeah the application
hand over only works if you can do HTTP
in our range ecstatic files and it's not
always supported it's a it's about its
aim yeah yeah we can create a sub flow
even after we have lost the the original
sub flow yeah so you guys seem like then
whatever system is load balancing on the
receive side to make sure that all the
flows are going to the same back end in
front of it you still have to maintain
that sub flow logic then in a net load
balancer even after it's lost the
primary sub for how long would you
recommend it hold that logic I think
that's well that's the to configure upon
based upon the load of this load
balancer on a client you can probably
let this the connection be it remain
active for a long time because basically
the client probably doesn't have
resource problems but in a load balancer
maybe a little bit less like a few
seconds or something like this it
depends on I think I would say it is a
configuration question yeah just ask you
know your advice with you
because you want to make sure that
you're essentially covering the time it
takes to restart the flow on the client
side plus the round-trip time for the
first four packets to arrive on the new
do you have any experience with this we
have an experience in this yeah but yeah
like I don't know maybe 10 seconds might
be good enough or not so it's yeah I
think it depends on the load on the
server and then based on that people
should configure it but for regular TCP
do anything like anyway and for regular
TCP it's the same problem you establish
a connection then drop it then the
server as still we'll still have it this
connection actually depends on how
you're doing the load balancing if you
do the location based on something that
the same calculation will result in the
same pass at the back end the load
balancer in the front can be stateless
right but in this case it can't be
stainless because it wants to keep the
fool right it wants to keep state so
that something that comes in on a on a
flow that would have been by some ecmp
going to server a wants to keep it going
in server C because the original sub
flows and servers so you actually do
have a different state maintenance
problem for it yeah right I mean if the
client you that
to create a support analysis make sure
there's only one hash algorithm in the
world I'm sure that demons
you don't know how show me that you
don't know a little bass illumination
would maybe I do follow what could be
another software all your Croatian
cultural achievement server that's why I
mean D there are no server too much but
anyway David edit is fine so what we
have a project and the proxy runs an
article better because Ricky provider
wants to deploy to put approach you
there multipied estimated the proxy and
fifty two people whatever to Google the
cohort and then it is world and in the
future if you actually support if you
have said were supporting by then you
can promote change although it doesn't
really make sense for the carrier for
your proxy at this point well so I guess
it was their Wi-Fi I mean but they they
do a lot of them on time traffic over
Wi-Fi because the internet are really
thin so I they will actually like it do
you like I can but let's say you go to
somebody like like mobile it's in this
difference and you say I want you to add
this to the interception proxy that's
currently deployed they'll go okay but
this is still going to only work for
traffic going over 3G because it's not
going to be able to pursue anything over
Wi-Fi because there's no way of telling
the the end post to activate its Wi-Fi
stack or to use Wi-Fi conserve the end
post has to change as well when a proxy
doesn't really help
I mean be able to fund a client has to
be done in multiple fight and it has to
be running the Wi-Fi already right
through the proxy to work not miss it
you know you can get hook without fun
let's not write home on it but I'd be
really interested figure out huh
thinking you'll have a server call back
fine yeah the server can also start
creating to the sub flow can initiate it
yes yeah was the load balancer problem
easier by putting in a very easy job but
the hash is not is unique per server but
then basically well we haven't
identified hers all locally unique on
the server then we will need an identify
unique in the in the data center or near
the server farm but that yeah that could
be it could be an ID to investigate this
so actually Jenkins suggested something
like this very very similar to this in
the past for how you gang together NDT
of Datagram TLS flows so feel us when it
sends back a cookie for a live news chat
Tommy just suggested reusing that cookie
across the multiple ones so you always
know if you see the presence
a flow destined to the same and DTLS
instance you could do something beside
me though about so how does it work with
bags right through the box no big
promise I was in the first couple
session over a PCP it kind of turned me
off when people was trying to figure
where all she was t-talking phases and
limited right so back then was
discussion only how do we put the
stopping pelo in your eyes oh thank you
Harry I don't know what is a final
strategy look like this into the payload
it is in the options it's just one
option type using subtypes and yeah
right but they already seem completely
ostracized
yeah that's all have you guys he's
another experiment there's a middle bus
take they do that Tomas if it ideally
look at multiple TV flows independently
yeah well there has been some analysis
by with me Sean on the middle box
behavior at IMC 2011 yes 2011 and
there's a paper showing you what does
the middle boxes - how many do remove
TCP options and if for example a middle
box is removing a TCP option and which
is we will fall back to regular TCP
seamlessly for the application if we
work across net we work across mailboxes
that are coalescing segments so like
taking two small ones and putting them
together in one big one or splitting
segments like we work across TS over and
so on it's not yet implemented but the
protocol preserve should work yeah
Wi-Fi link I'm curious it seems like the
typical problem I have a wife is like it
had a transient lawsuit does this handle
that scenario in the same way now when
it goes down well it will then switch
off the 3G when it comes back okay to
wait before establishing the new sub
flow you mean yeah and so yeah I mean
from a protocol point of view it is
possible to then yeah not switch to 3G
just wait a little bit and then try
again over Wi-Fi if the interface comes
up yeah in the invitation we don't have
a configuration to say wait a little bit
so yeah not yet we support things like
as I expect oh you're only single pass
for example establishing the sub flow
but not using it so that's also possible
yeah
when you have more two mobile devices
when they're both moving at the same
time you basically what you need is kind
of an anchor so a proxy anchor well
which has a steady guy Peters which does
not move and there's a draft at the ITF
about this okay yeah yeah yeah we have
done some measurements on application
delay how does it is affect it and the
results are in fact pretty good because
and it is in our SD i-- paper of this
year which will appear which is at the
end of this week and I can show you the
results later on if you want we have
some cross Skype yeah we use MP TCP also
over Skype and sorry what so sorry okay
yeah buddy FAQ got a Skype over MP TCP
and it works pretty well we did this
handover from Wi-Fi to 3G back and forth
with a Skype call and also video
streaming over Skype and it just worked
very well yeah so in terms of jitter
because Skype doesn't fill the pipe over
Wi-Fi so it will always use the Wi-Fi
interface face because it has the lowest
RTT so because we don't fill it because
of the the scheduler always sending it
on the low on fastest pass yeah we're
working on this and but it takes a lot
of time and yeah a lot of effort yeah we
don't have a real estimation but yeah it
which definitely should do this
you have to configure policy routing
based so that based on the source
address you send it on you send it to a
different routing table and then you
send it to the interface yes you are
well personally we did this measurements
on my notebook I have a script in the
network manager that runs automatically
when an interface comes up and it
configures everything so now well I just
put the script in slash et Cie Network
slash if up dot D and that's all
API no that's not on the list but
there's a draft and it's ongoing work or
drafted the idea about EDI empties be
specific API extensions you mean if one
site is two interfaces the other site
only one yeah then there's a the add
address option which allows a host to
announce hey I have another address
available
please establish a new sub flow to me
and these remove errors too well to
announce whenever we lost it and so
basically new sub flows can be
established on either side either from
the client or from the server okay so we
have prepared in fact a little video
about using MP TCP / switches
okay so what we do here is we have an MP
TCP server and dnmt TCP client the
client had three interfaces Ethernet
Wi-Fi and 3G y'all you'll see on the
right a traffic monitor that shows the
band that throughput going through which
interface the top is Ethernet in the
middle we have Wi-Fi in the bottom is 3G
so I do an ssh session from this
notebook to the MPSP enabled server with
x-direction redirection and then we open
the application experience server and
basically this application is just
something moving around and there's the
the the the upland over ssh we are
sending back what the application is
doing and at the moment you can see that
only ethernet is being used because ETA
or ethernet we have the lowest RTT and
then I disabled MP TCP diesel disabled
and Ethernet and you can see that the
traffic directly goes over to the Wi-Fi
interface basically the application
didn't didn't really saw any anyhow what
was affected by this handover and then
you see that we are switching over to 3G
because we disabled a 3G a Wi-Fi also as
you can see a little bit that the
application is moving a little bit
slower so that due to the higher the
because we have lower bandwidth over 3G
the performance is a little bit less
good so and as we switch back to Twitter
net the it just continues and it
switches back to to the Ethernet
interface so you can see that MPTP in
fact works across unmodified
applications over the network over 3G 3G
networks and so um always switch to the
main button the famous name okay so that
was it related work well there's been
have been some proposals of multipass
tcp but they didn't really go
any major like PMP TCP or M TCP there
are proposals different layers like
mobile IP or HTTP range HTTP range we
already talked about mobile IP it's a
good solution but the problem is it
cannot it is at at the layer 3 so being
at the layer 4 gives us more power in
terms of for example congestion control
at the transport layer like SCTP but
setp has problems with metal boxes and
other solutions over for data centers
like hadera and so on so what can we say
multipass TCP can be used now it can be
used by unchanged applications over
today's networks and it can move traffic
away from congestion links and handover
traffic from Wi-Fi to 3G off on 3G to
Wi-Fi and so yeah basically this was it
thank you very much
so I mean the the congestion manager is
the question is how do you how do you
bundle together the condition windows
are the different different connections
right so you could use that this
congestion manager right I mean that's
that's just a framework the congestion
manager like this is a specific
algorithm right so it's you can you can
you can take the same ideas from
multiple TCP congestion control apply
them to single path TCP as you can you
know if it can tell the kernel treat
these all of these sub flows or all of
these connections as if they are part of
a big connection then you can do the
couple congestion control right it's the
same thing I mean in terms of in terms
of throughput they would perform the
same right I mean this the rates are
decided by the congestion controller
rightit's yeah so I mean in the data
center example right instead of having
like a single TCP connection between any
pair of any any any pair of servers you
could have say 10 right and somehow the
applications would stripe data across
those and maybe have congestion control
on top leather to behave pretty similar
I mean you have some caveats but it's
it's not going to be very far off yeah
I wish it was as simple as that so the
product I mean today you can use any
condition control and what will happen
is that each of your sub flows will be
doing that congestion control what you
won't get is you own get furnace to TCP
at the bottleneck and you won't get the
nice load balancing right so that that's
completely fine I mean you can even
today I mean our code is basically just
one of those six controls you can say
enable coupled and then it works so the
short answer is we haven't done the
research for cubic we don't have a multi
path variant of cubic we think it's very
easy to extend compound and one of my
students is working on that now
but it's not yet ready yet so so yeah
for compound data we think it's pretty
straightforward for cubic we haven't
actually done any work it's different
enough from you you know that it
actually requires more work I don't
think there would be I mean there's
nothing like fundamental to New Reno
that makes it better to for multi path
we just started from it because that's
sort of the de facto whatever that's
based that's the baseline oh okay thank
you guys
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>