<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AGI 2011: Thursday Evening Keynote - Ernst Dickmanns | Coder Coacher - Coaching Coders</title><meta content="AGI 2011: Thursday Evening Keynote - Ernst Dickmanns - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AGI 2011: Thursday Evening Keynote - Ernst Dickmanns</b></h2><h5 class="post__date">2011-09-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YZ6nPhUG2i0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so here we are at a conference on
artificial intelligence in California
but not everybody knows that the former
governor of California also is
interested in artificial intelligence
for private and professional reasons and
a while ago it has a great honor to give
him a talk a scientific talk for the
former governor of California himself at
least he said he is the Governor of
California
in fact what he said was if you were
scientists and I'm the governor of
California now I have the great
opportunity and the great honor to
introduce a real pioneer a great
scientist the pioneer of self-driving
cars are from autonomous vehicles who
almost single-handedly created this
field in the 1980s when for the first
time he had cars that drove on empty
streets was up to 90 kilometers per hour
that's about 55 miles per hour I think
and and then in 1995 already had cars
that went from Munich to Denmark and
back on the Autobahn which is more than
a thousand miles in traffic with active
vision you know with cameras that do the
stuff that today's hot topic of vision
research active vision all is tracking
the salient and interesting points
tracking up to 12 other vehicles in
traffic 20 15 years ago
16 17 years ago in 1995 going up to 100
kilometers without any interruption by
the safety driver only when there were
construction areas which were not
modeled by by the system back then using
no GPS but just driving by active vision
and he is actually a pioneer of active
vision something that he called the 4d
approach to to him to vision into AI he
was born 75 years ago
he studied in Aachen he worked for the
German Aerospace he got a NASA
fellowship in Princeton he got his PhD
in Aachen he did a postdoc at NASA in
Huntsville but he also got to know
Boehner from Braun the leader of the
space project where he also worked on
the space shuttle reentry which is kind
of interesting because just recently the
last space shuttle landed he got lots of
prizes on machine vision especially on
this work of an active vision which
again today is a hot topic but back then
he needed it just for engineering
purposes to make these cars drive not by
GPS but by vision more like humans
derive 1986 - my age 7 he he was
essential his work was essential in
shaping this huge European project on
the Prometheus project which which back
then costed about was worth about a
billion dollar you had all kinds of
self-driving vehicle vehicles he was a
visiting professor at 19 in 1960 1996 at
Caltech 1998 at MIT 2001 at the
University of Canberra in in Australia
and he retired in 2001 and all the
students and went into industry - all
the major companies and Audi and Daimler
and BMW still have fill fleets of
self-driving vehicles driven or designed
by historians so it's my great pleasure
to welcome and stick once upon yourself
thank you very much for this very kind
introduction in the 1980s and 90s we've
been rather successful in demonstrating
what you can achieve in real world
applications with modest computing power
if you use the right methods so I
appreciate the invitation of the program
committee and especially of your mid
Hoover and Marshall looks and and
kurtsyn to give this presentation to you
here our work has been appreciated at
that time also by the IEEE community but
I must confess with respect to citations
in this community we've almost
completely failed so you don't most
never find us in the literature in this
field well this may be partly due to the
fact that these results we have achieved
to contribute it to skipping one of the
larger vision projects at the end of the
1980s the DARPA LV but you're the next
generation and I like to take this
opportunity ten years after my
retirement to present some of the early
results to you mainly by videos so first
I gave a short introduction oops on the
roots of dynamic vision 1942 90 then
I'll talk about the basic idea of
dynamic vision with application to grant
vehicles and to aircraft ground vehicles
this 3 degrees of freedom usually but
you do have also wrote three
controllable degrees of freedom but you
do have roll and pitch so we do have we
have up to five and the aircraft of
course to have six degrees of freedom
and all of this is in 4d four dimensions
where there's three space components and
one time component and there is no 6d
and this terminology of dimension no 60
is lamb but this is six degree of
freedom slam so don't be confused by the
terminology I use D for dimension and
there are only four in our meso scale it
work we developed two generations of
vision systems in these applications up
here and then
at the middle of the 90s we came up to
do the third generation as an
expectation based multifocal circuit
ignition system which from the function
is according to what vertebrate vision
is but it's of course realized
completely different its comparability
based I'll come back to this later on
and system integration and will be
discussed with the same applications we
have over here
so the first beginnings were in the
communities of biology and engineering
during and after World War two and of
course control play an important role
how to use it how is it used in biology
there are basically two basic approaches
one is it as a history over time this is
feed forward control you get behaviors
and the other one is linking it to
differences between the goal you have in
the state you are this is feedback
control and both do have their benefits
but it was soon recognized that if you
want to do almost any application like
is claimed in Venus book that's not
sufficient so in the 50s
of course people came along and said we
should have symbolic representations
Gould's high-level planning in order to
do trajectories and they also soon
afterwards claimed that they would solve
all problems of the world but
unfortunately there were some
discrepancy noticed after 10 to 20 years
and one big part of it was that handling
noise corrupted data is very difficult
but going back to centuries there was
cows of data on planetary orbits but at
that time we knew that the orbit should
be elliptic so he came to the idea and
said let's take the solution in a
parametric form and then just the
parameters such that the sum of all
error squared is minimal and this of
course has changed Natural Sciences and
engineering and has developed it over
the next 150 years stayed almost the
same and then when spaceflight began
of course one would have liked not to
take a bag of data but the data each
data as they come in and do it
sequentially and the second one to duyst
was Kalman because this approach has
been adopted generally it's his name is
associated with this and it's called
Kalman filtering so instead of the
solution curve to take the dynamic
equation which leads to the solution and
you look at the of course in order to do
a correct estimation you have to have
some knowledge about the statistics of
this approach and this approach also has
been applied to image analysis image
sequence analysis in the IEEE community
but they considered the image the image
sequence to be the process and feature
extraction the measurement process and
of course you have to have some motion
law so as I said let's take constant
position or constant velocity and some
superimpose noise in order to adapt it
but of course if you do have on the way
from the real motion to the image
features you do have perspective
projection and if you don't take that
encounter you're in trouble so we were
the ones who introduced in the early 80s
what we call the 4d approach we said
let's take the model of the motion in 3d
space and these are 3d objects carrying
features and when there is what has been
called catastrophic events when there
was a self accretion of features we can
predict that in this model so you are in
a much richer environment and of course
since it is three space component and
time and the basic part is motion of
objects that's why we call it the 4d
approach so we use recursive estimation
in order to do the estimation of the
state variables of the description in 3d
space and of course if these are agents
and they do have some control output we
also realized very soon that you have to
have a notion of as a knowledge element
of the knowledge base what can you
achieve by a certain motion
control output over time and there's one
more difficulty the fact that there is
no link between the object and how there
are the objects and the internal
representation in the computer so
everything has to be represented in an
internal world so we said well why don't
we go ahead and in addition to the real
world over here which is looked at by
the camera we implement an internal
world oops
over here and we say we can make
predictions where the features are and
then the difference between the features
this is the information which we will
use for improving the estimation and
then you come to the conclusion well
interested in positional features we are
interested in the state of the object
and the features are the intermediate
part to achieve that so objects are the
units where we apply we use knowledge
for and so we are looking for the state
of these objects and of course if there
are features which cannot be associated
to these objects already instantiated we
have to have the capability of
installing new objects and then of
course once we do have a good
representation we can make predictions
also which other types of features would
you like would you possibly see so if
you look at a car
you know if it's called does have to
have wheels you can look for wheels and
this is a much richer set also back
lines and front lights much much richer
set of features which you can work with
and of course when you know what the
state of the other vehicles are you can
make a decision how to control the the
process in the real world you're
observing over here and at the same time
we can use the same control putting up
put it out on the real world you can use
to input into the model we've been using
and if there are discrepancies we can do
an adaptation of these models so we have
three processes running a parallel
detection of objects the tracking of
objects and the learning process over
here so in our terminology objects are
used in 3d and usually moving and they
carry features as the basic knowledge
elements it's not quasi-static to the
aspects of stationary objects
subjects that's the general term we use
for objects with the capability of
sensing and of initiating motion so it's
maybe a little strange in the English
language but in in or Latin and the
German subject means exactly that the
terminology and usually English is a
little bit different I'm pologize for
that the specific hyper classes can now
be built for all kinds of animals
including humans and of robots legged or
wheeled it just depends on their
capabilities and we come to these
capabilities as the central part for
knowledge representation the
capabilities and perception capabilities
and decision-making and the capabilities
in control and control both of gays and
of locomotion so it's a somewhat
different approach and also the the
background knowledge you have to have is
a little different and we did the first
demonstration with a very powerful
little cart which is somewhere like 10
kilograms but it can accelerate like a
high-powered sports car 2.8 GS 8 meters
per square second and it's going to
balance this rod over here here and we
only take four inclined edges and
looking at these four elements you would
like to know the state of the of the
system and to stabilize it at a certain
point we should have with four window
areas all evaluating data even visual
disturbances with a night
looking rod can be handled a success of
modeling motion behavior over time so
this is one of my first PhD students
he's now my follow on it the universe
EFV so let's look at the differences
between the two approaches in mainstream
AI they took ten - if it was very lucky
maybe two seconds but usually 10 and 20
seconds to evaluate a full image or pair
of images because they wanted to get at
the speed of objects or features and on
the contrast in our approach I mentioned
since we have to linearize systems in
order to do this recursive estimation we
should take at least 10 frames a second
and we want to boil down the process to
simplicity that we can do that and in
addition to be able to do this we just
take small areas of the image the upper
left and the lower right corner and all
of this is somewhat like 900 pixels so 3
2 by 32 and they're grass and we're
looking for future positions only not
for motion components because if you do
have this model-based approach we make
prediction errors and feed them back and
then by doing this with the right model
we get the velocity components by
integrating the process which is a
smoothing process while if you
differentiate between the future
positions in images of course there's a
noise amplification and the biggest
difference of course was almost no money
available so we took processes
off-the-shelf half-a-dozen 8-bit 80-85
processes were the ones we used for this
card balancing while on the other side
in this country there were 14 different
architectures investigated under the
tapa project at that time Thinking
Machines was one of the the bigger ones
but all of them finally I think they did
not come to term and it turned out that
the approach was not so bad as we
initially thought our approach was not
so bad as beneficial this is the second
problem where the visual part is a
little bit more difficult we do have a
air-cushion vehicle floating on a table
and there are reaction jets to control
it and there's a docking partner so this
may be looked at as a satellite docking
problem and the features we get is
corner features but we could only
compute root of four of those and this
was one of the big achievements in this
program that the system itself would
half-a-dozen 8-bit microprocessor plus
one PC was able to do to select the best
possible combination of features using
elements of the Jacobian matrix in order
to achieve this since time doesn't allow
to allow it to go into the details here
I refer to our website Dynavision de and
there are more elaborate descriptions
and you find the video also there so the
main topic we've been working on was
vision with ground vehicles this was the
5 ton van which we equipped initially on
our own cost and this is the second one
which was equipped in the mid 90s in the
Prometheus project you can refer to so
if you sit in the car and you want to
drive on a road you see the steering and
you know the main major part of the
steering and you're lying out there in
the countryside because there's a band
which you recognize as the road and if
you look at the left and the right
boundaries of those again it's only edge
detection black and white edge detection
then you get the curvature and you see
the steering angle you need the lambda
over here usually vehicles to have a
common theory which is a rigid axle and
you do have the two wheels on top of
that so the this is directly linearly
proportional to curvature so once we
determine curvature we do have to feed
forward control part for driving on this
road and then of course there may be
deviations and in the near range we get
the information about where we are on
the road if this is symmetric
of course the road should the descent of
the road should be here in the center of
the image and from using a simple model
of a common steering for cars it's just
to two differential equations
we get the letter opposition on the road
and size stands for the letter of the
law city and now we even can estimate
the slip angle of the car which is the
better over here and this is
superimposed on the lodging on the feed
forward control so we don't have to do
any trajectory computations or planning
but we just do a feed forward control
and the feedback control if you use the
proper signals each one of those is
relatively simple if you use Cartesian
coordinates of course the straight line
takes two parameters this three this
four but if you use differential
geometry models that are the ones we
used even if the curvature is curvature
at the position of them of the camera or
the vehicle and there's a c1 parameter
times the arc length along the the
road-tripping the c1 is the curvature
change over arc length and now if you
drive at a certain speed vehicle the sdt
you can get a temporal derivative of the
curvature which is nothing but C 1 times
V and that's all you need you have two
sets of these equations and then you can
do direct you can directly go into
recursive estimation and of course we
don't want to determine the distance
traveled by vision but this every car
does have an odometer so we take this
directly from the odometer and this also
gives us the base for motion stereo
interpretation and this allowed us to do
a completely different approach than
what has been done in the IEEE community
they were looking for very slowly moving
cars in complex environments like the
campus and the car driving around we
said with a good background if you do
have knowledge about the environment and
the best defined environment in German
is the Autobahn so you do have certain
width of the lane and there is only
limited types of vehicles on there so
why not take those when we have to drive
80 100 km/h who cares
so we look 10 times and within tenth of
a second the vehicle moves just a meter
or two and we look at ranges 20 or more
meters so this was a simple idea and it
turned out it worked and timely Ben's
got interested so they said why don't
you make us
the demonstration in the skid pan that's
their skid pan and fruit cut so the
vehicle stands here it accelerates to
this point and then there is a spill
curve leading to this dr. bright circle
there's a basalt ring and there's a
concrete and the vehicles drive here and
sometimes they make it wet in order to
see what the curving behavior of the
vacancies so the idea was that the
system should pick up the difference the
the boundary between the the two parts
and then drive around this skid pan at a
speed so that the lateral acceleration
is point 1 G so 1 meters per second
square of course lateral acceleration is
V squared times curvature so if you do
it if you do estimation of curvature you
immediately do have the lateral
acceleration and this is the
acceleration to 10 meters then during
the increasing curve edge of the system
decelerates to 7 meters and this is what
it essentially been driving over here so
this was the demonstration with it in
December 1986 edgewood cart so VX
vehicle accelerates along the straight
line and now the the spirit starts and
the transition to this dr. bright area
should have been tangential but there
was a king and we said well let's see
what happens and the system goes around
and it made it so without any difficulty
so this convinced time loop ends I think
I'll skip here go to the next pots it's
convinced time Rabin's program European
for European traffic of highest
efficiency and unprecedented safety so
and we intended to use buried cables to
do lateral guidance of those vehicles
others claimed we should use active
sensing with radar or lead R and we of
course claimed humans to a vision why
don't we do it by vision so we said we
want to make a demonstration of that and
there was a part of Autobahn
near dingo thing it was not yet turned
over to the public over 25 kilometers
and this was stirrin you've seen these
with the cameras and here this was team
the computer system it has been a dozen
8086 processors no more and this is the
driver seat is empty and the vehicle is
driving here at 60 kilometers an hour
few days earlier we've been driving at
96 kilometers an hour 60 miles an hour
and for demonstrations with the video
team here of the radio station at the
side we didn't want to go faster so this
was a demonstration we did and then we
whoops what's happening now that's
strange this so the decision was taken
that the abandoned the the plan to do
guidance by inductive wires and this was
very well it had quite a few
implications because they asked of
course all the other European car
manufacturers and they all said well
let's go this way and we had about a
dozen industrial companies in come and
you're factoring that join this activity
and we had several a dozen up to sixty
University institutes joining so we had
a very good environment in order to do
testing of our approach in the next
year's there were frequent meetings over
several years with I guess more than
half a dozen different cars in the
different countries and there were a
variety of approaches also neural-net
approaches but our 40 approach with
realistic spatial-temporal models
evolved as Steve preferred one initially
only in European industry and finally
worldwide and also at the universities
but now when you do have this
differential geometry
presentations you are not confined to
just straight lines but you can take any
curved line as the base is to do another
curvature representation or curvature
investigation in the vertical direction
so here the system is standing and it
goes a little bit uphill and there's a
hum and then it goes down and the road
turns left and here it's going uphill
again after driving on this this was the
impression which the system ate which
was very close to what the environment
was so you can do both horizontal and
vertical curvature detected at the same
time with very little effort time does
not allow it to go into details again
you find all the videos in on the
website and a simple-minded engineer we
came to the conclusion that if we can do
this with roads over here why don't we
do it with obstacles on the road so we
take another half a dozen of these
processors image processors these were
8086 and we look at features vertical
features in a horizontal search and
horizontal features in a vertical search
and they sent to each other
and hopefully they will give us enough
information to get the state of the
other object relative to our own the car
but of course industry and many others
said well vision no we don't trust it we
do have more more relying on radar it is
available it's an all-weather solution
it does have low cost it does have good
range but low angular resolution and
there's no Road perception of course so
there were room for laser people who
said they do have good range and if you
do the proper design you also get a good
angular resolution but of course this
was high cost and it's new technology
and of course the vision people said we
know from humans they can drive it so we
do it per vision but the question is can
do it without binocular vision or do you
have to have binocular vision we claimed
initially and said monocular vision over
time should work so those people said
range estimation by the monocular vision
but you know that on one ray coming from
infinity into the that this everything
on that Ray is in one point in the image
so depth is lost in a single image so
the question is can you do it on a
driving car and I would like to make it
a short test and see who believes that
you can do it from this audience I'm
impressed very good at that time usually
there were less than 20% that thought
you could do it well of course we've
proven that you can do it
so the solution depends on the site
constraints you use automotive industry
is very conservative so I said we do
adaptive cruise control these are the
first this was the first assistant
system developed by radar as the primary
range sensor and the laser rangefinder
they continued to be in a development
phase of course there's a very famous
system developed here in this part of
the world and which has been extensively
used in the DARPA projects with
Sebastian Thrun but each of these
systems if you don't care for cost
they're good but they cost as much as a
premium car itself so it's very
expensive and I've been talking to some
people a few months ago on another
conference on automotive engineering so
they said again within a few months
maybe next year we'll have an
inexpensive system available with almost
the same performance but we've been
hearing that for two decades now so
let's see what will be the result one
year from now but of course we know that
humans don't need range estimation
because if you look at a at an
environment where you know you do have a
well structured surface it's either
horizontal or slightly curved then we
know that the image line if you have a
camera looking parallel to the earth the
image line is directly coding distance
so it should be one possibility of doing
obstacle detection and this is the
demonstration we did in 1988 at the test
track of time alabanza
in Russia at so there's a one a half a
square meter ton and the system looks
for vertical features in this direction
and when this some are found there will
be the vertical search for horizontal
features and of course looking at the
lower one gives you a direct information
on distance and since we are moving and
we do get information on how far we've
been moving by the odometer we do have
without any additional computation we do
have stereo vision motion stereo vision
so of course people asked what's the
accuracy can you compete with lead are
we said well let's let's find out so we
bought a like a laser rangefinder and we
did tests up to 40 meters others up to
80 meters and you see that here we do
have a difference which is in the 2 to 3
percent range which is exactly perfect
for car driving but in addition when you
look at a car up front and you want to
keep distance to that car after maybe 20
or 30 measurements you know the width of
the car and you know that the width of
the car is not going to change over time
so if if this width gets smaller you
know the system is moving away if the
other way around
so this also helps improving the
accuracy so this was the base for the
second-generation vision system we
developed on the transputer base
transputers are those European computers
with four links to the neighbors
unfortunately it was just one generation
and then they quit so we couldn't go on
but here we do have four cameras
wide-angle and Ottilia
and to look forward to looking backwards
and for video signals are given on the
video bus the entire system was someone
like 60 of these transputers 1000 was
used for road and lane recognition about
2,000 were used for obstacle detection
and of course we wanted to have
obstacles both in the EON and in the
neighboring lanes and was not enough
computing power to do the 3d shape
recognition or recognizing moving humans
could have done the signal
light signal once you hit the car of
course it's easy to to look at the
lights but here we do have a very high
data rate and by going to object
representations on this level we reduce
this data rate by two to three orders of
magnitude orders of magnitude so we do
have a representation of objects on this
object dynamic database and from here we
can do a situation assessment and this
allows us to do viewing Direction
control and make behavior decisions and
vehicle control and of course some
information to the human operator so
this was what we did at the final
demonstration in Paris you see in the
wide-angle image we do have three
vehicles in the near range and there's
this is the the part of the Taylor
camera shown here and there are also
three vehicles but this one and that one
is the same so we know we track five
vehicles simultaneously and the same in
the rear field of view and the next film
is going to show somewhat surprising
result we were driving on the autoroute
a1 near paris near the airport childr
goal and this vehicle came to the
conclusion the autonomous decision to do
a lane change to pass this truck here
starts acceleration and then suddenly
notices that that this truck driver also
decided well the car in front of me is
to slow it so it has a lane change so
this has to stop
acceleration goes back into a convert
driving more and says well there's
another Lane so why not make a second
lane change and after a while it says I
would like to do it and the human driver
sets the blinker and this is the signal
for the the car to really perform the
maneuver it intended and now there's the
acceleration phase and the system passes
this truck yeah and when the distance
between the overtaking vehicle and the
truck is large enough it says I could
change back to the medium plain and
again the driver sets the blinker and
the system can go can do this one year
later in 1995 there was the famous drive
of CMU our hardest competitor in this
country
hands off to America so they drove from
the east coast to the west coast with
lateral control done by the autonomous
system and the longitudal control was
done by the driver and we did have a
project partnership building in Oran Z
in Denmark and we said we want to have
more information on where our system
Brewers and where it doesn't work for
designing the third-generation system
and up to then we only had black and
white image information and only edge
detection nothing more we wanted to see
what can we do on this basis so we came
up with schedule how to drive and
usually we drove around 20 kilometers an
hour sometimes a little faster in the
north and planes up here and the rima
bogo haider maximum speed was under an
80 kilometers an hour which is under 10
miles per hour a little more and the
system did drive at these speeds and you
see every now and then it decelerates
this was when it was running up to a
vehicle up front you had to make a lane
change pass and then continue and this
is the statistics of this drive so
that's the lateral deviation from the
center of the lane usually it's state
within 20 centimeters point 2 meters of
the center of the lane sometimes half a
meter or maybe even point 6 but usually
it was very satisfactory and this one is
a display of the result achieved how
many parts of the track have been found
at which the vehicle was able to go a
certain distance and this is a hundred
kilometers so if you look here the
system has been able to do five drives
over more than 100 kilometers the
longest one 160 kilometers here without
intervention of the human driver there
may have been a loss of signal sometimes
but there was the capability of
autonomous reset and then the system
could go on of course there are quite a
few offsets sir where the system stops
where we had to drive ahead to intervene
and this usual construction sites we do
have yellow lane markings on the road
and why
lane markings and the black-and-white
this all looks the same so the system
said no I'm I'm confused please take
over okay
so these were three dimensions now let's
come to six dimensions and make it short
here we made experiments with this
aircraft you will see later on but now
you do have the task that this is the
runway and the system is up here with
three lateral with translational
positions and of course three rotational
degrees of freedom but the cameras
meters off the center of gravity of the
vehicle so a rotation of the camera here
is a translation at the position of T
camera and the camera could be moved it
could pitch in your in order to center
on the runway this is the runway over
here and the only assumption we made
said this is a flat countryside and we
do have a rectangular runway and let's
see whether the system can handle this
all by its own the control of motion in
three-dimensional space
or flying is much more difficult than
controlling motion on a surface in the
normal mode of operation the human pilot
provides both the sensory system and the
knowledge base for goal oriented
guidance of the aircraft
the pioneering work at UVM in the fields
of dynamic machine vision and of
electronic assistant systems has
demonstrated that vehicles capable of
vision are able to do this on their own
in the hardware in the loop simulation
facility with real TV cameras and image
sequence processing Hardware in the loop
in the years 1984 to 1991 it was proved
that fully onboard autonomous
performance capabilities for the landing
approached by machine vision could be
achieved with the aid of inertial
sensors even under conditions of strong
wind and gust disturbances the guidance
system enables the sensor to recognize
an aircraft state relative to the
landing strip sufficiently well for a
controlled safe landing the system
achieves this by interpreting
prospective distortions of the
rectangular landing strip and by
simultaneously exploiting inertial
sensor data radar altimeter and
aerodynamic velocity data in conjunction
with knowledge of typical motion
behavior of the aircraft in 1991 in the
framework of two flight tests campaigns
with the twin turbo propeller aircraft
Dornier 128 of the Technical University
of Brunswick the perception part was
validated in real flights the pilot
manually controlled the aircraft while
the technical eye and the data
processing system with the software
developed were able to estimate position
and angular orientation relative to the
runway threshold concurrently
so the pilot did make a go-around
and by the way the happened the loop
simulation motion simulation three axis
motion simulator
came to very close from here from Menlo
Park and I'm really surprised that
nobody in the vision community in order
to develop vision for vehicles use this
type of device which for human pilot
training is standard but it seems that
we've been the only one who did this
so let's come to the system architecture
expectation based multifocal psychotic
television and three types of
applications so first if the car drives
in this direction here there's a bearing
angle to a traffic sign and the task is
to recognize the traffic sign in
relatively high resolution so we do have
a wide-angle standard and the tiller
camera so while the system is driving
here it measures the bearing angle and
then when this is large enough 20
degrees it does make a circuit to this
traffic sign and then it's imaged
into on both high resolution and sent to
another computer for analysis so let's
see what happens come on yeah these are
the the two cameras
oops I'm sorry the two cameras and you
see the motion of the traffic sign is
the translational part is completely
taken out by this fixation so this is
fixation type vision and here we drive
on on the landing strip at our
University here's the traffic sign and
there's a sudden change to the traffic
sign and back and the change to traffic
sign taints a tenth of a second then it
takes another four or five frames in
order to detect it and now we will see a
slow-motion representation of the
process so if the sign is detected there
will be black markings this one is the
case now and now there will be very fast
it's okay and everything is blurred over
here and after two frames this one is
centered and now with the other camera
the
large images taken and sent back for
analysis and system goes back with a
cicada and continues driving so that's
normal saccadic vision version in order
to come close to human performance by a
technical visual system we should have a
vehicle I consisting of two wide-angle
cameras with a central with a field of
view of more than 100 degrees we should
have a camera camp the color camera were
the viewing range of see 100 meters and
at this distance the one pixel should
cover 5 centimetres that's what this
means in the one and in addition if you
do high-speed driving we should have the
capability of detecting the road lane
markings which is 12 and a half
centimeters back home and at 300 meters
so we need at least 2 pixels that's why
we said this is the size of combination
we need here you see in an urban
environment the to wide-angle images and
there's a central part of what overlap
where you can do steering interpretation
and of course where the other camera is
looking at this part here you can do
trinocular stereo interpretation but
more interestingly there's something up
front here you can see but here you see
clearly you see this is a green van and
you can read the license plate number of
this color which is not possible over
here but you can't see anything on this
car over here but with the high
resolution black-and-white camera you
see the license plate of this car and
even a phone number so this combination
of having simultaneously available three
different levels of resolution we think
is a very important part in order to do
meaningful decisions now I will show the
system integration with just and you
know just one engineering slide and then
three examples hyper adaptive cruise
control on an off-road mission
performance and helicopter mission
performance so this is the only
engineering slide I'm going to show you
and that will make it brief this is the
vehicle down here with all the hardware
sensors odometry and vision of course
and there is a gaze control capability
on
platform and the red loop is the vision
control loop the blue one is the
automotive part vehicle steering and on
both levels we do have the capability of
feed forward control and feedback
control and there are certain maneuvers
which we consider as a central part of
the knowledge representation you have to
have and this is represented here on the
knowledge representation level in
addition we have to have information on
the mission to people perform and of
course the actual state of the
environment which we find here then
there's a system assessment
complex and this says well go on as
you've been doing or change switch the
mode either engage control or in vehicle
control and in addition that this will
be show by the first next example if the
driver is in there he can do what he
wants but the system has to react to
this in a meaningful way so hybrid eptas
cruise control means in addition to
radar radar does have the problem that
it has good range resolution but poor
lateral resolution so we want to find a
combination with vision but which allows
the human driver to always be
responsible this is very essential for
the common effector industry they said
the human driver is always responsible
also for autonomous systems full of semi
autonomous systems so there was a
separate task assignment the
longitudinal control was performed by
the autonomous system so the human has
the leg three and lateral control was
done by the human he didn't have to tell
the system anything but just do a
lateral steering as he like and the
system had to to react so range was used
as the initial hypothenuse hypothesis
generation and the precise did lateral
determination and relative in the lane
has to be had to be done by vision so
this is the next video which hopefully
will start yeah so this is the ellipse
of the radar that's the the hairy lips
of the of vision and here we drive with
the normal standard image and this is
the Taylor image and this is
the lane detection which is done by the
system autonomously
the vehicle the relevant vehicle for
distance control is marked by this red
square over here now the human driver
does a lane change in the system but
immediately switches to the next lane
and thus the tracking and you see
there's some confusion radar tells there
is an an obstacle but it takes some time
for the system to make a distinction
between the one immediately here and one
of then after so after some time again
the human driver came to the decision I
would like to make a lane change so it
has the lane change the system again
switches and well that's it so that's
the way it should work second example
this was the final demonstration of a
joint project we had with in between the
Minister of Defense in the US and in
Germany and this was growing cross
country and on minor networks of minor
roads and doing negative obstacle
detection avoidance of negative
obstacles this has never been shown in
one of the DARPA projects because
negative obstacles they do have special
difficulties so we made a demonstration
doing this maneuver but this would take
too much time but I'm going to show you
is this approach on a road small road
without lane markings then there is a
turn to the right the system doesn't
know the exact position of the the cross
road and the angle of the cross rule
this is being determined while it
approaches then after a while it turns
off left and has to go in this direction
but then it has to determine this
negative obstacle and it has to react
properly to this so this is the here
somewhere here is the the ditch and the
camera you see as an insert in the film
later on stands over here and looks at
the vehicle approaching and then doing
this maneuver so we'll see in the next
film this is the vehicle starting over
here this is the control wheel and here
is the vehicle ie with five cameras now
because these the stereo system of
Sarnoff we had on board required two
parallel axes for the stereo system so
we do have our two cameras and the two
up up here and this system locks onto
the cross
and now the vehicle turns under the eyes
locked on the crossroads so it goes for
a while
and it picks up the the signs here the
the road side up here and then at a
certain point it has to turn left to go
cross in this direction we just start
doing right now and we should notice the
ditch coming up here we see it and the
system approaches it not when the yellow
marker shows up it has detected it but
now it sees only the right part of it so
it comes to a stop and does a circuit in
order to determine the width the
appropriate size of the thing the yellow
line is painted into the images and the
Beckler unless you have in there comes
from from painting it into the image but
now the system locks onto the corner and
drives around and after its tangential
to this corner it goes back to the
original trajectory and that's pin it in
order to speed up the third example is
helicopter flying so in brunswick again
airport of brunswick it flies towards
the initiative the initial part of the
landing strip there's a white lane
marking and the system has to detect
this optically then it flies along the
the runway for a while and ignores the
capital h later marker is painted on one
of the taxiways here and that's the
landing point for the helicopter so it
turns right flies in this direction and
then hopes to find and hover at that
point this is going to be the next slide
so this is the landing strip wide-angle
image to the image
and it looks for the has discovered now
the white stripe but this white stripe
is made out of made up of white
rectangles and concrete colors in
between and the system you see it has to
turn down in order to look at this point
if it's searching and then it comes to
the occlusion now I know where I am on
the landing strip and it makes a little
turn to fly for a while along the
landing strip and then it has a turn to
the right and you will see it here in
the three axis motion simulator where
the camera is on top of it it now does a
turn and flies in the direction of this
Junction and at this Junction when it is
detected well this this part is
unfortunately not in the image over here
in a few seconds this will come up so it
it starts detecting this part over here
now we can recognize it and we can see
as a human we can see the white edge
paint it on the on the landing strip so
it flies towards this landing strip
decelerates and at this point it comes
to a stop in order in order to don't
take too much of your time I fingers I
switch it off here and come to my next
point which is so what does it have to
do with the original approach and what
has been the things we've learned over
the two generations and this is
essentially my point of view how one
should arrive at general vision systems
for AI so it should be capable of not
just only detecting objects but also
classes of objects and it should be able
to detect subjects so these are objects
are capable of sensing and doing
decisions how to behave and what the
motion is you can expect and of course
in order to do this properly for
subjects we have to have knowledge about
their motion capabilities their maneuver
step stereotypical maneuvers they are
going to use so that's a very difficult
part and in order to alleviate the next
step I want to make is if you sure that
is it at this image you cannot but see
your horse but there is no horse what
you see is five parts
of a horse which nicely fit to your
illusion of a horse and the lady riding
on it but it's very strange you see here
we do have air which makes a hazy bluish
part of the woods behind there and the
horse is in front of this of course but
here this bluish part occludes the horse
that's strange
there's another tree occluding the lady
and the horse and of course we know that
if this is planar and the lion in the
image is going to give a distance
information so this tree here is more
distant than the horses or this tree is
so there's there's strange information
in this image and this was painted by
Magritte and Salvador Dali used it to
state to see is to think because what we
do when we see is we look at the
different information we have and come
to the conclusion how far is it
consistent or not here we come to the
conclusion yes there's a horse and the
lady riding on it but the painter he
wanted to play a trick on us so when we
now to come to dense traffic scenes and
that's the way we are working right now
with this is just four types of features
we do have non homogeneous regions which
is non planar intensity distribution
these are the white areas we do have
edges the green ones found by horizontal
search and the red ones found by
vertical search and we do have linearly
shaded blobs that's it and corners this
some of the most strict tests for
corners is that the blue crosses now if
you get all of these features which are
thousands and you would have to come up
what does it mean you immediately come
to the conclusion to say we have to come
up first with some conjecture what is
the situation I mean if you know you are
an Autobahn and if you know that's a
three lane Autobahn you immediately say
well these nonlinear and the edge
information I get here is marking my
lane and in front of me there's a car in
front of this car there's another
in the right lane there's a truck which
I'm just passing and that can see it's
wheel it's round and under oblique
viewing their conditions that's an
ellipse
there's no another car in front of that
on a bus in front of this one on the
left-hand side there are two cars and
this one may be a tree because it's not
too smooth so it may stay open what I
want to see is that the essential part
is make early jumps to object subject
and situation hypotheses because this
allows much more precise questions to be
asked and rich tests to be made so you
get the semantic context and use
prediction error feedback not just for
features but use it also for the models
you use in the recursive estimation part
and look at the convergence behavior
over time you can use several hypotheses
in parallel and if you move look at
these information to any elements of the
Jacobian matrix you have very rich
information to make a distinction
whether which ones are the right
hypotheses usually it takes us five to
seven cycles video cycles to come up
with a hypothesis and may take another
ten to twenty to make a decision well
that's that seems to be the right one so
intelligence is grounded in
understanding and proper control of
processes over time towards certain goal
and of course this is a image
representation we not just to a feedback
on the control but we do a guest control
we do a feature extraction control um on
the our knowledge representation we do
have the gestalt idea like the horse you
have seen in this image for objects we
do the object tracking on this basis and
we even can make predictions for the
situation so this is the overall loop
and I would have ended here usually but
two weeks ago there was the last flight
of space shuttle I've been working and
doing a piece of postdoctoral research
here in Huntsville working on the
shuttle reentry trajectory 1971 and it
took another 10 years another shot until
the first travel was flying in 1980
and we've been doing an experiment
you figured come on Africa and there's a
cabin with a robot arm in there this is
an experiment designed by Gertrude
singer from DLR but we didn't have the
capability of putting the computers on
board so we all the measurements on
board were sent via state geostationary
satellites to ground stations and then
through cables through the ocean and to
the Mission Control Center in over - all
this takes about three seconds and the
way up takes another three seconds so we
had to do remote control the first ever
done remote visual control by grasping
an object in space so this is the robot
arm in this cabin and in this on this
arm is the robot hand with two fingers
and there are two cameras sitting in
here because they thought you need live
stereo to do it we said no we can just
do it with one camera and we did this
experiment and here you see because the
difference between measurement taking
and the actual event that took place in
in the orbit is a six second time delay
and I think it's best seen Rotex the
most demanding single experiment made up
to now has been the first ever automatic
grasping experiment in outer space
only the robotic arm with TV cameras
installed in its hand were onboard the
Space Lab in the loading bay of space
shuttle Columbia circling in a low Earth
orbit between the Atlantic and the
Indian Ocean on May 2nd 1993
the computers for image sequence
processing and motion control were
located in the Mission Control Center in
Ober fathan - near Munich data transfer
via three geostationary relay satellites
and ground cables through a multitude of
processors on the way from orbit to
Mission Control and back to orbit
introduced a time delay of about six
seconds special care had to be taken by
precise temporal modeling to counteract
this destabilizing effect the so called
free flyer stored in a clamp when not in
use had to be picked up by a human
operator and set free in the center of
the cabin due to adhesive forces an
unpredictable rotation occurred when the
two fingers of the robot hand were open
the second try however under much harder
conditions now resulted in an almost
perfect catch the actual grasping
sequence was achieved fully autonomously
after initialization of image sequence
processing by a human operator well
let's see at the component development
since we started doing vision you all
agree there may be some discussion
whether it's six orders of magnitude or
eight or nine but there are really more
than a million fold computing power is
available know and with respect to
sensors and communication bandwidth it's
also very tremendous progress has been
made but what happened to the field of
software and vision system development
and if you look at that I attended a
conference two months ago
the intelligent vehicle conference in
baden-baden and I was very much
surprised how vision the vision
development is lacking if there is
progress it's maybe one order of
magnitude at most - but what is the
reason behind it well I'm I might just
suggest that thinking in 4d and spatial
temporal models and processes and
corresponding coding seems to be hard so
the semantic aspects of objects in
motion or movement of subjects are
neglected so there's a core knowledge
element is missing
and the other part with religious
disappointed me was instead of taking
the 320 by 240 images of course the
cameras now they yield 16 on the patron
funded and in these small images we took
relatively many pixels and to determine
one age now I've seen that PhD students
doesn't know so they use 3x3 elements
and do hundreds and thousands of
features and then they want to come up
the use probability measurements and
haven't knows what in order to come up
with a hypothesis for an object this is
for my point of view not the right way
to go the other point was that stereo
interpretation 33 papers have been on
stereo interpretation we know as a
humans with our base of seven
centimeters we can do up to 10 meters
but not more they're looking at 50 and
more meters and the camera babyface is
maybe 15 or 20 centimeters so it seems
to me that that's a waste of computing
power and of course they have to take
16-bit pixels since the instead of the
eighth one we usually do and I claim you
get the same information by looking at
semantics as I mentioned in the traffic
scene you don't need all the computing
power you know you don't need to waste
it
on this field so my conclusion is
realistic dynamical models including
control application to the evolution of
movement the trajectories come out of
that and the situations are key elements
of general intelligence and of course
now you do have certain domains in which
you do have the knowledge because there
are subjects that do have certain
behaviors and they have certain sensory
systems in order to be general like a
human to react properly you have to
determine what type of animal is it what
type of robot isn't what is he capable
of doing what are the usual maneuvers so
the capabilities for hypothesis
generation on the object subject and
situation level they are very important
and of course you have to have a
critical testing but now maybe
provoking statement the diversity of
human cultures may stem from an inherent
tendency in all branches of development
of human societies towards daring
systems of hypotheses daring systems
means on several level and they should
fit and they should be consistent to
each other and defending these
hypotheses not over Horrors or over days
but over decades over centuries and even
over millennia thank you for your
attention I might add if there are
compliments of course they are due to
the PhD students at about three dozen
working on these problems over two and a
half decades so they are the ones who do
deserve the compliments otherwise if
you're interested there's a website I
mentioned already before and for ground
vehicle application there's the book
that came out in two or seven other
questions yeah I did have two films in
there but can you repeat the question
yeah the question was whether we've been
doing more difficult parts like driving
in rain and with on cross country roads
with jagged boundaries this we started
in 1988-89 and there are films you find
on our website where you can see that
you have to adjust the type of edge
detection and it turned out to do to
drive on on country roads with with
grass growing into the road that you
just take a larger sized rectangle where
you determine the inclination of the
road side and you have to fiddle around
for a while and this is some point where
the system can learn by itself because
we do have generic
edge detectors and the system can play
around with the parameters and this can
be changed from one frame to the next so
the system can play around and see what
is the best combination of parameters
but we claim we can do it of course
there are situations where we get in
trouble I don't don't misunderstand me
but we find that this approach is really
what candidate also for difficult
situation driving and rain was one of
the first things that happened to PhD
students I want to finish the grade and
they say I have to to do my experiments
now whether it's raining or not then
even a wiper going in front of the
camera doesn't doesn't do any harm there
was another question how many near
misses and excellence did you have the
only one well of course you have to be
careful but the only real possibly
dangerous one was one which one PhD
student when he came back said well this
was an experience I wouldn't like to do
another time he was driving at
relatively high speed on the Autobahn
and suddenly the system had picked up
some information on an object which is
told that I should make an avoidance
optical avoidance maneuver to the left
and so the system started going off to
the left and then the driver the safety
driver noticed that there was another
car driving in the neighboring Lane so
he took over and then brake but
otherwise no we did have no single
obstacle that beam touch any other
object
the only thing he did was one of my
first PhD students when they made the
demonstration and in the skid pan at
daimler-benz the chief of research came
to see the demonstration and he parked
the car why in the previous runs we've
been starting our test so the student
came around this group and then he
backed up in order to get into the
starting position and he didn't notice
the car off so we said this was a
remaining impression we made
very switch paws of daimler-benz yep
autonomous ducky do you do you think you
can eat you can eat yes I know well
there's been quite a bit of effort in
the car manufacturing industry in this
direction because you can decrease drag
of these road trains but I think they
finally came to the conclusion that
driving very close to each other and
they've been doing that with Shandi
meter the 30 centimeter distance and
they say you can do it only if if you
transfer from the front we go to the
next the control application they are
going to do whether they are going to
hit the brakes or the throttle and the
final decision at least of the the test
drivers I've heard was that they said no
let's let's not try to do that because
we feel it's maybe maybe too dangerous
for the normal driver and the situation
may become too dangerous and I think
this has been abandoned I don't know
whether somebody is going to consider
this for the future
they are considering still say keeping
two or three meter distance but but not
these very short ones and docking I
didn't catch the charge well this could
have be done I think it's it's not I
don't see any problem why you couldn't
do it no and if these are special
vehicles they can they may have special
markings and no I I don't think there's
a problem you should make a decision to
pick been you you been he certainly has
some of the interesting question
we will discuss in that workshop various
scenarios I'm curious what you think
about driving as a scenario for working
toward really broad human level AGI luck
do you think driving cars in general say
in general urban environments on the
desert in Mongolia and anywhere the
humans can is that an AGI hard problem
like would you need to some sense be a
general human level mind there is solved
drive I think it's a hard problem if you
drive in cities and in villages in the
countryside where you have you may have
any type of animal and the system has to
be able to detect cats and dogs and of
any size and calves and cows and horses
children of any type with any type of
toy or vehicles and I think that the
most difficult part is that people carry
loads all they do have different
clothings and to adjust to that you need
a wide knowledge base but of course you
can do it do you think you can get there
by kind of incrementally advancing this
actually is described but you know what
you need is a representation of typical
behaviors of these animals you're going
to see if the subjects or the drivers or
if you know there are children you can't
make predictions what it's going to do
in the next second but usually as a
grownup person is driving on a bicycle
or walking you can make reasonable
predictions no I think this is well an
in line but you have to have a
representation of really motion of
objects not just objects that can move
will that do them but but the typical
motion of objects and let's say some new
kind of entity enters the enters the
environments a little flying space
capsules or something could the system
adapt to avoid those things the system
does if it recognizes there is an object
of course it can react to that so the
question we've been asked by by the
industry people come and you affect your
industry the human driver knows if there
is a
an empty bag of paper big one and the
wind blows it on the roads does the
system stop in front of this bag Oh
y-you know a human driver would go on
and that's common sense no that's common
sense knowledge I would say that's some
way to go in order to each other but
it's not impossible it can be done with
this sort of architecture good question
I don't know whether you're aware of the
details and whether I should to talk
about it once I gave a talk at each guy
in 1989 I think it was in Detroit and
then somebody some American colleague
came to me and he said you know he took
a little DARPA program and for me
surprising a few months later I learned
that all the vision projects were taken
off from DARPA and army research lab in
Maryland was the one who's doing the
next developments demo one demo 2 demo 3
and we did have a cooperation with Army
Research Lab and NIST and sound off in
Princeton so there is of course certain
what is it animosity of DARPA against
our work because we stopped their show
we didn't know about that they were
doing that and when we started we
started our own the Japanese started on
their own in 78 79 we should mention
that but not with digital computers and
they had analog ones but in 85 I learned
about the activities in the US and a
little bit later I learned that
performance level was quite different so
in 86 we did the first demo in
MIT at SPIE conference and people
started laughing at us because we were
driving along white lions and the year
later they said if you can go along
roads with our shadows of trees you show
that then we are convinced that this
prayer Crouch may work so in 1987 we
came back and to demonstrate the exactly
that and then people started thinking
about future we're closed down so are
you are you essentially saying are you
essentially saying that once you gave
your talks and people were so impressive
they said you already achieved all the
things that they want to achieve in this
program and they cancelled it no I think
you should you should look at it as a
from a different point of view I think
they were doing cost comparisons and I
was asked in this talk so how much money
did you spend on that and I made a brief
account of brief computation as it may
be about two million marks and then I
learned afterwards that I think in this
country they had spent 17 million
dollars just for the vehicle for one of
those four accelerator and of course the
huge program with all these
architectures 14 different architectures
with hundreds and thousands of single
bit processors they were looking at how
it is done in collection of carbon in
biology and they transferred this to
silicon but of course this is completely
different you can look at the
functionality and you can do that but
you you cannot copy because in in carbon
the reaction time is about one
millisecond but you have 10,000 cross
connections and in silicon you do have
what milliseconds microseconds
nanoseconds switch time and
communication bandwidth which is
tremendous there was a program both in
Germany and I think also here developing
an electronic eye and they thought you
would have in order to to develop this
you would have to make a computation
right at the sensing level in the image
because in the human brain we do have
120
million pixels in our eyes but there are
only 1.2 million connections to the real
part where image processing is being
done so there's a factor of hundred
reduction but it turns out that in
silicon there's no problem you can send
nowadays I think it doesn't video
signals right through one lineman you
can separate the computer from the
sensor so quite a bit of money also in
my country has gone in the wrong
direction because there was a well maybe
too fast in transferring architectural
points of view from carbon to silica you
should look at the what is the hardware
you're implementing this all right
Google is throwing us out of the campus
and while we are</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>