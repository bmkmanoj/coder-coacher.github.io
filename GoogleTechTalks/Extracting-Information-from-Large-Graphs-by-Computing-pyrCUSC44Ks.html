<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Extracting Information from Large Graphs by Computing... | Coder Coacher - Coaching Coders</title><meta content="Extracting Information from Large Graphs by Computing... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Extracting Information from Large Graphs by Computing...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pyrCUSC44Ks" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I
hope so my name is marine I'm actually
engineering the search quality group and
so I'm introducing professor van sombra
del from the université catholique de
la van in Belgium so he's a professor of
Applied Mathematics at UCL and is also
the head of the department of
mathematical engineer engineering over
there and it was my thesis advisor when
I was an undergrad student in Belgium
it's also currently doing a sabbatical
at MIT and today's talk is going to be
about a concept a new concept of
similarity measure between nodes in
graph and it's going to talk about
different application such as the the
web graph but also some
telecommunication networks and also
synonym generation from graph of
synonyms
okay very well so thank you I'm very
happy to be here for for for for this
talk I'm apologized for the rescheduling
of the of the talk I initially planned
at this morning at eleven and my flight
from Boston yesterday was canceled and
so I raft only this morning and I'm a
favor happy to be here even though the
it's raining the temperature in Boston
when I left it was about zero degree
when arrive so I'm happy to have an
increase in temperature so as marine
said I'm going to talk about similarity
in graphs and networks and let me
directly illustrate what type of tools
I'd like to to present and the tool I'm
going to present a appeared in in in a
paper which has a marine as well as a
co-author enzyme review two years ago so
the tool i wanna describe is a way of
expressing how similar nodes are in
graph and here is a very little example
you have a first graph graph a here and
you have another graph be here and then
from these two graphs reconstruct a
matrix that expresses how similar these
nodes that the nodes in the respective
graphs are ok so the matrix has as many
columns as there are nodes here and as
many rows as there are notes here and if
you look at the entries of the matrix
these entries are attempt at
characterizing how similar nodes are in
these respective graphs so for example
so this is what I will attend to do in
this similarity matrix and so for
example if you take note for here and
node 5 here so you take the fourth
fourth column and the fifth row you get
a zero which means essentially that this
node here doesn't at all look like this
node here if you look at the largest
entry in this this matrix the largest
entries in here and this entry
corresponds to node three here and note
for there and so this this
entry says that indeed note 3 in the
graph B looks very much like a note for
in gravity okay so what I'm going a do
during this talk here is to explain how
we can derive the similarity matrix
answer this this microphone is is just
going on and off is that connect and i
should i keep using the microphone or oh
it's can you hear me we well with the
microphone okay forward I'll continue
with the microphone and okay so what I'm
going a do during the talk is describing
the way we construct this matrix and
matrix is very inexpensive to compute
it's very cheap to compute and so it can
be computed on very large graph I
haven't represented here very large
graphs but it can be constructed for
graphs that have millions or more nodes
and typically one has 11 possible
application is situation where we have
one little graph that expresses some
structure that we'd like to extract from
the large graph okay but not other
potential applications are when go of
graphs are equal in which case what we
do is essentially characterizing how a
similar nodes in the same graphs are
okay so we just then have one graph and
we try to express how similar nodes are
in these graphs so here is another
illustration of the matrix you can of
course choose one particular node in in
the first or the second graph which
corresponds to picking I'd row row or
column and then ask which node is most
similar in the autograph so for example
here if we choose the node 4 it means
that we choose the fourth line and in
the fourth line you are asking which
which is the entry dad is loud which is
the largest entry this is the largest
entry it corresponds to column 3 which
means that the node here that is ladi
that this most similar to this one here
is not three okay so it's a it's a
dissimilarity matrix is a useful way of
characterizing similarities between
nodes in grass and as I said it's it's
easy i'll describe it during the stork
it's easy and inexpensive to compute the
cost is linear in the number of edges so
you may have
large graphs and it can extend to a
situation where we not only have grass
but we have networks so typically we
have weights on the arcs so if if this
corresponds to a website and this
another one maybe we have a weight on
the ark that tells how many links they
are and if this is a cellular phone
communications network then probably
this says how many how often this this
person who has called this other person
there and so on so it extends to the
situation of weighted graph so what I'm
going to drain the talk is that I'll
first describe how we came to that
definition which we came to by extending
a notion that was introduced by jon
kleinberg from from jon kleinberg from
Cornell which is the heat methods hits
method which I believe that most of you
know for for searching the web which is
a way an alternative way to the Tudor to
the PageRank computation and then I
described how to extend this notion of
leads to the computing similarity in
graph networks and then I'll go to a few
application and as Maureen said one of
the application will be automatic
synonym extraction of monolingual
dictionaries and I'll describe some
experiments we did on a dictionary okay
so the method of clean work of glenn
burke as you can see why this is a proof
not mathematical proof this is not
approved by simulation it's approved by
by google scholar that has attracted a
lot of attention in the scientific
community yes beginning what is the
motivation
be any similarity between bone and
gravity and low gravity they have
nothing to do with each other apparently
other than both round when they both are
lazy members yes so but wouldn't you
agree that that this node here probably
looks a little more like this one rather
than this one here right so enjoying
similar ok so then then what you're
asking I suppose is what notion of
similarity do you want to express index
in the in the mitt into similarity
matrix is that your question or what
type of similar ok so there are many
different ways of defining similarity
and this this one here contrary to the
usual definition of similarity usually
ships similarity is defined in typically
in terms of the number of paths or
weighted sum of number of paths that are
between two nodes and tues nodes are
said to be similar if they are sort of
close to each other or if the number of
puzzle whereas here the notion we
capture with the similarity matrix is
completely different from there in the
sense that the two the two graph maybe
the two nodes may be completely
disconnected and even though have a high
degree of similarity and the notion of
similar injury we capture is the one of
the sort of the structured position of
the node within the graph so this node
here has a structured position within
the graph that is somewhat analogous to
the destructive position of this node in
them in in this autograph and so that's
notion of that we capture so the notion
is is really not a measure of the
weighted sum or which wich are the usual
notion of similarity in graph so a
weighted sum of the number of paths but
but the notion of the structural
position of the node within the graph so
for example if we look at communities
typically and we have two different
graphs that represent communities
perhaps that if we if we identify notes
that are similar in the two graphs these
nodes will represent people that have
similar into connection to other people
okay so although they are not related at
all to each other because they have no
links but they play the same role within
their community so that's the notion we
want to express yes
the output itself is the similarity
matrix can be larger than the number
Edge's project can be
when yes well okay so I shouldn't I
shouldn't have phrased this way what I
mean is that once you choose typically I
choose a graph here which is a small
graph because i would like to extract
from a large graph I'd like to extract
features of that large graph that that I
expressed in the small graph that I've
here and then and then this computation
is the number of edges i have in the
large graph okay but you're right so if
in info generality if in particularly if
we have the same graph here and graph
there then this is a n square and we
need at least n square operation yes
okay yes so i'll come i'll come i'll
give the precise the formula for the
similarity matrix okay so it's not like
I understand some of your questions so
it's not like we had an idea of what
similarity we want to express and then
we can came up with a formula so it's
not the way around so we extend this
notion of Kleinberg and then we looked
at what type of similarity eight it
expresses okay but i'll give a very
precise formula for this okay so let me
come to a first quick description of den
berg's method since or method is an
extension of Glen burst methods and let
me first describe the inverse method so
the idea of glenn burke was to assign
two websites to web pages a measure of
whether the web page is a good authority
or whether it's a good HAP so if you
type in a search word like university
belgium then these pages would be good
authorities because these are the home
pages of different universities in
belgium ok so these pages will have a
very high authority score whereas this
page here has a number of links to other
Belgian universities and so this page
here will be perceived as a good help
okay so the idea of glenn burke was
simply to assign to every page the score
that is that is its hype score and score
that is a hop authority score okay so
every page has two scores not just one
not just a page rank but it has two
scores perhaps core and an authority
score now when are you a good help
you are a good hub if you're pointing to
good authorities and when are you a good
authority you're good authority you have
a good authority score is pages that
have a gud hype scores are pointing to
you okay so it's just a certain
definition and he expressed the certain
definition in this in this formalism
here so every page is given an authority
score and a hub score and web page with
a higher authority scores if they are
pointed by page that have a high hub
score and the other way round okay so
how do you how do you start and how do
you how do you pass the way that that
this is a certain definition well one
way of doing this is to just assign the
same hype scores and authority scores to
all page at the start and then start
iterating this this little linear update
so the linear update does what well the
haps tour of page Jay Happ score of page
Jay gets the sum of the authority scores
of the pages jay is pointing to and then
just the opposite for authority score
the author is called authority score of
HJ is just the sum of the happs course
of the pages that are pointing to Jane
ok and so we start with an initial
assignment of these have an authority
score and then we iterate this of course
this will explode with time because if
we give a initial assignment of 1 this
will just increase but then at every
step reduced rescale these two vectors
okay and if we do this then what we can
what one can prove is that independently
on what you started from in terms of
initial score this process always
converges and it converges to the same
value in depth that is independent from
the initial assignments of the hub and
authority scores and it corresponds
these two values these two vectors of
help scores and authority score
correspond to the to the eigenvector the
dominant a and vector of our dominant
eigen its dominant eigen vector of a
matrix that is computed by just taking
the adjacency matrix of the graph
x its transport and the other one is the
dominant eigen vector of the transpose x
the matrix so these are the singular
vector of the adjacency matrix of the
graph okay so it's a very natural and
this dissidents essentially the power
method for computing egg and vectors
which is which is a of course or
well-known way of approximating and
inventors of linear transformations okay
so now what I'd like to do is to think
and that's just that's just the
innovation I'd like to think of these
hype scores and authority scores in some
sense what we want to express here is to
assign scores two scores to every web
page and these two scores correspond to
a similarity to the hub or a similarity
to Authority and what I'm going to do
next and so Glenn Beck methods expresses
similarity between nodes in a graph with
this node in this little graph and this
note in this little graph and what I'm
going to do is just extend this ID and
have here a graph that is more general
that just a habit ority graph okay very
a very natural thing to do let me do it
just for a simple graph a very simple
graph with three nodes so it's the first
the simplest extension of this ID so
assume that we now have beginning a
beginning nor the central node and an
end node and we have this little graphic
you and now I start again with my large
graph and to all note in milara I'll
know associate three values not just to
have an authority but three values and
these three values will correspond to
whether the corresponding node is
similar to a beginning node in this
graph or central node in this gravel and
note in that graph okay so this is very
simple ID we just have known a three
node graph and we compute similarity to
this tree node so how do we do the
update well am I what is my score what
is my beginning score I'm a good I have
my big madman and begin score of page J
is high provided i'm pointing to pages
that have a high c score okay exactly as
we did for Kleinberg witness
in slightly more complex but we update
the value of J the B value of G the
beginning score of page J as the sum of
the center score of the pages i'm
pointing to and similarly for the end
score and then for the center score we
take of course the value both of these
guys and of the these guys there okay
and then we update this transformation
it's again a linear transformation of
the score so if you if you take bc and e
and and you look at how the hardy
evolved after just one iteration is just
a linear transformation and so for sure
we'll come up with another type of eigen
value problem and indeed what the
quantity that we're going to converge to
with this course is again an eigenvector
but of a matrix that is not constructed
from the initial G C matrix of the graph
but in a slightly more complex way as
the one we had for the for the Kleinberg
method okay so here's a little example
of a graph of which we've computer I've
computed the similarity with with this
graph and if you take for example the to
note here and you can looking in this
graph and you're asking which among all
these nodes which one is most similar to
this note in this little graph well it
means corresponding to the corresponding
column here and what the letter says is
that this entry the second and the third
entries are those that are most similar
which means that the note 2 and the note
3 here are the two nodes that are most
similar to the note 2 in this little
graph okay and we have as we can prove
as a property which is analogous to the
one we had for Kleinberg that if you
start from an adjacency matrix of the
graph then the central score which means
which by this i mean the score associate
to this node will be given by the
dominant eigen vector of the matrix here
BB transpose B plus B transpose B where
these where the B matrix is the
adjacency matrix of the graph there so
maybe I can just yes and if I know
denote this in a more
compact way i can use bc and II just to
put them in in matrix form so as to
construct the similarity matrix and then
one can see that if you look at these e
trade's and you look at how busy and air
are being updated okay then the question
is how is this matrix s the similarity
matrix s how is this matrix being
updated well you can express the update
of the matrix s in terms of the
adjacency matrices of the large graph
and of this little graph here and now a
is the JCC matrix of the large graph and
b is the Jesus matrix of this little
graph here and if you look at the
updates these are updates are linear and
you can write them in matrix form in
this way so that what the method
whiteboards down to and that's that's
the only equation you have to remember
in order to compute the singularity
matrix this is not of course writing the
matrices is not the most efficient way
of doing it you never you should never
write these matrices explicitly but what
the computations correspond to is just
making this update we start from some
matrix S which can be taking which can
been taken an arbitrary matrix and
typically we take a matrix with once
everywhere and then we update this
linear transformation and we converge
exactly as with the power method for a
in values of our eigen vectors dominant
eigen vector here we obtain a dominant
eigen matrix and this is a the S matrix
is the egg and matrix corresponding to
this linear transformation okay now the
fact that we using the GCC matrix let me
just give a quick reminder that the GC
matrix of a graph while I'm sorry is a
matrix that has its rows and columns
corresponding to nodes and has a zero
and once depending on whether there is
an edge between the corresponding nodes
okay so this is a just a small this is a
matrix for a small portion of the World
Wide Web about 60,000 nodes and as you
can see it's a highly structured it's a
highly structured matrix which clearly
shows that of course the
matrices should never be computed
explicitly because they have they have
for most of the entries these entries
are 0 but going to them we can see the
apparent structure of the interrelation
between between the nodes okay so now
this is the updating equation and I've
motivated this updating equation for the
particular situation where we have the
BCE graph and so the way this this
little graph appears in this formula is
that it's one of the two adjacency
matrices appear in this in this update
but there is no reason to limit yourself
to just graphs of this type and the same
formula exactly the same form and I can
be used for any type of craft and so
this is the way of one way of expressing
similarity between between nodes and
graphs and that's the updating equation
i have used in order to produce the
singularity similarity matrices that
i've shown at the beginning ok so the a
great mate for computing similarity is
not very very simple we constructed the
GC matrices of a and b we start with a
matrix that has once everywhere we
update and we need to take the even in
order to ensure that ensure convergence
we need to take the even eat rate we
compute the even normalized eat rate in
the sense that at after every step we
also no need to normalize the matrix
divided by its norm and then after
convergence will stop and we obtain the
similarity matrix which is which is
nagging matrix associated to this lean
transformation okay so here is again the
example of 24 and as I said also at the
beginning you can of course apply the
technique by using here and there the
same graph in which case you're put in a
matrix that expresses similarity between
nodes within the graph so here is the
similarity matrix one obtains by by
starting with this graph and having the
same graph to which we compare it and as
you can see the results are rather
convincing sense that what the method
finds out is that these two
which is of course the nuts referring to
do which that these two nodes are
similar to each other but there is no
similarity between the other nodes which
correspond to the fact that we have a
nonzero entries on the diagonal elements
and n matrix theory that express the
similarity between three and four okay
so let me briefly describe one first
short application of this to the
analysis that we did of a cellular phone
network this is a contract we have with
a mobile phone company in Belgian one of
the major mobile phone company in
Belgium they have record complete data
for six months period in 2005 and so of
course bedroom is not that a large
company so we have two million customers
but it's already its large by a Belgium
scale so we have two million phone
numbers and they've they've they've
formed about 600 600 million phone calls
between these these these telephone
numbers and what cellular phone
companies are interested in a typically
question like whether how the graph
looks like this graph or inter
connection how does it look like are
there people who are likely to move to
some other mobile phone company can
identify communities within that that
network and typically how well as a new
service or a new project spread in that
community and if I want to spread the
product in that community what should i
do should i who should i give the
project to where where should I target
the advertisement and so forth so they'd
like to have to do a viral marketing
okay so this is a the distribution of
calls received on this on this large
graph as you can see some some of these
customers receive a very large number so
this number of course received number of
customers on a log-log plot and so there
there are there are some customers who
have received more than a thousand phone
call but most of them here have received
just one this is a very large number of
customers who were just your receipt
just one and you have this distribution
of number of customers versus number of
course and this is
the type of graph you obtained in in
scale-free Network for scale-free
Network you have in line and a straight
line here which means that there is a
power law distribution of these these
degrees so this is a justin attend a
very fine that indeed we have a scale a
scale-free Network as we expect in the
case of communication between between
cellular phone and this is just a
reminder that while the usual random
Network model for modeling the
communication between cell phone
networks is is inappropriate and it's a
it's more like a scale-free Network and
these other type of networks also have
this power law distribution of of
degrees then this is a representation of
the graph way of communication between
between all these nodes and so in order
to produce this since we have to
millions notes of course we can not even
plot 2 million pixels on a computer
screen so we had to somehow identify
leaders or main people in communities
and in order to do that we used the the
notion of central score we had a
presented just before in order to
identify the person that we'd like to
keep in order to make a representation
of the graph so it looks like the graph
is is separated into part and once once
after having obtained it so we went back
to the mobile phone company and we asked
whether they have data about the
language because you may know that
invasion we speak both French and
Flemish and whether they had information
about the language and they gave us this
information and so it came out very
nicely as you can see you have this is
balloon so it's these here please speak
French is one speak Flemish and then you
have a well the green ones are Brussels
because brasserie sort of bilingual is
in between clinician and French so it
was a convincing argument that this this
representation of brass represent
something okay so let me now describe
another application which is the one for
automatic extraction of synonyms in a
dictionary and this will be the last
application and described so in the
dictionary graph will consider situation
where the nodes are the words in the
dictionary and there is an edge you too
v.v appears in the definition of you ok
so we construct a large graph and this
was the webster dictionary that we took
and again you can look at the in degree
distribution of the words or you have a
you have a some words have an in degree
of a thousand or even almost 10,000
which means that they used in 10,000
definition and so these are the words of
and so forth that are used in many in
many definition and so again we have a
scale-free Network which is the network
we construct from that dictionary so how
to extract automatically extract synonym
in that dictionary so here's the method
that we use and I then describe the
result we obtain so assume we we'd like
to find a synonym to lightly so we have
likely we first look at the neighborhood
graph so we look at all the nodes that
are used in the definition of lightly
and we loosed we used all the words that
use likely in their definition so this
defines a number of nodes and we then
look at the sub graph of the dictionary
graph that correspond to these nodes so
we reconstruct the sub graph so this is
the sub graph reconstructed from lightly
and this is a rather well we haven't
represented to all the words because
there are more words here but in order
to this this example to fit on the
screen we we remove some of these and if
the graph is is not large enough then we
may typically go one step more and look
at the words appear in the definition of
the words w and the definition of the
world so it we can take a diameter that
is larger than we have here and then
once we have done this we look at the
similarity score of the nodes that are
present in this graph with the central
node in this little graph ok so because
of the way we've constructed this little
graph we've looked at all the notes that
we pointing to and we've
all the book notes that are pointing to
us now we expect the nodes that have a
high central score to be a good
candidate for synonymy 22 or likely okay
and we use this to to rank automatically
synonyms so here is the result that we
get and this is just an example for the
world disappear and we have three
different methods here that that do
automatic synonym extraction and these
two are handmade then a synonym
extraction these are the synonymy
dictionaries that have been handmade and
this is just the last but line here is
just an estimate of how well the ranking
was as evaluated by humans ok so for
disappear the central method here is the
one we've used so this is the way this
is the situation where we compute the
central score of all the nodes that up
here in the neighborhood graph and we
rank the nodes according to the central
score and as you see as you can see
while we have reasonable candidate for
synonymy with with reformer method which
which clearly outperforms the other
vector method and the a crank method
which is essentially which is a matter
that was proposed by researchers at
Stanford that is essentially an
extension while an adaptation of the
page rank for for the situation of
extracting synonyms here's another world
science it's not quite clear what what
synonyms we could have four sides and
again as you can see well among the
automatic extraction of synonyms the
center scores performing with reasonably
well it doesn't perform very well as
compared to the to the words that have
the synonyms that have been composed
manually by humans but one drawback of a
4 method is that it does not allow us to
use composed words which which which is
allowed in human-made actually like
knowledge domain knowledge based and
field of study
which cannot be extracted from the
central score in the way described it
here okay lost so the last word is sugar
and again we performing reasonably well
as compared to the to the other two
methods that you can see that the hand
meet the human-made dictionary of
Microsoft seems to focus on a particular
aspect of the world of the world through
sugar so we we've had exchanged with Lee
petty harbour which is the French
dictionary and I now considering
implementing this method in there in the
cd-rom version of of the additionally
okay so that's about the conclusion of
of my talk so I've introduced a new
notion of similarity which is as I said
at the beginning which has nothing to do
with the number of paths between the
graph between the nodes in the graph but
which is which expresses a more
structured property of nodes in graphs
and this this similarity matrix is very
easy to compute we just need to start
with a matrix that has ones everywhere
and then iterates this this this
updating equation this normalized we
need to normalize of course but we
compute normalized iteration of this
equation and so which clearly shows also
that the method is inexpensive and so we
can it can be applied to two very large
graph and networks okay thank you very
much for your attention
wait what little person where did these
are JCC matrices they'll have weights
that correspond to the weights on the
edges instead of having one if you have
more have several links you just have a
value that represent is and it could be
fractional values also so yes yes in the
case of the three now you'd be the
central score since you're essentially
linear you can get a good central score
by either putting two lots of n guys or
coming from lots of beginning guys and
not both have you considered a nonlinear
case where you simply multiply the those
two things are some together or instead
yeah instead of taking this term we
could take the product that's what
you're okay so in the central score but
the way we define the century score is
by so if I can go back so what you're
saying is in these iterates here we have
a plus sign we could have a project sign
yeah we could have a product sign or we
could have this quantity to us well any
norm of course of these two we could
have some some power some power here
plus some power there and all types of
all these types of updates are possible
the advantage of taking Sam's here is
that we just 10 are in in linear algebra
and we can prove convergence and all
that but it may be more relevant for
application but probably more difficult
to prove then convergence but more
relevant for application to have
something that is more sophisticated
than just a plus yes a product for
example or something else yes
this mention the word convergence what
is what determines the rate of
convergence for this well since this is
just an etrade this is the power method
so it's the the vector will converge to
the dominant eigen vector and the rate
of convergence will be given by the
spectral gap so which is the which is
essentially the distance between the
first and the second eigen value of the
matrix okay so one would need to figure
out what what's the second egg and
eigenvector again value of this link
transformation is depending on the size
or in depending on these two graphs but
that's that's what it is right so it's
the second eigenvector a in value that
will dictate the speed of convergence in
practice after just a few iteration ray
reconversion that's that's what we've
roped off in practice
okay you describe a scenario where the
single art is perfect me too
similar to the nodes in a graph with
themselves I didn't see any wants in
that matrix there was like 0.40
something
if you had a similarity matrix that was
progress
so why don't I see wants what is the
perfect similarity what this is just a
matter of yeah okay so you ask me why
why don't have do I have a one here yes
yes we know last we we compute the
similarity only to be compared one to
each other so it's only the relative
values of these entries that have to be
considered and so here we've taken the
normalization where the sum of the
squares of all values sum up to 1 but we
could have normalized in different way
and we would have had once everywhere
okay so this is just the fact that yeah
I suppose how many values do we have we
have six so if you take the square of
this it could be 1 over 6 yeah so it's
just the scaling uh but but of course
similarity the similarities coefficient
that enter here should only be
considered relative to each other it's
not never an absolute notion of
similarity
Oh
hello s are using it each of these e
trade's gives only one color or one rule
now this is this is the full this S
matrix has as many rows as they are
notes in the first graph and as many
columns as they are not in the second
graph and so every creation just updates
typically updates all the entries of the
matrix it's a complete update of of the
entire matrix it's not just one column
but maybe I didn't get your question
this so this is finding similarities of
individual notes if you look at
extending it to sort of finding the best
match for the entire breadth so they can
flip back to the slide you run before
this original going to add the sort of
the small single class this one right so
like how would you find the best match
for all of grab a beer a beer and the
best match for a given single note yeah
they are good question too right so
whether how can we find a match of this
into into a large graph light like that
for example right so there are intrinsic
limitation of what can be possibly be
done because if you're given a graph
here which is isomorphic to another
graph there so they're just just a
remembering of the nodes then a perfect
matching is possible but deciding graph
isomorphism is a problem for which there
is no known polynomial time algorithm
okay so there is no way we were going
with such a simple technique so the
graph isomorphism problem okay so it can
only be aprox to a some approximation
yes maybe we're not going assault that
that that exact matching problem but but
it will so typically if you take graph
here and then you you start with a
little graph here and then you sort of
build a large graph around that one and
then you look at the similarity matrix I
expect that the that you yet that you'll
find the coefficients and by looking at
the coefficient matrix I expect that
you'll be able to to rediscover this
little graph in the large graph you've
constructed but but but not in a
completely systematic way because that
would be graph isomorphism yes
No
yes
yeah yes it's not perfect and of course
the example i have taken i've tried to
take the good examples on this matrix
right but you but whatever you come up
with the notion of similarity is not a
mathematical notion unless you define it
very precisely and so people have
different notion of similarity and for
example for you it would mean it would
sound like and maybe for most of you it
would sound like you'd like to have a
high similarity between four and two
right so which means taking four and two
and in fact it's not that bad because
this quantity you should you should look
in this in this corresponding column
right so it excuse me oh it's this one
here yeah so it's not the best you can
do it's the first it's the third value
but there are 0 is there but but
whatever you come with if I was to come
with another similarity measure you'd
been answered and satisfied by some
other relation probably and so I'm not
saying this this notion is perfect but
it expresses some of the similarities
between between these graphs if what you
have in mind is counting the number of
outgoing no outgoing edges and incoming
edges then of course all of this you can
can be thrown away right that's not what
we have in mind and and probably then
then a very simple algorithm is possible
if it's just a matter of counting
whether they have identical number of
incoming and outgoing edges this
captures something different
but there are other notion of
similarities and this is just one of
them which is easy to compute and yeah
inexpensive to country
other half of your issues
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>