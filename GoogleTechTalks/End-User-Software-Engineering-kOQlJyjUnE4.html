<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>End User Software Engineering | Coder Coacher - Coaching Coders</title><meta content="End User Software Engineering - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>End User Software Engineering</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kOQlJyjUnE4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody our speaker is dr.
Margaret Burnett who's from Oregon State
University telling you a little bit
about it she's a professor of computer
science she works on end-user
programming which is where I know that
her from we share an interest in the
topic and and use their software
engineering which is she's gonna explain
to you what that is and how it's
different than any user programming and
also on gender issues in these contexts
which which there's some fascinating one
so she's the founding project director
of the uses consortium which stands for
end user shaping effective software I
got that which is a collaboration among
a number of universities and IBM and she
brought with her because she knew it was
such a fascinating topic and she
wouldn't be able to answer all the
questions to other distinguished
researchers in this area allen Blackwell
who's from the University of Cambridge
and Alan cypher where you have a excess
of Alan's today who is currently at IBM
but you may know his work at Apple cocoa
and things like that so they were going
to expect them to join in and make this
a lively discussion you didn't get that
free lunch for nothing you have to work
for it now excellent ok so here I am to
talk to you about end-user software
engineering and I'll tell you a little
bit about that but then I'll focus on
the the part of it that's currently near
and dear to my heart surprise explain
reward so I'll tell you about that too
so starting off with well what is
end-user software engineering and what
kind of problem is it trying to solve
the beginning of this isn't a problem
it's just a reality there's a lot of and
user created software out there in the
world mostly it takes the form of
spreadsheets but there are a lot of
other forms of a to die ammok
dynamically created webpages simulations
in any manner of things email filtering
all of these things are created by
end-users so their programming and what
so far is supported is the create phase
so there are many many ways for end
users to create their own programs
unfortunately there's a lot of
literature that says that errors abound
in these programs that end-users are
creating and for example up to 90% of
production spreadsheets those are the
ones that people are actually making
decisions about have been reported to be
erroneous there's another problem too
and that is that end-users are
overconfident about the correctness of
the modifications and and programs that
they create so there are these big
reliability problems and so end-user
software engineering basically is what
happens after create how can we support
the rest of the software lifecycle so
we'd like to reduce errors and end user
programs that's what it's all about but
at the same time we don't expect
end-user programmers to have the
motivations or the training or even the
interests in software engineering
because if they did they probably would
have gone into computer science or
software engineering careers not
whatever careers they're really in the
research that I'm here to talk to you
about today is work that we've done in
the context of the uses consortium which
Robin mentioned and as she said EU SES
uses stands for end users shaping
effective software this is a big
consortium of researchers from seven
different institutions Oregon State
where I'm from Carnegie Mellon Drexel
Nebraska Penn State Cambridge in the UK
and most recently IBM so anyway we're
all working together on aspects of the
end-user software engineering problem
and that's the context of the research
I'll be telling you about today here's
kind of what what'a q what areas of
space are occupied by end-user software
engineering
there's this traditional software
engineering and languages group and then
there's what we might lump together is
HCI in psychology and then there's
education and so the the overlaps the
intersections in these bubbles are where
end-user software engineering lives for
example some of my colleagues are
looking at sources of faults or errors
that end-users make when they're
creating and modifying their own
programs and that falls over there into
into this intersection area some people
are working on ways to do static and
dynamic analysis of both the quote code
that these users are creating like for
example the formulas and spreadsheets
and also the data that's getting fed in
and out as a way to try to spot errors
to help end users prevent or or fix the
ones that have managed to creep in the
incremental 'no sand performance and
speed of these algorithms is a big deal
because of the way we need to
communicate with end-user programmers
they have environments in which
immediate visual feedback is expected
and then they have their own
expectations too so we draw findings not
only from what we know from software
engineering and languages but also from
education and how you communicate with
people that don't necessarily understand
the concepts that are behind what you're
trying to tell them
and of course HCI and psychology I'll be
talking to you a little bit about some
things that that come up in those three
stars but the big one that I'm going to
talk to you about today is this one if
we build it will they come so that's
where surprise explained reward fits in
if we build something wonderful for
end-user programmers how are we ever
going to get them to use it as a way to
guard against or fix errors that have
crept in so in the previous slide I used
words like static analysis and
incremental algorithms and that sort of
thing and when I do that a lot of times
what happens is people's mindset sort of
slip over to the software engineering
and computer science world so I don't
want you to forget don't go there
this isn't for you
it's for them okay so here are four and
user programmers for example a teacher
and that teacher might be using a
spreadsheet to keep track of his grades
we have a veterinarian perhaps she's
building some sort of dynamic webpage
that helps her manage appointments or
who knows
or maybe the musician is doing that okay
so these are the people we're talking
about supporting even though we don't
expect them to have any training or
interest in software engineering and we
don't expect them to develop an interest
in becoming educated in software
engineering either I'd really rather
have that veterinarian studying medical
books in her off hours than reading
software engineering textbooks so here's
the setting that I'm going to be showing
you some of our prototypes in this is a
research prototype of a spreadsheet
system called forms three which is
something that my students and
colleagues and I have built over the
years is a vehicle for studying this all
I really want you to note here is that
there are these cells lying around and
they're not locked into a grid so you
can notice that you can notice that
they're allowed to have meaningful names
like Monday Tuesday Wednesday and you
can notice that you can actually see the
formula and the data at the same time so
that's what there is to notice there
we'll be using this this prototype as
our as our way of seeing some examples
okay so if we build it will they come
well the first thing we have to talk
about here is what are we talking about
building so here's a segment on a couple
of devices that we've built to support
the end-user software engineering
the first one is testing a form of
systematic testing aimed at end-user
programmers so first we have to think
about well alright so if we're working
if we're talking about end-user
programmers and spreadsheets then what
is testing well we can define a test as
a decision that they make that the
output is right given the input then if
if you're familiar with the notions of
systematic testing from software
engineering you've probably heard of the
term test adequacy criterion so it takes
an infinite number of tests to prove
that your program is correct so we can't
expect end-user to sit here that
so we have to give them we'd have to
figure out some way to know when they've
done enough and that's what test
adequacy criteria do so behind the
scenes we've we've come up with a test
adequacy criterion that our system uses
to reason about when enough testing has
been done so the the strategy behind
this this device that we've come up with
which we call WYSIWYG which stands for
what you see is what you test excellent
a for Allen the strategy is to
incremental update what the system knows
about tested Necedah this formal
criterion allow the user to
incrementally inform the system of
decisions and immediately reflect the
new tested 'no sin border colors so
here's a little scenario initially the
spreadsheet is untested and there are
these red borders and that indicates
that they're untested and there are
little check boxes there and little
question marks in those check boxes what
red means untested and a check box is a
way to record a decision that that value
actually is working out pretty well and
so let's say that we've got our user and
they have noticed that this value is
correct so they check it off the circle
I've drawn here is something I've
superimposed it's not part of the
screenshot I'm just showing you where
the check mark is all right so this
border turned purple and these other
ones turned blue okay and so what
happened here is they they liked that
value given the inputs blue means tested
so now we know that red means untested
and blue means tested and it turns out
that anything in between means something
in between so purple is between blue and
red and therefore it means partly tested
so that's what they've done but there
aren't any more little question marks so
that's really indicating to the user if
they happen to know this that they can
check other things off if they want to
but they're not actually going to be
able to increase tested miss anymore
unless they do something different and
so what they need to do different
actually as well we have this
if-then-else formula and so they really
need to exercise some other candy
so they're going to need to change this
input any of these inputs really but
let's just say they choose this one to
change turn it into a one okay so now we
have a new question mark and this is
indicating that if we choose to say that
that value is right we're going to be
able to make more testing progress from
a systematic perspective and let's say
that that value really is right so we
check it off and now everything's blue
all right so that's basically the way
with you it works from a user's
perspective there's been a bunch of
empirical studies regarding WYSIWYG and
basically what they tell us is it works
really well always in these studies was
it participants have higher test
coverage they tend to show a functional
understanding of what it is they're
accomplishing with these these
checkmarks and what the border colors
basically mean and they also have an
appropriate judgment of how tested the
spreadsheet is and that was the whole
motivation behind this because we could
imagine somebody working on a
spreadsheet that they really care about
like let's say their income tax
spreadsheet or something like that
and they might test and test and test
and think that in fact they've done a
great job of testing find some error
near the end of the testing fix it and
leave with that good warm feeling that
the whole thing's been really well
tested when in fact that last fix could
have had really far-reaching
consequences and they may need to do a
bunch more testing so it was that error
that we were trying to guard against and
and we've done a fairly reasonable job
of doing that
in some other ways we sometimes gets
results sometimes we find that this also
helps them to be faster and effect more
effective at debugging on some kinds of
problem some kinds of bugs that sort of
thing and we also find less
overconfidence about the correctness of
their spreadsheets which is good so it's
it's it's worked pretty well for us
still keep in mind in these studies we
always introduced people to WYSIWYG
right and the theme of this cloth talk
is how are we going to get them to even
pay attention to it there's one other
device I want to tell you about briefly
assertions for end-user programmers so
assertions are ways of saying
about what you expected the program it
adds checks and balances that can
continually guard the correctness rather
than you having to notice and test
things as you notice them so it can't be
accomplished via testing and the nice
thing that caused us to think assertions
might be a good idea is that they don't
have to be all or one you don't have to
commit to designing this whole thing
before you ever have anything executable
you can just add one or two and you
still make some progress and having more
safeguards in your spreadsheet and of
course you can add them incrementally as
the spreadsheet starts to develop so
here's what they look like
here's that same spreadsheet what we're
looking at a minute ago this thing here
is a user assertion they've typed it in
it's a range it says that this
spreadsheets allowed to be between 0 &amp;amp; 8
and you can tell the user typed it in
because here's a little user picture
this one down here is a system assertion
so the system has figured this out by
itself it said that it thinks that this
cell should always be between 0 and 48
and and they got that because they
entered enough assertions on the cells
this depends on where it was able to
propagate those things forward through
the formulas to figure out what was
implied of course we also have a user
assertion they happen to have also typed
this and we have a conflict between
these two that's what the red circle
means we also have a conflict in that
the data value doesn't doesn't up
fulfill both of these it's outside the
range of that top one so we have an
assertion conflict and a value of
violation and so what we hope is that
these red circles or perhaps just the
the system assertion not looking right
and somehow alert them to the fact
there's got to be an error here because
there could only be one of two things
wrong here
either the formula is wrong or there's a
wrong assertion okay and in fact when we
did this in a study exactly what one of
our users said was there's got to be
something wrong with the formula good
user that's exactly what the situation
was so so that's the way assertions work
all right then
so how can we get end-users interested
surprised explain reward that's the
strategy that we've developed to do this
let's not forget who it is we're talking
about people who aren't necessarily
interested in software per se they're
just trying to get something else done
so the usual computer scientists view is
if we build it they will come
okay but why should they
so our surprise explain reward strategy
has been long influenced by this model
of attention investment which actually
we have the creator of right here in the
audience that's Alan Blackwell's model
and it models the way people make
decisions about what kinds of features
in software to make use of how to spend
their time because let's face it you
know they have plenty of options for how
to spend their time so they're trying to
be as productive as they can so they
have to think about the cost of the new
feature so how many of you have ever
used to spreadsheet raise your hand okay
leave it up there if you've used pivot
tables okay we see only two hands now
what about the rest of you are you a
deficient what's wrong with you that
features been in Excel for years all
right so you probably haven't used that
feature because you don't use absolutely
everything why not
it's not a good use of your time to
figure out how to use absolutely every
feature so how do you decide which
features to try out well you think about
perhaps the cost of the new feature
according to the model of attention
investment that's exactly what you do
not only the cost of actually making use
of it on a day-to-day basis but the
initial cost of learning it and even
figuring out what it's supposed to do
you weigh that against the benefit of
the new feature how many of you know
what would be good about tip pivot
tables well hopefully okay so there's
one person who didn't claim to be a user
of them who knows what would be good
about them but the rest of you don't
know so how are you gonna make this
cost-benefit decision if you don't even
know what the benefits are right very
difficult and then furthermore there are
the risks there are the risks that the
benefits won't really pan out or even
worse that you put the system into a
worse state than it was in at the
beginning and now you don't know how to
get out of it so okay so this
this forms the basis of much of our own
set of design decisions in the surprise
explain reward strategy so how are we
going to interest them what our key was
was to try to arouse the curiosity so
psychology researchers tell us and
empirical studies of programmers agree
that users and users programmers alike
pretty much believe their programs work
things they create work this is what
they believe and therefore they have all
the information they require they don't
need to go exploring new features
research and curiosity also suggests
that showing them the presence of an
information gap something that they
don't actually know makes them curious
so for example suppose you're walking
down the street you think you know
everything that's required to know about
walking so you're probably not thinking
about walking first I put my foot down
then I put my other foot down you don't
do this right you're thinking about
something else but then suppose you trip
all right so this has revealed that
there's an information gap that in fact
you really don't know everything that
you need to know about walking at least
not down this street so what do you do
next
you probably look down so surprised
showed them the presence of an
information gap to arouse curiosity
according to this theory when the
presence of an information gap is
revealed users will examine the object
that introduced this information gap in
order to find out what's going on you
look down because that's where the gap
was you didn't pull down a help system
up here okay down at the feet where you
tripped now this is an example of
self-directed learning and self-directed
learning is a very important attribute
of our surprise explain reward strategy
so surprised its job is to deliver to
explain to get them to seek explanations
explains job is to deliver them to the
rewards to number one make make clear
what the rewards might be for doing what
we want them to do and number two tell
them what we want them to do then yeah
okay
so let's see here so there are two
settings for surprises one of them isn't
on this slide so I'll just mention it
briefly so if you just kind of scroll
back in your mind to one of those
pictures it forms three that I showed
you the spreadsheet environment there
were these red borders okay so if you
fire up your spreadsheet and you see red
borders around a few cells you might say
I wonder why those are there you know I
never fired up spreadsheets before that
had red borders so that's a surprise
okay and you might seek an explanation
eventually for why that's there but
that's not the example I'm going to tell
you about today
supposing you've gotten that far okay
you already know what the red borders
are and you know how to use WYSIWYG to
the extent that I showed you you know
with the checkboxes and the border
colors now what we've found
coming to the slide here is that
subjects find that easy at first and
accessible and they and they are into it
but eventually it gets hard because
eventually you've covered all the
situations there on your spreadsheet
with the data you've got and you're
going to have to conjure up different
data to try to test the rest of it
otherwise you won't be able to make
testing progress you won't be able to
turn the rest of the cells blue and it
becomes harder and harder and harder to
conjure up suitable values to be able to
cover all that and so if you've done
your own testing you know this to be
true and so eventually our users often
get stuck and we have this feature
called help me test that they can push
that will then conjure up new input
values for them to help them have an
opportunity to get more coverage and
we've tried we've tested this in the lab
before and people like it they like help
me test and they use it more than once
we've taught them how to do this and
they use it more than once so this is a
good opportunity for surprises and we
can take off from here to try to get
them interested in assertions so what
we've done is when they asked help me
test to give them some values they're
waiting for an answer they've started a
conversation okay and so they're
expectantly waiting for something good
and they get it they get the new values
but they get something more - they get
assertions so here's how it works when
they when they do it help me test the
system changes the values and cells and
darkens up the ones that's changed to
whatever it thinks is suitable and then
it also gives them this other thing that
they didn't even ask for these these
assertions that that the system has
guessed they've never seen them okay and
so what they may say is what the heck
are these this is an information gap I
didn't know about these before maybe
they'll just ignore them that's okay too
now these assertions actually are pretty
bad this one it was a payroll
application you may recall when you when
I had the spreadsheet up there and so
this was supposed to be the hours worked
on Monday and and the assertion is
guessing that between negative 1 &amp;amp; 2 0
would be a great great example a great
range for the number of hours people
worked on Monday which is patently
ridiculous and this one is guessing zero
to five so these are bad guesses and
this badness of the guesses may be an
important part of attracting their
interest and say what you know negative
one to zero what are they nuts there's a
second surprise that happens well help
me test is trying to conjure up suitable
test values remember this is what they
asked for
it's also thinking and it's thinking
about assertions as well as about these
possible test values and and so it's
kind of guessing kind of changing the
assertions as it's coming up with with
suitable test values and sometimes those
will Vout will violate the assertions
it's guessed and then you'll see the red
circles kind of coming and going as it's
thinking and keep in mind the user isn't
trying to do something else at that
point they've started a conversation
they're waiting for an answer even so
all of this feedback is passive the
system isn't going to do that if they
haven't asked it to do something for
them it's trying to win their attention
but it's not actually requiring it
empirically we found that users go
several minutes before they act on this
feedback and we're going to return to
this issue later finally the explanation
okay so we've talked about this prizes
surprises are supposed to deliver them
to explain so they've seen these
assertions and what they might do is
hover over it in order to get a tooltip
here's our tool tip a little longer than
your normal tool tip first it says the
computers testing caused it to wonder if
this would be a good guard so this this
is the semantics of what they're looking
at
it says fix the guard to protect against
bad values this is the reward okay as
well as what we want them to do which is
to fix it okay and we give them enough
information to succeed at that action by
typing a range or double-clicking okay
so that is the charge of all of our
explanations is to cover that much
ground and to be short the reason that
we chose to deliver explanations through
tooltips is because as I mentioned users
seek explanations from the object that
surprised them not somewhere else the
reason that we suggested in action is
that our explanation system is heavily
inspired by the work on minimalist
learning that Jack Carrol and Mary Beth
Ross and have done over the years and
many of the principles that they put
forth are that a way to get people to
learn stuff is to get them to take
action and do it and that furthermore
users are very active and very engaged
in the tasks they're trying to do and
that using that as a setting to learn
about things is often the the way they
want to proceed and of course why we're
talking about the reward is comes back
to the attention investment model that
without making the rewards clear of
going down a certain path then people
have no way to make informed judgments
about how to weigh the costs and the
benefits if they don't even know what
the benefits are okay so what are these
rewards well if we're lucky a red circle
around a value will indicate either bugs
or incorrect assertions these these
rewards are good in a short term
perspective when they're first learning
oh good I got a red circle you know I
can hover over that and it looks like
I'm doing something right and then it's
also a good reward in the long term
because this is your way of the system
automatically spotting errors for you
help me test also behaves better when
this happens it's not necessarily
obvious to the user but help me test
once we have these assertions up there
it tries to honor them for the input so
that it picks better inputs and it also
tries to break them for the intermediate
ones so then it's actively trying to
find your bugs for you
trying to find contradictions also the
computer generated assertions might look
wrong that was an example I showed you
earlier this this thing 0 to 48 didn't
look right for the total number of hours
for a week should have been 0 to 40 like
what they said red circles around
conflicting assertions are of course for
surprises and then become rewards they
found the errors we did a study this was
a bunch of business majors at our
university who were familiar with
spreadsheets they had no assertions
experience we we did not give them any
experience with assertions during our
setup of the tutorial in fact we never
mentioned them at all so all they had
was surprise explained reward and what
we wondered is whether that would
succeed that enticing them to use
assertions and we found that surprises
of the sort I've described to you did
get their attention
eventually 94% of our users did use
assertions at least once the average
time before they started using
assertions in task one was 13 minutes
but once they discovered assertions they
got religion and Hort 87% of them kept
using them the mean was 18 assertions
per user and by the time they got to
task 2 the time to start using
assertions was 4 minutes in and we were
counterbalancing what was task 1 and
task 2 so yeah at first they did
interest assertions using help me test
so help me test would would have these
guest assertions and they would edit
them but eventually they would just take
initiative and do it themselves so they
just enter them on in so how many tests
trained them but then was not really
Ness
Fairey for them to continue using them
the rewards seem to be sufficient
because they kept using them once they
used them once they used them again
in fact over half of them actually used
them within the first minute of task 2
and they said things like this
I found them reassuring because I think
they help with accuracy this was in a
post session interview actually
questionnaire and the other one noticed
that that reward that wasn't so obvious
they said well I used them to try to get
the help feature to work better to stay
within my limits okay so we could be
drilling down into surprise or explain
or reward or all three of them but I
kind of suspected that you know 75
slides or whatever wasn't going to fit
in this period of time so I picked the
one that I'm the most into right now
which is explained so we're going to dig
down into explain and see how far we get
and after we do explain we'll see if we
still have time to drill down into
surprise some more so one of the things
that happens with surprise explain
reward is we thought wow this is great
you know this works wonderfully and and
we were using it on lots of users trying
to study some other things about them
and and we found that sometimes our
users continued to get muddled and
confused and so we asked at some point
you know well didn't the tooltips help
you and they said well yeah they were
helpful but they just didn't tell me
what I wanted to know and so we said duh
wouldn't it be nice if we knew what they
wanted to know so we did a study to try
to find out what end-users want to know
when debugging and so our experiment
with a pair think-aloud which worked out
really well we recruited pairs who did
not have any computer science experience
who knew each other from the beginning
so we didn't have to worry about a
warmup period and that sort of thing and
we gave them pretty much nothing except
each other so the instructor gave them
some very basic instruction about just
how you cope with forms 3 we gave him a
couple of spreadsheets that had bugs
there were no tooltips almost no
instruction the instructor was not even
available to them the
went into another room and said you know
if you have any problems you can contact
me over I am okay so yeah they were
available but that's kind of high cost
to start having to type in stuff so
people didn't use it very much which is
what we wanted so they had only each
other and then what we did was a content
analysis of their words to see what they
wanted to know so up here in the corner
is a little graph with five lines and
these lines you don't have to actually
read it's just a way of walking through
the next part of the presentation we're
going to talk about five kinds of
information gaps and right the first one
Oracle specification was the most
prevalent and so we're going to work our
way down so this is about whether the
values they're looking at or right or
whether the formulas they're looking at
or right right so the Oracle problem is
whether you you know you can tell
whether your output is right and
specifications are specifications so
here's an example when subjects saying
divided by 10 I don't know I guess it
should be times 10 talking about the
specification these gaps were not
centered on features okay
40% of the gaps were of this nature the
users were focused on the tasks not the
environment and its features so its
implications are first of all we need
more research into supporting this kind
of information gap because you look at
it you say well yeah of course they're
talking about the task and yet when you
think about what problem-solving is out
there very little of the help that's
available especially within the system
is about this okay so so that's one of
the implications
it's another implication is it's very
consistent with what Jack Carroll and
Mary Ross Mary Beth Ross and have always
been saying about the active user the
user that wants to stay engaged in their
task not go off and learn something else
so that was the most prevalent forty
percent the next most prevalent thirty
percent was about strategy what should
we do now was one example another
example was let's type it in see what
happens okay so they are and much of
what they were doing 22 percent of that
30 percent or
let's put it another way roughly
two-thirds of that we're strategy
hypotheses like that second one
proposing a a strategy to follow note
that these two are not about features
okay so many of these were global in
nature with and there's no real feature
or object even to tie that kind of help
to the third most prevalent was features
but it was only 16% of the gaps that
were expressed by these people remember
we gave them no instruction okay that
only each other and yet most of the time
what they weren't talking about with
features but here's one where they were
so with the border this purple mean it's
straight up right and blue means it's
not right okay question about the
features there now this type of
information can work well with the
tooltips and this is the kind of thing
that if we had the tooltips in there
would have probably done a pretty good
job at answering that but as I said it
only accounted for 16% of the gaps an
implication then is that focusing on
this type of information would address
only a small fraction of what our
participants wanted to know yes
okay so so the question for those of you
who didn't hear it was that this
particular question is kind of focused
on the way our tool works and the
question was did we also look at how
other tools worked as part of this in
that setting there there was there were
no other tools there were no pivot
tables or whatever but what we have
found is similar findings to this in a
study we did in Excel so the percentages
will be different though depending on
how complex the feature set in the
environment are but but the really
important takeaway is we can be saying
this is a big fraction we can be saying
it's a small fraction but we can still
tell it's a fraction and there are all
these other things that people are
talking about too a very interesting one
with self-judgement almost close to ten
percent it was nine percent of the
information gaps they were talking about
themselves not about the system not
about the tasks but about themselves
they were judging themselves they were
saying things like I'm not even sure if
we're qualified we to do this problem
right talking about us so these these
are instances of metacognition where
they're thinking about their thinking
and metacognition has been shown to be a
very important factor in learning and so
if what we're trying to do is get them
to learn how to use these features and
engage and in end-user software
engineering stuff then this matters it
also ties to self-efficacy which is a
form of self confidence and there's a
long background in self-efficacy theory
that that shows that it often can be a
predictor of success and so if there is
something that we can do to make their
ability to self judge more accurate we
can potentially tie what we can
potentially improve their debugging
persistence for example because this is
what self-efficacy theory suggests so
this was 9% of the information gaps so
improving the accuracy of self judgments
in terms of how well they're doing may
in turn increase debugging effectiveness
the implication of this one finally they
were the big gaps here's an example whoa
it's what one subject said and another
subject said hope in just about that
tone of voice so we called these big
sometimes the users aren't even able to
voice a specific question five percent
of the total we're like this and one of
the implications we found when we really
analyzed how these fell out along the
time line was that sometimes the timing
of these things you can tell what kind
they are so if they've just gotten a
load of new information and then they
just hit a Help button okay that might
be the moment for the system to guess
you know what I think this is a big
information gap and then hopefully do
something appropriate maybe even figure
out what the cause of confusion is based
on what kind of feedback just just
materialized okay so here are the
implications sent and opportunities
first of all information gaps should not
primarily focus explanations on features
the users greatest need in this study it
was about Oracle and specification
problems strategy issues in this study
outnumbered features two to one and
needs both global and local support and
the accuracy of user self judgments may
impact effectiveness so we've started
working on this now what I'm going to
tell you now is very immature emerging
research so you know it's not all that
far along and we welcome comments so
what we did is we drew from the above
results we drew from various education
theories we drew heavily from
self-efficacy theory and from
Schneiderman and Becker's research into
how to do video demonstrations and so
what we came up with was a trio of
explanation devices so we still have the
tooltips which are focused on the
features and feedback and rewards but in
addition they have links to strategy
videos videos whose whole purpose for
being well really have two purposes for
being
one of them is to support strategy and
the other is to support self judgments
they're not trying to be anything else
they're not trying to be about features
because we have the tool tips for that
they're just trying to be about those
two things there is also a textual
version of those videos in sort of
shorter form let me show you what those
look like I forgot to tell you about
that bit right so here's here's the tool
tip 60% of the cell has been tested so
it still needs more testing and then
there's a show me button that leads to
the video explanation of strategy if
they want to know more that has to do
with testing hmm wonder what that would
be good for or whatever here's the text
version so this would be leading to the
video on testing instead what they could
do is how could select how can I test my
spreadsheet and then they could either
hit show me or tell me depending on
whether they want a video or text that's
a side panel that they can do here's
what the prototype looks like right now
here in this corner these are two of my
students neeraja and Jared and they are
talking to each other about how to well
I don't remember which one this is maybe
it's test this spreadsheet and so
they're working together through the
problem these these videos there are
several of them each one is somewhere
between one and two minutes and so it's
very focused on exactly you know
whatever strategy they're trying to
impart import there are in part there
there a couple of things that are going
on here the headshot okay so you might
wonder well what's that therefore that's
there for the self judgment thing so one
of the things that self efficacy Theory
tells us is that people who see someone
they can identify with succeeding at a
problem may have their own self judgment
increase if they say oh yeah I could do
that I see that person doing that I
could do that so that's why they're
there and then of course the spreadsheet
is there so you can actually see what's
happening here's the textual version so
what happened this the the videos were
scripted
this takes word-for-word off the scripts
okay but of course what the textual
version does not have is the example so
it's shorter when it does have is
hyperlinks so you can you have sort of a
random access thing you can get around
from one to the other quite readily
so it has advantages and disadvantages
but but you can see that roughly the
content is the same so we just finished
doing an empirical study we haven't even
managed to get it out yet so here's just
a little sneak preview of what we found
this this approach closed roughly half
of the new gaps in our new study of
these three types strategy which the
videos were after
Oracle specification which the videos
weren't after but yet it seems that the
help with the strategy may well have
helped with the task performance as well
which is you know a very nice outcome
and self-judgment interestingly the
video versus the text form met different
sorts of enthusiasms it depending on
what the users were doing so when they
were first learning a task when they
first decided they needed to know
something about how should we test they
were slightly more likely to go with the
text form now I can explain this from an
attention investment standpoint if I
want to because it seems shorter but in
fact you know different it wasn't an
overwhelming difference text versus
video so a lot of it depends on kind of
what their own judgement was about how
much information they wanted what kind
of learners they are that sort of thing
then when they went back to an
explanation of the same topic oftentimes
they would switch to video and so that
was slightly more prevalent and so you
can imagine okay you know they saw the
thing one time and now they're realizing
you know gee this is kind of tough and
the video has more content to it because
you do have that example to look at at
the same time so that could be an
explanation for that on the other hand
other times you just want to go back for
a quick refresh and then the text would
better because it's more of a sort of
direct access kind of thing and in fact
in the interview later they were talking
a lot about making decisions based on
how long it would take so so this
attention investment thing definitely
was coming coming through again also the
males and the females seem to respond
very differently to the videos for
example remember that that picture of
Jared and Neary Shaw we've seen this in
several of our pilots as well as this
this final study we've seen you can tell
actually what they're looking at you can
tell by their head position we videoed
the subjects you don't really need eye
tracking for this and the females seem
to look a lot at the heads the talking
heads back and forth between the topping
and talking heads in the spreadsheet the
males seem to mostly look at the
spreadsheet you know every once in a
while somebody one of the two
instructors will do something kind of
strange and you'll see their head go
oh but mostly what they're doing is
looking at the spreadsheet the females
smile and nod at the speakers in the
video they're they're engaged with them
they're communicating with them
especially near a jaw you know the the
females are nodding and smiling when she
overcomes some obstacle and they're just
into it the males well some of them some
of them don't really want you to notice
they even need videos we saw one one of
our subjects it was the funniest darn
thing we talked to him about it later he
was actually it was a pilot subject so
he was an intern with us for the summer
we you know convinced him to watch one
of these things and he said I don't need
to watch him you know I don't I'm not
really a video kind of person you know
you got to watch it you know alright
fine and he sits in his chair as far
away from the darn thing as he can get
he leans back you know did city didn't
distancing himself and then finally you
know later we told him he did that I did
not well yeah he did and there was
another one where you know it's like you
know how nobody sees me watching this I
mean it was really interesting so the
bottom line here is having both forms
really really mattered okay we don't
have time to talk about a closer look at
surprise
unfortunately so I think what I'm going
to try to do is just skip to my
conclusionary slide Oh for heaven sakes
that looks good
conclusion no it's not the run line yeah
this is the right one
no it's the wrong one never mind let's
see one more time okay sit here
it's supposed to be ha well alright this
is not the right conclusion and I can't
seem to get to the right place
so we'll just stop here do we have
questions I've got one that I hadn't
noticed before and I might be confused
but I don't think so so I'm gonna push a
little more on that previous point and
that is this distinction between
learning about using your tool and the
distinction about learning whether the
spreadsheet is correct or not and I had
the feeling that when you're doing
surprise explained reward you kind of
confuse those two at certain points and
sometimes you were talking about the
learning that they were trying to get
hmm was learning how to use a tool and
the other time the learning was learning
whether they should trust their
spreadsheet or not whether this
particular spreadsheet was working okay
so yeah that's a great question
so let's see here the long the short
answer is that's deliberate what
happened at the beginning is at first we
really thought that it was all about
explaining the tool and that tended to
put blinders on us about what things
were really possible from an extract
from from our strategy our explanations
because if all we aspire to do is teach
them how to you know think with this
feature of that feature we're not really
accomplishing the goal which is to
figure out how to genuinely engage them
in software engineering behavior as
end-user programmers and that's really
what we're trying to do and so at first
when we were doing our content analysis
of what they said at first the Oracle
specification stuff we kind of pushed
over in a corner and say oh that doesn't
count because the help system can't
really help with that and we said hold
on a minute
you know we're thinking
inside the box here you know we don't
really care what it's possible for us to
implement but we care about is what do
they want to know and once we really
started thinking about that we started
to realize the possibilities that an
explanation could system could really do
to have as its goal engagement rather
than feature set so ultimately I
wouldn't call it confusing or I wouldn't
I wouldn't say that we confused it I'd
say we included it because we felt that
it was the more worthy goal yeah more
questions
okay well</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>