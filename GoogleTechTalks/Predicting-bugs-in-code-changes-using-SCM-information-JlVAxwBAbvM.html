<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Predicting bugs in code changes using SCM information | Coder Coacher - Coaching Coders</title><meta content="Predicting bugs in code changes using SCM information - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Predicting bugs in code changes using SCM information</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JlVAxwBAbvM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to introduce Jim whitehead he's
a professor at UC Santa Cruz and his
student soon kim um they've been doing
some really interesting work on looking
at software evolution and data from
source code repository so I asked him to
come get this talk or he kind of
volunteered when he found out I had come
to Google so thanks a lot Jim look bored
right yeah thanks Sean yeah John you
know I guess when you started working
here he's like oh you know and if any of
you are ever in the area and I'm like
well I'm in santa cruz i'm always in the
area you know so I said come on down so
I'm going to be talking about some
recent work we've been doing in our lab
at UC Santa Cruz on predicting bugs and
the work is done with my students on Kim
there's also another professor you Jang
at UC Santa Cruz and then my student
Jennifer Bevan as well and then Thomas
Zimmerman and andreas seller are at
Saarland University in Germany and
they've collaborated with us on that as
well so the kind of the idea here is
that if we can you know if we know where
the bugs are you know before they occur
that you can can take some steps to
improve the quality of your code so if
you know that a particular set of files
is much more likely to have bugs and
some other set of files you can spend
more of your QA time on those particular
sets of files so you can do things like
do more software inspections on those
files or perhaps you can run static
analysis tools on those files static
analysis tools often return a large
number of false positives and so they
you know generally you don't want to run
them across your entire code but if you
know that there's a high likelihood that
you're going to find bugs in some
section of your code it might be worth
your while to spend that additional time
so in general there's this know kind of
realm of techniques that are you know
very time expensive that you can use to
improve the quality of your code but you
generally don't want to use them because
you have fixed to a resources and you
don't really have a good sense of how to
prioritize the application of those QA
resources across your code but if you
did have really high quality data that
said you know I know where the coat and
where the bugs are in our code then you
could start doing some of these more
expensive techniques
okay so the other you know kind of work
that we do is that we want to improve
developer awareness that they maybe they
just created a bog and so most of the
time a developer injects an error into
their software and then it may be you
know weeks months you know sometime
later that that error actually manifests
itself and some external visible
behavior and then after that point in
time you're able to go and make a fix to
that and so there isn't a very strong
immediate feedback loop back to the
developer that there may be a bug if you
could actually provide immediate
feedback to the developer that there's a
high likelihood that they've just
created a bug than the developer right
then when they have all that context in
their head could maybe go and you know
re review their code maybe they could
get a colleague to do an inspection and
so on so that so you could close that
loop a lot faster oops so we have two
main results we're going to talk about
in this presentation here and so one is
this bug cash and then the other one is
on predicting buggy changes and so with
the bug cash what we do is we you know
take the software and we put it into two
bins and so typically we make the one
bin the the bin of buggy software we set
that to ten percent of the total size of
the code and we run our algorithm and at
the end of running our algorithm over
the in revision history of the software
that cash that ten percent will contain
you know 73 to ninety-five percent of
future bugs and the software so so using
this blood cash algorithm we're able to
with very very good results tell you
where the bugs are likely to occur in
your software in the future and what's
nice is now you have that ten percent
you can focus your effort on that ten
percent the other work that we do is we
have been using machine learning
techniques to analyze the the changes
the individual commits and so we're able
to we train on the entire revision
history after we have have our trained
machine learning algorithms you know we
can given an individual file level
change so that's about you know on
average 20 lines that have changed
an individual file with about 75 percent
accuracy we can tell you whether that's
going to have a bug in the future or not
okay thanks doc mass coming to grab our
if two things don't so which is which
things there's a miscommunication back
between two parts of a system where is
it right yes so we know so what we are
going by in terms of bugs are you know
we've mined the histories and so
typically we're looking for in the
changelog of a commit to an SCM system
there's two approaches you know if
there's a high-quality changelog we look
for the actual identifier of the bug
report and the bug tracking system and
so the developer will say you know just
committed a fix to bug number whatever
right and so then we know that that
particular commit contains a bug fix
oftentimes the commit logs are not that
high quality and so we will then just
look for keywords like fix repair etc
and so there's a small number of
keywords that usually indicate that
there is a a bug fixed there and so
we've done you know some work in the
past where we've also gone back and
manually examined these things to see
okay are we you know is this getting
things that really are bugs or really
aren't bugs and it's you know the
techniques work pretty well and they're
not perfect but you'll barring actually
going through you know several thousand
revisions of a software project and
manually you know inspecting each one to
say whether that's a bug or not this
technique is automatable and gets us
very good results for identifying what's
a bug and what's not a bug
okay so so in some respects so you know
what we're trying to do is take
advantage of the fact that use of
software configuration management
systems these days is ubiquitous all
projects of any consequence use sem
systems so you build up this history of
changes and the idea is that we can can
learn from those changes and especially
as people are fixing bugs they note in
there change logs that they have made
these fixes and so this gives us a
labeled data set and the data set is a
number of changes and they're labeled
with respect to whether they are a bug
or not a bug and so from a machine
learning standpoint you know we have the
Holy Grail we have a large data set
that's labeled already for us and so
this gives us the ability to learn from
this data set quite easily so there's a
couple a couple insights so the bottom
one here software can be classified and
this is classified in the machine
learning sense so you can do machine
learning classification algorithms and
you're essentially classifying a change
with respect to either it's a bug or
it's not a bug so you're classifying it
into one of those two bins and so that's
one of the key insights is that you can
actually view bug prediction as a
machine learning classification problem
and then you can bring all that machine
learning machinery to the table now the
other insight here is that bugs exhibit
some locality that they're not just
uniformly distributed over over the code
and over time that there are you know
certain files are much much more likely
to have bugs and then there's certain
periods of time you know that bugs are
likely to be clumped together in time
and not evenly distributed over time and
so you can take advantage of these
localities to improve your bug
prediction accuracy okay so for the
first part of the talk i'm going to talk
about this bug caching work and then i'm
going to turn over to some for the bug
prediction worth of bug classification
work at the end okay so mentioned this a
bit and answering your question but so
what we have here is we have this
timeline of changes and so this is the
the evolution of a particular file over
time these tick marks
and commits a particular commits a
particular points in time to that
software and so what you know what you
have is you know initially there's some
software change that know where the
developer makes a change and injects a
bug at some later point in time you have
the the observation of this as some
problem that needs to be fixed so the
you have an entry in your bug tracking
system and then at a later point after
that you actually have the the fix that
was made and it was injected and so
where we start is we typically start by
looking at these log messages sometimes
we start here but we're most typically
starting with these sem commit messages
and then we have to work backwards to
find find the actual fixes and so turns
out there's a you know an algorithm that
some of our collaborators developed
Thomas Zimmerman and andreas eller which
gives you the ability to using the SCM
annotate feature to work backwards to
identify you know which particular
revision actually had the initial error
in it turns out that the you know there
are some some issues with that approach
and so we've done some reason work where
we've improved the the algorithm that we
used here but you know in general with
no pretty high accuracy were able to
work back to what the original error
producing change was and so what's nice
is that you know most of the work on bug
prediction no kind of does under the
lamppost kind of work you know it's easy
very easy to get this bug fix data but
it's you know until recently has been
very very challenging to reliably get
back to the bug inducing to the the
error injecting change here and so one
of the kind of things that we've been
able to take advantage of is as recent
advance that allows us to get back to
that sure
to the source code management system is
going to say tell you lifeline this
slide myself right well yeah so there's
a number number of problems so one of
the problems is that you know you can
many lines could have been changed here
including changes that involve just
cosmetic changes or formatting changes
and so that can throw you off so if
you're like well okay you know the
majority of the lines that were changed
we're in revision for but the majority
of the lines were just formatting
changes then that's going to throw you
off the other thing that throws you off
is that the you know there is no good
way to map a specific line back in time
so the sem in a cage you know does its
best but but it can get confused and in
particular you know you can get thrown
off by the fact that no one challenges
to try and go from a particular line to
the function or method that contains it
the SEMA annotate can't tell you for
example that you know that the function
may have changed its name between when
the bug was introduced and when it came
later Oh something that I do a good job
of answering that or and I'd so we
that's what I can say is that you know i
can go go into that question a little
bit more in depth with you after the
fact we sort of just have written a
paper paper on that kind of going into a
number of the issues no I guess sort of
you know you know in some respects once
you see the algorithm that we use to get
back here it's like what's you know
relatively straightforward to do that
but people haven't been doing that and
there are a number of little glow
gotchas along the way sure
right yeah so that would throw us off as
well and we don't have a good a good
answer to that so there's some you know
there's some class of changes where
there might be an environmental change
and so the environmental change is what
led to to this change here and so so
yeah we will then you'll kind of go back
and say well okay no the the initial
change you know that kind of led to that
bug yeah we're just going to track it
back in that individual file now we
still feel that that's useful and we
feel it's useful because even in that
situation if there are many other files
that you know would would have needed to
be changed due to the same kind of
environmental change or if that kind of
environmental change is occurring
frequently we're still going to pick
that up and so we may be you know seeing
that the the you know the fix inducing
change you know we may get that wrong
but in some respects our bug classic
classification algorithm is still going
to give good results sir is officially
noxious did you get a pious but people
don't understand it at all thing and
then fixing bugs and then possibly know
what right oh right repeat okay okay so
in a large system you may have you know
a in effect where people are fixing only
the code that they know and not fixing
other parts of the code fixing bugs
nothing the source but passing around
into the code they know best we write or
they'll be you know kind of patching
around and sort of avoiding some area
the code that they don't like so there's
in some respect were not know so we
haven't really seen a strong effect like
that and in the projects that we've
looked at and we thoroughly know that
there are these know areas of the code
that are kind of you know magic areas of
the code or you know don't touch areas
of the code that you are kind of magical
and you know people get into trouble if
they they've changed them you know and
so our algorithm would get would get
thrown off and that you know we wouldn't
see any changes in those magic sections
and we wouldn't flag them as being
particularly buggy and so yeah we would
get thrown off in that case yeah I want
to move on okay so we go through this
revision history and what we're looking
for our certain kinds of bug localities
so we we have four main kinds of bug
locality so temporal and so the idea
here is 0 so first of all let me use in
terminology here so we use this term
entity we had this problem and that we
want to have a term that talks about
either a function or a method so we use
the term entity here to mean you know
function / method so you know if a
function / method introduced a bug
recently it will introduce another bug
soon so this sense that you know you
know if there's a problem in a
particular module we probably you're not
going to fix all those problems right
away that is going to continue to be to
be problematic this notion of changing
you know if it's if you've changed it
recently no we're finding across all
projects that you know almost invariably
some some percentage of all changes are
going to be buggy we've seen this as
high as 70 80 vault percent of all
changes in bugzilla seem to be buggy so
you know almost every time they touch
the code they're adding another bug
those projects it's around ten to twenty
percent so so if you just if you make a
change at all that immediately
makes it suspect if you've added a new
entity we find that suspicious and then
if you've introduced a bug nearby in the
sense that it was you in the same sem
commit or in the sense that was
logically coupled that it will also tend
to introduce new bugs soon and so the
idea here is that you know if a
developer has some mismatch in their
mental model and they're making a change
then that change will you know that
problem in their mental model will cause
them to introduce errors in a number of
different places not just one so what we
found is that this temporal and spatial
locality contribute the most about bug
prediction ok so the bug hash oh sorry
question right so we we didn't use a
developer locality in this particular
one developer actually does end up being
one of the factors in the bug
classification work and what we find is
you know for some projects it's a good
predictor most the time it's actually
not the strongest predictor and I think
the reason for that is in the
classification work is that you know a
given developer is both committing a lot
of good changes and buggy changes and so
the fact that the developer touched a
particular piece of code doesn't mean
that that's a particularly good
predictor so whether that's going to be
bad code a good code it's essentially
even you know it factly the developer
touched the code means well it might be
good it might be bad and so but that
maybe it's just a quality of those
machine learning algorithms that it you
know it doesn't try to make a model of
you know developer bugginess in
sometimes so so we use the bug
localities to take our software so we
you know go through all the software and
we can use this at either the file level
or the entity level so typically we set
this at about either 10% of files or ten
percent of functions the bug cash
operates where we have an initialization
step so we essentially take our cash we
pre load it with a certain number of of
entities or files or entities
we perform an analysis where we work
through the revision history you know
adding some things into the cache taking
some things out at the end of the day we
have our bug cash filled with the the
functions of the files that we predict
to be the most fault prone going into
the future and so and you know again
this shows that you can do it at
multiple levels of granularity at the
file level we tend to do best but it
also works at the function or method
level and there's a trade-off between
kind of the smaller you make your
granularity you know the harder it is to
do that prediction but on the other hand
you don't have to look at it at as much
code to figure out what the bugs are
okay so in the initialization step we
start off by extracting the bug fixes so
we go through the revision history and
we get back to those we get the bug
fixes we then attract these fix inducing
changes by working backwards and then we
do this preload step in the bug cash and
right now we just do a simple yo the
files or entities with the largest
number of lines of code we say well if
it has a lot of lines of code in it then
it's immediately suspicious and it's
surprisingly effective so even after
running through our entire analysis for
the Apache 13 project one of the ones we
looked at it still you know you know
that lines of code ended up predicting
eighteen percent of the ones that ended
up in the cash at the end of the day
okay now during the analysis stage we
process all the revisions of the project
and so we compute hits hits and misses
here so that we observe where the the
fixing dusing changes are to give in
revision if a bug occurs in a file or
entity that's currently in the cache we
call that a hit otherwise it's miss and
so in some respects a cache hit can be
viewed as a successful prediction of the
bug so we had it in our cache a bug
occurred and so we call that a
successful prediction for Miss this is
where we change the state of our cash so
we then fetch the file and this is our
temporal locality and then we grab all
the nearby entities and put those into
the cache as well and then we also do
kind of a prefetch and so you know we
also
add in any new entities that have been
created or modified at that time and
then of course since our cache is fixed
size we need to have a removal strategy
and we've actually explored a couple
different removal strategies for taking
things out of the cash okay so here's
what our results look like so we've
analyzed a number of substantial open
source projects across several different
languages and so the projects are Apache
13 subversion postgresql Mozilla jaded
Columba and Eclipse and what we find
here is that now our best results are
with the Mozilla project and with the
Eclipse project and they actually get up
into the 90s is not at all uncommon for
us to be in the 80s and then subversion
is our worst one in the low 70s so what
you can see is that you Lisa crossed a
set of open-source projects we end up
being able to at the file level tell you
where you know which are the most buggy
files and so this kind of sorting it
into you know the most bug prone and
that the knot is likely to be bug prone
this works surprisingly well most
complicated
well in some respects the the you know
so software complexity is has
traditionally been very hard to measure
turns out lines of code is about the
best you know complexity measure out
there and so in some respects our
prefetch is already loading that the
most complex files are most complex
entities in and it does pretty well I
know for Apache 13 it you know is about
eighteen percent at the end you know
that that most complex you know you know
the most complex ones ended up being the
most bug you the other thing I'll you
all stress about this is that you know
this this algorithm can be run in a
cabinet in an adaptive sense or in an
online sense and so you know as your
project is continuing to evolve you know
you can do your initial analysis you can
get your your sorting and then you can
rerun this weekly or you can rerun it
monthly and you know you can be getting
this updated list of what is predicted
to be the most bug prone files at that
time that you run your analysis and so
it it will change and it will adapt to
changing conditions over time okay so
these are results of the file level so
when we do looks the same project but at
the function or method level you know
you know as we stated the so first of
all this one ends at eight not at one
and so it's you know the results are
lower so the worst is Mozilla here which
is just under fifty percent but we can
you'll get up into the high 60s and you
know hovering around 60 is not you know
not uncommon and so so again there's
this trade-off between well can we know
that the finer would get our predictive
granularity that the harder it is to do
because there's you know it's a smaller
smaller target we're aiming for
they're right I know some do you have
any insight on that one or I know
sometimes the stuff we get back is just
weird the the weirdest stuff I think of
the predictors are the features and the
the bug classification stuff coming up
next but yeah we don't have don't have a
good explanation for this one question
so in this work I don't think we took
that into account no so we have done
some work in the past on doing automated
tracking of file new function function
names across renames so we call this the
origin origin mapping or entity mapping
problem it turns out with pretty high
accuracy you can actually figure out how
these things change across across
revisions but it's a pretty time you
know time consuming process and so we
didn't and run it on this one but it
could potentially be done and and sung
has a suspicion that if we if we did run
this that that would improve this a
great deal that there's some sense that
you know we kind of lose a lot of
history and we lose a lot of information
when a functionary gets renamed and so
if we could kind of hold on to that that
that might improve our results a bit so
he tried a couple different cache
replacement strategies least recently
used so the the least recently found bug
we unload we change count weighted so we
unload the file or entity with the least
number of total changes bug count
weighted so the you know the file or
entity with the fewest number of total
bugs we take out and so then the you
know the question is you know which one
of these works best and the answer is
well it kind of depends so so here we
have bars for the least recently used
for the bug and for the change and and
we have that at the file level and then
also at the function or method level and
and what we see is that at the file
level you know pretty much the LRU is
the best and there's one or two cases
where it's not but you know in those
cases it's not that far off but then
down here
it's kind of a mixed bag on the bug you
know bug based algorithm seems to be
best here and then LRU doesn't work best
there so it depends on granularity and
it also seems to depend on the project
so you know one of the kind of take away
lessons we grab from this is that if
you're going to do this for real there
would be some process where you would
you know essentially train the the bug
cash for your particular project and you
would learn for your project what are
the parameters that give you the best
results for your specific project and
you might even want to go through and do
a retuning effort you know every month
or every six months or something like
that okay so one question is well what
can you do with this so we'd mentioned
you know doing inspections or static
analysis where you can also end up
giving developers awareness of this so
you know there's a sense that if you're
working on some code that's in the bug
cash you know that bug cash code is
riskier in some sense and so maybe you
want to tell your developer that you're
working on a risky function or method
you're working on a risky file so we've
done some work or done integration into
Eclipse and you know you'll get some
some icons indicating that you're
working in in riskier parts of your code
sure
so therefore if you got a piece of hook
is being touched all the time by lots of
people and has five bucks maybe it's a
blessing from that something's one
question once it only has a month yes
and then we don't do that and are no
defense of that practice is you know if
a particular file is being touched a lot
of times and it's its absolute number of
bugs is still quite high that means
you're still spending a lot of money to
be fixing errors in that file and you're
still spending time there time is money
and so you know you would want to know
that that's one of your problematic
files and normalizing that away and
saying oh well you know even though
we're spending you know ten percent of
our time on it you know you know
relatively is not you know not one of
the problematic files I know my feeling
is that's not giving you an accurate
measurement or an accurate feel okay so
at this point I'm going to turn it over
to song and he's going to talk about the
bug classification work so the previous
podcast work we can identify the file or
a function based we can identify buggy
function or volga files but if you think
about all the development process most
development processes change base you
change something to add new features or
you change something to fix something
like that so we want to try to identify
each change as a buggy or number but if
you can identify or classify the changes
it's a very nice because one thing is
the prediction is pretty much localized
we looking at only 20 lines every G
changes about 20 lines of the first code
so we identify a change as buggy we have
to look at only 20 lines of the source
code also the prediction is just
immediate maybe else as soon as you
submit a code to the same systems you
automatically say your source code
contains code Oh buggy code or not so
it's much easier to remove the potential
box
so to do that we use the machine
learning algorithms and we basically use
two algorithms like vases and support
vector machine which is implemented in
da car through kid and then we analyze
the 13 open source projects basic idea
of the machine learning is you can take
bunch of futures with a label to set and
then it can train a classifier and then
later we can feed on unknown set over
the futures and then version learning
algorithms can classify the instance is
buggy or Lamborghini this is the basic
idea of the machine learning and for the
futures are you think about the changes
so we have basically two piles so all
the piles and new files and then fit in
the files we can compute the health as
well has been changed these two files
also the file name and director named
can be one of the futures and we also
compute the complexity metrics over c
and java source code and then we compare
the complex matrix of the new file and
all the file and they used all complex
matrix as a futures also all keywords
you typed in in the sm logs what you
have between change it we used to all
words in the same change logs at the
futures finally we use that some change
metadata such as authors food changes or
when you commit the source code that
kind of information we use every
possible information fitting changes as
a future and this is the list of the
project we analyzed including Apache and
Eclipse sir version and so on for
example if you look at the Apache
project it's written in C language we
look at the revision from 500 to 1000 it
contains 579 clean changes and 121 ok
changes and in the future number we use
this from 11-1
is almost 1,100 features if you look at
the TCC project we used almost 2,700
features for the classification and this
is the results from the classification
overall projects basically we have about
seventy percent of the accuracy with
above 40 to 50 recall the next occasion
we want to we want to know about is that
we have we have used a bunch of
different futures from different group
for example we get all keywords in
editor Delta or deleted data location is
VG future groups are more most
significant for the predicting box so we
try to combine different features from
different group and then try to classify
again and see what kind of future has a
very strong predictive power so this is
the results from the modular accuracy by
future combination from each alphabet
indicates the group of the pictures the
tilde means we used every picture but
that future so for example here the tilt
a means we used every future except the
editor Delta and so on if you look at
this graph mostly they are good if we
use all the futures and it's really bad
if you use only new source code futures
the Eclipse we have very similar shape
of the results here we look at the
although a Bridget accuracy by pooja
combinations what you can find is maybe
using all feature is not the best
the best is using all features but
accept the complexity metrics is the
best of course this is a bridges so you
know each day each project maybe have a
different behavior but also one thing we
noticed in here is the a and D which
means editor Delta and deletes the Delta
and new source code which means all
filters from source code itself also
have a pretty good accuracy rate that
means this classifier can be embedded in
the editors so whenever you edit us
something we can monitor what you change
in the editor and then we can also
predict if your change contains folks or
not so based on the previous experiment
you can find out the best combinations
of each project for the accuracy so
again here the best predictor
combination is pretty much different
based on the project also we try to look
at the individual futures a little bit
here I'm not sure you can read each
character here but the idea is we try to
identify the pub the most significant
individual futures to predict the box
and then we link them using chi-square
algorithms and then also we look at each
if each features is contributing box or
a numb bug for example hear the applause
means the future is contributing to bug
my Nancy means the future is not
contributing to work which means clean
change so if you look at the complexity
metrics major poo Apache obviously the
Delta means Delta over the complexity
metrics so whenever you change a lot of
a source code that means a count of line
you are contributing to bug or if you
look at the plans editor Delta I'm not
very familiar with of Python this is
written in Python whenever you use the
self in the source code
you are more likely to creating a bug
obviously this we need more like
research to see what is really patterns
in here what kind of parents you can
find out in bulk code but this gives you
like overall idea is that there is some
patterns that introduce bugs yeah 3 45
introduces work but the generals right
correcting does not fixable a fair
pantry you can make any change as long
as it doesn't use it because the f is
highly likely to produce bug the results
here are the best pitchers four
classifications vary by project and
projects so there is no across projector
model here also the best algorithm
classification machine learning
algorithm varies by product product and
what you can find is about we we can
predict about seventy percent of the
accuracy and Nicole range but this is we
think this is very promising but because
what we used is it's very basic stuff we
just exchanged keywords from the source
code we didn't perform any semantic
analysis or very expensive as the static
analysis for these futures but still we
get this kind of high accuracy rate so
we think if you can apply more
complicated algorithms or more future
engineering techniques we're gonna raise
about five to ten percent accuracy rate
we're much more data what is so if you
are interested in you're more than happy
to show it to okay
alright thanks hyung so kind of the
summary high-level summary of the talk
is that well it's now possible to
classify files as fault prone or not
fault prone with very high accuracy the
results that we got across many
different open source projects showed
that very consistently we were able to
to do this and the other result is that
is now possible to classify individual
file level change you know on average 20
lines of code have changed here as
likely to have a fault or not likely to
have a fault and this can be done with
you know not stellar accuracy but still
very good accuracy probably enough
accuracy that developers might pay
attention so in my view this raises a
number of open questions so one is you
know we've been doing this in the lab
we've been analyzing open source
projects and we haven't tried to inject
these results back into the project
itself and see how this knowledge would
cause developers to change what they do
how it would cause QA people to change
what they do and I think there's a lot
of open questions there so one is you
would doing this would this affect the
accuracy of our predictions it's
conceivable that this might make our
predictions get more accurate as
possible might make them get less
accurate you know we're not we're not
sure what what's going to happen there
we're also very curious with this fall
classification work you know if you tell
a developer you know dear mr. developer
we are 75% likely that you have just
made a change we regret to tell you
however that we cannot tell you what
what that error might be or where you
should start looking all right so so how
would developers react to that kind of
knowledge you know you know they know
that it's probably right right but you
know where do you start right and so in
some respects you know some of our
future work that we're looking at is can
you give the developer any hint within
those 20 lines where to start looking
for likely places that there might be a
bug and and you know what developers
like this at all and the Sun pointed out
it's conceivable that you don't even
have to wait for the commits I know you
could be that you're typing along there
in your development environment and then
all of a sudden you know being you know
we're not quite sure exactly what you
just did but we now are pretty sure that
you just have a bug right and so it
could be the thing you just typed or it
could be something that you've done
previously and but it does seem that we
can you know we can do this and so how
would how developers react to that right
do QA organizations really want
essentially more bug reports on you
would you know is this information on
prioritizing the bugs you know into you
know that well this ten percent is most
likely to have the the problems you know
is that really useful or does that just
end up being you know another piece of
interesting but not really actionable
information so we'd like our analysis to
do is produce both interesting and
actionable information so our feeling is
that you should be able to take
proactive action and bring down those
bug rates and those files you know
possibly by doing restructuring possibly
by doing code inspections and so on but
it may be the case that you know that
that doesn't work and then you know can
we you know can we improve these approve
these techniques the other open question
is we've been looking at open source
projects you know it's quite possible
that's on the characteristics of open
source development could skew our
results and in particular we know that
the pattern of people coming on two
projects and leaving projects and
industrial settings is generally quite
different from those and open source
projects and so it's it's quite likely
that that may may change the accuracy of
our results but again it may go up it
may go down as well so we very much like
to try this out in an industrial setting
and in many respects that's you know
that's what motivates us to come here is
that we would like to define projects
and people who are willing to allow us
to come in and to run our analyses on
these projects and you'll give you the
results and allow us to to also publish
those results as well so that concludes
our talk and thank you for coming
so questions question so it's that's
what he said is change efforts so I
guess I don't see how you're you're
being more precisely that
all our brain
right okay so so the question is well so
we all know the change has risk now what
do you what are you doing kind of above
and beyond this um well so what I think
this work does beyond just the the
change has risk is that it can tell you
you know specifically those you know
those areas in which changes occurring
which are much more likely to have you
know risk that manifests itself into
bugs so you know so it is entirely
possible to have no files that are
especially with the bug classification
work it's possible to add files that are
changing and that we don't predict them
as having having bugs and so it's you
know it's certainly not the case at
least for that that change will
necessarily cause it to be you noted as
having having more bugs now for the bug
cash yeah and the fact that a file is
changing a lot is very likely to cause
it to end up in the cash but the you
know certainly for the classification
work no it's not a one-to-one you know
change means we say it's buggy question
large
sure he tell
Thomas
leave the change
right yeah so something that you've done
a little bit of looking into a kind of
you know frequency of large changes in
the files right so in the know you want
to address that question or I so and so
we've looked at done some look at you
know kind of how often these large
changes occur you know they don't occur
super frequently but they do occur
you're right yeah we actually manually
inspected all the changes in the open
source case mostly there are changes
like 20 39 range but a big change as you
mentioned 101 or 202 outer lines are
mostly feeder permitting change or you
change some api's and that affects all
the things and then we found out
actually we in this research we didn't
exclude the kind of changes but in in
the current research we exclude the dead
kind of change we think that's kind of
post positive
some
okay so the question was you know it
would be nice if you could for you know
kind of medium size changes you'll be
able to say whether there's a bug there
or not and so you like you know I
believe we're my senses were doing that
right and so there's you know so we say
that our average you know change is 20
lines but the changes that we're feeding
into the the blood classifier you know
they have a quite a range there and we
haven't done a specific analysis to say
well what is the accuracy at certain
change sizes you know I I i guess this
you know this workers produce enough
surprises that I'm not going to go out
in a limb and say predict what that
would be like but you know my sense is
that that would be pretty uniform you
know once you got above se 1015 lines in
your change you know because you have a
fair amount of data in there and so
these classifiers tend to work pretty
well if they if they have something to
work with and so I would imagine that
maybe the accuracy might fall off if you
get to very low numbers of changes but
but even there you know it's things like
if often being a you know a high
predictor you know they still might do a
good job changes we should just
interviewed you
how I favoritism knows the flag all
sufficiently large new country Jesus
coming bugs right I don't I tell maybe
we should come out how you feel like I'm
handing out half of these you so so the
question is so how well does the the
does a bug classification yeah how was
about classification work handle
editions of new code so it's pretty much
depends on the what kind of code you are
creating if the new code is something
people you have been seen in the
previous of change histories and they be
identified that pretty similar code as
for example buggy then your you co new
code will be classified as buggy code as
well but is based on the previous
histories is a tensor of certifications
charts you suggested
go on
push it on
right yeah so so here's the table so the
question was you know so so there's a
previous chart and let's see I think it
was this one here which was suggesting
that okay if you look at everything
except for if you use everything except
for the complexity metrics that produces
the best result for classification
accuracy but yet if you come down and
you look at this table here and you
start looking at it you know you know
for example especially with Apache right
and those are that's that's showing that
at least for Apache the complexity
metrics are pretty good predictors and
then you are also the questioner was
also pointing out that there are a
couple of other features in here which
are good proxies for some of those
complexity metrics and they were also
getting getting raised good so so I
guess kind of 22 points here you know
one is that what we're finding and doing
this kind of evolution analysis is that
you know the kinds of analysis you have
to do have to be project specific and so
it's really not possible to say train a
model on one project and then apply it
to another project the only time we've
seen that and the literature to work
even kind of well is if you take say you
know an earlier version of apache and
train it on that and then try to apply
to a later version of apache but that's
essentially the same project and they're
just kind of treating them as two
different projects so and so that's kind
of one answer that you know yeah we
would fully expect that some project who
would work well some projects would not
work well and so what that you know bar
chart was showing is that on average
across many projects not including the
complexity metrics works well but that
it's I would say it's not surprising
that they work well for some projects
the other thing I'll point out is that
it's often the case with these features
that even if you run your analysis and
say well it seems like we do a little
bit better not including it in that you
know if you you know it you know if you
are running a separate analysis where
say you're you're annoyed get back to
the chart so I can point here you know
so so this may say here that okay you
might just want to throw out the
complexity metrics but then of course
for those projects where the complexity
metrics work well you know you would
presumably have a lower lower predictive
accuracy
on the other hand you might you might
not necessarily lose all of that and
that because the machine learning
algorithms are essentially kind of
waiting all of these features you know
it may just be that even if you remove
one whole class of features it'll just
reweighed all the existing ones and it
still may get fairly decent predictive
accuracy and so it's so there's a cut
you know feels to me sometimes with this
machine these machine learning
algorithms it's almost like magic going
on under the hood because they're it's
like they're they're able to adapt you
take out some piece of information it
still seems to do well and you're like
how can you just remove this whole huge
piece of information or our class of
information that still does well right
and so area it's and of course the it
just drives me crazy you know it's like
you'd like there to think that there's
some kind of know like rational causal
model for you know why do bugs get into
the code right and and then you look at
these features here and you're like well
you know sometimes if you add if you
know that's a high predictor but not
always the case and so it's not just you
can't just you know label if as the
bogeyman all the time and you know
sometimes the author is important but
most of the time the author isn't you
know it's just you and so like you know
yourself is bad but but in other
projects you know similar kinds of
constructs aren't so yeah it's really
it's a real head scratcher here and so
it's you know I guess it's kind of you
know highlight some of the pluses and
minuses of using machine learning and
that it can can see patterns you know
there that you wouldn't as a human ever
been able to pick out on the other hand
what you get back as kind of your
predictive model is just is just weird
right it doesn't it doesn't give you
this kind of nice you know you know if X
happens and then y happens kind of
causal model
just to follow up on that that we
deleted Delta autumn patchy with
right this plus 18 so that means you're
taking an if statement help right right
so that's actually reducing the
complexity so you're more likely to
introduce a bug and apache if you
reduced the cyclomatic complexity and if
you increase the psychological well well
I'm with Apache that get you coming and
going right so you know if you add and
if that's a really good predictor but if
you're removing if that's also a pretty
good predictor lower lower number so
someone one means it's the best possible
predictor and then and then a very high
number no means it's higher and so and
then in each in each line you get the
top five predictors top five predicting
features for that class of features so
that's why for example on your
complexity metrics for Eclipse I'm
they're all up in the hundreds you know
that just happened to be those were the
top five best complexity metrics for
eclipse but they you know overall you
know in the project they're bad bad
predictors just that they happen to be
the best the best five of one's for
complexity combinations and files in it
exactly that's so you do a check-in and
you change three files are you taking
into account
combination specific combination of
vials that you've changed are you able
to say something about which of those
three files is more likely to have
right oh yeah and this one over to the
song so the question is you know so if
you in a commit that has multiple files
and that's a fairly common case you know
does the the bug classification
algorithm Canton say anything about well
you know are any of the files more
likely to have bugs and others or does
it kind of do some aggregation across
all the files and so on let's kick
cancer's we didn't take an account of
the code changes we look at the
individual file changes but I think
that's a good idea to look at thanks
question right so the question is have
we done anything to predict the severity
of the bugs so unfortunately no the the
and this is in some respects one of the
reasons we would like to get into an
industrial setting so the the bug data
that we have to work with is not
uniformly labeled with severity data and
so you know in some industrial settings
you know they are very good about
labeling bugs with particular severity
and so you know if we can it's certainly
if we can get our hands on such a data
set we can certainly do that prediction
we just don't have that information
available but yeah we would be really
interesting to see you know are we
predicting all the low priority bugs are
we predicting high priority bugs are we
predicting them fairly evenly with
respect to priority level yeah would be
nice to know you know it's like are we
are we flighting the system with
predictions of minor stuff or you know
are we really catching some some of the
good stuff and you know at present we
don't have a good sense of that
release
my cousin
right excellent question i'm going to
turn this one over to son he actually
has done some some recent work looking
at the the time between the bug inducing
change and the the bug fix change and
i'm trying to remember the graph that he
produced but anyway yeah so the question
was you know you know have we done any
work looking at kind of the time it
takes between a bug introducing change
and a bug fix change and what is that
frequency distribution look like of
those times I mean in open source case
when you look at the real photo
introducing point and the real photo
kicks point is every G 100 days into 200
days to fix the kind of data also we
think the some files that takes a longer
time to fix the bugs maybe it's some
indication of a pokey or number okay so
the picture work maybe we can take that
in account is use one of our pictures
for these classifications I'm trying to
remember the shape of that crap as I
recall it's like if it's it's a normal
yeah but it's a normal budget with a
pretty pretty fat pretty fat middle so
there's a okay Larry thank you very much
great great questions and thanks for
thanks for coming out and we you know we
can answer lots more after the talk to
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>