<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Self-Supervised Relation Learning from the Web | Coder Coacher - Coaching Coders</title><meta content="Self-Supervised Relation Learning from the Web - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Self-Supervised Relation Learning from the Web</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/d83DP6ft3_Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so good afternoon everyone a so most of
the talk will be technical I do have
some demos towards the end of a text
mining system that we developed clear
forests which is a company that I
founded a and I can show you if you're
interested towards the end but then
we'll see how we are on time so this is
totally unrelated to the demo so let's
start with a little background about
information extraction so there are two
main somebody needs to press accept okay
so there are too many proaches to
information extraction the first one is
the knowledge engineering approach that
basically means that you're right world
you write rules and that was deported at
clear forests took a we developed
actually a dedicated language called
dial the clarity of information analysis
language it used to be declared to be
coy as well I was a product person so
the initial version was actually a a
logical language but now it's actually
much more like a C++ language object
oriented etc but the just the name a
staid and the approaches that are based
on rule based systems in most of the
competition's proved to be a the best
approaches we completed in three
competitions I told the people in plunge
to a says ace in 2002 so as for people
that don't know stands for a automatic
content extraction that's the
competition that is administered by nice
supported by DARPA a enviar universities
and institutions think tanks that
participate in it so
first one was a sin 2002 so we
participated in English task there are
three languages that we do english
arabic and chinese so in two thousand do
we did english we came number one in
2004 we did arabic came number one and
in 2002 we also did the kbd cup which
was about analyzing a pubmed articles to
identify when you find new information
about a genes of the Drosophila the
fruit fly and we also came number one so
basically i think it just proved my
point that if you put enough effort into
writing the rules you can get the most
accurate system a the main problem is
that it does take a lot of time you need
really very good programmers in order to
write rules and you'd need a very good
engineering environment to support it
because you need to know how to debug
the rules and anybody that tried to work
with rule-based systems know that varies
usually there are very interconnected so
if you change one rule and you think
that you solve ten problems you discover
that you created 100 new problems so
it's really very hard to control a how
the rules affect each other so you need
all those really strong debugging and
tracing and profiling systems a and the
main problem is that its language
dependent so it means that for each
language that you want to work in you
need to write a different set of rules
the second approach is to use machine
learning so there and this is the
supervised approach so there you need to
provide an annotated corpus so you just
provide like a richly annotated XML set
of documents and there are not so many
that are available the linguistic data
conn sodium LDC is the main body that
provides all those annotated corpuses
and you can work on them but if you try
to count how many annotated documents we
have and which is about
20 years of annotations it will be a few
thousands that's it not more than that
so that basically shows you that it's
really hard to annotate documents
accurately takes them a lot of time to
do it and in each annual competition
that they create a chase we just
annotate 400 document 300 for the
training data in 100 for the test data
so if after 20 years we have about I
don't know 6,000 annotated documents it
tells us that there is a serious problem
with this approach now the good thing
about the machine learning approach is
that a snot language a it doesn't depend
at all at the language i mean if early
have an annotated corpus in chinese i
learn it for a chinese if i'll have it
in Arabic are learning for Arabic it
doesn't matter a I mean the algorithm
doesn't care about the language at all
and in addition if for instance in
English if I'm changing the
capitalization the orthography I just
rerun the system when I have the whole
text in uppercase and the system will
learn to cope with text all in uppercase
in a rule-based system usually you would
rely on the capitalization especially in
the part of speech component and then it
would be a problem you'll need either to
write new rules or do other tricks that
we try to do but it's not a problem for
the machine learning approach you just
retrain in the machine learning approach
we didn't know we just changed the
training data and reran the algorithm I
mean we'd use the classic algorithms
hmmm CRF and there is some degradation
but not significant like one or two
present something like that okay so I
think it's clear that neither of the
approaches is good enough
I mean it's definitely problem with the
rule based approach or definitely
problems with the machine learning
approach because you need all those
hundreds of documents that you need to
annotate and as we said it's not so easy
to do them anybody that try to do
annotations in a very consistent way
knows that it's extremely boring and
this is why it's so hard to get
documents annotated so then around 2004
2005 several researchers around the
world started to think about a
unsupervised information extraction so
one of the pioneers was they earn its
yoni in a university of washington and
they bill didn't know it all family of
systems are actually few of them know it
all know it now know it whatever a so
know it all basically the system that
can learn facts from the web and i'll
show you like two or three slides about
know it all and then this is just as
their motivation for the work that we
did and i'll show you also experimental
comparison between the results of the
know-it-all people and they work that we
did a so the main thing for knowitall
initially was just to do entity
extraction basically to create semantic
lexicons out of the internet so for
instance you want to know you want to
get list of composers or a list of
basketball players or a list of
scientists or list of cities or list of
countries so clearly cities and
countries are much easier than list of
composers and list of scientists a so
i'll show you what was the main approach
so i'm actually jumping to the next
version of of know-it-all because as I
said the initial version was just entity
extraction the new version that we
compared against is the one that is able
to extract relationships between
entities so basically it's built on top
of the entity extraction component so
think about is rocking like in two
passes the first pass you identify the
entities and then you try to correlate
between the entities so they used
they call generic patterns so we are
hard-coded that the main disadvantage
that we try to fix its hard coded a and
they are not so flexible so for instance
for acquisition they used things like
noun phrase was acquired by another noun
phrase a or noun phrase this acquisition
of another noun phrase and wearisome a
patterns for mayor a person is the mayor
of your city so clearly the good thing
about this approach is that it will give
pretty good precision but the recall is
lacking because in many cases in the
text it won't be it would fit such such
rigid patterns if you want to find
something that is you know you'll say
Microsoft a giant software from whatever
I know it's not politically correct to
talk about Microsoft here that's fine
acquired another so you'll have so many
words in between most patterns will not
catch it so this was our motivation
basically to use pattern learning to try
to get a system that will have the same
precision but much better recall ok
let's what I'm going to talk about so
the system that we created is called
esterase stands for self supervised
relation extraction system and we're
actually four papers that we published
about it a first one was in ACL 2006
then in emnlp 2006 a then some european
conference intelligent isthmus 2006 and
finally in I CDM 2006 just a month ago
in Hong Kong a ok so what's the input to
the system the system takes us input the
name of the relation that you want to
extract and the name of the relation
maybe like acquisition nurture CEO fame
joint meeting like one person meant
another person so basically any relation
that you might be interested in but in
addition you should give also the types
of the argument so basically you give
the schema of the relation so the name
of the relation and the types of the
arguments and the types of the argument
would be like a company person location
so basically the types of the named
entities that are the arguments of that
relation I'll show you actually two
versions or is one version that realized
just on a shallow parser that extract
noun phrases and a version that realized
on a named entity recognition component
so there we need to extract also the
types of the arguments and you'll see
that various about fifteen percent
improvement when we use a named entity
recognition component so here is the
architecture of the system so let me
I'll explain the the architecture and
then later we'll go deep into each of
the components of the system so as I
said the input is a set of a key words
it's basically described the relation in
many cases it can be just one to like
acquire or merge and that basically will
will use wordnet to find synonyms and
from that we will start to build our
corpus so we will build the corpus just
by using we use the Google API to a get
the sentences major problems with it I
don't know if there is somebody that
works around me they'll be happy to talk
to them because it has like so many
problems that day we had to find some
workarounds around it a anyway so as I
said we get the cured we get a the types
of the argument
of the relation we get all the sentences
that contain one of the key words of the
original set so if it was like merge we
would find all the ways that you can put
merge merge merge are merged and we will
try to find all the sentences contain
any of the words so this will be the
initial corpus basically sentences that
contain one of the words in this initial
juror list okay and then the next step
is to find seeds so in order to find the
seeds we used the same generic roles
that are used by know-it-all the good
thing about a voice patterns is that
they have very high precision so for
instance X acquisition or basically will
plug any verb inside of another noun
phrase and we will then get pairs of
entities that have this particular
relationship between them okay now we
take only the top ten pairs so we count
how many times we found each one of the
pairs inside this corpus and we take the
top-scoring a pairs and those pairs
would be our seeds I'll show you an
example what are the seeds that we got
for the acquisition relationship after
we have the seeds we find out of the
original corpus all the sentences that
contain one of the one of the pairs so
for instance the pair would be HP and
compaq so we look for all the sentences
that contain HP and compaq clearly
inside the sentence are to be also one
of the words if it's like acquisition
acquired acquisition acquired in all the
variations of Fey of the world and we'll
have the two entities
so based on this set of sentences that
contain the ten pairs usually would have
like several dozens of sentences for
each one of the scenes so we will have a
corpus of about 200 sentences 400
sentence and something like that and
based on this corpus we start the
pattern learning process then we
actually identify all the patterns that
we have between the pairs of entities
and I'll show you exactly how we do the
patter learning then based on day after
we learn the patterns we can do instance
extraction out of closer yeah please
change camera to include screen
presentation yeah are you fine
okay yes yeah so so after we have the
patterns we can use the patterns
actually to go back to the original
purpose that one that contained in some
cases millions of sentences and we
extract out of the original corpus all
instances that match one of the patterns
we I mean versed on the way i'll talk
about scoring the pattern so we don't
take really all the patterns that we
generate because we generate really tens
of thousands of patterns we just take
like the top scoring three hundred
patterns and we use those three hundred
problems to extract all the instances
then after we get all the instances we
use we use a predicate independent
classification component to give a score
to each of the extractions to each of
the instances and based on that we score
all the instances that we have which is
that we actually enables us to decide
what is the threshold that we want to
use and of course if we'll set a very
high threshold precision will be very
high but recon will go down if we will
go lower than of course we call it will
go out precision when going down so I
can actually draw the precision recall
graph just by changing the parameter
what is the threshold that I'm using for
a scoring the instances and I'll show
you what's results when we compared it
to the performance of the know-it-all a
system so those are for instance the
feed that we used for the acquisition
relationship I mean it was generated
automatically by the system and you can
see pretty good the only thing that
actually did work so well this one
because it got also a base san
francisco-based which is probably
standing that we sure
have gotten rid off within a but other
end it looks pretty good okay so let's
focus now on the specific components
that we have inside the system so the
first component that I'm going to talk
about is the patterning component so we
have all the sentences that contain the
pairs that we built like for the
acquisition you saw the top ten pairs
that we used and what we need to
generate now is a set of positive
examples and negative examples for the
pattern learning process why do we need
the positive a negative basically to
score the patterns because we would be
interested in patterns that cover mostly
just positive instances but it's very
important that the patterns will not
cover any of the negative instances so
positive instances that's easy right we
can just take the sentences that contain
one of the pairs and generalize it into
some positive pattern but how do we
generate the negative ones so we're all
sorts of IDs that people try to use and
we used some new ideas that we came up
with so we so i'll talk about them in a
minute so this is how we generate the
positive instances here is an example of
a sentence this one and basically we
replace the arguments we've attribute
one attribute to I mean here it's binary
if it was an inner-ear relationship then
we would place it with a set of
attribute one till attribute so this is
what the sentence would look like after
we do the generalization I'm missing
negative instances one okay so here are
some ideas how to a generate negative
example so one thing that was suggested
actually by a row
girish man from NYU and then also the
paper from Washington used it the same
technique was that you should learn
actually several relationships in
parallel so you don't learn just
acquisition alone you learn like
acquisition merger co may or whatever
and then the positive examples of the
other relationships would be the
negative examples of the current
relationship so that make sense but we
wanted to do more so one easy trick is
if you have an anti symmetric relation
for instance acquisition is an anti
symmetric relation because you want to
know who was they acquire and who was
required so if you reverse it you'll get
a negative instance emergere it doesn't
work because it's symmetric so by the
way we as we will see it's very hard to
distinguish between an acquisition in a
murder because just in accounting there
is no clear distinction between what's
an acquisition and what's the merger so
we saw for instance at HP and compaq in
many cases appeared as an acquisition
other cases it appeared as a merger so
that definitely sends the system into
some problems I'll tell you about the
next generation of the system which is
totally a unsupervised and they're
definitely we had the issues with it so
finally we just put them in the same
cluster a okay the other and this is the
a what is written in this slide other
trick that we did and actually boosted
our a precision a lot was to take the
sentences and identify inside the
sentences all the noun phrases and then
basically change the assignment of
attribute one in attribute to in the
case of day the binary relationships to
the other noun phrases in the sentence
so basically let's say you have five
noun phrases in the sentence and let's
say that the correct ones are the last
two so I'm taking one and I'm moving
into the first noun phrase taking this
and moving into the third noun phrase of
course there are many combinations that
you can do each one of them is the
negative instance the good thing is that
you're using the same vocabulary of this
relation you're not using a vocabulary
from another relation because one of the
products that we saw is when you try to
learn from other relations but using
their vocabulary actually not a good-
instance usually when you try to do a
classification you're interested in near
positives and the way that we describe
generates much better near positives
than just using other relationship as
your negatives it a as I said we we have
two modes of operation one is the
shallow parser mode where we just
generate noun phrases and the other mode
is the mode where we also do the named
entity recognition so if we have the
named entity recognition we will change
a company for a company and a person for
a person if we have to name the shallow
parser mode we will change a proper noun
for a proper noun a general noun phrase
for a general noun phrase so basically
we keep the typing scheme to try to be
as near positive as possible so here are
some examples so this is a another
example of very same time a where we
replaced again the names of the entity
with attribute one attribute oh and here
are some negative instances that we can
by using doing the swapping that I just
described okay so is that clear and as I
said we can to use with symmetric and
anti-symmetric so in symmetric
relationships if we swap the order will
get another positive instance in the
anti-symmetric will get in
so now we get to the pattern to the
pattern generation mode and in this mode
basically we take any pair of sentences
any pair of generalized sentences and we
try to find a more specific
generalization of the two sentences
obvious you know the classic dynamic
programming algorithm how to find the
least a from a generalization of faith
of the two sentences a and I'll show you
an example how it works first before
i'll show you the example I need to say
couple of words about the pattern
language that we use so the patterns in
this case are sequences of tokens just
words Skip's we have two types of skips
unlimited skips so basically you can
skip any number of tokens and limited
skips that can skip only until we find
another noun phrase so we are not
allowed to skip over a noun phrase and
that actually helps a lot to a restrict
the patterns that you get a any in
addition of course we have the slots the
arguments of the relation so if its
binary will have only two slots NRI will
have more according so here are some
examples of Faye patterns a you see
attribute1 skip was acquired by
attribute to etc so at this point I want
to say a couple of words about a
anchoring you can look at the slots I
mean what's written here is attribute
one attribute to and in some cases you
can see that attributes are not anchored
it means that next to them you have a
skip which means that you don't have a
clue exactly when the attributes when
where is the attribute inside the
sentence so if we have slots that are
incurred from both sides that's the best
to means that really the precision will
be very high so later when we do the
scoring of
patterns who will utilize this
information so patterns that have encode
floats will have higher scores than
patterns the dot have and chords loads
okay and you can see things we have to
then we have the scoring for the two of
them are uncovered only one is anchored
from two sides on one side so we have
basically three levels we have onion
cord we have semi anchored and fully
anchored okay so we talked about a
generalized function and basically to do
the dynamic programming I just need a
cost function to define what would be
the cost of matching any two elements so
of course if the two elements are
identical the cost would be zero and
then we have other costs that really
doesn't matter what is the exact cost
function as long as qualitatively it
retains their relations that we have
here so let me show an example a so this
is the first sentence towards this end
argument one attribute letter in July
acquired argument too and then earlier
is your argument one fired argument okay
so you see it's similar sentences but
not the same so toward here in the first
sentence doesn't match anything on the
second sentence so we have a cost to
then earlier here than anything again
this matches base so it's cost zero and
of course argument won an argument one
of course always need to be a Mitch to
each other we cannot match anything else
it's an infinite cost based on the
dynamic programming we find this was the
best match and yes
use its tool in in July yeah probably
should have put it in two lines a ok so
this is like the pattern that we will
get and now you can do Potter
simplification because to skips is one
skip I mean doesn't make any difference
and clearly when you have a leading skip
you're also not interested because
inherently any pattern you'll have a
leading skip so you don't need this and
in addition that this is the like stop
word so we can just remove it and
basically the pattern will start with
the comma here so that would be the
pattern that we got from the previous
two sentences questions about this after
we have all the patterns that we
generated yes 3ds pairwise oh yes that's
the old version in the new version
actually we took one of the classic data
mining algorithms they a priori
algorithm and we did in a different way
much faster it's actually more
sequential patterns and that gave us a
huge boost in performance a one of the
things that we do after we generated the
patterns is to look at the pattern and
see if one of the original keywords is
maintained in the pattern if we do not
see any of the world any of the original
keywords I mean after we expanded it
with wordnet then it means that this
pattern is not doesn't have any
substance I'll show you an example in a
minute and then we delete this pattern
only patterns that have at least one of
the original cures or one of the
Stingers will be maintained okay so for
instance you can see here in the first
example this doesn't contain anything
any acquire require the position board
purchased nothing so it has zero value
so we just deleted here the purchased
which is
of the synonyms of the original keywords
and this is why it's kept as a pattern a
and now we do the scoring of the
patterns we have the positive a negative
example and I mean the formula doesn't
matter so much we played with different
types of formulas how to do the scoring
and it didn't make such a difference the
only thing you care about is that you
want to have as many positive examples
that will be covered by the pattern and
almost none of the negative examples so
this was a pretty good a scoring
function and this is what we use and
then what we do is we take the top three
hundred patterns and this is our pattern
set so here are some examples of
patterns that we found this is like for
inventor that X invented why a which is
not such an easy I mean here we the
initial version was written in perl so
you know in Perl the dot is something
that matches any character so the dot
star is basically just taste this kid a
and this is for CEO okay a for the
shallow parser initially we used the
open NLP a which really sucks and then
we didn't use it anymore I mean too slow
and what we did is we used our own
version of a CRF based a noun phrase
extractor which works about ten times
faster and is more accurate so let's
summarize where we are now so now we
have all the patterns that we extracted
I mean the top patterns after we scored
them we took the top 300 we went to the
original corpus that in many cases
contained around 1 million
sentences and we pulled all the
instances that match to one of the
patterns now we want to score the
extractions because some of the
extractions will not be correct so we
want to rank also our extraction and for
that we wanted to build a classification
model now usually when we talk about
classification that sounds like
supervised learning which means that we
need the training data now if we be
training data that would be probably
predict a dependent relation dependent
so we cannot assume that we can train
for its relation give example of the
relation that basically kills the whole
approach yes original corpus which has
millions of sentences yes are they
dissonance is that you gathered from the
web how did that end up being millions
basically any sentence that would
contain one of the cures like Mirage
Mirage or not the ones that are related
to deceive we started with a big corpus
that contained just sentences to contain
any of the cured yeah this is what I
said the Google API correctly yes yes of
course you know Google returns only 1000
eats per query so you need to start the
player you know to Edwards I'll usual
tricks even with the usual tricks for a
problem a but yay finally after a pain a
painful process you get to a very large
corpus that was definitely one of the
most difficult things I'm sure
internally that's something that
probably not have a problem with a so
yeah that would be a relation a so
basically as I said to Bill the training
data for each predicate is out of the
question so we want to move to a
predicate
when I'm saying predict a time in
relation I mean I'm using them
interchangeably we want to move to a
predicate independent erosion which
means i'll do the training only once for
one predicate and will be used for any
other protocol so initially it doesn't
sound it when it's going to work right
but you'll see okay so here are some of
the so in order to do a the
classification model we need features so
we want features that are predicate
independent we cannot use anything
semantic because anything semantic will
be predicted dependent so we need to use
only syntactic features of the patterns
so here are some examples of features
that we used so the first one was just
how many different sentences supported
the pattern the pattern that generated
the instance so if we have a lot of
sentences that's good then about the
pattern itself that generated it how
many positive and how many negative
examples this pattern matched so of
course the higher the number of positive
and the lower number of negative that
would be good thing about the anchoring
I told you about the anchoring before so
of course patterns that are that have
only slots that are fully anchored would
be better that pattern that includes a
totally unencoded slots how many non
stop words the pattern have if we have
more non stop words means the pattern is
more specific and then we give it a
higher score a if there are proper noun
phrases in between I mean in the skips
and finally in the scripts themselves
how many words matched skip and again
the rationale here is that if we have
many words
that match escape means the skip was
longer usually it would mean that
probably the extraction as a lower
probability we would give higher
probability to shorter skips okay again
what are the six features that we
thought about you can think about maybe
another six and we can add it to the
model as easy just to a try and see if
it works better but as you'll see it
works pretty well for us so we didn't
really see a urgent need to think about
additional features definitely if
somebody is ids for additional features
i'll be happy to hear so we use
specifically we use the logistic
regression implementation and it was
done at Rutgers David Lewis and his
colleagues it's called bbr you probably
are familiar with that a which is a
similar performance to a svm a svm is
actually a little better not by much but
it is a little better we actually did
some extensive experiments recently and
we saw that svm as a little better than
vbr a so BB are actually is the
regression model so we had to convert
all the numbers into binary numbers so
this is how we basically did the
discretization from the numeric numbers
that we got before to the binary a
numbers so from the six features I mean
you can see the two and five we throwed
and we converted it into a 15 binary
features well it's actually more than
classifying it's actually we get a
number between 0 and 1 which is what the
probability that use the correct
extraction that this is really an
instance of the relation okay so you if
you'll set the threshold if it's above
the threshold you classify it as a
correct instance it's below the
threshold you classify it as an
incorrect instance
right so this is I'm getting to it I
said we don't have training material a
for just any predict that but we pick
one predicate and for dead predicate we
do give training material okay but we
use it for all the other credit cards as
well so we do it once and it's used for
any other any of the other predators a
ok in addition we also used a named
entity recognition component so when I
think I talked with Ryan this morning
and he mentioned that you are using some
rule-based component to do named entity
recognition we actually also used a very
simple rule based a named entity
recognition component it's actually
amazing how simple it was was like two
or three rules for each of the relations
then it worked well I mean in this
context we do want to do some other
experiments where we don't use just this
low level and rule-based component but
also like craf and the better models for
entity extraction so we used this very
simple component a in this case for the
relations that we peaked we just used it
for person and company so what are the
only types that we were able to
recognize of course if you want to
support other you'll need to write
specific rules or train a on those
entities those types of entities okay so
I mean I don't it's not really important
the specific scores of the named entity
recognition but the main thing is we
converted it into one additional feature
so previously we had 15 binary features
now we added another feature and that
created a better classification model
okay so one of the major questions that
we wanted to explore was really can we
train on only one predicate and can it
be used for all the other Predacons so
that was one of the key questions that
you want to check and what is the
benefit of using the name entity
recognition component so the 16 features
rather than the 15 features and then of
course to see for we are any better than
the know-it-all a system they know it
all it was adopted to extract relations
not only entities so very actually had
two versions we had the original
know-it-all and an improved version
called know-it-all PL it was also able
to learn patterns so remember initially
when I started to describe no I thought
I told you that they have only a very
limited set very rigid patterns so very
extended it by learning patterns and
you'll see all the results here and the
last question that we wanted to ask was
what is the Torico of our system that's
a problem because all the time we just
talked about precision but to measure
recoil that's pretty hard like if we
have a corpus of 1 million sentences I
don't tell slaves that can go over the
one wheel in sentences and tell me what
are all the extraction that you can get
out of it so measuring record is almost
impossible so we still found the heck
how to go around it what we did is they
vary say a commercial database called
the platinum something that I got from
my friends from NYU and they have the
subscription and they gave me all the
mergers and acquisitions in 2005 in 2006
minus any and then I could actually
compare at least get an estimate for the
record in the following sense I took all
the list in there like I think 40,000
mergers and acquisition just in 2005
amazing
so we went over the list and for each
one of them we looked if there is at
least one sentence in the corpus that
contains both arguments now we didn't
look if you truly a true instance of
flag acquisition the only criteria was
if I have a sentence that contain both
two arguments so basically we're
actually being too harsh on ourselves
but that was the experiment so if we
managed to extract that instance and we
got one if we didn't manage to extract
it we got zero and basically this is how
we measured the approximate rico someone
you are happy yes but it could be that
you recall is only ten percent correct
that's the difference between a classic
information extraction system in a webex
project this is what we are comparing
again a web extraction system you just
care to find at least one simple a one
instance of each event or each relation
maybe it appear 20 times I don't hear if
I click once good because basically we
just want to collect records so if the
effect appeared 20 times but I
identified it only once that's fine we
maybe if you want to find all possible
instances that's a different problem
that it's a classic information
extraction system but in this context we
are interested mainly in building
database of factors we're not interested
in finding through rick roll of fate
like it through information extraction
although I must say that don't think
that if it was 20 days we found one
usually we found like 16 out well so
throw that we didn't do a full measure
because for that I will need the
all those slaves go one by one
unfortunately I don't have it but based
on you know a very small number of
experiments that we did we get pretty
good record so let's talk about how we
do the training so we picked one of the
relations we ran that relation over like
1000 documents a 10,000 sorry 10,000
sentences no document 10,000 sentences
and from that we got something like 200
or 300 extractions and then manually we
went over the 300 extractions and said
yes or no is it to correct instance or
not and this this was our training data
so whistles are trying it on for one
particular relation Fredette we build
the model and then we use the same model
for all the other predicate okay now how
did we measure the precision for the
other predicate just by using sampling
we let's say that we extracted a 10,000
instances we pick two hundred out of the
ten thousand instances and manually
checked a is there a corrector okay I
think I talked about that so here's a
simple a of the output of the systems
you can see here which is like HP and
compaq and you can see all the sentence
that where we extract it you can see the
merger and acquisition is written all
sorts of different ways and all of them
here is that the good thing that it
appeared okay a so let's talk about the
cross classification experiment so here
in each case we used a
here a predicate that we used to do the
training with acquisition and you can
see the result when we use this as a
model on all the other Predacons and
this is when we used a nurturer as the
training predicate and the result that
we get on all the other four I mean
including odds in merger and you can see
that qualitatively it looks very similar
I mean of course there are some
differences but it looks similar enough
and we got the same performance when we
used any of the other three predicas we
had five predicate cinahl a merger CEO
Maori inventor what are the five
predicate that we tested it on the edge
is a number of extraction number of
instances that were extracted so here
this is the experiment that we did in
comparing to know it all and no it'll PL
so you can see here we have four systems
so this is now at all nitrile PL the
original version of s res when we did
not use the named entity recognition
component and here when we use the named
entity recognition component so as I
said we get a precision recall graph
just by varying the threshold so we have
all those instances that get the scores
and we just very the threshold and this
is how we get a different point and you
can see that pretty much day this
version was the best I can tell you that
in most of the credit cards on the same
precision level our record was three
times better than they know it all
system three times better that's a lot
so it means that we found like eight
thousand
extractions rather than 2,500 so that's
pretty major it's not what I'm talking
about like one or two percent and at the
same precision level a bizarre for the
other credit cards of co phan mayor oh
and this is for inventor by the way you
see for inventory that we don't have a
named entity recognition component yes
now they ran it I mean we share the
sentences and they ran it yeah it was it
through collaboration yes so so anybody
has a guess why we don't have the
asteroids with fan named entity
recognition in the inventor case i mean
i showed you for grunts in all the other
cases so i do am i showing you only
three year the argument isn't right i
mean for invention we don't have the
capability to identify this invention I
mean we don't have a a named entity
recognition that is capable to do it so
this is why we didn't Travis yes aim
okay so so in general why why our system
better than I thought so we actually try
to really understand better when is it
working significantly better and when
it's the mirror so what we found is that
if there is a lot of redundancy okay if
there is a lot of redundancy they know
it all is doing well too which is
actually relates to a and I know your
name your question a because if they
were able to find one instance even if
you had like 20 instances and we found
only one out of it so if we had a lot of
redundancy for the specific fact usually
there are able to pick it up to we are
able to pick it up even if the
redundancy was very small so for facts
that were very widespread very well
known know-it-all picked it for facts
that were little-known we are able to
pick it much more than variable so if
you think about it like for intelligence
purposes usually it'll have two
interesting things on you want and that
means that something like no it'll
probably will not be good for that now
the main difference of course comes from
the pattern learning component in their
system they used a fairly simplistic
pattern learning we use the more
elaborate patterning and this is why we
got we were able to find many more
instances then the simplistic approach a
so this is you can see the redundancy
here you can see that for mayor the
redundancy was very high and this is why
no it will did almost comparable to a
two-hour system and in for acquisition
merger the redundancy was much lower and
this is actually where we excelled in
comparison so I told you about the exact
name of the subscription it's let the
SBC platinum this is where we got all
the acquisitions and mergers from a
talked about that so you can see here
that if something appeared only once
then in merger are able to get the fifty
percent of the time in acquisition only
about thirty eight percent of the time
and as we get to like eight we were able
to get to higher than eighty percent
recall which is not bad eh I mean for
many of the effects many of the not well
known facts they appear like hundreds of
times so here eight is usually something
that is not very well known okay so here
are the conclusions let's talk a little
bit more about actually the future plans
which i think would be more interesting
so other things that we're working on
now is extending it to enery
relationships rather than just binary
and in addition we want to move to a
totally unsupervised mode remember the
input in this case was the name several
key words that describe the relation and
also the schema like how many arguments
and what are the types of the argument
so in the next version this was actually
the paper that we published already in
icbm we started with just a corpus with
nothing and we try to identify first the
names of the relations and then later
learn for each of the relation use
Astra's is like a subcomponent so we
have basically two phases the first
phase relationship identification and
then relationship learning after we
identified revelation and it worked so
so I mean we're definitely not very it
it was interesting enough i guess to get
accepted but there is a lot of work
still that we need to do in order to get
to a really commercial system it will be
useful I mean when it's totally
unsupervised I think for s res it works
pretty well and definitely where it is
how to improve it even more now I know
if I have time for them or I'm sure that
meeting so fortunate so it's okay I
think you should take some questions and
one might be it's fun funny horses the
exam where they can say afterward
otherwise I shouldn't afraid three but
that's not my type you can speak
question yes I you may have mentioned
this when you train your classifier just
syntactic classifier which one did you
eat it on look pretty good like you said
what am i a ride them all of them and so
what did you end up picking one yeah I
mean for the 5x products would be
preposition now is actually you're at
least frequent I yes this is the less
redundant least issues imagine you just
trained on one particular information
right and a lot of these these guys are
given flight logistic regression into a
specific domain it doesn't seem like
it's happening right because it was
purely syntactic feature nothing to do
with this demanded a pretty good if the
features would depend you know the
specific wording then it would be
horrible but we basically look just as
the structure of the pattern you know
that how many words match the steep and
how many non names before words I have
inside the pattern was are purely
syntactic just trying to evaluate you
put this of the platter that's pretty
general this is why I think good for all
the other religions I try using maybe a
little more lexical features and then
seeing it over pin no we did no but did
you have also tried to throw in two
trainings not clemence's you're going to
produce the same classifier and
everything at the end where I didn't
chain it both acquisition and merger
that's possible when you have the data I
do a possible these are approached
applicable to offer domains in a sense
tech do Texas basically business news is
and is it any kind of property in
business use news for example the
uniqueness of HP and compaq but lets you
learn you back
could it be like if you use it on our
kind of views with what such a period at
in politics your names of Nations and
you have many false positive Lynetta
Kizer and unite instead they appeared in
so many contexts not with a particular
predicate with many other predicate
which are completely unrelated could it
be that your system will mix learn
something yeah my magnet error will need
to try and use other mechanisms in order
to vary but remember we are starting
with sentences contain one of the
variations of the original relation and
distracted Israel in the u.s. barely so
many different context is not such a big
problem because i'll pick only the
sentences that would contain and it's
think about the relation okay if you
really tricky whatever i meant sure that
y'all had built in fresh since we
actually could keep askin you're a
writer so there will be suits the trail
running all over hardship which ended
here so we also do not get ripped off
the schedule okay thank you so much
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>