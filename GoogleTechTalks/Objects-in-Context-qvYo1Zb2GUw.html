<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Objects in Context | Coder Coacher - Coaching Coders</title><meta content="Objects in Context - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Objects in Context</b></h2><h5 class="post__date">2008-01-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qvYo1Zb2GUw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today I'll be talking about object and
context and like Hartman said this is a
version of the talk that I gave it I ccv
this year or rather last year but there
are also some new contributions that
karoliina gauges and I have worked on in
the past few months so let's dive into
the problem right away and we're
interested in object recognition and
given a test image would like to a
identify all the objects in terms of
their boundaries then we would like to
actually label them to do the proper
object recognition and a traditional
off-the-shelf recognition engine would
do something like that so the tennis
courts gets the green area of the image
gets recognized as the tennis court the
person gets recognized as a person the
tennis racket is given the right label
and then the blob the yellow round blob
in the right corner gets recognized as a
lemon clearly to us it's seems like a
wrong answer but there is hardly any
difference between 11 and lemon and a
tennis ball but ideally would like an
object recognition system that would
recognize it correctly in terms of
context so the idea of this talk is that
all the objects that are recognized and
are present in the image should co-occur
or coexist with one another such as
tennis balls and tennis courts and
rackets and person tend to tend to
coexist in the same image but lemons and
tennis courts aren't likely to be found
in the same scene so as the overview
we'd like to perform segmentation on the
image first to identify all the object
to identify all the segments boundaries
of all the objects then perform segment
classification to provide labels for all
the segments that we have in the scene
and finally do some sort of contextual
reasoning to determine whether all the
labels that were given for all the
objects in the scene whether they all
makes
with one another or something is out of
sync the first question that will
address for segmentation is we want to
figure out why it's segmentation useful
for recognition well first of all
segments can represent either entire
objects or object parts they can exploit
the explicit information about objects
in the end Essene themselves it is quite
convenient to localize and use shape
using segment boundaries finally the
signal-to-noise ratio gets increased
because we'll only concentrate on the
segment's themselves and not the clutter
the background information in the image
and all of these things make sense
however there may be a problem the
segmentations that we get they may be
all wrong and then we really get no
benefit of doing that um so one first
complicated question that we want to
address is how do we get a good
segmentation what does it mean to have a
good segmentation and the the first
thing we'll talk about here is the
problem of q combination and that refers
to having an image how do you combine
color and texture and brightness and
contours how much of each to consider
and which constitute good features in a
particular image and to illustrate this
point further I'll just we'll just look
at point sets in two dimensions but this
sort of scales up to images rather
simply but it makes it more intuitive to
to illustrate these points so for
example given points in two-dimensional
space we can segment or cluster them
into groups based on proximity just
based on distances between points and
this particular clustering seems like a
correct one one group is marked in red
and the other is marked in green however
if we want to consider a different view
or a different feature of the data such
as density of the points themselves then
we can come up with this particular
segmentation or clustering and as you
can see both of them
seem reasonable if we would like to
combine da density and proximity we can
get yet another segment ation or
clustering of the same data and all
three of these are equally meaningful
depending on the particular question
that would like to answer about this
data to combine these these kills 11
fairly simple approach is just to use
compact convex combination where the
matrix matrix CF is just a similarity
measure for each of the Q's vector P is
just a Q combination vector that tells
us how much of each feature we would
like to pay attention to and big f is
the total number of these cues that we'd
like to combine and like I said since
it's a convex combination approach the
sum of the vector P must add up to 1 the
next question in addressing the quality
of segmentation and doing parameter
estimation is how to pick the moral
model order of the particular
segmentation problem in other words how
do we know how many segments or clusters
there are in a problem for example going
back to the point sets one more time we
can consider this particular clustering
for two groups or three groups or four
groups and perhaps if we do this once
the solutions for K 2 3 and 4 all seem
reasonable so now to validate our
results let's consider taking the same
original data set but adding just a
little bit of noise either perturbing
the distances between points or just
changing the absolute coordinates and do
the exact same segmentation or
clustering with the same number of
groups two three and four as we can see
for the value of k equals to the
solutions did not change the same
segments or clusters reappear the same
happens for K equals four however for K
equals three we see that the grouping in
this
lower triangle has shifted because some
of the clusters became closer than
others depending on the noise that was
introduced to the original problem
therefore we can say that in selecting
the model order we can use this sort of
perturbation to determine which values
of K are useful and which ones aren't
and to determine what's useful and
what's not we use this notion of
stability and this is how we define
stability and essentially what this
means is we can take the same data set
perturb it a hundred times either right
like I said adding noise or changing
distances between points or just adding
noise to the pixel data and perform in
the same segmentation of clustering
multiple times over each of the
perturbed instance of the data and then
we will look at the label consistency
per data point over over the multiple
iterations so here si is the label
consistency and we would just like to
add up the number of points that remain
with the same label over the number of
iterations so in the previous example
that I showed if the labels don't change
for example for K equals 3 then we will
say that the solution is stable because
if you add noise the labels or the
output of your segmentation doesn't
change but if you just add a little bit
of noise and things start shifting
around then the value of K is probably
not native to the particular problem so
now let's look at some examples of
stable segmentations and like I should
like I show them in the previous slides
they may exist multiple values for
different case for example for k equals
3 k equals 4 the solutions were equally
stable and four different combinations
of few parameters solutions may may be
equally stable and useful so here we
consider a combination of nine different
segmentations for values k from two to
10 resulting in total of 54
segmentations and
here just a few examples and as you can
see the different segments depict this
different aspects of the image starting
from the cow the grass the building and
even the cows here's some more examples
of stable segmentations and here you can
see explicitly how the sort of trade off
between different cues plays plays a
role so in the left column are the
original color images that we'd like to
segment the two middle columns are just
examples of some of the stable
segmentations and then the right column
are the various human segmentations
provided by the berkeley segmentation
database and for the human segmentations
the darker the particular segment
boundary is the meaning means that more
people agree on that particular boundary
so let's take an example of this snow
and cloud in the sky and the first
stable segmentation that we see just by
just by permutation only considers color
as a similarity measure between
different pixels so here we see that all
the white stuff including the snow and
the clouds is separated from the from
the sky and is separating from the rock
hope part of the mountain that's there
however if the texture Q gets turned on
the sky I'm sorry the clouds are
separated from the snow because they
have different textures and we can see
that another segment boundary is
introduced over here here are some more
examples from a completely different
domain these are biopsies of human
tissue and we can see that these
different segmentations really pick out
different properties of the images
whether it's color whether it's texture
whether it's human glands or individual
cells and this is very powerful approach
to data mining and segmentation
depending on the particular problem now
you may ask these images or images of
real scenes or landscapes are rather
complicated and combining all these
different features and cues and the
number of groups may result in a huge
number of segmentations so how would we
deal with all these stable segments and
because potentially it can be a total
computational and combinatorial blowout
but as it turns out the number of stable
segmentations is almost always less than
1% of all the possible segmentations so
if we consider the examples that I
showed in the previous slides if we
consider all the possible parameter
combinations it can easily go go into
tens of thousands but it turns out that
the number of stable segments is always
less than one percent and the that's the
number of an important and interesting
segment for even complicated images is
very small and we call it a short list
and it's typically under a hundred of
segments so now with stable
segmentations at hand let's return to
the overview of our own proposed
paradigm and we'll modify it a slight
bit instead of performing this
particular flow on on the entire test
image what we would like to do is we
would like to consider first all the
stable segmentations and then apply the
flow that we introduced earlier to all
segments in parallel that way each
segment gets treated as an image and the
processing gets done on the individual
segments and that will allow us to
perform the contextual reasoning and
more accurate recognition that we spoke
about so now let's concentrate on the
second aspect of our flow and that's the
segment classification there's
traditionally there's two kinds of
models for object recognition and they
can be broken down into two broad
categories degenerative and the
discriminative the constellation parts
based model intuitively seemed more
appropriate than the
been talked about for over 30 years now
but it just turns out that for
articulated objects they're so
complicated and it's so hard to find
instances of all possible configurations
of a particular object that they they
tend to get so complex and the bottom
line and the reality basic methods like
the bag of features work a lot better
and that's what we'll consider in this
work so now I'll quickly do an overview
of the traditional drag of features
method and then we'll see how we combine
it with our segmentation approach so
traditionally given a few given a
training set images are represented
using some interest point descriptor
whether it's sift or answer your
favorite descriptor then always feature
all these descriptions high dimensional
space depending on the dimensionality of
the descriptor and then the vector
quantized to construct these code words
and here that this is the example
dictionary of code words and then each
training image is represented by a bag
of these code words it's nothing that
that a spatially order lyst histogram of
these code words for every training
image and then for every category we
come up with the model of this
particular combination of bags so then
when given a novel image we apply the
exact same transform and then use some
classification scheme whether it's an
SVM or a nearest-neighbor to come up
with a best label for the unknown image
so the motivation on combining of babel
features with the stable segmentation
approach is that will ideally I'd like
to have a novel image segmented out such
that every segment represents an object
then take individual segments run and
through some object recognition black
box and then come up with a labeled
image where each object gets gets a
label
let's get them to just a little bit more
detail and see how we actually combine
the stable segmentations with the the
classification scheme so here rather
than trying to come up so fee is the
description of a bag it's just a
histogram of a particular image sq
refers to the key with segments from a
test image and all we're trying to do
with just trying to find a
nearest-neighbor of a test segment to
the training image so here with we treat
each segment as an individual image and
we're trying to just find a
nearest-neighbor image from the training
that will match the test segment and
then we'll assign the same label to it
now for labeling there's two kinds of
possibilities one for multi-class and
one for single class recognition and
they essentially the same except for a
single class recognition will pull over
all the possible labels and sum up all
the probabilities and give the best some
label to the entire image now one
interesting thing is that when we're
doing the stable segmentations it is
quite possible that will come out with a
very small segment that will have a very
high probability of being something
let's say it's one pixel large and it
will be white and it will have a perfect
correlation to snow for instance but
then we know that the one size one pixel
size segment it's probably just noise
and it's not very informative so when we
do the labeling we weigh the likelihood
of a particular segment SQ be in a
particular category see by the size of
that particular segment relative to the
size of the biggest segment because if
the segment is too small then chances
are it's not very informative now to
quantify the importance of using
segmentation with the bag of features
we'd like to ask for a fairly
straightforward question one as can
segment in an image improve
nishan accuracy the next question is
does the number of segments affect the
recognition accuracy third is does it
even matter what kind of segmentation
you use why go to all these stable
segmentations and not just randomly
partition this the image and finally is
it beneficial to perform localization
and multi class recognition using
segmentation so empirically to answer
the first question without ink with
including stable segmentations on these
sort of traditional Caltech and Pascal
data sets we can see that including
stable segmentations and leaving the
recognition engine the bag of features
completely untouched we almost see
increase in performance by a factor of
two just by isolating the segment's from
its from its background and implicitly
computing the shape of the segment we
can see such a dramatic increase the
next question that we want to answer is
um does the number of segments segments
affect the recognition accuracy and the
individual lines and the labels don't
really matter but what's interesting is
we can see a trend on the x-axis we see
the number of segments and on the y-axis
we see the recognition accuracy and we
can see that for all of these 20
categories we see that as the number of
segments increases so does the
recognition accuracy but eventually at
some point it plateaus off meaning that
as you increase more number of segments
more segments become useless and
erroneous than the good ones and
directly the recognition accuracy
eventually will fall off so it's nice to
find this inflection point where we can
stop and say this is the good number now
a more interesting question is what
about the quality of segmentation how
good their is segments or do we need to
actually give this benefit so for that
let's consider a simple example I think
this comes from the Berkeley
segmentation database and we can reason
about it
by considering all possible segmentation
algorithms so if we can if we think of
it as this continuum of segmentation
accuracy the best on this right side are
the human segmentations because that's
ultimately the kind of segmentation we
want to achieve and on the on the other
end of the spectrum are the block
segmentations where you just partition
uniform in the image and hope that that
sort of provides you with some benefit
and arguably all the other segmentations
that have ever been invented line
between these two ends of the spectrum
and our approach of stable segmentations
and the random segmentations these are
the two most familiar multiple
segmentation algorithms that are there
that currently exist lie somewhere in
between so just for the sake of argument
would like to compare our approach to
the one of the block segmentations to
illustrate that the quality of
segmentation actually does matter and
again empirically we can see that with
block segmentations we don't the beasts
egg we don't achieve as good a results
as we do with stable segmentation but
what is interesting is that even
completely sort of brain-dead segments
that just partitions the image uniformly
outperforms the recognition engine
without segmentation at all even the
simplest grouping of interest points
that are completely order 'less without
segmentation even the simplest grouping
does help the recognition accuracy and
we can see that in these empirical
results here are some examples of both
the stable segmentations and block
segmentations on the caltech and pascal
data set so in the left column of the
original images in the second column are
the segmentations provided by the stable
segment using stable segmentation in the
third column are the block segments and
the final column are the grand truth
results laid
by humans and we can see our results
with stable segmentation match the human
performance quite closely here's some
more examples and in this particular
data set I think this is Pascal the
ground truth is only provided by the
bounding box so the results that we get
with stable segmentations are actually
much more accurate than those provided
by the ground truth bounding boxes
finally the last question is is it
beneficial to perform localization using
stable segmentation the answer here is
also yes because there's this convention
that if you have an overlap of fifty
percent between ground truth and the
estimated bounding bounding box then
it's successful in here there are only
two categories where the accuracy is
less than fifty percent so finally let's
review this part of the talk we've shown
that segment in an image prior to
recognizing its individual parts is
beneficial on the number of segments
doesn't improve recognition accuracy so
single segmentation approaches probably
will not work nearly as well as those of
multiple segmentations next is the
quality of segmentation approach does
matter but even the most trivial
segmentation of uniform partitioning
improves recognition accuracy and
finally it is beneficial to perform
localization using the segment
boundaries so now let's return to our
original problem and this is what we
have by considering individual segments
whether it's the tennis racket or the
person of the tennis court or the yellow
blob we get these labels for the
individual objects but we stick with we
see that there's still a problem the
yellow blog which is supposed to be a
tennis ball is still recognized as a
lemon because we don't put it in context
with the rest of the image
so finally let's go let's talk about
this most novel aspect of this work is
the contextual reasoning for image
recognition when they ask what is
context and context maybe it's any data
or metadata that's not directly produced
by the appearance of the from the image
so for instance it can be any nearby
image data the surrounding pixels around
the yellow I mean around the blue box
all the stuff in red is context it may
be seen information some sort of a
Flickr tag or Google Google metadata or
finally it can be a presence and
locations of other objects the existence
of other objects in the scene is a
tremendously important q and a feature
for accurate object recognition so to
sort of relax for a second let's let's
look at a couple of examples so I show
you this image and then I ask a question
was there dolphin in the picture clearly
the answer is no however however nobody
examined the image very closely but just
because we look at an image and we see a
desert we know that there are no
dolphins so that's a great example of
contextual reasoning that allows you to
do object recognition another example is
let's look at these three blobs that
these arrows are pointing at and try to
reason about what these things are so in
the leftmost example it's perhaps it's a
person trying to cross the street in the
second example maybe it's a car parked
next to a building and then the third
example maybe it's a remote control or a
wallet next two bottles on the desktop
however it turns out that these patches
are all identical pixels and if you put
them in different environments they may
represent completely different things
this is another example of contextual
reasoning for object recognition this
it's been around for ages Biederman has
proposed these five sort of topics of
relations between objects such as the
inner positions between objects does the
object interrupt its background the next
is support that things have to rest on
surfaces they can just float in the air
next is the probability and that's the
one which objects are likely to occur
with which other objects Nick the fourth
is the position where in the image can
something occur can can a car the up top
in the image or does it usually appear
at the bottom of the image and finally
is the familiar sighs what is the size
of a particular object with respect to
other objects in the image and all of
these topics except for number three
have been addressed and vision fairly
well but this inter object interaction
sort of has left been left behind and
that's what we'd like to pick up on in
this talk they exist sort of for this
inter inter object interactions they
exist two kinds of definitions of
context and this is their recent work so
i won't i won't say too much except for
introducing it a little bit on the left
is the scene based context paradigm and
what this refers to is that before
identifying the objects in the scene
would like to understand what seen the
actual image belongs to before we try to
understand what whether there is a
fridge or a coffee maker in the image we
want to identify the scene is this a
beach or is this a kitchen or is this a
football game once the scene has been
inferred then only the objects that can
exist in that particular scene for
instance in the kitchen they can only be
a fridge or a coffee maker or sink only
those objects are considered as possible
candidates for possible objects in this
particular image the flip side of this
approach is the object based context
rather than trying to perform accurate
inference
on the scene of the particular image we
we sort of were sort of agnostic to the
scene we don't care what the scene is
because the end goal we want to find the
objects in it so instead we perform
multiple stable segmentations we give
probabilistic labels to the individual
segments each segment has a likelihood
have been every kind of category such
that the total sum of them is one and
only then we pick the objects from all
these possible combinations only those
that tend to coexist with one another
for instance if we have found a kitchen
if we have found a sink and a fridge as
a combination of all these possible
object labels then we'll say yes these
two are the likely candidates because
they tend to co-exist and only then we
can say once we found this the sink and
the in the fridge then perhaps it's a
kitchen scene so here the reasoning is
sort of flipped in these two different
approaches and in this stop will
advocate for the one that's object based
so for object-based the other existing
other existing uses of context people
tend to reason about objects on a on a
pixel level labels tend to be given to
pixels rather than segments or regions
of the image and this is sort of
intuitive and the nice outcome of this
is that every pixel gets a label and you
get perfect so labeling but the problem
is that you get discontinuous segments
and computation this becomes just a
nightmare because images are so big at
the other end of the spectrum there are
approaches that operate on the entire
image level with the flip side of this
is that the the reasoning of this
becomes 2 to general and sometimes
looking at the background an average
background of all different phases may
just give you blur and not be very
what we introduce is the segment level
reason about context rather than
operating on labels of pixels or reason
about the entire scene as a whole we
partition like I showed earlier we
partition the image into a number of
different segments and then provide
labels for each segment so here the
context based object recognition
approach here this probability
conditional probability on the Left
depends on two factors the contextual
agreements among the categories
multiplied by the actual appearance of
each of the segments so dependent and
and in the appearance is the probability
of a particular segment being a
particular category and the contextual
agreement is the is the likelihood of
certain category labels coexisting with
one another and you may ask well how do
we know what things coexist how do we
know that lemons don't go with with
tennis courts but tennis balls do so
there's two kinds of sources of this
context the most trivial one is just by
looking at the training data we can just
do basic counting of which categories
coexist with which other categories this
requires strongly labeled data and
sometimes it's not available the
alternative is to use Google sets and I
don't know if everyone I assume everyone
knows what Google sets are but the idea
is that if we if we just crawl the web
and look for combinations of words
coexisting in the web we get a pretty
good idea of what things co-occur with
one another for instance if we look if
we just look some some pair of
categories like grass and tree we see
that there is a blue means that there is
coexistence and green means there isn't
we see that grass and tree do coexist
with one another but then for instance
building and sheep don't tend to
co-occur on the web very often so this
is
an interesting source of contextual data
although it's not image driven but
there's there's high correlation between
the two and it's very very interesting
there's two kinds of Google sets the
small and the large ones and then this
work we've used the small ones because
we found that the large Google sets tend
to be too broad and they tend to to show
that everything sort of coexist together
and it's not as informative so now we're
able to perform the accurate recognition
where the label of the tenth of the
lemon gets switched to a tennis ball
because the late the tennis ball lemon
although it may have lower
appearance-based probability but
contextually it fits much better with
the entire scene so now let's look at
some results including context and we
can see that with and these are again
very sort of well accepted data sets the
MSR see data set and the Pascal and we
see that with context whether Google's
using google sets or counting in the
training there's a dramatic improvement
especially in the case of MSR see one of
the problems is that the number of
categories in all of these fairly sort
of toy data sets is so small and that
capturing the interaction between
categories is kind of difficult because
not that many images have the right
categories code co-occurring with each
other in MSR see there are 21 categories
so we can see the improvement is larger
because the these context matrices as
we've seen in the past they're denser so
as we increase the complexity of the
data sets and as for example the caltech
dataset has 256 categories in it as the
number of categories go go up the the
density of these contexts matrices will
only increase and as it increases
importance of context and the utility of
context will be only improved and we
only think we think that the recognition
accuracy will increase even further as
we scale up and the number of categories
increase here's some here are some
examples in the left-hand column we see
an example segmented image using stable
segmentations second column is the
recognition without context third one is
recognition with and the fourth is the
the ground truth and I'll just pick one
example here in the second row we see
that the room the yellow blob in the
blue background was labeled as cow but
in fact including the contextual
reasoning the label was switched to boat
because votes tend to have co-occurring
water more much more often than than
counselor and hand in this in fact
matches the the ground truth here's some
more examples from another data set and
similarly here we can see that this
object was labeled as Cal makes the
person but it turns out that cows
co-occur with people less often than
dogs so the label was switched the dog
because the dog and the cow have similar
appearance and this is the source of the
confusion of course there's some failure
cases for example here cars tend to
originally car and grass were labeled
correctly but due to the construction
contextual constraints carsten to
happened be found on roads more often so
the grass was switched the road and
maybe offliner after the talk I can
answer some question questions how this
can be fixed and see how that performs
now this is all great that we
incorporate the co-occurrences between
objects with one of the obvious things
that we can also include is the spatial
spatial orientation among objects like
the sky has to be above water like like
people have to be above the ground and
things like that so let's let's also to
this source of context let's add spatial
constraints something and the four
constraints that will add is below above
around and inside so below meaning that
the grass is below the water so the red
is below the white above meaning that
the water is above the grass around
meaning that the grass is around the cow
and inside meaning that the cow is
inside the grass and we'll learn these
rules and the distributions of all these
interactions from the training set
unfortunately we can't find the generic
source of information for this like the
Google set so we only have example
experiments with examples from the
training set so now we the model for
object recognition with context is
exactly the same except that the core
occurrences between a pair of objects
are not only considered do these things
exist with one another or but it's more
specific as to where in particular do
the things tend to happen with respect
to one of the above the low inside or
around and here are some of the effects
of incorporating this kind of spatial
context let's also pick an example let's
say this one here in the original image
appearance just based on the appearance
this segment on the top may either look
like sky or water so apparently it looks
more like water so the recognizer labels
as a as water but then when one once it
considers all the other objects in the
image it's which the switches the label
from water to scry because sky is
usually above everything else so to
conclude we've shown that semantic and
spatial contact
is useful and can be applied to
essentially any of the shelf object
recognition approach sources of context
may be either taken from training data
or the generic ones like the Google sets
we've shown that segmentation for
recognition and particularly for
recognition is a useful is a useful step
to take because it provides additional
information like the shape description
and it reduces the amount of noise
that's present in the image by getting
rid of the clutter and in the future we
hope to incorporate parts based models
into our approach because sometimes the
segments tend to break a particular
object into two parts like the top of
the car will be one part and then the
bumper will be the other and combining
the two together is an intuitive step
that needs to be taken and finally sort
of a bigger aspect that needs to be
incorporated into in my opinion all
object recognition methods is the
introduction of feedback into the
recognition system because even as
humans when we look at something briefly
we may miss some detail of the scene but
if we pay closer attention to it we can
identify things that weren't seen
previously so with that I'll conclude
them take questions thanks
you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>