<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Compressed Sensing Meets Information Theory | Coder Coacher - Coaching Coders</title><meta content="Compressed Sensing Meets Information Theory - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Compressed Sensing Meets Information Theory</b></h2><h5 class="post__date">2009-10-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/palSLbrieCo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">is coming in to talk about compressive
sensing which looks at can you get
reconstructions or get additional
information compression doing
projections
yeah so first thanks of all thanks to
Michelle for inviting me over and I was
telling Michelle several minutes ago
that it is an honor to be giving a talk
at Google because I've given talks at
different companies before but Google is
the first company that I'm giving a talk
at and I actually use your products I
use Google Mail as I obviously use your
browser I obviously use your search
engine so this is this is entertaining
now my talk is really going to be about
compressed sensing that as Michelle just
mentioned deals with acquiring signals
from a reduced number of measurements
and my special angle is an information
theory angle where that was my
traditional type of background i'm i've
also done signal processing machine
learning developed hardware in the late
90s so i have a varied background but i
saw that a lot of ideas from information
theory could be useful for compressed
sensing and in 2005 I started bringing
in some of these ideas before I began I
first of all want to thank all of my
collaborators on this work Deana who is
sitting here is a collaborator on a
upcoming paper and additionally I want
to mention that you're invited to
interrupt me at any time so let us begin
I think there's no need in this forum to
say that sensing computation and
communication have had a radical impact
on our society over the last several
decades that's because these
technologies are fast readily available
and cheap now there's been a lot of
progress in individual disciplines that
many of you guys were telling me that
you work on and you ladies you know
signal processing communications
networks obviously computation is a
strong side at Google but nowadays were
really faced with new types of
challenges we have data that is really
streaming at us from all types of
different directions audio data image
data video data financial data I was at
a hedge fund for a year and a half
seismic data you name it and the data
acquisition is actually being performed
on a global scale
massive amounts of data we want to
process that and at the same time our
analysis is being hampered by Moore's
which is kind of slowing down it used to
be that every two years the clock rates
were doubling that's not really the
situation anymore so we have a lot of
challenges but I want to discuss the
opportunities and I think that what is
that I want to discuss the opportunities
and I think that our planet there is a
lot of beauty on our planet and if we
look around and we try to find
constructive solutions there are
numerous contributions that we can make
and the contribution that I'm going to
discuss today is compressed sensing
which is really a new way of how to
acquire signals now if you think about
the old-fashioned way that signal
processing works you have this analog
data that's the input you know it could
be a picture and you used to have a
camera and you would be sampling the
analog data and as long as you sample
fast enough at the Nyquist frequency
which is twice the highest and off
frequency in the data then these n
numbers suffice to later do anything you
want reconstruct the input perfectly now
of course n numbers could be quite a lot
we want to transmit from the encoding
side to the decoding side much less
information we want to compress and a
compression algorithm in many signal
processing out applications really means
that we take these n numbers it's you
can think of it as a long vector and you
compute inner products relative to some
basis vectors in which the data happens
to be sparse most of the numbers will be
very small a few of them will be big and
we're gonna keep the K largest ones
transmit them and that's the whole
encoder now the receiver or decoder side
it takes those K numbers and it forms a
linear combination of the K relevant
basis vectors and that gives us the
approximation X hat to the input X and
what you'll note something interesting
here there's a lot of stuff happening on
the encoder side while the receiver is
actually very simple you have this
first of all this sampler and whereas
Moore's law in the past at least clocks
were doubling every two years or so in
an arc the digital converters clocks are
doubling every six years and they are
simply not fast enough in many
applications another challenge is on the
compression side a lot of computation
and of course I don't need to tell you
that often times the encoder is actually
a battery-operated device such as this
and we want that to consume as little
power as possible be fast etc and
nonetheless this paradigm has worked for
decades this DSP digital signal
processing paradigm and that is because
many types of data are indeed sparse
you'll take a look at this picture from
MIT and here are the appropriate
corresponding wavelet coefficients and
you'll see that most of them are dark
blue now what are these wavelet
coefficients here you'll see the sub
bands with a higher temporal
spatio-temporal frequencies and these
sub bands contain the high frequency
information the details and most of them
are dark blue dark blue means close to
zero small amplitude now at the lower
frequencies you have a lot of
information bright colors red yellow and
you'll notice that most of the
information here is really focused in a
small number of coefficients so we took
a very large number of pixels and and we
translated it down to K numbers it's of
course not only images it's also video
which Michelle deals with it's also
acoustic data this bat it's chirping and
a chirp may sound something like and in
the time domain the x-axis is time the
y-axis is the signal itself and you can
see that the signal is pretty wild you
would need to sample it pretty rapidly
but when we try to represent this in an
appropriately chosen Gabor basis all of
a sudden that x-axis is time the y axis
is now instantaneous frequency and that
bat chirp at any point of time is
dominated by two or three frequencies
very sparse now if you think about the
night
mr. Shannon sampling theorem that serum
is telling you you have the highest
analog frequency in the data and you
need to sample at twice the highest
frequency in order to later process your
input but the thing is that this is a
worst case bound for the very large
class of all band limit signals and it
so happens that for many signals of
interest as I just mentioned we have
more information we have sparsity and
ideally what we would like to do is to
directly sense the compressible
information at a much reduced
measurement rate and that's where
compressed sensing comes in emmanuel
Candace and David Donoho they showed
that sparse signals can be recovered
sometimes perfectly from a small number
of linear measurements there's been a
lot of additional research since their
earlier works in 2004-2005 and the idea
is perhaps a bit startling you take your
analog input X and you take projections
inner products relative to basis vectors
that are white noise so random
projections and I'll tell you later how
this can actually be performed in analog
in some systems at Google you are
probably interested in digital types of
random projections and we will discuss
that later
but all you really need to do is take
these em random projections and analog
each of these random projections is kind
of like a sufficient statistic in my
information theoretic terminology I
think of it as a few bits of information
and if you have enough of these random
projections enough the number of
measurements M needs to be my mildly
larger than K the number of big numbers
that were interested in if you're let's
say for 5 K measurements you often have
enough information to recover the signal
quite well so the encoder side is very
simple all it does is take these random
projections in analog or perhaps in
Google's case in digital the receiver
side in contrast must be aware of the
basis in
your data happens to be sparse and you
run an optimization routine and that's
what I call the decoder and this
optimization routine could be
computationally intense and we've moved
the action in the traditional DSP
paradigm it was happening in the encoder
and in the compressed sensing paradigm
we've moved it toward the decoder now
let's give a bit more details the
compressed sensing encoding and I'll
start introducing my notation I have a
column vector X and it's of length n and
could be a large number but but it's a
sparse column to keep things as simple
as possible for right now I'm going to
assume that these blue boxes K blue
boxes are nonzero and the rest of it is
0 this is of course a simplification and
I'm taking inner product so each of
these rows of the matrix Phi I compute
an inner product between the row and the
column vector X and that's one inner
product one measurement and I accumulate
these inner products into the
measurement matrix Phi and the matrix
vector product Phi times X gives me the
measurement vector Y and once again I
have M measurements and M is mildly
larger than K now for emphasis random
projections and the random projections
are interesting because no matter what
basis you happen to be sparse in you
will still get several bits of
information from each of these
measurements and this leads to the
concept of universality your new
versality to any basis vector in which
you happen to be sparse I'd like to
illustrate that you can actually take
these things in analog this is has
anybody of you in the room seen this
slide okay so nobody well I know that
one person has but she's very modest
this is what we call it Rice University
when I was a postdoc there in 2006 the
single pixel camera if you think of that
projector what perhaps not that one but
many projectors nowadays the way how
they work you have a light bulb and it's
reflecting light which is bouncing off
an array of really tiny micro mirrors
and that array of micro mirrors
is being controlled each micro mirror is
pointing in a different direction and by
controlling their orientations we
control what amount of light gets to the
screen we're doing something which is
analogous to that but kind of duel we're
taking light from an outside screen and
outside scene I apologize and we are
bouncing light off an array of mirrors
onto a single photodiode and of course
we need some lenses in between that
Hardware type of stuff now by
controlling the orientations of the
mirrors I control whether photons from a
pixel in the outside scene reaches my
photodiode or not and that basically
means that I am adding up photons in the
optical domain this is an inner product
an optical inner product in analog and
that's all we do we take an we take that
photodiode and onto digital converter
that is our entire encoder okay now of
course the receiver side the decoder has
a lot of signal processing in an
optimization routine so that's where the
hard work is but these are random
projections in analog and I want to
emphasize a single photodiode so the
thing that I have in my pocket which by
chance does not have a camera on it but
many of you do have these things in your
pockets with cameras on them they have
these million pixel cameras and they're
cheap they're cheap because silicon
happens to be sensitive to light at the
waveforms that are interesting to us so
you can build these two three dollar
chips with a million pixels a million C
CDs but suppose that you're you're
interested in some other type of light
like x-rays or whatever and each pixel
each sensor would cost you a thousand
dollars how are you ever gonna build a
million pixel camera where where each
pixel cost a thousand dollars now maybe
at Google you have those budgets maybe
the defense agencies have those budgets
but very few people very few
applications can build that type of
camera and here we use one photodiode to
give you an idea that this kind of works
here is a 64 by 64 image of the letter R
4096 pixels and you also see a 400
wavelet term approximation which is
pretty good and when we took our
compress sensing system with 1600
measurements and 1600 is 38% of the
number of pixels so the number of
measurements was greatly reduced this is
what we got now it looks pretty fuzzy
but the fuzziness is actually not
because our math is mistaken the
fuzziness is because if you put a real
ordinary camera at the light plane of
where our microarray was this is what
you got a very very fuzzy thing so the
hardware was off and later on after I
was no longer a postdoc there they
improved the camera performs a bit
better but the main take-home point is
you can actually take these things in
analog I would also like to mention not
only the encoder but also the decoder
side the problem is as follows we are we
have this long vector X and we're
measuring it with a matrix that has far
fewer rows than columns and the
resulting measurements Y this is
basically an you know an underdetermined
system we we cannot give a unique
solution there are infinitely many
solutions we have an entire subspace of
possible solutions this linear system
this is an ill-posed problem okay what
do we do well we know that X is sparse
so intuitively what we do we search
within the subspace of explanations for
the most likely explanation and the
notion of likelihood can account for the
types of priors the types of information
that we have about our input X now the
contribution by people such as Canada's
in Donoho earlier in around 2005 was
that linear program decoding can work
very well from a modest number of these
measurements in case in some cases
actually you can get perfect
reconstruction if the measurements are
noiseless and additionally linear
programs are of course computationally
tractable polynomial complexity after
these initial discoveries there
variations along this reconstruction
theme my co-author yet a needle for
example who is with us here Joel Trapp
who is at Caltech probably at the moment
they did greedy types of algorithms
other optimization approaches were also
performed there has been progress I will
tell you about more later but the real
picture is that compressed sensing
changes the rules of the data
acquisition game instead instead of
sampling of the Nyquist rate you can
take far fewer measurements now on the
hardware and software side we have this
beautiful notion of universality no
matter what basis you happen to be
sparse in if you know that basis those
sufficient statistics will allow you to
reconstruct and this universality really
simplifies the design of both the
hardware and the algorithms now on the
mathematical side because we have a
sufficient statistic from each of these
measurements and the bits kind of add up
if you have very few measurements you
can get very little information and you
can do simple stuff like detection or
end recognition if you have a lot of
measurements a lot of bits you can
reconstruct your original input quite
well and these types of ideas in our
opinion might feel the next generation
of signal acquisition devices I've
already told you about new and new
imaging devices I will not mention
distributed source coding algorithms
today I might take that offline with
some of you who might be particularly
interested in video coding and I will
discuss other types of topics and my
emphasis is on how information theory
inspires compressed sensing and I'd like
to pause and also mention that I believe
that at Google you are aware that there
are many types of measurement systems
that are linear we have this y equals
Phi X possibly plus noise type of
problem this appears in many areas of
science and engineering
I will emphasize one of them later on
and you need to think about this is not
just compressed sensing these principles
will appear in other areas and I'm going
to begin by talking about information
theory
bounce some earlier work from 2006 with
sarva-dharman Beranek and very recent
work from last week with downing duel
from Northwestern University and Schlomo
shammai in Israel so originally our
target here in compress sensing is to
reduce you know the consumption of
resources measurements our resource it
can be the rate of an analog to digital
converter or it could be power or it
could be you know the cost of the
hardware resources and this motivated
David Donoho to write several years ago
why go to so much effort to acquire all
the data those n numbers if when most of
what we get will be thrown away we only
are interested in those K large numbers
all the rest is going to be thrown away
now let us start adding complexity into
our model our model considers a signal
entry X and that is the inst location in
X this yellow box and that location is a
product of two things BN which is a
Bernoulli random variable probability
epsilon of being one small probability
of being one and the rest of the time
it's zero so you have these white boxes
which are zeros and you multiply that by
un un follows some distribution PU there
are some minor technical conditions and
the product the product of the two gives
us this yellow box now because you can
take various values it can be again
minor technical conditions any PDF that
you want instead of blue boxes we have
colored boxes so we've made this a bit
more interesting and you'll note that
the distribution of P X because the
Bernoulli has a high probability of
being zero I have this Delta at the
origin of the PDF for X and I have with
small probability P you and that small
probability is reflected in the smaller
amplitude and I'd also like to mention
that epsilon could actually be one
and in that case xn is equal to un and
we have a completely non sparse colored
input we can do this our math can do
this and an issue which is crucial to
understand is that typically in most
compressed sensing systems maybe not the
ones that you're interested in at Google
per se but most systems will be analog
and analog systems i've designed
hardware in the 90s in the late 90s and
all citizens have a lot of problems we
have measurement noise we have
nonlinearities we have cross stop
between wires I've seen these two things
honest Colossus copes with my own eyes
it's a very messy problem and
consequently for ease of analysis we are
going to assume in our analysis Gaussian
noise and nonetheless these results can
be generalized to other noise
distributions so one step further in
complicated and complicating my problem
I have this matrix times vector Phi X
instead of calling that Y I now call it
Y zero the noiseless portions of the
measurements and I multiply it by the
square root of gamma I'll soon tell you
what gamma actually means and I have the
noise I add a additive white Gaussian
noise vector unit norm zero variance and
that is the vector Z and if you think
about it it can easily be shown that as
long as the columns of the matrix have
unit norm on average we miss might be a
random matrix as long as they have unit
norm then the signal-to-noise ratio of
each of these measurements will be gamma
that's the operational meaning of gamma
now in 2006 I showed the following
result I had this insight that you have
the noiseless measurements y0 and
they're being corrupted by noise and
that means that at the output you have Y
and this is an information theory a
channel now what is the what is the
multiplication by the vector Phi
this matrix vector multiplication is a
type of encoder a compression algorithm
in information theory we say that we
compress the source and then we apply
another coat to protect against
corruptions of the channel the
compressed sensing measurement the
matrix vector multiplication is nothing
other than these two encoders the
compressed sensing reconstruction
algorithm or decoder is nothing other
than redoing the channel code and
redoing the source code compressed
sensing is a communication system and
this led me to understanding that we
have a measurement channel and each of
the measurements is giving me providing
me some additional information about my
input the more measurements the more
information and that led to a very
simple bound on the number of
measurements required
so consider a sparse signal X and it's
rate distortion function is Rd I'm soon
going to explain what that means R of D
we want to achieve distortion D it can
be any distortion metric that you want
mean square error mean absolute square
error polkadots whatever okay deep
distortion metric and we want to operate
with signal-to-noise ratio gamma and
what we're really interested in is
abound on how many measurements we need
we use the limit where the number of
measurements m and the length of the
input n they're both blowing up to
infinity together but the ratio is held
constant Delta and Delta will be
positive in most problems of interest
and this is the bound in the numerator
we have the rate distortion function R
of D means the number of bits per input
element that are required to achieve
distortion D in the denominator we have
the number of bits that each measurement
gives us the capacity of the channel
very very simple result based on this
early single letter expression a single
letter expression in information theory
parlance means you blow up the size of
the problem m and n go to infinity
together
with the same ratio everything scales up
based on this single letter bound there
were other authors with similar single
letter bounds and I'm I've listed quite
a few but there are many more papers but
these are all bounds and our goal with
dong Ling Kuo and Shlomo shammai was to
give a precise characterization out of
bound but a precise characterization of
optimal compressed sensing optimal in
the sense of the ultimate performance
now we have these end numbers entering
our system the matrix vector
multiplication and the channel noise
let's just think about that as yes this
is basically in information theory
there's a there's a very well-known
theorem called source channel separation
the put things is informally is possible
it means if if you if you give me a
thousand bits and each measurement each
usage of the channel gives me two bits
and you need a thousand bits then we
need 500 measurements okay that's a very
well-known theorem does that help your
help address your concern this this is
completely a trivial result in
information theory information theory
the whole contribution was to set up in
the previous slide to explain that
compressed sensing was actually a
communication system that was my
contribution this model over here the
theorem is trivial for the information
theory community now from the compressed
sensing angle on the other hand the
contribution is we finally have or had
in 2006 a very early and course bound on
the measurement rate required
okay yes what does the term single
letter okay single letter means that we
want one term that as we blow up the
size of the input and goes to infinity
we have one expression R of D that tells
you how many bits you need per input
element to achieve distortion per
element of D so the distortion will be
proportional to n and the number of bits
will be proportional to n but the single
letter expression means that everything
both amount of distortion and the number
of bits will be going linearly growing
linearly together okay does that clarify
things more questions coding yes yes and
no no this the single letter is
basically the limiting performance of
how many in in my specific case of R of
D how many how many bits per input per
entry per number in the input of that
every box of that long vector X okay how
many bits to achieve this torsion D for
that one block yes yes
okay so there were earlier bounds we
wanted to give a characterization a
precise characterization and we have
this input of length N and the
measurement matrix and the noise think
about it just as your channel it is
assumed here that you know the
measurement matrix and it is assumed
here that you know the basis in which
the data happened to be sparse under
these assumptions that is expression
okay so those are assumed to be known
okay so you know you know the
measurement matrix you know the basis in
which you happen to be sparse you know
that the noisy measurements Y and given
them you want to reconstruct the exit is
this clear okay wonderful okay so given
this measurement channel including the
matrix vector product we then have the
noisy measurements we are given just as
mu to asked we are given the measurement
matrix Phi and the sparsa fiying basis
and given the data the noisy I'm sorry
the measurements the noisy measurements
we ideally would like a posterior to
enable us to estimate this ends location
xn and our estimate will be called xn
tag we want to know ideally ultimately
we would like to understand what can be
said about that
ence location the quality of our
estimate xn prime given the noisy
measurements and the matrix now ideally
if we could compute the posterior that
would be a sufficient statistic for
everything that you want to know minimum
mean square error absolute error
getting the support set exactly right if
the signal happens to be precisely
sparse anything that you wanted to know
this would be a sufficient statistic but
this is a problem of n dimensions and it
is very complicated computationally to
compute the posterior not going to
happen especially when n blows up to
infinity now what we want is a very
simple characterization of how the
posterior looks and the quality of these
metrics how well we can basically
reconstruct using different performance
metrics and we were approaching this
problem using a large system limit where
once again m and n number of
measurements length of the input blow up
to infinity and the ratio is held
constant and of course positive for
problems of practical interest and now
this is the main slide on the one hand
you have the system this entire vector
and you have this big channel and you
have the posterior and we would like to
estimate xn prime xn tach on the other
hand you have this scalar description
one input goes through some channel goes
into Z then we have a posterior and we
estimate X Prime now suppose that you
Genie told you everything else about the
the input except for xn you were trying
to estimate the enth location given
information on everything else so
basically you could subtract off
everything else and you would be having
one column of the matrix times that nth
location plus noise and of course you
would be multiplying by square root of
gamma and you would have this single
number times square root of gamma plus
unit norm Gaussian noise and the
posterior would be very clear this is a
scalar Gaussian channel one input one
output very clear what we've shown
formally is that
when you do not know these other guys
you do not know the rest of the input it
so happens that each of them you
reconstruct it with some error term of
course it's going to be a noisy
reconstruction in practice when we have
measurement noise and a finite number of
measurements and each of these we will
be subtracting them off but with noise
and the accumulation of all these noises
intuitively this is kind of like the
central limit theorem we will have
additional Gaussian noise added into the
problem and one way of thinking about it
is I will be multiplying by the same
square root of gamma and adding Gaussian
noise with higher variance another way
of thinking about it the way how we
present our results is we multiply by
square root of gamma times Zeta where
ADA is less than 1 so we are multiplying
by less and then we're adding our unit
norm Gaussian noise and that is the
entire posterior in the large system
limit basically the bottom line is that
estimation quality given the noisy
measurements Y and the measurement
matrix Phi will be just as good as a
noisier scalar observation and an
important point here is that ADA the
degradation term can be easily
so-to-speak in quotation marks computed
how easily well we have this fixed point
expression and when you solve it where
the MMSE is basically the minimum mean
square error for some random variable
that appears in this problem this this
thing can be solved and you know one
second okay I mean this is trivial ok we
can compute the degradation and the real
take-home point once again a scaler
mysterio
the posterior looks just like a scalar
Gaussian noisy observation but degraded
with more Gaussian noise now I must I
must admit some you know caveats here
our analysis is not truly rigorous
because we're relying on the replica
method which is widely accepted in
statistical physics we're using a
symmetry
sumption and these types of assumptions
they've invalidated numerical numerical
numerically to be almost precise in
various communities but nonetheless
these are assumptions and perhaps not
exactly precise and they've been used
for CDMA detection as early as 2002 by
Tanaka and by my co-author dung lingual
and his PhD advisor Sergey overdue later
on in 2005 so this is not a serum it's a
claim based on well-established results
that's one caveat I also want to mention
that there has been a very interesting
recent analysis by Ron GaN at all and
what they did they took that paper by
goooooo and Verdoux and based on that
they showed the minimum mean square
error estimator quality not the
posterior but this minimum mean square
error as I mentioned earlier the
posterior enables us to give you any
metric that you want the posterior is a
sufficient statistic additionally what
they did which is very interesting they
extended the analysis of guell and ver
due to additional compressed sensing
algorithms in particularly the lasso
which I'm assuming that many of you are
familiar with in the machine learning
community now another result that we
have that was result number one
decoupling and I'll take this a bit
quickly you basically have a fixed
number L of locations in your input and
your input is becoming longer and longer
but a fixed number L of locations and
you're asking not just the posterior of
xn but the posterior of L locations L
fixed locations now this is a posterior
given the noisy observations Y and the
measurement matrix Phi and you'll note
that in our problem in our setting Y and
Phi are themselves random so this is a
posterior of something random this is a
complicated Beast but it turns out that
as you scale up the size of the problem
this complicated beast takes the form of
a product and once again convergence and
distribution in the large
system limit to a product of L scaler
degraded Gaussian observations and once
again you know we have a take-home point
here the individual posteriors we have L
fixed numbers that individual posteriors
are asymptotically independent the
interference in location number one does
not really matter much when you look at
location number two and I will
demonstrate this graphically later
we have great interest in sparse
measurement matrices and that interest
arises because the old-fashioned way of
doing compress sensing with these dense
Gaussian matrices the lot of people
studied these matrices because they are
dense the matrix vector product requires
a lot of computation and in contrast we
considered a sparse measurement matrix
we were inspired once again by
low-density parity-check codes which
have been used in the information theory
community and we have mostly zeros in
our Phi matrix the Phi matrix mostly
consists of white zero boxes but a few
red boxes we have probability Q of
having any single red box and given that
you have a red box we have a
distribution P Phi for that red box once
again minor technical conditions Q needs
to be not too small not too big as the
size of the problem grows to infinity Q
can become very very small so this will
be a very sparse matrix as we scale
things up but the conditions are not too
stringent and the bottom line here is
that each row will on average contain n
times Q of these red boxes and because
of that all of a sudden the matrix
vector product is very rapid and this
gives us fast encoding and decoding now
in 2006 with savathun Beranek we started
thinking about using belief propagation
to solve this problem belief propagation
once again
is well-known from the
information-theoretic community I was
speaking with Michelle earlier and she's
very familiar and we represent the
measurement matrix by a bipartite graph
which you can see on one side of the
bipartite graph you have the
measurements y the noisy measurements
and on the other side you have X the
input which we are attempting to
reconstruct now what we're doing we are
estimating the real valued input
iteratively by passing messages back and
forth from the measurements to our
estimates of the posterior back to the
measurements and so on and gradually
this converges and we take into account
our statistical familiarity of first of
all the input and secondly the noise our
specific moment implementation relies on
nonparametric belief propagation that is
belief propagation for real numbers
several people quite a few people have
been working on that recently this is
again an active area of research my
collaborator Danny vixen Sommer Federer
and Shelvey have another very
interesting paper from last year active
area now why am i mentioning these
sparse matrices because it so happens
that under those mild technical
conditions the same result as before
degraded observations with the same
degradation ADA identical mild
conditions on the degree of sparsity not
too sparse but you know just sparse
enough so sparse matrices are good and
additionally and this is actually a
theorem this no longer relies on these
replica method assumptions theorem proof
belief propagation is asymptotically
optimal for sparse measurement matrices
so you should definitely if you have the
ability at Google to use to choose your
measurement Maitri matrix use a sparse
one and belief propagation will give you
the best reconstruction that money will
buy and let us now use the belief
propagation to illustrate some of these
results first of all decoupling I have
two axes here this is
the posterior for the second entry of
the input and this is the posterior of
the first entry of the input I had an
input the vector X of length 500 and I
took this input in the first two entries
it was a Bernoulli input ones and zeros
the first two I fixed them to one for a
problem with length 500 fixing them two
locations to one doesn't really change
much because 10 percent of the numbers
are once so changing two doesn't really
matter much doesn't really change the
statistics by much and if you think
about it well of course we would like to
estimate a one for both of these
locations but regardless of the
posterior that was estimated for the
second entry of the signal the posterior
for the first entry has this same type
of form you can see the flatness of the
surface where the vertical axis is the
density the density meaning a histogram
over thousands of simulations of this
length 500 problem okay now if you look
at this histogram this density putting
aside the decoupling you'll see that
most of the time it's pretty heights
near one so most of the time the
algorithm thinks oh yeah it's close to
one it's probably one if we were forced
to make up a hard decision sometimes the
algorithm makes a mistake and it thinks
that it's probably zero next I want to
illustrate the performance of belief
propagation compared to other algorithms
on the x-axis we have the number of
measurements which begins by a hundred
it went up to approximately 800 this is
for a problem an input of length 1000 my
input was not Bernoulli but it was a two
component mixture Gaussian okay so this
is a more realistic input and the y-axis
is the minimum mean square now clearly
the x-axis num the number of
measurements when you have more
measurements you improve your error this
this is obvious to everybody
what I want to illustrate here is that
compressed sensing via belief
propagation actually works and we have
this these various curves and these
various curves are different algorithms
of course more measurements better
performance lower square error and these
curves include iterative heart / holding
which is very fast you'll see that soon
ko samp which is Deanna's algorithm it's
also quite fast
it's a greedy algorithm these two are
related algorithms that's why their
performance is similar we have the
original linear program which was the
innovation of compressed sensing and we
have GPS R which is an implementation of
fast implementation of the lasso and the
last cell is a relative of linear
programs they happen to be slightly
better belief propagation the red line
you can see that it's clearly better of
course it's better because it's
asymptotically optimal when we wrote the
paper about belief propagation we had no
idea this this result is from the last
we were we became aware of this very
recently but belief propagation will
give you the best reconstruction that
money will buy if you're allowed to use
sparse matrices how fast is it well the
belief propagation is n poly log n
square of log N and I took a problem
where I am varying the length of the
input end and I fixed the number of
measurements as 40% of n and I'm
comparing different algorithms where the
x axis is the length of the input end
and the Y the Y the vertical axis is the
time in seconds that it took to run the
algorithm and you'll note first of all
that this very sharp slope that reaches
3000 seconds for problems of size 3000
that is linear programs linear programs
have cubic complexity it was good as an
initial demonstration that compressed
sensing has promised but it's perhaps
not very useful in the real world what
about these other algorithms as I
mentioned earlier in a hard threshold in
the fastest algorithm when n is 10,000
it is 32
times faster than ours that's pretty
good and we have these other competitors
koh-sama Deanna's algorithm and co SAP
is pretty good its slope is pretty good
but when we reach 10,000 were almost as
fast and why because we use a sparse
measurement matrix and they are not
making an assumption of sparsity they're
just doing matrix vector multiplication
quadratic time now if I used the sparse
matrix format in MATLAB this would
slightly improve their performance but I
wanted just to take their code unaltered
I did evaluate their performance with
these sparse representations in the
MATLAB okay what about the other
algorithms that Co SAP and the ihd are
greedy
well the linear program is terrible we
just discussed that and we have the GPS
R which is a fast implementation of the
lasso and for problems of size 10,000 we
are 3/4 times slower but we're getting
there another point of interest is that
for problems of size 10,000 R code it
might be n poly log n but it runs for
half an hour that doesn't really make
sense to any of you does it the problem
is that in this n poly log n I have a
constant which requires me to run an FFT
fast Fourier transform for vectors of
length approximately 500 and in the
signal processing community we are
educated to believe that the FFT is very
fast but when you have a length 500 FFT
it takes quite a few microseconds and
when you take n poly log n times quite a
few microseconds it adds up to half an
hour this is not something even with
hardware implementations and parallel
hardware and whatever this needs more
work and now I'll tell you about fast
yes
the input yes
and as the length of input yes I I will
I will soon I will soon get to those
algorithms okay I will soon get to those
algorithms so again half an hour problem
of size 10,000 speed it up in hardware
parallel whatever it'll still be too
slow and that's why we're yes another
question yes you need to integrate that
everything every statistical knowledge
you have about the problem yes no the
FFTs is because in the belief
propagation it requires convolution and
FFTs enable us to perform convolution
very rapidly yeah yeah doing convolution
the old-fashioned way takes a lot of
computation even when it's sparse eeeh
more questions so we have an interest in
these very fast algorithms as mootools
pointing out algorithms that are sub
linear in the length N and we will take
a specific setting where once again I
return to these perhaps colored boxes
instead of blue but my signal is
precisely sparse the white boxes are
zeros this is not what I told you before
of a thing that could be you know any
input but this is a strictly sparse
input where the white boxes are zeros
and I have neglected away the noise toy
problem but if you think about it at
Google you'll have your video data you
will have those n numbers 10 million
numbers per second and out of those 10
million numbers only hundred thousand
wavelet coefficients or whatever
coefficients will be large and the rest
you will force them to be zero this
might actually be a problem of relevance
to Google ok how do we look at this
problem how do we solve it very fast and
I'll soon tell you how fast is fast
here's a toy problem why the
measurements equal Phi the measurement
matrix times the input the vector X and
I have put question marks for the vector
X and I'm going to ask directed
questions at the audience to help me
solve the vector X so one of the
measurements is a zero and I want to
remind you that X is sparse and I'm
assuming that the non sparse locations
are real numbers and not ones or twos
but in integers and I did use indeed
integers for this toy problem but you
know real numbers so what does it mean
can anybody tell me probably mu two can
tell us what does it mean of a
measurement is exactly zero for this
problem any volunteers will do what do
you think any other volunteers okay so
any that is definitely correct any any
other guesses taking into account that
yes your muting again
excuse me I'm allowing negative numbers
but these are real numbers a continuous
distribution real numbers and this is
exactly zero it's not quantized to zero
exactly zero what do you think exactly
the second and third elements which I am
measuring are zero so now I'm gonna put
down these two will be zero and I had a
bipartite graph and I am reducing the
bipartite graph okay reducing the
bipartite graph means these gray areas
are no longer necessary the first
measurement which was zero is no longer
necessary the second and third columns I
just figured out that they correspond to
zeros they're no longer necessary okay
graph reduction second observation is
that we have these two measurements
which are equal to one once again in my
toy problem I actually use the integers
with think to yourself I have identical
measurements on real numbers identical
measurements no quantization what does
it mean identical measurements excuse me
the second to last value yes so Michelle
was pointing out that in the second row
it is picking up these two numbers the
third or rather the fourth and the fifth
number are being picked up and in the
next row in the third row we're picking
up the first and the fifth and the fifth
is common to both and v will be equal to
one the fifths number will be equal to
one that's this question mark and the
first number will be zero and the fourth
will be zero once again I've reduced the
size of the graph the last question I
have this very small system of equations
left the measurement equals four and I
have one in the measurement matrix what
is the question mark
okay let's see what's here guys it is
actually three like some audiences would
guess four but this is a trick question
what I forgot to tell you what I forgot
to tell you and I forgot to tell you you
know by design is that when I figured
out that this guy that I'm measuring in
the last row is a one I need to subtract
it off and this phase of the algorithm
was actually discovered in the last
several weeks by a fourth semester
undergraduate okay this last phase these
are the three parts of the algorithm
very simple algorithm that you guys just
redesigned by yourself you redevelop the
algorithm earlier I was talking about
ten thousand in half an hour my fourth
semester undergraduate one hundred
thousand in 15 seconds okay now let's
talk about the theoretical results the
number of non-zeros per row needs to be
proportional to the links yes
because these come from a real valued
continuous distribution and the
probability that they will exactly be
minus one point seven three two and plus
one point seven three two the
probability of that is 0 because we have
noiseless measurements okay so when you
have noiseless measurements the
probability of things summing up to zero
by mistake is zero now when you have
only eight bits on your measurements you
have a likelihood of making mistakes but
when you have 64 bits everything is
going to be fine again this is a this is
this is a toy problem the input is
exactly sparse and but once again like
continuous distribution for the input
for the non zeros if it's continuous the
probability that you have multiple
numbers summing up to exactly zero it's
not going to happen in practice again
you know 2 to the 64 representation
level so what's the probability
to make insurance kind of fear to blow
up and it's kind of so the real the real
question is how many bits do you store
your measurements at and you know so if
so if Google will do this type of stuff
with eight bits maybe occasionally you
will have mistakes with 16 bits you will
rarely have mistakes with 32 bits it'll
almost never happen okay okay
no theoretical results the number of
non-zeros per row needs to be the length
proportional to the length of the input
divided by the number of blue boxes and
the number of measurements needs to be
the number of blue boxes times some log
factor as I said before typically five
times the number of blue boxes so
instead of a hundred thousand numbers
that Michelle would send for video maybe
five hundred thousand numbers but our
encoder is n log N and our decoder is
somewhat larger than K sometimes some
log terms and Mootoo would probably
notice that this is sub linear in N K
the number of blue boxes is much smaller
than n but I have a caveat here this is
the actual graph reduction step and it
so happens that setting up the data
structure takes n log n the setting of
the data structure requires to have
locations for each of the Reds locations
in the matrix now if in my cell phone I
would have this algorithm pre-programs
and the data structure would be set up
it would be n log n I'm sorry if it's
already set up it would be K poly log
something if I need to redo this data
structure from scratch it is n log N and
this is a subtle point but this we need
to be fair with the community and
despite the n log in problems of size
100,000 run in 15 seconds now why do I
think this is
particular interest to Google because
Google obviously does distributed
content distribution you have servers
all over the world and for my problem of
a hundred thousand where I have five
thousand blue boxes if you give me
eighteen nineteen thousand measurements
I will be able to recover the problem it
doesn't matter what servers in the world
will send me those eighteen thousand
numbers I will be able to solve this
problem some of your servers will crash
some of the tcp/ip will not arrive on
time when if you get me eighteen
thousand numbers second I will give you
the hundred thousand this is a matlab
implementation it can be improved in C
I'm sure that your programmers are even
better than my brilliant fourth semester
undergraduate now once again toy problem
strictly sparse noises measurements now
I have a third topic on distributed
compressed sensing which I indicated to
Michelle that I will skip if people will
be interested we can do that offline
later but let us go to the part of the
talk which kind of gets back to what
might interest you this is not just
about compressed sensing this is about
linear measurement systems we're
interested in the unified theory for all
kinds of different systems that measure
things linearly and not just the theory
but also fast algorithms
what is the the contribution of our
algorithm first of all we are Deanna and
I are extending it to noise and secondly
as I mentioned a very simple algorithm
and that's why we find it very pleasing
you know no no absolutely are this out
is this this algorithm no belief
propagation knows no nothing I describe
the entire the log factors are a bit
different they they might be competitive
I'm not really sure let's take this
offline ok so going back yeah yeah no
this is these all of these results are
competitive with each other like there
are plenty of fast algorithms and as I
mentioned just setting up the data
structures and login I have no need to
lie about that if it's pre-programmed
it's it's sub linear and n if it's not
pre-programmed you need to set up the
data structure now linear measurements I
was with a hedge fund for a year and a
half and let me describe approximately
how numerous hedge funds are trying to
predict stock prices so in their world
they have family and French in the early
90s described a three-factor model where
you have different factors that drive
thousands of stocks the returns of
thousands of stocks let's say the stock
market index goes up by 1% today there
will be one there will be some companies
that on average move more aggressively
than the market all other
other things being equal they will move
one-and-a-half times faster so one and a
half percent other companies are kind of
you know relaxed companies don't want to
give any examples but let's say they
only move one half of a percent all
other things being equal that's one
factor other factors are relative to
earnings are you an expensive company or
a cheap company for example I think it's
well understood in this room that two
years ago relative to earnings Google
was an expensive company but that's
because Google grows very rapidly so
there was a reason for that there was a
reason why Google was expensive so there
are numerous factors that are used to
attempt to predict stock prices and the
way how it works you have X the input
and the input here which you are trying
to estimate is the factor returns how
much each of these factors drives the
stock prices the matrix Phi is basically
every row of the matrix corresponds to
one stock in your training data in the
past so many months of data or perhaps
many seconds of data for fast trading
firms for many many companies so this
could be millions of numbers Y or it
could be gigabytes or terabytes and
based on this training data and you know
that expect the exposure you can compute
the exposure of these companies to the
different factors you want to estimate X
now of course you have I would not call
it noise I would call it the unmodeled
portion of the stock market and that is
Z now in their problem the unmodeled
portion is typically very large very
difficult to predict the stock market if
it was easy
any of us could make a hundred percent a
year now let's compare compress sensing
to finance in finance if you can compute
those factor returns then you can
predict the future if you can compute
them you predict the future you invest
on it you make money for your
everybody's happy so the goal is to
estimate the factor returns X
conditioned on knowledge of the
exposures Phi and the data the past
training data Y we have compressed
sensing we have finance in compress
sensing Y is very short in finance Y the
training data is very long in compress
sensing X the input is very long but
sparse in finance X the number of
factors is perhaps a few dozen in
compress sensing it is implicitly
assumed that the noise is minor in
finance the unmodeled portion of the
data is massive and that's why although
you have a lot of data it's very
difficult to predict the market because
the unmodeled portion of the data of the
past training data stock returns you
have a lot that doesn't really sit
inside your model now compressed
messaging of course we talked about that
finance not only is the data in it noisy
and because of that you need a lot of
measurement you have nonlinearities here
you have stock market investors whose
behavior is changing over time this is a
non-stationary environment complicated
problem and that said we understand that
compressed sensing is a linear
measurement system finance we have some
people who use linear models medical
imaging tomography it can be
approximated linearly same with seismic
imaging in the oil industry they attempt
to see what's underground using things
that can be approximated by linear
measurements Google does information
retrieval what I'm getting at is that in
these different areas of Science and
Engineering we have linear measurement
systems the goals are fairly common we
want to minimize resources once again
speed of the analog to digital converter
cost of the hardware power bits you name
it we want robustness to imprecisions in
our model and robustness to noise and
we also want things to run quickly now
some people call these inverse problems
I viewed this as a slightly different
information theoretic type of problem
but where I want to go to is a theory
and a way how to really really rapidly
process extract information from these
types of linear measurements thank you
very much
questions
okay
anybody who wants to ask and definitely
join us for lunch I imagine yeah you
could I think you could use tailor
approximations as some of the columns if
you had via Taylor approximation some of
the columns would be measuring it as a
square of something I think that that
could be done to do some non-linearity
I've I've not really thought about that
in detail but you know when I say Taylor
approximation that gets you thinking
yeah I I think that we need more
measurements I don't really know I'm
aware of fountain codes I haven't gone
into the details I don't know what the
details</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>