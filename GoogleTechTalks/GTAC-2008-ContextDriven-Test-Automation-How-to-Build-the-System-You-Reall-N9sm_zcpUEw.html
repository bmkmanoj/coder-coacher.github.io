<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2008: Context-Driven Test Automation - How to Build the System You Reall... | Coder Coacher - Coaching Coders</title><meta content="GTAC 2008: Context-Driven Test Automation - How to Build the System You Reall... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2008: Context-Driven Test Automation - How to Build the System You Reall...</b></h2><h5 class="post__date">2008-11-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/N9sm_zcpUEw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my talk is called context-driven test
automation how to build the system that
you really need I'd like to start with a
little bit of background and where the
idea for this talk came from back in
January of this year I was asked to lead
a team to examine all of the test
automation tools that we had f5 network
has a lot of different products and a
lot of the product teams had built their
own test automation systems and we
wanted to take an inventory and try to
figure out where do we have overlap
where do we have gaps and how can we
avoid duplication of effort as we're
expanding our toolset going forward so
we looked at eleven different automation
systems that were being used around the
company and we talked about the
capabilities of all of these systems and
we thought that there were common tasks
they're basically six common tasks that
all of these tools had to deal with you
know you had to have some way of
organizing and distributing your tests
and controlling what tests run you had
to have some way of doing up setting up
your test case the most the thing that
absolutely everybody had to do and had
to handle in their tool to configure f5s
products the first thing you have to do
is login so in order to do any tests you
had to log into the box you needed your
test case needed to have a user ID
password and an admin IP address so that
you could find the box out there and log
into it so they all had some way of
getting this information then you have
to actually execute the test you have to
evaluate the results you have to do some
sort of tear down and clean up and then
you need reporting all the tools had the
ability to send out email and show you
what the results were of this test run
and the CTO was not really happy when he
learned that we had written that piece
of code 11 different times and we're
maintaining at eleven different places
so then we spent some time talking about
all of these tasks test distribution and
run control some of the tools were
pretty sophisticated about this they
could say you know there was data within
the test
the test framework had the ability to
look at that data and say this
particular test can run on this piece of
hardware and this piece of code and this
release of the software but it can't run
on this piece of hardware so if I'm
running on this piece of hardware I'm
not I'm going to skip that test other
tools had no real way of doing that
automatically you had to go in and
manually edit the set of test cases that
you were going to run setup and teardown
similarly there was a pretty big range
some of the tools took care of a lot of
the work in setting up and configuring
the box and getting it into a particular
state before you ran the test and some
of them left everything to the test case
test execution and verve and
verification for the most part all of
the tools left the actual execution of
the test of the test case and the test
greater there is one tool that has some
capability for data-driven test you
define the data set and then the tool
writes the actual executable code but
for the most part that was left in the
test writer and verification was another
spot again where there was a lot of
variation for some of the tools was very
simple to tell whether or not a tested
passed you got you checked her single
return code or you looked at a single
value in the database other tools and
other tests were very complex you had to
analyze multiple streams of HTTP traffic
or even look down and grab individual
packets from the network flow to figure
out if the test had actually run
successfully and reporting as i
mentioned everybody had a system for
emailing out results when the tests were
done some of the tests had web gooeys
for examining the results looking at
historical results looking at trends and
some some didn't some didn't bother with
that and then you know so far so good we
all agree that these tasks these these
are the tasks that matter that we're all
doing and when we started trying to
discuss the relative importance of the
tasks and what we should work on next
and how we could share and what
requirements were we started argue about
things and that really surprised me I
thought we're all doing test automation
here we're all do
basically the same thing so this should
be easy to agree on it really wasn't we
didn't agree on how you should store
results now some tools used a database
and kept a lot of detailed information
some tools didn't save anything you know
all they had was a text file with a
bunch of pass/fail results we didn't
agree on the proper scope of an
automation framework some tools provided
a lot of helper functions and made it
much easier for the test writer to write
his test case other tools provided
almost nothing all they would do was was
execute the test and there was one
developer who said that you know these
one tool developer who said these test
cases that that your system has provided
a lot of help to these are terrible
because now that test is wedded to the
framework you can't run it outside the
framework and the first thing I do when
a test fails is I take it and I go over
the developer and I've run it on his
hardware so you guys have screwed
everything up by doing it this way and
for me you know my personal breakthrough
on this the moment when the light bulb
came on was in an email conversation
where the developer said his test has to
run outside the tool that's his number
one requirement and I looked that email
only thought what what are you talking
about that's not a requirement I don't
need that and I had to stop and take a
step back you know his tool has been in
production used for a year they're very
happy with it and ain't my job to tell
him that's not a requirement so you know
I had to stop and think about this and I
realized that we were doing different
things you know we were approaching the
problem of test automation from slightly
different directions with slightly
different intentions we had different
contexts for the problem and once I
realized that that we were trying to do
different things rather than the same
thing then well no wonder we don't agree
so we talked about this fact and we kind
of all took a step back and looked at
the problem from a higher level what are
what were each of the teams trying to
accomplish with the different tools that
they had built and as we started looking
and this and thing about the context we
came
with a few questions that really seemed
to be key to determining what your
context was and what your tool was going
to do and how it should work the first
question is who's writing these tests
what do they know how to do what is
their day-to-day work flow and how does
writing tests fit into what they do
every day and how does looking at test
results fit into what they do the second
question is who's looking at the results
is it just the people writing the test
or there are other people looking at it
what other information do they need to
go with the test results what level of
detail are they looking for the third
question what decisions do these results
influence this was something that we
didn't talk about originally but cain't
became clear to me a few months later as
we were getting close to a major release
point we were we were taking code that
was being developed by different product
teams in different countries on
different code branches and having to
integrate them all together and we chose
to use a couple of these test automation
tools as quality gates so you had to
pass on the X BBT before you could merge
your branch into the trunk so that was a
decision that was being influenced by
the results that came out of a tool
and looking at all of these things and
the tools that we were we had built and
we're using we came up with four
contexts that we're using our setting
and we called these contexts the
individual developer context the
development team context the project
context and the product line context at
the individual developer context that
tests are being written by developers
the developers over the ones that are
looking at the test results and the
decision the primary decision that we're
influencing there is code check in if
the tests don't pass you don't check
that code in the next context is the
development team context and in this
context the test may be written by
developers or testers the developers and
the testers are probably both looking at
the results and you may have program or
project managers looking at them also
and this is used to decide whether or
not to merge a branch and where we are
in terms of being ready for release so
you have to pass these tests in order to
do a release the third context is what
we call the project context and in my
experience this is kind of the default
context this is what people are usually
talking about when they talk about test
automation and that's what tripped us up
most of the people in this group were
testers and test managers and we were
thinking about the kinds of automation
we write and we assumed everybody was
talking about project context automation
but there were some developers on the
team that we're talking about other
contexts and if it hadn't been for them
we probably wouldn't we wouldn't have
had the initial problems in having a
conversation but we would not have
realized some of the things that we
figured out later at the project level
now the tests are written by the testers
there looked at by the testers and use
them to see have we met our project
milestones yet and are we ready for
release and finally there's what we call
the product line context and the
important the distinguishing trait of
product line is that you're testing code
that's already been released it's out in
the wild in production use these tests
are typically written by testers the
results are looked at by testers by
program managers sometimes by senior
management
and you're using these tests to gate
patches and updates and maintenance
releases one of the interesting things
that we saw was the tests often are
useful in multiple contexts so you may
use the same test in the development
team level at the project level and of
the product line level and the
difference really is the framework that
you're using to control how those tests
are run when they run and what kind of
reporting you do on the test so let's
talk about some questions specific
questions you might be trying to answer
so will the refactoring i just finished
break anything this is the kind of
question that you're trying to answer
with your individual developer
automation we actually didn't spend a
whole lot of time talking about this
because as i mentioned most of the
people in the task force we're testers
and test managers and we don't have as
much experience dealing with this kind
of test but the most common example is
unit test you know j unit and unit
whatever technology you're using
characteristics of these they need to be
very fast ideally here every time you
you save your code you're running these
tests again so you've got a they've got
to come back with an answer in seconds
or no one's going to bother your testing
some very small discrete piece of
functionality you know you may be
testing a single procedure in an API
writing these tests you really have to
know the internals of the code and these
tests should be considered part of the
code deliverable if you haven't written
these tests and delivered them you're
not done with your code yet there's a
lot of good literature about this I
personally am a big fan of Gerard Gerard
Meszaros book X unit test patterns for
figuring out how to handle these kinds
of tests so another question you might
be trying to answer if we merge our
feature branch to main are we going to
break anything and this is the
development team context these tests
you're focused on usually you're focused
on a particular subsystem particular
kind of functionality in the system they
still need to be fast but they don't
need to run in
seconds it's nice if you can but they
don't have to you won't fail to meet
your goals that way these tests may go
through the user interface it's
accessible to your customers or they may
use it in interfaces not directly
available to the product users writing
these tests you don't have to know as
much about the internals of the code but
you do usually need to know a lot about
the protocol of feature areas that are
being tested and once these tests have
been implemented it's usually pretty
easy to migrate these up to product and
project tests my team we maintain a
build verification test that was
originally built by one of the
development teams in our organization
and these tests are oriented towards
finding particular types of race
condition bugs that are really difficult
to isolate and they wrote this test back
when they designed the system we were
doing builds we were doing daily builds
so they could run the test against every
build as it came out and no this bug was
introduced yesterday so get the list of
check-ins that came yesterday and you
this bug is in there somewhere since
then we switch the continuous
integration now we do builds every five
minutes sometimes we're not able to run
it against every build but we do run it
as many times as we can so that we we've
managed to narrow down the slice in
which you're going to find that bug even
even more tightly are the build of our
product becoming more stable or less
stable this is a question that you would
address using project context automation
in the project context you're focused on
the user level behaviors of the system
speed is always desirable but again it's
not essential or not as essential at
this level we we typically run these on
a daily basis your your infrastructure
is more complex now now you're having to
think about things like hardware
dependencies operating system dependent
these browsers multiple language support
may be another complication that we've
run into is that we typically are using
these tests against multiple projects
and against multiple releases so we may
have a test that passes against version
7 dot for dot too but it fails on seven
off for dot 3 and then it passes again
on 7 dot for dot for that fixed got
pushed out from 743 2744 so the tools
have to have some way of handling that
level of variation reporting becomes
much more important in this level you'll
typically have project managers and even
senior management wanting to know about
the results here now they're thinking
this is an indicator of the state of the
software right now and how close we are
to douche being able to release this
graphical tools that can show trends are
very useful we've built tools that let
us drill down and you start with the top
level results that you can drill down to
very low levels of detail and look at
the standard out standard error and log
files for any particular test that
you're interested in and finally the
last question well this patchwork for
customers that are running the basic
professional and premier editions of the
product using service packs one two or
three and this is a product line context
question so again product line you know
you're testing code that is already out
there in the field sometimes these tests
are very long you know my team maintains
a tool called the long-term stability
test and we take as soon as a release is
approved the floor it's made available
to customers we take that code and we
put it in a long-term stability test and
we run tests that are designed to to
push as much traffic through as possible
and expose any memory leaks now we have
cases where we ran it for 97 days and
then we got an out of memory here it'd
be nice to be able to run those tests
faster but we haven't been able to
figure out a way to do that
please graphic through it line rate you
can't push it through any faster your
goal here is to ensure that your hot fix
release through your patch it will fix
the problem you said it was going to fix
and it won't cause some other problem
and break something else false results
can cost you a lot here in terms of your
reputation with your customers so
reliability is a big deal on these kinds
of tests we find that these tests can be
very challenging to maintain now we have
you may have tests that go back four or
five years tests where the person who
wrote the test isn't it the company
anymore the person who wrote the feature
isn't it the company anymore so you need
to to make these tests as clear as you
can so that when somebody goes to look
at this code when no one's look at it
for years you can make sense out of it
and run control is a is a huge effort
here you know you have you only want to
run the tests that are valid for the
particular combination of hardware
software release service packs patches
if you want to run only the tests that
are valid for this particular
configuration and not the other tests
because again you are much higher risk
for Falls failures or false positives
that way so now I'd like to look at a
couple of case studies of tools that we
built and how they address the problems
in the different contexts the first tool
that will look at is a tool called the
ite integrated test environment this is
the tool that my team has built and it's
our primary responsibility the ite is
built on top of staff and stacks so I'd
like to thank elfrid for giving that
little introduction about staff and
stacks yesterday so I don't have to talk
about that quite as logical
now as Alfred said staff is an open
source framework it was originally
developed at IBM it provides a lot of
different capabilities that are made
available as services that can be used
across platforms by different different
operating systems and different
programming languages the ite was
developed I was felt by testers for
testers and now we hope the developers
would like it and use it too but that
really wasn't the primary target we've
written a lot of code for the system in
Python in addition to the tax code and
we encourage people that are writing
tests to run within the IT to use Python
but we support a lot of other languages
there are tests in it that are written
in C written in perl written in java
even some shell script stuff the primary
design criteria for the ite were to make
completely hands-off execution so you
could run it in a lab and it would just
run and nobody would have to go check on
our babysitter go in and hit return at
the end of a run to give the test
harness the capability to figure out
which tests were valid and which tests
they should run all of our tests are
tagged with metadata that described the
versions of the hardware that the tests
are valid for and the releases of the
code reese's are the product code that
the tests are valid for and finally to
provide services and reduce the burden
on test case writers especially in terms
of setup and teardown which can be
fairly complicated given our products
now ff5 our organization has not
emphasized historically it's not really
emphasized development skills and hiring
testers so we couldn't assume that the
testers were going to have really strong
coding skills so we provided tools to
make it easier for them over the last
year or so we have shifted we're looking
for strong scripting skills now when we
hire our testers and the kind of the
overall design goal of the ite was that
you'd be able to fire off a test run
when you leave it at night and when you
come in in the morning you'll be able to
look at your results and have a
good sense of what the current state of
the product is it's now I'm going to
talk a little bit about how the six
tasks are addressed within the ite so
distribution and run kind of control the
tests and the system code are all stored
in source control we make the tool
available as a Linux truite all the
tests are all the tests dependencies and
all the framework dependencies are
contained inside this troupe so you
start up the troot and you've got
everything you need you don't have to go
grab anything from some other source we
provide a tool within the truth so that
you can update the tech you can grab
updated test cases without having to go
out everything is stored in source
control and as I mentioned the tests are
tagged with metadata various types of
allow us to control runs and we're
looking at ways to tag the tests so that
you know if you've made changes to the
compression code you can grab only the
compression related tests and we tag
them with various properties that will
make it easier to define a run and only
run the specific tests that you care
about and not have to take the time to
to run everything test setup so in
keeping with our goal of easing the
burden on the test case writers we
provide some tools to help you configure
the device under test we also provide
tools to configure an Apache web server
that is very commonly used in our tests
and to configure some other tests
services that you might want to use
execution we do provide some some
limited data driven capabilities so you
can define a data set those datasets get
stored as XML and then at runtime we
parse the XML and generate the
executable Python code that tool is
fairly immature right now and it isn't
used widely so for the most part
execution is left to the test writer you
have to figure out how to make your test
happen verification this is another area
that's left largely to the test writer
although we do perform some health
checks and look for core files so if
your test you know if your test cause
there's a really serious error will pick
that up for you don't have to
you don't have to do as extensive checks
for that tear down we group our tests
into what we call sub jobs which is
basically you know of a directory folder
that's got some arbitrary collection of
related tests and as each sub job is
completed we do a fairly extensive
cleanup of the box and make sure that we
get back into a known state results
reporting so we sent an email that says
this many tests to run this many paths
this many failed this many aborted we
use an abort state that indicates that
we were not able to configure the Box
the way you wanted it so we didn't try
to run your tests and then we have more
detailed links in there we provide that
same data for each sub job and lists
which tests were in which tests pass
English has failed we also have web
pages for looking at these results we
have a database behind those web pages
where we stored all the results from our
every room we've done in the last two
and a half years so hundreds of test
runs hundreds of thousands of test
results on the web page has drill down
links so in three mouse clicks you can
go from the top level that you know you
know ninety-seven percent of the test
paths with three clicks you'll get down
to looking at standard out standard
error and log files from a particular
test that you're interested in the
second tool like to talk about its tool
called the X BDT so the X BBT is written
in perl it was developed by a developer
for use by other developers with testers
as a secondary target both the IT and
the X BBT word were written before we
had these conversations and came up with
this idea of contexts where the ite is
really aimed at the project and product
line context the xB BTW is aimed at the
prior to the development team context so
design criteria for the x BVT the test
should be able to run inside or outside
the tool without any modification and
the developer wanted to impose as little
overhead as possible on the test writers
and as little overhead as possible on
on running the test so you should be
able to run this test these tests on any
box you have and not have to worry that
it doesn't have this package of that
package installed now the idx BBT
provides many fewer services then the
ite does but in in return it imposes
much less structure and much less
overhead on you when you're writing your
tests so how does the ex BBT handle the
six tests so distribution and runtime
control the tests are all stored in
source control and they're stored in in
the product code branch so to get the
right test for the version of the
product that you're testing you just get
them directly out of the source control
for that project there's no matching you
know there's no matching of test cases
you know matching within the tool of
test cases two versions of the product
runtime control is done by what are
referred to as test manifests which are
basically XML files listing the tests
these manifests can be nested to
arbitrary arbitrary depths I think the
deepest he's got running right now is
about four levels if you want to run if
you want to run some tests but not the
other tests you go in you have to go in
and manually edit these manifests and
comment out the tests that you're not
interested in for this rod set up the X
BBT provides the login credentials in
the IP address for the box and passes
those on to each test case and then the
test is responsible for all other
configuration issues test execution is
left for the test writer to take care of
results verification against left for
the test writer teardown is left to the
test case the expectation is that every
test will clean up completely after it's
done and leave the system exactly the
way it was prior to the test
reporting as the ex BBT runs it
generates it writes for text file at the
end of each test with its pass/fail
results at the end of the run these text
files get mailed out there also stored
up on a web server so you can go look at
the web page and see all the text files
from all the various runs there is no
database so if you want to find out what
happened with this test yesterday you
have to go up you go up to the web page
you have to search until you find the
date and time that you're interested in
for a particular run and then search
through that file yourself to find the
test result so what did I what did I
learn doing all of this now the first
thing and the biggest lesson for me back
when we disagreed and we couldn't agree
about what was important in a test
framework and what the best way is to
solve these things were nobody was wrong
we were all right for the individual
problem that we were trying to solve
there are lots of different approaches
to doing test automation most of them
will work there are approaches that
don't work for anybody but most mostly
you know your ideas will work the one
that it's going to work best for you in
your individual circumstances is going
to depend on who writes your tests who's
building your framework who's looking at
your results and what decisions you're
trying to make the thing that we've done
what we've taken away from this is
rather than try to build one framework
to test them all we're now focusing on
building shareable components the staff
framework is a great model for this it
already had everything set up is
individual shareable services that are
cross-platform cross-language so we are
looking at ways that when we're building
features building new functionality that
is of interest to more than one team we
can provide that as a staff service some
examples of things that we're doing
right now each of the product teams has
their own smoke test and there's one
team that has to smoke tests so rather
than try to get everybody to agree all
right your smoke test should all run in
this one tool now we've written a simple
CGI script that everybody's tool can
submit their results to
and you can see it all on one page now
our next step you know taking it one
step that idea one step further is to
write a staff service that allow you to
submit results so that anybody anybody
who writes a tool or anybody who's got a
tool don't be able to submit their
results will put them in one reporting
tool and one GUI Xin when you want to
see the current state of a bill you'll
be able to look at all the different
modules and components all in one place
now one of the things we found when we
were looking at looking at all the tools
you know there were five different web
buoys so a lot of work had gone into
that but nobody was really happy with
what they had everybody had a list of
things they wanted to add ways they
could make their GUI better a lot of
them reminded me of the things that
James Whittaker was talking about
yesterday in terms of visualization we
really want to be able to see what's the
current state of our software in our
test that's a lot more likely to happen
if we can concentrate all our resources
on one GUI rather than writing five or
six or even 11 different go easily
so what good does this do you I'm back
and back on my first slide I told you
that I was going to help you figure out
how to build the system that you really
need and to do that you've got to figure
out what your context is so who is going
to write and maintain your framework a
little if you can have dedicated people
or dedicated team to maintain your
automation framework you can build a
much more capable framework then if it's
going to have to be maintained by a
tester who's going to be doing this as a
side project in his spare time in
between trying to get his major releases
out who's going to build and maintain
the tests that are running within this
framework the more skilled your test
writers are the less they need help from
the framework to make all the tests work
so if you've got highly skilled
developers writing your tests maybe a
framework doesn't need to be as complex
as if you've got people with lower
levels of development skills how are you
going to use the tests now if you're
going to use your tests to gate
check-ins the tests have got to be
really quick if you're going to use them
to validate new builds they can take
longer the BBT that my team builds takes
about 90 minutes to run the first 30
minutes ago for installing the product
takes 30 minutes to do a full install
the product and then a test run for
about an hour that was great when we
were only doing builds once a day now
that we've got continuous integration
and sometimes we've got builds coming
out every 10 minutes on 15 different
branches that all want to have the BBT
run it's become a bottleneck we've gone
from one machine to six machines and we
still can't keep up on days when there's
a lot of code being checked in so what
we're doing to solve that problem you
know we're now separating out the test
so we have an a separate we don't have
it yet but now we're building a separate
tool that will test nothing but installs
and then if those installs paths we spin
up a virtual machine and do the real do
the hours worth of tests on a system
that we know will install correctly one
of the problems that we run into is that
our installer tends to be a little bit
flaky especially in certain certain
phases of the development cycle
so we would have we would do an install
to run the BBT and the install would
fail and then somebody would have to go
and recover that box so moving to
virtual machines virtualization and
other james's points by moving them to a
virtualized system if something goes on
in the tests we just snap shot it and
snapshot to look at later and shut it
down and we haven't wasted half an hour
and we don't have to waste another 15
minutes we're covering that box to
usable state and the final question
yourself how long are these tests going
to live if these tests are only going to
be used for one short term project you
don't need to put a lot of work into how
you're going to control the runtime
execution of the tests but if the test
might live for five years or ten years
you know if you've got a 10 year product
lifetime like Microsoft's does you're
going to run into cases where these
tests are going to work on some version
they're not going to work on other
versions they're going to have to be
tweaked to work with one service pack or
another service pack and you'll need
ways to to control and say this version
I want to run this version and test if
we've got service pack one installed I
want to run that version of the test if
we've got service pack 2 installed on I
know that on version 35 this tests going
to fail I expect that much your system
has to have some way of handling that
variation and those expectations
so in conclusion wow I've been talking
faster than i usually do more time left
and I thought define your context you
know who's going to write the tests
who's going to look at the results what
decisions are your results going to
influence and then think about how those
how do you answer to those questions
affect your implementation of the six
actions and yes I know I only have four
bullet points up there for the six
actions so if you do all those things
you'll be able to design the system that
will fit your needs and gives you what
you're really trying to do you know
solves a problem the particular problem
that you're trying to solve with
automation a few acknowledgments you
know the only reason I was able to put
all this together was I had the
opportunity to look at eleven different
tools that were solving 11 different
problems all at the same time and you
know there was a lot of work by some
very talented engineers building all
those tools that enabled me to look and
see this so I'd like to thank the other
members of f5s cross-functional tools
teams that took the time to demonstrate
their tools and explain how they work I
like to thank the members of f5
automated test team and special thanks
to john hall and brian agita principal
test engineers at f5 who took the time
to sit with me and take my blob of
observations and turn them into a
hopefully coherent whole to ryan allen
senior developer at f5 who always
challenges all of my assumptions reminds
me that his context is different than
mine you know he pisses me off in the
process sometimes I end up with a much
better set of ideas because of it and
Brian Brannigan our director test
engineering who encouraged me to take
the time and the energy to write all
this up and submit it to the conference
so with that I will open up for
questions yes
test so the question is what is mm-hmm
so the question is what do I mean by
test target and I think you're you're
interested in test that would be useful
across multiple contexts ok so my I was
really interesting to me to see John
Thomas and Marcus presenting yesterday
their idea of small medium and large
tests seems to me to match on some level
to my idea of developer tests
development team tests and project tests
we found in looking at it that there
were a number of teams that had built
tools that worked that could be useful
at the development team context at the
project context and at the product line
context and it was largely a matter of
which tests and how many tests you were
running in a run that determined which
of those contexts you were dressing the
best now the tests often were identical
and the main issue was how long is it
going to take are you going to be able
to generate the increment the different
pieces of information that the different
audiences want your developers want a
different set of of information out of a
test then your senior executives do so
you know if to satisfy the developers
you may be able to get away with just a
text file that they can grep through to
find the results for a particular test
for project managers and senior
management they want to see a graphical
tool that shows them a trend line a past
print line is going up
that is handled you know we we give that
responsibility to the test team the
tester and the test teams so as an
example we are now using the ite to do a
build verification test so in the
existing build verification tool that my
team runs as I mentioned you know it was
written by a bunch of developers it
dresses some very specific concerns that
they had but it really didn't fill the
test teams need for a bill verification
test because it only looked at this
narrow slice and as testers we wanted to
see a broad slice so we we've worked
with the test to the functional test
teams to define a set of tests that give
them that broad slice and we run that
within the ite framework but it's a very
small set of tests and it finishes in an
hour so at that sense we're using the
same tool to address the development
team context and the project contest
we really didn't run into the case where
we thought the context that s been
written for the wrong context that
wasn't a problem that we ran into we
really hit this idea of different
contexts not because people were writing
tests for context that didn't work but
because we were trying to agree on how
to how to share components and when we
didn't realize that we were having
different we had different contexts we
couldn't agree about what was in what
was most important yeah
so we have one product team that has a
comprehensive strategy that really
addresses all four contexts and they
have two different tools that they're
using they have one tool that was
originally written by one of the
developers that is targeted at the
individual developer context and the
development team context and the
developers write most of their tests
using that tool then those tests are
those tests are rolled up in those tests
are accessible to the testers also but
for the end to end comprehensive tests
that the testers have produced for the
project in the product line context
they've actually built a different tool
there their tool provides a fairly
comprehensive API for generating you
know and a lot of helper functions for
testing their product they have set it
up so that the tools can work well
they've set up a one-way direction so
the the tools that the testers are using
have the ability to pick up and execute
the tests written by the developers but
they don't really go in the other
direction yeah
so we actually we have the desire to do
packet level captures and tests at the
test level by both to have that be done
by both testers and developers and there
are a few testers that are writing
automation that works directly at that
level most of our automation is not at
that level the typical test that runs in
the ite the tool that my team has built
you know you you log on to the device
you configure some virtual IP or a pool
or something VLAN and then you send HTTP
traffic or ftp traffic or UDP traffic to
it and we look at what comes back in
those traffic streams and we don't get
down to grabbing individual packets in
the test that that our testers are
writing for the most part yeah
all of the all of the projects that have
been tested with the ite right into a
common database so we have the ability
to do data mining the way the database
was originally designed in the way our
reporting tools have been designed right
now it's very difficult to look at
results for multiple runs so you know if
we do runs on three different versions
of hardware for build 37 you have to
bring up three separate reports to see
what happened and that's one of the
that's next thing up on our plate is the
ability to aggregate runs so to generate
a single report for build 47 that tells
you what happened on all the different
platforms
so that that de soon who used that route
was made about two and a half years ago
and at that time other tools like VMware
we're not mature enough and I wasn't
actually I actually wasn't with the
company when that decision was made but
but Brandi guiter who was there and
sitting right there you can you can
point out if I'm getting this wrong that
decision was made
virtualization is what we're looking at
right now you know if you look at the
across the whole range of products and
you know our right now f5 has bunch of
separate products and we're in the
process at the development level of
merging all those together so they're
just different modules of a one product
and they have very different topologies
that they work in you know we have some
products that the basic test harness
consists of about seven different
machines you've got a client a server
wayne accelerator a weigh-in simulator a
way an accelerator a server and a client
and that's your test harness in order to
and in order to make it easier to share
those kinds of tests we are looking at
how to how to virtualize that entire
environment is one chunk so that you
could spin that all up as one piece the
discussion yesterday about cloud
computing is something that we're
interested in and we're trying to figure
out how to do it kind of an internal
level now how can we define a common how
can we come up with a schema for
defining what a test harness looks like
so that we can then virtualize those
tests harnesses and any anybody with any
tool would be able to say I need a test
harness you know I'm running this test
my test harness needs to look like this
and call out to a staff service that can
spin up the collection of virtual
machines that's needed to test that
tests other questions yeah
so open source then the question is did
we find problems that were difficult to
solve and did we find open source
solutions to the those problems that you
or did we run into open source tools
they couldn't solve the problem so the
ite and the tools that my team is
responsible for we've used staff is sort
of the backbone then some other open
source the major other open source tool
that we're using is Python web framework
called Jango and we're using that to do
all of our reporting you I and to manage
our reporting database and we've also
used Django to do our data driven tests
so we have a the tool for defining the
data sets for data-driven test that's
written in Django Bryan am i leaving any
other anything else had yeah we use a
code review tool called review board
that's an open source project that we've
been really happy with we didn't find
that we had to make major changes to the
open source tools from the most part
they were pretty easy to adapt to our
purposes other questions yes
the performance and stress testing is
actually not addressed at all within the
context of the tools that we were
looking at that's handled by a separate
group at f5 and they were invited to
participate and but we didn't get a lot
of information from them yeah and you
know we actually I think that that might
be really interesting to see is that a
separate context or does that fit in
here somehow and we've had some
conversations recently about other
contexts one that we have kind of
tentatively identified is what we're
calling an experimental context now
right now we're doing proof of concept
with selenium and a commercial test tool
called I test to determine whether or
not these tests these tools can automate
what we're trying to automate so that's
kind of another context where the
question you're really trying to answer
is will this test to work for what I
need to do yes
yeah all the tests all the tests are
stored in a single they're stored in one
source control tree we have recently had
to you know started branching those to
make it a little bit easier that the
original the original concept was all
the tests would live in one place yeah
yeah and that bad approach was taken by
some of the groups that were developing
test tools but it wasn't in particular
person was taken for the IDE okay yes so
now in the the product group that I work
in you know we have a couple of
different functional teens that are
writing tests a couple different
functional test teams writing tests that
are being added into the ite that are
being used at the project context the
product line context and now we're using
certain uses at the development team
context in the smoke test the developers
have a separate set of you know they
have their own set of unit tests that
does not interact with our system at all
at the moment that's something we've
been talking with now I've been talking
with 12 development managers about that
how can we how can we figure out a way
to migrate those and would it really
make sense to migrate those how much
value will we get out of that yes
so the question is how do we autumn how
do we automate the process of the test
teams development in the life cycle so
you know we have as part of our
development process we have when a new
requirement is slated to go into a
release a test focal is designated for
that requirement and the test vocal is
responsible for figuring out how are we
going to test this what are we going to
need to do we have just recently started
also designating an automation focal for
that requirement so the automation focus
job is to look at this requirement are
we going to be able to automate this
requirement with the tool set the way it
exists right now or do we need to add
additional capabilities into our
automation tool so that will be ready
when this feature is is built and we'll
be able to write automated tests for it
does that answer your question ok and
lydia's given me the signal let's click
further up okay five minutes other
questions
mm-hmm
so the BBT and in in our particular
environment right now the BBT is
actually the the BBT is a very
complicated harness to set up and we
only have we have a few dedicated pieces
of hardware that can run the BDT test so
if you need to reproduce a problem on
the BBT if you need to investigate the
details of that we actually have to take
that test harness out of production use
for the BBT and turn it over the
developers to look at the ite is
designed to run at into you know to run
on individual machines and it's not very
difficult to set it up and run on other
machines and connect it to a specific
piece of hardware that you need a test
so that's actually a little bit easier
to to set it up and investigate a
problem there okay yes it depends on the
problem sometimes we have all the
information we need out of the logs and
the core files and there isn't any
further work that's needed the type of
bugs that the BBT was designed to catch
our are these very deep raised condition
bugs that are sometimes extremely
difficult to track down and we do have
to turn the harness over the developer
sometimes for a couple of days for them
to figure out what's really going on
other questions
yes
we used XML just for an initial
implementation because you know we
understood XML and it was easy to do we
discovered pretty quickly that for a
complex data set XML is a really bad
choice um in the words of yesterday's
speaker it's got way too many pointy
things and it's really easy to hurt
yourself okay other questions all right
well thank you very much for your
attention
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>