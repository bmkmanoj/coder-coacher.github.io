<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using Influence in Understanding Complex Systems | Coder Coacher - Coaching Coders</title><meta content="Using Influence in Understanding Complex Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using Influence in Understanding Complex Systems</b></h2><h5 class="post__date">2009-04-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6FyH3qA1FPU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'll go ahead quick introduction I
actually went to grad school without so
I know him from a way back when over the
name alex is a professor here science at
Stanford all sorts of interesting issues
related to programming languages and
software systems and he's here talk to
us about using influence yeah can Obama
like up all that yeah I know right
thanks a lot bill is this my god I don't
know if you don't have no idea okay yeah
if you can hear me find I won't worry
about it but so today we're gonna talk
about debugging in a in a production
context and a couple of things were
going to touch on our anomaly signals
something we call the structure of
influence graph and influence and then a
bug in a particular robot that you may
have heard of Stanford's robotic car
that we we worked on using these
techniques the context for the talk I is
that we really want to get a handle on
how we can make better use of the
information that we already gather about
production systems so there's a
tremendous amount of log data that's
generally gathered when when people run
systems in production and and it's our
feeling that a lot of this information
is not adequately mind or used and we
can do a better job of extracting useful
information from it and we do one case
study at the end of the talk where I'll
actually hopefully prove that point you
but our model of a of a typical system
is something like this you have some
kind of inputs on the on the left and
outputs on the right and then there's a
cloud in the middle with all kinds of
stuff in it and the inputs go into the
cloud and stuff comes out and it's not
really clear what's going on and every
now and again a unhappy thing happens
where a bad output arises and then
usually it falls to some either engineer
or a system administrator to try to
figure out why did that bad thing happen
okay so if we drill down one level if we
look inside the cloud in there is some
kind of complicated system it has all
kinds of component
that can talk to each other and you know
some of the features typically are
there's going to be feedback loops in
the system there will be resource
sharing so there'll be dependencies
between components that are not explicit
but only because they share a resource
or competing for that resource and
further bars there'll be there'll be
some sub clouds there'll be pieces in
there that we don't have control over
that we can't actually go in and look at
ourselves code we got from other from
from other organizations whole systems
that we've purchased and run as part of
our organization and so we have their
dear Pete there are places in there we
don't have excuse me no problem there
are places in there we don't have
complete information about what's going
on in the system and furthermore we have
some data that we're gathering log data
typically but some kind of signals that
we're taking out but not for all the
components and different components are
different quality and amounts of log
data so the data will be noisy and also
all right so that's the setting so just
to try to make this as clear as possible
we're really interested in taking the
system as it is with these kinds of
constraints and try and see we make more
use of the available information that's
coming out when there is a problem help
diagnose those problems alright so what
are the challenges well also has already
said this measurement noise and
imprecision there's missing components
interactions generally these systems are
complicated and very large and and then
the data the information that we're
actually gathering from a different
components can be of any kind so they're
different kinds of data different
semantics to the data and we need some
way to to systematize that and make
sense of it so this is not a new problem
people have looked at this in the past
but the typical solution has been to
change the problem a little bit and
almost all the solutions that you see
actually think all the other solutions
that you see essentially boiled down to
adding some kind of instrumentation to
make the problem a little bit easier to
solve and we're all for instrumentation
and don't don't object to that principle
and I certainly worked out a lot
projects where we were happy to add
instrumentation but there's an issue
there which is that you can't really get
past this problem that you're going to
have pieces that you just don't know
anything about because they're not yours
and so there's always going to be
components in there it's going to be
some level of abstraction below which
you can't look and if you if you assume
that you can add enough instrumentation
to make all the dependencies explicit
which is what these other systems do
then this immediately doesn't scale up
at some point because you have some
building block where you just can't do
that you just can't break it down all
right and so if we assume that there
will always be these clouds these pieces
that we don't know the structure of in
our systems what can we do because we
can't figure out what the input-output
relationship here is we can't see what
on this side effects what on that side
there's the only just we just can't look
inside and what kinds of techniques can
we bring to bear to deal with that
problem all right and this is really the
organizing idea for for what we're doing
which is to use statistics and so
instead of trying to explicitly I go off
of the instead of trying to build upon
explicitly given dependencies we're
going to use we're going to turn our
weakness here into an A into a strength
by using statistical information about
well what happens on the left and what
happens on the right to infer
correlation okay will not allow us to
get we won't be able to get dependencies
out of that but we can certainly get
valid statistical correlation if we do
things right okay and this is different
from dependencies and we're going to
build up everything using statistical
correlation rather than dependencies and
and this will lead to different kind of
output so you will see that we have
different kinds of effects and you get
when you actually have an explicit
dependency graph for example we might
have bi-directional correlations we
might not be able to tell what the
direction of influence is between two
components because they seem to mutually
affect each other and so it might just
wind up with a bi-directional edge
eventually saying these two components
are somehow locked together but we don't
know exactly what the path
of data flow is between them all right
so how can we saw about the rest of the
talk is going to be about how we compute
influence and we're going to start out
by talking about anomalies and then how
we do the correlation in time and space
and finally how we actually do the
inference so if we want to compute
correlation between components where we
don't really know how they're connected
up you know what kinds of hooks do we
have because all we see is a signal
coming out of one component signals
going into coming out of other
components where do we have some kind of
leveraged where we can see what they're
the one opponent component is affecting
another and remember we don't assume we
know the structure in between well so
one thing we can do is to look at
anomalies so where something happens
that strange on one component something
is very different from the abnormal
behavior we could then look at the other
components and see if they respond with
anomalies in some kind of relationship
that we can understand so if component a
hiccups at time T and component B
hiccups at time T plus 1 and that
happens for many different times make
every time opponent component a hiccups
I'll component be eventually hiccups we
can begin to infer there is some
relationship between them ok so we're
going we're going to compute these
things called anomaly signals and this
is going to be our model of individual
components so all we're going to do is
is map every components behavior its log
file at whatever signal we're gathering
from it to some kind of a stream that
measures how different its current
behavior is from its and from its global
behavior all right and that just gives
every point in time we'll have a measure
of surprise how unusual the behavior of
that component is currently you know
compared to what it has done in the past
all right so here's an example anomaly
signal just mapped out over time and
then we'll use a standard statistical
technique cross-correlation will take
these signals from different components
and we'll cross correlate them and that
will give us some notion of whether
there is any first of all correlation
between the components
and in addition will give us some idea
of what the time lag is between them so
for example here if this is the cross
correlation graph between the signals
for component I and component J is my
pointer working yes this what this
signal says is that there is a peek here
where component I a component I signal
is highly core components eyes anomaly
signal is highly correlated with
component J's anomaly signal with this
much delay so component I goes first
something unusual happens on component I
and then after this amount of time
there's often something unusual
happening on component component J
alright and that's how you compute the
cross correlation but that's not so
important is what it actually does
alright as the intuition then how are we
going to make use of this information so
one idea these signals actually i'll
show you some actual anomaly signals
later but these graphs are very very
busy and they extend for long long
periods of time so we're going to add
introduce a threshold parameter saying
if the cross correlation at any point if
there's a peak that's above a certain
threshold then we're going to say that
component I in component J have a
relationship that there is a there is an
influence between them and that will
lead to these graphs okay we'll have
graphs of influence where the edges mean
that there that there is an influence
that exceeds some threshold between them
and the delay gives us the
directionality of the edge alright and
it's quite possible just to point out
that will get directed and it will turn
out I'll explain how we decide what kind
of edge later but we'll wind up having
very arbitrary kinds of graphs the grass
could be have directed edges
bi-directional edges undirected edges
and also no components might not have
might not share influence with other
components in which case they will be
disconnected from the graph alright so
now let me tell you that's a high level
sketch of how we're going to do this and
let me tell you a little bit about how
we compute the anomaly signals and I'll
just walk through the different pieces
anomaly signals the influence and then
finally the graphs and then then some
examples
so again a timely signals just surprised
over time how unusual is the current
behavior and because it's a measurement
over time we need some kind of
time-stamped signal so that's one
assumption we do make is that we have
information with time on it okay and
that's a pretty often satisfied
assumption about the kinds of signals
that you gather from from real systems
and in fact we're going to go even
further than that we're just going to
focus on the timestamps we're not even
going to look for the most part of the
contents of the log all the examples
we've done so far the only bit that we
actually use is the time and i'll show
you how we do that right now okay so why
use timing well timing is pretty
Universal signaling get it from most
components we were gathering any
information at all you generally have
time and so I say ubiquitous signal and
it often carries other information and
information that's hard to get from
other kinds of tools so let me give you
a couple of examples here's a little
chart showing how light takes an HTTP
server to process different kinds of
pages all right so what is the point of
this is so there's one there for HTML
one for PHP and one for a just the time
just serve a 404 not found page ok
that's a distribution of x and the point
is that if I just tell you the time that
it took to serve the page you already
know a lot about what kind of a page it
was alright so there is semantic
information encoded in those timings
similarly i mentioned that we're going
to deal with shared resources so the
ability to actually you know infer
influence even if it's there's no direct
edge but it's just competition for a
resource here's an example of that and
how timing can help if i have pairs of
processes just trying to talk over a
socket here's the timing for them to
succeed in having an exchange of packets
just based on how many pairs are sharing
the socket and as you would expect if
there's a single pair talking on the
socket you know the packets go through
fairly quickly but if I have lots of
pairs talking on the socket the
distribution is heavily shifted to the
right and so again the timing of the
results on this particular shared
resource carries a lot of information
about about what was going on at the
time
in that particular resource so here's
our model here's how we look at time so
we're going to actually look at the
inter arrival times of messages in our
signal in our log file for example so we
look at the difference in time stamps
soba trains a time from one thing
appearing in the log to the time of the
next thing appearing in the log and
we'll need a historical distribution the
second line there that represents what's
normal okay so I'd say it's like an
average over a past log or over the
entire log the current log if we're
doing this retrospectively and then we
need a recent distribution so so so over
some time window over some local window
just what's the current distribution all
right and then all this is doing this is
the KL divergence it's a standard
information theoretic quantity but let
me just you know say if you haven't seen
it before let me just say a little bit
about what it does so you know we we're
looking at the current window and you
know how much information is in there so
what were the distribution of inter
arrival times you know this over some
local very sorry sorry short period of
history and then work we're just
normalizing that some by how much
information there is their relative to
the historical distribution so if they
if the recent distribution is very
different for the historical
distribution this term over here will
bump up the score and if it's very
similar to the historical distribution
then as you will see all these numbers
will be close to one log of base 2 of
one's going to be close to zero when
those terms will be will will tend to be
very low okay and so this is just you
know the current score is weighted by
how much they vary compared to the
historical scores all right okay it's
what are the parameters in that model
well there's a window size we had to
talk about how much recent history we're
going to look at and that you know
effects tolerance to know it to delay
noise and how sensitive we are too small
anomalies okay so there's very very if
we have a very short window then we're
very sensitive to small perturbations
the anomaly signal if we use a big
window that we're much less sensitive
accepted
very large anomalies and then the bin
width okay so what we considered be
distinct so so how wide we make our what
how we group the anomaly scores together
okay and we use heuristics for those
they're well-known heuristics first for
determining these things and we use use
standard techniques for that so the
signal distribution then is going to
look often something like this okay so
because we want these to be anomaly so
maybe the idea whole purpose of this of
this model is to capture the unusual
behavior most of the behavior is going
to be grouped around scores that are not
unusual at all okay so is going to be a
big peak just for normal behavior and
then hopefully because we're technique
I've depends on having some anomalies in
order to gain a foothold for determining
where the correlations are there'll be
some anomalies you know at somewhere
else in the graph and this is sort of a
typical distribution which bimodal so
you have your normal behavior and then
every now and again something odd
happens and there's another peak way out
here you know distinctly separated from
the normal behavior and actually this is
actually not so unusual a graph to see
for a lot of components
okay so here are some actual anomaly
signals taken from a robot and it's not
so important what they are they are it's
a fairly complicated system actually
we'll talk about this system again later
and this is not all of the components in
it but one thing that's kind of
interesting just visually is how
different the different signals are I
mean so it's very very different
patterns but if you were to stare at
this for a while and let me speed things
up for you and to look at these signals
you would see that there are some groups
that seem to have similar structure so
all of these components seem to be
somewhat related okay and that they have
spikes in a similar pattern a couple of
components here that seem to bear some
relationship to each other is a big drop
at some point and that seems to be
correlated in time and a little harder
to see but there is something going on
in these I me because he has a very
similar pattern between those two at
least in the first part of the graph all
right so what we're going to do now is
try to figure out what these we're
trying to fight we're going to try to
discover those automatically ok and I'll
talk about that next
so now structure of influence graphs or
cigs as we call them so I mentioned
before that we're going to use cross
correlation to identify correlated
anomalies and cross-correlation we've
never seen it before it just does
exactly what it what we want it compares
two signals and says whether there's any
correlation to them perhaps with some
offset and the cross-correlation
essentially tries all the possible
offsets and picks the one that gives the
largest the largest correlation and neck
and it gives the magnitude of the
correlation and also the delay now I
mentioned before we'll use this
parameter epsilon to determine when the
correlation exceeds a significant
threshold that we want to have an edge
in our graph but will actually use
another parameter alpha here that's a
delta from zero or distance from zero
and that's the threshold will use to
determine what direction the edge should
have right so if the edge is within the
band minus alpha 2 positive alpha if
that's where the peak is of the cross
correlation then we'll say there's
essentially no delay between them and
we'll have an undirected edge okay if
it's on either side then we'll have an
edge in that direction and if there's a
peak on both sides then we'll have a
bidirectional edge okay so that's the
interpretation of what the edges mean
alright so there's gonna be one vertex
per component in our graph and then the
edges summarize the cross-correlation
shows you where there was significant
cross-correlation exceeding epsilon and
then the direction of the edge tells you
where the peak is or how many Peaks
there are in the case of a
bi-directional edge alright okay alright
so that's just a summary what I just
said okay
so what's the interpretation of this so
so at Jason components in the graph
graph components are connected by an
edge share some kind of an influence so
in this graph we would say that you know
a proceeds be so anomalies on a proceed
anomalies and B a and D are roughly
simultaneous and D and E that's kind of
interesting case when you have these
bidirectional edges either there's
really they really both influence the
other but another common pattern that
you see is that their periodic so if you
think of D being a sine wave Andy being
a cosine wave then the you know the the
peaks and anomalies on D and E are
offset by some amount but they oscillate
between one and the other I'm sorry well
it could be i'm sorry so there could be
a so it could be a periodic behavior the
idea where you know something happens on
a that class something happen on be that
causes feedback let me have done it okay
so that's that's one way in which they
can influence each other the other case
might be that there just was some point
in time where something happened on a
and that cause something happened on be
okay and then another point in time
independent something happened on B and
that actually caused something to happen
on a but it wasn't a cyclic thing where
it just you know happened over and over
and over again these are actually I
don't think I said it very well they
were actually just in screen in it huh
yes and I haven't talked about that yet
but that is something that's going to be
very interesting when we get to our
examples because you've spoiled my
surprise which is that often looking at
these graphs you can because remember we
we work on systems we don't know all the
components we don't assume we know the
whole system but often you can spot in
the graph where there is another
component you're not modeling because
you'll see a whole bunch of influence
tightly connected among components that
where you know there's no connections
between them and what that tells you is
there's somebody else that you're not
seeing who's affecting all of these
that's not in the model yeah so
another thing here from this example if
ad are highly correlated but there's no
equity to be because argent yeah nearly
roughly quarterly well you can't tell
just by looking at one graph but that'd
be an example right and one thing you
can do is and actually our current
interface doesn't support this but we
have have proposed we plan to do this at
some point is you actually very that
threshold with a slider you can see the
edges come and go so you get a feeling
for you know where the the best graph is
I mean so way you kind of like to get a
graph with a reasonable number of edges
and so you would why i said that epsilon
automatically you know just so show the
most important edges now we don't remove
transitive address you okay yeah
right right yeah well ma'am well partly
we're here partly we're here on a
fishing expedition because we would like
Amanda that this is as explicit in the
abstract you know what to drive this
kind of stuff we need a lot of data and
from realistic systems and so if there
is interest IV I'd love to talk after
the talk to anybody who who we willing
to try this out and of course we'd be
willing to do most of the work we just
need the data set yeah
where
I didn't understand to quit I couldn't
follow it faster sorry so I got three
components a b and c so it's soit's when
you say mediated what you mean i mean
that that that eight is not directly
influenced e but a yeah so partly doing
statistical correlation so we will find
the correlations even if there's no
explicit dependency so we will but but
you would see in that case if there's
really a chain of dependency you would
see a influences be be influenced to see
in a separate edge a influences see okay
that's how you would we wouldn't do that
right now I mean I don't know why we
couldn't do that but just the semantics
of what this means it means that you
know whether there's a correlation or
not and there is additional information
and knowing that a influences see
because the influence may be either
magnified or damped by passing through
be and so the presence Ramesses that
edge from A to C actually tells you
something that you don't get just by
having the two edges A to B &amp;amp; B to C
right oh yeah so the question was what
about transitive edges I think I can
summarize it that way if I have a
influences B&amp;amp;B influence to see you know
why would I want the edge a influences
see okay all right okay
alright so continuing on the meeting of
the graph structure if we have a click
this is actually the thing I want to
talk about if it's so often you can tell
things about the system from the
structure of the graph so if you see a
click in the graph that for example is
often indication of a shared resource
okay so there may be the case that these
guys don't actually have a dependency
between each other they don't actually
talk to each other but they're all
competing for the same resource and the
fact that there's a clique indicates
that when there's a lot of contention
they all influence each other because
you know one guy getting in there and
getting it stops the other ones from
from doing their job and of course it
since it's completely symmetric the
edges will be from everyone to every
other one all right and our or none
modeled component is another possibility
in that situation and there are other
graph structures that suggest
interesting things but clicks are one of
the most important and easiest to give
intuition for in a short talk all right
all right now influence so one thing we
want to understand was how influence
propagates so certainly influence
includes things like functional
dependencies so if a is taking input
from B then then you would expect to see
be have an influence on a but there are
other situations in which there's
influence like shared resources and such
and so how does influence actually move
through a system and so there's a couple
of different ways that influence can
propagate one is just for timing okay so
we already saw some examples of that
with the HDL fast a server can HTML
sorry an HTTP servic and process are
different kinds of pages and and the
resource contention over a socket and
then there's also it can propagate
through semantics so you know do
different kinds of data take different
amounts of time and then of course that
might subsequent subsequent components
I'd also get that different kind of data
all right and of course you can have
examples with both okay so message
content typically affects influence at
least in the timing model by affecting
the processing
I mean that's that's where the the
semantic content gets translated into a
in effect on the timing of the of the
logs all right so we just have a few
experiments and these are just very kind
of micro benchmark type experiments to
convince ourselves that that this
measure actually that influences
actually interesting property and
actually shows up under lots of
different circumstances and will
actually you'll be able to find
influence propagating in lots of
different situations so here's them
here's the model and it's very simple
we'll have a source a single source okay
which is generating work for the rest of
the system tasks are just simple
components okay and then we have
resources and the thing about a resource
is it has a capacity only let so many
tasks access it at once okay so a source
just produces data that's passed along a
channel a task takes that data passes
something along to another task and if
there's a resource then the the resource
mitigate I'm sorry mediates how often
these guys get to have access and get to
run all right so there was going to look
at chains of these things all right and
only the head and the tail will be
instrumented so we'll get information
about the timing of events at the source
we'll get information about timing of
events at the tail but we won't know
anything about what's going on in
between we won't we won't let our system
see that will treat that as a cloud okay
that we can't look inside and the
question is you know for different kinds
of parameters and different kinds of you
know experimental setups can we see
influence at the tail that based on
what's happening at the source okay for
what kinds of clouds what kinds of
structures in here how's that affect the
propagation of the influence so we're
going to pass some stuff down a channel
and we'll just watch what comes out here
okay and will generate the anomaly
signals for the source and the tail and
will
we'll compute the cross correlation and
see how much influence there actually is
alright so what kinds of experiments can
we do well will vary the chain length of
the number of resources in those in
those chains will vary the message
timing and also the effect of semantics
so we'll have basically two different
kinds of messages a zero into one and
then you know different tasks will do
different things depending on whether
they get a zero or one for example will
vary the anomaly strength in the
background regularity okay so we'll
you'll fiddle with our anomaly signals
how we compute them and and add noise to
them and things like that and we'll do
things like you say we didn't have all
the data say we deleted some of the
messages from the log file say we only
got half of the log you know does that
have any effect on this how does that
affect the influence if we just throw
away half some of the data can we still
detect the influence or not all right
okay so here's I'm just gonna show you a
bunch of graphs so here is a graph of
the influence for this kind of
experimental setup where the chain
consists just of a single basic
component so I just have one component
in there between the source and the tail
can we see what's going on and they
think might be a resource or it might be
a task okay and so we varied this in a
bunch of different ways I won't try to
explain all of them now but a couple of
important things to notice first of all
before we did the experiment we computed
the baseline so if we just took two
completely uncorrelated tasks you know
things that we're just not connected at
all and we simulated them with the same
parameters what would be the influence
between them what would be the measure
of cross correlation because there will
be some nonzero cross correlation
between them even just for random
processes and so that line represents
the baseline okay anything below that is
noise anything above that is signal and
you can see that for all the experiments
we're getting some signal okay so there
is influenced being propagated now this
is the anomaly strength down here
and basically as you approach zero here
that means the sit that the anomalies
are the basically no anomalies as you
get close to this axis right here there
are no anomalies at all in the stream
and as you go out this way the anomalies
get stronger and stronger and stronger
so the timing variation is larger and
larger okay are we and i forgot to say
one thing i forgot to say is that we
actually inject a period of anomalous
behavior into the stream so we have run
this for a long time but at one point in
the stream we actually changed what the
behavior and make it anomalous ok so we
skew the distribution of timings or
whatever and to introduce some anomalies
ok so down here we you know these are
extremely weak ok and out here those not
introduced families are quite strong so
what this is saying the fact that we
actually get some signal even when
there's essentially no anomalies in the
input stream you're saying it's just the
normal variation in timing is sufficient
to find this you don't even need to have
really bizarre behavior to to get some
signal because there's enough variation
just in normal behavior that we notice
that because even a little bit of
variation a normal behavior will cause
variation the other components you don't
need to have you know big disturbance to
the system in order to observe even some
anomalies ok all right ok now here's
some experiments varying chain length ok
so just making the chains longer and
longer and again you can see that for
various different configurations we
still get signal we can still detect the
influence between the head and the tail
even when the the chains of components
are extremely long all right up to 14
actually just we can't see was out at 14
so up did you can see yours up to 10 ok
so we do fine up to 10 here we vary the
regularity would be varied about the
amount of noise in the timing so we add
some noise to the timing and you can see
that as we increase the amount of noise
so that regular behavior and almost
behavior look more and more similar you
know the yes the influence drops off but
we're still able to detect it ok we're
still getting a signal above the
baseline and then finally adding
something to the measurement
measurement noise is a little bit
different from timing noise but it has a
similar kind of pattern whereas you had
more and more noise you know the the
strength of the correlation decreases
but it's still noticeable above the
baseline measurements and finally here's
the effect of dropping log entries so if
we just discard a certain fraction of
the log entries with some probability
this is how much influence we still see
and the important thing on this slide is
that even out here where we're throwing
away half of the log entries okay the
there's still a significant amount of
influence detected and what does that
mean so when you throw away a log entry
that changes inter arrival time because
if i delete a log entry you know the
time between the next one and the
previous one goes up by a lot all right
because it essentially as a whole
there's a whole event that's missing and
yet that doesn't that doesn't that's not
enough as long as even up to deleting
half of those to make the influence look
like noise yeah yeah yeah yeah just pass
over each log in with it flip a coin and
discard the log entry with that
probability okay so what does this mean
what the point of these experiments are
just a show that influence is a robust
notion okay that even when there's a lot
of noise even when there's a lot of
imprecision in the data even when you
don't have we have a very complicated
system with all kinds of stuff in it
there's still detectable influence we
are even across although even if you
have all those things at the same time
you can still find the influence of one
component on another all right and so it
should this would suggest actually
should be useful in real systems because
you know you'd say it's not there's no
simple way to defeat it all right and
and so they'll still be some signal even
if you have all these other problems
okay now let's talk about Stanley so
Stanley is a robot car built at Stanford
and using the darpa grand challenge some
of you may know Sebastian who actually
worked here at Google for a couple of
years following this project so that's
yeah the robot is the car not the guy
sitting on it that's Sebastian all right
and I mean look okay we go oops so
Stanley actually won this race and won a
two-million-dollar prize it drove 132
miles autonomously no driver in a little
under seven hours and here's the big
moment where it crosses the finish line
is what you see in all the press
articles as it is it as it wins the race
we don't see is am I having trouble well
you don't see are these pictures so this
is during the race and that's Stanley
driving off the road okay so you can see
the road there is actually extremely
well delineated there's no even even a
robot should be able to see the bare
dirt in front of it and yet Stanley
decided that the bushes looked safer
than the than the dirt road and went off
and the question is you know why did
that happen and actually this happens
several times during the race probably
almost cost them the race at a number of
points and there's over 17 times during
the race when Stanley swerved because
apparently it saw phantom obstacles in
its map so it builds a map internally of
what sees in front of it and they could
actually look at the log data afterwards
they could see these objects in the map
that we're clearly not there in the road
all right and then clearly how did those
things get there all right so now I'm
gonna do is I'm going explain the bug
view and then I'm going to explain how
we can use influence to find the source
of the bug or to help find the source of
the bug
are we doing on time we're doing fine
okay so here's what happened so in order
to hand out a little bit about how
Stanley work sorry to explain this but
so Stanley has bunch of lasers on top
that it uses to see the road in front of
it so and that's just normal
echolocation kind of stuff so you know
it sends out laser signals it gets a
return and you know they can tell from
the angle of the lasers and now what the
time was you know where those obstacles
are now the trouble is that when you're
out driving in the desert or even the
highway you bounced around quite a bit
ok so Stanley to take that into account
has an IMU and inertial was that CM
stand for what now that measurement
that's what the M is for inertial
measurement unit it basically tells it
which way the car is pointing currently
today when it's driving flat the
inertial measurement is straight up
saying we're flat you know we're
oriented correctly to the to the plane
of the earth alright and it uses that
plus some information from the lasers to
decide whether it should keep going or
whether it should take some kind of
evasive action alright so when it's you
know oriented correctly it doesn't see
any obstacles in front of it the lasers
say everything is clear then it can just
keep driving all right now if it goes
over a bump that will change the angle
of the lasers and it might see the road
in front of it ok cuz now the lasers are
pointing down so it'll see stuff in
front of it but it knows at least that
it's pointing up ok me knows it went
over a bump because the inertial
measurement unit says that it's not
oriented correctly anymore and so it
expects to see the road and so you know
it sees stuff in front of the lasers it
still keeps driving because they know
things well that's just you know because
we went over a bump and the lasers are
now pointing down instead of looking out
to the horizon alright ok but then what
happens sometimes is that the lasers
would that would go over a bump ok and
then the lasers for some reason would
stall they would fail to deliver their
current readings ok
the car would return to normal position
so the IMU would say now it's pointing
straight up then the laser data would
come in saying it saw something in front
of it okay and it would appear that the
car was oriented towards the horizon the
laser saw something and it would decide
to take evasive action all right so
source sometimes for some reason the
laser data didn't come in at the right
time you know at the time with the bump
and was delayed and so that by the time
the laser data arrived the car was you
know the mercial inertial measurement
had returned to where it should be and
the car thought there was an obstacle in
front of it when in fact that was just
looking at stale its tail laser data all
right okay so I turned out to be it
sounds like a timing dependency in fact
it was but it's a very non deterministic
boggin like shows up once in a while and
it took them two months to find this
unfortunate we didn't have our stuff
ready at that time okay so we weren't
able to help them find it but they did
eventually find it after a two-month
debugging search and they actually they
worked pretty hard and it wasn't just
that they you know somebody worked at a
part time for two months I mean think
they were intensively looking for this
for a couple of months and why did it
take so long well so here's the
hand-drawn dependency diagram of
stanley's components okay so this is all
the stuff in the vehicle and all the
different components and the lasers as
you can see are up here in this corner
alright so they're a source and of
course the output is and you know
somewhere far downstream and there are
many many connections between these
components and so basically they had to
start at the swerving behavior and try
to work backwards through the components
to figure out what was going on and that
just there were so many possibilities
and so many ways in which this could
have gone wrong it that just took a long
time to do you know to narrow down and
home in on what the problem was
alright so here's what we can do so just
using why I've told you so far we take
the all the log files because it was a
fair amount of log data from the race
actually and we compute the anomaly
signals and there's just some anomaly
signals and for the lasers and for the
GPS velocity estimation and then we
build a cig okay here's the structure of
influence graph with the threshold set
at a particular level and well we can
see here in this oops in this Sega a
couple of interesting things so let me
just mention a little bit about the
notation there's a little bit of
compression here so when there's an edge
so this gray box with an edge out of it
means that there's an edge every member
of this box to to air to the head of the
edge okay so it's just a way of
summarizing lots and lots of edges all
right so you can see if the lasers are
in a click okay so specially that they
all influence each other perhaps there's
a missing component here that that
influences that lasers are somehow tied
to and as you would expect though the
lasers influence the planner so the
planners the thing that makes decisions
and so information from the lasers and
amelies and the lasers proceed anomalies
in the planner okay so let's in order to
help find the buggers one more thing we
need to do so the one thing we don't
have is any kind of representation of
the swerving behavior itself so we have
we had this behavior that was bad and we
need to somehow isolate what caused that
from the rest so essentially what we
need to do is to add another component
to the model saying when did the car
swerve all right and so what we did was
we just took you know from the video
figured out what time what times during
the race the car was swerving or very
much how'd you do it was it from the log
files you did that from the quote okay
oh yes from the x given in the quote
right so I just want to you know the x
given in the journal paper saying it
swerved at this time it swerved at that
time we just went in and added a signal
that went from zero to one at that time
okay saying there's an anomaly at that
time so we just marked the places where
it swerved in time and then we asked
how is that correlated what influence to
swerve share with any of the other
components in the system okay and this
is the thing that pops out and the key
insight here is that the lasers have a
very strong influence on swerve so does
the planner but you can see that the
planner anomalies proceed swerve and
actually it's bi-directional so they
haven't it the same that it actually has
a bi-directional edge there but
preceding that and influencing the
planner are the lasers which we saw
before okay and there's nothing else
really involved except for temp which
I'll talk about a little bit in a minute
okay that's the temperature sensor
alright and this turns out to be the key
insight so this from this graph and what
I've told you we can deduce where the
bug probably is because it turns out
that the lasers share a bust okay and
then when that bus gets congested the
messages are delayed and that's where
the problem was okay so it's a missing
component here which we can see from the
klique that's that's holding it they can
potentially with that when that shared
resource becomes a bottleneck then that
causes these messages to arrive late and
we can see that all these anomalies are
highly tied together alright so I would
help and that cut and it cuts out tons
of other stuff in the system saying of
the problem is probably not there okay
so they tell us that this would have
been sufficient for them to home in much
more quickly on the bug if they had
known had this information alright
what's going on with temp so this is why
race was in the desert and so the
temperature rises constantly during the
day and so so it turns out that just
because of the time when they swerve
because the timing of this war was in
one part of the race and not the other
part of the race that there's a inverse
correlation with the temperature sensor
because the temperature readings were
lower during the time it swerved and
much later in the race when the
temperature when it stops swerving and
the readings were much much higher so
that's a spurious that's a spurious
correlation
okay and that arises mostly because we
had very very sloppy specification or
the bug our swerve our swerve component
you know was was was just drawn very
clumsily we didn't really we didn't band
out exactly the points where it swerved
okay so if we had done that presumably
that wouldn't have not been as much
influence with temp all right so what
are we going to do now so we've applied
this today as I to other races that the
Stanford robots have been in we've we've
looked at using this in security
settings where we ask if there's a
correlated anomalies between say all the
web browsers on your network okay so if
all the web browser start acting funny
around the same time that's an
indication of some kind of fast
propagating attack okay and there's a
technical sense in which we can show the
asymptotic Assam sorry asymptotically
perfect anomaly detection there and now
we're trying to extend this idea of
using of querying these graphs because
it seems like the really the most useful
thing the ability to take the cig and
ask questions so I adding extra
components like the swerve component and
playing what if scenarios what if
nothing happens here what if something
happens there what do I know about the
pathways you know of influence so that's
our current effort and we have one large
data set which is some data that Adam
Adam owner who I is that is working with
this is actually the primary person
working on this I'm just the the talking
head okay so I Adam is here in the front
row we have one very large data set that
adam is prepared from super computer log
so we're going to try out but what we
really like is to have more data from
complex systems and the way these kinds
of techniques work and all my experience
in this kind of analysis suggests is
that you know the bigger the system the
more data the more benefit that you get
from these kinds of automatic techniques
I mean you're trying to boil the ocean
you're searching for a needle in a
haystack and so you need very very large
haystacks
you know to have a chance of finding a
few good needles and so we would really
be interested and the reason we came
down here today we're hoping anyway is
that somebody here would be interested
in in helping us out and possibly we can
help you out with the analysis of of
some complicated system if you have
interesting log data and so I'll leave
it there and I think we have time we
should have time for questions perfect
yes which was actually right so the
question was what what data was actually
being gathered from the lasers in the
log file so Stanley actually had
extremely extensive log files for all of
the input components so anything that
was you know reading data from the
external world they logged everything
they logged the time and the value and
the reason for that was so that they
could replay things in the lab so they
could actually replay races try out new
algorithms you know on simulation
without having to go out and drive again
because it was was a expensive thing to
go down to the desert for three days and
gather all this data and so all we're
using is the inter arrival time of the
messages that's the only feature that
we're using in all of these experiments
so
imagine that you might have some signals
where you know when this value is high
then that is strong yes yeah so the
question is you know could we could we
can we make use of the other data in
those logs and certainly we could okay
we haven't done that yet I mean the
anomaly signal abstraction is general
enough that if you have a way to map you
know anything any kind of signal into an
anomaly signal to say you know what's
normal and what's different then you
could just plug that in as another
signal to date we haven't we haven't
found necessary and and part in I went
over those micro benchmark experiments
kind of quickly but part of the point
and some of the points I made earlier
was that even when you would like to get
semantic information out of these out of
these logs and make use of it the timing
often has carries that information
anyway because if you're passing
different kinds of data around things
take the distribution of timing of the
law get messages becomes different and
so there's an we often pick it up that
way that's not to say there wouldn't be
useful to make use of it to make use of
that extra data in some way we just
haven't done at this point yeah
also
car car battery rights a question is
what you know with another way to find
this is a question by finding the
problem or about designing a robust
system yes uh basically system is Boston
kind of try to design a process that we
both like today may happen it's just
stupid problems but if I couldn't hear
the last part time so what may happen
either designer of our system but there
is still some problems but the problems
of the design right how business you
know the collectibles let's say would
have a possible tension through the
newest EP are possible tension
right exactly exactly right right yeah
so the question is you know what happens
oh well so the example is you know what
would happen if there was more
redundancy in the laser so you had a
completely separate set of lasers with
their own pathway and and so the system
was more robots two anomalies in one set
of lasers versus another okay so I'll
just say that in the case of Stanley
good way this isn't this is not
answering the question yet so but
there's a pretty tight power budget when
you have an autonomous vehicle and so
you can't so actually the number of
lasers they had was already somewhat
redundant they had more lasers than they
actually needed but you couldn't put
completely independent systems on there
and stay within the battery constraints
you know what the what the card is able
to generate support because they also
had a lot of other stuff in there all
right I think I think the general form
of your question is you know what if
there is something in there that absorbs
influence what if you have something in
your component that does dampen
influence so you have you have some kind
of a component that is say reading in
from three or four redundant components
and voting and then you know passing a
lot so yeah so at that point that will
that will that will definitely cut the
influence that you won't see as much
propagate beyond that point the signal
will get a lot weaker all right and yeah
I guess the be honest answer to that is
that something we would like to explore
I mean how often does that actually
happen because we mean so far i would
say i'm i'm surprised at how well
influence does propagate i would have
thought that even in the simple
experiments we did we would see more
problems observing influence at the end
of the log chain ok so my feeling is
that unless you're specifically
designing that in it's unlikely to
happen ok that mean the reason that you
dampen influence is because that's what
you wanted to do and if you know that
then probably you can use these tools in
a way that would help you you know
confirm that that's what's going on yeah
because you know you expect to see the
influence drop across that boundary and
if it doesn't then the other things not
working
way it should right yeah so this is more
of a comment but here at Google the
notion of blogs i think is rather
different from what you have on Stanley
and here who I'd say that gorgeous teen
is monitoring information so this system
is seen as many operations per second at
this point time and we have that over
some about history and you can correlate
that to some underlying system and its
CPU usage or its memory usage or
something like that so so in terms of
getting people to think about this I
think monitoring ok ok sure so so the
question was you know this google has
gone logs or gathers data about its
running systems they look very very
different from the kind of information
that we get out of stanley and you know
one thing about Stanley is that it is a
robot it's it's a real time system and
so timing data you know is one of the
most important things and so you have a
living the fine grain timing fits very
well into the timing model that we're
that we're presenting that I presented
in this talk but certainly may even the
kind of data that you mentioned we could
easily I could easily imagine how you
would convert that into an anomaly
signal you know maybe so you know in
particularly out that if that
information is being logged on a regular
basis they every made it it's dumping
that into the log then timing is not
going to be the right notion ok you'd
rather use you'd rather use the number
of outstanding you know whatever it was
you had a count of how many you know
requests are being serviced or something
like that and if given period of time
and you'd use that as the signals those
numbers would become the thing you would
convert into you know it's generally a
servicing way more requests that normal
are many fewer than normal and that
would be the basis for it so it has to
be some understanding of what was
actually in those log files but but but
thank you for the comment yeah so weird
we're not interested just in timing data
I don't wanna i want to make a whole bit
hope that's clear yeah
think we need to stop to you're sure
okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>