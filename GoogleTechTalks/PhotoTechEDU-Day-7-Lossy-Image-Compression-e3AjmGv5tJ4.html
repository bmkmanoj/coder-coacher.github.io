<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoTechEDU Day 7:  Lossy Image Compression | Coder Coacher - Coaching Coders</title><meta content="PhotoTechEDU Day 7:  Lossy Image Compression - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PhotoTechEDU Day 7:  Lossy Image Compression</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/e3AjmGv5tJ4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay well thank you all for coming I'd
like to talk about lossy image
compression and I'd like to start with a
statement of the problem so unless the
image compression we have an image
that's a rectangular array of pixels and
we'd like to represent that with as few
bits as we can but not so few bits that
we have to give up too much quality so
that's the trade-off and we can think in
terms of two extreme points just to get
our head around the problem so at one
end of the spectrum we have perfect
fidelity which is lossless compression
lossless compression is where we insist
that the decoded image is bit for a bit
identical with the one that we encoded
and if I do that then I'm going to have
to spend more bits of the ideal code
length would be the base 2 logarithm of
the probability of that whole image
using a suitably a predictive
probability model and at the other end
of this hypothetical spectrum I have
zero rate that's when I send no bits at
all and they're the decoder has to just
guess what the image was both of those
are kind of extreme points the lossless
case is actually a realistic scenario
the case where I don't send any bits
probably isn't but it's good to think
about so for anybody who's worked in
image compression you've seen this image
before um this is the Lenna image it was
used a lot in the early days of image
compression and then because people
started up computing their performance
numbers on this image then other people
used it to compare against that they
could compare apples to apples and it
sort of got a foothold so if you flip
through the I Triple E transactions on
image processing for example you'll see
this image a lot here's an example of
compression about a factor of a hundred
to one compression using the standard
jpeg algorithm and i'll talk a little
bit about jpeg but i won't go into the
details of the standard but you can get
an idea when we're spending more bits
about 790 kilobytes i have the original
RGB representation and then if i
compress it
a suitably course quality factor with
JPEG then i'm down to about 7.2
kilobytes we know how to measure a bit
rate we just see how many bits come out
to measure the quality it's a little bit
trickier one of the measures that people
like to use is called pica pica SNR or
PSN are and it looks like regular
signal-to-noise ratio where the
numerator instead of having the signal
variance we have the range squared so
the range of possible values
traditionally with an 8-bit pixel
representation all you have the
luminance scale from 0 to 255 so we have
255 squared in the top and then the
quantization error or the total coding
error in the bottom so many papers
complain about a PSN are as not being
the greatest measure in the sense that
it may not reflect actual subjective
quality and there's been a lot of
research in various places over the
years to try to come up with automatic
measures that better capture or better
track the subjective quality assessments
I'll perceptually weighted mean squared
error and things like that but p snr is
unambiguous it's easy to use it's easy
to compute oh it's what other people
have used so you can compare your method
against other techniques and it's a
pretty reasonable thing to use in
certain situations namely when the
overall noise is lowest the quality is
high it's a pretty good measure and also
it may be meaningful at lower rates when
all you're comparing techniques that are
that are similar and what they do that
are comparable
so to compress an image we're going to
have to take advantages advantage of
certain properties of the image and also
of the all receiver of the destination
or the person looking at the imager so
for any kind of compression we typically
want to eliminate the redundancy in the
source that's true for both lossless and
lossy compression we'd also like to
exploit the human visual system for
instance if the human visual system is
less sensitive to say noise along edges
if we can hide degradation along edges
and images and we'd like to do that and
then aw in the trade-off do a better job
in the non edge regions for example
similarly if we're not very sensitive to
spatial resolution in chrominance then
we might like to take the luminance
component and encode that with very high
accuracy or acuity and then give up
something on the chrominance components
and we know that we can do that in fact
in general we'd like to omit the
irrelevant information concentrate on
what's important for the task or again
with respect to the human visual system
and we're always going to have this
fundamental trade-off between the
quality and the in the bitrate by the
way please I'll stop me at any time if
you have questions along the way it's
natural to ask what the best we could
possibly do is and if we're willing to
make certain mathematical assumptions
and adopt certain types of models then
we can answer these questions to some
extent if I insist on a perfectly
accurate replica of the original so if
I'm doing lossless compression then the
quantity that limits what I can how well
I can do the minimum number of bits I
need on average to represent a source
without loss is the entropy and that's a
basic result in information theory
Claude Shannon showed it back in the 40s
and it's I have to emphasize it's an
average measure um it's something that
you can't beat on average and you need a
probability model right so the entropy
is a probability of
it's a it's a function a love P of X so
it's not really a function of the
physical source as a function of your
model of the source if you have a better
model of the source then you can get a
lower entropy if you try to cheat by
coming up with a model that has a really
low entropy well then you get hit when
you trot when you compete the cross
entropy where you try to code with
respect to that model then you get hit
with a lot of bits okay so that's lost
less compression now is there an
analogue in the lossy case and in fact
there is it's a little bit more
complicated and instead of a single
point it's actually a curve it's called
the rate distortion function and all it
takes two things to compute the rate
distortion function one again you need a
probability model of the source so
that's the P of X but then you also need
a distortion measure and that distortion
measure has to satisfy certain
conditions in order for you to be able
to plug it into this framework and the
quantity itself can be expressed in
terms of a hypothetical test channel so
I have some input to the cypha thet
achill test channel I look at the output
and I consider all such test channel
such that my distortion that I get by
using that channel doesn't exceed my
specified distortions of a parameter of
the point on the curve and over all of
those hypothetical test channels I take
the minimum of this is the mutual
information you can see this mutual
information if it's more familiar
probably if you multiply top and bottom
by P of X so the mutual information
between the input and the output of the
hypothetical test channel over all
possible test channels that achieve the
specified distortion is the definition
of the rate distortion function and the
important point about that is it's a
quantity that you can't do better than
so if somebody tells you I've got this
probability model that I really believed
and I've got this distortion metric that
I think is the right one and oh by the
way I want to encode at this bitrate
with this distortion but it's it's not
on the right part of the rate distortion
curve so here's typically you have a
rate-distortion curve that looks like
that if the source is discrete then you
can losslessly encode it with zero
distortion at some finite rate if it's a
discrete sort
so that's just the entropy this if we're
using mean squared error would be the
original signal variance and so the rate
distortion function is a curve that
looks something like that and you can
never aw you can never get into this
region down here so if somebody tells
you I'd like to code down there and you
believe the source and you believe the
distortion metric then you can't do it
okay so let's talk about actual
approaches to compression well one
approach the compression we can call
semantic compression and so I tell you
it's the lenna image so that's my that's
my code and that's about if you compress
that sentence it's maybe 30 bits 40 bits
something like that and we've
represented the whole image but we've
got a varying degree of fidelity
depending on what you come up with in
your head and it's not very useful as a
coding scheme and it's not very easy to
do I should say I mean if you if you
think about pattern recognition and
image understanding etc in a certain way
then maybe we could imagine image
understanding techniques and advanced
sort of artificial intelligence analysis
of national scenes as trying to do
something like this but when we say
compression what we really mean is
something to which we can apply the PSN
our measure at least that's what I mean
and so we're talking about techniques
that try to preserve the intensity array
the image so one approach is
differential predictive encoding it has
its roots in one dimensional signal
processing speech compression and the
idea there is you're going to encode
pixel by pixel you're going to go
through the image and maybe a raster
order and for every pixel you try to
predict what that value is and the rule
is you have to predict it based on
things that the decoder already knows so
things that the decoder is already
predicted so the decoder has a noisy
version of the previous pixels that it's
already decoded all the encoder can know
what that is and so it forms its own
prediction knowing that decoder will do
the same thing then instead of sending
that value it sends the difference
between the prediction and the actual
value than the
oder knows okay and the advantage of
doing that is if you've got a very good
prediction then that difference will be
small on average it's not going to have
much information in it so it'll be
easier to encode you can get away with
just sending bits when you really need
to okay so that's differential
predictive I'll encoding and the
prediction residual that error signal is
actually if you if you have a really
good predictor it's usually an easier
thing to encode it doesn't have
correlation it's probably got a pretty
decent probability density function that
you can deal with etc another approach
is vector quantization this is a great
theoretical interest and also some take
some actual coders have used this and
have done pretty well the idea here is
to divide up the image into blocks and
we'll see that that's a recurring theme
in glossy image compression then we
treat each block as a point in space as
a vector and to encode we're going to
replace our input vector with its best
representative in some finite set called
a code book of vectors oh and then that
is what you reconstructed the receiver
and there the goal is to minimize the
average squared error with a constraint
on the bits you spend on the index for
the code the code book index another
technique or general class of techniques
is transform coding and here we analyze
the image into a set of frequency bands
and we will allocate the available bits
we have to those bands that are most
important and we can do this differently
from region to region and here the idea
is that we'd like to just allocate the
bits to the spectral and spatial regions
where they're really needed this family
of techniques includes jpg or jpeg 2000
or at least the wavelet coding part of
jpeg 2000 and it's not my goal here in
this talk to go into the details of
these standards but I I will cover the
principles behind the
standards so it's the most widely used
compression technique it's also used or
parts of these techniques are also used
in video compression it's an old
technique it goes at least back to the
early 60s with long and shultiess and
I'm going to talk about transform coding
sort of vanilla block transform coding
first and then we'll talk about some of
the generalizations of it so here is a
transform coder and I'll walk you
through the different parts of it let's
take a look at it for a minute so we
start with the the input and again these
are you tile the image and when I say
image you can typically do each color
component separately or you could do the
luminance component in chrominance
components separately so let's say you
do a 0 a decomposition into luminance
and chrominance so you take the
luminance you title it up into a bunch
of blocks non-overlapping blocks and you
input one block at a time and you
process it and then you get a block at
the output and then you tile it back
together at the receiver with what you
get out so the first thing we do is we
apply a transformation to the pixels in
that block so think of this block it's a
two-dimensional block but think of it as
I'll just sort of unroll it into one big
long vector so now you've got a big
column vector so this transformation
think of it as a matrix that you're
applying to this big column vector okay
and then at the other end you're going
to collapse it back into a
two-dimensional block oh so we'll get
back to this the goal of the
transformation is to try to make this
vector more compressible in an easy way
and we'll talk about that so now that
we've got the transform coefficients
we're going to allocate bits to the
different coefficients and tell the
quantizer is what those bits are and
depending on how we do it we may or may
not have to transmit that information
explicitly to the decoder
now the actual a lossy step in this
whole thing is the quantization that's
where we take a real number flowing
point number that comes out of the
transform and convert it to a discrete
value so that it can have an index so if
we didn't have the quantization we'd
have a perfect a lossless process but
then we wouldn't have any we wouldn't
have a digitization at all it would
still be floating point so let me ask
I'll can anybody think of why we use
scalar quantizer here I mentioned a
minute ago the possibility of vector
quantization we'll get to it but i'm
wondering if people have an intuition
already yep
right computationally much more
attractive to use scalar quantizer and
so another part of that though is we'd
like to use a simple scheme like scalar
quantization because it's it's
computationally much simpler the
question is can we get away with it and
and so another part of I think the the
reason is that the transform actually
lets us get away with scalar
quantization we we still would gain
something by doing vector quantization
in terms of performance in terms of mean
squared error but because of the
transformation we don't gain as much by
doing it will get back to that okay then
we've got after we have these discrete
values after the quantizer these are
quantization indices then we encode them
and transmit them over the channel and
when I say encoding I mean both the
compression and whatever protection
against channel errors you might want to
do if you're not transmitting these
images but just storing them on a disk
and you probably don't have to worry
about that but the encoding then would
still involve the lossless compression
that you would do on the index on the
quantization indices okay let's talk
about the transform for a minute so the
transform in traditional transform
coding is linear it's typically
orthogonal and the goal here is to
compact the energy or the information
present in the input vector into ask you
all components as possible so after
transformation all the important
information should be sort of packed
into only a few of those coordinates in
the resulting vector we should observe
that if the transform is orthogonal then
it preserves all Euclidean norm and so
if we care about mean squared error let
me model that intermediate processing so
across all the quantizers think of in
the vector domain think of that adding
this vector ada and if I care about mean
squared error I can represent that
quantization error by the way as
additive whether or not it's independent
I can always just define it as being
whatever changed in the signal I'm just
thinking that I'm adding that
and the fact that the transformation
preserves norm means that if i look at
the mean squared error of ada or the
mean squared value of ada i should say
then that will be the mean squared error
for the overall system because by
linearity i can represent y as being in
terms of the distances or squared values
the sum of x plus 80 right so the reason
I want to do that is because it allows
me to simplify the problem a little bit
now I can say that the overall error for
my whole system is equal to the error
that I get in the middle so whatever I
do in the quantization of the middle is
going to translate directly into the
denominator of the PSN our measure okay
so we'd like to compact energy and how
do we do that well let's choose the
block size we'll come back to that also
in a minute but for now let's say we
choose the block size so here's a recipe
it's a famous recipe for coming up with
a transformation that packs as much
energy as possible into as few
components as possible so you estimate
the covariance matrix of these vectors
so you have a bunch of training data a
bunch of examples you form for each
vector you form the outer product with
itself will subtract off the mean then
form the outer product with itself and
take the average that will give you an
estimate of the covariance matrix you
find the eigenvectors of that covariance
matrix and you normalize them and then
you use those as the columns of your
transformation and if you do that
that'll diagonalize the covariance
matrix in the sense that after applying
this transform if you then compute the
covariance matrix that would have no
off-diagonal elements it will be zero
off the diagonal so it'll be
uncorrelated and the other property is
that if you were to then take the
product of those diagonal elements and
all those diagonal elements are the
variances of each coordinate take the
product of those variances and that will
be minimized for a constant trace the
trace is the sum and that's the total
energy in the signal so if you constrain
the total energy which will be
constrained if this thing is orthogonal
then you minimize the product of those
diagonal elements in that wall
translating to coding
but that is the sense in which this
compact energy it's in terms of
minimizing that product so doing this
saw is well known it so it has different
names depending on what field you're in
it's the karun and love transform in
electrical engineering it's the hotel we
transform maybe also an electrical
engineering or maybe statistics and
principal components analysis is another
name so how many have heard of at least
one of those great so one question is
why then not just always use the karun
and the web transform ya the dct does
just about as well without require that
ok i buy that completely yep send all
dimensions are right
right so some of them are completely
zero and you know they're always going
to be 0 then you've never sent me yeah
or well with the hype well right um
about the same distortion as the others
we'll get to that yeah so let me just
amplify what you said oh the KLT is data
dependent and it's relatively hard to
compute all so that's a little bit extra
cost and you have to ask well how much
performance am i gaining by doing that
relative to the DCT for instance also
you have to tell the decoder what it is
right so you'd have to transmit that as
aside information so that would be an
extra that that would come at the
expense of the bits you have for the
data itself so instead people have sort
of settled on using the discrete cosine
transform as a one-size-fits-all
transform that is a pretty reasonable
job compacting energy for a wide variety
of sources particularly images positive
positively correlated sources and uh you
can see that it's just a sampled cosine
it's a discretized cosine with the right
frequencies and the right for each basis
vector and the right normalizing factor
to make it orthogonal
so here's a oh I should mention all what
I showed you was the 1d formula and this
is also 1d transform it's a matrix that
applies to a vector when you actually
applied the DCT and image coding you do
it separately first could do the rows
and then you can do the columns it's a
separable separable transform that's
what it looks like for N equals 4 if you
look at the top row you'll see that
that's just taking the average as you go
down you increase frequency here's a
here's an eight by eight an eight point
DCT rather again the top row is
computing the average of the block right
the DC value and as we go down to the
bottom we see that the coefficients
alternate every song every every other
sign is different so that's the highest
frequency basis vector right so let's
talk for a minute about choosing the
block size and let's say our goal is
just to compact energy because that's
all we've really talked about so far so
one experiment we could do is try to
estimate how much energy compaction is
going on for each block size and then
choose the best one so here's an
experiment for for several different
block sizes we compute a DCT of that
block size on on wheat I'll the image
into that block size and compute the DCT
on each one and then we skim off the
highest absolute value K coefficients
out of the in this case it's a 256
squared image so it's 65536 total pixels
so out of that we choose K that are the
biggest ones right and we do that over
the whole re tiled um transform domain
image ok so that's the experimental
setup here and we're going to plot the
reconstruction error as a function of
the ratio of K 2 K over 65536 so that's
what the x-axis is it's a fraction this
is a fraction of coefficients retained
and it maxes out at 1% in this graph on
the y-axis is the total reconstruction
error I get by keeping only those
coefficients and setting all of the
other ones to 0 0 it's a little bit hard
to see can people see that a little bit
great so the one was probably hardest to
see is the N equals 2 56 and I can just
tell you it's it's over here on the left
oh the thing to take away here I think
is that the large block sizes have a lot
of energy compaction if we if you if we
look at keeping very few coefficients
and there's a little bit of crossover as
we go farther to the right but by
looking at this graph we might be
tempted to say well if we use a really
big block size than we get the best
energy compaction now it's a fact that
JPEG doesn't use very big blocks that
uses eight by eight blocks so we're
going to try to figure out why that
might be let me first show you the
result of reconstructing using a
1-percent so that was at the right-hand
endpoint that's using one percent of the
coefficients when N equals 4 n equals 8
n equals 16 n equals 64 n equals 256 so
we can see that it starts to get
interesting at N equals 16 or so this is
n equals 8 &amp;amp; N equals forward or are not
useful
so let's say we were back instead of
keeping one percent of the coefficients
I don't think I have the pictures so one
percent of the coefficients is right
here where were these three curves
there's actually a third curve in there
that's hard to see where these three
curves some are kind of are on the same
point in terms of distortion but if we
were back here somewhere we'd probably
see that the N equals 64 and N equals 2
56 is significantly better so let's say
we were trying to do a very aggressive
compression ratio like that would we
then I'll choose a big block size what
are some of the reasons why we might and
might not so by looking at energy
compaction alone it misses something so
in particular let's consider what i'm
doing here is i'm plotting the log of
the average squared value of the
magnitude of these coefficients so in
the case of the four by four I take
every transform domain block and I
square the value when I average that
across the whole image for every block
in the image and I take the log of that
and that's the that's what I'm I'm
displaying a scaled version of that and
I do the same thing for N equals 16 when
i get to N equals 2 56 well the whole
image is 256 squares so there's only one
block so there's nothing to average but
it's a squared value that I'm showing
the log of the squared value so the
reason I put this up is to indicate that
there is some statistical regularity or
predictability in the distribution of
energy that you get when you have
smaller block sizes that you don't quite
have when you when you have n equals 256
so remember this experiment let's say we
actually want to design a coder where
I'm taking I'm skimming off the top I'll
take K coefficients and sending those to
the decoder so I send the decoder you
know 100 numbers or whatever and the
decoder has to reconstruct the image
well the decoder knows that it has the
biggest numbers the most important
numbers but it doesn't know where to put
them so I have to tell the decoder where
these things go
and to tell them where they go requires
some bits and it'll be easier to tell
the decoder where they go if I have more
predictability right so the way I've
just described the experiment I've got
it by brute force I would have to
specify the locations of each of these
coefficients in this transform domain
tiling right so 65,000 locations there
about k coefficients that would be all
the base to log of 65,000 choose K right
so a big a big number if I just do it by
brute force anything that I can do to
reduce that number would help and in
particular there's some statistical
regularity here and if I even go a step
further and say well you know what I'm
going to use 16 by 16 blocks and I'm
always going to give the same number of
this two coefficients in the same
position in the transform domain then I
wouldn't have to transmit any site
information at all in to the extent that
this sort of distribution of energy
pattern is is regular and predictable
across images well then that's a win I
could get away with that in the case of
using the whole image as the transform
sighs well then I've got a very detailed
information here I could not reliably
say that well this black pixel is black
for this image and it's going to
continue to be black for some other
image because the other image might have
diagonal structure along some different
line and you may have to send that so is
that is that clear to everyone there's
one more related point so the point that
I already made was we have to tell the
receiver or what this information we're
sending corresponds to and there's an
activity map where there's some
information that has to be sent either
implicitly or explicitly for that
there's also the issue of adapting to
non stationarity so if I take a big
transform of the whole image that
averages over everything in the image
there might be some region of the image
that has all all large all low activity
regions there might be other regions of
the image that are highly detailed maybe
I care more about a face in the image so
I want to do a better job on the face by
using a very big transform it doesn't
allow me to get that sort of selectivity
across the image
so all choosing n equals 8 seems to
achieve a good compromise so jpg uses
that and other techniques so I guess
could use that also okay let's talk
about once we have the transform
coefficients let's say we've done a good
job with our transform now what do we do
yep
allows you to fund and
cool
I think in the in the very earliest work
on transform coding they did explicit
bit allocation where they had a side
channel that would transmit those bit
allocations and their wood and there's
not a well-known formal theory but in
some of the early work they paid
attention to that so maybe in the work
by Chen and Pratt in the 70s they might
have it seems familiar i mean i remember
encountering this question of how do you
quantize the and send the bit allocation
zone you send the local variances and
can you predict the variances because
that's where the bed allocations come
from and that sort of thing as for a
formal theory I don't know I guess it
would have to be the way i would do it
as an engineer is empirically for a
given code or once i decide that this is
what the code is going to be like let's
then see what we get by trading off
giving more a side channel versus main
channel yep

and the size is kind of havoc but if you
want to do this
antenna switch on that Illinois deleted
their erotic pragmatic reasons why what
a good theory that actually casted as an
optimization
maybe the race I yeah if there is I
don't know yep yep
this high resolution well maybe though
maybe the issue you brought up about the
visibility of the blocks I guess yeah in
terms of the solid angle you know the
that it takes up maybe there's a
perceptual reason for why depending on
the presentation size of the overall
resolution of the image you might then
care about the block size so yeah I
think there are lots of trade-offs to be
made I'm going to I'm going to talk
about other ways to get rid of the
blocking artifacts or at least mitigate
the blocking artifacts I'm glad you
brought that up because blocking
artifacts is a huge issue I'm skipping
hear this ok so let's talk about the
quantizer so a scalar quantizer takes
some real valued input and it produces a
discrete set of output values so for
every input that happens to lie in this
region it would always produce this
single output value so it's a
discretization I'll transform
discretization function i should say
there are two varieties that we care
about there's a fixed rate quantizer
which is very simple let's say I have a
quantizer that that map's everything
into eight possible output values then I
could simply represent each output value
by three bits for example another flavor
of scalar quantizer is a variable rate
quantizer and in that case I don't
really have a tight constraint on the
number of output values I could have
modulo practical constraints i could
have in principle infinitely many output
values as long as not all of those
output values get used with a lot of
probability in particular as long as the
entropy of the output for that input
namely for those quantized for those
transform coefficients I'll happen to be
if the entropy is finite if the entropy
is small
okay so empirically if we were to plot
the signal-to-noise ratio in DV of these
quantizers as a function of rate we
would see that they all kind of have a
similar characteristic after a while
they they have a slope of a 6 DB per per
bit but they have a different overall
quality factor which I denote it is as
epsilon there yeah was that a question
expands consider a tapered quantization
on i-84 closet I need to hear that again
sorry quantization vs. uniform
quantization yeah okay also the question
is what if we were to I'll use a
different steric staircase
characteristic see if I can so here I
kind of shown a uniform quantizer that
peeks out this is called a mid tread
quantizer because it has 0 as a
reproduction value it sort of straddles
the the midpoint if instead I were to
use actually that's not you know from
that's a little bit tapered so this this
is kind of optimized for something that
has more probability in the middle so if
you optimize the quantizer
characteristic for a certain number of
levels let's say eight levels and you
say I know my inputs going to be
laplacian for instance that's a decent
model for the transform coefficient
sometimes where should I put my
resolution of my quantizer in order to
minimize mean squared error for that
quantizer and the idea there is you put
more resolution where the input is more
likely and you give up some resolution
where the input isn't very likely and so
that's a very important thing to do from
the point of view of mean squared error
if you have a fixed rate quantizer when
you have an entropy coded quantizer what
happens is instead of constraining
instead of necessarily spending more
resolution on the highly probable
regions what's going to happen if you do
that is for those regions not only will
your resolution get better so your error
goes down for those regions but your
your entropy is going to be more because
if you finally quantize those high
probability regions you're going to have
more a greater number of highly probable
output
you see your entropy goes up so you pay
a price so there's this trade-off and if
you try to do the optimization and to my
knowledge there's two kinds of analysis
for entropy coded quantization that that
you see in the literature one is purely
numerical far Varden and others I guess
modesty no I did some work on that a few
years ago and what back in the 80s and
then there's some sort of asymptotic and
analysis analytical treatments go black
and holsinger in ish and Pierce and
others in Bennett did work that show
that asymptotically a uniform quantizer
under an entropy constraint is actually
optimal for a wide variety of sources in
a wide variety of distortion measures so
uniform quantization sounds like a very
simple thing to do it turns out to be
surprisingly close to optimal for a
broad class of conditions and so people
do it I want to qualify that by saying
when I say uniform quantizer I'm talking
about the decision regions the decision
reasons are uniformly sized the
reproduction value should always be the
centroid in order to minimize squared
error and for the innermost regions for
something like law seein source or a
gamma source or broad tail density oh
that may be far from the midpoint of the
uniform quantization region that that's
a great question i hope i didn't say i
hope i answered and i hope i didn't say
too much that yeah
okay so um empirically we could just
look at the error we get as a function
of rate when we use these quantizers and
in the case of a fixed rate quantizer
I'd have a bunch of a bunch of discrete
points on this curve that I would get in
the case of an entropy coated quantizer
I could change it more continuously by
changing the quantization resolution for
a fixed input and then see what the
entropy I get out is and in all cases
I'll see a curve that kind of can be
approximated in that manner with that
parameter epsilon and if I just add up
all of the contributions from each of
those quantizers in my transform coder I
just got the bottom expression which is
the sum which I show again at the top
here and remember what I'd like to do in
bit allocation is allocate bits to
minimize the sum of these quantization
errors subject to an overall budget on
my number of available bits so that's
minimizing a smooth function with
respect to inequality constraints i can
use Lagrange multipliers this is my
constraint I say the total number of
bits I'm spending I can't exceed my
budget are this is my total error so I
form this Lagrangian lambda is the
Lagrange multiplier and I take the
partial derivatives with respect to a
particular are and if I do that this is
just an exponential oh this just picks
out the are that that we're
differentiating with respect to and I
get a constant times the distortion
itself again I has to equal a constant
for all J for all our sub J if I wanted
to solve for the RJ i would i would say
that it's proportional to the log of the
of the standard deviation or the log of
the variance and if we in the early work
on transform coding people actually did
this explicitly so they would apply the
log variance rule and then they would
have to jump through some hoops to
ensure that when we have small variances
which we will have for some of the
transform coefficients right that's
because we're doing compression and
we've compacted away some of that energy
so we'll have a lot of small values this
formula the OP
formula would tell us that some of the
bits should be negative and we can't do
that so we have to re optimize but we
don't do that anymore because there's a
better way and going back to entropy
constraint quantization so if you
consider just for a moment the idea of
using a uniform quantizer and we know
that from an entropy constrained point
of view that's a pretty reasonable thing
to do for a broad variety of sources and
distortion measures we care about
squared error but including other
distortion measures so let's use a
uniform quantizer and let's consider the
high rate case so that's the case where
all of these quantizer so I've got some
staircase and let's say all of the
levels are being exercised to some
significant extent I could then
approximate my error for any one
particular quantization region the
contribution of that from that region
would be approximately uniform between
plus and minus Delta over two and the
error associated with that squared error
would just be the square of that density
between plus uniform between plus and
minus Delta over two so I get Delta
squared over 12 for that error on so
it's only a function of the step size
and when I look at the output entropy
I'm going to get some number of bits but
the error doesn't depend on that those
number of bits it only depends on that
step size that's my parameter if i set
the step size i set the quantization
error at least asymptotically so the
nice thing about this is if i just use
the same step size for all my
coefficients and if we go back and look
at this rule that i had by saying set
the same step size for all of the
coefficients then I actually implement
this because what this says is that my
quantization error which is everything
but that quantity should be constant in
fact all that quantity will be equal to
Delta if I use the same Delta for all of
my coefficients so that's kind of nice
that's a simplification and JPG
essentially does this but it has some
provision for multiplying these step
sizes differently for all different
spatial
frequencies and I guess chroma and
luminous might be different also okay so
let's go back to the transform right so
now I've sort of been talking about
separable transforms but now I want to
make that restriction explicit so I'm
only going to talk about separate wall
transformation and I'd like to interpret
the block transform in terms of multi
rate signal processing so the way I'm
going to do it is let's consider an
image row I'm doing separable processing
so I consider the row and the block
transform then lops off I wrote all
blocks of eight say and applies the
matrix transformation to each block I
get my transform coefficients and then I
invert the transform and reconstruct the
row another way to think about this is
in terms of a convolution let's consider
this blue convolution colonel so each
line here represents a different
weighting I'm going to take the weighted
sum of in this case four pixels compute
the weighted sum and store it away here
I'm then going to slide over by 4 i'm
going to skip over and do the same thing
and store it here etc then I do the same
thing for a different set of
coefficients I convolve record the value
slide over and I'm not showing all the
other two filters that I'd have to use
to apply the scheme I hope you like my
artwork
lawsy artwork yes I can interpret that
in terms of the critical critically
sampled filter banks a critically sample
just means that I'm subsampling by
whatever I have to make sure that I
don't increase the number of real values
that I see crossing this boundary in
crossing this boundary had preserved the
total number of degrees of freedom I
have a set of K filters I subsample each
one in this case uniformly by a factor
of K so I throw away all the samples
except every caithe one I do some
processing the processing would be
quantization in this case I upsample by
inserting k minus 10 s between adjacent
samples in my subsampled stream and I
put it through an interpreting filter
and I add everything up and it's
plausible that in the absence of
intermediate quantization or any other
corruption of these intermediate values
that I could design these filters such
that what i end up with is the same as
original that would certainly be the
case if i if each one of these things
were were kind of ideal right if each of
the bands were ideal so I'll come back
to that question about how to do that
but first I want to motivate all this
whole approach so now I've got a filter
bank instead of a block transform let me
think in terms of the input spectrum so
we know that the input is correlated in
terms of this power spectrum correlation
and power spectrum are related the power
spectrum is a Fourier transform of the
autocorrelation so we expect an
equivalence there if i look at the
spectrum for a 1d signal and let's say
the spectrum is highly non uniform it's
it's non flat so here's 0 to PI and
here's the the mirror image of frequency
so if you think of on the on the unit
circle so this is the Z plane with in
filter filtering terms there's zero
frequency and there's pie so I have a
spectrum that has a lot of energy down
here and it sort of goes down to zero as
I go over the pie what I'd like to do is
allocate bits to where the energy is
where the information is so I split up
the spectrum into these bands
put more bits on the more important
bands and if I look at what happens to
each band after subsampling well
subsampling this is after van pass
filtering I just pick out these to say
and after subsampling well that changes
that remaps my my unit circle a court
you know the Nyquist theorem and all
that so when I resample it remaps where
pi is and I see that the resulting
spectrum looks a little bit more flat in
this example and you'll see in general
since these vans are pretty narrow they
have less time the spectrum has less
time to change in that band so when you
stretch it out over the whole spectrum
by subsampling in fact you expect it to
be flattered and a flat spectra means
uncorrelated so we've done two things
we've compacted the energy by picking
out by separating the spectrum out into
these bands and then we've also d
correlated these sources because after
subsampling we have less correlation
okay I'm going to sort of skip over yep
fine but as you know oh ah yep great
great question so I will come back to
that what about aliasing well there's an
existence proof that we could get
perfect reconstruction in the absence of
intermediate quantization the existence
proof is that we can interpret the block
transform in terms of the filter bank so
then the question as well as the block
transform the best you can do and we can
tweak from there but this at least shows
that you can get away from the aliasing
problem because a DCT implemented as a
filter bank is your perfect
reconstruction okay so let's look at the
DCT as a filter bank I could simply take
each row in that forward transform
matrix and plot its frequency response
treating it as a filter and if I do that
I see that I get some band separation so
kind of hard to see the different colors
but there are also different line types
oh you can these are side lobes down
here but you can sort of I'll make out
eight different bands there these are
the first two armed convolution kernels
and there are two things that we'd like
to do here one is we'd like to get rid
of these blocking artifacts so one way
to do that is to allow these kernels to
overlaps from block to block and
the thing we want to do is maybe give
different frequency of selectivity to
different parts of the spectrum so when
the filter length is greater than k then
we have something called a lapped
orthogonal transform one question is do
we want to try to get these all ideally
frequency selective filters or would it
be better to use something else and we
notice that when we try to increase the
frequency selectivity we get more
ringing and perceptually ringing is very
objectionable so let's try to optimize
these filters while minimizing ringing
and so we can increase the filter lamp
and we see that when we do that the side
lobes start to go down and if we look at
the time responses we see that the time
response is when we allow them to grow
they'll ring but if we optimize in a
very careful way the ringing isn't a
problem and that's for the second
frequency band yep by compression is it
helps right right so I actually thank
you um so right so sharpen Gaussian
filters and Mexican Hat filters etc have
exactly one excursion below zero and
then they stop bringing in my
optimization procedure that produced
this actually respected that so good
thank you good point the second
opportunity for improving our
compression is to use different
bandwidths and the intuitive reason for
this is we want to have short spatial
extent filters for edges because we
don't want to be smearing what's
happening the edges all over the image
at the same time if we've got a nice big
blank region we'd like to compress that
well by using say just one basis
function for that whole big blank region
so we'd like to have different sized
filters and correspondingly different
frequency selectivities and one way to
achieve that is to all assemble all
multiple filter banks in a tree
structure so we recursively divide up
the DC axis of the DC region and we
could apply the separately two images
for instance I do a two by two
decomposition both vertically and
horizontally then I repeat on the DC on
the on the low pass
band and I keep going I'm gonna skip
this
let me actually skip the vector
quantization okay um so here's a
separable decomposition where I use a 3
by 3 decomposition yep yeah so dick
wants me to come back for another one I
kind of chose the material for this one
you know on the basis of what i thought
i could get through but yeah i'll be
i'll talk about the VQ again and in fact
is probably good to skip it completely
rather than to try to cover it so we
have a scheduling snafu if anyone's here
for life of a query I'm going to figure
out where we're supposed to be since it
doesn't bear to be here so hang out here
okay i don't know if that buys us a
couple of minutes i shouldn't go over
anyway but at least I feel oh yeah are
you are you here then or oh then let me
just stop I'd like to just wrap up in 30
seconds though so um let me just talk
about the top bullet so I'll we trade
off rate infidelity the transform
actually rearranges the input to make
the regularity more exploitable and by
thinking of the transform in terms of
the filter bank we get some good
insights that allows us allow us to get
rid of the blocking artifacts bit
allocation and entropy constraint
quantization all work together nicely
thanks a lot
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>