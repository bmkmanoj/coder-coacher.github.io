<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Quantum Computing Day 2: Image Recognition with an Adiabatic Quantum Computer | Coder Coacher - Coaching Coders</title><meta content="Quantum Computing Day 2: Image Recognition with an Adiabatic Quantum Computer - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Quantum Computing Day 2: Image Recognition with an Adiabatic Quantum Computer</b></h2><h5 class="post__date">2007-12-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vMvC-wv1ayo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon and thanks for coming to
the second tech talk on quantum
computing we'll talk about image
matching with an adiabatic quantum
computer and I'm very pleased that I'm
being joined by Jody Rose who is the
founder and CTO of d-wave and this
company is pioneering the design and
manufacturing of adiabatic quantum chips
before we get to explaining how this
works and it still goes to one piece of
series that we haven't covered in the
first talk and this is decoherence also
wanted to use it as a quick reminder
what we learned last time you might
recall that the base principle of
quantum computing at least in the gate
model is that you have a state and
here's this one zero zero state and you
push it through a gate and get some
result which you measure now the GAC so
to speak was kind of computing is that
you don't only have to do takes as one
state but you can create a superposition
of states and this is still even though
it contains eight base functions in this
case it is only one physical state so
with one clock cycle you push these
eight functions through the gate and the
gate operates on all of those
and learned that we can extend this game
to letter n qubits and then again was
one clock cycle one gate operation you
would operate on two to the N states
unfortunately and this is why we need to
learn about decoherence superpositions
tend to be fragile and decoherence is
essentially the name for the process
that destroys the superposition that's
why we need to study it to understand
practical quantum computers so the
coherence what it does again a qubit can
be or as typically a linear
superposition of a two system sadya
alpha A plus beta 1 and we learned that
the measurements you would get in case
of size the qubit is in the state would
be alpha modulus square or better
modulus square so your result would
consist of probabilities however this is
very important to understand a
superposition is physically something
very different than a statistical
mixture what is the statistical mixture
statistical mixture is more like a
shower game where your state is either
in stay or your system is either in
state 0 or in state 1 you just don't
know it
so probabilities enter because you have
insufficient information not of the
fundamental quantum mechanical reasons
and the coherence basically is the
process that takes the superposition and
turns it into a statistical mixture so
the typical situation where this process
occurs is where you have a system you're
interested in here your system s and
it's embedded in an environment e
typically much larger so the formal
element you use to describe the
situation is causing density matrix and
we're not to arrive at just explained
density matrix is used by quantum
physicists to describe this situation
which typically are always in with real
quantum systems and the density matrix
describes superpositions
these are called coherences in the
density matrix lingo and those are
described by off diagonal elements so
whenever those elements here are nonzero
you have super positions and again these
are the ones you need for quantum
computing and the guys on the diagonal
they indicate statistical mixtures again
this is not useful for quantum computing
called populations and decoherence now
in a more formal way of saying it is a
process of destroying off diagonal
elements and only leaving diagonal
elements and this is the density matrix
formalism and here one curious fact
again situation where we have a system
and the environment and the most general
state having slided here is described by
this expression essentially a linear
superposition over those product states
out of states of your system and your
environment and you might recall that
those were always normalized that see
the coefficients have norm one then if
you would in such a situation computer
density matrix you would find that the
elements look like this and if you look
how large are my elements you would see
it's bounded by this expression and this
expression if you take a closer look
looks very much like a scalar product
out of two vectors and each of these
vectors is not longer than one so you
have something like two unit vectors and
you try to compute the scalar product
between those two vectors now what you
find and this is just a basic geometric
fact that if you take two normalized
unit vectors but out of a big space and
the environment has let's say M degrees
of freedom so this is the size of your
space M can typically be very large so
in this situation if you try this out
with a little metla program you will
find a look into a mass book you will
find
that as M goes to infinity the
likelihood of your scalar product being
zero goes essentially to one so
basically simply by getting into contact
with macroscopic environment
superpositions disappear at least this
is sort of the traditional picture
however my mad Sangha then it's hopeless
this is just the very general top-level
view what you indeed have to do to
understand decoherence correctly you
have to look at the details of your
system how this works and then we find
that can be physical mechanisms that
protect coherence and this is actually
the trick you need to employ to build
actual quantum chips and here I would
like to hand it to Jodi and we have to
do a quick microphone transition where
two mics yeah excellent
okay so I was a theoretical physicist at
one point but I became what's more like
systems architect over time and what I'm
gonna do is go back in time back to when
da was first started and what I'd like
all of you to do is imagine that you're
gonna start up a quantum computer
company and what you're going to do is
there going to be something like the CTO
or the chief architect so in that role
there are two big decisions you need to
make and often when people talk about
quantum computing in the literature the
second one is what they talk about
primarily which is what are you going to
build the thing out of but there's one
thing that's more important than that
and that's what computational model you
will use so in conventional classical
computation there's this dominant model
that has evolved over time sort of like
the implementation we use of the Turing
model some universal model for
computation which is now embodied in the
way that we build processors in quantum
computing there are several equivalent
Universal models for computation and you
have to pick one of them so one of the
things I'm going to argue very strongly
here is that it really matters which one
you pick because they're not all the
same and some of them are inherently
better and by better I mean easier to
build so in this picture I've shown to a
two dimensional chart so in the columns
you have these computational models and
the one that is the dominant paradigm
that consumes almost all of the
literature is something called the gate
model which Hartmut touched on briefly
and the gate model is extremely well
characterized many people have worked on
it it's it's not exactly mature as a
field of study but it's certainly the
most mature field that quantum computing
has from the theoretical standpoint what
we do is not that what we do is
something called adiabatic quantum
computing and I'm going to describe in
some detail what that is the second axis
the second dimension is the actual
hardware you choose and what we work
with are materials called
superconductors
and I'm gonna briefly again describe why
that is but there are as many Hardware
ideas for building quantum computers as
there are physical systems that people
study in labs
so the idea fundamental driving idea
behind quantum computing is that you're
using physics somehow directly to drive
computation that's very basic and most
fundamental and as a consequence of that
really any physical system which obeys
the laws of physics if you can make it
behave quantum mechanically and you can
gain control over it you can potentially
use it as the basis for a quantum
computer adiabatic quantum computation
is actually a lot easier to understand
than the gate model it's it's in some
ways much more intuitive but it doesn't
appear that way when you're first
introduced to it so there's some novel
ideas in here that you kind of wrap your
head around first before the you know
the simplicity of this comes through so
the basic concept is that in in any
quantum mechanical system you can that
that's small enough in some way that you
can formalize you can write down an
operator which is called the Hamiltonian
and in the last lecture I understand
that Hartmut introduced this idea if
you're not comfortable with the idea
just think of it as a matrix so this H
is some matrix that has some properties
its hermitian it's self adjoint for one
when you diagonalize that matrix you get
a set of real numbers which are the
allowed energies of the system so in
some ways of thinking about it this
matrix is just some object that
describes what states and energies are
allowed in your system the energies are
the eigenvalues the states are the
eigenvectors that correspond to those so
if you want to find those all you have
to do is diagonalize this matrix so in
some ways of thinking of it quantum
mechanics is nothing but a huge exercise
in matrix diagonalization
the issue is the matrices are
exponentially large and so usually you
can't just brute force solve
Schrodinger's equation which is the
double-edged sword that you're trying to
use actually when you build a quantum
computer so the way that aqc works is
that you have a Hamiltonian
this matrix that has terms that have are
functions of time so you have a quantum
system and you somehow have knobs on
your box and you can turn these knobs
and actually change this matrix so think
of a matrix with time-dependent
coefficients as you change these knobs
the eigenvalues of the matrix change so
what I'm plotting here are the at lowest
say 12 eigenvalues of some big matrix
and those numbers those energies
correspond to the allowed energies of
the physical system as I change these
knobs in some particular way so the idea
behind a QC is that it's comes from
condensed matter physics originally is
that if you begin your evolution in the
ground state that is the lowest allowed
energy the system that you cannot do
better than energetically at the
beginning and then you turn these knobs
slowly enough what will happen is you
will always remain in that blue state
now what it means to be in that blue
state changes because you're actually
changing the physics of the system when
you're at the end of your computation
when you go in and you read the bits the
bits encode the answer to the problem
you're trying to solve so somehow you
have to make it so that the morphing of
this Hamiltonian encodes the the
algorithm and and the answer that you're
looking to solve now that process is not
simple but there are proofs that show
that this approach is universal which
means that any algorithm that you can
run on a quantum computer you can port
to this way of thinking about things now
one of one of the confusing aspects of
this even to experts is that a QC the
way it was originally defined as an
exact algorithm a QC fails if at any
time you leave this blue thing and
that's just a definition of what what it
means to be a QC there's a related model
which allows you to not get the exact
answer and still consider it a success
and this is called quantum annealing and
the idea here is you do everything
exactly the same you still turn your
knobs but you turn them real fast
and by turning them real fast you lose
the guarantee to always get the right
answer but you sometimes can get real
good answers anyway and one way to think
about this is as a heuristic so if
you're familiar with things like genetic
algorithms are simulated annealing or
taboo search or any of these things
there's no no optimality guarantees in
these approaches but sometimes they work
real well real fast and when you just
turn the knobs as fast as you can you'll
lose the optimality guarantee but
sometimes you get a real good answer
real fast and that's called quantum
annealing we have a little movie that we
generated to try to show you what
quantum annealing might look like in a
very low dimensional space so here's a
here's a little movie okay so imagine
this thing here is this potential energy
that your particle sitting in is it
imagine the ball is the state of your
computer so when you actually going to
start it over again if I can without
screwing things up here and watch watch
as I screw it up
okay you start in the global minimum so
that's the lowest energy state right
that's where we started and then you
more if you turn the knobs of your
Hamiltonian and the energy changes and
the ball here got trapped in the wrong
state so this is more like quantum
annealing than a q-see quantum mechanics
allows you methods to escape from these
minima that are not classical in that
case this was an illustration of
tunneling under a barrier so that if you
were doing this classically you'd get
stuck up in that ministy of a minimum
but this annealing process that it uses
these quantum mechanical effects gives
you pathways out of traps that you don't
have classically so this is a concept
you can have when you think about a
q-see or quantum annealing you have a
surface like a rubber sheet and how you
pull down the rubber sheet is your
program in time and where the ball ends
up is the answer and when you think
about this classically you've got this
ball rolling around on this rubber sheet
trying to find the lowest point and
classically it might get trapped but in
quantum mechanics there are pathways out
of traps that you don't have classically
you can you can tunnel do other variant
things that use quantum mechanical
effects
okay so that's a little introduction to
a QC the second thing I wanted to
briefly touch on is why we use
superconducting stuff over all of the
variety of other approaches to quantum
computing and there's one clear reason
it makes this approach by far and away
the best of all of them and it's real
simple pragmatic thing it's the only
approach that anybody has ever brought
forward that you can actually build
today using standard semiconductor
processes all of those other things that
I showed you require technologies or
fabrication concepts or infrastructure
that just doesn't exist so you have this
long ramp up process to bring up
infrastructure this stuff and I have
some chip at least one chip downstairs I
can show you and I have some pictures
that I'll show you later
you manufacture them in a standard
semiconductor fab using absolutely
common procedures there's really no
difference you can get hundreds of
millions of things on a chip without a
lot of effort just like you do in
semiconductor stuff so that's the main
reason kind of a corollary to that is
that these superconducting things allow
you to build large devices things that
you can actually see with the naked eye
that behave as if they were quantum
particles and that has to do with the
way that the physics of superconductors
work this is a well-known thing that's
it's being dot like it's well understood
in in the literature why this is so
there's other reason for doing this one
is that the historically people have
always thought about using
superconducting metals as the basis for
very very high performance processors
this traces back to the late 70s and an
IBM project the reason is that the
physics of the devices are operate over
a very short time scale so you can build
extremely fast digital logic there are
devices that have been built that have
operated at I think the speed record was
770 gigahertz so you can build
ultra-fast clocks you can build logic
that operate at those frequencies and
you can run these small-scale circuits
at speeds that you just can't get to
using CMOS because of a variety of
issues the other thing which is related
is that
superconductors don't generate heat when
you pass current through them and what
this means is that you can opt you can
build ultra-low power devices in in fact
you can do this in principle
thermodynamically reversibly which means
that without any generation of heat
whatsoever the of course you need to
dissipate heat when you erase bits
because of physics but as long as you're
not erasing bits you can actually build
in principle thermodynamically
reversible circuits and operate with
zero power dissipation okay so we chose
superconductors we chose a QC so now
what so you're the CTO and you've made
these two decisions the next thing you
have to decide is what what problem
you're going to try to solve so what
we've done is we focused on applying the
technology to the solution of this the
reason is that this was the easiest
thing to build there are all sorts of
other things you could do and we'll
probably do over time like factoring
like quantum simulation like some other
things this just happens to be the
lowest hanging fruit so what this is is
a canonical binary optimization problem
where you have a linear and a quadratic
term those X's are zeros and ones binary
variables and what how it works is that
a user will present a vector in a matrix
of real numbers so the person who's
writing the application somehow once the
solution to this provides us with an H
vector and a J matrix and what we are
attempting to return is the minimum e
that is a configuration of those binary
variables that minimizes e so if you
forget all about quantum mechanics and
about the details of what's going on and
you're just interested in the utility of
the thing we've built you can think of
it as a black box solver of that so if
you can pose your core hard problem in
this format you can send it to us via
this very easy-to-use web services type
API it gets sent to the processors it
gets crunched this configuration is
extracted from the hardware and sent
back to the user yep
to choose J or both J so the question
was whether the user gets to choose J or
or and H and the answer is yes so H and
J are both can be chosen not entirely
arbitrarily I'll show you that there are
some limits on this but in principle
arbitrarily and in practice to some
level of precision in in both you're
sort of wondering why it is that we
chose this problem the answer is kind of
neat the optimization problem I just
showed you is np-hard
so it's intractable there's no known
algorithm that will give you a global
optimality guarantee and in polynomial
resources so it's one of these things
that is mapable to a large number of
different problems
fundamentally hard for reasons that are
not easily solved or may never be solved
but it also is the formalism in which
physicists describe a huge variety of
physical systems so that's something
called the Ising model so if you have a
physics background and you take any stat
Mack the Ising model is one of these
things that you always learn about it's
kind of this pillar of the field where
pretty much any system you can think of
which has some local like objects you
know they can be anything from like
heart cells to birds to spins or
whatever where you have some local
property and the some pairwise term and
what you're trying to do is find the
behavior of that kind of a system that's
that's that can be modeled using this
model so you've got this really really
hard math problem and on the other hand
you have this fact that lots of physical
systems actually have the exact same
math so what we were trying to do is
build a programmable fabric so imagine
you have this grid of all of these
things and what we want to be able to do
is create an analogue version of
whatever binary optimization problem you
have by actually building the the
physical object that has the math that
is identical to the problem you're
trying to solve so we're trying to
physically
your math problem programmatically onto
a mesh of these devices to build as
programmable fabric to be able to solve
these types of problems so the the
fundamental object from which this
fabric is built is called a flux qubit
it's a very simple device there are
there's a loop of metal so this is the
outer big black loop here is what we
call the qubit loop this thing is
interrupted by a smaller loop which
itself is interrupted by two devices
called Josephson junctions which are
just weak links between metal leads
metal insulator metal they're very
simple the device has three terminals
two in one out the two input terminals
apply magnetic fields to the big loop
and the small loop respectively why an
inductive coupling so these blue lines
carry currents in proximity to the
device which generate magnetic fields
which couple into the device the signal
line out is a very sensitive
magnetometer it measures magnetic fields
which is fired it's activated when the
qubit is ready to be read out and what
that does is it senses the direction of
the magnetic flux through this loop this
device is constructed and operated so as
to only support two physical states in
the regime in which we operate it one of
them is current which flows
counterclockwise around this loop that's
digital zero and the other one is
current which flows this way around the
loop that's digital one nothing else is
allowed so those two currents have
magnitudes of about a micro amp give or
take and they are the two basis States
in which this the computation or the
measurement is carried out when you
measure out the qubit what you're doing
is measuring the direction of that that
current via its magnetic field that it
generates this is a picture of what the
physical qubit actually looks like and
you'll note that it doesn't look at all
like the diagram I just showed you the
important thing here which is another
very interesting thing about
superconducting qubits is that the
only thing that gives this thing its
cubed eNOS is the topology here the fact
that this is a closed loop you can
deform these wires as much as you want
under certain constraints and it still
is the same device so that allows you if
you're an analog engineer to lay out
this metal in a way that gets you what
you want in terms of the design of the
fabric so what we see here is I'll just
sort of trace around it so if you start
here and you go around this little
transformer and that one and that one
and back to here that's the big loop
this thing over here is the small loop
and the Josephson junctions are here so
this thing that this this line here
carries current in proximity to the big
loop and biases it so that's one handle
in this is the one that biases the this
the cjj loop that's another handle in
and this here is the measurement device
the the sensitive magnetometer that
reads out the state of the device when
you're ready the other device that's
very important is the thing that sits
between qubits so if all you had was a
bunch of qubits splashed out on the page
it wouldn't be very useful for a reason
that will become fairly clear it's
basically trying to solve the problem
with the HS and J's with no J's so if
all you had are H as it's it's stupid
it's very easy so you need to have
something that pairwise connects the
qubits so what this particular device
I'm showing is is one way to do that if
you have a qubit over here on one side
and another cubed over here and in the
middle there's this structure which
couples the magnetic flux from here and
this one and the trick is that by
applying certain biases to this device
you can create a situation where it's
energetically preferred for the two
qubits to be in the same state which is
called ferromagnetic coupling opposite
States which is anti ferromagnetic or
zero and you can tune that number which
is in effect the J in your Ising model
to a fairly high range so in these these
unit
say from - - - - with a flat part around
zero so if you want to actually set a J
between 2 and minus 2 in these units you
can just set it by varying your flux
bias the current on this line to
whatever number is appropriate and it
will set the coupling strength to be
whatever you desire it to be this is an
optical photograph of the 28 qubit chip
that we ran at the supercomputing demo a
couple weeks ago these little devices
here are the qubits they're the the H's
in your Ising model the linear parts of
the objective function these line II
things are actually small loops that you
can't make out at the scale which are
the couplers and those set the J's the
quadratic part in the Ising model the
way that you actually physically set
these are via setting DC currents on
these input lines so each of these input
lines sets an H or a J by setting a
particular value of current the way that
the processor is designed it's designed
to enable this idea of a QC or quantum
annealing
and physically the way that you do this
is by programming the h's and a
transverse term which is related to the
the flux you put through the sub loop
the little loop on each individual qubit
and the J's programmed on the the
couplers alright so what do you do with
this thing now to actually solve
problems so the way the way that we did
it in the supercomputing demo which is
just one way that you can use all of
these knobs so each of these
time-dependent functions is its own knob
so if you're going to program this thing
at the level of machine language the
handles that you've got are choosing
what all of these time-dependent
functions are so if you start at 0 and
you end at like 10 microseconds or
whatever you can go in and define these
wave functions though the forms of these
functions
it's just time-dependent functions there
is you know 28 of them 28 of them and
then a whole bunch that couple the
pairwise link devices you go and you set
all of those you allow the thing to
evolve and then you get some answer so
the the simplest way to use the system
to solve this problem
begin by turning off all of the HS and
J's you set them all to zero and you
ramp up the tunneling that at every
single qubit to the maximum that the
physical system allows so that's step
one the second thing and by the way I
have a whole bunch of slides that show
how this work physically and if there's
time and during the question period I
can go over it the second part is to
decrease that tunneling which you can
think of if you're used to thinking of
simulated annealing think of it as a as
a disorder so you quench disorder which
is sorry which is removing this term
slowly you ramp it down
that's quenching the disorder that comes
from this term simultaneously ramping
your H and your J's to their target
values which are given to you by the
user so those are the things that are
defined by the application that that
you're you're solving your problem the
third thing is you remove the disorder
altogether so basically you remove this
transverse term and what you're left
with is a form that has a linear part
and a quadratic part and these Sigma's
are operators whose eigenvalues are the
values of the qubit when you measure it
so they're plus 1 minus 1 they're binary
variables so when you read out the
qubits what you do is you select you
you're basically projecting these guys
out measuring a set of digital zeros and
ones in effect which become the answer
you submit back to the user so the
original binary optimization problem
that had this linear and quadratic part
that's exactly the Hamiltonian at the
end so the energy of the physical system
is in one-to-one correspondence with the
problem you're trying to solve and then
you report back the answer that you got
from the qubits and that gives you the
user his answer okay so the last thing
I'm going to describe is why we know
this thing is a quantum computer and the
way I'm gonna do that is by illustrating
some basic physical principle that you
can show at a single qubit which is kind
of the the the lead-in to the more
complex theory so I'm gonna I'm going to
talk about one qubit and then I'm gonna
I'm gonna make some statements about
what happens at large or qubits without
I explicitly showing them so the first
thing I'm going to do is imagine you
have a Hamiltonian which
can is not diagonal in the measurement
basis so what I mean by that is that in
this jargon you have a Sigma X poly
matrix which means that the off diagonal
elements of your Hamiltonian are not
zero they're in fact one this is very
easy to do with the type of qubits that
we've got and the all you have to do is
set the bias through the middle loop to
zero and then set the bias through the
small loop to a particular value and you
generate this Hamiltonian so this is
very easy to do when you do this what
you find is that the allowed energies of
the system are actually super positions
in the measurement basis so what that
means is that these circulating current
things which are the things I actually
measure if I go in and measure it are
not physically allowed by themselves so
if I if I they're not I gain States of
the system as a better way of saying it
so when when the system relaxes to its
equilibrium values the two equilibrium
values that it's allowed to have are
both superposition States so if I'm
doing a quantum mechanical experiment
one of the things that people love to do
is I'll take a qubit with a Hamiltonian
like this and put it in one of those two
states to begin with so here I'm saying
I'm going to initialize in one of the
persistent current States I'm going to
make it so that I know that it's
circulating like this even though my
hamiltonian says that's not an
eigenstate so what happens if you solve
the Schrodinger equation you find that
in the in a closed system where there's
no environment what happens is you start
to get oscillations of the probability
so if I time is equal to zero you have a
probability of being in the initial
state one because that's where you put
it and then as time goes on it goes to
zero and then back to one and then to
zero and then back to one so this is a
well-known thing
this type of vasila Tauri behavior now
if you add an environment you can't it's
easiest to think of things in terms of a
density matrix in the way that hartman
introduced so in a closed system like
the one I described it turns out that
this is the density matrix where the
diagonal elements are the represent what
the state of the top left and the bottom
right
are the ground state and first excited
state energy so you have a half chance
of being in both well if the
off-diagonal elements are zero when you
expose this density matrix to an
environment and you integrate over the
environment which is a procedure that is
generate something called a reduced
density matrix so if you if you don't
care about the state of the environment
you typically do this when you're doing
these calculations and what happens is
quite generically
you find that the density matrix becomes
time dependent and these time scales T 1
and T 2 are known as decoherence times
more specifically T 1 is a relaxation
thing where you go from say a higher
energy and you emit a particle and you
go to a lower energy whereas this one
here D phasing happens between energies
states that have energies that are
similar which kill super positions
between them so what happens very
quickly in this case is that the system
diagonalizes in the energy basis and
those are going to be very important
words where the number here and the
number here come from are set by
temperature so you can think of this as
a two-state system and at zero
temperature all of the stall the stuffs
in the bottom as you increase the
temperature into infinity they become
equally likely so I'm just going to skip
through here to this which is the
punchline so now imagine after this
decoherence process you've got this
diagonal thing and then you say well
didn't we just lose everything we wanted
because we've diagonalized this matrix
and like Hartman said isn't it the
superposition stuff that's the thing
that really saves us and don't we just
have a statistical mixture now in fact
this is exactly a statistical mixture
but the key question is of what so when
you look at the what this really means
the energy basis so the ground state and
the excited state are not the same as
the 0 1 States and if you think about
this geometrically they're kind of
rotated 90 degrees to each other
so when you take this matrix and you
rotate it in the way that's appropriate
so that now what's on the diagonal are
the zeros and ones whereas before they
were the pluses and minuses what you
find is that you get off diagonal terms
which don't go away there's their
equilibrium properties of the system and
so this is very important is that the
concept that most people have that if
you take something quantum and you
expose it to an environment noise heat
all that if you wait long enough it's
going to become classical that's wrong
that's only true sometimes and there are
examples where you can have where the
long time equilibrium limit of an open
quantum system contains superposition
and entanglement and all of the other
wonderful things that we need to do
quantum computation so in this
particular way to do things time is not
the enemy it turns out temperature is
the enemy so when you're thinking about
what you need to have in order to run
this type of quantum computer it's not
the same as in the gate model so in the
gate model there's a very short period
of time this decoherence time and during
that time the system will go classical
because of the way that you have to
manipulate these gates so the
requirement to be able to do all these
gates put some constraints on what
you're doing to the systems Hamiltonian
and when you do that it turns out that
as an immediate consequence after this
decoherence time passes the system is
classical so in order to fight that you
need to go in with control that operates
much much shorter time than the
decoherence time and fix errors actively
you have to go in and do things active
feedback a whole bunch of stuff and just
to give you a scale the t2 times in
these devices are at least microseconds
and in some qubits they're much much
shorter than that so the being able to
go in and control the devices on things
timescales are say a thousand times less
than a microsecond isn't physically
impossible but it's extremely hard to do
active feedback on superconducting
circuits for example with nanosecond
time scales it's just very difficult
but in a q-see you don't need to do that
because the things that you're using to
harness are these equilibrium properties
and what you have to fight is large
temperature so whenever anybody asks you
like what the decoherence time of the
cubit is you have to say well what model
are you working in because if it's a QC
it doesn't matter as long as it's short
it sits under some threshold so this is
something like a threshold theorem for
this this type of stuff so you might ask
okay well fine
are your cubits in that limit and they
are so we've developed a set of tests
that extract the the parameters that you
need to extract in order to show whether
or not you're actually in the
appropriate limit we've done that this
is all published stuff you can look at
Hartnett's literature stuff and you can
you can't a have a full introduction to
this but we have a way to extract these
things and show that we're in the right
appropriate limit so the last thing is
just there's another thing all it talked
about a single cubits now and there's an
obvious question as well as you increase
the number of qubits what happens to all
these arguments and it turns out that
the thing you need to do is keep the
number of energy levels that are within
the ground state of the system
polynomial in the size of the problem as
the problem scales so think of it this
way let's say you had a hundred energy
levels within temperature of your ground
state all of those will become thermal
so what that means is that you'll have
equal occupation of all of those 100
levels if you really need to have the
ground state nothing else you've roughly
got one in a hundred chance of getting
the right answer so as that amount of
encroachment on your safe zone increases
the chance of you getting the right
answer goes down and the question is how
do those ground
how do those levels pack as the problem
gets bigger so I don't know what the
answer is in general and no one does yet
but what I can say for certain is that
with the systems we've built we can
actually measure the appropriate ranges
and what we've shown is that in all
instances of the problems that we've run
up to 28 cubits the widening of these
energy levels doesn't kill the gap so
that means is that the there's always a
separation between the ground state and
the first excited state and what that
means is that any results you have here
is in fact equivalent to any results you
have here and you can operate these
things as if there was no noise and for
those of you are familiar with the gate
model this is a kind of passive quantum
error correction that you have a
threshold under which you behave as if
there was no noise so this is in effect
removing all of the errors from your
stuff shielding your the system that you
care about which is this ground state
from all of the errors by an energy gap
so in our case so far there are no
energy levels within T of the ground
state so far it now that's going to
change so this is my last slide can we
operate say a million qubits in this way
which is what we ultimately want to do
more than this if we can I think that
the answer is yes but I don't think it's
going to look exactly like what I've
showed you I think that there are
differences in the way that you encode
information into these devices will
probably be required the ways that you
quench this disorder are going to change
and the key issue for me is not whether
or not we can build this thing because I
think we can based on what I understand
of the state of the art now and fab the
issue is this physics / algorithms issue
which is how do you make it so that the
number of danger levels gets pushed away
from the ground state and the gap is
widened as much as it can be so that the
broadening doesn't hurt you as much
that's the key issue and that's a
difficult problem by the end of next
year we're going to have about a
thousand cubits so we'll be able to do a
lot of things experimentally and in
practice with these things that are
novel so we'll be pushing this thing
into a regime where no one has ever been
before even 28 qubits is not really all
that interesting from the point of view
of applications you can do some
proof-of-concept stuff but it's not
powerful in the way it needs to be to
solve big problems thousand qubits is
starting to get close so I think that
that'll be somewhere around the crawl
over where we start to get real utility
out of these things and people only care
about the black box are gonna be ok with
it and then try to push it to about a
million cubits within five years so I'm
going to now hand it over to Hartmut
who's going to finish by describing the
application we ran at supercomputing
2007 yes yeah the answers yeah the
answer is yes and in fact the one of the
objectives of the hardware program is to
increase the scale of the HS and J's so
actually increasing the strengths of
those numbers of course the HS and J's
have to be compatible so if the eight
the J's or the HS overpower one or the
other the problem is no longer hard the
hard instances are ones where they're
about the same order of magnitude give
or take but know that that hasn't been a
feature that we've observed when we do
the experiments on the small Mis
problems is that the gap remains
consistently large regardless of the
particular instance that we saw
the connectivity of the qubits is up to
the designer so in the particular one
that I showed each of the devices is
three connected but that is just a
feature of the particular architecture
of that chip the limitations to how
connected the qubits can be are
ultimately having to do with the fact
that the bigger you make the qubits the
more noise is introduced into the into
the qubits and every time you couple a
qubit to something else you need one of
those little Wiggly transformers which
increases the perimeter of the qubit so
the chip that we've got now that I
showed you is in the smallest coupling
connectivity limit it's still three
connected and non planar but it's
nowhere near as connected as the later
systems we hope will be it's in the
minimum connectivity limit but there's
no reason why you can't build
arbitrarily not physically connected but
logically connected systems this is a
sort of gets down into some issues about
how we actually embed problems into the
hardware maybe impossible on that to
later ok so I want to use the last
minutes to explain some applications and
as I had said in the introduction of the
course this is an unusual lecture series
in quantum computing in the sense that
we want to look at it strictly from the
perspective of synthetic intelligence so
what can we do in areas like pattern
recognition or on machine learning with
this device and here I'm showing us the
algorithms that we showed a couple of
weeks ago at the supercomputing show
which is a first example that gives you
a flavor essentially what we did was
it's image matching
just to remind you image matching is an
np-complete problem and image matching I
think this picture shows what it means
image matching means you have two images
and you would like to find
correspondences between image structures
that correspond to the same physical
structure in the outside world and in
order to do this you can set it up as an
energy minimization problem where you
basically try to solve your problem by
minimizing this term where you have one
term that it's responsible for making
sure that what you are matching indeed
has high local image similarity and the
second thing if these are really the
same object there should be some
geometric consistency should not be
completely wiggled up so the second term
here enforces geometric consistency and
now the name of the game is to map this
problem on to the quadratic programming
problems that Jodie has shown you which
the d-wave chip is designed to solve so
how do we do this given two images we
find interest points which you can
basically think of you look at the
autocorrelation function of the image
and we look at local Maxima of the
autocorrelation function essentially
what this means as we find blobs and
corners in the image and then each
interest point gets described by some
feature vector here you can plug in your
favorite type sift feature we like kabab
wavelets but pretty much any in
well-known process of describing locally
your interest point would do and then in
order to generate determine the energy
functions that rewards image similarity
we basically take a scalar product
between the feature vector extracted
here at interest point I in the first
image and the feature vector in at
interest point - in the second image now
to take care of the in geometry we look
at pairs of interest points and generate
a vector s that describes the relative
position and other geometric relations
between those two interest points and
then similar as before now we look the
question is is I and alpha does this fit
ok with a match I want to do between J
and betta and lose a sort of moves the
same way or not so much the same way so
there we create a term here which is not
as simple as a scalar product but it
essentially measures how compatible with
these two moves be and then we plug this
in to an energy functions that now has
exactly the shape we would like to have
it is a binary quadratic optimization
problem and then these terms here
generates H's please are essentially
responsible for local image similarity
these guys here will become the J's we
saw earlier which in foster metric
consistency and then we ship those over
to the d-wave chip we are some web
service
it runs the adiabatic annealing process
and then we get back some of the qubits
will be in state one and some will be in
state zero if I get measured the ones
that are in state one actually
corresponding to the best set of
corresponding points we can find and
just to be clear I started out saying
image matching is np-hard and last
lecture I told you quantum computers are
not known to solve NP hard problems so
what I'm not expecting here is in this
process to always find the best solution
but due to the effects tunneling other
elements we had I can reasonably expect
to get much better solutions than with
any classical machine and you know let's
quickly take another look at the same
problem which might illustrate what we
have done in a different way may be
easier to understand
essentially what we have been doing we
set up an adjacency graph each vertex in
the graph corresponds to match candidate
pair so structure I with structure alpha
in image one and two and I have caused a
lot of corresponding correspondence
candidates and essentially what I do I
bring only those candidates into the
graph if the basic image similarity is
better than a certain threshold and I
create an edge between two matched pairs
whenever the geometric consistency is
worse than a certain threshold
so essentially guys so have an edge
between them they kind of want to go in
two different directions they don't
correspond to a nice smooth match
between image one and two and then the
energy function is so before here in
this graph theoretical language what it
does all the guys the qubits that come
out as one essentially correspond to the
maximum independent set so this is the
largest set of matched candidate pairs
that play well with each other meaning
their geometric transformation there's
one smooth transformation creating those
correspondences
no no it's still the same we try to find
the ice with the alphas and the J's were
the betters but only those who will
allows that are geometrically consistent
basically if the chip would be large
enough if let's say I have that's 128
let's say here's is this equation I have
let's 128 candidate pairs that where's
the image similarity is larger than the
threshold if I were to have a chip that
has hundred twenty-eight qubits I could
in one sweep gets a solution right now
the killer chip has only 28 qubits so we
had to do a little bit of mechanics to
break it down into smaller subproblems
but this was only thought of that's easy
current level so if I would have taken
less match pairs I could have done it in
one speed or if you have n in your first
image and M in your second image
interest points then there are n times M
pairs exactly now this is quite a big
number and therefore you have these
simple constraints here throwing out
those that look right why is that bad
that you're not interested in to get it
down a little bit and that's also where
you can see why the problem is hard
because you have n to the power of M
mappings so this is a huge number of
mething x' and an image matching we can
never hope was our current mousetrap
meaning a server center to find the best
solution in all circumstances again with
this process we can hopefully get much
closer to the best solution okay so we
are pretty much out of time I will leave
the learning part for next time this is
the problem where I wanted to invite you
to actually to help me
map the training of the neural network
onto this or other binary classifiers on
to the d-wave architecture but since we
are out of time and leaves us for last
time for next time and just want to show
you kind of one feel good demo that
comes up PMI essentially feel good slow
ah we're not online so yeah given that
we are out of time I shouldn't mess with
it too much what I wanted to show you
and I is a program which essentially
dance is the image matching but it is in
a way that is very familiar to you we
have a little MATLAB program that
performs is imaging but using the actual
hardware so the MATLAB program has a
Java portion in it or cause Java
routines which talk to a web service
where again we discussed that what you
have to do is you have to take an H
vector and a J matrix this is basically
what I just explained you will be
reduced to then I have two ships this
age and the Jade to Vancouver to the
adiabatic quantum ship it runs I get
sort of the maximum independent set or
the best set of matching pairs back and
then I can plug it back into my
algorithm but it's basically a matlab
based platform that we have to do all
sorts of experiments and again image
matching is the first one and I think
that machine learning might provide
being as this might be probably yet more
interesting problems that one should try
to map onto the specific chip
the agent Jason Cote geometric
compatibility and the ages correspond to
image similarity so the H is literally
computed by a scalar product between the
feature vectors it's as simple as that
so maybe I'm a leaf should have made
sure that we are online because
obviously this program talks into a web
service and seems like yeah not
connected so yeah would terminate the
lecture at this point and maybe in the
beginning of the next lecture I will
show this often it's easy when you get
your answer back to verify it in some
fashion here to do this an automatic way
and the image matching case probably
wouldn't be easily possible so you can
actually computer say a lower bound or
an upper bound for EMIs could be and
then if you get a number that is that
bound is guaranteed to be default so so
it's not it's not as good as you'd like
but there are ways to bound so that if
you if you actually achieve about you
know that it's often but in general
if it's MPD that means it's polynomial
verify exactly but not all NPR problems
are in NP is the problem right so the
decision version is np-complete so if
you ask the question is there an M is
greater than five or something like that
then that is verifiable obviously
because you've just get it at my ass and
you count if you if you have an
organization the optimization version
where you're asking for what is the
biggest step my ass that's not as far as
I know let's put it like this I was
thinking it more from the practical
situation where you don't have ground
truth data if I would have ground truth
data for the right matches between those
two images then it would have cost me
very some easy for me to verify it was
the solution I got back it's correct or
not in that sense it's indeed verifiable
very cheaply I was when I said no it was
more like typically I don't have any
ground truth data for image pairs but
sort of if it's all complexity classes
the if you were to have ground truth
data then verifying it would be cheap
yeah probably I should just mention that
you you don't only have the option of
getting to the that at the level of
abstraction where you present the vector
in the matrix there's also an API for
going in and actually fiddling with the
machine language so if you wanted to
write your own algorithm which is you
know controlling these things over time
you can do that so really the the
hardware is extremely flexible it's much
more flexible than just the thing that I
showed you can do a lot of different
things with it now in terms of proving
optimality with algorithms that's
notoriously hard for adiabatic
algorithms there there there's a there's
a huge Greenfield of unknowns in terms
of the performative theoretical
performance of this type of approach in
the worst case
yeah I think so and the probably the way
you do it is look at the way that it's
done with simulated annealing where you
can get some some some balance now you
probably do the same thing here with a
different disorder the the transverse
field disorder instead of temperature
nope no I mean all of these tips are I
don't know exactly what you mean I mean
every every chip that like this is
nominally identical because it just
depends on your fab right as long as
your fab is good they're all the same if
you change the interconnect among the
qubits you probably will get a
qualitatively different behavior but
probably not quantitatively different
for here
yeah actually that's a very good
question and there actually is a way
that you can use feedback in these
systems but it's not obvious
so which what you can do it turns out
that with this type of a system let's
say you start the system in a particular
state so you choose a bunch of zeros and
ones and that's your original state if
you run it there's a there's a sense in
which you can never get a worse answer
no matter how fast you go through this
evolution so there there's a more or
less of a proof in the regime in which
we can move but no matter how fast you
go the answer can only get better and
you can think of it like this here's the
state that you started in and if you go
on the straight line all the way up
you're in the same state at the bottom
the only thing you can do is come down
off that state so any time you're
allowed to do a transition you get a
better answer so what you can do is pick
a random state run it as fast as you can
if the state at the end of it is
different than the one in the beginning
you have a proof that is better feed
that one back and repeat and this is
some kind of a probably stupid but you
know easy to analyze heuristic for
converging on some good local and every
time you shape you have a theoretical
guarantee that you're not gonna get
worse you can you can be the same so
when you actually do this you can find
it no matter what you do you're always
in the same state you started in which
makes it library so fast it doesn't kick
it up it doesn't ever kick it off the
trajectory that that is the state you
started it might just push it past -
yeah you can you can curve down into
these energy levels that are better
represent better solutions but you can
never cook up yeah
yeah we've done we've done something
like that on some of the smaller chips
we're experimenting with all sorts of
ways to operate these things so the the
annealing schedule I showed is this sort
of stupid easy thing to try when you
don't know any better because we have
all of this control over all of these
knobs we're experimenting with ways that
you can fiddle with the knobs either by
using feedback or by doing something
that has some kind of structured
randomness so some kind of Gaussian
flickering or something like that it's
very hard to analyze these things with
first principles theoretically so the
approach we've always taken is try lots
of experiments and see what works and
then drive in that direction this idea
of good I know anybody has these are
actually interest points exactly these
guys either didn't pass a special that
says this guy didn't find he and the
flower picture anything that was similar
enough local image wise or maybe it did
but then it didn't fit well with the
correspondences established by these
guys typically if you switch off the
geometric constants trains and only look
for image similarity you will find a
whole bunch so the geometric constraint
is important to weed it down and get
only a few correct ones
there's actually a whole science behind
it the various ways of doing it
typically you have what's called an
interest point operator and this checks
for as I say little blob structures and
the corner structures actually were sets
of interest put a paper uptick
it's like compares ten such approaches
the literature image scaling is actually
evolving in I lost over this portion
earlier but we you might recall we and
it sort of when we compare this here
their image scale rotation is sort of
taken out of the comparison so if two
pairs what sort of is the vectors
themselves that connect them might sort
of look rotated but this is taken out of
the actual comparison scaling same thing
something which is close the chips will
be very in fact they already are quite
similar in many ways to you know an AMD
or Pentium chip except they have very
low levels of integration but other than
that they're fairly simple the big
difference is if this all needs to be
put in a very shielded cold environment
and so when you the actual chip is the
size of you know there's thumbnail but
the actual machine is about ten by ten
by ten of refrigeration so the actual
system right now is fairly significant
terms of footprint
oh no you believe these things you have
you have to think of these things as
accelerators in their their special
purpose processors that do something
very well and so I think you know you
got to think about these things is just
this application-specific coprocessor
it's it's off in the corner somewhere
it's really good at certain things but
can't do anything else that's the way to
think about them at least in the short
term is that if you have some important
problem like a machine learning thing
and you offload it to this type of
processor it might be able to do
something transformative and awesome but
you could never do even simple other
things like one over R squared where you
know the neighboring magnetic fields
they're their potential to cause people
here insists you know proportional to
one over R squared their distance from
the one
being ruined or is it some kind of other
effect not simple it's more like a
complexity issue no it's it's it's more
like imagine you have two boats on the
surface of the lake and there's a wave
on the lake and the two boats are going
up and down at the same time okay so
think of that as like coherence
everybody's happy in the quantum world
now imagine you got like a beaver
slopping exhale somewhere and creating
ripples on the pond
so the those ripper ripples will start
to move the boats out of phase a little
bit now imagine the beaver is some
source of noise on your ship it's an
electron which is hopping back and forth
for some reason or like a little spin
magnetic spin is rotating or something
so you have this late and there's a
gazillion beavers all around the outside
slapping their tails at the same time
and the boats start to jitter out of
phase so the time it takes for the boats
to go out of phase is something like
this vehicle Aaron's time so the more
beavers you have in the harder they slap
the shorter time it takes for the thing
to go will completely lose coherence you
know it's every it's every physical
system that couples to the the quantum
system that you care about so in a
physical qubit which has this magnetic
nature
anything that's magnetic has this same
dipolar coupling it's 1 over R squared
to the the flux the threads the chip
it's actually not 1 over R squared if
you're too close to it but there's some
function of distance and then you have
capacitive coupling so there's
electrical coupling if you have an
electric thing that generates an
electric field there's a like there's an
electrical capacitive degree of freedom
of the qubit to so basically anything
that if you're going to write down the
physics of the system couples to the
thing that you care about is a potential
source of the
the way Glee and last one just making
the qubits larger does that diminish so
that it's very non-trivial question as
to how the cubed size effects the noise
there are some theoretical models where
when you increase the size of the qubit
the effect actually there goes down and
it has to do with a surface volume to
search volume to surface area argument
in practice though what we find is that
the noise scales linearly with the
perimeter of the qubit so the longer you
make the line the more of the noises and
what that's telling you is that there's
a source of noise that sits under the
wire or in close proximity to it so that
when you open that you increase the
perimeter the noise increases can common
with your increase and that we've seen
another laps around the world also so
there's something in the chip itself in
probably in the insulating layer that's
creating a noise source that is smallest
when you can make the qubit as short
yeah so the total time from when the the
problem is submitted to getting the
answer is about a millisecond with the
28 qubit chip of that time about a
microsecond is the actual annealing so
the this picture I showed we starting at
zero and going to one where all these
energy levels morph that takes about a
microsecond the other time is taken by
IO and a substantial portion is in chip
thermalization so it turns out that
after you measure the qubits you fire
these readouts heat is jumped onto the
chip and you have to wait for that heat
to go away before you can do it again
and so about half that time is just
taken up with waiting for the chip to
cool down before you run it again
it's all so that the the easy answer is
that anything you can map to a quadratic
binary optimization problem the answer
is yes the issue though is that the
mapping may cost you computationally so
let's say just you have some problem and
it takes it you you it's some kind of
quadratic cost by which I mean your
problem when you started it requires in
has n variables but it takes n cubed or
N squared qubits that's no good because
N squared or cubed when n is a thousand
blows the number of qubits up way too
much so the what we're looking for these
problems that are natively mapable into
quadratic optimization problems like
this without incurring some kind of
polynomial costs
okay so we caught it today</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>