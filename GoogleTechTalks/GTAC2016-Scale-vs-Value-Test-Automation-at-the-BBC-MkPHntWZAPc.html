<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC2016: Scale vs Value - Test Automation at the BBC | Coder Coacher - Coaching Coders</title><meta content="GTAC2016: Scale vs Value - Test Automation at the BBC - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC2016: Scale vs Value - Test Automation at the BBC</b></h2><h5 class="post__date">2016-12-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MkPHntWZAPc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright enough for me let's get into our
final block we have
jitesh and David from the BBC and I'll
hand it over to them hello everyone my
name is David brookhurst I'm an
engineering manager at the BBC and I
look after the teams that build our TV
applications console set-top box apps my
name is Joe Gotha I'm a Test match for
the Titan team which is the BBC test
tools and infrastructure team and we
bring software engineering test
development so we work for BBC design
and engineering which is the Technology
Group within the BBC so people think of
the BBC is a broadcaster or you know to
a news outlet but to me the BBC's a
great technology innovator and so 8090
years ago the BBC was there you know big
part of the development of TV and radio
broadcast technology we had our own
computer in the 80s you know robotics is
obviously a big part of what we do we
more recently I guess the Internet age
we were one of the very first companies
to have a video on-demand
application in iPlayer has been around
over 10 years now so I think the last
time I counted weird about 30 mobile
applications and four TV apps across a
whole range of devices which is which is
kind of what we're going to talk about
today in trying to scale our automated
tests to cover as many devices as
possible we started to learn the real
challenges and actually the value in
test automation and where that value was
so one of the things that makes the BBC
unique is is the way that it's funded so
we're publicly funded it's not a
government tax but but a license fee and
that gives us a very large remit that
that's established our mission in the
the Royal Charter was signed by the
Queen this is the one I keep on my desk
so but but basically our mission which
is inform educate entertain that there's
two very important parts which which is
very relevant to what we're talking
about today so so are kind of our focus
isn't on profit or growth but but in
value for money and reach so so value
has to be a big part of everything we do
and also reach so specifically something
we refer to as the universal obligation
so what that means to us in design and
engineering is that we have to ensure
that our services are available to the
as wide an audience in the UK as
possible and so if you think about smart
phones TVs and and we've had other
speakers the last two days talking about
device fragmentation you know we can't
actually just choose which devices we
support based on capability we have to
we have to get our apps to work on the
devices where the audience is so that
means it could be a $50 tablet or it
could be a $5,000 Smart TV and in fact
we started really to become overwhelmed
by the other challenges of manual
regression so to get a star we start
with behavior driven development or BDD
the first understand exactly what it is
that we're trying to build and what we
started to do was run three amigo
sessions so we got developers the
testers and Puritans
all come together and collaboratively
understand exactly what it is we'll try
to build with this would then produce a
feature file this will list the key
scenarios or acceptance criteria for
that feature and we use this as a
starting place for what we're going to
automate and we got pretty good at
automating this way and we got them
running on physical devices but the
problem was who still had these
massively long-running regression test
cycles so what we wanted to do was take
these automated tests and actually run
them as many different devices as
possible this way those tests passed
knowning hadn't certified the acceptance
criteria but now also had an executable
specification so we got pretty good at
doing this and so we put some quiet
energy solutions here one of our dev
teams actually grabbed a lot of phones
that they wanted grab a spare chair and
a laptop and actually stuck it to it and
we'll actually
move around a mobile mobile device
testing rig so you don't you put wheel
this around to wherever there were run
the test and actually deal with any
issues that came on prom was this wasn't
very scalable I mean we had twenty odd
development teams and building test work
like this for each one wasn't work and
also each team will been Hatcher you
have to reinvent the wheel every time
understanding had to get a test to run
on all the different devices deal with
the Wi-Fi issues the stub data issues
and generally just having the device in
the state that fund the automated tests
so what we did was we created the Titan
team which of his test tools
infrastructure team and either software
engineers embedded within the test
discipline to help build test tooling
and infrastructure for the test
department and one of the first things
we got these guys abilities rig which
they called the hive CI and they build
maintain and run this for the whole the
BBC and it gives us four main features
so yes so one of the big things that the
hive does for us is it just keeps
devices in a state that's that's ready
for testing so if anyone's done a lot of
kind of run automated tests or even just
do manual testing on device well the
first challenge is find the device it's
probably hidden in the drawer somewhere
once you've found it you have to then
charge it you have to get it on the
right Wi-Fi network you then have to
clear off the SD card that's been filled
up with screenshots and so so what we
really wanted was a system where we
could just we could just plug a device
in and it maybe be ready to go so the
picture here is actually for our TV
applications so what we do is we this is
essentially a holding app we can launch
a television into the holding app
through through basically broadcast
hooks and then and then it's ready and
waiting to run tests so the second thing
that the hive does for us is is
inventory management and monitoring so
you know good just having all these
devices there if we don't know what
state they're in and really we wanted a
totally zero effort approach to having
an inventory of devices so so we can
literally just plug a device in it
registers we know what the type of
device it is we know what state it's in
and we're ready to go
and also it's so you can see a number of
those devices of red they
you know whether the they've fallen off
the network or the Wi-Fi is gone flaky
or the SD cards filled up you know
someone can go and investigate that
device and they importantly they stopped
running tests but but also more recently
we've we've got a bit more advanced and
some of the devices you can actually
recover so they'll try and clear off the
SD card that try and fix the Wi-Fi
settings and I guess a real key thing
that the hive does is actually run those
tests so teams anywhere in the BBC can
write tests they can then choose which
tests they want to run on which devices
they want them to run on and then the
hive just manages that scheduling for
them so they don't have to kind of worry
about that there and it'll also do
things like retry so if this flaky tests
or anything like that it'll it'll retry
them I think we allow a single retry
that we for every test and importantly
it's because it's like every team across
the BBC all building apps they're all
using different frameworks so it really
has to support any any type of framework
that people be using to automate their
tests and then a final kind of the the
fourth big thing that the hive does for
us is is just result collation so if you
imagine for for a release of iPlayer or
video on-demand app we might have a
hundred to a thousand different test
runs we want to bring that together in a
view so you can just see at a glance of
there's no failures or or if there are
you can drill down and investigate them
and also it smooths out any flakiness
we've got so that the developers don't
have to worry about where our tests
fighting might not be so solid so this
is yeah this is an example of jits team
setting up one of our TV hives so we
found this corridor that didn't seem to
be in use it turned out it was a fire
escape but it's a absolute brilliant
place to build your testing setup so and
that's that's just running all our TVs
on the holding apps they're ready to run
tests in fact some of them are running
this is kind of more typically what a
what a mobile hive might look like so we
can kind of set these up anywhere just a
you know inexpensive PC hardware running
Linux and our and our hive daemons and
just plug in a device it registers it's
on the system it could run tests and
then for iOS will we do the same with
Mac minis so we started getting pretty
good at writing automated tests maybe a
little too good and trying to struggle
with exactly what was in these tests
with test Suites they had so many now
that they didn't understand what wasn't
wasn't tested anymore we also started to
drown and see a flaky read test whenever
anything went wrong
they wouldn't know whether it was a
natural defect an issue with the test
framework itself or actually
infrastructure issue so and the burden
on these teams just to try to maintain
these tests was getting bigger and
bigger and the value that they were
given were just getting less and less
and it was time to ignore them so we
looked into seeing what we could do next
to try and see you know how can we help
these teams deal with this test
automation issue that they're having so
we went back to the test pyramid we
looked at the automated tests that we
had an important look to see how many
then we could push down into the
integration in unit test levels where
they burn a lot faster a lot more
reliable a lot easier to write but we
did look out we didn't want some
automated UI tests when you try and
create the peak of the pyramid and we
started speaking to our dev teams and
they all mentioned that they were
running a small suite of tests all the
times kind of like a course we're test
or smoke tests the other slightly
different needs room and all has
slightly different tests and somewhat
bigger Suites and somewhat small Suites
so we looked into these principles that
these guys are running and create a new
term called human tests we one of the
first things those teams said is what
the head of human tests well it's
actually an acronym it's not for prudes
core functionality understood by old
mandatory and automated so we take the
first to prove core functionality was
the main thing your app should do
burro's it was our app was a video on
demand service so it's video playback it
doesn't really matter if you can add it
to favorites download a program if you
can't play that video back your end
users are just going to leave
so what the core thing your app should
do understood by all our dev teams
didn't understand what was and wasn't
being automated anymore so it's up to
those teams to come up with those tests
themselves but not only that everyone
within that team must be able to execute
those tests whether they're technical or
not and also the status of those tests
should be visible through the whole team
either by our dashboards or email
notifications mandatory it's got a
formal part your delivery pipeline when
other developers commit any piece of
code and an app's bill the Puma tests
are run if those tests Phil everything
stops dead you don't get no release and
nothing gets pushed alive if it's up to
the team to then debug those tests and
figure out exactly why it's gone wrong
and last for only it's automated it's
got to give you fast feedback it's gotta
run in the five minutes or less and it's
got to run a representative devices of
your end-users now one of the key
reasons of calling on pure test was it
gave the team a common language when
they said what are a few min testing you
exactly what they were but it also
stopped them from putting in tests that
weren't relates few more tests so any
other Suites that they want to create
they could create them but it keeps the
fewest Suites very small focus and fast
so if use were big success for us and
when we were looking at the kind of 200
old tests we'd have that were across the
whole UI and trying to boil that down
into sort of 10 really useful core tests
we started to look a bit more about what
we were actually testing and what was
that core focus of the application so so
think about a video-on-demand app video
playback is surely the most important
thing that we're actually validating and
what we realized is that although we had
tests that took every single journey to
the play button there was nothing to
actually press play and validate it that
that playback was happening so and part
of that was because you know some of the
automation frameworks we were using
didn't really support that kind of
journey you know that kind of testing so
but instead of spending all the effort
in building out all these applications
focusing that time instead on just
building some simple libraries that
would help us do things like confirm is
video actually playing back on this
device or or is audio coming out of the
headphone jack
so as well as kind of focusing our core
our core test to the stuff that was
really useful we also started to think a
bit more about the kind of tests that
are actually really useful to run on
devices so things like performance tests
where actually you do want to be able to
run those on devices and see what the
what the difference is so so for example
live video is very important to us so so
we were looking at what's that what are
the start times for live video playback
and that was just information we didn't
have before so when when the hive wasn't
running automated tests and the tests
weren't actually using the physical
devices they weren't actually do much
else so we thought well why not use the
devices to run tests every couple of
hours and give us a performance feedback
on what I was actually doing and so try
and help the team keep focused and have
a common language of what these types of
tests should be we came up with a new
acronym again for the cat theme called
lion or live insights and operational
notifications mean behind these tests is
to run every hour and these will give us
data on how long the app take to start
how long till video playback does video
playback how long about call journeys
taking and then putting canary tests
around these areas or threshold type
notifications so if it went but above or
below a certain mark it will reform team
show and let them know that there are
apps performing a different way this way
they didn't have to wait for the end
users to tell them but they actually had
these tests running constantly
throughout the day across a
representative devices as David
mentioned earlier our remit is to reach
a wider audience as possible and so we
need to make sure app is accessible as
possible as well as a picture of one of
our colleagues been muscled Rose who was
actually helped us build a lot of
accessibility testing and actually
automate some of the feedback loops
around here so what we do is we leverage
the high of touching screen readers s
devices on the hive and what we can do
is we can actually send our human test
to the hive enable the screen readers
and then capture the output and then
compare that against a known set of good
output and then report DIF back to our
dev teams now it's not necessarily
automating accessibility testing but
what it does do is gets our dev teams
thinking about accessibility a lot
earlier than they did before
whereas before that actually released
the app name wait for feedback from the
external users now they're thinking
about it every time they committing a
piece of code and proofing build and
thinking hold on how is it affecting
this court journey and then think about
accessibility setting testing a lot
further down the stream so to ettore
some of the sort of points what we said
so it's really it's all about value for
us so and the last kind of four or five
years have been this journey of
discovering that in trying to scale our
automated testing we were kind of
missing the point of the value of them
and and I think those learnings are
really kind of two things the one is to
focus
certainly your UI tests on on the actual
absolute core journeys and push
everything else down the pyramid and the
second one is to focus on the things
that are really valuable to run on
devices so performance tests compliance
tests and that's that's where you really
get the the return for the amount of
effort you have to put into this kind of
thing so if you want to find out more
about our approach or tools check out
our open source website so so everything
we've talked about today it's developed
in the open on our github
BBC org so you know if you want to set
up your own device cloud internally or
you just want to look at our humor
approach or try out some of our
libraries please give it a go and and
talk to us if you've got any questions
thank you thank you
great thank you both
let's go into questions I don't have any
woody comments at this point other than
to see I guess across the pond there's a
lot of the same same issues we run
astray what do you use to automate
actual video playback verification so I
mean what we tend to do is a very very
simple approach that just works across
as many devices as possible which you
know an ADB screen shot frequently to
literally just capture you know every
half a second images are you getting
enough of a diff that you can confirm
that the videos playing back and that's
actually been amazingly successful it
can spot when we've got buffering
problems it's been really good you
surprised that how the dips actually
have that unique fingerprint so we were
actually where'd identify when programs
actually starting and actually how one
programs different to the other so it's
been very simple but it's working well
yeah we've not found we needed anything
more complicated than that it's really
fulfilled everything we need it what are
you going to open-source it's all it's
all open source all open sourced oh yeah
I am paying attention at this point so
some images of your player show that it
supports cast how do you test cast
functionality that's the way most people
do by actually saying over casting
connecting to it
we haven't actually managed automate
this year without all sorts of issues
within our billing buildings with Wi-Fi
networks and casting two different casts
themselves so right now that's still
very much a manual process we've we have
some stubs within the app which will
mimic a cast being there and so we can
automate the functionality of it or does
it cuss but ensured its customer I'm not
sure do our hopes within the app which
kill cast on and off work but the actual
casting of it itself
it's still very much a manual effort
what we found is actually still need
human person sitting there telling us
actually not only does it cast but the
experience of casting is actually it's
good as we expect it to be so I mean we
can automate it that's fine we can know
it would
Cass but still don't tell us how good it
is and it's no good if it casts to a TV
and then we don't know if it's actually
playing anything or the audio is not
there or is stuttering or it's just a
really poor experience any questions in
the room head over that way but how
about 4k playback it's not good any 4k
playback yeah but hey guys so it's
really interesting
I'm from the app business server in
Kings Cross and we've literally got the
same problem and we are working on
something to do with testing is called
journeys and stuff and it seems a lot of
other people have hit the same snag why
do you think it's happened now that
we're starting to see this whole scale
versus value and they were having to
look at picking and choosing the right
tests to run yeah so what we found is
four years ago everyone said look the
test pyramid and it said push everything
down into the lower levels but before
five six years ago you want to automate
anything on me about you just couldn't
do it
then almost and all these tools are
showing on Afghan calabash and then our
natives evolved our express little
bottom it's good UI testing so everyone
just went pretty much mad and thought
right we can automate that hell out of
all this and do that we don't have to
get our developers to do this anymore we
you know outsource this to test
engineers a software engineer says do
verbs and tests and so what we all did
was built those automated UI tests who's
easy well it seemed easy and we can just
build more and more and more and then
you could very into pronto hold on how'd
you run this on so many different
devices how do we you know bring down
the test time so we can paralyze it as
much as you want you can stub out the
test data you do all this but actually
what we've realized you know this was
actually hoping this bugs are still
getting out there I mean the automated
tests only check a very thin narrow path
of your app and think above beyond that
it's not going to check and also what we
found was it wasn't telling us whether
or not the experience was any good for
end users whether the video playback was
of a good quality for us or audio
playback was of any good so what we
started to realize was a corfu core
tests were actually telling us the most
does the yeah stop can I get to the main
feature to the app kind of hit
play button and actually they were good
enough I'm pushing a lot of the other
testing lower down level and then
actually have our people test their
manual testers or beat up a test program
turn is exactly how it's actually
performing and do people enjoy using it
so I think that it's been a journey for
a lot of us even though we all knew we
should be pushing it down we were kind
of tried and garner Akeno this doesn't
work I have unfortunate to talk to like
a lot of other organizations and they've
all hit the same thing as someone you're
having like three main automated UI
tests I think that's enough for us so
that's my tip it's very good well thanks
to testing David thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>