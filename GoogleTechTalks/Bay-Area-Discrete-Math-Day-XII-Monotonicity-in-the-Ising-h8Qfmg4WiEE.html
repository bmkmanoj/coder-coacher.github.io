<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bay Area Discrete Math Day XII: Monotonicity in the Ising... | Coder Coacher - Coaching Coders</title><meta content="Bay Area Discrete Math Day XII: Monotonicity in the Ising... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bay Area Discrete Math Day XII: Monotonicity in the Ising...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/h8Qfmg4WiEE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">our first speaker of the afternoon is
Nick wine injure from google will be
talking to us about monotonicity in the
Ising model
see let's fit this to the screen so yes
so the using model rice I believe in the
morning mentioned it briefly is one of a
class of models who study in the
computer science and statistical
mechanics fields has become common and
unlike some of the other models she
mentioned um it's it's not at least
necessarily an inherently iterative
thing it doesn't have to evolve in time
in its simplest form it's just a
probability distribution on the set of
vertices a set of configurations or spin
configurations of vertices of a certain
graph and if you're a statistical
mechanics type person the way you deal
with it the way you prove the things
you're interested in about it is to look
at it on finite graphs and then try to
take some sort of thermodynamic limit as
the graphs you know for example lattices
on some number of dimensions gets larger
and larger but I like the using model
purely as a combinatorial object I think
it's worthy of study in its own right in
the sense that you can make some very
natural and simple conjectures about it
that when you look at them you say oh my
gosh that absolutely must be true and
then when you go to try and prove them
it turns out to be quite a challenge so
I'm going to talk about a couple of
those so the easing model as I say is is
it is a probability distribution on spin
configurations of graph and a spin
configuration is just an assignment of
plus and minus ones to the vertices if
you are a statistical mechanics person
you think of these these vertices as
particles of say atoms of some metal in
embedded in the lattice and their spins
as magnetic spins up and down and they
have magnetic interactions between them
and they may have some external magnetic
field and posed upon them and so the
probability distributions definition
encodes that you you specify it by
giving the weight of a typical
configuration Sigma in terms of these
pair interactions J sub XY and the
external fields H sub X if for example
for a given edge in the graph X comma Y
you have a positive value of J X Y that
means that whenever Sigma X and Sigma Y
the spins at X
why are the same so their product is one
you get a bonus you you the probability
distribution prefers such configurations
and likewise if you have a positive H
sub X that a given vertex X that's a
field that causes the probability
distribution to be weighted more heavily
toward configurations in which Sigma X
is 1 and if all these weights are
positive the configuration the model is
called ferromagnetic again you know from
the physics folks who say oh these are
the models you actually want to use to
study ferromagnetism this one other
little terminology thing that's
important the normalization constant
which is just the sum of all these
weights of course is called the
partition function of the ecig model and
its really if you want to do any serious
calculation computation about using
models you end up working with partition
functions so a very simple example here
let's see if I can next slide so just
just to give you the most trivial
possible example that's not completely
trivial you have a graph on just two
vertices with an edge between them and
these the three and the two are the
extra magnitudes of the external fields
of the two vertices the one is the pair
interaction between them and so you want
probability of this particular
configuration you say ok this is minus
one so it gets e to the minus 3 plus 2
minus 1 because they're different so
that's an overall weight of you to the
minus 2 and then you divide by the sum
of the weights for all of the different
possible configurations so ok why is
this interesting well for one thing you
might want to study some correlation
properties of it it's a well-known fact
that's of interest if your correlation
theorists that if you have ferromagnetic
interactions all positive external
fields all positive peer interactions
then the resultant measure has what's
called positive association which just
means that if you consider that space so
the probability space you're working on
is plus or minus 1 to the N you look at
that as a boolean lattice with sort of
the obvious partial order more
us ones is up and then any to increasing
events in that partial order are
actually positively correlated in the
probability distribution so all right so
in particular that means the following
if if you have a ferromagnetic easting
measure let's say it has zero external
field to simplify things and you take
two vertices V and W then the event that
Sigma V the spinet V is one and the
event that Sigma W spend it w is one
these are both increasing events and so
they are in fact positively correlated
the expected value of the product of the
spins is greater than or equal to the
product of the expected values so how
strong is this correlation well an
obvious measure of that strength would
be just the influence of v on w which is
ok you take the expected value of sigma
w given that Sigma V equals 1 and
subtract the expected value given that
Sigma V is minus 1 and so you get
intuitively what you get then is a
measure of how much flipping the spinet
V check is expected to change the spin
of W how strongly the spinet V can
influence the spin at w
and you might generalize this notion of
influence by saying okay if we fix the
spins of some of the other vertices if
we perform a conditioning on a certain
subset of the vertices a to say we're
going to fix some configuration of plus
and minus ones for just those vertices
and then reconsider the the influence
that V then has on w what should happen
well again intuitively the influence
should go down the more you fix
configurations of intermediate vertices
between V and W the less strongly they
should still be able to influence W
because again if we go back a few slides
the definition the easting models the
influence just works through the pair
interactions of adjacent vertices the
spinet V influences the spin at W
because the spinet V influences the spin
at its neighbors which in turn
influences the spin at their neighbors
and so on and if you fix some of those
neighbors or some of the more generally
the intermediate vertices spins then
whatever whatever influence you had
ought to be lessened and that is the
conjecture the conjecture is due to
Claire Canyon of anonymous Ellen you've
all Paris and will be referred to
hereafter as the KMP conjecture and this
is all it says that for any set of
intermediate vertices you take in any
underlying graph no matter how you set
the boundary condition for any vmw you
decrease the influence of v on w in the
ecig measure when you set that boundary
condition so a simple question how far
can we get on it
so kak M&amp;amp;P in the paper where they State
this conjecture proved it in the case
where the underlying graph is tree and
and this is a reductive but there's a
technique that relies pretty heavily on
the tree structure of the graph if if it
happens that you want to just restrict
the spin of a single intermediate vertex
then it's not so hard to prove but even
then it's not trivial it follows from a
fact about easing models called the gks
inequality and what I managed to do is
to prove it in the case where you have
an arbitrary underlying graph an
arbitrary number of vertices but where
you restrict the boundary condition to
be either all plus ones or all minus
ones and also in some very strange
fairly small cases where after you're
done restricting your intermediate
vertices in a you only have a few left
in your graph so ok
so the question okay then it's a good
question the question is why couldn't
you prove it simply by fixing one vertex
at a time and the answer is that a lot
of the ferromagnetism conditions that
you need to hold just don't any longer
hold after you've set a couple of
vertices I mean in any using model
you're right you can you can look at a
model with a couple of vertices fixed as
another kind of model with different
sorts of interactions and external
fields on the smaller graph the problem
is that once you try and fix a second
vertex especially if you fix the two
vertices in different ways then some of
your external fields end up being
positive and some of them end up being
negative and all sorts of basic
properties that you get to use when you
have all ferromagnetic interactions just
go out the window and are no longer true
so yes that's why it's not quite that
easy but in the case where you have all
plus ones are all minus ones what you
can do is do is use a very interesting
sort of method that seems to work
unreasonably well for these particular
sorts of models called the duplicate
variables method and so again you have
these partition functions and it turns
out that when working with e sink models
you can encode you can rewrite just
about any problem any inequality you
want to try and prove as an inequality
that involves partition functions alone
and by a series of reductions to doing
that you get that okay the inequality
you want to prove now becomes an
inequality with a product of for
partition functions on each side so this
looks like a really messy thing to deal
with but we're going to make it a
slightly less messy thing by going as
follows so we're going to group the four
partition functions on each side into
two groups of two and we're going to
think of each product and this is where
I should
so partition a single partition function
remember is just a sum of weights over
possible spin configurations of graph so
product of two partition functions in
general you write as says you want Z to
if you want to write it as a single some
you write it as a sum over pairs of spin
configurations and you have a paired you
know some weight function weight of
sigma x a weight of tau and then the
thing you do is you just regroup this
summation as a summation over a sum as a
summation just over subsets of your
original vertex set and say ok of s and
the way you do this is you simply
collect into one some and although some
ends over here where the two spent
underlying spin configurations agree
precisely on s and disagree precisely on
s complement and so the reason why this
is at all useful the reason why you
would ever want to do this is just that
again we're dealing with pair
interactions in external fields only and
if let's say you have a configuration
Sigma which is plus here and minus here
and minus here and a configuration tau
which is let's say plus here and minus
here and plus here so this is your s and
this is your ass bar these weights Sigma
and tow are determined in the zero
external field case only by the pair
interactions and if you have the pair
interaction between a vertex here where
they agree and a vertex here where they
disagree then no matter what you do no
matter how you set the agreeing spins of
these vertices or the disagreeing spins
of these
cease you'll get opposite sides for the
product of these two and the product of
these two and so the weights the
contributions the weights there will
cancel out and so you get a partition
function what this sum n turns out to be
is a partition function for an icing
model that is now restricted only to s
times the partition function for another
easing model that is restricted only to
s Bar and the usefulness of that is that
if you now have these nice a nice pair
of sums on each side where their sums
over subsets of your original vertex set
then you might hope to conclude that
your inequality is true using this
wonderful tool called the lv today can
for functions theorem which says that if
you want a thing like this to be true it
is sufficient to show that these cements
satisfy a certain local sub modularity
condition and indeed that's how you go
about doing this KMP inequality in the
all pluses case is you just reduce their
using this process and you show that the
local sub modularity condition is in
fact satisfied but the duplicate
variables thing again is sort of a a key
and cool method so let's see all right
now the second of these conjectures how
am i doing for time ok pretty well the
second of these conjectures involves
what you get when you do try to do to
convert the ceasing measure into an
iterative model into something that
evolves over time so we want to make out
of this static probability measure a
Markov chain that converges to it that
has the static easing measures its
stationary distribution and a very
simple way to do it is by defining the
steps that might call heat heat bath
steps where at a given time step in our
iterative process what we're going to do
is pick a vertex in our
so we've started we started at time zero
with some fixed spin configuration we
want to evolve toward the easting model
is the stationary distribution so we
pick some vertex in the graph we fix the
spins of all the other vertices and we
just pick a new spin at X according to
the appropriate conditional easing
measure so essentially we erase the
spinet x + reset it in a random way
again according to these and measured
want to converge to and iterating this
process gives you what is called the
glabra dynamics and the glabra dynamics
just says well do this at each time step
and you get a markov chain if you if you
do it and you can do it with random
choices of vertices or you get a prop
you or you can even fix the sequence of
vertices you choose deterministically
and again okay we get a natural
monotonicity property that we would like
to be true out of this if we have a
ferromagnetic easing measure then in
that easing measure the more we
strengthen the external fields and the
more we strengthen the pair interactions
the higher the expected spin in that
easing measure of any given vertex so if
it's true at the stationary distribution
and we have this Markov chain that
converges the stationary distribution we
might naturally ask okay is it true at
every step of the way well it's clearly
not true if if you don't start with the
highest possible configuration as the
starting point of your Markov chain
that's fairly easy to see if you if you
start with an all ones configuration for
it sorry if you start with an all- ones
configuration for example and iterate
this process then the stronger the pair
interactions are the more likely it is
that you will remain in all minus ones
and so the lower the expected spin of
your initial vertex but conversely if
you start at all plus ones you should
have more likelihood of staying at plus
one in any given vertex the stronger you
make the pair interactions the stronger
you make these preferences for adjacent
vertices to have the same spin and so
again that is the conjecture the
conjecture is due to Paris and it turns
out to be quite difficult to prove even
what you would think are fairly trivial
cases so here is what you might think as
a trivial case and so for example we can
think of just doing a two step process
that in some sense approximates it we
can specify a given vertex X and fix and
rather than fixing the configuration of
all other vertices we can choose a
random configuration of all other
vertices according to our stationary
measure now conditioned only on the
fixed spin and X we can do that in step
one and then in step to choose a random
spin for X
and so in some sense what we've done is
run the glabra dynamics to infinity on
all of these vertices fired all of these
vertices again and again and then at
time infinity plus one here we are
picking the spin at X and we want to
know that after that one firing of x we
get monotonic behavior of the
probability that the spin is one so I
managed to prove this and the proof
again is a duplicate variable sort of
method you want to show that the ratio
of probability of one to probability of
minus one is increasing in a given
parameter you can rewrite this as this
product to two probabilities should be
greater than that product of two
probabilities you rewrite that again
using sort of standard leasing model
reduction techniques as part as a
product of these two partition function
should be greater than or equal to
product with those two partition
functions and then you do a duplicate
variables rewriting which again makes
each of those products into a single
summation over subsets of the vertex set
and it turns out that if you do that
then you can show that this particular
that every some and here is greater than
its corresponding some out there but
it's only the particular method of
putting things into bins that duplicate
variables give you that seems to allow
you to do that and again this is only a
very simple case so this is an
unreasonably effective method it
probably points to some much deeper
property of the easting model I've only
really been able to scratch the surface
of it but i hope this demonstrates that
at least there some interesting problems
that arise from the purely combinatorial
consideration of the model and I think
I'll stop there
questions first finger yeah you can say
slightly more about why you can't just
keep moving
collaborating for
okay so so the question is to the
elaboration on why you can't simply do
it by essentially by an induction right
where you you do you restrict one vertex
at a time so again so I'm going to go
back to a slide that I skipped over so
at the end of all of your at the end of
all of your reductions to partition
functions you basically get that if you
want to show if you want to show
anything at least by this general method
of let's see if we can prove things by
partition functions and duplicate
variables then you have to prove this
lemma about a sub modularity property of
pairs of pairs of partition functions of
easing models where you have different
external fields but the same pair
interactions and in order for that to go
through you need to do you need to make
the assumption that of these two
external fields one is not only positive
but bigger and absolute value than the
other and if you try to do an inductive
proof where you're not restricting the
setting of the boundary condition where
you're not saying I'm just going to set
them all to +1 at once then that
assumption when you when you get down to
the point where you need to prove
something like that this assumption
about your external fields that you've
reduced to is no longer true and in the
absence of that assumption that lemma
just doesn't go through I mean there's
easy small counter examples
ok the next stop will be at three-thirty
let's take the speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>