<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Max-margin training and inference on structured models for information extrac... | Coder Coacher - Coaching Coders</title><meta content="Max-margin training and inference on structured models for information extrac... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Max-margin training and inference on structured models for information extrac...</b></h2><h5 class="post__date">2008-06-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dW9aIyaQNqA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">maybe pressure can be easily done
somebody they don't have long
before that I remember the bathroom shoe
syllabus percent and all shapes more
permissive dirty person
and she's original
so standard machine learning tasks on
which there is more than 50 years of
research is to predict one of a fixed
set of flash labels given some input X
and typically the excitation to be a
fixed dimension but of course when you
apply machine learning on complex
environments such as this the single
prediction model is awfully inadequate
so there is a lot of interest in current
much amongst current machine learning
researchers to predict output which is
not just one of a fixed set of class
labels but rather a collection of if you
will class labels so you can think of
your output as a vector of predictions
and the vector length is not fixed it
depends on your input X and the input X
is also not a record like what we had in
the old days and in fact the output may
not just be thought of as a vector it
can be any arbitrary structure for
example you could be predicting an
alignment of two schemas or of two
sentences in two different languages
which tells you the correspondence in
the both in the words between the two
languages or you could be given as input
a sentence and you could be predicting a
parse tree of that sentence so
essentially you know this framework
looks very powerful and also somewhat
intimidating right in the sense that the
space could in fact be infinite infinite
so how do you learn right in the old
days you knew how to classify things but
as here you have a possibly infinite
space so how do you get your mind around
it so the first abstraction which
enables you to tackle the problem is to
define what is called a feature vector
so which feature vector is defined
jointly on an input X output Y pair and
the set of possible features for any
given problem instance is fixed so you
have some capital K features and then
the model is defined just in terms of
the K weights you know these are real
values which are attached with each
feature and then you can define for each
possible output why offer over an input
X a scoring function so we are going to
use s X Y to denote a score of
predicting output Y for any given input
X and then the prediction problem that
the classification problem becomes that
of finding the highest-scoring y so your
prediction problem is to find this y
star for which the score is maximum now
this could be challenging because
typically for this structured problems
the space of possible Y's is very large
so that is the next abstraction which
helps make this feasible and practical
and that is the decomposability of the
feature vectors even though we started
off defining the feature vector jointly
over any input X and output Y pair in
practice the feature vector would
decompose over smaller parts of Y so
although you would have to maybe predict
say a set of 1 million class labels in
practice the feature vector would
decompose over much smaller subsets of
this 1 million labels perhaps two or
three of them and corresponding to
different ways of defining the feature
vector and different methods of
decomposing that feature vector you have
to design different inference algorithms
these are also popularly known as the
map inference algorithms for solving the
prediction problem and this is one
challenge this is one problem which
occupies machine learning researcher
serveth how to design the inference
problem the second is of course the
chaining problem during training the
assumed that for any input X we have one
correct way and you are given n of this
correct input/output pairs and you have
to come up with the best W so in this
talk we will see examples of both an
interesting prediction problem and
issues that arise when you try to drain
the WS you know in terms of efficiency
and generalizability for structured
learning tasks and my application and
also for many of the machine learning
researchers one big motivating
application one information extraction
and we will see how different scenarios
of information extraction give rise to
different
kinds of structures that you have to
predict and for each of them either we
can fall back on existing inference
algorithms or in one case we have to
actually think about new inference
algorithms so this field this is what we
will be doing in the rest of the talk so
we'll be seeing structured models
applied in from an information
extraction and we will give an example
of an inference algorithm and address
some issues during training structure
structure models so actually this is
like part of really ongoing active
research in machine learning on
structured learning so in information
extraction the most classical setting of
statistical approach of information
extraction you are given an input
sentence and in the the most common
approach you actually think of this
input sentence as being consisting of
the set of tokens which comprise that
sentence so in this input sentence you
are given nine votes and the extraction
task is that of finding from this input
sentence your structured fields so that
is the top level tasks and in machine
learning when you apply the structured
approach you break it up into the task
of predicting for each word a label and
the label would be one of the structured
pipes that you're interested in so
suppose if you are interested in
extracting from sentences which appear
in blogs whether there are book titles
and people names mentioned in blocks
then you would be interested in maybe
predicting whether a word is part of a
book title or not or whether it's part
of an author or not or if it's none of
the above so now actually if you were to
reduce it to standard multi-class
prediction problem you could want to
predict for each of these ayat word one
of these three possible class labels but
it turns out that you gain much more by
recognizing that it truly these nine
predictions are not independent
predictions you do much better by
capturing the correlation in the
prediction of adjacent words but also
other thing people have found out over
the years is that it's not necessary to
capture the correlation of arbitrary
other words it's just enough to capture
the dependency of adjacent words or may
be of two adjacent words but let's just
consider one adjacency
for the moment and that directly gives
us a way of decomposing this otherwise
daunting feature vector which you would
define over your input X and your vector
of nine outputs instead of that now we
can do compose it into a feature vector
which just involves pairs of adjacent
Y's the ihy that you have to predict and
the I minus one it by Y and then any
other property that you want to in that
you want to capture of the word around
the eyath word in your input sentence X
so with this form of the composite EECOM
position of the feature vector and for
this output structure which is a chain
of prediction you can actually solve the
inference problem very easily using this
well-known which will be algorithm this
is what was used for example for in hmm
inference in CRF inference and it's very
well-known and you can you have a linear
trivial linear time algorithm for
solving this problem now in information
extraction this is some work that I did
in 2004 where we claimed that actually
this classical method of word labeling
for information extraction is inadequate
when you want to exploit more powerful
features and in fact you might be better
off thinking of extraction as a
segmentation problem so you take the
same input X of 9 words and instead of
your output being just a label
assignment to each of the words you
think of your output as a segmentation
of the sentence where each segment is
the entire entity that you are often so
instead of trying to label each word as
whether it's a title word or not you
find the entire segment which comprises
a book title similarly over there you
try to find this segment which comprises
the also names and because you don't
know the length of entities in at once
this is actually now a much larger
output space you have to simultaneously
take up your sentences into segments
which it's very each segment comprises
an entity that you are after and then of
course you have to assign a label to the
entity so now the output Y is not a
vector of labels but rather a
segmentation and in this case also it is
adequate to define your future
vector over you know it's also suppose
if your output in your output you
predict P segments and note that even
for a fixed input end the number of
segments is variable but for any given
set of P segments you can say that the
feature vector decomposes over the
segment the jh segment that you are
predicting any properties around the gh
segment and just the label of the
previous center segment so that's why
it's actually also called a semi markov
model so you know normally you know with
Markov models I what are commonly used
but semi Markov model turns out to be
quite useful not just in information
extraction even for many of the speech
recognition tasks anymore and in this
case you know the previous Viterbi
algorithm can be extended through as
kind of two nested loops and you can
find the optimal segmentation the map
output in quadratic Texas so here also
we have a we can fall back on an
existing inference algorithm now I will
present a third instance of applying
structure learning to information
extraction where we had to come up with
more interesting inference algorithm so
this is when you want to do information
extraction not for each sentence in
isolation as I presented in the first
scenario but rather over a collection of
sentences so here is a example of set
three sentences and say I am i I am
interested in identifying from the
collection of sentences that I am shown
say in a document all occurrences of
company names and people names and
organization names this is actually a
classical NLP extraction task which
comes up in mock and a set of it now
very often you know when when people are
shown examples of information extraction
the moment you see the proper nouns
which you relate to then you think of
extraction for a human being as a very
trivial task so just to obfuscate that I
am replacing you know words with some
placeholder so why is very likely when
you think when you see a sentence like
this and you are asked to find out for
each of these
words whether it is part of a location
name a person name or a organization
name then in a sentence like this why
you don't know whether it refers to a
person name or an organization but on
the other hand if you see another
sentence which says that mr. X lives in
Y then it becomes very clear and
particularly both of the sentences are
one after the other in the same document
that Y perhaps is a location name but it
is you know when you do statistical
machine learning you are not going to
assert on anything as a hard-and-fast
faster because a third occurrence of Y
actually may not be using Y in the same
context as the first two occurrences so
here is a third sentence which says that
X by Y times daily now in this case you
would want to label Y x in this third
occurrence of Y as part of an
organization so basically for each
prediction task you have to take into
account not only the context of that
prediction in its linear occurrence in
the sentence but also across sentences
you have to capture correlation so now
this is again a task of predicting a
vector of labels but the dependency
amongst the predictions is bit more
complicated so in the old case you would
just have dependency amongst adjacent
labels that you could use which IV to
find the optimal labeling but now when
you have links amongst words which are
the same and you want to kind of add a
bias which says that all words which are
linked through these crosses didn't to
get the same label then your task your
inference task actually becomes harder
so for this example I have this one lik
one to the click come corresponding to
word X and this second click
corresponding toward Y now the feature
vector in this case decomposes in terms
of two kinds of features the first kind
of features they capture the dependency
within a single chain of adjacent
predictions this is what you see over
here and the second kind of features
they capture the cross cross shape
relations so now I will over the next
two five
minutes give you the inference algorithm
that we had to come up with to solve
this task this actually turns out to be
quite interesting particularly when you
look at the fact that you know this this
is the collective labeling problem and
many researchers in the past have tried
to come up with information extraction
scenarios using collective labeling and
they had to fall back on algorithms
which basically thought of this
prediction problem in the context of a
Markov model and you know when you think
of it as a Markov model it's a graphical
model and can you just use existing
graphical model inference algorithm like
belief propagation so for example in
2004 when you scan Munem and actually on
McCallum and his graduate student
they used message-passing on this graph
you know just normal belief propagation
you cannot solve this using exact
inference because the graph gets very
large so you have to use approximate
inference and loopy belief propagation
was one method which has been tried
other researchers have used sampling or
greedy local search but you know none of
them have guarantees and what we did
didn't this my symbol 2007 paper was
sure that actually this graph has
interesting to interesting aspects which
if exploited separately could give rise
to much better algorithms so the first
observation is that if you for example
could afford to ignore all the
dependency of adjacent predictions and
if you just looked at the edges which
are across the different documents then
these edges have this property that they
are associative in the sense that they
prefer adjacent the in every edge the as
this bias that the nodes that it
connects should get the same label this
arises in many other scenarios like for
example in social networks we want to
claim that friends of smokers are
smokers
similarly neighbors of locations are
locations so when you have
a special kind of graph normally when
your graph for example is a complete
graph and if your edges can take
arbitrary kind of this scoring matrix
then the problem soon becomes
np-complete you cannot solve this in any
tractable way but for the special case
of associative edges people have shown
that many existing algorithms can give
you exact solution when the number of
possible labels that each node can take
is two so in the vision community for
example Wyckoff in 1999 showed that a
min cut based formulation you can take
this graph and in the redraw it a bit
with the source and sink node and use
these associative edges to come up with
edge weights and then if you find min
cut in this graph then the two ends will
give you the set of nodes which should
be labeled zero and one and that is
optimal then there was this a nice
support in 2005 by karma Grove and
Wainwright where they showed that even
the vanilla belief propagation algorithm
which normally gives you no guarantees
of optimality will give you the exact
solution provided you do message passing
in a particular stylized way like tree
based message pass passing and you will
get the optimal solution then you have
to predict to labels so we start from
here we know that if we had if we did
not have this associative edges we had a
collection of chains we can solve the
problem exactly and if we similarly if
we ignore the adjacent edges and if we
just had this associative edges we can
solve the problem exactly but not with
the two together so what we said is
actually you should be treating this as
a cluster propagation algorithm so we
put it in the framework of belief
propagation but belief propagation where
the scheduling happens at a meta level
at a higher level so you create this
cluster graph where your clusters are
kind of super nodes which correspond to
chains where you know how to do exact
inference
or two clicks so these are clicks which
contain only associative edges and even
here you know how to do exact inference
at least for the case where the number
of labels is two but for other cases
also we can design efficient algorithms
with approximation guarantees as I will
be coming to shortly now once you create
this kind of a meta graph then you could
just put it in the framework of belief
propagation where for each node you send
a message to its neighboring node on the
the the best possible labelings score
you would get for each possible label of
the neighbored neighboring vertices okay
so this is actually you know I guess you
know for this part if you know what
belief propagation you know the main
steps of belief propagation and in a
plus still graph then then you will
realize that the two inference problems
that you have to be able to solve this
this that for each chain you want to
find the best possible labeling of the
nodes in the chain for each possible
label of the shared vertex when you have
to send a message from a chain to a
clique similarly from a click when you
have to send a message to a chain you
have to find the best possible labeling
of all the vertices in the chain for all
fixed labels of the neighboring shared
context now for chains this is an
efficient Infernus algorithm which we
have already gone over but when you have
arbitrary clicks this becomes difficult
and we will see that surely one can
design good community real algorithms
which was not possible earlier when if
you just restrict it to standard belief
propagation kind of frameworks so now
the problem that you have to solve for
within each click let's just abstract it
as false what you have when you look at
each click with say n nodes and where
each of these n nodes can be assigned
one of em possible labels is a set of
two criterias you have for each node its
own preference for the label that
so each node has a particular score
attached to whether it should be labeled
as location or person name organization
name or none of the above
similarly though for the entire clique
you have a score which if you were to
handle in the normal framework totally
puts you in the np-complete space but
the clique scores have a particular
property that they are associative and
also the fact that the click scores are
kind of symmetric in the sense that you
know the the the scores are such that
you know if I have a set of 100 nodes
the scores just care about what fraction
of them are labeled location and person
name and organization they didn't and
they don't care about which of the
hundred nodes are labeled location or
personal organization that's the
property of a symmetric function the
order of the arguments doesn't matter
for this kind of graphs and because of
this you can actually design algorithms
which do not even depend on the number
of edges in the graph so there is an
algorithm that we designed which runs in
order n M log n time where n is the
number of nodes in the clique and M is
the number of labels that you can assign
to the clique so this is can be actually
in practice much much better given when
you are scanning long documents you
could often find words which appear say
hundred times and then this pays off a
lot so I just wanted to give you a
flavor of the kind of inference
algorithms that you might have to design
when you would try to apply structure
models for information extraction and
for more details you know this work
Pearson ICML 2007 conference can look up
the algorithm and next I wanted to go
into training but if you have any
questions I can take that
so even played this too big documents
and roughly how long does it take so
actually this one is so suppose if we
apply the sequential model if it takes
said some time X with this one it takes
three X whereas if we were to apply
belief propagation it could take 10 X 50
X of depends on the size of weeks
so this training part it's a kind of a
total gear switch so here we are in this
situation where we are given in correct
input-output pairs so you might be given
an examples of sentences and they're
correct vector of predictions or in
examples of sentences sentence pairs and
the correct alignment you know the way
of course that be anything and because
we are in this structured world we don't
assume that for all possible wise you
have the same error there are some wise
which are more correct than otherwise so
in general the user in structured
learning typically wants to specify his
or her own error function so the error
function for any input I will take as
argument the prediction that you are
making why and output a real value so a
very typical error function say for the
case where your output Y is a vector of
prediction is a hamming error so the
hamming error tells you the number of
positions in which the correct output Y
I for the eye at example differs from
what you are trying to predict Y so
let's assume that you are given this
kind of an error function and you are
given this couplings of correct
input/output pairs and in the training
case you want to find the W for which
the error is minimized on the training
data but of course you are only
interested in those w's which will
generalize to unseen instances and for
structure learning we want to do this
efficiently while being able to handle
the fact that for any given X there can
be exponentially many possible Y's so
that is the main challenge and for
standard single predictions for example
support vector machines because they
give you a margin between the correct
class and the incorrect class have been
shown to generalize very well so in the
last five years people have come up with
extensions of the support for
the Machine kind of paradigm to
structured models these are called max
margin trainers and the basic philosophy
of max margin trainers is as follows
your goal is to come up with a W which
will make sure that for each eye the
correct why I the correct outputs score
which is the dot product of the weight
vector in the feature vector is greater
than equal to the the feature vector
times the weight vector score of any
other incorrect Y but you want to ensure
a marching it's not just enough to be
greater you want to make sure that it is
greater than equal to a margin which is
proportional to the error of that output
so things which are which are which have
large error should be proportionately
that far away from the correct output
this is what you would like to do for
structure prediction but this is the
hard margin case like for the in the
case of SVM you want to soften it and
the way to soft is is software it is by
adding a slack variable so you aren't
the slack variable which for each
example I you add this I I slack
variable this I guys have to be greater
than zero and you want to minimize the
slack you want over all examples the sum
of the slacks to be as small as possible
but of course you don't want to over fit
on the examples so you also add this
regular I Center which is the norm of
norm of theta so this has been found to
generalize well and for two reasons
because first because you add the
regularizer and second because you
ensure this marching now this is a
quadratic optimization problem and you
could go ahead and just solve it using a
kewpie solver but there is one catch and
that is that the number of possible such
constraints you could have is
proportional to the number of incorrect
ways and that could be exponential in
the number of
in the in the length of the input so
this is one problem but so that will
come to later but the other issue unlike
when you do single class prediction for
structured prediction it's not very
clear whether it does make sense all the
time to ensure this margin which is
proportional to the error of the output
is that the only possible way in which
you should be ensuring generalizability
or putting a margin what are the
alternatives make sense so in 2005 there
was this piece of work by so kinda at
least Austin Jorkins and three other
people where they should that another
formulation of structured training which
makes also a lot of sense is to ask for
a margin like they are we asked that
each correct input/output pair should be
separated from all others by a margin
which is proportional to the error but
in this slack scaling approach we
claimed that this error based margin is
too large you don't need this you just
ensure that there is a margin of 1 like
in conventional SVM just make sure that
all the correct incorrect ones are just
a distance of 1 away from the correct
way but then since you will not be able
to meet this requirement
exactly you allow for the soft slackness
and this slack variable though is
penalized in proportion to the error
this makes sure that if there is some
incorrect Y which gets within this
margin of 1 just add a penalty which is
proportional to the error of that
incorrect Y so here then you are slaves
gaining the slack variable thus i I
instead of scaling the marching which is
what you do in the first case now this
change of the objective actually is more
sensible I will show you numbers which
see that you know in this other method
where you ask for a margin which is
proportional to the error is asking for
too much you don't need this for
training you know as
you know solve the simplest problem that
needs to be solved that's the philosophy
one should follow for training machine
learning problem it's solving more than
that is required and therefore sometimes
it might be cutting corners and
compromising something else which is
more demanding and important to solve or
so so but so this is also in the rest of
this talk we will both be worrying about
what are sensible formulations for
structure training and we want to do
this while still maintaining
tractability we have to respect the fact
that the space of possible wise is
exponential in the number of the groups
so first now let's go back to the
computation concern so so this is you
know otherwise it is a kewpie ready to
be solved by a kewpie solver except that
the number of possible constraints is
exponential in the number of is in the
length of the input so you know in the
people have come up with a solution to
this approach
so William well understood concept in
optimization theory which is to use
cutting plane you don't put all the
constraints at a time start with no
constraints and solve this QP you get
some value of W find out the constraint
which is most violated with your current
W add that resolve your QP with now your
new set of constraints you will get a
different W and do this iteratively
until you know until you find no
violating constraint and this is
probably correct so this is the way to
tackle the X potentiality so so now when
you apply it for structure learning to
find so the the the most important thing
to be able to solve efficiently is to
find the most violating constant so
these two kinds of formulation give rise
to two different optimization problems
to be solved in the structured part so
when you do margin scaling the first
approach of doing training you have to
find the Y for which the score plus the
error is maximum now you can do this
efficiently only when both the feature
vector and the error function decompose
over the input while otherwise again you
are left with x-4 an enumerated
potential possibilities and like for
example if the error function is hamming
error you can decompose the error into
error at each position of prediction and
then this are max problem can be solved
efficiently so this is what is used in
practice everybody uses margin scaling
because the inference problem that you
have to solve is exactly the same as the
inference problem you would have to
solve if you just did normal prediction
of the substructure model so it easily
fits into your you know infrastructure
of inference for structured models but
when you try to use the slat scaling
approach you have to find the most
violating constraint which will
correspond to finding the why for which
the score minus whatever is your current
value of slack at that instance team
divided by the error is maximized now
say the score decomposes over individual
parts of Y's and the error function also
decomposes over individual parts of Y
for this particular R max problem it
still does not give you a decomposition
over the entire objective you you have
no tractability it does not help you
that the error function is say hamming
error which is additive over different
positions because it's in the
denominator in the previous case because
it was in the numerator it was nicely
additive and you could solve the arc max
efficiently but because it is in the
denominator this cannot be solved
efficiently but we know that this is a
better loss function you know it is
sexually empirically I will show you
shortly shortly that we get much better
results with slap scaling it's just
because of this difficult inference
problem I was just really surprised to
find that both of these methods they
were proposed on the same time and
people just stopped talking about slack
scaling margin scaling is what was used
in all you know structured learning
tasks which have been used which have
been written in the past 3-4 years there
has been lot of work on coming up with
better optimizer different form
and different algorithms but no one has
kind of visited this issue of whether
this is the right training objective or
not
so here the results from two information
extraction scenarios one way are we are
trusting it as a sequence labeling
problem and second we are where we are
casting it as a segmentation problem
these are three data sets this is a data
set where we try to extract address
elements like city names area names and
Road names from address strings and
structure addressing the second is the
citation domain we are given citations
from computer science papers we want to
break it up into author names and titles
and conference names in here and so on
so this was a released in UMass and this
one is a very well-known Cohen L data
set where in from the mock computations
sorry not the mock competition it's
started on different computation and on
the y axis we are plotting the span f1
error which means 100 - the f1 accuracy
so lower values are better the blue
lines are with the margin scaling
objective and the red bars are with the
slack scaling objective and of course
slack is much better than margin scaling
for segmentation tasks they don't see
much of a difference but definitely for
secret sleeping you see a lot of
improvement with using flex cables so
you know I'm just telling you they're
slack scaling is a better more sensible
objective is that because the inference
problem is so difficult it kind of
vanished from the literature
so this first contribution of ours is to
show that the slack inference problem
can actually be approximated very well
you should not give it up so soon and
there is a very simple way of
approximating it which also is
theoretically sound which will give you
the benefit of slack scaling without
paying the performance penalty so recall
that in slack scaling in order to solve
the optimization problem using cutting
plane we have to find
the Y for which the score minus this
inverse of the error function is
maximized and the our paint was that the
decomposability of the error function
did not help us in solving this
maximization problem efficiently so the
first observation is that the the part
which is painting us is actually not
that bad in the sense that this part
which is the inverse minus of the
inverse of their function is actually
concave in the denominator it's concave
in ebuy and because of that you could
use variational methods to rewrite this
as a linear function as follows so this
particular function is equal to the
minimization over all possible lambdas
lambda is a scalar value all positive
lambdas if you went over and you took
this linear combination of the lambda
and the error and minus this part which
the most notable part about this that
other part is that it does not depend on
Y ok so when you do this approximation
you can actually solve the outer
maximization problem more efficiently
but was just just look at the nature of
the approximation on the x-axis here I
am plotting error of 1 on the y-axis is
our difficult function which is minus of
the inverse of the error function and
this blue line is what we are trying to
approximate so this is a concave
function as you can see so instead of
trying to find you know and you know the
best way based on this concave function
what we will do is approximate this
concave function by a collection of
lines derived from choosing different
values of lambda for this right hand
side function for any possible lambdas
for every error function there is one
lambda for which it will be exactly
equal but if you pick some other lambda
also the the the right hand side
function will always be above this
concave function now this gives you an
upper bound if not only gives you an
approximation it gives you an
an approximate upper bound so here is
how we finally approximate the slack
inference problem so our goal was to
find the Y we have exponentially
possible many possible Y's for which the
sum of the slack minus this function
which is the inverse of the error
function is maximized so this is equal
to this by the variational approximation
we have not gone into derivation of that
this is not too difficult to see and now
if you interchange the max and the min
you get this inequality that this is
actually less than equal to min over
different lambdas and now once you've
picked a lambda you can just find the
max of this function now there are two
very useful properties of this
approximation so instead of solving this
maximization problem I'll be solving
this min max problem I do first a
maximization after picking a value of
lambda and then I'll search over the -
all possible lambdas there are two
properties one is that the inner
maximization problem is now very nice it
is linear in it uses the error function
and the lambdas in a linear way and if
the error function and the score
function decompose the same way you can
solve this maximization problem using
the same kind of inference problem
algorithm that is used for margin
scaling that is one good thing and the
second good thing is that the inner
maximization problem is convex in lambda
if the inner maximization problem is
convex in lambda for all possible
lambdas you have a convex function and
you can search through the minimum of a
convex function is easy to find using
golden search so we use golden search
technique to find the the best possible
lambda and now this gives us a good
approximation of slack inference so
these are some numbers on the same data
set you know so slack inference you know
it was it just so happened that for
information extraction sequence labeling
and segmentation tasks we could solve
for the optimal slack in four
exactly by using an expensive quadratic
algorithm but for all possible tasks is
not even possible to use slack inference
like if I wanted to use this for
dependency parsing or for alignment I
cannot solve for slack inference so so
in such cases the approximate slack it
is totally feasible and it approximates
in terms of accuracy if you see it is
very close to the slack in fact in some
cases it is even slightly better you
know so in all of these five cases you
see it's a good way to reach the slack
accuracy without taking the penalty of
inference the one where you yeah the min
max well so the minimax is yes because
of the search of a lambda because we
interchange the min and the max this
would this would be exact but because we
interchange the min and the max it
becomes a proxy but there is a bad news
the bad news is that it's not guaranteed
to actually give you the correct
implementation of the cutting plane
algorithm based on the slack objective
in the sense that you might have a
scenario where mu lambda can find you a
violating constraint even though a
violating constraint exists it's not a
problem of discretization it's a problem
of impossibility you cannot there may
not be any Y which will give make the
violating Y the highest scoring in terms
of the sum of slack plus lambda times
error function so this is a counter
example not okay well in practice but it
is not guaranteed to be correct and I
was not even hoping it to be you can
guarantee to be that they just seems to
do the job well and it is a kind of a
tractable function now and then that
later Sydney's thinking what is so
sacrosanct about slack scaling it was
one of two methods which was proposed by
people who are actually optimization
people did
care about information extraction or
structure learning application as much
as I thought I did okay and I still do
so so then I thought there was there's
got to be a different objective which is
just it has to be naturally more
tractable and it has to be a better
error function and and we came up with
something better
so so actually if you think about it
suppose if I created an artificial
problem as follows
I took say you have 1 million prediction
tasks to make they are all independent
of each other and I just arbitrarily
concatenate them and I create an
artificial structure learning problem
out of it the features don't couple the
vice a good objective should do no worse
than solving the million in independent
prediction problem independently
otherwise there is something wrong and
that is indeed the case when you use
slack scaling you would still just ask
for a margin of 1 even though you have 1
million independent predictions so I
just artificially created a vector of 1
million predictors and because I always
asked in slack scaling a margin of 1
independent of the length of the output
that I am predicting I am asking for too
little in slack scaling in fact in such
cases there was a paper of Thurston's or
Kinnison kgd 2006 Volta showed that you
can formulate even in debt independent
classification problem as a structured
classification problem by artificially
concatenating samples and that they are
equivalent and they will in fact turn
out to be the same as margin scaling so
in this artificial scenario of
concatenating examples and creating one
structure prediction you can in fact
argue that margin scaling is better than
slack scaling but margin scaling is not
good when you are actually solving a
truly structured task because then it's
asking for too much margin scaling is
overfitting the task and slack scaling
is under fitting it's asking for to
reimagine so we actually came up with a
different loss function I will skip this
example just
go to this new formulation that we have
which claims that in fact this is what
you should ask be asking for for every
correct input/output pair you should
want on margin of one but you should the
slack should be separate for separate
error positions so so so I am
considering scenarios in which the error
function say decomposes over these parts
which I am denoting a C so in normal so
I just do to make it comparison easy
this is the slack objective in slack
objective you have one slack variable
over the entire example entire instance
for each instance are you ever single
slack objective here what I am claiming
is that you should have for each
possible part for each possible position
over which the error decomposes have a
different slack variable and then ask
for this margin of one to be applied
over all wise which does not agree in
the cioth position with the correct Y so
this is what is saying for the ayat'
example in the sealed position whenever
a wide disagrees you you require this
margin and by using this alternative
loss function actually you get a much
simpler inference problem and much
higher accuracy and and there is this
notion of coverage which I have used to
argue that actually it is in fact more
sensible and here are the final numbers
so this last set is the new objective
which the inference problem is easier
gives higher accuracy then finally you
see even for segmentation tasks you get
a significant drop in error by using
this neutral objective when I'm done
see here what happens is you need to
find 4 so you fix say a C position C
like when you're doing hamming it you
fix to say the 5th position and you want
over all possible wise which disagree in
the fifth position the Y which has the
highest score you can ignore this error
because the error is just tied to that
position
so the error does not it's a standard
than prediction problem so in fact you
just if you want to do this
simultaneously over all positions you
find the max marginals</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>