<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Search Engine Architecture Based on Collection Selection | Coder Coacher - Coaching Coders</title><meta content="A Search Engine Architecture Based on Collection Selection - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Search Engine Architecture Based on Collection Selection</b></h2><h5 class="post__date">2008-01-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KpZpsu2wM1s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I would like to try to keep this talk
kind of no confidential this stuff was
researched a little before Google before
I joined Google and also actually this
is going to be published kind of soon
we're trying to submit on transaction
information systems so I will ask you
not to say things that are Google
related or whatever because we are
trying to put this on YouTube or Google
Talk series whatever public stuff so if
there are questions that are related to
Google maybe we can keep that for the
end when we stop recording so Google
doesn't endorse and we doesn't even like
this kind of work as I told you this
stuff is public but still maybe you
didn't see them yet so that's why I'm
here
so we present this work I did on how to
use collection selection on a search
engine architecture and actually slides
are available online yeah the URL is
kind of easy to remember okay the
motivation is what you expect so the web
is growing a modern that changed the way
it's used so people are expecting good
information live information and expect
to have results fast the index is
growing and also the data that store in
the index is growing you know there are
more quality signal total quality
signals that are used in one indexing
ranking and finding pages you know in
the web and plus I know and a parallel
architecture is probably the only way to
manage this thing there is no big
machine that can hold just the indexing
one chunk there are several research and
parallel architecture the only way to do
it there are billions of pages that are
stored today in in index and there isn't
it for partitioning this data into many
manageable chunks so it is to be a
parallel architectural parallel a
parallel information retrieval system
architecture is what you probably know
there is a front-end that collects the
query from the users
and underneath there are a bunch of
servers that are keeping a part of the
index and each in its index is queried
and there is our merged and sent back to
the user so this is a common you know
simple simple architecture but the the
problem is over here so we have an index
in an information retrieval system the
index is a data structure that matches
documents with terms so every time a
term appears in a web page we put a
little X here and we have a big matrix
at the logic level that's that's you
know showing our terms are appearing in
various documents and in literature
there are two main ways to split this up
into several chunks one is split them
auras on telly so one server is
responsible for a set of terms and the
other one is this one where one server
is responsible for a set of document and
knows about all the terms in these
documents here so if you have this
architecture every time you get a new
query
you're gonna contact all the servers
that are holding the terms so if this
server is old in terms that are not in
your query this will not be working in
this architecture you're going to
probably broadcast the query to all the
servers and every service with and every
server will tell you which documents are
matching your terms so this is the basic
idea there are different advantages and
problems with these architectures so as
I told you if you split the terms only
the server with relevant terms will be
involved in answering a query and this
can be can bring to reduce the computing
load of the other of the overall system
back to their problems of a balancing
because some terms could be more popular
or more queried or you know whatever
more complex to to answer also if you go
into details there is an average
communication pattern between servers in
term partition architectures because
they've got to exchange the posting list
and it's more costly I won't go into
detail anyway the document partition is
as better balancing but as I told you
all documents are queried so all servers
are involved into the
query and we were to try to see a way to
reduce the computing load we just with
an architecture that's document
partition so that's the goal of this
research the the main architecture you
know the rural architecture structure
IDs there is an interface that does
caching and collection selection we will
we will see this into details and on the
backend you still have your search
scores you're collecting statistic data
but the core is here we perform a
collection selections so every time you
get a new query we choose which servers
are going to be queried for the specific
request okay so the main contributions
of this research are the three that are
listed here so the document models new
is different and we represent a document
now with the terms the tower that appear
into the into the document but by the
queries that are able to recall the
document and this is actually a better
representation and allows for more
efficient partitioning and selection
also there is this new concept of load
driven routing so when the query is sent
to the servers we also consider their
instant load at the moment and we base
our decision on this on this information
and then there is nothing that's going
comment a question that differently from
a basic cache that just improves the
throughput it's also able to improve the
result quality okay I need to
acknowledge my co-authors and other
reviewers that contributed to this work
okay
other contribution that probably I won't
have time to show today are a more
compact collection representation this
is more compact at the state-of-the-art
quarry and it's actually outperforming
its results also there is an a new way
to find documents that are contributing
only to a tiny fraction of the results
so about after the collection as in our
experiment as low quality results and we
could just send them to another
supplemental index set of servers or
something like that and also introduce a
new way to update
the next so to add new documents to the
index and a contribution actually it is
relevant in in the academic field not at
Google is that actually the the test
that was quite big for for the standard
so was six million documents and two
million queries
it's nothing like things were treating
at Google but still okay so first of all
let's pick on how to improve the
partitions so out to split documents
into servers so we are able to offer a
better service to two users so we have a
document collection and we need to win a
partition strategy to map the documents
onto several servers and one strategy is
just to do a kind of random assignment
and then broadcast the query to each
server so just to have load balancing we
spread the documents all over randomly
or you know round robin fashion with no
control and every time we get a new
query we just we just broadcast it to
all servers you know there are the
strategies like doing document class or
based on the content or the links or
stuff like that but in this talk I would
like to show you how to use past usage
so information from the query logs to do
cluster into document clustering and the
way it works is as follows so you have a
set of training queries and those can be
I don't know the top queries from your
query log or a sample or whatever and
you record every time which documents
are matching which queries so let's say
query one is recalling document 1 4 8 10
and 12 and so on and so you've got all
documents here in my case with 6
millions you've got all your training
queries in my case was about 200
thousand and then you perform
co-clustering so ok document j is
returning by medic weary and
co-clustering is a very state if it's a
very robust and fast algorithm that
sorts columns and rows into blocks like
this so your is the calculation is tried
to aggregate
ones and zeros in inbox any of a class
of matrix that can be seen as you know
three
by three you've got three document
clusters and three query clusters so
every time we have a new query clusters
we just concatenate the queries together
and we call this query dictionary and
this would be used later for collection
selection while the document clusters
are set one per server so every server
will hold one of the clusters so the
Kukla string is a very stable algorithm
I told you was presented in in
literature about four years ago and is
able to find the clustering that
minimizes the loss of information
between the original matrix and the
cluster analysis so it has a very strong
theoretical basis the the implementation
can scale up because it's a it's prone
to be parallelized very well and it's
very robust because you can change the
test period you can change the number of
clusters you can change also the matrix
model I won't spend time on this but
across all this change in the results
are consistent so it's very very very
reliable the way we use this for
collection selection is that when we get
a new query we compare it against the
query dictionaries we created and we can
rank the query clusters and then we use
the result of the clustering to choose
the document cluster so we have a rank
of each cluster and I will show you this
in more detail in a second so let's say
this is the result of our cholesterin
process this is the sum of the scores
that appears in the rows and columns
contributing to this little block here
so you're summing the contribution of
each document in this cluster and it's
an each query from this cluster you sum
it here you have a matrix every time you
get a new query q you you do just you do
just a search of the query against the
dictionaries against the query clusters
so imagine that you have in this case
three query clusters and each of them is
described by the concatenation of the
queries and you look for the query into
this set of
issues you do know random you do it
usually in from use your information
retrieval stuff like TF PDF or a copy
2500 standard things anyway you get to
rank you're able to rank your query
collections and then you do column by
matrix and you're able to rank the
document collections and you can order
them so let's say that the third
document collection is more promising
than than the other ones so you can sort
them and you say this is the first this
is the second and so on what you do now
is that you start querying the servers
in that order so every time you get a
query you start carding first the most
promising servers server and then so on
we did some experiment to verify what
was the quality of the results we could
find using a subset of servers such as
the most promising this pyramids was on
a set of about six million documents and
we use about 12,000 queries for training
and we use about 800,000 queries for
tests
so we dis represented three weeks so in
second compare with Google this
represented through three weeks of
queries in 2003 to this most search
engine called I didn't write it
tada BR whatever so anyway this will
represent it as small as this
represented a real sample of queries to
a small search engine and we tested with
queries from the subsequent weeks these
are unique queries and these are also
repeated queries so this is why so
smaller the document class the documents
were splitted into sixteen document
clusters and we created 128 query
clusters and we measure the intersection
that is the number of results we could
find in a subset of servers we were
aspect to the full index and also we do
we use these more complex matrix the
actually was measured was not just
measuring the intersection but the the
value so the score of the results by
mean these are complete gave similar
results this sum mathematically describe
better quality max but I mean the
intersection is just the
this is the magic for intersection so we
intersect the results we get from from
our server set with the ground truth
okay whatever that simple anyway let's
look at the result one second so we try
different partitioning and different
selection strategies and the first one
was just to use quarry that's a standard
collection selection architecture on a
situation where documents were randomly
assigned to documents so if we just do
collection selection we tried to query
the first server the one that's believed
to be most promising we get let's say
dot three documents out of five then if
you do say if you do mathematics is just
one seventh it's just you know random
stuff so collection selection is not
helping you if you have random
assignment well if you do the the
classroom as I just showed you and use
quarry on the same thing you're able to
have a big improvement of about five
times so if you do if you potentially if
you partition your documents beforehand
and you scurry you're able to get about
one third of the documents that we would
get for them from the fully next but
just on one server out of sixteen the
seventeenth server in our experiment was
just the set of documents they were
never never find never recording
training so all the documents that
queries couldn't find ever during the
training were set over here and these
are our if you want supplemental index I
could did contribute much the result so
if you look at the main set of documents
using just one of over 16 servers you
get one third of the results so and you
can choose your you can choose your
point you can choose your quality by
increasing the number of servers if use
our new collection selection strategy
you can go about ten times ten percent
better than quarry so this is actually
outperforming the state-of-the-art and
this represent would happen so if you
choose to use only one servers one
server or two servers or whatever for
each query you select the most prominent
and you query just one - out of all your
set you have this nice this nice
precision cord so using just one one
server you can get about one third of
the results you get from the full
collection and if you want to have I
don't know two third of the collection
to use about half of the servers but
actually this region is very interesting
because this means that if you have a
peak load or if you have failures or if
you need to accommodate a lot of peak
load you are able to give reasonable
result with very very limited resources
but we can still improve this anyway
there are already some very interesting
idea in this point so when you do the
partition in the popular queries are
driving the distribution and this is
good because actually popular queries
will find better results on average also
as I told you we are representing the
documents in a new way it's a lower
dimensional space and actually it's also
better when you're trying to do other
type of clustering based on vector space
it's a more efficient collection
selection it's about 150 smaller than
Cori and also interesting you can build
this representation while answering
other queries so you know the the search
engine could be running in the meantime
and you keep collecting results I tried
to find out what were the weak points
and one could be the actually there is
dependence from the training set but we
changed that and didn't affect the
results too much it's true that we
cannot manage terms that we've never
seen before
so when we do the training we
concatenate the queries we got from the
whatever from the users so our training
set is a limited vocabulary if you have
a new term you're not able to find it in
any query cluster so this is actually
problem you can resort you can go back
to Cori car is a technique that use all
terms in the dictionary it's much bigger
for this reason but actually if you look
at the average at aggregate at the
aggregate result it doesn't prove too
much while our information incremental
cash technique is actually up in a lot
in this case and I was a US
in a moment also the collection
selection depends on the document
assignment you do at the beginning of
your configuration back so if you add
documents you don't break performance
and this was tested ok so let's see what
happens with low distribution so this
strategy has the same drawback of third
partition architecture so you know some
servers could be more popular than
others so there can be a big difference
in log between one collection and one
other and another because for instance
this collection is holding all documents
of you know very popular topics and this
collection is holding you know not so
popular stuff so that can be a big
difference and actually this is a cost
to the search engine because there is a
machine that some are utilized so what
we like to do is force the load to the
same big level ok the load in this case
is very approximate measure it's the
number of queries in a sliding window of
about 1,000 queries so this is just
approximation anyway if you use this
architecture you're still reducing your
peak load from in a hundred percent that
would be serving all queries to all
servers to about 1/4 and with you know
this limited load you're able to get
about one third of the results so we
introduced two load balancing strategies
and in simplifying these the way it
works is like that every time you get
you got a query and we rank the document
clusters for the query we give a
priority to each collection and the
priority goes from one for the most
promising server down to zero so it's
linearly decreasing and every server
chooses to okay we broadcast the query
and every server answer if it's load is
smaller than its priority times the
threshold we set so when this is 1 so
for the most promising server the the
server I will always answer when this is
decreasing the server I will answer all
with only if it's priority is only if
it's loads
more than the priority times the the
load and this is a little variation
that's that's not important but anyway
in this in this configuration a query
get answered by the most promising
servers the most of you know the one
that we reacted IIST and also the server
that are at the moment idle or
underutilized in this configuration we
are modeling the cost as just one so
every time enhance requests a server is
answering a question a query we had one
and we count the number of ones so very
simple model but we are we're gonna work
on this and we're gonna improve this in
a moment
anyway if you look at the result with
this little strategy you're able to push
your load from from the blue bar to the
red and the yellow one the yellow is the
little you know the different version
that I just skipped quickly so this just
says snapshot at a certain point of the
load that's that's being held by any
BIOS servers in the configuration so if
you just take a snap so that the of the
lot of your servers you're able to you
know all of them are running to pick
pick pick power and actually the results
are improving a lot are improving a lot
because this was what we had before when
we chose to query let's say four servers
per query so every this is the initial
configuration so let's say we have a
query we find the most promising four
servers and we query the top four
keeping the same peak load that's about
1/4 of the of the hundred-percent
we can get about 10% results better just
by using these this strategy that's
using idle load so without increasing
computing power this technique is about
to improve about 10% of the result I
know this different version is adding a
little more but at every measurable
level we know added computing load this
tree can improve your result you know
significant significantly also ok
caching is another interesting point
when you do collections ok ok this is
some background so result caching is
using
all the web search web search engines
because it's able to prevent popular
queries to slow down the servers so very
popular queries are not sent to the
backend and are intercepted by the front
end it just sending but the results back
to the user it's interesting also
because when you've caching it's
reshaping the the distribution of of
your queries and so after caching not so
popular queries become very popular you
know compared to the to the full set
here we introduce a new caching system
that's that's taking care of the
collection selection there is a drawback
when you just do caching with this
strategy so if you get if you have a
query and you do collection selection
maybe only one or two servers are
actually answering your query so if you
catch your cash in on a part of the
result and if you're serving this
information back to the user the user
will find the graded result so what we
try to do with incremental caching is
that every time we get the same query
we're going to add new results to the
cache so over time popular queries will
have a full set of results so this is
interesting because repeated queries
will have actually the full set of
results and they will see no negative
effect of collection selection okay so
what what's happening now is that
incremental cash is yes reducing the
reducing the load to the back end
because it's intercepting a part of the
of the queries but so see improving the
result quality at the same time okay and
this is the new set of results if you
add incremental caching to you know low
driven routing what happen is that
you're able to move from let's say 3.5
results over five over the top five
you're able to get four results out of
five using the same load you would have
to guarantee that just four servers are
queried every time so I would say that
in different ways if you configure
system so that you're able to query at
least two until this fourth server per
query with the same peak load you're
able to get about you know four result
out of out of five and this is true more
or less with different metrics and
different configurations so if you
change the cache size if you're using a
static cache if you're changing the
training week the test week across all
different configuration actually adding
no load you can get you can get very
good results so before we were using a
simple model cost where a query cost
just one by actually we refine this and
we use real time in for the queries and
the results actually were confirmed
so which Anjali does you know the
simulation infrastructure so at this
time the server's determine their own
their own load measuring it for instance
their computing time to answer a query
and they sum up that over time and still
the answer if their load is smaller than
the priority of the query times the the
you know they are computing power so
again the blocker so the interface the
front-end is ranking the collections and
it's given different priorities to each
collection for a query so it's
broadcasting the query to all the
servers with a target priority and each
server will choose to answer if the
instant load is more than the priority
times the peak load and so if if the
priority is one for the most relevant
server this will always answer and for
less relevant servers the they would
answer on if they are idle or you know
underutilized and as it's important to
choose the cost that's not simply one
because the cost varies a lot from
milliseconds to seconds so this was
actually on you know in the machine at
my university you know there were other
applications running you know all this
all that stuff but still mean it this
was a gross a gross approximation anyway
so we actually partitioned the documents
um to see different servers every server
at the local index so every server
was able to answer queries for the
documents it was holding and we measure
the timing of each query so actually the
real timing was used to compute the load
and drive the system there was a little
a little change here
so before we measure the peak load of
the system and that was our I'm sorry
that was our threshold for for um for
dropping queries back if you take the
sum the the log the the peak is very
high so actually did the average the
load I mean this was this is small
correction but anyway the results were
confirmed so actually we're even okay
well confirm is actually are even better
because if you're using collections
select what is this one yeah so if
you're using the low driven strategy and
collection selection you're able to
guarantee about 2/3 of the results you
get from the full index by using the
computing load that's guaranteeing you
just the first the first most
authoritative server so if you are able
to tune your systems so that at least
one server is guaranteed to be able to
answer to the query using the idle load
and the caching you are able to get
about 2/3 of the results with no added
computing pressure I also did some
results on a bigger load on a bigger I'm
sorry query logs this is actually 2
million queries and results are more or
less confirmed ok so this concludes this
was pretty fast so there is time for
questions in this work I try to present
an architecture that's using heavily
collection selection so documents are
partitioned onto several servers and
only a set of servers is used to answer
these no different strategy with the
incremental caching can get actually
very high result and reducing heavily
the the computing pressure on the system
and it was verified you know with
different simulations and different
configurations so the way it's going to
be used is like these if you are if you
are trying to reach a given procedure
you can use you know fewer servers and
if you're given a number limited number
of servers your chart you know you can
get better precision and this was
confirmed different metrics so this this
actually is showing out to you know this
shown a trade-off between cost and
quality and it's boosting the you know
focus on top results at the expenses of
low-quality results but it can be useful
if you're serving a very big collection
of data and you're you know and you can
afford lose some results also this can
be interesting because the low driven
routing can be used dynamically so if
there is a peak load or if some machines
are failing or whatever you can use this
strategy to reduce the computing load to
accommodate more users more queries
another interesting thing is that still
while even if you are using collection
selection and you're using servers
independently there are ways to have
consistent ranking so when you get
results and you merge them out to the
servers the results are actually
consistent with what with what we would
get with a full with a full centralized
inside index and at the end I was
telling you briefly before the
incremental caching is actually good
also to reduce the negative effects of
selection so if you have terms you've
never seen before and you're not able to
do a good collection selection by if
that queries for some reason popular so
there is a new term that becomes
suddenly popular like I don't know
iPhone or whatever what's new today
anyway so if you have something new and
everybody's asking about that the query
will get populated then you will get
foo-foo correct results for the new term
there are other things that are very
interesting you know they are very good
at some more more potential reason more
potential so if you're doing posting
list on the local index the posting list
is the line in the if you have your
document and term matrix the posting
list is the line so it's the line that
for every term gives you all the
documents or
so if you're doing if you're caching
terms on the local index you will get
someone more focus queries because only
queries that are relevant to that server
will get there and so actually the cache
you will have less you know there will
be less noise for the for the cache for
the local cache in there in the server
also there is a way to add new documents
that's not that's not breaking
performance so there is a way to add
documents to the collection without
breaking the performance and actually
it's very important because you want to
be sure then when the collections are
changing your collection selection
function is still working well and at
the end there are ways you can improve
the caching not just by using I don't
know least reason to use algorithms and
are you algorithms for for the cache but
for instance you could use information
based on query frequency or maybe the
number of servers you're ready polled so
if you have a cache line that's fat
because you've queried a lot of servers
maybe you want to keep that over time
instead of a swapping it out and that
concludes I've got some backup slides if
there are questions on you know the
training on adding documents and
otherwise we can just call it today
thanks for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>