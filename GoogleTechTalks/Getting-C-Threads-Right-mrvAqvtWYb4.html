<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Getting C++ Threads Right | Coder Coacher - Coaching Coders</title><meta content="Getting C++ Threads Right - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Getting C++ Threads Right</b></h2><h5 class="post__date">2007-12-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mrvAqvtWYb4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">why are we interested in threads well
the this is sort of fairly canonical
argument future causes and performance
enhancements as we all know are likely
to come for multi-core processors it's
going to come from having parallel
machines rather than enhancing single
called performance often don't clearly
not in all cases especially especially
around here programmability
is a limiting factor there is in fact
lots of research on alternate
programming models for multi-core
processors or parallel machines in
general there are many people working on
transactional memory there's a lot of
interest in message passing models
clearly there's also interest in data in
more flexible data parallel models that
we could use to program these machines
on the other hand if we're talking about
paralyzing a single non numerical
application I think if we look at the
status quo at least pretty clearly feds
unlocks actually dominates particularly
if you look at what's going on on
desktops the main reason that they do I
suspect is essentially because they're
their threads and locks have been around
for a long time
they clearly predate the widespread use
of multiprocessor of multi-core
processors they were there primarily to
deal with multiple events themes and
things like that that had nothing to do
with parallel machines they were
invented primarily they will originally
use primarily as as a programming model
for the reasons but that makes them
widely available and it's and the
natural mechanism to exploit on the
other hand if we look at threads and
locks in general in many contexts in
many circles at least they have a bad
reputation certainly we all have horror
stories about intermittent failures of
multi-threaded applications there's a
general feeling that they're too
complicated we commonly hear statements
like locks don't compose which i think
is somewhat true though it's actually
hard to make that precise if you do is
if you do a web search for the phrase
threads are evil you get lots of hits so
the point of this talk really is that
not so much that these problems aren't
real at all I think some of these
problems are real and we should be
looking at alternate programming models
as well on the other hand not all of
them not all of the bad reputation here
is inherently deserved not all of it is
really rooted in profound problems
sometimes the issue I'll argue is just
that we as and we got things wrong the
first time and we haven't gone back and
fixed some of these things up until now
a lot of this wasn't all that important
because parallel machines were multi
basically multi processing machines were
relatively esoteric most people didn't
deal with them
on the other hand now that's clearly
totally changed and in fact I think it's
I think it's important to fix these
problems because now they are now they
have become important and I'll try to
convince you that most of these problems
in fact are fixable the problems aren't
really inherent I'm gonna be a little
bit sloppy here about what's actually my
work and and other people's work here
there were many contributors to a lot of
this there's a partial list up here last
time I checked two of the people up
there actually worked for Google that
may have changed by now it's hard to
keep track but so there are many other
people who worked on this as always when
looking back it's easy to complain about
stuff that was done in the past in fact
I don't know if I'll get this across to
you some of what was done in the past is
in fact quite clever and sort of and is
it's quite clever but it didn't quite
make it it's sort of it's it's an in
very interesting near-miss in my opinion
it's so it's very easy to take cheap
shots at stuff there was done 10 or 15
years ago he and I Paul I apologize for
that but nonetheless I think is these
are things that need to be corrected
some of the solutions here some of these
problems actually are difficult
technical problems some of them are not
so some of the solutions I'll point to
here in fact quite obvious when you look
at it and in some sense that's part of
the point we in some cases guy even got
things wrong that all in fact I think
not that that profound so as a group
what we've been
doing here is basically time to identify
some of the problems that we that we
have with multi-threaded programs at the
moment and trying to resolve those
whenever whenever that's possible the
there are a number of things we've been
trying to do some of the technical
solutions in fact turn out to be not
non-trivial and I'll try to give some
hint of that at least here so working on
the technical problems has suddenly been
part of this effort there's also been an
effort to try to push some of this
through the C++ Standards Committee
Lawrence Cowell he was in the room he
has been heavily involved with that as
well so the plan here is to the goal
here is to provide proper support for
threads in the next C++ standard which
is for historical reasons
well maybe historically is not quite
clear yet this is referred to as C++ 0x
the the joke here for a long time was
that X may actually be 0 X a the the
status of this as of the moment is that
the current working paper for the next
C++ standard actually includes a lot of
what I'm going to be talking about but
not everything so it includes the
threads memory model that I'll be
talking I'll be touching on here which
basically defines what it means to
access a shared variable from a fed
which is sort of the foundational
question that we're addressing here it
also includes with thanks in large part
to Lawrence in atomic types and
operations library in the current draft
and it almost includes the threads API
as part of the language definition as
opposed to the definition of an
auxiliary library that you use in order
to get threads we're trying to do some
other things as well here in addition to
the C++ Committee we've several of us
have been talking to the C &amp;amp; POSIX
committees currently at least my
understanding is that the C committee is
fairly likely to do things in a way
that's similar to the what the C++
committee is doing which would be a very
good thing
we've also several of us have been
spending time
- very talking to other interested
parties and the other interested parties
it turns out here mostly processor
vendors in order to address some of the
other issues in particular this the
issues here as I'll show you extend down
to the hardware level in a few cases so
one of the things that's actually
improved dramatically recently from my
perspective is that both Intel and AMD
have released new memory model
specifications for the for the what's
commonly known as the x86 architecture
which resolve some of the issues that
I'll tell you about later here and I and
we've been trying to sort of spread the
word about some of these issues to try
to resolve past misconceptions the rest
of the talk here is basically structured
as a list of problems and then
explanations of the solutions that we
expect to use in order to resolve those
problems most of these in fact are in
this are now in the C++ CRX working
paper but I should warn you they're not
yet in a formal standard so anything can
still change at this point so the again
the main goal here is try to convince
you that first of all they're problems
here on the other hand these problems
are solvable and I this is this a
solution for the most part they're
solutions on the way so stopping with
the list of problems here that we
encountered I actually want to start
down sort of at the lowest level at the
hardware level if we if we're talking
about multi-threaded programs basically
everything relies on them on what it
means to assign to a shared variable and
to access the value of a shared variable
that that basically gets us to this
issue of memory consistency or memory
ordering which is underlying most of
this in order to get you better feeling
for what we're talking about here if
you're not familiar with the issue I'll
sort of give you one simple example if
we have two threads one of which one of
which assigns to X
and the other one assigns to Y both of
which are shared variables and then each
said leads the other variable the
question is can can both of these feds
see the PA see the initial value of the
variable fail to see the assignment and
the other fed with most people's
intuition of how goods work that
shouldn't be possible because one of
these sets has to go first intuitively
thus one of the assignments of one has
to go first and therefore the other
third should be able to see a one there
in fact some some basic algorithms that
rely on the fact that this can't happen
and a lot of programmers when they first
start to program with it's basically
assumed that this is the case they
implicitly assume what's what's normally
called sequential consistency which are
you can informally define as the feds
behave as though the steps of the
individual threads are just interleaved
so by the normal interpret by people
it's normal intuition this outcome here
should not be allowed whether or not
this allowed is allowed in fact it's
determined by all sorts of things
including the hardware the compiler and
thread libraries and so on and basically
everything on since everything is built
on top of the hardware for all of these
things to do their job correctly to sort
of provide the right semantics here we
need to get the ha the understanding of
the hardware right the interesting
observation here is that if we look at
this particular outcome
most Hardware in fact even if you assume
that the other layers of the stay of the
stack in fact translate this directly to
Hardware instructions the way you expect
most Hardware will in fact allow this
outcome here most Hardware will in fact
allow both of those to read 0 the reason
for that is that most Hardware when it
executes the store instruction in fact
does not make the result so those that
store instruction immediately visible to
other processors it just goes into a
right buffer a new processor where it's
not visible so if these execute
basically exactly in lockstep
then the the initial storm may not
actually have made it to visible memory
yet or two coherent cache by the time
the second statements are executed so
both of them may in fact CD the initial
value so what happens here well
fundamentally the fact that the hardware
doesn't quite implement what we expect
to see shouldn't be a problem because we
have that and all sorts of other
respects as well we normally just try to
he try to hide any sort of miss features
like that in the software higher up in
the stack right so provided we actually
understand the hardware goals we should
be able to to exploit those Hardware
goals to implement some usable
programming model that we can all agree
on and that that we can then program to
and that can be implemented reasonably
on the hardware possibly by tell telling
the hardware something else widely held
beliefs at least as far as I can tell
about this are that in general allowing
the hardware to cheat like this and give
unexpected results is probably a good
thing because what happens is that most
of the time we actually don't care about
our simultaneous accesses to shared
variables like that we as you'll see we
generally try to discourage people from
introducing simultaneous accesses like
that anyway and most of the time we
don't care and since we don't care the
hardware should be allowed to do
whatever is cheaper which includes
giving us these unexpected outcomes
occasionally we will care about outcomes
about disallowing things like the
outcome on the last slide in that case
what we do is we provide and hardware
generally provides other instructions
that allow us to notify the hardware
that we do care about the memory
visibility ordering these are normally
called memory fence instructions on most
hardware they take that form in any case
that allow us to get back to sequential
consistency to the behavior that
programmers expect so in those cases
basically the software stack can hide
the fact that the other Hardware will
order these memory accesses underneath
the covers so that's the theory so what
actually happened well if we go as I
said recently things have gotten better
here fortunately on the other hand if we
go back for this to some sometime around
2006 in fact if we looked at many of the
architectures what actually happened was
that not only did they have somewhat
weaker memory ordering they not only did
they allow this kind of weaker memory
ordering they often didn't specify very
carefully in ways that that some of us
could understand exactly what the real
what the ordering was allowed what the
semantics were and what they weren't
this was this was shared between several
architectures at the time in any case
the result of this as far as I can tell
was was general confusion and sort of
weird then some weird things happened
higher up in the software stack which
probably sort of tended to aggravate the
the issue because it also made it less
clear to the hardware designers what was
really expected of them so I put up a
couple of interesting consequences here
that we found in this process I guess
GCC is commonly used here so if you look
at the what GCC compiler provides it
actually exports a primitive directly to
the programmer that allows you to
control the sort of memory reordering it
basically has it has a call called
underscore underscore sync synchronize
which is intended to ensure that memory
operations are not only ordered it cost
this call so it's defined in the
documentation as a full memory barrier
on the other hand if you look at its
implementation on x86 it actually at
least last time I checked actually
generates a no op which it turns out by
no reasonable interpretation of the x86
memory model is actually what it should
do and be consistent with that
specification and the problem is as
we'll see from the next slide as well
I'll hit that on the next slide it's
actually not clear that this is fixable
just fixing the implementation here
they once solved the problem for
performance reasons
another interesting consequence or
another interesting situation that at
least to me seems strange and I've I
have never gotten so a complete
explanation of this from Intel AMD
engineers is that the pen around the
time of the Pentium 4 the architecture
introduced something called alpha and s
fence load fence install fence
instructions on the other hand based on
the understanding of the x86 memory
model that most of us had but which
wasn't clear from the description at the
time loads and stalls on x86 in fact I
ordered anyway so it's not at all clear
what these do so as far as I can tell
for most code elephants and s fence on
pentium fours are in fact something like
hundred cycle no ops this is not
completely to actually but so there are
some funny corner cases where you might
actually need this which makes this more
interesting if we look at the
performance assumptions that went into
this the the assumption here was that by
limiting memory ordering to explicit
fence instructions we only have we only
required the hardware to enforce
ordering of loads and stores precisely
in those cases where we needed it so we
should always be ahead in fact that's
also not the way it worked out as far as
I can tell so the if we look at the
performance of these instructions and
I'm sort of picking on in a slightly
obsolete chip here which was probably
the worst offender in some ways for a
Pentium for the cost of one of these
instructions that actually enforced
ordering was in general on the order of
100 cycles in the best case that's
assuming no cache misses and so on so
that that's kind of an expensive on the
other hand if I compare that to as far
as I know around that time there was
really only one architecture that really
enforced sequential consistency at the
hardware level and that was pa-risc
pa-risc
in fact was reasonably performance
competitive which suggests to me that
suddenly it did not cost a hundred
cycles per memory operation in order to
enforce a lot of thing on a paas
processor so in fact the theory here
doesn't seem to have quite meshed with
the our the either the other interesting
thing that happened is that we ended up
with some memory models in which it was
actually not at all clear that thin
defense instructions that were provided
by the hardware actually could be used
to get you back to full sequential
consistency and hence if in fact this
was not is it full sequential
consistency is not implementable it
turns out you also can't implement the
Java memory model so you can't implement
some of the programming languages as
defined on top of it so the example here
law states why this might be an issue
and this is fine it by now semi famous
example here I guess called referred to
as the IRI W independent reads of
independent writes example so the idea
here is this looks kind of complicated
at first glance but what's going on here
is threads 1 &amp;amp; 2 just our independent
writes to different shared variables and
3 &amp;amp; 4 leave them in opposite order and
convince themselves that the other one
is set first so good that Priya reads
things in such a way as to prove
conclusively that that X was set first
and thread for weeds things in such a
way to prove conclusively that that Y
was set first the question is can this
happen the problem is that it's not
complete it's intuitively not at all
clear that just by inserting fence
instructions here in the middle of
between the two loads of that's--they
and thread for that as a result we
preclude this sort of outcome because
one could still intuitively think of a
machine where dead 1 &amp;amp; 3 are somehow
located close together for example
they're feds on the same processor core
and they're 2 and thread four are
located close together so they can each
see each other's rights first and just
in something offense there that prevents
those loads from being reordered in some
sense doesn't
change much of anything yet this this
program is fully fenced we have fences
between every pair of instructions so
it's not clear that we can do any more
either okay going on here the good news
is that this has actually gotten
significantly better recently if we look
at the status now as I mentioned earlier
Intel and AMD in fact published much
better descriptions of the the memory
model of those those processes which
clarify a lot of those issues so in
particular if we look back at the
example on the preceding slide the the
IRI W example it actually turns out that
on x86 we can get sequential consistency
for that example on the other hand
somewhat surprisingly which I think
nobody knew before the specs came out is
that this actually requires that you
implement the store the stalls in the
first two threads into X&amp;amp;Y using an
exchange in starch so so actually the
good news is that we have a solution the
bad news is that I suspect we still have
a bunch of JVMs out there that in fact
are technically broken and don't follow
the rules because they didn't know what
the rules were at the time at which they
were written in general on other
architectures similar things are
happening and we seem to have fairly we
seem to have fairly good stories pretty
much everywhere as far as I can tell we
either have fairly good stories that
we've convinced the vendors that they
have to do something about it ok moving
up a level on the stack here two
languages and compilers the programming
rules I will claim the programming rules
for dealing with threads in general have
been pretty unclear it's in some
languages more so than others in
probably the the language that's sort of
in the best that's in the best shape
here is Java the the Java memory model
which defines the basically the
semantics of shared variables in Java
was fixed in 2005 it turns out there's
some ongoing discussions about some
these issues here and in fact the the
issue that I just talked about about
implemented bility on various
architectures was one of those that came
up since then so things haven't
completely settled but there is a mostly
consistent story there if we look at net
and OpenMP I think my personal opinion
is that there's a lot of clarification
that could be it could be used there for
the rest of this talk I'll actually as I
hinted not to inconspicuously in the
title here I'll focus mostly on C and
C++ where the assumption so far is it
has been since these languages don't
explicitly talk about threads that I'm
programming with the base language plus
a third library generally for my
purposes P threads because that sort of
has and has is that's backed by a
standard specification so let's look at
the C and C++ with its programming rules
the general rule I think probably
everywhere though I'm suddenly that's
the case now and the past was not
completely clear on on Windows platforms
I think but in general the rule is that
we disallow simultaneous access to
shared variables what's commonly
referred to as the data arrays so we
don't get to access the same variable
from two threads simultaneously if at
least one of them as a right that's for
example the rule if you look at the the
POSIX standard unfortunately many people
who program with threads and C++ don't
look at the POSIX standard so but it's
fairly clear that that was intended to
be the wall this actually is a rule that
goes back fairly far fairly far it was
basically what was what was prescribed
in the 883 standard it was explored in
Sunita's obsolete obvious thesis after
that it turns out that this solves a
whole bunch of problems by just
insisting that programmers not have data
races in their program basically you
can't tell whether a lot of nasty stuff
is going on under the
so you can't tell for example whether
your compiler
reorders loads and stalls underneath
because any program that would be able
to detect whether two stores really
ordered for example would have to would
have to read those essentially at the
same time without synchronization so any
program that does that would have some
execution with the loads and the stalls
core or curl simultaneously so it would
have a database so basically it gives
the implementation license to cheat
without getting caught so this happens
now this applies not only to compiler
the ordering it also applies to the sort
of hardware reordering that I talked
about at the beginning so if the
compiler now the hardware is allowed to
delay stall instructions by stalling to
a write buffer because you can't tell
any code that can tell is wrong by
definition it also has the the
interesting consequence and which
simplifies the compiler digest job
significantly is that the the C or C++
compiler may in fact rely on the
assumption that variables don't change
out don't you a synchronously change out
from underneath it
this in fact potentially has some fairly
profound consequences which I think a
lot of people don't appreciate though it
seems to really encounter the the single
earlier current practice which is a
common issue here so here's an example
of the kind of assumptions that a
compiler is currently allowed to make
and what can happen if that assumption
in fact turns out to be false so we
assume we have some unsigned integer
variable ax and then we check that X is
less than 3 and then after doing some
other stuff that doesn't touch X we
execute a switch statement on on X let's
assume that that switch statement is
implemented by indexing into a branch
table and branching indirectly to one of
three locations depending on which case
were in since we previously checked it
check that access ideas you know one or
two the
Pilon is allowed to Eli that the rain
check on the the branch on the indirect
branch to the branch table and just
assume that that's within range and that
works so long as we follow the rules now
what happens if in fact somebody doesn't
follow the rules and asynchronously
changes X between the conditional and
the switch statement well what happens
say say X has changed to 5 there by some
other thread in the middle what happens
is that the switch statement will
mentally go ahead and use X to index
into a branch table indexing off the end
of the branch table and branching to
whatever location is there causing some
weird stuff to happen basically causing
the plugin to take a while branch so the
important thing is that in this model by
introducing by introducing a data race
where we weren't allowed to have one we
get something which in fact doesn't
correspond to just leading a different
value from the shared variable we get
complete nonsense at the end and that's
allowed by current standards so what's
the reality here as far as I can tell
the common attitude is that data races
in fact aren't so bad and that some data
races well in fact there was a paper
published in peel the IEEE fairly
recently about benign data races so I
mean clearly in some cases people
believe that data races are okay so for
example the kinds of things that that
people commonly do is that they maintain
approximate counters in ordinary shared
variables by just Inc by incrementing
the counter in multiple threads without
any sort of locking or synchronization
and then reading the counter
asynchronously since these are
approximate counters that only supposed
to be statistically valid the assumption
is that well this may go wrong every
once in a while because you have a race
so you might actually lose one of the
updates on the other hand it's not so
bad
another fairly common idiom that people
use is something called double check
locking the idea here is that you lazily
initialize a value and
you you avoid doing the expensive
synchronization on every access to that
to that variable by checking the flag
that says has this been initialized
ahead of time and only doing this Inc
repears to not have been initialized
it's another one of these things that
that mostly works is a fairly common
idiom on the other hand it involves the
data arrays in many environments people
actually combine this with atomic
operations that are somehow provided in
platform-specific ways so on Windows
there are these interlocked operations I
already mentioned on GCC has these as
underscore underscore sync operations
which are often used for reference
counting and these things are often
combined with reading the resulting
value using ordinary sort of using
ordinary variable operations using an
ordinary variable reference all of this
really isn't particularly well defined
at the moment and lead accesses
generally appears the data race to the
compilers so you have to worry about
things like what happened on the
preceding slide where you end up taking
a while branch happening fortunately and
this doesn't happen very often in
practice so we we rarely get caught by
this on the other hand it seems fairly
clearly against the walls on the other
hand we have the problem that locks are
expensive enough that they you can't
quite tell people just not to do it not
to do this properly synchronize your
program because especially for something
like reference counting in C++ which is
fairly prevalent the cost of doing this
any other way is in fact for is is too
high okay here's an interesting
complication which which always enters
the or very often enters the discussion
at this point when I bring these up and
that's well if you share these variables
and they can they can change
asynchronously shouldn't you just
declare the variable volatile and the
answer here unfortunately is sort of it
depends so if we look at the current
Java definition after 2005 at least
then the answer there is fairly clearly
yes for something like double check
locking or in some of some other idioms
in fact that works just fine so in Java
if I do have an initialization flag
they're called X in it and I declared
volatile I then assigned to X and set
the initialization flag that in fact
guarantees that X is set before that X
is visibly set to other threads becomes
visible to other threads before X in it
becomes visibly true in other threads so
if I check 2x in it in another thread
and then read ax I'm fine
this is often implemented by by putting
a fence fence Hardware fence instruction
before the volatile store there and
there's other stuff that's required to
implement this I want to point that out
explicitly because there's an
interesting contrast here in contrast if
we were programming in openmp here's a
quotation from the OpenMP 2.5 spec and I
think this actually is more less
unmodified in 3.0 draft reference that
modifies the value of an object with a
volatile qualify types b a.type behaves
as if there were a flash operation on
that object at the next sequence point
which essentially means that an openmp
the implementation should and so defense
after the assignment to accident if I
declare sanded volatile here which I
haven't yet figured out why it says that
as far as I can its I've had a lot of
trouble figuring out any use case in
which that's actually what you wanted to
say but nonetheless it's clearly not
consistent if we look at the C with P
fed situation the relevant quotation
here because it makes it cleaner than
what's actually in the standard as a
quotation from Dave Putin half was one
of the people who wrote the standard
there originally volatile provides no
help whatsoever in making code thread
safe and if you read the standard
carefully carefully that's why that's
what it says not not as clearly but so
we have a bunch of different really
different situations here
so what's actually in this in the C++
OEX working paper basically again
largely thanks to Lawrence yeah
concurrent access to two variables is
allowed but they have to decode be
declared specially as atomic objects so
for example you can declare a variable
as we're having an atomic as being of
type atomic int and then that provides
operations to concurrently access the
variable safely and for our purposes the
concurrent accesses to an atomic int are
no longer viewed as a data race that's
the way this is this is dealt with
formally other than that the c++ working
paper actually doesn't make much change
to what's actually what's currently the
official status quo it again really
affirmed that there are no benign data
races in c++ so we're wherever you have
a data the other way to say that is
wherever you have a data race you should
be using Atomics otherwise the behavior
is undefined and that includes taking
wild branches it's important to be
explicit here that c plus the c++
volatile qualifier in fact still is of
no help here so we haven't changed that
either there was a lot of discussion
about changing this it turns out it's
really difficult to do because c++
volatile in fact has some current
semantics which would be difficult to
preserve if we also used it for this
purpose so that's why the decision was
made to instead invent something new
call it Atomics rather than volatile in
c++ even though that introduces this
unfortunate translation problem between
c++ and java in that java volatile c++
Atomics and c++ volatile are not java
volatile it turns out they're in fact
some other more technical problems here
that come up so far we said we've told
the programmer to not write code that
includes data races on the other hand it
turns out that compiled
in fact sometimes introduced code that
adds data erases which is is rather
unfortunate there's one sort of filling
well-known case of when this happens and
that's when updated that it's when
you're updating a small struct field and
I'll have some detail I'll give you a
detailed example of that it turns out
this can also happen in more complicated
cases if if clever optimizations result
in speculative stalls basically if they
result in rights back to locations that
weren't written in the source the
compiler will have to be careful that
what it's writing back is what it was
what it thought was there before on the
other hand in a multi-threaded
application that's it's not safe to do
that necessarily because by just writing
back what was there before you may be
hiding an update in a different thread
very occasionally this actually does
happen and the annoying thing is that if
you look at the current standards it's
not at all clear that this is disallowed
at least so for the Select field update
example which is probably the one that's
been the most people in practice assume
we have considered well consider the
structure up there which is sort of a
particularly tricky case which is in
fact by the the current working paper
miss compiled by nearly every compiler
so what happens is I have a bunch of bit
fields in between two character fields
in a 32-bit struct and I assigned to one
of the bit fields and one of the
character fields simultaneously the way
that's normally compiled these days is
say on it particularly on 32-bit
machines but probably also on 64-bit
machines is the is I copy this lecture
as a whole into a temporary I implement
the bit field update by updating the the
bit field in the temporary value and
then stalling the the result back to the
whole structure on the other hand the
character field update and the other set
is implemented simply as a byte store so
that's fine and on controversial it's
the bit field updated and third one
that's controversial and as I suspect
you'll most of you already realize this
isn't going to work so we can actually
step through this here quickly so if we
look at how this is actually this can
actually be executed
we copy X into a temporary we update the
C bit field in the temporary now we
assign to the D field in the other
thread we copy the temporary back to X
and we've just overwritten the update to
D so what happens here as a result I've
lost the the effect of thread two so as
I already said this behavior is actually
something that's both allowed and fairly
common the if you look at current
standards in fact it's worse than that
this is what implementations typically
do implementations typically run into
this problem with bit fields the
standard actually allows this to happen
even if I'm just lighting two feels a
and D in this structure yeah yeah
suppose thread one wrote to erase of
zero yeah
wrote to erase up 22 yeah no one would
expect those to be atomic because X is
not atomic but their rights to 0 and 22
would be so if you don't expect that
when an X is an array why would anyone
ever expect it to be true in this case
Oh
archaic I the the question if I can
summarize it correctly is why would you
expect this to work because X is not
declared to be atomic expect that to
work for the same reason I would never
expect the array case to work okay in
fact I'm not sure I quite understood the
array example you asking about
simultaneous updates to two array
elements
to talk it's just that yeah oh okay no
no this is this is a good question I
think it's good to be clear about this
the in fact if you look at a lot of
existing code it makes assumptions along
these lines usually it's in the form of
I have a structure and it has say these
fields in it and I would like to protect
C and D by different locks in some
applications that in some applet will
actually let me postpone that and what
we do is actually some way in the middle
which i think is is probably added but
hopefully you'll agree is a better
solution but okay so here's a quick
summary of what we actually say about
all of this in the current working paper
so subject to the no data arrays this
rule and you know I in response to that
question I should be clear that the no
data erases rule applies at the level of
scalar values so you may not
simultaneously access to scalar values
at the same time from the different feds
so in particular this that does not
automatically prevent simultaneous
simultaneous access from two threads two
distinct field
what distinctive a elements so subject
to that will each update effects a
memory location which the standard
defines is either a scalar value or
continuous sequence of bit fields the
standard then defines exactly which
assignments can be seen by each
reference to a memory location which is
essentially what you need in order to
define the semantics of shared memory
here for ordinary accesses there has to
be exactly one assignment to a
particular memory location that's
visible to a particular access to a
particular load the way this is
structured is if we look back at the
preceding example of reference to X dot
D after completion of both of those feds
in the last example in fact is
quiet to see a value of 1 and because
those two threats access distinct memory
locations they in fact are not defined
to be involved in a data race so as a
result the proceeding implementation of
bit field assignments as I outlined it
there is incorrect so we no longer allow
that implementation 32-bit 32-bit values
on 8-bit processors in fact you work out
generally ok because of the fact that we
don't allow aces so that that's in fact
not an issue we don't there's an issue
in the implementation of Atomics on such
processes for non Atomics because we
don't allow simultaneous access without
some sort of synchronization between
them you can't tell whether a particular
32-bit store is implemented in four
pieces on one so as a result of this if
we the preceding implementation is not
OK on the other hand assignments to the
the bit fields in the middle in fact may
not occur concurrently those those do
form a data race if I have one pair
assigning two bit field xib and one to X
dot C and hopefully that answers the
last question so that's sort of a
compromise solution you can still use
bit fields but they have weakest
semantics than you might otherwise
expect but it only affects bit fields it
does not affect adjacent character feels
this rule actually it turns out has some
other more subtle consequences which are
of interest primarily to compiler ideas
I mentioned earlier this notion of
speculative stalls introduced by
compilers so here's an example of the
kind of optimization that abstractly you
would kind of like to see in your
compiler on the other hand it's not one
that we allow anymore
so this is a simple loop that traverses
a linked list
and counts the number of positive
elements in the linked list it's fairly
common to transform this to a loop that
initially actually I should say I should
emphasize the account in this particular
case is a global variable that might be
shared between threads it's fairly
common in current compilers to optimize
this to code that loads the initial
value of counted or register before the
beginning of the loop then traverses the
link then executes the the loop that
traverses the linked list just
incrementing the register value and then
storing the register value back to the
global at the end this transformation is
not technically safe with more than one
thread because if for some reason I know
that I'm invoking this on a list with no
positive elements I know that the
original code never touches count so
there is in fact no data arrays involved
if I run this concurrently with another
thread that does update count on the
other hand the transformed version
always stores to count so I've
introduced the compiler potentially
introduces the data arrays by performing
this transformation there are all ways
to avoid this in this example so in this
particular example you can if you
perform the store only if only if the
register value is nonzero that will
generally take care of the problem sorry
actually you have to do this a little
differently you have to count in a local
variable you have to initialize a local
variable to zero and then if you changed
its value from zero then you store then
you add it at the end that'll work ok
so the consequences of this is we are
losing some useful optimizations on the
other hand it does give the program at a
much much simpler inconsistent story
than what we've had before it prevents
some really mysterious compiler
introduced bugs and it
outlaws primarily optimizations that we
can at least ly place by code that's as
far as we can tell is only slightly
slower so I don't think this is a good
you have a major impact on non code
performance
so that was sort of at the low level
here's another interesting problem
actually another one that Lawrence has
been spending a lot of time thinking
about this one is this one is unlike the
previous one it's is C++ specific so
what's happening here is fairly commonly
what I think it's fairly common to write
code which at some point during its
execution stops some demon helper third
so we have main thread running at some
point the the helper thread here is
started and at some point in the main
thread exits in a C++ program when the
main thread exit exits various these
static destructors distract us for
statically allocated variables get
invoked and go away so at some point say
there's a variable down here which is
used by this by a by a shared library
that's referenced from the demon thread
the shared variable that's used by this
library goes away because it's destroyed
now the problem is in this model the
demon thread is still running there's
nothing to prevent the demon thread from
accidentally waking up sometime after
that and trying to call into that shared
library now accessing a very one of the
static variables of the library that has
already been destroyed at that point I
as as far as I can tell this is a fairly
common scenario in multi-threaded C++
programs which I suspect hasn't
generally been identified as a major
problem because pasta sees only cache
when they're about to shut down anyway
and nobody notices that that much but
nonetheless it's not a good state of
affairs for this particular one actually
we don't yet have a what I would
consider to be a complete solution the
current state of affairs in the working
paper is we either you basically have
two choices
you can either shut down all threads
properly before passes exit that's not
any necessarily an easy thing to do
because some of those threads may be
waiting on i/o and you don't have a very
good mechanism for getting them to break
out of the aisle wait oh the other
alternative is and this is Lawrence's
invention there's a way to exit the
process by only exit executing certain
special cleanup actions and not
destructive not all these factors at
which point you sort of get a semi
graceful exit which is guaranteed not to
crash however on the other hand some of
the things that you expect it to be
cleaned up might not be cleaned up okay
another problem here dead library's part
of the story in making all of this work
is that in order to prevent races and
programs we need to have synchronization
operations like like lock and unlock in
this slide that ensure mutual exclusion
and so on and we would like them to to
prevent memory in the orderings with
respect to those synchronization
operations so if we look at this
particular example here this basically
just incrementing X inside it inside a
critical section we suddenly want to
make sure that the implementation of
these synchronization primitives is such
that we can't just take say the load of
X into the the temp variable and somehow
reorder it out of the critical section
so we want to prevent this reordering
from happening either by the compiler or
by the hardware there are a couple of
mechanisms that are normally used for
this we normally prevent this reordering
by the compiler by neglecting to inform
the compiler what lock and unlock all at
all so they look like a Paik functions
to the compiler and therefore it looks
like we compile to the compiler like
lock and unlock mytouch x and therefore
you can't replace you can't do this sort
of movement of the memory operation
because that looks at the compiler
already like it might be incorrect on
the other hand the other part of this
has to be that we have to prevent the
hardware from doing the equivalent of
that
we normally do that by putting fencin
selections into the implementation of
lock and unlock so clearly this is
disallowed those fence instructions are
fairly expect if le and very expensive
so we need to be careful that we
understand exactly where they need it on
the other hand so far actually it's been
fairly unclear what kind of auditing is
actually what kind of reordering is
actually allowed and therefore what kind
of fence instructions we are we allow so
if we look at Java for example it's
fairly clear we're not allowed to move
things out of the critical section as
one might expect we are allowed to move
things into the critical section in both
directions if we look at the P thread
standard and read it very naively
basically what it seems to say is this
that we can't move things either way
these things just prevent reordering and
cost them if we stare at that really
hard and think about what it really
means you can prove that you can do this
and the result is not observable it
actually turns out you can't do that you
can't move things in past the lock and
the reason is this example and as I said
don't ever write code like this so the
the idea here is that I set some
variable x equals 42 and then say say
I'm done with accessing variable X by
acquiring a lock which is sort of the
opposite of the way things are supposed
to work I then go ahead and wait until
the lock is set which I can do by
calling tie lock repeatedly and then I
claim that at this point here it should
be the case that X is 42 the problem is
this should work this is expected to
work in some sense at least sufficiently
won't mind or something but it it breaks
if you if I order the assignment of X
equals 42 and the lock acquisition so
that means that in this particular case
it's end
I can't move the assignment of x equals
42 into the critical section past the
lock so what do actual implementations
do well we looked at some open source
implementations a while ago this is I I
think fortunately this isn't also
probably improved so a bunch of
implementation on weekly audit Hardware
NPTEL locks tended to in fact assume the
weaker implementation here so they in
fact tended to allow movement of things
into critical sections in both
directions but not but not out which in
practice is probably okay but
technically incorrect some of them I
think often just because the the
hardware sort of gates longer guarantees
anyway implement this where nothing
moves either way in some cases that's
probably - that's stronger than required
I think on Itanium this is probably too
slow some of them did that I think and
we found one interesting case that did
that which you don't want to use I think
that's what fixed okay C++ zero access
solution to this is slightly weird so
movement into critical section is
allowed in both directions so how do we
deal with that funny example that we
didn't want anybody to write well we
tell you that tie lock may in fact fails
furiously which turns out it makes the
example that I had up there clearly
incorrect so it still behaves I no
longer have a memory consistency problem
I just have a long I just have a long
piece of code which actually was the
goal here I claim because the this is
this as I said this is not code that you
should be writing anyway and I know of
no cases like been told that there are
some really obscure cases where this
actually interferes with legitimate code
all uses of tie lock that I've ever made
continue to work under this
interpretation
so together with all the other rules the
important point from this slide here is
that by by using the slightly weird
definition we basically allow the
standard to guarantee that memory
operation reordering is invisible is
completely invisible for the programmer
to the programmer for data arrays three
programs so even if you use tie lock or
whatever you don't have to in the bottom
line is you don't have to worry about
all this reordering stuff that we've
been that we've been talking about here
that's not quite true because in fact
there are some low-level library
facilities that actually allow you to
write a really fast hand-tuned code that
violates this rule however that's really
clearly identified when you're doing
that and so long as you don't do that
you get that guarantee so without doing
that basically data arrays free programs
behave as though the steps are just in
two of the individual threads are just
interleaved you have a couple more quick
slides here libraries in general that
don't steal specifically with feds I
think there's also been some some
missteps here the general wisdom about
locks and for example general-purpose
container libraries I think it's safe to
say is that if you're accessing a
general-purpose container library and
multi-threaded code the client knows
whether or not that container is being
shared between threads so the locking
should occur in the client this doesn't
apply to things that specifically are
designed for threads use they're there
the walls may be different if you don't
do that you get weird performance
problems and possibly even deadlock and
there's a lot of experience to
substantiate that at this point I think
on the other hand if we if you look at
what we actually have it's sort of mixed
in part because people were learning at
the time so the original Java
collections like vector are synchronized
implicitly and violate this wall and on
the POSIX side in fact we have this
problem as well there's this interesting
phenomenon that if you write a simple
file copy program using using put see
and get see in POSIX you try this on
your favorite Linux machine
if you fork it empty said at the
beginning of this copy program which
immediately returns and does nothing it
slows down by about a factor of 10
that's because POSIX requires that put C
and get C implicitly acquire and release
a lock so they they're fed safe in that
very strong and usually unnecessarily
sense and it turns out that behavior is
triggered once on Linux at least once
you fought the initial thread so the C++
CRX solution here is that we basically
follow what sort of the STL convention
in the recent java convention which is
that containers are not saying do not
acquire do not visibly acquire locks by
default the details here still have to
be determined we're not really really
done with this part yet so where does
this leave us
well I mean the one thing that this has
taught me at least is that if we start
out with a single threaded language and
Advent as an afterthought that in
general doesn't work all that well it's
I think we really do need to get the
remaining problems fixed as that we're
introduced as a result of this it's
quite likely that there actually is a
better general-purpose parallel
programming model at least for certain
applications I'm not sure that there's
anyone that's quite as general in on the
other hand I think it's useful to be
able to fix sort of the threads and
locks programming model underneath if
for no other reason then that we can
actually get a reasonable evaluation of
the alternatives as a result and if you
look at these alternative programming
models what's generally underneath in
the implementation is threads and locks
anyway so we better get that right okay
thank you
any questions
I know Java there's a concurrent hash
map which basically tries to put things
into separate hash maps in order to
improve performance when it's being
accessed concurrently where does that
fit into this idea that the the client
does all the locking because it doesn't
have visibility into the internal
implementation of the data structure
there I was trying to be fairly careful
to to weasel out of this ban issue the
there are clearly some data structures
that we'll need to know about pet
specifically and that's an example of
that that are specifically designed for
multi-threaded access so in that
particular case that's not a data
structure you should ever use if you
only have single threaded access so the
idea is the interface basically is
designed for concurrent access and as a
result the implementation can take
advantage of that that certainly makes
sense I'm all in favor of providing that
sort of facility in addition to the the
basic containers earlier in the talk you
said certain kinds of race conditions
are common but failures from them are
fortunately rare and I was all surprised
to hear you use the word fortunately
I've been writing concurrency bugs for
several decades now and I'm always happy
when they show up early and sad when
they show up late I actually agree with
you I was not being entirely serious
that way I wonder if there are any
facilities contemplated planned or
recommended for improving the things
that you didn't want to do but did
anyway like you know when I write those
concurrency bugs for the hundred and
first time to help help me find them
sooner are their language features or
process or design features or
programming practices to help and we've
been thinking about this a little bit I
mean I think it's clear that in the
eventual story he has to involve a lot
of help from tools and some of those are
out there we've where ever we had a
choice in some sense we've tried to make
things a little bit easier for the tools
so the the sort of absolute prohibition
against data races here in some sense
makes things a little bit easier for
data
action tools because if it can detect
that you actually have a race you have a
bug there's no fault no notion of a
false positive in that sense but I think
that's mostly an orthogonal issue it's
an important issue but I don't think we
have any great answers there yet very
early on one of your first slide said
that some of the objections the threads
were that they're dangerous and I forget
you know threads are evil was the thing
we put up there
you haven't been particularly
considering the previous question you
haven't done anything to convince me of
otherwise yet I still don't see any
reason to trust anyone in this room
particularly including myself to write
threads correctly I mean not implement
them but to use them correctly
is there any is there anything going on
in the C++ standards to help address
that to help us try to trust more people
to use these incredibly dangerous tools
I mean I think there's a consensus that
there is a serious problem there and
that probably most programmers should be
using higher-level libraries that are
built on top of this so the the problem
at the moment as far as in C++ committee
is concerned is primarily one of timing
and that's that we were under a
relatively short deadline and we have to
get sort of we have hopefully enough
time to get the Foundation's right we
unfortunately don't currently have
enough time to get those things in but
everybody realizes that those are
important as well I just want to address
that last question too it is true that
threads are still hard to use and evil
but it if you don't know what your
threads mean then it is completely and
utterly impossible to write correct code
not just really hard so this is a this
is a really important effort to actually
define what what it means what a thread
actually executes Thanks
any other questions Oh which libraries
are you or would you like to see
implemented but I obviously don't have
time to implement there we haven't
invested as much effort into it as I
would like sort of the the kinds of
things that have been under the
discussion I probably only only sort of
one level up and maybe not not high
enough
I mean we've been looking at thread
pools and sort of threads building
blocks kind of kinds of approaches so
intel has proposed the the threading
building blocks library to the Standards
Committee and that was sort of pushed
back for timing reasons I I think that's
probably not high enough I mean there
are lots of lots of small things that
need to be added as well I'm there's
there's some interesting things we keep
stumbling over just like getting a
reference counting right is really hard
I didn't yell I hop them optimal is
really hard and that's certainly
something that should be provided in a
library and there are there a bunch of
things like that
so my question is about shared variables
and can you make them more safe by
disallowing things like address taken on
that so that it is easy for analysis to
catch bugs we haven't and we haven't
really been looking at disallowing
things like that I mean suddenly you can
you're right I mean you can make the
analysis easier by not using those
features I haven't told much of an
argument to actually disallow them in
the language I think it would be
problematic to do that at this stage but
definitely make analysis much more
visible and once on on shared variables
anything in go wrong yeah hard to track
them around on the other hand I mean you
two'll can suddenly detect violations of
that wool and there's nothing to prevent
you from from warning of violations of
that at all
so you're free to do that in any way in
some case in some sense so I'm I I would
be worried about trying to convince
people to actually really slick the
language in that way because it's it's
very hard to convince the committee to
outlaw something that's in fairly
widespread use in particularly if you
can work around it somehow it seems like
the Java world is spent an awful lot of
time putting things like concurrent
container classes and that sort of stuff
into their standard library do you ever
see that happening for the C++ standard
and if so how many years are we talking
is this you know five years ten years
it's actually conceivable that this may
happen shorter in shorter time period
I'm not sure I have a probably
Lawrence's sitting there's a there's
more experience at predicting how the
committee behaves but there is an effort
to to put out another library technical
report which is sort of non-binding
addendum to the standard on a fairly
short time scale in fact at some point
there was discussions with there were
discussions of possibly getting that out
earlier than the the actual next
standard because there are fewer
bureaucratic hurdles that has to go
through so it's
conceivable that it might happen in
there we even within the next two or
three years but I I'm not really good at
predicting these things historically the
compiler development process usually
involves something like write a compiler
convinced yourself it's close enough to
write send it out the door somebody
finds a bug sentence in a bug report
certainly one of the problems with
concurrency related bugs is they're very
often unpredictable and it's hard to put
your finger on the bug and I wonder if
you have recommendations about things
that compiler writers can do to help
verify that they're following the
concurrency related rules to reduce the
number of these very hard to pin down
errors that the compilers are
introducing I think that's that's an
interesting problem it's probably also
still a research topic unfortunately I
think some other people have been
working on on building infrastructures
for testing concurrent programs to try
to do either so to increase the cross
section of these these bugs but I don't
know of any good solutions I mean in
some sense in some cases we were trying
to sort of minimize the number of
possible untestable bugs so I mean the
absolute prohibition against data erases
again is probably a good thing because
my feeling is that current compilers
make the assumption that they're no
asynchronous changes and those
assumptions sort of really and in a
fairly unknown context actually impact
the generated code so even though these
violating that assumption of what really
actually caused the sort of behavior
that I elevated within the switch
statement
I think testing for its absence would be
absolutely hopeless but I mean other
than that I don't have any any great
ideas it's still a hot problem I think
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>