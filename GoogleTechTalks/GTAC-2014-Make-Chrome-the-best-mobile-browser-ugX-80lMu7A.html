<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2014: Make Chrome the best mobile browser | Coder Coacher - Coaching Coders</title><meta content="GTAC 2014: Make Chrome the best mobile browser - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2014: Make Chrome the best mobile browser</b></h2><h5 class="post__date">2014-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ugX-80lMu7A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Our next speaker is Karin.
Karin has been at Google for over six years
and is currently focused on creating cutting-edge
test infrastructure for Chrome on mobile devices.
Take it away, Karin.
&amp;gt;&amp;gt;Karin Lundberg: Thank you.
Well, and everything works.
So firstly, I'm the automation test lead for
Chrome mobile.
I have been in Chrome mobile for about three
years.
Before that I spent time a lot of time on
other products at Google.
When we talk about Chrome mobile I want to
make sure you guys know what we mean by Chrome
mobile.
We mean Chrome on various Android devices,
tablet, phones, whatever is out there for
Android, and we also mean Chrome on iOS, both
tablets and iPhones.
There's kind of a weird echo here.
So those are kind of the things.
Android, it can be lots of different devices.
Also might work on nontypical devices.
So Chrome has been built around these core
principles.
The four S's we call them.
Speed, stability, simplicity, security.
They're kind of self-explanatory.
Speed, of course you want a browser to be
fast.
Stability, we don't want a browser to crash.
Really annoying when you're actually just
trying to load some pages and do some browsing.
Simplicity, most people don't really care
about browsers.
They just want to see Web pages.
So we want to make sure the browser is very
simple to use so people can do what they want
to do with the browser.
Security, obvious again.
We want to make sure that people trust their
browser, that they trust using anything like
online banking, buying stuff online, that
kind of stuff.
So these -- Chrome was built around these
core principles, but we also use the some
of the same principles when we look at how
do we test Chrome.
Both how can we test Chrome and how can we
make our test fast, stable, simple to write.
Security is one that kind of doesn't fall
into that.
In case you don't know, most of Chrome is
open source, so we don't really care about
the test resource anything being secure because
we share it with the world.
So I'll talk about each of these in turn and
we'll start with speed.
So again, we want to make Chrome the fastest
mobile browser.
We want a variety of tests.
We really want to look at tests like smoothness,
making sure when you load a page, everything
is fast, looking at jankiness, frames per
second, all the useful stuff.
We're also looking at okay, I'm starting up
Chrome.
We want that to be fast as well.
Whether you start up Chrome from inside Gmail
or whatever it is you're looking at, Facebook,
whatever it is, Chrome should start fast.
And no matter whether you have Chrome started
a second ago or an hour ago, it should all
start fast.
So we also test those kinds of things, and
we test various things that is really, okay,
how fast is Chrome, and we want to make sure
we're fast.
The kind of tests we run is a mixture of public
benchmarks like Octane, Dromaeo.
There's a lot of other ones out there, and
very Chrome specific tests.
We typically look more at regression of form
more than anything else he so even though
we do benchmarks we don't do a lot of comparison
with other browsers.
We want to make sure we're fast.
Doesn't really care what other browsers are,
as long as we are fast.
That's the kind of things we care about.
The Chrome specific tests, those can be anything.
They can be make sure you can load a new tab
fast, any kind of things.
And run all performance tests continuously
just as we do functional tests.
They run the same way.
No difference.
The one thing with the performance test is
we actually upload all the results to what
we call the perf dashboard.
I'll show you a picture of that a little later
and this is pretty new.
When I started three years ago we didn't have
this dashboard but this is amazing because
we used to have -- we have these things called
perf sheriffs, those responsible for reviewing
all the perf regressions, and they have to
look at a lot of different graphs, especially
for platforms.
We have a lot of platforms we run on, so it's
really a lot of pain.
With this new dashboard, it's not new anymore
but it was new compared to when I started,
we have automatic detection of regressions
and improvements.
And we base on that to use automatic alerts
so it's much easier to actually triage these
things.
And this is was an example.
I was just looking at the dashboard when preparing
this talk.
I was just looking at some of the alerts.
This is probably one of the worst alerts I've
ever seen.
22,000% regression.
That's not normal.
In this case, you'll see that we have a lot
of things, a lot of -- somewhere around zero
and then one of the benchmarks goes up, one
of the measurements, the metrics goes up to
400.
So in this case, looking at the graph would
have also been a lot easier.
We could have seen that but we have also a
lot of graphs, a lot of platforms.
There's also a lot on this page which happens
to be fine but 10%, that's more than usually.
In some cases the regression is very subtle
but the tool is still able to do it and able
to provide these results within triage, which
make the whole process of dealing with these
regressions much easier.
So how do we actually run these tests?
Well, we have one performance test framework
that we use across almost all platforms called
telemetry.
It can perform any arbitrary actions on a
set of Web pages and then they can report
these metrics to upload to these dashboards.
The beauty of telemetry is it's a Python framework
that works on all platforms: Windows, Linux,
Mac, Chrome OS, Android.
Not yet on iOS.
We are working on that, and I'll explain why
iOS is a little bit different.
It uses Chrome DevTools to control the browser.
Again, iOS is a little different and it allows
us to (indiscernible) any kind of regular
stuff.
This was actually implemented by a mixture
of our GPU and speed team so we have team
(indiscernible)) developed by some of our
developers.
One of the things that telemetry does, we
heard a little bit earlier about hermetic
test and dealing with the network.
We don't really want to do those kind of things.
We don't want to do any kind of network performance
test either.
If you do any kind of network communication,
that can affect the result, can introduce
some flakiness and this introduces noise.
So we use Web page replay instead.
Web page replay allows us to basically run
the test and record the request and response.
Then we have that page set, and we have the
page set when we want to test continuously.
And that way there's nothing with any network
condition.
Specially important for Android because we've
seen a lot of issues as soon as we introduce
wi-fi, especially with our lab.
We get flakiness, we get a lot of delay that
way.
So the way we do it on Android specifically
is we have a Web page replay server running
on the host that the device attached to and
then we talk to the host using USB.
So no network.
And on Windows and Linux it's even easier
because you can actually run it on the same
machine.
So telemetry for Android.
What are the challenges we had?
Well, some of these tests are quite long running
and anything you run on devices tends to be
a bit slow.
So we really spent a lot of time on running
tests in parallel on multiple devices and
speeding things up.
But the thing we saw when we started doing
that was that even when we have, like, two
Nexus 4s, two Nexus 5s, same devices, same
model, supposedly, we run the same test on
them, we actually see slight variances.
So we actually see (indiscernible).
It doesn't make the (indiscernible) as useful.
But we actually now run the same test on the
same device, which then can be only problem
if the device actually turns out to go offline,
which also happens every once in a while.
We've also seen that because we do this continuous
testing, even though the devices are connected
to a host server with a USB or USB bridge,
they tend to once in a while run out of battery.
The worst thing that can happen is if devices
run out of battery, they go down, and somebody
has to go and turn it back on.
We have people who do that, and they get kind
of annoyed with it.
So we actually -- this is something we mainly
do for performance tests.
A lot of the other tests don't have the same
issue.
But for performance tests, we look at the
battery level, and then if it's below a certain
threshold, we wait until the device has been
(indiscernible) just to avoid this thing.
Devices still go offline and still becomes
unresponsive.
So we, again -- I'll talk a little bit more
about that later.
We do have people who still have to go every
once in a while and deal with that.
Telemetry for iOS.
That's a lot more challenging.
In general, iOS has its own challenges.
So on iOS, we cannot use the Chrome rendering
engine, so if any of you are familiar with
Blink, we cannot use that.
So we have to use the UIWebView that's provided
by WK WebView with iOS 8.
So there's no Chrome (indiscernible).
But the good news is, UIWebView and Safari
and everything else is based on Webkit.
So we still have the Webkit remote debugger.
It's not all the same APIs.
Not everything is there.
We made some changes at some point both for
telemetry and for actually Chrome driver,
so WebDriver, two of the DevTools that are
not available in Webkit.
It's also not quite as easy to actually connect
to the UIWebView as we do on Chrome.
We have various thing on Chrome that we hooked
in so you can actually connect always to Chrome
using DevTools.
So even if you're just developing a new Web
app if you have never tried DevTools, you
should definitely do that.
But there are some things in place that we
don't really want to.
Apple doesn't really want everybody to connect
to an UIWebView in every app.
I completely understand that.
It just means that it's sometimes -- they
have a restriction that you can only connect
to an app that build yourself.
There are some issues there.
We can easily use it, but there are some issues.
Kind of like we can't make necessarily everything
available to the public, because we actually
have teams outside of Chrome using telemetry
because it is a public tool.
Anyone can use it.
And anyone can connect, like Chrome on Android,
and use it that way.
So this was about how do we make Chrome fast.
How do we make our tests run as fast as possible.
Some of these things are actually general
things.
And Ankit talked about some of them a little
bit this morning.
This is a little more specific to mobile.
(Indiscernible.)
You should only write integration tests for
things that cannot be tested on unit tests.
Everyone kind of knows that.
Unfortunately, that's not the case for us.
We moved very, very fast.
We shipped the first releases of Chrome on
Android two and a half years ago, the first
beta.
The first two publics was the summer two years
ago.
So we moved very fast.
And, unfortunately, it also means that we
haven't moved as fast with the tooling, and
generally the mobile tooling is not as good.
So for Android we have (indiscernible), which
tends to be integration tests, which run on
the device.
That should be jUnit tests.
iOS, we use something called KIF.
We have a tendency to write KIF instead of
unit tests.
So that's kind of the thing that we are working
on trying making it faster.
Making better test framework.
For integration tests, something that we've
also seen that, again, might be obvious, you
should really only use the UI for the actual
test.
So if you're writing a UI test, don't set
up the test.
Set up the environment using UI.
Set up the environment in any other ways,
like ideally calling into the app.
We spend a lot of time on convincing everybody
that this is okay.
We really want the test to focus only on what
the test is focusing on.
Then we can have other tests to focus on the
setup pieces reusing the UI, but not in the
same test.
So for Android, a lot of stuff we really do
has to deal with devices.
We run everything on devices.
I'll explain why later.
And we have to make sure that that runs fast.
So the setup of the devices, the setup of
the test, and the running the test, everything
we try to do in parallel, which speeds things
up a little bit.
Well, in some cases, quite a bit.
So a lot of our tests have data dependencies.
So we try to write our test hermetic, as Ankit
mentioned, which means instead of connecting
to some HTTP server, we actually have the
local pages wherever we can.
So that's a lot of data.
And we really have to have that on the device.
It actually turns out that pushing data to
devices was a huge bottleneck for us.
We worked on that and kind of two times in
the last year and a half to improve that,
keep improving it, because it is big, and
we have, I think, one test about 200 megs
of data or something like that.
So it's a little insane at times.
Other things we're doing, this is, actually,
again pretty recently, we're trying to avoid
running tests on devices where we can.
So if we have any kind of test that's pure
unit test that just say it's, for example,
(indiscernible), that doesn't have to run
on the device.
We can test that on the desktop instead of
much faster.
We don't have to push anything to device.
So we're working on having more of those kind
of frameworks in place.
So iOS.
Yeah, iOS, again, is a little more challenging
for us.
We run tests on simulator where possible.
Unfortunately, we have seen that we can't
catch everything on simulator.
So we still have to run on devices.
But we have various system (indiscernible),
like, we want to run test before we submit
changes.
We try to do that on simulator.
And then once changes are submitted, we run
that on devices, and then we kind of deal
with flakiness or deal with issues of failures
at that point.
We're a little behind on iOS compared to Android
for a lot of things.
So the next thing I want to talk about is
simplicity.
So make the Chrome user experience as simple
as possible.
Some of these things are not really related
to automation, but it's kind of to provide
a big picture.
UX should be -- of course, I think everybody
knows that.
Make sure that the features are simple to
use, that they look good, that it's just a
good user experience.
And we also do manual exploratory testing
of things to make sure everything works the
way we (indiscernible).
But we also found that automated testing also
promotes simple UI in the sense that if you
have an automated test that's very difficult
to write, you're, like, why do I have to click
all these buttons all the time to get to this
thing?
Probably that's not a problem with your UI
framework or UI testing framework.
It might be an actual problem with the UI.
So we use that kind of thing to also say,
okay, is the UI simple?
If it's simple to write a test, good, the
UI's probably simple as well.
So this is a good point to talk about what
kind of automated functional testing do we
do.
So we'll start with Android again.
We have a large list of tests.
And sometimes I wish we didn't have as many.
They're calling different areas.
So for Android especially we see a lot of
code with Chrome and desktop.
And that code is written in C++.
So we are running a lot of the same C++ unit
tests across Android, Mac, Linux, Chrome OS
even.
There's also a few integration tests, something
we call processor test home grown that run
more integration tests also in C++.
We just recently introduced Robolectric to
do host site.
That's a test that runs on the desktops, the
out test, unit testing, we want to move to
much more unit testing.
Android integration test is really where we
have a lot of the Android-specific testing
done.
Generally, the integration test, they've been
very useful.
But we've seen lately, especially if anyone
has heard anything about Lollipop, that sometimes
we can't just use integration test, because
while we prefer to push the test down the
stack, so more unit tests, integration tests,
some -- there are some we need to test across
application (indiscernible) the whole thing
works.
So we do have a few UI automated tests that
are really those (indiscernible) automated
test for anything that integrated with all
applications.
So iOS.
We also -- we share some code with the rest
of Chrome, so we have some C++ unit tests
that we write.
We also have some very specific for iOS.
The majority of our tests are KIF tests.
It's an open source framework called keep
it functional.
We actually forked KIF before version 2, because
when they -- version 2 was a significant rewrite
of KIF, very different API.
And we had a lot of KIF 1 tests, and they
were actually working for us, so we created
a fork at that time.
This is integration testing again.
It's run inside the application.
But we only test the application on test.
But that's okay, because we don't have as
tight an integration on iOS with the platform,
although there is some integration with other
Google platform, but not like --
So how to make Chrome tests simple to write
and run.
So, again, some of these are general and some
are not new to Chrome, it's something we found
out, a lot of us, before we -- a long time
before I moved to Chrome as well.
One of the big things is that developers should
really write their tests and the features
in the same language.
So if you have developers writing the test
in one language, that -- the features in one
language and the test in a completely different
language, that's a disconnect.
There's a consequence.
And it makes it less simple to have the test.
They have to keep changing all the time.
As the testing team, we are the ones who introduce
a lot of new test frameworks.
Some come from developers, but (indiscernible),
especially for Android, a lot of the new ones.
We have to provide some interesting sample
tests.
We have to make it easy to get up to speed
with these frameworks.
Also with utilities.
Again, this is kind of tied into the next
one, make it easy to perform setup without
UI.
We try to provide a lot of good utilities
for people to get to state where they want
to test something.
That makes the test easy to write.
In Chrome, a lot of it is starting a new tab
and load a page.
We have utilities for that on all the platforms
that makes those kind of things easy.
I already talked about that one.
Also making it easier to run the test.
I'll talk a little bit specifically, especially
for Android, about that.
But developers, some developers, like (indiscernible)
just want to run it on a command line, want
to make sure that everybody can run their
test easily.
And we want to make sure that, okay, I've
written my test, hopefully.
I've written my code, written test for the
code.
I even turned off the code review, which we
do with everything we do, and I got , looks
good to me, who want to submit it.
So we have various things we can do with that
this help with this.
We can even submit our code to something we
called trybot.
I've written my test.
I think it's good.
I'm about to send it out for review, but I
want to make sure it passes on all platforms.
And I don't necessarily have access to all
platforms.
I personally don't have access to (indiscernible).
I sent out and tried.
It runs on all the platforms.
And then I know whether (indiscernible) I'll
break something.
So now I submitted.
I get code review, looks good to me, and I
submit for code review.
I want to submit the change.
So we extend the trybot with something we
call a commit queue, same test, same infrastructure,
but at the end, it says, everything passed.
We've got to submit your things for you.
This is a very old (indiscernible) I did a
long time ago, but it has these little commit
things.
So all I have to do is, I have to click that
check box.
That sends out the command.
Actually, (indiscernible) later (indiscernible).
Hopefully, it works.
How does this sort of go specifically to Android?
What are the things we looked at?
If we looked at the list of tests, the kind
of tests we run for Android, it's a very large
list of different test frameworks.
We have one unified test runner for all the
different types of tests.
It's not like I'm running a C++ unit test,
what do I do?
No, it's the same test runner for everything.
It determines which test to run based on the
type, the filter for some of our (indiscernible)
tests, we can say only run tests for this
feature, only run tests related to (indiscernible)
or whatever it is.
We can kind of determine those kind of things.
The test runners also handle anything with
the devices, any communications.
So if you're familiar with Android, anything
with ADB, anything with the sharding, it handles
those.
It also handles any kind of issues with those,
any kind of flakiness.
So I talked earlier about how developers should
write the tests in the same language as they
write the code.
So we actually -- Chrome, again, it's a big
part of C++.
Everything that was there until Android came
along with C++.
Android came along with starting to do Java,
and some of the developers that have been
working on Chrome for a long time were, what
the heck is this?
But out of (indiscernible) stability right
in both of them.
Specifically, develop the function of Chrome
on Android, focus on Chrome on Android in
both languages.
When they write new features, they will write
both C++ and Java.
A few times, they will only write Java that
is only Android-specific UI.
But that's (indiscernible).
So they will write unit tests (indiscernible)
C++ code.
They might write C++ integration tests, and
then they'll write Java integration tests
for the pieces they have in Java.
They stay in the same language, although they
have two languages they typically write in.
Except for UI Automator, all our test frameworks
can call application code directly.
That makes it a lot easier, again, instead
of using the UI to set up, you can call directly
and open a new tab, open a new page, whatever
it is you need to do.
iOS, slightly different, but some of the same
things.
We have a lot of common utilities.
Trying to set up the app, clear between tests,
which actually turn out to be a little bit
difficult for us.
But those kind of things, common access, we
have a lot of utilities for that.
When I started on Chrome about three years
ago, we hadn't even released Chrome on iOS
yet.
But we actually had a very good testing team
and we were doing life stuff to write tests.
At the time, using UI automation, I don't
know if anyone ever used it, but it was a
little bit weird for them in that they were
coding in Objective C, and then went to JavaScript
to write their tests.
Very different frameworks.
Very different ways of writing things.
And, again, I will say that I am actually
very proud of the team in how many tests they
had in UI A. But it was very difficult to
maintain, difficult to write, difficult to
do this content switch.
It was written at a time when tests were written
very late in the cycle, because you finished
everything in objective C and then went to
JavaScript.
So now we're using KIF, (indiscernible) objective
C. It's much more -- people write and generally
test much earlier, much happier with writing
tests.
Again, we run tests as part of the application
so you can call direct.
You don't have to do the UI.
So the next thing we want to talk about is
stability.
That's the next test.
Basically, you don't want to see any of the
things at the bottom.
We don't want to see a sad face.
You don't want to see the, oh, unfortunately,
Chrome has stopped working.
You just want to browse.
What do we do to make Chrome the most stable
browser?
Well, we try to catch crashes as early as
possible.
I'll talk about the specific stability tests
we have for that a little bit later.
But we also review what is happening out in
the wild.
One of the things with a browser is that people
view very different pages.
We can't always predict -- we definitely can't
test all the Web pages out there.
So we're seeing different crashes.
We're seeing crashes when we go to the public,
we call it our stable channel, that we've
never seen in our testing.
And it turns out to be -- often turns out
to be something with a specific setup, the
specific Web pages used.
But we're reviewing those crashes very, very
rigorously and looking at them and trying
to deal with them as fast as possible.
For Android, we're lucky that we have a public
beta channel and we have a (indiscernible)
amount of users on it so we can get those
kind of stability numbers very early before
it goes out to the larger public.
We do try one of the things we have about
six weeks in beta.
And one of the main things we do in beta -- I'll
talk about it a little bit more later -- is
look at the (indiscernible) numbers, make
sure we fix those before we go out to the
wider user base.
We also have our own crashes from our test
-- so we can use the same, actually, UI to
review them.
We can even compare, okay, these crashes we
found them in the public, found them outside
when we shipped, but we didn't see it in our
tests.
Why not?
And we can kind of look at that and compare
tests with the real data and see what's going
on, are our tests not as useful as we want
them to be?
So for Android, we have a number of stability
tests.
We have a test we call popular URL.
Basically, what it does is runs to a list
of popular URLs.
It also scrolls up and down to make sure.
It uses ChromeDriver.
The ChromeDriver are internal implementations
of WebDriver that works on Android and on
desktop.
But, sadly, we actually find a lot of crashes.
We haven't seen a lot of crashes lately.
But when we started doing this, we saw more.
We've stabilized them a lot.
We also run these on a mixture of tablets
and phones, Nexus, non-Nexus devices.
So we try to get coverage from a little more
than we actually normally do.
And I'll talk more about what devices we normally
run on.
On the same kind of setup, we run Android
monkey tests.
We ran 50,000 events.
We lately see more crashes on monkey.
But that's the nature of these random events
just stresses the browser.
Unfortunately, it is random events.
So reproducing these is also a little more
tricky.
We also have this relatively -- well, relatively
-- it's this -- we used about 11 months thing
called Clusterfuzz that we've also used.
I'll talk more about it later, because it's
actually part of our Security team that developed
this.
Developed for finding security issues.
But we also use it for finding crashes in
general.
So on iOS, there's no iOS monkey, at least
none that does what we want to do.
So we still have the popular URLs test, but
we combine it with some kind of random events.
So we run through a top list of URLs.
We also have some URLs that are known to crash
Chrome or the UIWebView.
And we run through those, especially when
the new iOS versions comes out, and hopefully,
that new WebView issues are fixed.
Scroll up and down and perform those random
events.
It's based on the same framework that we use
for functional testing, KIF, that way, it's
easy for anyone to fix, for anyone to run
and improve, hopefully.
We have to handle (indiscernible) about flakiness
already, even those early in this conference.
So how do we make our tests as stable as possible
and avoid flakiness?
Since this is mobile, we see a lot of general
flakiness from the devices, just dealing with
devices, a lot of flakiness with that.
And we also have seen some general flakiness
in our frameworks.
And those are really what we as a team work
on.
If there's a flaky test, we have this thing
we call Sheriff that is monitoring our continuous
builds.
And they will look at, okay, this test seems
to be flaky, they have various tools for that.
They'll disable it, create a bug, and developers
are supposed to fix them.
Hopefully, they do that.
If they don't, the test is basically dead.
So what are we doing specifically for Android
to eliminate the non-test flakiness?
Devices, everything is devices.
We reset the devices, we remove a lot of data
from the devices.
We restart them, we set them up a certain
way with certain properties for -- again,
I talked about that a little bit earlier,
for performance tests, low battery, we ignore
them, and we try to deal with devices going
offline both before and during tests.
So we have various places where we actually
test which devices are online and run tests
on that.
And we try to ignore when devices go offline.
We even -- our Infrastructure team set up
some things that are automatically notifying
our sys admins that will actually then go
and look at devices offline.
Normally, a reboot will help.
Sometimes we have to completely reinstall
or reset the devices.
We also have to deal with ADB flakiness.
Anyone who's run tests on Android devices
has probably run into issues with ADB.
So we do a lot of retries, a lot of resetting.
(indiscernible) to show that the phone restarted
a lot, run tests.
We do the same thing, lots of restarts.
That's also the reason our tests are kind
of slow at times.
iOS, already mentioned this a little bit.
Test on simulator is not only faster, but
also more stable.
But we (indiscernible).
We have seen issues that are only on devices.
We are trying to figure out a better way to
talk to the devices and kind of improving
some of this stuff.
And I suggest that if you're interested in
running any kind of testing devices, there's
a talk later today by some of the folks on
-- that we work very closely with on better
ways and more stable ways to work with devices.
We're working with them closely on how we
can improve this both for iOS and Android.
Security.
So (indiscernible) making Chrome the most
secure browser.
It's probably one of the most important things
you can do as a browser is to make sure that
you're secure, make sure people have confidence
in that you're secure.
So we have various things.
You probably -- if you're at all interested
in security, you probably heard about the
Chrome reward programs (indiscernible) and
all those kinds of things that are going on
to just kind of get people to report issues,
make sure -- so we can fix things as fast
as possible.
We really try to find any security issues
ourselves, same as other things.
You don't want them to go out to the public.
And we have this tool called Clusterfuzz that
was implemented by our amazing Security team.
It (indiscernible) it uses fuzzes to trigger
crashes and can automatically determine crash
can it potentially be a security issue, yes
or no.
And it's -- runs a variety of tests.
I think we have 40-plus fuzzes or something
like that.
I forget.
Some of them are actually close to popular
URLs.
Run through a list of URLs, see what happens.
Others go actually go in and change the DOM
or do (indiscernible) things with (indiscernible)
pages and see if that (indiscernible).
Any kind of things that could be security
issues related with that, go in and check.
If it crashes, this tool is so amazing, it
can actually determine more or less automatically
if this is a security crash or not.
It also goes, actually, in and this is one
of the things that's been most successful
for us compared to other stability tests,
is that it goes in and it tries to reproduce
a crash and tries to find the minimal test
case that can reproduce a crash to make it
as easy as possible for developers to actually
reproduce locally and fix things.
It can even go in and when things are fixed,
it will actually go in for the things that
are reproducible, try and run them and see
if the things are really fixed.
Of course, not all crashes are reproducible,
but still this is very useful.
In the time it's been working for Android,
we found much more crashes early, let alone
we also find security issues earlier.
So this is a big shoutout to our Security
team for this.
Unfortunately, we don't have support for iOS
yet.
That's what happens with a lot of our tools,
that iOS is kind of a little bit behind.
We do want to do that soon, because we really
want to see more of these crashes.
We also have a few functional tests for various
privacy security areas.
Chrome, we want to make sure incognito is
still incognito, those kind of things.
I actually had one more bonus section, scalability.
We're a browser.
We want to make sure that we work for all
Web pages, we don't crash, all that useful
stuff.
But we can't really test the entire Web.
So we depend a lot on users as testers.
Ankit was mentioning this with trusted tester.
Luckily, for Android, we have a beta channel,
but we have a large amount of users running
on this.
They get new features early.
We always ship new features to the beta channel
first.
You can look at the stability numbers, we
can actually get this data before it goes
out to the larger population.
We can fix things.
We also do experiments.
Some of these new features that we public
on the beta channel, if people opt in to send
metrics, we look at how people are using these
new features.
And sometimes we find that people either don't
really find the features, they don't use it,
they don't find it or find it useless.
We don't really know.
We know they're not using it.
In some cases, people will tell us.
That's also the good thing about having people
on the beta channel.
They're typically very vocal and will tell
us when things are not working.
So we had issues where we basically never
published that feature in that state to the
larger population on our stable channel because
the feature just wasn't working as we expected.
So the beta channel allows us to do those
things.
And it allows us especially with, again, testing
the entire Web.
We get more people using on more Web pages
doing things that we don't actually do in
our own testing.
And we also (indiscernible) be able to dynamically
enable features, which is -- actually was
pretty new, especially on iOS.
I remember when we implemented it.
It's been in Google3 for some of our Web pages
-- Web apps for a long time.
But it was pretty new when we implemented
it.
And it actually allows us to get some of this
data and do more experiments as well.
So scalable.
We use that a lot, actually, for our test
and build infrastructure.
So some of the challenges we are seeing on
Android, let's start with Android.
So Android we run, I think, 30,000 is a little
conservative.
But those are at least some of the ones rerun.
We actually run a little bit more than that.
And we always run on devices.
So we have a continuous build machines, we
call it build bots.
Each of those has between one and eight devices
attached.
We try to have more towards the eight than
the one.
When we only have one device attached, it's
mainly because it's running a very special
configuration.
But when we want -- when we have to do large
-- scale largely, like when people actually
-- they continuous build machines that people
run on when they're submitting new code, then
we have eight, and we shard the test, run
everything in parallel.
Things run faster that way.
But that's a lot of devices.
I don't have any numbers on the devices lately,
because -- I know how many we kind of got
to (indiscernible) the thing.
But it's a lot of devices.
Keeping them up is very tricky.
And when they go offline, people have to physically
go down and restart them.
We mainly run on -- well, we run on homogeneous
devices on each build bot, because each test,
we don't want to have a Nexus 4, Nexus 5,
or a Samsung S5 or something like that.
We run on the same configurations.
And then instead, we have different build
bot machines for different configurations.
We mainly run on Nexus devices with user builds,
user debug builds.
This has been historically what worked for
us, especially with older Android versions.
With newer Android versions, especially with
KitKat, our tests start to get a lot more
stable, because KitKat is starting to get
a lot more stable.
(Indiscernible.)
Everything matured, so it's gotten better.
So we (indiscernible) potentially, and we
are working with some other teams and looking
at not running only Nexus devices.
We have some non-Nexus devices.
We mainly use those for performance and stability
tests and a few things.
But mainly Nexus devices, because they are
the same kind of configuration.
You might ask, why are you running all these
tests on devices?
We tried an emulator.
I know some of my colleagues on Google3 have
had big success with emulator.
We never had that.
And we also are very hardware-intensive for
the rendering.
And those kind of things have been unstable.
A little bit flaky on the emulator, especially
if you're looking at running it on, like,
Compute Engine or something like that.
So it just hasn't really worked for us.
We are definitely investing alternatives.
We don't really want to run all these devices,
and again, we are working with the teams inside
of Google to find better alternatives, especially
working a lot with the team that used to be
Appurify, I don't know if you guys heard it,
it's called Appurify, we're working a lot
with them since they still have some stability
issues for running on devices as well.
For iOS, the challenges has been some cases
worse, some cases better.
The advantage of iOS is we don't have the
same explosion of Android devices, that we
have on Android devices, so there are fewer
devices, which makes it a lot easier.
But we have more issues with those specific
devices.
So we used to run the test first on an iPod,
then on an iPad.
And we saw this weird flakiness for our test.
And our test ran perfectly fine locally.
We didn't understand it.
So we pulled one off and we created one built
machine with iPods and one machine with iPads,
or iPhones and iPads, and things just started
getting better.
So something with the way we were communicating
with the devices when we had more than one
device attached just made things unstable.
And we haven't come up with a good way of
running multiple simulators so we don't have
the same kind of (indiscernible) running things
in parallel that we have on Android, so it's
lucky we don't have to scale to as many devices
because we don't really have the capability.
As for Android, we are working with various
teams to come up and improve that but we're
still struggling with some of those things.
And again, please help us make Chrome the
best mobile browser.
If you are doing Web applications, WebDriver.
ChromeDriver is our implementation of WebDriver.
By all means, please use it.
Please report if you are seeing any issues.
We are monitoring the various bug reports
and everything (indiscernible), we try to
make it better every time.
So please let us know if it doesn't work for
you.
And if you just in general have issues with
Chrome, we have a public bug repository we
call crbug, link to here.
Report bugs.
We really want to hear from you.
We really want to make Chrome the best browser,
so any issues you have, let us know.
Or if you just have request to more tests
we should run or more features, please let
us know.
And that is what you have.
And I also have the yellow button which means
I should take questions.
[ Applause ]
&amp;gt;&amp;gt;Sonal Shah: Thank you, Karin.
That's pretty amazing.
So since we have been taking some live questions
for the last two talks, why don't we take
one from the moderator link.
&amp;gt;&amp;gt;Alan Myrvold: So great talk.
From the moderator link, when you had the
identical devices that performed slightly
differently, did you ever investigate that
and figure out whether it was due to minor
hardware revisions, other running applications
or something else?
&amp;gt;&amp;gt;Karin Lundberg: As far as I remember, our
speed team did most of it.
As far as I remember, it was typically slight
hardware variation.
Like if you look at a Nexus 4, that was -- the
first Nexus 4 coming out versus the last Nexus
4 coming out, slight hardware configurations
was really (indiscernible) the problem.
Or slight thing, like the hardware is slightly
different, I mean it's tested (indiscernible)
spec, but the spec typically have some variances
in the spec that you allow to be inside something.
So those little things just accumulate and
you see some noises.
&amp;gt;&amp;gt;Sonal Shah: One more?
&amp;gt;&amp;gt;Alan Myrvold: So how does the fuzzer investigate
crashes?
Looking at the stack sometimes doesn't help
reproduce it.
How do you reproduce crashes found by the
fuzzer?
&amp;gt;&amp;gt;Karin Lundberg: So the fuzzer will basically
take whatever test case it was.
So a lot of our test cases are HTML pages.
So take the HTML page and it will actually
look at it.
It will actually try to limit what's in the
HTML page.
So some of the HTML pages have CSS, JavaScript.
So actually try to remove some of those pieces
and come down to the minimal set -- the minimal
page to actually produce it.
&amp;gt;&amp;gt;&amp;gt; Regarding the stability issues when multiple
devices connected, I face the same problem
with Android devices as well.
I work on a native application where I cannot
work on a simulator because it needs Google
push to be enabled, and from KitKat, I believe
you cannot add a Google account to that, so
Google post service cannot be in a build.
With this multiple three -- I generally run
on three devices which are interdependent
on each other to make the tests, tests running,
so once one device goes offline, I tried multiple
ways, like even rebooting or restarting the
port, but once it goes offline, it sometimes
comes up and sometimes doesn't.
&amp;gt;&amp;gt;Karin Lundberg: Once a device goes offline
you're more or less screwed, to be honest
because once off line it's very difficult
to get back up without manual intervention.
That's what we've seen.
And we have this system admins that keeps
our lab running in general, and I don't think
they were very happy when we added Android
devices to the lab because all of a sudden
they spent a lot of time on that.
We do a lot of trying to restart before this
happen and using ADB and restarting ADB with
other device.
But the people you should really be talking
to, I suggest you talk later today when Jay
and I don't know if Manish -- he's probably
not here, but at least Jay I know is here,
is going to do a talk about dealing with devices.
They've solved a lot of these issues and they
solved much better than we did.
So we're doing a lot of retries in these things
but we still have a lot of devices going offline.
These guys have barely no devices go offline.
So they do some magic.
They're smarter than us, I guess.
&amp;gt;&amp;gt;&amp;gt; I'll just get the names of people to talk
to.
&amp;gt;&amp;gt;Sonal Shah: We have time for one last question.
&amp;gt;&amp;gt;&amp;gt; I was wondering, how do you automate performance
testing of the network stack, both in general
and in realistic scenarios?
&amp;gt;&amp;gt;Karin Lundberg: So the good thing is that
even though we are doing this where we have
the Web page replaced, we actually can still
trigger the network stack in these kinds of
things.
So we still use telemetry to do network stacks.
We just don't talk up using the Wi-Fi to trigger
the network stack.
so we still -- even talking to the host actually
triggers the network stack, and that way we
can still test the network stack with that.
So we have a team dedicated to network stack
completely, and I know they've done a lot
of telemetry tests as well.
&amp;gt;&amp;gt;&amp;gt; But for Wi-Fi and GPS, you don't do any
field testing?
&amp;gt;&amp;gt;Karin Lundberg: No, we don't do any testing
for Wi-Fi.
It's something we always wanted to do but
we just can't get the thing stable enough
and we have a big issue with -- in our lab,
we had to put up more access points to even
have Wi-Fi doing.
We have no devices on 3G at all, which is
sad.
And we're trying to -- again, we're working
very closely with Jay and the others that
will talk later today to deal with those kind
of things.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>