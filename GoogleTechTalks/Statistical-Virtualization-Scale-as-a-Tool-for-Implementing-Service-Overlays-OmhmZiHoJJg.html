<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Statistical Virtualization: Scale as a Tool for Implementing Service Overlays | Coder Coacher - Coaching Coders</title><meta content="Statistical Virtualization: Scale as a Tool for Implementing Service Overlays - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Statistical Virtualization: Scale as a Tool for Implementing Service Overlays</b></h2><h5 class="post__date">2008-01-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OmhmZiHoJJg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so our guests we are ready to start soon
so I helped was returned to use huge
vault key he's a professor at Santa
Barbara in UCSB and he'll be talking
about sadistically to the virtualization
scale as a tool for implementing service
all the overlays when the rich both kids
I think a systems guy with a way to be
touch of scientific computing and he
used to work in a various places right
you work at Louis Livermore and you I
guess you met a lot to go with their and
then University of Tennessee San Diego a
programming center and UCSD and then we
had a pleasure to get him at Santa
Barbara so he's there for a couple years
sorry thank you sorry thank you very
much and thank you all very much for
coming to listen to me speak to you
today i wanted to talk a little bit
about or i'm prepared to talk a little
bit about some work that's been going on
in my group that has really been focused
on the scientific computing community
which ordinarily doesn't evoke a sense
of familiarity when you think about sort
of large scale computing in the
commercial sector but which i think at
the end i hope i can show you really has
a lot to do with what it is that i think
that you guys do on a daily basis and
that's kind of a surprise for us for hpc
people who are used to working with
tightly coupled parallel machines the
idea that large-scale distributed
computing has something really important
to say in terms of performance of these
scientific codes is kind of novel so
that's my goal today okay so um this is
a slide that that I have blatantly
ripped off from San Diego supercomputer
center in it and and it's very colorful
and and really all it says is that in
addition to the fabulous work that goes
on here American competitiveness depends
in large part on the ability to do very
large scale computations for science and
engineering right and and and you make
you can think of that in terms of like
parallel computing where you have a
single machine
that is capable of very fast
communication and large scale computing
but increasingly we're starting to think
about it in terms of national scale
computing and there's a very very
unfortunate term that the National
Science Foundation is coined called
cyber infrastructure I think it's
unfortunate on because it's not really
evocative of what's going on but but if
you hear that term that's what it's
really going on is the National Science
Foundation is interested in trying to
figure out how to use national scale
computational infrastructure to forward
science and engineering at the moment in
terms of scale and and and topography if
you will we're talking about something
like thousands to tens of thousands of
processors but those processors are
often located in clusters or very
tightly coupled machines and a variety
of networks high bandwidth networks
within machines sometimes high bandwidth
networks machine machine between
machines but a low very high latency low
latency within the interconnect
paralleled this systems clusters which
are commodity machines that are linked
together with commodity networking
typically you are popular and they're
very very common in research settings
but but the computation to communication
ratio which is really a critical factor
in doing a large-scale high-performance
computing is often insufficient to
support some of these scientific domains
and and so for that reason you tend to
buy or there are a certain population of
these large-scale machines costing from
between 50 and 150 million dollars at
the moment and they're you know between
two and ten million dollars to run right
so if you buy one of these things and
you put it in your machine room and you
pay for the power and the lights and the
security and so on and so forth you're
looking at that kind of an expenditure
and and really hard scientists chemists
asst chemists physicists biologists it's
other than computer scientists don't
have the expertise or the interest or
the resources to run these machines
right these become scientific
instruments that they want to use but
they don't want to maintain and so in a
science setting we think of that as the
mainframe model right which is there's
one machine an instrument and it's
expensive and it's going to be shared
between a large number of users and each
user a scientist is given an allocation
some right to occupy the instrument for
a while and from an economic perspective
if you're trying to do something like
account for that especially in a
scientific context what that means is
that utilization is your measure of
utility you want the resource to be as
utilized as possible downtime is bad but
then you also look to see where
utilization is missing and often will in
my estimation mistakenly interpret that
missing utilization for a lack of
utility and this as I will talk about
later is a problem I believe another
feature of the mainframe model is that
the administrators of the resource make
policy and that's because there is a
large cadre of users who have you know
diverse needs and and those needs must
be balanced they have often conflicting
concerns and you really want to make
sure that the resources heavily utilized
so so this is not a very egalitarian
approach and it's also an old approach
right i mean basically has been around
since the mid 60's it's kind of shocking
that that's where we are okay but we are
making progress and and with a very very
good networks that are available
nationally at the moment um you can
think about aggregating these resources
to do things that are larger or faster
than you can do in any one spot but it's
it's a little tricky it's not a
commodities game right I mean Google
does this a lot there are lots of lots
of machines that you have in there are
lots of places and and you manage them
dynamically behind the scenes but in
some sense they're commoditized and for
this kind of work you can't really
kemana ties things because the
computation the communication ratio
drives the way that the program is
written and writing multiple copies are
dynamically reconfigurable copies of the
programs is very difficult especially
for chemists so so it's not a
commodities gain and while you can buy
lots and lots of bandwidth you
can't buy less and less latency so at
some point your program becomes so
tightly coupled that no matter how much
bandwidth you're willing to put in one
spot if the latency is not low enough it
will run inefficiently right and a third
feature of this is that legacy support
is critical uh these codes which are
often you know hundreds of thousands of
lines are written over the course of
many decades and the intellectual
impetus for that has graduated and moved
on many times over so so you can't
rewrite right and and and there many man
years of effort that sometimes go into
these codes and they're verifying that
the science and the codes is correct or
whatever so you're stuck with things
that have a very long life cycle that
have to run in this environment so it's
it's a challenging environment uh and
and and the approach that's been tried
and I'm going to try to argue that this
is the wrong approach other we've
discovered at least that it may be
partially wrong but the approach has
been tried as we call middleware and the
idea here is to say okay well these are
these machines has some sort of local
software environment we're going to put
a veneer that everybody is going to
install at the correct red level in a
way that's compatible over the top of it
and that users can embed their legacies
within a workflow sort of a higher level
program that stitches together these
these components okay so that's kind of
we're hoping to go drilling down a
little bit though each of these machines
is space shared all right and and what
that means is that like scientific
instruments users queue up to get a
piece of the instrument dedicated to
them okay but for various reasons we'll
talk about this queue is not managed
first come first serve absolutely not so
we're really talking about a bunch of
very very expensive resources that
individual users can queue automatically
using software tools to take advantage
of and there's a scheduler that sits in
between site in this queue and these
running jobs these are jobs and when the
jobs evacuate jobs are selected from the
queue and allowed to occupy pieces of
the resource in a dedicated way so one
of the questions that immediately comes
up if you're a user of one of these
systems is how
along with my job wait now this sounds
like an easy problem right it's a
queuing system we're good at queuing
systems right and furthermore the jobs
have very well understood
characteristics you know how many nodes
they need you know how long they run for
you know what their input sizes are you
know what their output sizes are they
don't very very much it's not an
interactive program it's a set of
ordinary differential equations that has
the same size arrays pretty much every
time you run it so so you think wow we
can solve this problem by running an
online simulation or by doing some
queueing theory you know all a Klein
rock or whatever it turns out you can't
you just can't and people have tried for
25 years right when I was at Livermore
in the mid 80s before I even work with
surge in Mitrovica on high-performance
functional languages I worked on this
problem all right and um and there's a
lot of efforts there are commercial
solutions that you can buy they don't
work very well when you buy a queuing
system from one of the commercial
manufacturers it usually has a
prediction technique built into it
almost all the sites turn it off because
a bad answer is worse than no answer at
all if you lie to the users about how
long things are going to take they get
really really angry so so that's there
but no one uses it it's just a hard hard
problem here's why this is a trace of
job wait times measured in seconds for a
year and a half from a NSF funded
supercomputer in the premium service
queue okay the way you read this is time
runs along the x-axis and the amount of
time in seconds is the height of the
blue dot so I think there's 35,000 blue
dots and each one of these tick marks
which you can almost see is a month ok
this is seven orders of magnitude
variation seven orders of magnitude that
is a tremendous amount of variance okay
and this is the premium service queue I
actually used to put I've given this
slide in a number of different context I
used to put the name of the machine and
what the service queue name was on there
and some people from this center got
very angry because they said rich you're
making us look bad right this you
you can't tell people that that this is
the good news okay i can tell you i just
want to tell you where it's the good
news this is typical and in fact this is
a very well managed machine right i
showed you the best graph i could find
from production recent production if you
look at older graphs if you look at
graphs on poorly managed machines they
are worse than this so you're now going
to make a statistical prediction from
distributions that vary over seven
orders of magnitude in terms of days 30
seconds or three weeks that's pretty
much the scale we're dealing with okay
that's a hard prediction problem and
that's why batch queue prediction I can
explain why there's so much variation in
some some regards but before I do let me
tell you a little bit about how we came
to this problem so I've been working on
this for a while when I my first job at
San Diego in 1994 was to solve this
problem and I didn't but I didn't
because I was sort of following what it
was that the community did and what the
community does is it works with
something called expectation now we're
going to get a little statistical here I
promise I won't get to statistical but
but expectation is kind of a misnomer
right the expected value is the value in
a collection of values that you could
replace all the values with and still
get a similar aggregate measure right
it's not the value you expect to happen
next and i'll give you an example of why
that is let's imagine that we have a
hundred jobs varying through several
orders of magnitude right ninety-five of
those jobs wait 10 seconds or less one
job waits a thousand seconds one job
weights ten thousand seconds and so on
and so forth right the expected value is
that number 111111 swen I teach this
class I say okay for a million dollars
if I say that's the first hundred jobs
make a guess as to what's gonna happen
to the hundred first job what do you
guess you almost assuredly do not guess
that number right this is a good
aggregation but it does not give you a
lot of inference power about what's
going to happen next in this case the
mode which is the most frequently
occurring value is the one that you
would like to use that's hard to
estimate and so what we actually do is
we estimate the quantile
right if ninety-five percent of the jobs
wait 10 seconds or less I can say
ninety-five percent chance if you will
that the next job will wait 10 seconds
or less assuming that that's a random
sample and we can deal with Aurra kauto
correlation in the future so the first
realization we had is get away from the
moments it's about quantiles and the
reason it's about quantiles is we can
make inferences about individual events
in very very widely varying data if we
know the distribution okay so how does
one estimate quantiles well you really
want to do this well it's not enough to
estimate the quantile you actually have
to estimate confidence bounds on the
quantile because the quantile is like
any other statistic a statistic and and
it has an estimation error and so what
we really needed to do was develop a way
to estimate confidence balance from data
on quantiles time series data and then
we're going to use one bound or the
other as a representative of a
conservative estimate for the Quan Tao
so if we're talking about the upper 95%
estimate to estimate the upper 95
quantile we're going to use the the
bounds estimate but then you have to
tell us how certain you would like us to
be about that estimate and for
everything else that comes the the
quantile may vary but the confidence
bound on the quantile is ninety-five
percent its upper or lower depending on
whether you're going one side or the
other um but but if you if you take that
to be conservative then no matter how
bad things are this should be a
statistical guarantee on that event at
that level of certainty you will get at
least that much if not more alright so
so we invented essentially a
nonparametric time series predictor
forecaster I'm out of three separate
statistical methodologies right we
needed a quantile estimator and uh and
when I say we it's myself a PhD student
of mine dan Nurmi was looking for a job
case you're interested and John Brevik
who is a PhD mathematician from Berkeley
but who decided he wanted to come and
work in our group for a couple years on
these kinds of problems right and so so
we needed a way to estimate quantiles
from data
and and his idea was to use a
formulation based on a binomial
representation of the data and I won't
put up the actual values or but if
you're interested in what the specific
formulation is I'm happy to talk about
it offline then we needed a change point
detector it turns out that that if
you're estimating quantiles from
distributions and you've got time series
what you really need to do is top your
time series up into regions where the
distribution is relatively stable and
that that's often termed stationarity
although that's a bit of a misnomer um
and so so we had a we had to invent a
way to sort of figure out what the
meaningful the the minimum meaningful
statistical history was in order to be
able to make this estimate and then and
this is so these two things are fairly
general you can take pretty much any
correlated time series and throw at it
the system and and these two things will
work together and make a balanced
forecast for you but in for the actual
queuing system we also needed an online
clustering methodology that's model
based which is not the same thing as
these these mean based techniques and
that has to deal with non first come
first here we serve queuing disciplines
and you'll see why later but we had to
build three things we had to investigate
mathematically and then put together
three separate techniques to make this
time series thing to make predictions
okay this is ten years of supercomputing
it's about two million jobs and they're
from a whole variety of organizations so
nurse is do e Lawrence Livermore
National Labs that's los alamos national
labs that is the San Diego supercomputer
center Texas Advanced Computing Center
San Diego different machine from the
mid-90s and this is the current Terra
grid and the reason we took such a wide
variety is we didn't want to bias our
results based on our methodology right
these traces existed long before we
started uh and these are different cues
so these are different queuing
disciplines different priorities we
don't know what they mean sometimes the
information has been lost but the jobs
were tagged this way every time a blue
bar crosses a red line it's a successful
operation of our methodology so while we
can't provide you a proof that this
works with all the auto correlations and
the other potentially damaging
statistical
argument you can make against our
methodology empirically that's one hell
of an evaluation this technique on
legacy data works and that's the 95
quantile that's the hard one medians are
much easier you can do it with a lot
less data the tails are the tough part
so if we do the tails when you start we
can we can do harder things so you know
i apologize i actually do use that when
i give this to the students from an
architectural standpoint but the
methodology that we use to do this in
real time and i'll demonstrate this for
you in a moment is sort of morally
similar to what we believe goes on at
Google you have a crawl right and the
crawl basically goes around in and
gathers up content which is a new
archive and we archive it with something
called the network weather service but
you you park it and then you run a bunch
of analysis on it to do things like page
rank well our analysis is cubits and
then you publish the results and you
publish it according to a number of
different api's and so do we so so from
an architectural perspective we've sort
of got a scalable venue a scalable
analysis venue for taking now not
content but log data what happened to
the jobs in real time right and it's a
real time system running this analysis
on it on publishing the results okay and
so we do have hopefully a number of
different interfaces this interface was
done by a student Ryan Garver who has
gone on to work on Ruby on Rails and I
think if he was a ruby on rails
programmer when he did this interface
this would look a lot better so I'm not
going to exercise we're actually in the
process of rewriting it but what I will
do is show you what the NSF
supercomputing community has done so
this has actually gone into production
at NSF this is a web service based
portal run a Texas Texas Advanced
Computing Center and it talks to this is
what all the terror good users use it
talks to Santa Barbara when users want
predictions now this is still them the
delay here is not us that's their
resources page
here is us okay so you can ask questions
like at this very moment if I have a 16
CPU job and I want it to run for four
hours and I want to know what the
ninety-five percent confidence bound is
this moment what should be happening if
everything is going to work is that it
goes to UCSB to our system computes
these numbers and returns them so there
is a ninety-five percent chance or more
right because it's a balance that a 16
no job that wants four hours of run time
will start in the next 26 minutes at
oakridge if submitted now there's a five
percent chance it will wait longer than
130 two hours on this machine and we
verify these numbers right part of the
reason we did this as a service venue
and not as middleware is because it's
easy to get this wrong and there's lots
of ways that this can fail right some of
the statistical some of them having to
do with failure modes in these machines
so we're constantly analyzing the
answers we send back ok now this web
page is the third most popular web page
on the terror good user portal and the
other two are the other two pages I just
went through the homepage and that first
resources page you cannot get to this
page without going through that ok so so
if you're asking well does anybody care
right because 25 years people have been
doing this the answer is they seem to
and I have some data on on that as well
ok so that's exciting and we did that
and boy we thought we were smart you
know we went wow we saw this problem and
we have these users and all this kind of
stuff and the very first thing that
happened was a group at San Diego called
me and said rich this is not what we
want they said we have a brain imaging
application and the brain imaging
application takes four minutes to run on
16 nodes of the various supercomputers
that we've got and we need the answer in
eight minutes
can you use your system to predict
whether we're going to make the deadline
because if the answer comes in an eight
minutes and one second it's no good to
us and and so what we said well you so
what you want the probability right you
want to know what is the likelihood the
probability that your job will meet the
deadline we said okay we think we can do
that all right I'll go back to the
terror good user portal and I apologize
for the speed with which this thing
seems to cycle and we'll get two more
hits for those other pages to to do then
we go to us and here on the left side we
have invented the following thing I want
four cpus and I want a runtime of two
hours and all right and I want a
deadline sorry I want to know if it will
start in the next hour by the way a lot
of this work was done under the auspices
of grid computing if anybody comes to
speak to you about grid computing ask
them to show you their software in a
demo mode because this stuff works um
and there are the probabilities at ORNL
that job has a ninety-seven percent
chance of starting by your deadline in
the next hour Purdue Lear it's only a
thirty-five percent chance so we could
invert the the answer that we give you
ought to make deadlines or to deal with
that boys right so um great here are
some of the questions that inevitably
come up when I get to this point in the
talk uh great what happens if everybody
uses this we don't actually know we've
done some simulations the simulations
indicate that that the size of the user
community doesn't matter other things do
matter like how busy the machine is and
less busy is is a problem which I'll
explain but we've taken some steps and
in particular we don't consider job
cancellations and making the prediction
and that is so that users who look at
predictions are not able to affect the
predictions we make by cancelling their
job wallets in queue that's very
important we can make a much better
predict
tighter bounds than we're doing if we
did that but then it would be
susceptible to feedback and so far it
hasn't been and then there's this other
question about whether user behavior
will change as a basis of what they see
from this and and we don't know the
answer to this question there's some
evidence it's been running in various
forms since last for a year now there's
some evidence that nothing is happening
there sign is that something is
happening we haven't been able to tease
it tease it apart yet from a statistical
perspective autocorrelation just freaks
people out and they say look you know
this thing is heavily auto correlated
isn't this a problem and it it isn't it
isn't in some part of it it is in the
sense that for the change point detector
you need to know what the
autocorrelation is to know whether an
event is rare or not based on the fact
that it's correlated with other events
that have occurred recently so we do
consider it explicitly it computes an
autocorrelation when it does this but in
another respect it's not in the sense
that we're looking at jobs that go into
the queue and predicting them based on
jobs that go out of the queue and the
amount of it time in between is very
very large and if you look at the
correlation between these jobs and these
jobs it's actually a lot lower than it
looks at a time series so so we actually
the binomial methodology is very very
robust to auto correlation for reasons
we only partially understand um and and
there isn't enough autocorrelation in
these two measures to actually confound
it right and the last thing people say
is yeah well which you're talking about
this like I guarantee you're guaranteed
95% and guarantees you know don't come
with probabilities and my answer is yeah
but they should every guarantee that you
are given legal guarantee or quality of
service guarantee or whatever has a
failure probability the difference is we
know what it is and will tell you and in
fact you can tell us how certain you
want to be and that will affect the
nature of the answer another criticism
that and this came out of one of the
reviews that we got for this work was
well this is just an interesting party
trick right great you have figured this
out and and nobody really cares and and
you know they don't care because you're
not actually making a prediction for an
individual job you're making a balanced
prediction and um and and to illustrate
that criticism you know this is the
upper bound
27 minutes and you know that if you have
a choice between that system and this
system you'll wait no longer than 27
minutes with ninety-five percent
certainty great but if I asked you to
compare these two systems say Pittsburgh
and Purdue and I say the upper bound is
fifty five point two hours there in 59.4
hours and I can't tell you anything
about what's going to happen before that
there's a five percent chance it'll wait
this long but there's a ninety-five
percent chance it won't wait that long
and I'm not going to tell you what is
going to happen is that useful and I
have a lot of users who said no that's
not useful is not a specific prediction
and all the administrators we tried to
talk into getting to a run this
basically said that said no one is going
to use the bounce prediction stuff right
um so so we have quantiles but it's
abound and that bound maybe may not be
you to a utilitarian until we started
talking to the weather forecasting
people this is a cartoon that depicts
without time scale kind of how weather
forecasting is done right you've got a
series of observations and they're taken
periodically and and they may or may not
be from sensors that can be controlled
they go through some analysis and some
cooking right data products are produced
then there are forecasting models and
and in the case where these are not
dynamically controllable instruments
some information will be fed back into
this analysis and assimilation
potentially and and in the the old
school this happens over the course of
months it's a very slow process and so
you're producing files and files are
being consumed and then from that come
forecasts that are then given out at
Oklahoma Kelvin Drogo Meyer has put
forth the idea that that this cycle can
be made to run in real time you have
dynamic Doppler radars and you have
tornadoes that are moving across the
country so this is going to not be
planetary whether they call it mesoscale
weather and what you want to do is run a
bunch of this and then redirect the
radars to know where to look next and
you want the time scale on which these
things come through
to be rolling forward at shorter and
shorter intervals because if it's
heading for Oklahoma City you want to
start getting people out of the city but
it's very very expensive to have false
positives so these guys want to launch
based on a static doppler radar picture
on demand high performance computing and
to keep doing it in this cycle right
okay so that's hard to do if you have
seven orders of magnitude on variation
but it's also hard to do for these other
requirements the first is that you can't
go in and change the codes before I
started this work my specialty was
building these highly adaptive
high-performance codes I would spend
years engineering these things and and
they would be able to move themselves
and shape themselves and I said give me
the code i'll do that to your code and
they said you can't touch it it has
numerical properties that we have spent
years verifying as soon as you start
playing with the data structures we have
to go through a rivera fication step and
that's going to take a long time for gay
you're stuck with these codes so again
there's this long cycle and so for good
scientific reasons you have a lot of
manners invested in this you have an
incremental and sedimentary code base
and a build model that expands but never
contracts so even just building the code
is hard the second thing is is you're
using you're dealing with users who are
not very tech savvy I suspect that a lot
of the innovation in the commercial
sectors driven by people are very
excited by the technology and what it
can do for you and that does not
describe typically hard scientists in
this space Fortran is a good tool MPI is
an okay tool depending on who you talk
to it used to be a really really bad
word the web is just kind of puzzling
too many researchers anytime you start
talking about a network you start making
them angry and forget grid computing
grid computing is basically sort of a
formal way of doing this and and it's a
bad word in many of these contexts so so
you're constrained in the solution
technique that you're going to be able
to use here nonetheless what what Kelvin
and his colleagues had done was to try
to present this code as a workflow so
we're going to wrap the legacy pieces
linked them together with BPL which is a
language a scripting
language for doing this and then run
them on dedicated machines so if there's
an emergency they have a stockpile of
clusters that are just waiting to run
these things and they've gotten the
national centers to implement preemption
so if Katrina is going to happen or they
think Katrina might happen away you go
but there are a lot of non-emergencies
right you need to do development you
want to experiment with the models you
just want to know what the weather is
going to be and hear what they want to
do is to make reservations they want to
say at 6pm we want to run and when you
know how long we're going to run for and
we know how much space we're going to
use and we know how often we're going to
do this because these experiments cycles
are fairly well understood and hence the
problem when you start allowing users to
make reservations you lose utilization
because you're blocking out those
resources and the users they use them
but the resources are completely gone so
there's a loss of utilization that has
to do with reservations and furthermore
but whether people are not the only
people who want to do this if
everybody's getting seven orders of
magnitude variability a reservation
sounds like a really good idea even if
you're not predicting the weather and so
as soon as you put this into place
there's a huge fight and they administer
the center's don't want to be the
arbiters for this so this was the state
of play in 1993 I'm one of the first
things that I was supposed to do while i
was at san diego as well and and they've
been a lot of different approaches put
in to allow this and none really made it
into production currently you can make
reservations but it's done by hand takes
weeks or months to set up it's usually
only available to a few users you can do
it but you have to justify when you're
doing it and really the only
justifications are you doing a demo for
your funding agencies or there's an
emergency or there's some big event and
and they're usually manipulated by hand
this is actually changing a little bit
partially I believe in response to the
work that I'm about to show you but but
the bottom line is there's not an
automatic method in place that is
generally available and I came across
this while working in the context of a
large project that had been run for many
years by a number of high performance
computing luminaries except for that one
who we're working
trying to figure out what programming
abstractions we needed to give
scientists so they could do this kind of
thing more easily and and what we
decided to do in that context was to try
to see if we could use this predictive
technique to build what looked like a
reservation but was was really a
statistical party trick and that project
was called V grad so we worked on that
we worked on it for a while and and we
were staring at graphs like this and
here this is the previous graph I showed
you but I've superimposed the upper 95%
quantile right and you can see how it
moves and and we were staring at that
and one day I realized the width of that
is one month this doesn't look all that
stable but if you zoom in it's really
pretty stable that's a month right so
what this means is that I make a
prediction now of the upper 95% quantile
it's good for a while maybe several days
that's good that means that the
distributions are stable even though
there's a lot of variation we're not
going through change points every five
minutes and that's what you would expect
next thing you need to know is how it is
a little bit about how it is that high
performance computing scheduling works
and the important word on this thing is
backfilling basically if someone wants
to use a large fraction of the machine
because its space shared you have to
drain some number of processors which
essentially means idle them if I want
half the Machine half the machine has to
be idle in order for me to take
advantage of it and it doesn't go idle
all at once processors basically die at
their own rate so and so while those
processes are idle you're losing
utilization and so somebody at Argonne
came up with the idea saying well wait a
minute we're going to make our users
tell us what the maximum runtime is for
their jobs and we'll backfill into the
idle deadline to get the utilization
back and there's two kinds conservative
backfilling and liberal backfilling but
what this does is break first come first
serve in a very nasty way because now
there's this game theory how big a job
can I slip into the backfill and users
constantly play with this and so centers
constantly adjust the the knobs
sometimes those knobs get turned three
or four times a day right so what you've
got is this kind of game
between users that are trying to run
their short jobs in the backfill right
and centers that are trying to keep
utilization as high as possible and
that's what causes all of the
difficulties but if it's relatively
stable with respect to the quantiles we
we made the following observation we
said well okay let's say we want a
one-hour reservation that starts here
and ends here and we have an infinite
allocation we don't care how long we
actually run for we just need to be
running during that time slot because
that's when the forecast is necessary
right and this is 12 hours into the pass
or more appropriately this is now and
this is 12 hours into the future we want
a reservation for 12 hours from now well
if we submit a 13 hour job which covers
12 hours of potential wait time and one
hour of runtime and it starts any time
before that time we make the reservation
right it's 13 hours it might start
immediately in which case we burn 12
hours of allocation it might start right
here but if I know the probability right
I know the probability of making this
reservation but if things are stable if
I wait an hour and submit it now I only
have to submit a 12 hour job and so on
and so forth notice what happens about
here that's where backfilling occurs at
some point your job gets short enough
that you get back filled and the
probability of making your deadline goes
way up and that is a consistent effect
it's not an effect that occurs in
exactly the same spot but all of these
systems use backfilling so in some sense
statistically we can reverse engineer
where the backfill is at any moment and
delay your submission until the latest
possible moment that meets your
probability target so let's say you want
to be seventy-five percent certain you
want to point seven five percent or
point seven five probability of making
your deadline and or better right all we
have to do is compute this trajectory
which we can do online very very fast
now walk the schedule backwards to find
the first place where the trajectory
crosses the point seven five line and
that's where we're going to submit your
job in the future okay so user specifies
a reservation start time a note count
how many nodes how long the job is going
to run for what the reservation time is
and what the target probability is
and we compute when to submit that job
in the future if there's a time in the
future we think we can submit it such
that you're going to make that
reservation with the target probability
and here is one set of results we've
done this a lot on the NCAA tera grade
fully production same sort of it's
actually worse than the data i showed
you 12 hours from now 16 knows one hour
75 percent probability we submitted 356
job requests over like a month and a
half or two months something like that
there were only two hundred and fifty
seven jobs that actually got submitted
which means ninety nine percent or
ninety nine times the system came back
and said I can find no time in the
future such that you're going to meet
the seventy-five percent probability but
of 192 slots that were successfully
acquired is almost exactly 75 percent of
257 we were off by one that is a miracle
right because because we made a
reservation on that system that doesn't
support reservations that has that level
of variation at the target probability
we wanted to make it we've tried this
for a big collection of jobs and
machines and times and these are the
numbers that's a conservative estimate
of 50 that's a conservative estimate of
75 we didn't quite make it here and this
is instructive because it's one machine
and it's one machine that has a non
statistical behavior and it turns out
that if you want to do this trick you've
got to be able to detect that and I'll
talk about what we're doing about that
talk about future work okay so in
general with maybe a little bit of an
error that we think we understand this
works and it doesn't require the
center's to lose any utilization and it
doesn't require them to re engine their
system in fact they can't stop us from
doing this they don't know the
difference between a job we're
submitting under this job's control and
what a user's really doing it's an
overlay okay now you say well rich how
much am I paying in terms of allocation
and I asked well what did you pay for
your reservation slot and they say well
each of the center's charges me
something different and they change the
charging and I don't really know and I
say well I'll tell you
on average you burn one point one nine
times your actual runtime if you want a
fifty percent target probability it only
goes up to 1.2 to its 75 percent but
it's fairly substantial you're going to
double your execution rate if you want
to be this certain about a single job so
there's a cost but it's quantifiable and
it's your cost and it's not set by the
centers based on some non-economic thing
it's a purely a function of the demand
on the machine so we think that that's
an attractive property okay let's say
you don't like 75 percent which are kind
of fond of the 1.22 number well since we
know what the failure probabilities are
if we're willing to assert the failure
probability is on two machines are
independent and they look very very
independent you can start combining them
and in particular if you combine three
of them and they each have a twenty-five
percent chance of failing the chance
that they all fail is 25 cubes so the
chance that at least one succeeds is 1
minus 25 2 which is point 99 but because
we cancel the other jobs your allocation
cost is 1.2 to we just now made what
looks like a fairly reliable reservation
at a very small extra cost through
replication and we run actual hard
reservations they're not this reliable
if you make a hard reservation there's a
probability it's going to fail and it's
not one in 100 so um we think we solved
this problem and we think we've solved
it very very neatly using statistical
techniques and and if you don't like the
probabilities that we're getting as long
as we have enough scale plugins are
enough machines that are operating
independently so that we can make these
decisions we can boost the probability
up to you want but what you sacrifice is
you don't know what's machine we're
going to run you on it's a virtual
reservation the resources are
virtualized okay so we brought that up
and they said well we're not interested
in that what Richard in co allocation
collocation is where you're going to get
a reservation on two machines
simultaneously and and this is this is a
dubious use in a production setting but
it's very very useful if you're trying
to experiment with scaling factors right
and so the center's are very resistive
to this because it's even worse
in terms of utilization it started
showing up as a demand in 1996 and last
year NSF the people who essentially
allocate the funds for this mandated the
co allocation will be supported can we
do that the answer is yes but it's
tricky and in particular the joint
probability goes like the nth root if
you have n resources in your Co
allocation I'm up so in particular if
you want a ninety-five percent certainty
of two reservations meeting the deadline
each one has to have a point 975
probability of success and as n gets
bigger that number gets bigger we meet
then need to think about how we can find
probabilities arbitrary size but this
probability boosting thing comes to your
rescue right if I had 6.75 reservations
to make Vark by the way is what we call
this virtual advance reservations for
Q's if I had six of them and I can break
them into two groups of three each of 1
is 0 point 99 the co allocation
probability is 0 point 98 mean to co
allocation no changes to the center's no
loss of utilization no screaming and
yelling okay I'll start to finish um
where are we qubits that's basically a
predictive methodology and it's the
thing that gives you the bounds and it
consists of two general pieces the time
series are the binomial estimation part
of the change point part and a third
piece which is specific to the cues it's
ending his first year in beta release
the site is it's been supported in
production full on terror good
production since January first of this
year and in the last year we've been
doing about 3000 hits steady-state per
day we went through two months where we
did like a hundred thousand hits a day
and what was going on was a couple of
groups got so excited they thought were
going to take the site down and we're
scraping our webpage and we finally
explained to them you know we're gonna
keep it up we've got the the data and so
this is really our steady state about
3000 hits a day about 100,000 hits a
week this is from a user community it's
between three and four thousand in size
so that's a pretty good hit rate um
from such a small user community Vark
the reservation mechanism is an alpha
release it's ready for beta but we're
getting a lot of pushback from the
Centers they don't really believe that
this stuff works and and so we're going
through and a very extensive evaluation
period at this moment and Cove arc the
co allocation thing if they're not happy
with Vark they're really not happy with
Cove arc and here I think their
unhappiness is justified in the sense
that when we do these Cove arc things
we're seeing failures not in the
statistics but in the actual mechanisms
we use to launch the jobs and reap them
and we can't tell if they're our
failures or failures in the underlying
systems the failure modes in these
machines are nasty and when you start
combining them particularly in this way
it doesn't look to us like this is
working yet but we think we can
understand it okay there's a couple of
open issues one is statistical it turns
out that light load is a problem that
sounds weird because of the machine is
lightly loaded you sort of know what's
going to happen but one of the things
that users do when they see a lightly
loaded machine is they submit an
unreasonably large number of jobs
simultaneously now this is not good from
a center perspective you bought this big
machine you want users to run jobs that
require more than one node at a go but a
lot of users write scripts that submit
thousands of single processor jobs using
it as a big cluster we can't stop them
from doing that although we try to
discourage them but the problem is if
you can corner the market on delay if
you can influence by yourself what those
seven orders of magnitude variability is
going to do to your job we can't make a
statistical prediction it's not a
statistical problem right and so um so
we need to actually filter for this and
we've experimented with one filtering
technique and it works in many
circumstances but I'm not satisfied with
it I think we can crack this problem the
next six months the second problem is is
really a cultural one and and this is
where I I feel much more akin to you all
then then my colleagues in the sense
that administrators and these are system
administrators or administrators who
hold the purse strings are just not
comfortable with the idea that they're
big expensive machine needs to be
treated phenomenologically what do I
mean by that I am doing an analysis of
what the thing does without explaining
why it does it like you do pagerank you
know you figure out what's the right
page you don't really know why it's the
right page you just know what to look
for to know that it's the right page
well that's what I've done right there's
a phenomenon out there that that I am
studying automatically with these
methodologies and and these people their
responses no we own the machine we
control it it's not a statistical
problem it's a mechanistic problem we're
just going to fix it and my response is
why I've been waiting and the users have
been waiting and if you can do it this
way you should do it this way but I
think their arguments I understand I I'm
sympathetic to them all right it feels
weird to be doing this here's some
lessons that we think we've learned and
and and this first one may not sound
unusual to you but sounds very unusual
in my community is really possible to
use online forecasting to build useful
services in dynamic environments I've
been doing this in a in this
context in other contexts for a while
and and a lot of people just said look
this is purely an academic exercise
you're never going to be able to build
anything that anybody's ever going to
want to use out of this stuff and they
just turn out to be wrong dead wrong but
I will admit my first hundred attempts
that this problem failed right I worked
on this problem for 10 years before we
started to crack what was going to work
um and and I did and my failed for the
same reason everybody else's does you
just don't want to use a moment based
approach right it's really really
difficult to make inferences and
variability kills you another lesson
which i think is interesting at least
for our community is that this notion of
combining services in an overlay which
we talk about a lot right really
requires two things it requires a
success probability to be associated
with each service and one that you can
rely on we talk about success
probabilities but if you actually
measure them you need something hard
here and you need scale and this is a
real eye-opener scientific computing
people hpc people think of scale as the
enemy the larger things get the more
likely they are to fail the larger the
Layton sees the more the variability
it's not your friend and in
they're just wrong about that in this
context the more machines you give me
the more I can ensure independence the
better job I can do right those numbers
come from about 15 machines which is
really what the NSF is funding in
production at the moment there's about
30 machines only about 15 you get used
seriously you triple that number those
numbers get way better okay um another
sort of controversial statement and this
is one that I missed until we did this
is middleware is not the best approach
now for a long time and in this
discipline we were talking about the
software veneer idea it's really an
operating systems idea where you're
going to put a service layer on top of
everybody's operating systems but if you
do that and you and you do that at least
for monitoring systems you don't get the
data you need the data has to be good
and the reason you don't get the data
you need I think is because there's kind
of a conflict of interest so we
originally put the software out as
middleware and a bunch of people stood
it up and they said well well our
machine looks lousy we're just gonna
change the probability numbers and so
suddenly their numbers looked a lot it
really really good but our experiments
we're failing we're like what are you
doing and they said well your point fine
95 number looks really bad so we made it
point to I'm like well don't make it
point to because we can't make
inferences from that and they said well
but making it look bad so so if you put
the monitoring software out there people
are gonna manipulate the monitoring
software for their own reasons and those
reasons are not necessarily your reasons
um and and so at least for monitoring
I'm fairly confident in this for service
composition in general I think it's an
open question um and and and that's
because there are a lot of complexities
involved with things that are not just
sort of scraping the data and making an
inference that may may come up the bite
us okay in the remaining time what i
thought i would do is then try to draw
some parallels between this kind of
research which ordinarily at least in my
discipline we don't think of as having
much to do with what you do and what it
is i think you do right and and so the
first similarity i'll say is this is a
different kind of computer science
research this is not the kind of
computer science research that typically
gets published in systems journals and
it's because you've got to work with
real users none of that none of that
happens unless they're
real users and real data in real time
and that's really hard if you're an
academic intruding on someone else's
work cycle so that you can verify some
theory requires an awful lot of
machinery that my competitors do not
need to build and you sort of experience
this as well if you go in there and you
tweak something on Google and it gets
twenty percent slower like governments
topple right I mean you know it's you're
like a national resource now and I
expect a law that says Google response
has to be 27 milliseconds or someone
goes to prison right okay so so or maybe
not um so so really working with users
at scale is a big challenge and it's
really a challenge for computer
scientists and it's and it's very hard
to reconcile that with the incentive
systems that are built into more
academic computer science but I think we
need to second similarity is that this
notion of scalable service venue where
essentially you take the data you
manipulate it you look at it massage it
and make sure it's what you want it to
be and then publish the results is
really the right model right and uh and
that's we didn't think it was going to
scale you've shown that it can scale um
and and really the community I think
needs to just come around to that and I
think they are and the last similarity
is this is really a triumph of applied
statistics and and so are you right from
systems builders perspective we don't
trust statistics and and in Google's
case you're looking at content and we
systems people can say well but that's
the content the machines we understand
the machines now in fact apply
statistics were great for machine
performance in fact in some sense they
were better than machine then for
content because machines behave the same
way over and over again typically so um
and and also and this is true for you
guys as well the results get better with
scale I think that's a key key result
but there are some differences the first
is that there's an incentive mechanism
based on the use of these systems which
doesn't exist for you and and which some
people argue is just broken or that the
very best counterproductive I don't want
to argue one way or the other I'm just
saying that this is an assertion that
you can make
I think defend and I think it needs to
be studied and also we have a huge
legacy problem right we can't make a sea
change in any of this without
potentially obviating a huge investment
that can't be recovered so so you need
to be very very cognizant of legacies in
this space if you want to do this
research okay what do we have planned
next for this work well we're going to
try to expand the model we're going to
try to say okay why don't we set up our
own job submission site and if you trust
us begin or you're willing to give us a
proxy certificate or some other magic
voodoo you can send your job to us and
we'll run it for you and we'll run it
for you cheaper and with better response
time based on where you're entitled to
run okay um and and the reason I think
this is going to work is because it
separates these concerns again the
centers are concerned with utilization
and we're concerned with getting your
job done fast and because we can focus
on that and they can focus on
utilization I think we can meet in the
middle from a Google perspective this is
sort of YouTube for jobs in the sense
that you're going to upload your job to
us and then potentially get back what
you want no it's a bad analogy but but
we think of this as opposed to just
giving the information we're going to
take responsibility for the job and that
introduces complexities particularly
with respect to legacies that that we
really need to study another thing that
we think we can do is we can predict the
availability of services themselves so
now we actually know what's going on
with the queues there's a tool called
grid ftp grid ftp depending on
configuration can be reliable or
unreliable very first thing that people
from North Carolina said is well can you
do the same thing for grid ftp can you
predict whether my grid ftp is going to
work or not we think we can write we've
actually done it for machine
availability we monitor the labs at UCSB
the student labs where students
routinely reboot the machines we make
phenomenally good predictions about how
much time you're going to get before
that machine reboots phenomenally goods
startling Lee good in fact and the last
thing is you can think of all of this is
qos for dynamic environments this is
where we're going
before I say thanks to everybody i need
to say thanks to i just want to take
this opportunity to say that we lost a
visionary in this field in ken kennedy
last year if you don't know who ken
kennedy is um he's really really the
person who has been driving a lot of the
thinking in compiler and runtime system
work for high performance computing over
many many years and the V grads project
was initiated by him before that there
was a grads project I've been
tremendously fortunate to work with
Kenan and personally I miss him
tremendously and I'll end here by saying
thanks to NSF not only for funding this
work but for just giving me access right
a lot of this is being able to look at
the logs and and make inferences about
what's happening the center's I mean I
may have portrayed them as an adversary
and and in many respects or at some
levels they are but in terms of giving
us access and helping us do this work it
is absolutely impossible to imagine
doing it without their cooperation they
really are interested in what we're
doing they're just skeptical about the
results this is my lab the mayhem lab
and I'm just going to plug my students
dander me again looking for a job John
brevik at cal state long beach now
graziano bertelli you cannot have
matthew allen just about to graduate are
all folks who worked at some time or are
currently working on qubits we have a
sensor network a sensor networks effort
it doesn't sound like it's related and
it's a whole other talk as to why it is
that that's going on you hired yay when
at Google Kirkland from this project
he's very happy there and I've got two
more excellent students who are working
on that and then in the utility
computing space we're spinning up an
effort Lamia Youssef who will be
graduating this year Sri Ram a radical
plan who is brand-new and dimitris agora
turn off who is my postdoc and we'll be
out and then I mentioned a little bit
about incentive systems you really do
have to think about that and I have a
student Andrew mutts who's extremely
excited about that work this is me and I
thought it would be absolutely remiss
not to end with our gratuitous mashup
that is where we're doing our hits today
we did 2319 since midnight the one thing
I'll say and this is another interesting
research problem is a lot of these hits
come through the terror grid user portal
in Texas and we don't know what the IP
address was that actually initiated it
because it's being proxied so a big
proportion of our traffic looks like
it's coming from one place but that's
the google map rendition and i'll stop
there questions
I'm sorry job this year honey I
different glossy you know one part will
be fed job with you kind of more support
then you have a real time job like this
or run more water service we have
developed in jobs which came crashing in
time in any halfway I think for a
relaxing doll toys which higher than the
others so cago decision degree computing
room you know that right so the way
that's typically handled is each
resource supports multiple queues and
the ques have different resource limits
so for example almost all of the NSF job
no one in SF center support what they
call an interactive cube or a debug
queue and the idea is that your job can
be no bigger than say 16 notes and can
run for no more than 10 minutes and you
can submit to that Q and we we use that
information we predict for each queue so
we'll give you a different prediction
for your interactive to then we would
give you for your standard queue right
or a premium service queue right premium
service queue they charge you more for
your allocation so instead of a one for
one occupancy they might charge you 1.5
to one but in theory your job gets out
of the queue sooner so so there's a
there's a queue breakdown and then
within the queues the hidden so that's
that's published right you know what the
queues are on the machine but what you
don't know is where the backfill is
that's hidden and it changes and so we
use the clustering technique to recover
different service classes automatically
we determine how many service classes
there are and what the boundaries are
there are automatically constantly for
each queue so we can figure out if your
job is a 16 no job and it's going to run
for an hour it goes in this service
class in that queue but if you were to
submit it to this queue it would go in
that service class and we make two
different predictions bad job best bet
yes oh I'm sorry I mean the point is
that can I think they're a panic two
years like don't let me know this is all
bash these machines again are run
entirely in batch mode you might think
that that's unique to the the centers I
got a call from an oil company
um I won't say which oil company but an
oil company about three weeks ago batch
batches popular that's all I'll say
thanks for coming sure absolutely so you
mentioned own areas yeah
I understand because from my positive we
can break small I'm either what kind of
machine yeah good question sorry I'm a
great question so at the moment there
are approximately 30 scientific
instrument class machines and and and
that's not that's a very broad category
so a good example is data start probably
one of the most useful machines so the
scientific community is San Diego
supercomputers data starts an IBM SP XX
with the RS 6000 based processors it has
256 nodes and each node has I think
either 4 or 16 processors from fat nodes
that have 16 processors but most of them
are four processors and they share
memory and then the inner connection
switches this very very fast federated
switch which is very low latency ok so
there's data star there's a blue jean
what a lot of people have been doing
eldorado which is the cascade memory
system and the AMD processors so so one
class of machine i guess is commodity
processors hooked to some fancy switch
and there's and these things are big
they tend to be thousands of nodes not
tens of thousands of nodes but thousands
there's another class which is very very
large clusters so the NCAA Terra grid
node is a got a thousand ia64 dual
processor cluster nodes and mirror net
subha me we actually run in production
the one outside u.s. system unit subha
me so by means the fastest supercomputer
in japan it's faster than the earth
simulator it's a bunch of sun processors
hooked together with InfiniBand and and
we're running on that system as well but
but they tend to be very large resources
that you log into and they're they're
only housing places that have sort of
the maintenance facilities necessary to
keep them running machine that image is
pretty much like a question
it's kind of like a cluster yeah I mean
you know logically it would look like a
cluster to you right I mean it would
look like a shared memory cluster where
the nodes potentially can share memory
and multi-core is blowing that all right
i mean there's gonna be a lot more
shared memory at each node but yeah
that's exactly right well up until now
the answer that question has been no but
i think that's what's turning people on
because you were given an allocation on
one machine and and not on another and
then and and no one knew how long it was
gonna wait but as soon as we put this up
or maybe you know coincidentally um NSF
changed its allocation policy and get
what's called a roaming allocation which
basically means you get to run on any
machine you want and as soon as that
came out i think our hit rate went way
up because now people actually have a
choice but the architectures are
different an ia 64 is really different
than they are a 6000 in terms of
granularity and your codes have to be
built to port and and not everybody's
codes are so it's not a completely fluid
decision-making feel good questions yes
sir read all about the effect with the
colocation scheduling where you have
submit multiple jobs we really only plan
to run one of them the best one that
gets scheduled are you worried about the
effect of canceling no well okay so
we're not worried um should the center's
be worried but the answer that question
is probably not okay so this is this is
a deep question um the the question is
if we're going to do replicated
submissions and then one pays off and we
cancel the others what effect does that
have you're trying to collect right no
and the reason is because we don't look
at any jobs in cute if you cancel we
never see the data for this reason we
could we've actually experimented with
what would happen if we looked at data
for in queued up in the predictions get
much better but there's this risk
however you might ask well gee if we're
prompting a lot of cancellations or
we're doing does that affect the
individual schedules in the backfill and
the answer is we don't think so if
you're doing conservative backfilling um
from a theoretical perspective it might
we ran a billion simulations and we
couldn't make it happen but
theoretically could if you're doing
liberal backfilling though not gonna not
going to affect you I don't think
sorry-looking the roller hit up after
graduate though cuz very good yeah
that's a good question so uh question is
where's the data with the overhead the
startup overhead and it's in that
depends there are some jobs for where
where you have to move an awful lot of
data and so the data gives the job at an
affinity to a particular machine now
that's true in a lot of cases um but
there are some jobs for which the data
movements actually pretty quick and and
a lot of data in this space is kind of
different so there's a 40 gigabit link
between San Diego and ncsa you got to
have a lot of data before 40 gigabits on
a three hour run isn't going to get you
the gigabytes isn't going to get to the
data there but there are some jobs for
which that's true however what we have
found particular we working with the
urgent computing people though these
people are worried about disasters right
the emergencies and they want us to be
able to predict something called next to
run status so all of the center's agree
to not kill but essentially put whatever
job comes from them at the head of their
queue if they present a particular token
one of the questions you could ask is if
I had such priority how long would it
take me so we can actually predict that
for them right but they have a staging
problem they need to stage the data so
one of the questions they asked is well
could be you predict the lower bounds on
the file transfer time and it balances
what you want you just need the data to
arrive before the job starts you don't
need to know when it's going to arrive
we can do that now whether it's
independent or not I mean there's we're
not doing it and it's because another
feature of working with real users is
don't get it wrong if you put it up and
it's wrong you will never do it again so
so it takes us a long time to do these
things cuz we spend a long time
convincing ourselves this is gonna work
right and and I mean I have had these
moments of utter depression when I
thought this is never gonna work it's
never going to work right I mean I'm
just looking at a guy who's never going
to work because if we get it wrong and
it doesn't work we're dead
I'm glad I was well other questions
you're all immensely patient for coming
to listen to me yell at you today and I
thank you tremendously
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>