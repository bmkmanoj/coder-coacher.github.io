<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2014: The Challenge of Fairly Comparing Cloud Providers and What We're Doing About It | Coder Coacher - Coaching Coders</title><meta content="GTAC 2014: The Challenge of Fairly Comparing Cloud Providers and What We're Doing About It - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GTAC 2014: The Challenge of Fairly Comparing Cloud Providers and What We're Doing About It</b></h2><h5 class="post__date">2014-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_YstK4EEzhQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">&amp;gt;&amp;gt;Anthony Voellm: So I'm Tony Voellm, Anthony
Voellm, not to be confused with a Anthony
Vallone with the beard.
Turns out we were both physics majors, both
from the East Coast.
My first job at Google was to take over his
job, which really confused people, so I had
to change my name so just call me Tony.
Usually when you see it in print, I use Anthony.
Thanks for having me back.
After I heard all the stats the last two days,
I feel particularly honored to have actually
been selected to speak this year.
Two years ago I spoke about the sort of budding
cloud.
Nobody really knew what it was.
And then last year I got to be the jokester.
So I’ll try to add humor.
You're free to laugh.
If it's not funny, don't laugh, but if it's
funny, you can laugh.
And as I looked back at the pass talks that
I had done, the one thing I noticed with myself
is I had blue and orange and now I have gray,
but for me I kept looking at these pictures
I just noticed my hairline was moving back,
so I might not put these pictures again.
So what do I do?
I actually work here in Kirkland.
I literally work right across the street on
the second floor.
I head up the Cloud performance team.
And my job is to make the most performant
and the most price performant cloud in the
industry.
And in order to do this, we have to measure.
We have to know where we are.
We have to know how the landscape looks.
And it takes a lot of automation.
And so what I'm going to do today is take
you through, like, how did we come up with
the automation, what are we doing to evolve
it.
I'll try some demos here at the end and if
all goes well and you aren't using all the
Wi-Fi, I should be able to run something,
and it will be good.
So here's the brief overview of what I'm going
to do, and an overview is good because I'm
known to talk fast, so thanks to the transcribers.
So I'm going to give you a brief overview
of Cloud and benchmarking and give you some
of the definitions.
I'm going to go through the history where
I parallel the sort of coming of cloud computing
side by side with how benchmarking has evolved
over the years and what's happened there.
Then I'm going to tell you why this is -- we're
sort of off base in what we've been looking
at so far with Cloud.
If all goes well, you'll see a really cool
demo and then we'll wrap it up.
So with that, what is Cloud?
Cloud is a word we've all heard and honestly
there's only been one person who has been
able to explain what cloud is and has the
most accurate definition and that's this guy
here, Eric Schmidt.
And he says, &quot;I don't know what cloud computing
is but I know it's a marketing term.&quot;
Perhaps quite accurate, but for our purposes
I want to give you a little bit more context
of what I'm talking about.
So we've been in this transition over the
last couple of years, and we're still in the
very early days of this where we're sort of
taking the workloads that we used to run on
dedicated work -- you know, dedicated data
centers or workstation under our desk.
So instead of running our local networks on
our local infrastructure, we're start to go
move them into the cloud and going over the
Internet in order to access resources.
And by resources I mean things like machines.
That's Compute Engine here for Google.
Storage where you have, like, object storage.
Put the file in the cloud, pull a file out
of the cloud do some sort of manipulation.
There are really fast and rapid services like
key value pair services which are important
to building Web sites where you have a key
in a value like data store, and these are
really fast and this is sort of infrastructure.
And the picture I'm showing you here is pretty
different than what we're most used to looking
at, which is when cloud is often talked about,
it's talked about as infrastructure as a service,
platform as a service, and software as a service,
this tiered pyramid of layered things.
And that used to be true but not so much anymore.
So for example, Compute Engine is virtual
machines in the cloud, and this would be considered
infrastructure as a service except that there
is this ability to dynamically provision and
de-provision machines on the fly.
So is this platform as a service or is this
infrastructure?
So what you see is this blending of ideas
in order to make our job, quite honestly,
super easy where we can build scalable infrastructure
in hours and days instead of, you know, months
and years.
So at Google we look at cloud computing like
this.
We look at it as compute.
There's things you're going to compute on,
whether these are App Engine apps, things
that you're serving for your platform, or
whether you're serving them from a VM, we
think of this as the compute that you have.
We think about it as storage across, you know,
database and these key value pairs and object
stores.
If you really stand back at a ten-foot level,
you really can see storage is storage.
Bytes are going in, they persist, and we can
get them out later.
And then we have these services that help
connect all these other pieces and give you
the extra capabilities.
Like end-points allows you to convey messages
into the cloud and out of the cloud and do
things for you, and services like Big Query
allow you to do massively parallel query on,
you know, hundreds and hundreds of terabytes
of data in seconds.
So for Google, Cloud is really this set of,
you know, services that work together to solve
a particular problem.
Benchmarking.
Let's take a quick look at the definition
here.
So Wikipedia, one of my favorite services.
I definitely donate to this frequently and
I use it.
So benchmarking.
Benchmarking here is defined as the act of
running a program in order to get a relative
measure of performance, to get us some number
about how it's acting relative to something
else.
Well, this is a technical definition.
I think a more astute definition might look
like this, which is lies, damn lies, and benchmarks.
You know, how do we find the truth about the
performance.
And so hopefully I'm going to give you some
truths about the cloud performance and how
to fairly look at, you know, cloud and how
to measure it.
So let's go through our history here.
Cloud started long before any of us, or most
of us were likely alive.
1961.
This was the really early days.
John McCarthy, it's often said he opined one
day computing would be a public utility.
This means you can turn it on or turn it off
and you only pay for what you use.
That was a really brilliant observation in
a world in which it was still a single mainframe
at a university or a particular center.
And in the '60s you sort of had this evolution
of technology as well, so we had the microprocessors
came about, we started to think about how
we connected computers, but on the benchmarking
side we needed a way to measure these platforms
in order to know is one better than the other.
And the reason why this mattered is if you're
going to buy a million dollar machine, you
want to make sure that it's very cost efficient
for the type of computation you need.
So here you see the advent of really early
benchmarks like the Gibson Mix, ADP and PCM,
which were in the 1950s to 1971 and the first
benchmarks basically looked like this.
There were instructions computers would run
for you.
They did statistics, they took all these programs
that would run, they figured out which operations
were most frequently used, and from that they
created a distribution, a very sensible thing
to do, and then they would just run this distribution
of instructions over and over and get some
measure of what performance looks like.
And so here you can see in this particular
example, the most used instruction was a floating
point add and subtract which tells us computers
were doing some type of early scientific computation
because of the floating point.
If it was financial it would have been some
sort of fixed point notation, which was used
early on.
A lot of the data I have in the slides come
from the site Roy Longbottom.
He was a U.K. assessor -- I would call him
a researcher for the U.K. government, and
the government would collect all these stats
about data, and he published all these metrics.
I turned them into graphs, so I want to give
him credit because he has amazing data out
on his site.
So what does this data look like?
The data looks like this.
So for the Gibson Mix, it was run from the
early days when it was first invented all
the way up to about the mid seventies.
And here you have some really interesting
things that show up in the data points.
You have Moore's law.
You can see Moore's law showing up in the
progression where the data is kind of going
up and up.
But you have something else that's interesting
in this Gibson Mix.
And by the way, this Gibs KIPS, K-I-P-S, is
kilo instructions per second, so that was
like thousands of instructions per second.
So you see these points that float above the
graph, and I have this arrows that says &quot;error
or ingenuity.&quot;
And my guess is this is the first time we
started to lie with benchmarks.
Some really bright engineer, she figured out,
hey, we can optimize like this, and if we
run the control mix like this, we can make
our platform look astoundingly great, and
the fact that it's so far off of this Moore's
law curve, I don't know, it could have just
been an error in the data.
My guess is ingenuity.
Which takes us into the '70s and '80s.
So if the early part was around, like, instruction
sets and mainframes, the '70s and '80s were
about this growth of ubiquitous computing
around networks.
So we saw the advent of ARPANET in government
and universities.
We start connecting up machines and mainframes,
and we start to get this notion of, you know,
using resources at a distance, kind of.
They weren't quite used that way yet but the
thinking was there to do it.
In the '80s for sure we had all kinds of consumer
services, bulletin boards, BBS.
Things were connecting up.
We started even talking about how do we put,
you know, groups of machines together.
The benchmarking world was equally interesting.
So in 1972, one of the -- sort of the next
generation benchmark called Whetstone was
invented, and it was really trying to get
away from this instruction set mix and computation
that was going on at the time and say, well,
we have all these languages.
You can program in any language.
How do we assess how platforms are doing with
particular languages?
And so that progressed for a while, but you
can imagine there's a lot of complexity when
you think about how do you compare, you know,
Java and C++ even today.
It's more of, like -- more about the platform
underneath.
We also saw early HPC, so high performance
computing.
You know, parallel processing type workloads,
like LINpack which were doing big Gaussian
eliminations trying to find eigenvectors,
is your bridge going to fall down?
So they were trying to assess how good is
a platform at solving these types of problems.
As a joke on Whetstone, there's an evolution
in benchmarks from the '70s to '80s which
was called Dhrystone, and Dhrystone -- You
get it?
Whetstone, Dhrystone?
So Dhrystone was this evolution, and what
they were realizing is you can't just look
at algorithmic computation.
A lot of what we're doing is memory access.
So Dhrystone really tried to add in pointer
indirection and memory gathering as part of
the computation.
And you'll see why this was so important in
a slide or two.
Along the same lines we have graphics.
Graphics is starting to bud in the '80s, and
getting more ubiquitous.
People have figured out how to, like, optimize
for Dhrystone, so then suddenly you have Dhrystone
2.0 because that ingenuity shows back up.
And so let's take a look at how this looks
in the data.
So Whetstone.
So that was the original.
One thing you could learn about benchmarks,
benchmarks run for a really long time.
You think that they die, but for some reason,
they seem to have a long life.
So Whetstone was run, literally, from the
'60s even until today you can go find results
of people trying to run a Whetstone benchmark.
Typically, now, you'll see other benchmarks
that are there.
And I'll show this in a slide or two.
What was interesting about this data, though,
is the U.K. really looked at mainframes in
this period.
And mainframes are important because of this
evolution towards cloud.
But they literally just stopped running it.
So sometime in the '90s, mainframes are dead.
Long live the PC and connected systems.
And it grows up from that point on.
So we kind of see this transition from the
blue dots to the red dots.
Also on this graph, I threw in there, you
know, when Google was founded, so computation
wasn't all that powerful at the bound.
When EC2 was founded, you see computation
was much more powerful when the first cloud
services were launched to the public.
Not only were we having this evolution of
benchmarks and this move from mainframes to
sort of more workstation type machines, which,
by the way, at that time period were $50,000
apiece to just stick this thing under your
desk.
It was quite expensive.
We had this evolution in processor technology.
And so here I have a graph showing why this
was important.
So this graph here basically shows Moore's
Law for computation, which is the red line,
which is the steep line that goes up.
And there's another law, I can't remember
what it is, it's not Moore's Law, but a very
similar one for memory, where it's not quite
as steep of a curve and we hit this inflection
point sometime in the '80s where processors
can process data quicker than we can stick
it into the processor.
If you look at this graph, can anyone tell
me the first time an L1 cache appeared in
the platforms?
Anyone?
&amp;gt;&amp;gt;&amp;gt; (Off mic.)
&amp;gt;&amp;gt;Anthony Voellm: Read around 1990, if you
go back and look, the first time we see Intel
processors is right around the end of the
'80s.
And the reason they had to do this is because
they had to have some way to keep data close
to the processor and make it fast.
So we have this evolution not only in processors,
but we have this evolution in how we think
about systems.
And so before, we thought about systems as
islands.
We'd put our computation on them.
We'd run it really fast.
We'd get our results.
But in 1985, you start to see, you know, papers
talking about distributed applications and
why aren't these things coming about.
And the big Achilles heel at the time was
the network.
So to have cloud, I often say there's only
three things that matter in cloud.
You ready?
Networking, networking, networking.
Without a network, there is no cloud.
And here, they're sort of recognizing the
fact that if we can get fast networks, we
can move things from where they are now in
terms of computation and what are running
into a remote location.
So this was sort of an interesting paper.
Well, this led into this whole, you know,
series in the '90s around MPI computation.
So MPI is message processing interface.
It's a way to write distributed applications
in a cluster.
It uses multiple machines.
It makes it much easier to do this.
Used in scientific computing.
So you see things like MPI come about.
PVM actually had happened, it was called parallel
virtual machines.
It was really pulling also clusters of machines
together.
That happened in, like, 1989.
And then you see Beowulf clusters coming in
for Linux, Linux was sort of taking off and
they had this notion of let's just create
clustered computers.
Why do we need a mainframe if I can get the
same amount of computation for a tenth the
price?
So this happens all throughout the '90s.
And at the end, I'm sorry, Ramnath, I'm going
to try your name, Ramnath Chellappa has the
first definition of cloud computing.
So if we look back, John McCarthy talked about
having this elasticity property and utility.
Ramnath is often cited as the first person
to actually put a definition to cloud computing.
On the benchmarking side, things were a complete
mess.
We had databases.
They were growing.
Data was outgrowing what we could consume.
We didn't know how to measure who was better.
You know, was it, you know -- you know Oracle?
Was it Microsoft?
Was it Sybase?
Who had the best database?
DB2.
There were many databases.
Well, the group of individuals that were,
you know, doing a lot of critical thinking
about this came together.
There was a conference that happens every
couple of years where, you know, people come
and talk about transactions and computation.
But out of the series of conferences came
something called TPC-C.
This was one of the first well-established
database benchmarks, led by a guy named John
Gray.
And this put order to chaos.
So this sort of plays into my idea of understanding
cloud, and measuring cloud is a bit of chaos.
How do we put order to this?
The way they did this with databases was to
establish a benchmark.
By the way, TPC-C 20-plus years later is still
highly used and cited.
It's been tried to be displaced by E and others.
It just never happened.
But C is really OLTP.
This was transaction processing.
We still buy stuff, and so these are still
relevant benchmarks.
Video is sort of growing up.
And you see early sort of Spec benchmarks.
Spec 92 is actually Spec's first computation
benchmark.
This is where you see yet another transition
in benchmarks where we move from, you know,
trying to measure instructions, you know,
to sort of this language thingy to sort of
looking at other types of attributes about
the machine and memory, to sort of Spec, which
is, you know what, maybe it's the workloads
that matter.
Maybe these microprimitives aren't giving
us enough information.
And in Spec 92 was around LISP and other things.
By Spec 95, which was probably one of the
most popular Spec benchmarks, was recommend
around getting to other types of workloads,
like Maxwell equations, Gaussian elimination,
water flows, mesh topologies.
And even interpreted languages like Perl show
up in here as a measure.
But we're seeing other transitions that were
happening throughout this time period.
So you think about networking; right?
Networking, networking, networking, the only
three things that matter in cloud.
Well, it turns out, in 2006, we had actually
ratified networking standards for 10-gigabit
networks.
But here we are many years later, about eight,
and who here has a 10-gigabit network at home?
Anybody?
Nobody?
Who has a one-gigabit network at home?
A few of you.
You're very lucky.
I wish Google Fiber would come to my neighborhood.
You start to see, like, gigabit.
So we've had the technology around networking.
It was available in server rooms.
It's being used.
But there's enough networking capacity today
to these data centers that, you know, it's
quite usable.
It will be awesome when we all have one-gig
and ten-gig connections everywhere.
You know, video benchmarks continue to progress.
Not quite as important to cloud.
There are some workloads, you know, AI or
other workloads that make use of GPUs.
What's really interesting about graphics is,
it was one of the places where the most ingenuity
around benchmarking happened.
And you see these amazing points where suddenly,
you know, they go straight up off this curve
of progression, which was pretty amazing.
So this group really figured it out.
The other group that's really figured out
benchmarking is mobile phones.
[ Laughter ]
So we go from the '90s to the 2000s.
We have this progression, right?
We're seeing benchmarks come about.
We're seeing the workloads change.
We're changing how we measure.
We're getting closer to what could be cloud.
But we had to have some other things happen,
which was the Internet.
Internet comes about.
We have, you know, further progressions of
things that help us build scalable applications,
like memcache is born.
In 2003, we have Eric Schmidt starting to
actually talk about cloud computing for Google.
Then you had one of Google's first cloud products,
which is App Engine.
And it comes out in 2008.
On the benchmarking side, there's a pretty
interesting progression, but the two things
that I'll point out here are, one is HPL.
So you remember back in the '70s, we were
talking about LINpack.
And this was single machine Gaussian elimination.
Suddenly, we're in the 2000s and now we're
talking about HPL, which is clustered machines
doing Gaussian elimination computation.
So we're scaling this out.
The other thing that's interesting here is,
vConsolidate.
This was one of the first virtualization benchmarks
to show up, done by Intel.
And then also Hadoop.
So we're furthering this notion of clusters
of computers and looking for ways to measure
them.
The way this looks in the data is sort of
like this.
And it forced us into it.
So computing forever, Moore's Law helped us
out.
We had this completely incrementing, you know,
computation.
None of us had to do any work and our machines
would get faster and thus our applications
would get faster.
And if anybody played load runner when it
first came out, the little dude would barely
move.
And I got a 40-megahertz machine and the dude
moved all over the screen.
Things got faster automatically.
Well, if you look at the blue line in the
middle, you see that it sort of goes up and
to the right and then it humps over.
And this was processor speeds.
So while each core in a processor had always
gotten faster, suddenly, they weren't getting
faster.
And then the way Moore's Law sort of transcribed
is, we got multicore platforms, so two core,
four core, eight core in a single processor
package.
But this means we had to get much smarter
about writing parallel algorithms and parallel
computations.
And this sets the stage for cloud, which is
inherently parallel, because you can get multiple
machines, connect them together in virtual
networks, and run workloads on it.
So right around 2006, this is where we're
going.
So now we enter into the 2000s, and we're
almost to this decade.
But in the -- well, I guess we're in our decade
now.
So 2010.
We have the progression of cloud products.
So you can see our icons are sort of growing
up here.
New things are showing up.
Compute Engine with these sort of colorful
brackets there are stacked computation.
SQL is out there.
We have all this computation that's there.
If you look at the benchmarking side, what's
fascinating about this period, it's the first
time ever that a workload that had been traditionally
run in a data center, sorting, suddenly was
more efficient in the cloud.
And so Matt Bar (phonetic) went out and ran
a computation.
In minutes, they spun up 4,000 cores in the
cloud, ran a sort of, you know -- ran a TerraSort.
So a big sort.
In under a minute.
This is the first time that happened.
Suddenly we hit this point where computation
in the cloud was now faster than what you
could spend tens of millions of dollars to
spend yourself.
So this was pretty important.
Well, the other thing that you can notice
in this time period is, Google progressed.
You can tell when the really cool icons come
out just how important cloud is.
So one of the things you can do if you want
to try the Google Cloud, cloud.google.com.
It's a top-level domain.
You can just go, log in, spit out VMs.
This is the whole suite, everything from translation
and Analytics to computation.
So it has also grown up.
And having, you know, a -- you know, a more
complete set of services is important to moving
workloads into the cloud.
But I'm here to tell you, the way we have
measured things for over 50 years is about
to fall apart for us.
And here's why.
The way we've often looked at measurement
is like this.
You can think of this as xUnit, Tony unit,
whatever unit.
When it comes down to measuring things, we
all generally fall into a very similar pattern.
If you're going to go benchmark something,
you have to have things set up.
So this is sort of the setup phase of what
we're doing.
So you have a setup phase.
The next thing you're going to do when you
benchmark is you warm up.
You're going to do go run a bunch of queries,
you're going to go run some computations,
send some networking packets, store some bits
on a disk, warm things up, make sure the system
is really ready.
Then you might have some sort of preexecute
phase, which is, you know, not quite what
you want to measure, but it's not really warmup,
but it's just getting things ready to sort
of collect and measure.
Might be measurement steps.
Then we execute things.
And we execute for, you know, minutes or hours
or days, whatever it is, trying to really
get a good measurement of what is this.
And the reason why we want to know what this
is, is, we're trying to predict, if we go
build this new architecture that we have for
our application or our automation or whatever
it is we're building, is it going to scale
to meet, A, our user demand, what customers
want from us, the way we want the system to
work.
And so we spend time trying to figure out
what are things that look similar to my application?
And how do I go measure these things on this
new architecture platform so that I can project
if I'll be successful or not.
And so we spend a lot of time on execute.
Well, then when we're done with execute, we've
got our numbers.
We do some cleanup.
We tear all the resources down, and we publish
the results somewhere.
So we put the results out.
So these are the measurement steps.
Can anybody tell me what's wrong here for
cloud?
What happened?
&amp;gt;&amp;gt;&amp;gt; Not accounting for network bandwidth and
caching, trying to spin things up (off mic).
&amp;gt;&amp;gt;Anthony Voellm: Bingo.
So I'm going to key off the second part.
The two things that were said, A, we're not
accounting for networking.
Maybe.
But the second thing is, we're not accounting
for spinning up things or spinning them down.
So if networking, networking, networking,
is the most important thing about cloud, the
second thing that's the most important is
elasticity, elasticity, elasticity.
It's a utility, I turn it on, I pay for it.
I turn it off, I don't pay for it.
Guess what we're not measuring here?
We're not measuring the cost of provisioning
and deprovisioning resources.
We're only measuring this very tight inner
loop of, oop, here's how fast networking is,
and we're done.
If it took you two days to get the machine
up, you run this computation for a couple
minutes, and then you tear it all down, that's
not very efficient.
So what you need is a system that's very -- you
know, you need a system of measurement that
accounts for this most important property.
So thank you.
So here it is.
We've been looking at the wrong things.
And so if the measurements you typically look
at when assessing cloud providers is only
this really tight inner loop, you're missing
an important part of the computation, which
is how fast can you get what you need and
how fast can you give it back so you stop
paying for it?
That's an important part of the equation.
So the things that we want to measure, though,
as part of this execution, in addition to
this elasticity property, there's four core
properties.
Throughput.
Latency -- these are pretty standard.
Overhead.
Overhead is important in the virtualization
world, because the providers are going to
use some amount of your resources to support
whatever thingy you get.
So if it's a virtual machine, there's probably
some amount of computation that's being borrowed
to do, you know, storage or networking or
something else in the cloud.
And so you want to sort of pay attention to
overhead.
And the other part is cost.
Cost.
Cloud, it's awesome; right?
Everything is sort of billed by the hour.
It's ten cents an hour or whatever it is.
At the end of the month, sometimes it feels
a little bit like death by credit card.
You get all these little transactions, and
at the end, you're, like, whoa, where did
that $100 bill come from?
So cost matters here.
And this is why elasticity matters with cost.
Because if you can provision and deprovision,
you can control your cost.
If you're in this mode of just spin up resources
in the cloud and leave them forever, you may
be better off doing something else, like -- but
there's definitely benefit to somebody else
hosting it, and it can get geolocated.
There's lots of benefit.
But you should think about the elasticity
property so you can really make the most of
it.
This is particularly important for a lot of
the conversations that we will hear today
and will be held tomorrow -- or -- yeah, or
later this afternoon.
So what are we going?
This is the interesting part.
What we don't want to do is go out and say
in order to get a fair assessment of cloud,
we should take everything from everywhere
that's ever been done and go run it and get
a number.
That wouldn't really serve us well.
There is such a thing as too much choice is
a horrible choice.
And so you want to be very selective about
why are you choosing the things that you're
choosing.
So one of the first properties as we looked
at tackling this problem for Google was how
do we keep things minimal?
What types of workloads are important?
And there are even really important ones on
here that I don't have, like SaaS bench, which
measures, you know, other types of systems
in the cloud around database and, you know
-- you know, managing people and that kind
of stuff.
But there's lots of benchmarks.
So the first thing we had to do is think about
how do we keep things minimal.
What do we want to focus on.
For sure that elasticity property is going
to be part of anything we do and measure.
But we have to choose other workloads.
And I'll show you some examples in the demo.
The second thing we have been thinking about
is, the things that are being built today
are unlike the things we saw in the past.
Games like Candy Crush, when these are a billion-dollar
game, it looks very simple to us on our phone,
that doesn't exist without a huge amount of
telemetry going to a back-end and figuring
out if Mary, does she want to buy the blue
-- you know, have the little blue crystal
show up or the little green crystals?
We find out that if we put more blue in this
thing, she clicks on blue more, so we're going
to put more blue on hers, and this will lead
her to play the game longer and lead to some
sort impression on an ad or whatever it is
to monetize.
Or, hey, maybe John likes the pink crystals,
so we're going to sell the pink crystals to
John and say, hey, you know, for ten cents
more, we'll put pink crystals in there.
And this creates delightment for us.
It's trying to engage us.
But what this takes is massively parallel
systems in the back end doing computations
against streams of data.
There's millions of users streaming all this
telemetry in.
It goes into these big back-end cloud systems,
and they get crunched on, right, to sort of
figure this out.
So the thing we have to think on about workloads
is are the traditional workloads going to
really address how we compare clouds or think
about how we might compare clouds?
And so we've looked across, you know, Hadoop
workloads, which are, you know, popular for
handling these types of computations in streams
and running benchmarks.
We've looked at, you know, how might things
get stored in the cloud.
You know, object stores are very different
than file systems.
You know, they're write only, typically.
They have no locking.
There's all kinds of things that you don't
get.
You get a shared name space, which is great,
but they're different.
And we're sort of looking out around how are
the topology of the things that we're building
actually fundamentally changing.
So are things running in single zones or across
zones.
So if you think about somebody like Netflix,
Netflix doesn't ever want to see a failure.
So for them, they are stated as using a cloud
provider.
They put everything in the cloud.
Everything that streams comes from a cloud
provider somewhere.
But they talk about, you know, we need to
be in multiple regions of the world and multiple
zones so if one goes down we can keep playing
videos and half of America won't go up in
arms because they can't watch, you know, Pinkie
Pie at 8:00 o'clock in the morning.
And if anybody has kids, you'll appreciate
that comment.
There are other factors, in terms of building
benchmarks or building ways to measure and
analyze things.
We have really paid attention smaller to what's
going on in terms of blogs.
What are all of you talking about?
You know, what is technical sales talking
about?
Who is coming to the cloud?
Why are they coming to the cloud?
How do we make everybody come to the cloud?
My personal opinion about the cloud is we
will get to the limits of human imagination
for zero cost.
That's why I work in cloud.
I think that is the future.
We look at the industry analysts, where are
they going.
You know, third-party reports.
So we're trying to get all this data about
what's important.
It's amazingly hard to figure out what's important.
But other places we look are like what do
providers say about themselves?
What do we want?
Well, what I want is to default.
You install something, you run it, it runs
great, you do nothing.
I'm kind of lazy that way.
I think it's very hard to figure out how to
tune a database or tune some no SQL cache.
We shouldn't have to do that.
It should just be done.
But we also want to pick things that are very
transparent to the industry.
So if you're going to measure something, you
want people to understand why you're measuring
what you're measuring and how it's being measured.
And so we looked at a set of properties that
we had around benchmarks we wanted to look
at for ourselves.
You know, ease of use was important.
Developers, we hate sort of running stuff.
So if we can make it easy, we'll run it more.
We wanted them to be free to use.
Just it makes it easier.
We wanted to be relevant to our customers,
so we had all these sets of properties in
this third column here that we're going after.
And then of course we want to stick with our
methodology.
The methodology has to be really adhered to.
Measure everything, measure the elasticity,
and just keep -- you know, rinse, lather and
repeat over and over.
So we're going to stick with our methodology
we talked about.
We actually built a system internally for
us, and I'll show you a demo of it, where
we measure the cloud, we look at the provisioning
aspects and so forth.
We have ways of processing and sort of passing
results around.
We actually use our own products.
We built or data repository on Big Query.
We had billions of data points of, you know,
performance measurements and we actually run
Big Query to boil this all down and give us
a single answer and computation.
Nothing happens without pretty visualizations.
As engineers, it's hard to look at, you know,
tables, so when you see graphs, they really
help out so we have systems to look at graphing.
And this top left graph here is networking
improvements over the last year for Google.
We went from, like, a thousand -- you know,
one gig that we could sustain VM to VM throughput
up to now, which isn't even shown here, all
the way up to -- well, I'll leave that to
the cloud live event neck week.
If you haven't registered for the cloud live
to see it online you should definitely watch
the cloud live event.
But let's just say what Google can do with
networking is impressive.
So with that, I'm going to fearlessly attempt
a demo.
Let's see if I can get this working.
All right.
Looks good.
Look at that!
You can tell I'm very creative in naming my
machines.
Voellm dash Macbookair.
My last one was VirusinSpamnet, but I got
so many emails from security over that, I
decided when I upgraded probably enough not
such a good idea.
So what I'm going to do is show you an example
of basically measuring provisioning time in
the Google Cloud.
So we have this tool called Artemis.
It's our internal benchmarking tool.
This is what we really use in order to look
at measurement.
So what you see here is it's going to kick
off, and you'll notice a couple of things
as this runs.
One is I use my own project.
It's targeted at cloud - GCP.
That's Google Cloud Platform.
The benchmark that it’s running is just
a basic boot test.
So let me go boot a VM in the cloud.
In fact, this one is booting up in U.S. central,
so somewhere in Iowa little electrons are
flowing into a data center to give me a machine.
It's going to boot, basically do a little
check on the machine, and then tear it down.
So one of the things we did with this tool,
though, that I think was important is we built
a tool that actually works like a human.
And if you look at the instructions that are
running here, it's running this tool called
gcutil.
This is really the tool we provide to everybody
for cloud -- for getting resources in the
cloud.
There's actually a next gen one called GCloud.
I just didn't have time to update this for
this particular discussion.
But these are the command line tools, and
so what's cool about this is you literally
get as a developer, a tester, an engineer
instructions that you can just copy and paste
instruction by instruction and rerun the benchmark.
So when I talk about being transparent about
what's happening, this is what I mean about
let's be this transparent.
Here's exactly what I'm doing.
And so here you can see as it spins up, it's
targeting things like -- let me go try the
pseudo app get.
And so there's a pseudo app get update.
It's running an update inside the VM.
Let me get all the latest bits that are out
there.
I run an update, and then after it does the
update, it starts to tear things down.
In fact, we actually have the boot time measurement
encoded in this blog where it says boot time
39 seconds.
But guess what we're waiting for now?
What are we waiting for?
Yeah, the tear down.
Deprovisioning.
Did you see how fast provisioning was?
It was fast.
We got a VM like that.
The tear down, a little bit longer.
This is where we're making decisions do you
want to delete your disk?
If you really want to delete your disk, we
better make sure that data is really gone.
We do all kinds of encryption in the cloud
to make sure your data is safe and secure.
And so here we're tearing things down.
You can see the deprovisioning step.
Delete the firewall, delete the disk, delete
the instance.
And here in about a couple of seconds we see
how long this took.
So it literally took us, from the time I ran
the first command to all the way to being
able to log into the machine and doing the
app get update took 39 seconds.
And for Google if you run one or a thousand
of these, guess what?
It's not that much different.
Can you imagine what you can do with a thousand
machines in, like, 39 seconds?
So one thing I usually tell people when they
come to Google and they ask me what's the
hardest thing about working at Google?
My personal opinion is the hardest thing about
working at Google is it's terrifying to think
everybody has this super computer at their
fingertips, and how am I going to make good
use of this resource?
It really puts you at the limits of your imagination,
as I mentioned before.
But our whole end-to-end thing took about
two minutes.
127 seconds.
I round down.
I'm a perf guy.
It looks better, two minutes.
So that was one example.
The other thing we did with our tool is we
recognize whenever you have an automation
system, you have to think in incremental parts,
like how are engineers going to use this tool?
So we could run this.
It takes a while.
But there may be things we're doing, we may
be tweaking experiments on virtual machines,
there may be flags we're flipping trying to
say, hey, can we change behavior in the network?
Can we change behaviors to storage?
How does this look?
You don't want to always have to wait for
all these resources to provision and deprovision.
When you get your final measurement you do,
but when you're engineering your automation
system, you don't want to have to wait for
that.
So what I did earlier was I actually booted
up a couple of machines.
And so Artemis runs in four stages.
It has what is called a prepare stage, the
execute stage, the tear down stage and the
published stage.
It runs kind of like the test methodology
I mentioned before but we made it increment
jam.
So what I did earlier, actually last night
is I kicked up two virtual machines that are
also running in Iowa.
Already running, and what I'm going to do
now is just literally run how fast is it to
run VM to VM networking on these machines
in Iowa.
And so it's going to run this test called
iperf.
Iperf is just a common measurement tool that's
used out there for benchmarking networks.
So it spins up, runs for five seconds, dash
dash time equals five, run my iperf for five
seconds, and boom, it's done.
It tells me two measurements here.
So in Compute Engine there's this thing called
an external and internal IP that you can run
over and an internal IP.
An internal IP is a machine data center IPv4
address, like a ten-dot address.
So you can see if you select an external address,
it literally routes our traffic to the edge
and back in.
So from one VM, it's going to the edge of
the Google network and back in.
And for the data center address, the ten-dot
address, it's literally going VM to VM.
So here I'm showing two gigabits per second
on a one core VM.
So this is an N1 standard 1.
So I have one logical core running, and it's
able to do two gigabits a second machine to
machine, which is the highest number in the
industry core for core.
It's impressive.
All right.
So I'm going to switch back.
So this is the provisioning.
And then if I want to do the deep provisioning
I can just say run stage dash dash cleanup.
But I'm not going to attempt the demo.
I'm going to switch back.
Which I think I hit one -- all right.
So with that, I showed you the automation
that we run.
And so how can you get involved?
So here's what I would like to do next.
We've obviously measured our own cloud.
We've thought a lot about methodology.
We've thought a lot about workloads.
How does that help if you you're trying to
make decisions in the marketplace about where
to put your automation systems?
Well, what would I love to do is for all of
us work together.
Whether we use GitHub or something else, we
can figure that out later.
We -- You know, we want to obviously work
across the industry.
That means you.
We want people to suggest or delete benchmarks.
So there's been this notion in the benchmarking
world that benchmarks have a version and they're
done.
I think things are moving much faster.
To put benchmarks into a standards body for
three years and be wait for something to come
out is going to be a challenge.
I think benchmarks should be more fluid and
that means we're adding and deleting things
that are relevant as we think about cloud
and as cloud is evolving.
And this is sort of the last point which was
really we need to evolve benchmarks for cloud
as the market evolves.
We really don't know honestly where cloud
is going.
What I do know is most of what's going to
be in cloud isn't here yet and it's going
to take all of you and all of everybody who
is kind of watching online to move the workloads
there.
And as that happens, we'll understand it better
but we just don't know yet.
So with that I invite you to send me an email.
I put it really small.
Voellm@google.com.
If you're interested in benchmarking and want
to try to do something that's more open and
shared, we'd love to talk to you.
So with that, thank you.
[ Applause ]
&amp;gt;&amp;gt;Sonal Shah: Thank you, Tony.
&amp;gt;&amp;gt;Alan Myrvold: So we have a question from
the moderator.
In your presentation you talked about benchmarking
against different cloud providers.
What about benchmarking the same application
over time within Google Cloud to see if you
can find regressions in Google Cloud?
&amp;gt;&amp;gt;Tony Voellm: Absolutely.
So if you remember the pretty graph I showed
you with the dashboard, that's what we do.
So we run Artemis multiple times a day, day
in, day out.
Depends on the workload.
Some workloads don't change that often, some
change a lot, so it's continuously running.
We actually use Jenkins to run the automation.
Seemed like a really good tool, so we just
used it, and it goes and scripts around running
Artemis, and then we publish all this data
to our back ends which are all running on
cloud.
So yeah, good question.
We definitely can find regressions, unfortunately.
Any other questions?
No?
All right.
Thank you.
&amp;gt;&amp;gt;Sonal Shah: Thank you.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>