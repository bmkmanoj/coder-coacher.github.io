<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Seattle Conference on Scalability: Lessons In Building... | Coder Coacher - Coaching Coders</title><meta content="Seattle Conference on Scalability: Lessons In Building... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Seattle Conference on Scalability: Lessons In Building...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v0nVKlC-haI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay our next speaker is another Googler
from our Kirkland engineering office
working on a totally different project
which is google talk lessons and
building scalable systems oh say your
name this is reza the fruit good morning
everyone I know this is the last session
before lunch so the more i speak the
later you'll eat so to motivate you guys
to actually pay attention to me i put a
little puzzle in one of my slides and
there's a little small price for it so
you guys can actually pay attention my
name is reza berries I've been at Google
for three years I primarily have worked
in our communication products I spent
most of my time on google talk from its
beginning I've also worked on gmail
google groups and orchid and some
infrastructure that's used by our
communication products so before I get
started and talk about you know some of
the lessons that I've learned in
building scalable systems I just want to
give everyone a quick overview of what
we built with google talk so we started
by building you know the actual service
that uses you know the jabber protocol
and an IM client with a bunch of
peer-to-peer features and i'm not going
to talk about that at length and after
we launched at in about six to eight
months after that we integrated the chat
experience directly inside of gmail and
we really focused on blurring the lines
between these two different forms of
communication so we added some features
like you know archiving all your chat
messages and you know when you look at
the reply from somebody you could
actually reply back to it not in as an
email but as an IM and you know there
are few other features like that that we
did with the Gmail team after we did
that that launched we went and worked
with the orchid team too so orchid is a
social networking application that that
Google has is incredibly popular in
Brazil and in India and and it looks
just like any you know it looks like the
rest of them and you know we integrated
I am experienced directly inside of this
and has actually been very popular in
those countries and as you can see here
on this orchid page you know you see all
these green and orange balls and they
indicate users presence and you can
actually click on those and chat with
those people
so this application gets a lot of page
views and every page view on this needs
to fetch the presence of you know 10 to
12 people and more recently we've
launched an embeddable talk gadget that
you can put on on your igoogle page
which is the customizable you know which
is a personalized homepage for google
com and you can also put this on any
other web page that you have one of the
other features that it has is that you
could do in line media so if I in this
example somebody sent me a link to a
YouTube video and I can just click play
right there and actually play the video
well if you know the answer so the
puzzle is that that i was referring to
is right underneath the link to the
video anyone anyone you know oh well i
guess people in the front have an
advantage but it's basically well i do
prefer people to sit in the front and
they're all empty so i think one person
read it so it's a good question is what
is the answer for what is e to the power
of I times pi so if someone knows
answers you can just scream it out any
time thank you I think you were the
first one so come after the talk and
I'll give you a t-shirt so at a high
level this is what we built we have you
know Google Talk service that's used by
all these different clients so we have a
bunch of web clients like Gmail an
orchid in iGoogle page there's the win32
client they're a bunch of third-party
jabber clients like iChatting game and
trillian that also can work with our
system and a bunch of mobile
applications like in on the blackberry
and on the nokia devices and Sony and
things like that and we also use the
jabber protocol so we federated but all
the other networks in that domain and
and livejournal and jabber organ
earthlink are a few examples so most of
my talk the rest of the soccer is going
to be talking about the google talk
service which is basically set of
servers that we built to handle
messaging and presence and let's just
talk a little bit about some of the
challenges that we faced so the work
that we do is very different from you
know search and in other
Google we handle a lot of small packets
every single day for messages and for
presents so in a typical day you know we
have you know millions of users we had
we handle billions of these packets that
go through our system and we handle them
you know less than 100 milliseconds and
at our peak you know that the peak hours
you know the number of the QPS on our
servers is in the order of hundreds of
thousands so that's actually pretty
significant even by Google standards i
granted the requests that come to us are
a lot smaller than you know didn't you
know but with it what the document
servers and things like that handle but
regardless is still a lot of traffic
another thing that we have is because of
the way you know we've decided to build
the application all the packets to and
from users need to go through both users
endpoints and both of those endpoints
need to you know apply some business
logic so if I'm sending him I a message
to Amanda you know my endpoint is going
to decide to let's say archived ichat
message and her endpoint is going to
decide if whether or not I have the
permission to even center a message so
all everything needs to go through both
both of these endpoints and another
thing I was very challenging for us is
we after we launched the system you know
obviously it's a brand new service it
doesn't have a lot of users and then we
integrated with gmail which was a pretty
big system at a time and later with
orchid which is another in a very big
system and those things are very
difficult to do because over 90 have to
take this little system that worked
really well for your for your existing
user base and scale at 10 x times maybe
50 x times to handle a lot more users
and i'm going to talk about some of the
things we did it we did to make those
overnight launches actually successful
and finally one of our challenges has
been that we've had to work with a lot
of different types of clients and
interact with a lot of different Google
services that you know even at the
beginning we didn't know that we were
going to integrate with so these are
some of the insights and lessons that
I've learned or eight of them and i'm
just going to just go through them the
first one is measuring the right thing
sometimes people come to us at Google
and say you know so how many I messages
do you deliver or how many active users
you have and I agree those are very
important product questions that measure
how successful you are
but they're not really right engineering
questions that that that can tell you
how busy your servers are so in our
world the hardest part of what we do is
managing everyone's presence and showing
the right presence to the right people
and enforcing all the access controls
and privacy policies we have with
respect to that so the number of
presence back is in our network is the
number of connected users that we have
times how many people they have on their
body lesson how often their state
changes so when I log into my gmail
account i might have you know 20 buddies
and all of them need to now know that
I've come online and their presence
needs to be fetched in you know I need
to see it and ten minutes if I step away
for lunch you know for ten minutes my
presence goes away and again everyone
needs to be updated so that number works
out to be you know billions and as you
can see that growth is not linear within
with the number of active users that you
have for example when we integrated with
orchid they have a lot of friends so
that really changed you know the amount
of traffic that we had in our system
even though the number of users only
went up you know so much and you know
it's to the point that if tomorrow while
users decided not to send any im's or
send three times as many messages to
each other it really is not going to
make that much difference in our load
since most of the work that we do is
with the presence packets now let's go
back to the gmail and orchid launches
and talk about what are some of the
things that we did to actually make
those launches successful so weeks
before we launched either of those
services we actually launched our
servers in productions and our servers
we're talking to the gmail and or to the
orchid servers in production out
everything was happening and but none of
it was displayed to the users this
helped build a lot of confidence in what
we had and also help the other team you
know have confidence in what we had and
with things like this we generally do a
small percentage experiment you know we
start maybe with one percent or even
maybe a tenth of a percent and slowly
ramp that up to you know larger numbers
so it orchid in particular for weeks
before they we launched they were
calling our servers to fetch presence of
people and most of the times answer was
oh no they're not available but at least
we knew that we can handle that much
queries from them for second and we knew
that at least from a networking
infrastructure there was not any
bandwidth problems between our data
centers and also in our servers failed
and we went to another data center and
things like that happened that things
were still working correctly with gmail
with it we did a similar thing when
users started logging in to check their
email and logging out we simulated them
coming online and offline on our network
and we kind of simulated all the packets
that we need to get sent just to see if
we can handle all of that so doing these
things you know for weeks and months
before a launch really helps us be
prepared you know for that night
warriors turn on this flip and all of a
sudden you get a lot more users so one
of the things that you know every
everyone today I think I've talked about
is you know about google is you know
sharding you know some people use the
term partition should refer to the same
thing but it's basically a way to divide
up either your data or your traffic or
in our case really both of them across a
set of machines and in a Google Talk
case you know the endpoints that I was
referring to earlier are shorter to you
know different set of servers and one of
the things I was that's very important
that you know that I've lunch and I
think most things I google do a pretty
good job but is that the number of
shards you have need to change most of
the times you add more of them but
sometimes you move to newer machines
that are much more powerful or you
really improve your code in which case
you can reduce the number of charges
that you have and you know doing any
sort of recharging like that needs to be
done you know in an easy way with zero
downtime and that's how that's always a
challenge you know when you design a
system to actually make it like this an
order to lessen and I think adding all
of them that I talked about I think this
is probably one of the most important
ones is abstracting your systems from
each other so there's so that the Gmail
team doesn't know that much about our
complexities or no orkut team doesn't
know that much about our complexities so
and this is even more important windows
when you know the systems are owned by
different teams that might even be in
different offices for example g- gmail
and orchid don't know which data centers
the google talk servers are and they
don't know how many servers we have they
have no idea what our sharding logic is
what our fail
logic is and that's all great because we
can change any of those without ever
having to go to them and say can you
change me to make this configuration
change on your side or can you update
your code with this and we've made their
product you know the API is between us
in a way that we can just do everything
that we want on our side and it's
actually is great so they made the main
way we did this is we built a set of
gateways that are yes no so up so what
we've given them is a logical name for
where we are there that there are you
know the infrastructure that we that
they use to make calls to us does a
runtime discovery to find out which data
centers were in and so if we saw last
week in fact with orchid we moved our
data centers and we we brought up this
new data center and during they took the
other one they made no changes on their
side and the RPC infrastructure was
smart enough to slowly route the traffic
to the new data center and this kind of
thing happens a lot you know inside of
Google and it works really well another
lesson that was important was that you
know a lot of times many building
applications you're not you know you're
not really thinking about the details of
what happens inside of gfs and big table
and some of the other systems that were
you know using underneath but it's
important to be really aware of them
because it is going to impact them for
example with your RPC system you know
you want to know if if you know if gmail
is making RPC calls with these gateways
that I was talking about are they going
to or all the gmail server is going to
create a TCP connection to all of our
servers or a subset and how do we
configure that or is there any health
checking involved could that be a
problem is it is it just too aggressive
so those are the things you need to be
aware of and tune it even goes a layer
lower than any of the code that we write
for example in IM we handle a lot of
open tcp connections and most of these
tcp connections don't actually have much
activity so the traditional pole and
select operations in a linux on work
really well for that because the running
time of going through the select loop is
order of the number of open socket
connections not the ones that are
actually active
so that there's a new kernel operation
called epoll that does a much better job
with that and you know we you know
decided to use that you know early on
any and it made a significant difference
another problem you know or insight that
we've had to deal with is you know
mainly around operational problems and
I've listed a few here and you know
unfortunately this list is usually
longer than this so when we when we
first launched we have a whole bunch of
graph that showed the number of
interactions between gmail and google
talk and the number of bandwidth that we
use and sometimes wherever spikes in
these things the spikes happen when all
of our users disconnected and
reconnected in another data center
because there was a downtime or when we
pushed a new job to our servers and all
of our servers had empty caches so we've
worked hard I you know to smooth out all
of our graphs so that you know things
like that don't happen so if users get
disconnected it all just happens in a
much smoother way another thing that you
know we've spent a lot of time working
on is making sure that problems don't
cascade into other problems you know if
a server is sick you know they
shouldn't accept work and if you're
talking to a server that you think is
sick you should probably just back off
and talk to somebody else and small
things like that actually go a long way
to make your system very reliable and as
you can imagine everything that I'm
listening here as it has a bunch of
interesting stories behind it that I
that I can't talk about so the next one
and this was kind of an interesting one
was that you know any sort of retry
logic that you have inside your client
and inside of your server could actually
become a deos attack against your own
servers so in a one of the early
versions of the Google Talk client that
was not launched externally this is like
back in the day it it basically you know
when it came up it just checked the
version and it you know again some other
place to see if it needs to be auto
updated and there was a bug in that
where there were these clients that were
attacking that server that you know just
to see which you know which version they
should you know the client should be on
and they were 18 Googlers had that had
these things on their machines
and a few of them were on vacation and
we actually had to go and disconnect
their cables because there was a bug in
the client that was just retrying in a
loop luckily things like that didn't
make it outside of you know the
Googleplex but things like that you know
that are an issue and you should be
aware of them finally another thing that
we've done that has been very helpful is
that we run different farms and servers
for our different applications we want
to make sure that if gmail has a problem
and they take down their gateways that
doesn't affect orkut and vice versa it
allows us to have much better essays for
our clients and doesn't you know and it
prevents cascading problems so i can
probably spend a whole hour talking
about this i'm going to keep this pretty
short so any scalable system is a
distributed system so there's a bunch of
regular distributed systems problem that
you need to solve for example you need
to have fault tolerance in every
component that you have everything that
you think is reliable will eventually
fail and so we've done a pretty good job
with that and another thing you know
that we did is you know when you run
these servers in production you could do
any sort of CPU profiling on your
machine but you want to know what was
happening what slow and production so we
did a bunch of work to actually be able
to do performance analysis of our
servers in production we're not actually
affecting you know their ability to
handle live traffic and that's been
instrumental for us to actually improve
our code another item is really around
monitoring but you know there's a lot
about you know when you run a lot of
different servers when and you have
problems you want to be able to look at
what's happening at your system at a
very high level so what we do at Google
and this is used by a lot of different
teams is that we have agents on every
machine that collect information about
the machine itself of configuration of
the machine the health of your
application the health of the machine
I'm even down to you know the
temperature of the motherboard and all
of this information is sorted and
repository and from that repository you
can have real-time alerts you can have
all sorts of pretty graphs you can go
and do you know time analysis and go
back in time and see what happened and
you know
you can notice that oh well after we
push that binary you know these these
computers are not getting slower and
this other health metric has gone up and
accuse on the gateways I've gone up so
you can actually understand what's
happening and that's been that's been
amazingly helpful you know whenever we
have a problem with first thing we do is
we just go to that monitoring console
and start to look at problems another
thing that's really important at a place
like Google where a user request comes
in and gets this patch too many backends
and those back in talk to other backends
is that you need to follow a transaction
and its context as it comes in and it
goes to all of these places and you need
to log enough of this data so that at a
later time you can reconstruct what
happened and see what's actually slow so
an example of that you know with us
could be maybe there is a particular
back-end server that that has a very
popular bought user bot that's taking up
most of the CPU and it's actually
affecting the rest of the users and the
hard thing about doing all of these is
that you need to make sure that you're
doing it in a non-intrusive fashion and
not affecting your servers yes well I
mean definitely single digits you know
if you for example logging is you most
people log debug logs and things like
that and you know that's you know
generally one to two percent of your CPU
and people think that's acceptable so
with everything here there as non
intrusive as something like logging now
for something like the transactions
there's a lot of different things you
could do you could for example decide to
only do some sort of sampling or maybe
you can always sample one percent but
when bad things are happening you know
increase that for on some of your
machines so I just want this is my final
slide here I just want to talk a little
bit about some of the software
engineering practices that we have that
it's made things very successful for us
one is you know every binary that we
built on our team and almost on every
team at Google we need to make sure that
this thing is both backward and forward
compatible we want lots and lots of
different types of servers and they all
interact with each other in ways that we
can't even really conceptualize it out
ahead so the best thing you can do is
make make sure that you're
anibal because you never know what could
you know you can have an old client
talking to you though you know the gmail
team might decide to roll back their
build and you want to make sure that
things are always working another thing
that we did early on that was very
helpful is that we have an
experimentation framework where we can
enable features for a small number of
users either they could be explicit
people or just google errs or maybe just
the percentage of users and this is not
just features this could be a new
caching scheme that we've implemented
and this allows us to actually
experiment and see what works really
well and finally all of our engineers
and this is true for all of google have
access to the production machines we can
and this is just amazingly you know just
empowering you can you can just go look
at the logs you can understand what's
happening you can push new bills to them
whenever you want and it's great and it
allows you to experiment and iterate
very quickly and it also makes the
system better because you're also in
some ways you know in charge of running
them so so with that any questions
before lunch yes so all of our services
have availability metrics and most of
the services have different tiers of
servers that we have so there's always
the one server that is really a
playground you can put any build you
want on it and it has maybe two test
users and it I mean nothing can go wrong
with with the Moors sorry I have to
repeat the question so the question is
uh sorry so the question is how do you
say I spin at a small one
well we always know what if the
experiment was on for what percentage of
the users and we don't enable them to
let's say ten percent at a given time so
it I'm not sure I mean I'm not sure I
understand your question yes we do screw
things up attic occasionally I am not
there they're definitely been cases
where you know the entire system has
been affected by a human error and I
think that happens for every system we
don't really have a graph of every
experiment that we've ever tried and
every bad thing that's ever happened we
take things like this pretty seriously
and not all of the engineers on the team
have the confidence to you know make a
big change it's usually the people
who've been around a little bit longer
who make their big changes yes can you
talk a bit about your RPC mechanism the
okay that's that's a great question you
know that that is one of the you know
one of the Google infrastructures that
has not been very publicly discussed and
not sure how much of it I can talk about
but that could you know have its own
tech talk so maybe we can talk about it
during lunch or something yes
okay so how do we do profiling on
production servers so so I google you
know our servers are written in
different languages and a lot of the
original work was done in C++ and
there's an just amazing libraries
designed and end up in that world our
servers are actually implemented in Java
so and we were at the time when we
started the project there were only two
or three systems that Google that were
written in Java so we work with some of
the other teams and we built some new
stuff where we can you can go to a
server and say okay I want to start
profiling and feminists later you can
you know look at what's happening
unfortunately I can't talk too much
about it but in jdk 15 and higher it
actually gives you a lot of ways to
instrument what's going on inside of
your server and there's also things in
addition to you know the in-process
profiling that we do there's a lot of
stuff that we do outside of that by you
know so we monitor how much of the
physical memory of the machine is free
how much of the CPU is free and things
like that yes there what charting so the
first step of it is making sure that you
have enough abstractions between your
servers so if the gmail servers knew we
had 300 machines in this data center it
really can't do dynamic charting because
you need to restart all the gmail
servers at the same time and that's just
impossible so the first I was just
making sure there's enough layers of
abstraction and servers be in between
where you can do that for us now is a
four-step process we can do it in less
than an hour we you know if we want to
let's say have twice as many servers we
take down those servers and their backup
servers that are handling the traffic
and then bring up you know twice as many
servers and just change some metadata
configuration and and since we use a lot
of runtime discovery since our clients
use on time discoveries things they will
just auto detect that they're twice as
many servers yes
the different applications do different
things with that and for us there's a
lot of small packets that are going back
and forth and we want the overall
latency to be very quickly so we don't
short across datacenters we want to make
sure that most of the traffic it stays
within a data center because going
across is usually expensive I think
that's it so let's go eat</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>