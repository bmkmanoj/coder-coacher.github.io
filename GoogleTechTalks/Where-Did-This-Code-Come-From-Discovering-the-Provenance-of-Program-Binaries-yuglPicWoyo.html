<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Where Did This Code Come From? Discovering the Provenance of Program Binaries | Coder Coacher - Coaching Coders</title><meta content="Where Did This Code Come From? Discovering the Provenance of Program Binaries - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Where Did This Code Come From? Discovering the Provenance of Program Binaries</b></h2><h5 class="post__date">2011-05-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yuglPicWoyo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so our speaker today is Nate Rosenblum
Rosenblum is a doctoral candidate at
uw-madison you should be defending this
summer his interests are our systems
security program analysis and machine
learning particularly the intersection
of those and such a pity squared to talk
about discovering the providence of
program fighters thank you Scott awesome
right so Scott's out of Canada just
wisconsin-madison work with bart miller
Jerry drew as well I work on the
paradigm project which is a binary
Schools Project you asleep ado
instrumentation and program analysis
binary modification and so forth it's up
spend a lot of time over the last five
years looking at machine code and weird
variations that come up in which encode
and so forth and this is caused me to
think very hard some of the time about
how these variations arise and what it
means really work comes from aspects so
so this is let me to pursue and of
course a researcher comes from now um
people we talk about machine code and
the characters attention code at
cocktail parties that will talk to you
very often but so you probably never
thought about this before or heard
anybody talk about it but but there are
number of other areas where people do
talk about programs that are more
interesting to usually when people talk
about protons they're talking about like
the story of ordination of something
discovered how is stored has transferred
and so on and so these things are very
natural talked about in the context of
fine wines or archaeological artifacts
they're not something that's often
applied to look that in terms of a
program by Harry's Ashoka so so I'm sure
we can have a lively discussion about an
interesting talk about
subjects and maybe if I had studied
those instead of program analysis then I
would be in a warmer place where it was
not winter in a little bit since I've
done the work that i have feel that I
have this topper words are about proven
on civ program miners not not the
specific makes other words so the issue
that we start with here basically is
that in terms of perspective of somebody
who's looking at machine code the
proponents of that code like a program
binary that you find on your system that
shouldn't be there for example is really
a black box but there are a number of
questions and can ask about this we know
something about the way that programs
can generate so for example we know that
programs probably were compiled from
some source on commissioners lot of
different characteristics about this
compilation process asked questions
about what was the Potter family was inc
new or was this microsoft compiled
binary which versions and what options
were used and so forth what was the
source language programming there's also
other aspects of this proton strainer or
hierarchy than we think about which is
things like maybe there's code that ends
up inside of your program static linking
the seson thing about system that you
were on different versions of boost
library for example could show up as
differences oh these are also kind of
aspects of the way programs are
generated things can happen after the
compilation process one of the things
that we worry about a lot in malware
analysis tools that have applied to
office in order to rewrite by
there's also things like IBM compiler
does a BF later I suppose does a lot of
whole program optimization and so these
are kind of the toolchain components but
the way that the program is one of the
things that we think is really
interesting to look at and I'm going to
talk about in this talk is it's also
authorship as an element of protons you
think about it at the highest level
somebody has an idea for a program and
they make much of different choices
about how they're going to implement it
and never a few this this tool chain
process and so the style of the program
is somehow intrinsically linked to the
way that the executable code so this
talk we're going to cover that as well
as some of the research from zone
inferring characteristics of the tool
chain of the vision is to produce
program minors before we get into that I
just want to talk really quickly about
some sort of immediate applications
where protocols questions come up so say
for example that you're a software
vendor and buggy program and I realize
this is maybe not the best so how did
this see Hawaii program and so you have
this deployed in some why distribute a
scenario with its source project and
you're getting back with reports and you
need to figure out of course has to
address these bug report something wrong
to filter and guilty people's computers
crash but what if what if it's not your
fault what if you know for example there
was a buggy compiler to produce this
code or some sort of set up being that
ability with other software on the
system I know for I for one sort of
detect a market not taking mood when I
contemplate this it's not
this is something that's hard to tell
because is if you're open source mentor
for example you don't have a lot of
control over the production process to
your code went through when it was about
then maybe maybe you have it your build
system the district courts are covered
and set up such that record some of the
copilot version was made it so you can
detect whether somebody compiled it with
with a version of GCC that has a bug in
its of three optimization level but your
program doesn't run isolation is linked
against a lot of other soft for me
systems that you also don't have control
over and oftentimes or all the time
don't have annotations a citizen with
them saying exactly what process was
used to produce these things so this is
a place where you might want to be able
to infer the kind of characteristics of
the children that produce the program
the other really pertinent area or where
this comes up is in software forensics
dealing with malicious codes and
software intrusions it's kind of thing
so there's a lot of different work that
gets not of course in forensics you have
to reverse engineer programs that D
compilation try to understand them more
they can be really relevant what kind of
tools were used to produce a program in
kind of operations because maybe you
have security analyses that are tailored
to specific production chains that sort
of thing the other question of course
that comes up in law enforcement and for
as X is this where who did this
intrusion to produce this tool that sort
of thing and so
so the this authorship problem of
detecting if you can whether program is
written by a particular person or a
field a nation-state something like that
is a very interesting relative question
comes back decisions so in this talk I'm
going to talk about how we don't have
work the last couple of years to recover
protocols from X cubed machine code the
general approach we've used is is to
build up statistical models of various
aspects of the program on chain and then
to use these things to infer a
characteristic spot binaries for which
we don't know using statistical machine
learning techniques so i'm going to
introduce the kind of the way to
represent programs to build up these
models and they go on to give a couple
of examples about how we've conferred
lot of details about the compiled tool
chain process for doing this and also
some work we've done on stylistic
education education programs from bishop
and then i'll just briefly covers future
directions of this research but never
the interesting i should say now before
we get into it then if you have any
questions at a time this will be a much
more interesting talking okay so first
model basically this is all about how
we'd form our presentations and programs
that pull out characteristics that we
continuously around
probably where I get into that I just
need to take a quick digression and
we'll talk about the problem that led us
to this whole robots especially to a
giant pokemons issue first place which
was as I alluded to earlier I work on a
tools project that does a lot of minor
analysis instrumentation one of the
things you have to do if you want to do
my imitation of courses find out where
the code is in the program to begin with
this sounds with a trivial for people
who have not worked a lot on binary
analysis but until you did it's not
often times you don't have debugging
symbols with commercial products which
source code is available and so if you
want to do instrumentation of these
programs it can be difficult to find the
code much of it you can find by going
from the community program but what if
we cannot do issues this leads out of
course the entire problem of coffee
stated codes incredibly defied the
actual program instructions is a finer
general the general idea is that we know
that there's code somewhere in some
object but we don't know where it is and
the only thing they have is a stream of
lights so this whole problem is
compounded with the fact that most of
the work that gets done in this area of
course is on the x86 processor ins
instruction set so that's a very dense
instruction set so for example if you
start parsing a string of body certain
set of baloch instructions if you start
parsing from such element instructions
doing that note with an entirely
different stream of instructions that at
least for some time will will be totally
different and some of these are
instructions and some of them are not
parse data for example you can get
something that looks like about
instruction stream maybe a little bit
weird but
would execute on machine so so the
general challenge that we face is is
find a code in binaries what we don't
know where start to start parsing some
random place on the semi-final looking
for the observation we started with was
that there are some regularities in the
way that compilers layout code like a
function entry points and so we set up a
pro where we modeled compilers specific
function jerk want free animals
basically regular leasing instructions
the compilers would admit at the sort of
functions and use those to scan for coal
miners we took a probabilistic modeling
approach where we just huge basically
the likelihood that any given address of
the binary was a danger point to a
function based on some property selected
tumors the second learned a bunch of
ways this is who this model and wrinkles
grip this and we work pretty well f1 is
a metric that's used for evaluating this
kind of information information
retrieval task basically is the mean of
precision recall how generalizes
accumsan more or less and and we do a
pretty good job of this depending on the
compiler which is which is how we
figured out that we really need to be
able to figure out from code what
compiler produced it as a models were
tied to neutral atoms we're just briefly
say something about the representation
we use for this we were just looking at
sort of instruction level of
characteristics of the code as I say you
can disassemble x86 code points and come
up with instructions and so you came up
with this idea of using we call EDM
features to represent characteristics of
the codes that were associated with
changing points and then with
specific choices made with a compiler it
sounds like idioms are supposed to have
some sort of a person into content but
I'll just say that that's it's not true
in fact they don't mean anything in
particular these are just any pattern of
instructions up to three instructions
long that we find it liners so there are
hundreds of thousands of possibilities
exist there's just a few characteristics
of dimension about these things the
first is that they have wild cards ways
of alighting specific details of the
instruction centers tons of move
operations in x86 sort of abstract those
all down to a single would also be alive
immediate values and so forth there's
also an graham based representation just
bites that we didn't use and some of
this early work but we found very
productive later on even though it's in
some ways stevedore than the idiom
concept
so so this was the work that took us
into protons and 10 and sort of informed
us that we needed to be able to figure
out things about the way that figure out
what veluka pada wasn't abusive
underneath but we quickly learned
through bitter experience that the model
of a program that we're using right now
which is basically looking at as bites
and some sort of abstraction work
instructions is not sufficient and so we
thought about what other ways we could
represent the effects that compilers
have programs without sort of a higher
level from scratch and so we took a look
at the properties of the control program
so this is a essential part of our model
of programs digital photograph is
influenced by the compiler in much
different ways right and chooses how it
can play out this blocks and with the
contents instructions to them are we
also incorporate features having to do
with the procedural control flow graph
call graph functions are laid out in the
binary and having associated with each
other part of this is dependent on the
program structure and what it does
author preference and how they structure
their codes so you can see how this one
will come in later authorship but also
the compiler is a choice about how to
lay things out a lot of variability in
there we also look at external
interaction and this has particular
relevance the way that people style
chisholm
the external function officials question
you haven't mentioned source language
wasn't one of the things that you were
also trying to figure out like didn't
come from CNBC plus ones yeah that's
something that we have them and look at
I have specific examples of that i will
take the control flow graph we could do
a lot of information about that it's
absolutely sure if your C++ that's a
little I have my things you participated
a slide that I have mineral but yes what
about run times that are just in time
compiled I well this isn't amanda with
that at all looking only at basically
compiled c++ torturing kind of codes I
think that I think that the techniques
we're describing here could apply just
as well i'm not sure what kind of usage
scenario you would have where where you
would want to do that and if it's being
you're thinking a program that's just in
time compiled well don't the same goals
apply to for example Java oh yeah
absolutely iteration so yes of course
and so I think that you could apply this
directly to java bytecode for sure okay
well if you're doing train to authorship
attribution so forth and for toolchain
the system
it's just not something that the
undirected but and I think you would
probably use other features of course
but probably not continue
okay so just just I think wrap up this
binary code model we want to be able to
pull out characteristics of these sort
of higher-level graphical features
programs but but that can be really
difficult to me we don't want to do
extensive graph matching this being a
pretty hard problem and so we use some
ideas that have been brought up brought
up in the wealth of bioinformatics
community also the security community in
terms of doing matching of the
description of programs or other kind of
graphical structures using really small
suburbs so a graph 'let is with
holograph let is a little three node
connected subgraph of a program where
each one of these nodes are some sort of
code element it could be a basic block
at one level representation or they
could be some sort of summary of
multiple using blocks some of the little
reputation doesn't really matter so the
essential elements of gravity represent
notes and how to live code and also some
type edges and the notes now covers and
so r applets are general sort of
template for a variety of different
characteristics in extracting the matter
so so one of these that we've used is
instruction summary documents work
basically each of the levites here is a
basic block and the color associate of
the node is a summary of the kind of
instruction this is this is inspired by
some work that was done on polymorphic
how our detention a few years ago and so
we divided up all of the the x86
instructions into four categories like
arithmetic instructions or privileged
instructions branches
and just color basic blocks based on
their kind of instructions that show up
in practice this is really sparse you
don't have a lot of halt instructions
the same basic block is is floating
point operations so so there's
relatively few number of college of this
thing so um so with these and them the
instruction level features and some
other high-level things like the kind of
external libraries that are called we
form a number of a small number of
templates of ways of describing code but
when you instantiate these things for
friggin binary get an enormous number of
features that describe that program all
of the different patterns of
instructions that show but in all of
these little 33 noted subgraphs total
graph and so on and these form the basis
for the modeling approach that we're
then use to learn about protons
characteristics so the general overview
of the way we do modeling basically is
to have some amount of codes we're
trying to capture this can be in digital
function or an entire binary or some
arbitrary segment of a binary in terms
of its bytes and then for each one of
these things is some associated feature
vector and the feature vector just
encodes thing is like this function
contains sequences of deep compiles to
push EBP something or it has some
segments of the control flow graph that
looks like this grappling and so the
models we use neither can treat these
these code chunks separately we do that
sometimes like look at entire programs
and join classify whether they were with
my particular person or we can look at
them in concert with other coaches like
we might do some sort sequence analysis
program to find regions of it that are
corresponding to a different compiler
than other regions of the program this
is just a very general overview of the
party similar to go through it in multan
sections
so the other important part of our model
approach is is doing feeder selection
each of the different types of feature
that uses graphical features in grams
indians so on they can instantiate as a
sentence 100 thousands or even millions
of features of our entire train corpus
so our general approach of course is to
cut this now and otherwise we run into
problems of overfitting that data as
general as well and so the general push
for this is to extract a bunch of
features from training programs and feed
them into some sort of large scale
computing infrastructure which for
certain types of feature selection we
can do to save the approaches to feature
selection we can do to efficiently
figure out which features are effective
in some way on tuning set for for doing
the kind of problems with them to do so
we do this have you heard of bud and
this amount of machine learning expert
but I heard recently that and i think
this techniques been around for a while
there's something called support vector
machines which is with automatically
figuring out what features it's an
algorithm for automatically figuring out
the feature section yeah so stops you
learn do you give it any thought to
using that yeah absolutely we use we use
scrubber machines for a lot of our
classification and so so there's two
kinds of things going on here with that
the first is that svms in fact any other
machine learning technique basically
it's going to look at all the features
to get it in this feature span so find
the ones that are most important for
discriminating it like separating the
future space or forming models and sort
of things the any machine that the
problem pretty much can be broken down
to an optimization problem so defined
parameters that do this the issue is
that if you have an enormous feature
space and you have some limited amount
of training data you can effectively
learn this problem where the learning
problem will fit exactly to your drain
but it will generalize very well
basically memorize the examples and so
you want to do some sort of feature
selection first to find the optimum
number of different features in use or
the best features people use to get the
best generalization performance issue
keep adding more generalizations justins
go down so in this case then there's
lots of different ways to deepen your
selection and some of them are more
efficient others is a little more
amenable to paralyzation others in this
case we use it's really heuristic
approach to frank features based on sort
of the co-occurrence training data and
then the selection we do in the large
competing infrastructure is to find the
number of those that most you can use
without running into problems in
generalization and then machine learning
approach itself like SP Amma's or some
other model they figure out which of the
features from salata are the best for
fitting no sense you need an initial
colleague of press right that you run
that into an Espeon yeah so I think that
covers the general approach pretty well
so so we can get a little more specific
if we look at some work we've done on
one of the recovering properties of the
compiler children so at a high level you
can think of programs as a set of
choices that you use when you select a
way a pretty spro Graham source language
and that's going to determine which
compilers of course you can use and each
of those has a version associated with
it and there's lots of options you can
use when producing the code and what
we'd like to do is to recover paths to
go through this there are some possible
paths I've gotta listen apparently it
don't believe that Microsoft could
compile fortune could but but the game
compiler can
just ignore those but so the general
idea basically is that a program is some
path through this space of selections
and the different program might be a
different path when the goal of
toolchain protons recovery is to figure
out what these properties were so so as
I was saying earlier we came to this
problem because we wanted to use our
compiler specific function in gpoint
models to figure out where code wasn't
binaries and we couldn't do that so the
first way we approach to a chamber
country was was a looking at programs
it's just a sequence applies we're
trying to figure out whether the Intel
compiler over the micro where the
commune compiler was used to produce
them so the idea is that a program made
up of segments of code at the sort of
compilation unit level produced by
different compilers and we don't know
what those my baby it's just a just a
blank sequence of bytes and so the
general model we use looking at this is
that as you scroll along through this
code at some point there's an inflection
where where these bites were produced by
a different compiling like the case of
this commonly comes up just in normal
practices if you compile program using
the Intel compiler on Linux there's some
static domains code food which is
obscene that makes its way into binary
usually end in terms of constructing
data structures for the operating system
and those are usually produced by
getting so so anyway the notion is that
that we're going to find parts of the
sequence that correspond to one compiler
we did this by every bite sort of
assigning to it a note in a
probabilistic model and associating
these things with sequins
like in da compiler or GCC data that's
working the feature we use for these
Gideon features which of course
correspond to instructions and so they
spam more than just the single bite in
the binary we are just labeling this
individual by one at a time so since
this is a joint model we use what's
called a conditional random field and
just really quickly talk about this it's
it's not terribly important the details
so you're familiar first ignore what I'm
saying but so CRF is is a it's a
probabilistic graphical model which
means that there's probabilistic
dependences between things and basically
it's going to assign a probability of a
set of labels based on some evidence and
this is going to be based on both
feature functions and a set of weights
which are which we learn future
functions are just what I was
introducing earlier if there's an idiom
for example if you want you might wanna
particular sequence of bite instructions
exists bite consider otherwise and the
crs job is to learn the weights
associate with this so you can make out
a probabilistic estimate of what labels
should get out of a sequence so for this
we use what's called linear chain CRF
which just it's like a it's like a
Markov chain basically it is a each note
each bite is sort of conditionally
dependent on its neighbors else the
great notes in this picture are data but
that's not important this is really nice
because exactly in for instance kind of
molly's it's a tractable thing to do
whereas in general structured
probability models is so what we do is
learn through training data models that
been model of sequences of cochin air
conditioned compilers and we evaluated
this by both by looking at programs that
were naturally empowered life
others and also programs that contained
code from multiple different compilers
by linking together code where each
compilation unit source file is produced
by you there ICC or GCC ran over and
then looking these things together come
so we can do this with actually turns
out to be quite high accuracy you know
uh ninety-four percent accuracy labeling
these sequences and this gives us about
eighteen percent improvement over the
task of finding code in binaries so we
tried creating a model basically a fine
code it's independent of the kind of
pilot produced it almost twenty percent
increase in its accuracy by by V in this
prior knowledge of which so once mean
even to do this we wondered you know if
there was possible to find out more
details about the keyboard book I mean
about the the compiler that produces so
here we was looking at the compiler
family and we wanted to add in things
like the source language he asked about
or the version of her pilot produced or
its optimization level as an example of
things in my wonder about the option
screams who use the same kind of model
but in this case we're not looking for
code we know where it is already in
trying to do some sort of higher level
analysis of it so we cut the program up
into functions instead of how detailed
are you on optimization lobe I this
study was just low versus high like we
love together Ozier oh no one who is you
have free in in GCC we have legit
whether you can look at
for particular optimizations I think
probably someone should cannot really
clearly add some of them probably plus
so Beth look clearly right and then
probably I mean if you think about the
difference between 0102 in GCC and
that's probably what a lot where the
distinctions come up in terms of sort of
more aggressive optimizations at orgy
with its some point it has to be about
the instructions that are in it so it
causes a huge variance in the
distributional assumptions are headed
ass and then this shows up here do the
same
so this approach was actually quite
similar to what we did when we were
looking at the fight sequences except
we're looking at functions so functions
are the unit that we're labeling the
hounds is supposed to individual bytes
we tried a much more complicated
graphical model this is this great
structure where we're sort of
independently labeling the language and
the crowded family optimization of a
little diversion using both idiom and
the graph the features that I introduced
because one of the things we observed
just by looking at supplier code that's
produced by others is that different
versions of the compiler tend to do
things like reorder the way that
branches are laid out for example in
groups this comes up quite a bit so we
evaluated this on a large set of
programs looking at three different ways
of doing this thing the first is just
using spms individually classify
functions for each of these different
components without incorporating an
information about the adjacent functions
layout in the binary and so we get
pretty high results with that with
everything except for the version the
basic power of incorporating these this
the CRFs basically is that that it
allows you to cooperate with the
structure of the pilot will structural
program is it's conceptually I think
it's pretty simple it's more likely than
two functions that are adjacent in the
binary we're produced by the same
compiler so the second two columns are
or both conditional random fields the
first this is blue the better scores is
one where we've basically we're treating
each of these components separately and
not doing this whole joint grid
structure model that I showed previous
slide and this turns out to be the best
way to do it so you wonder why I'm
presenting anything about this more
complicated model at all especially
because it's slow digital in its
approximate
and it also give you a possibility
because labels like you can tell you the
function was produced by seeing produced
by GCC version 2005 law optimization
which is 2005 isn't Microsoft and so
forth this can be kind of weird and the
reason that we looked at this at all is
because it allows you to get partial
label results and it allows you to so
you can it can give you these labels
like cgcc 2005 and low and it can give
you marginal probabilities of these
things so you have high confidence that
was written and see how comprehensive
isn't easy see high confidence that
itself it's a low optimized program and
maybe really low confidence just barely
two thousand five wins out and so it
gives you a joint way that yellow dress
but in practice we find that the
trade-off in terms of performance is
such that willing to sacrifice this kind
of flexibility in the model for from a
much faster with heavy thug can you
exploit like the huge joke you get it
improvement on the version yes between
the two hand so version is very
difficult to tell her and you know
especially if you get down to like minor
versions of the compiler there's no
difference in the code being laid out
because there's just minor bug fixes but
the majority of the difference here this
number has to do the microsoft matter so
at the high level the reason that we do
so much in your change your half is it
basically you can think of these these
markov like Kabbalistic models is sort
of having incursio so so it's really
clear for one function and then it's
less clear for the one adjacent to it
well it gets to get to be conditionally
depend on the one that's your Vitamix
that sort of drags up the probability
prediction arranger but what's going on
in these numbers is that for the
Microsoft compiler there's very little
variance in the code admitted across
different versions 2003-2005
sometimes in some programs will get
something like seventy percent bitwise
identical code being so there's not a
lot of churn in the codeman submitted
from different versions in the person
compiler the cogeneration engine
basically doesn't change very much which
we thought was an interesting result in
such if you look for at the new compiler
if you look 34 to 36 or something like
that their codes renovated there's a
huge amount of China going on in terms
of the cogeneration so I sorry I still
don't understand it what the maybe I you
answered it and I missed it miss the
answer he mean or the question was why
did using the crs dramatically increase
the version yeah yeah so I guess it
didn't sorry I'm absorbed sure no it's
fine hundred so it's like to show this
but basically sometimes it's if you were
looking at number here individually
coffee this is looking at sort of each
function individually and saying can i
crack the classified the version okay
and then the CRF each adjacent function
inside of a binary is is related to like
the prediction in this movie made about
that it's going to be related to the
ones that are adjacent to it and so for
some of these in might not be very clear
all and so it's kind of random chance
whether its uses we're going to the
three to five and then for some of these
it's going to be much clearer and so so
the prediction for the one where it's
not there by itself is influenced by the
properties of the womans adjacent to it
so that drives has a sort of inertia of
back to work functions that are close to
each other get us on the same label and
sorry I understand the improvement but
why the difference between the top three
the improvement on top three is very
small the improvement in the version is
huge right what is it about the version
that makes it so sensitive to the
looking at the
the Jason the answer is that it's just
much harder to perfect the version is
much less variation in terms of like
going from especially in Microsoft and
Pilar going for more version to another
so so basically you can imagine that for
most functions in a binary sort of the
sort of like random chance even chance
of which which one will be assigned and
then so but then some of them is very
strong and so and so those tend to when
you look at them all together and the
sequence they tend to force everything
to flip it to the correct the correct
assignment so you get a big jump in
there the rest of these things it's
relatively easy to tell the difference
so there's not much room for so it's
really easy to tell the difference
between different Pilar different
optimization levels in San compile the
code is a radically different so it
seems like this this is saying something
very specific about version the versions
the difference between versions of
compilers and that it has to do with
placement of functions I thought you
said that the version was so low on SPM
because it was sort of pulled down by
the by the Microsoft compiler on well
that's what ERISA was so poor it's yeah
it's also it's also important it's also
for in the GCC in my CC it's just it's
really bad what's the same I think
another way to look at this is that is
that there's a relatively little
variability between versions in terms of
code generation for compilers I guess we
kind of hoped that to be the case of
hope that were close to figuring out how
to do this at this point and but in some
cases when you look at the linear CRF
results in some cases the variation can
be pretty strong so so imagine there's
some code constructs the show up in some
function
that leads to radically different go to
being admitted and that's what's causing
this kind of model to sort of boost up
oh thanks was your question in LA we can
yeah can take it off while later yeah so
so I'm bogus shift gears now a little
bit talk about some more recent work
that we've been doing in authorship
attribution so we looked at the tool
chain and during these components and
i'll just say from the gang that we
didn't have a lot of em that this was
going to work out we were quite
surprised with results weekend but
basically the idea here is that we want
to see if you can tell characteristics
about the person who wrote a program by
looking at the execute code so the
general idea here is that there's a
couple of different people and they
write maybe the same program and we have
sort of intuition that everybody writes
programs a little bit differently i mean
you work with other people and after a
while you can kind of recognize their
code for better it works but so for
example here like these two guys have
decided to write this iteration over
some something using a saw that for me
it's worth though it was obviously using
singles plus animal maybe it's not and
there's been research over the last
decade or so occasionally about whether
or not you can tell you do authorship
attribution of programs based on their
source code and this is this is
something that has a long history of
using literature I think there's a
there's a Dan Brown model / maybe a
movie coming out about this I'm not sure
Shakespeare something like that but
anyway it's something that people have
been interested in literature for a
while and more recently interested in
terms of program analysis but it's all
been done at the source code level and
you can kind of get a sense of why it
work well our hypothesis is that the
kind of difference is that people use in
the way the programs and then it shows
up in binary code as well
we have some things to back us up on
that so we've looked at some programs
that were doing the same sort of thing
or there they represent the same
functionality in fact Google helped us
with this quite a bit there's only two
in a minute and then we compiled them
down and took a look at their control
flow graphs and looking at these things
you can see that there ought to be
properties that we can extract that say
something about how this code is
different one of these guys stl and as
you mentioned earlier it's full of small
functions and laws of regions and so
forth the other one was written in C and
it's much less complex what they do the
same thing so before we could capture
this we needed some additional features
about the code to try to catch more
about the way that people make decisions
that we can structure the programs and
what we decided to do is to try to
incorporate more long-range control flow
of information our intuition being the
people the way the structure the control
flow of their probation so recall that
we had these graphs that are just basic
blocks and we had these instruction
summary graph once we use tool chain
analysis that were like three connected
sub graphs and so one way to get long
branch information is just to condense
these things now we use a process for
basically select a three nodes that are
connected and then incorporate their
neighbors and then so the colors of
these nodes would become the summaries
of the union of the neighbors and we can
do this recursively to get sort of
arbitrarily long distance control flow
information condense down to these three
graphics we really focus on the sort of
size three graphics because we can
leverage some properties of the
structure of them to make this sub graph
matching problem a little bit cheaper so
so this is one way leave incorporated
longer range information is another one
is to incorporate the sort of
interprocedural control flow like the
way the people structure relationships
between functions in their program a
relationship
outside functions we created these this
call graph once we call them which are
created basically by doing reachability
analysis on notes that make calls in the
program and ends up with you having
something that's a DFA for Reverend for
recognizing call sequences where the
color of each of these nodes is has to
do with the external call target or
local methods which are which are
columns because they don't translate for
confirm so this is just a couple of
additional ways we represent code for
this operation excuse me authorship
analysis so learning about program
authorship and evaluating it can be
really difficult because getting ahold
of an IDL data set is really hard
obviously isn't they were those author
labels and so one possible source of
that is open source projects but it's
really difficult to mine I'll just tell
you it's really difficult in line open
source projects for sort of author
associated code information have to be
consistent especially these churn or
copy paste code consent would you also
like to have a parallel corpus and this
is mostly for evaluation we want people
to be implementing the same programs so
you can control for confounding
variables like the Faculty's into
production quality and linguistic
homogeneity like people or even c or c
co sponsors had salmon of control for
kidnapping information so we found a
couple of good for this and one of these
was the code am competition i took to a
very helpful i detecting yeah i think
first and a very helpful engineer who
suggested ways to just scrape all the
data from the coding competition so
that's exactly we did we also use some
programs from a operating systems course
at leaders its content these data sets
are a little bit different for each
other the code Jam we've got a couple of
years with the contest data and we just
extracted all of the programs that were
written in C++ or C and would compile
with the compiler version installed on
my worst nation
so without multiplication so that was
that was relatively few but it's a few
hundred people per year had no between
eighteen sixteen submissions of the
cosian competition and met these
criteria in terms of using simple and
C++ compiling the machine so the bottom
here is a distribution of the number of
programs available for a person the cs
537 course everything was written in c
and there were remarkably fewer examples
i think that there were something like
april nine eight projects in the class
nobody had more than 7 completed or that
i could build and credit to be blad less
than seven let's drop the course there's
also some differences here which is that
a lot of this 537 code is caught
provided template code they started with
these projects and so this ends up
making the learning problem a lot more
difficult what about some of the
beginning programming classes I mean I'm
sure the thought came up uh yeah it's
going to be objective but but most of
those are done in Java ah it was and so
I'm so sorry we were working on decks
Cuba code this is important so we first
thing we did was try to feel authorship
attribution and basically we mind these
program data sets for for all their
features and there's a large number of
these things then we use the feature
selection process that I leave it to
earlier to to reduce this to a set 1900
and used and we used support vector
machines to to further define models and
declassification so the procedure we
used was to take you know rent 20
programmers and random from these data
sets and take a look their programs and
form data files from them
and repeat this kind of experimentation
20 times we found was with somewhere in
the high 70s accuracy on the code Jam
data set we can tell an individual
person based on my machine and this sort
of generalizes across any selection of
over any progress that we got the five
to be seven dataset turns out to be much
harder so we also evaluated how well we
can pick out the right author in the top
five of the most likely authors basing
the classification member this has sort
of this is because operationally you
would want to be able to do something
like this I think for from our
identification you're trying to find the
most likely candidates oh so when we do
that we get relatively high scores both
with code Jam examples and somewhat
somewhat higher on 537 but it still
remains a difficult data set so one of
the conclusions might be tempting to
draw from this is that students have
less distinctive styles of programming
but I don't think you can get there the
first is that the 537 dataset has as
much less data per person so it it
admits a kind of training scenario which
there's just really not enough
information to learn exactly what style
a person has from their coach training
data and the second thing is there's a
lot of template code provided we will
just best we could to eliminate this
stuff but but I'll they'd be the perfect
job and then you know if you observe
people being taught in these courses the
instructor is sort of guiding people in
the way to do the programs they have a
lot of feedback and so I think there's
something them all there whereas the
code Jam this is individual contest
competition
so so authorship identification like
this is one interesting it's one
interesting application but but another
thing you could try to do is this
cluster programs with style without
knowing who particular office are so
this has relevancy in the malware world
if you are looking at the large set of
which is software that exists in the
current computing environment and you
would like to know whether you can group
programs based on stylistic
characteristics to maybe see that
there's more to different groups working
or software offers this is a really
challenging problem because you know
retraining data to do this but you would
like to be able to take a bunch of
programs and split them up into clusters
and say okay we have a couple of
different offers but you just as well
might have a scenario where you found
the fact there's programs in
functionality you're just clustering
this thing so so how did kind of cluster
these things automatically without
training data is a relief is really
difficult problem so just really quickly
goes of this approach which is the
notion here with clustering in general
is that you're going to find things that
are sort of nearby to one another in
some space the far apart from other
clusters in this case so so this totally
contrived data example we have a couple
of different groups with possible
clusters but there are no more likely
than some other groups with their
assigned two clusters so one way to
approach this is through what's called
distance learning the Mahalanobis
distance is a generalized distance
metric we're basically data or they got
your social patient so for example one
way to transform these data points is to
squish down the y direction so that the
cluster is that you would draw from this
data are much more obvious
in the course the question for this kind
of approach is how you get this this a
distance metric wrenches especially if
you don't have a lot of training data
and so the ax person we took for this
study was to was to realize that we do
have training data in a sense we have
some programs that know who the office
work maybe we can somehow learn a way to
transform data so this style is the
important dimension of this problem then
we can maybe apply to something else so
what we do basically is is that going to
a lot of detail we learn a distance
metric that separates programs in a
misdial space and then apply that to the
data for other programs and then find
that they separate into style as well
without having any kind of information
about the people who wrote these
original programs we use some relatively
recent work on on finding distance
metrics in this kind of scenario it's
called large margin nearest neighbors
method unfortunately this is a required
solving a semi definite programming it's
kind of expensive to do although this is
sort of a one-time cost and so it has no
real effect on the scalability or
performance of over actual clustering
algorithm so we evaluated this on one of
the code Jam data sets and this is a
comparison of doing clustering like ska
k-means clustering on either data using
like clipping and distances in the
feature space or this learned metric and
what we found is that as we increase the
number of training set authors to get
metrics a straightforward and
furthermore did we get sort of stable
improvement by using this magic when we
have different numbers of authors in the
test set so the takeaway here is that we
get something like a twenty percent
improvement by by appropriating the
stylistic information and this tells us
is that we can in fact learn to
recognize
the stylistic characteristics of binary
is now in such a way they generalized
different sets of binaries with identity
in general we get normalized mutual
information is a it's just a metric for
evaluating clustering one is better we
get about seven to three on this this
problem which is which is pretty good
but it's certainly not perfect this is a
very difficult problem in general I
think to pick out all their style these
programs so I'm just about how to time
here was touched really quickly on a
couple of different future directions
from this book the first is that one of
the assumptions we've making this entire
time this state sort of especially with
regards to the authors is that people
write an entire program at themselves
but I think that's obviously not true
and I think that especially use a lot of
research being done in the in the
malware community or the Tencent our
community about the way that the
components are sort of being shared
around by different power projects and
command and control software for verbal
nuts sort of moves around between
different components and so one thing
need to be interesting to do is see if
you can track this kind of relationship
based on the stylistic properties or
tools and properties of the compiled
code another thing would be that we've
sort of assumed that each person has one
style of programming but that may not be
true it could be that people have
different styles programming and they
use different approaches when they're
tackling different tasks so this raises
a lot of different questions about how
you can tell style versus functionality
and programs apart that has a lot of
applications for some recent research
into some more advanced machine learning
techniques and it brings up this issue
of how to interpret the clusters that
form when you
cluster based on stylistic information
another really fascinating I think
application of this is looking into the
sort of the social network at the
underground our economy maybe you have
seen you have a bunch of programs and
some number of authors out there who are
biting these things or nation states in
groups or whatnot you may be possible to
sort of segment these programs based on
stylistic characteristics and from this
to infer relationships between the
groups that are producing Stuxnet for
example on the group's attic recess some
other asian those softwares I think this
is an area where there's probably
productive from tribunal anyway just to
sum up the approach we've been using is
to use many different simple features of
programs huge number of templates and to
take advantage of the structure of the
program to do things like that we find
relative a surprising detect author
style including miners so without making
take any questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>