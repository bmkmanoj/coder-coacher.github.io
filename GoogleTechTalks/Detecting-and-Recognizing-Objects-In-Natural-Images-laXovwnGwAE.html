<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Detecting and Recognizing Objects In Natural Images | Coder Coacher - Coaching Coders</title><meta content="Detecting and Recognizing Objects In Natural Images - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Detecting and Recognizing Objects In Natural Images</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/laXovwnGwAE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">by himself or in some interaction so
let's see do I is this working
so yeah just for people who may not work
in visionless usually put up a default
slide saying vision is hard because
people often think it's really easy why
it's easy as our brains were evolved to
do it you know our intelligence is
basically in our cortex and at least 50%
of our cortex seems to be doing vision
in one way or another and so the if you
think about it and sort of intelligence
terms vision is just looking at the room
and interpreting it is arguably a far
harder task than solving the most
difficult mathematics problem or
building the most complex software
system I was once his to the harvard
mathematics department for saying this
but I think it's still objectively true
doing doing vision and you know if you
take away the vision part of your brain
and the part that does motor control and
a little bit that does language is not
really very much left
so intelligence is often what's going on
at the level of vision and perception
okay so vision well you know why is it
hard well vision requires decoding the
image I'm passing it into components
such as objects and the difficulties due
to the fact that images are very complex
and also extremely ambiguous and one way
of pointing this out was the observation
that if you look at all the possible
images which are just 10 by 10 10 pixels
10 pixels in the x-direction 10 pixels
anew in the y-direction and count out
how many images there are you see
they're far more of those and all the
images that have been seen by all humans
over the whole period of evolutionary
history you know allowing for how many
billions humans have ever existed and
you see 30 images a second and so on and
you live you know on average 60 78 years
life nobody you know no one has seen all
the possible 10 by 10 images it's a very
high dimensional space it's a very
complex space okay so here is an example
a quick example to test your visual
ability so these squares this one and
this one which one is is brighter has
anyone seen this example before very
sophisticated people here all right
everyone's seen it by now
I still find some people who don't but
anyway they're the same but they look
very different and the reason you know
the reason appears to be that when you
look at an image like this you'll not
just your brain it's not just
registering the intensity that you're
actually getting here directly it's
doing some complicated inference it's
figuring out that this thing seems to be
in shadow of here and so its intensity
is actually different from what you
actually perceive it to be so it's doing
your inverted process okay okay so here
is a typical image illustrating I think
Everest from the tibetan side
illustrating some of the degrees of
complexity that you get in a visual
scene and the work that we're doing one
part of it is related to what we call
image passing paper that we published a
couple of years back which were given
the
the basic flavor of of what we want to
do the idea being that an image let's
take this one can be thought of as being
composed of a number of different
patterns patterns for the person
patterns for their you know a
hierarchical sort of representation of
patterns here is a person here is a face
here is a body
here is the sports field the sports
field is made up of certain components
the spectators are also made up of
certain components which could be
texture of different types or and
sometimes sort of larger elementary
objects so think of the visual world as
being made up of a large number of these
patterns I organized in some sort of
hierarchical method like this and then
the idea of interpreting an image in our
terminology would be passing it taking
the image and sort of decomposing it
into patterns so you can take this this
picture two ways one way is you go up
here which you means you take these
patterns and you stick them together to
make the image the other is you start
out with the image you decode it by
taking it apart and getting this
representation for it and once you've
done that you have solved the problem of
detecting objects in the image
recognizing them and understanding the
the entire thing and so our approach was
to formulate this in terms of generating
the image by a probably stick grammar
and grammar allowing you to have a sort
of fairly abstract level of knowledge
representation probabilistic so that
things are you know not purely
deterministic here is a very simple
conceptual picture of this the image
would be a scene it would contain a face
it would contain some text and will
contain certain amount of background and
so you could generate an image by coming
up you know with the scene node a
certain probability of there being a
face summer in the scene a probability
of text etc the model would enable you
to generate these and this will be you
know since that examples of texts and
static phone poles or faces so that
would be the sort of degeneration
process starting from you know starting
from this abstract publiced ik model for
the scene
and then generating it now the task of
interpreting it would go would go
backwards would have certain dynamics
you'd have a grammar you would adjust
you'd have a grammar of representation
of the scene in terms of these these
nodes and the elements in it you could
do various operations like here you'd
start out by inserting the image in
terms of text and background without
realizing there was anyone in the scene
and then over here you do a move a
translation on the graph which involves
creating a node structure for a face and
then explaining that part of the image
in terms of the face model and other
types of transformations can be done on
this also I was not planning to go into
details of this this type of thing if
people want to give details give me
feedback and the more questions people
ask well I can adapt to the audience and
make sure I'm on the right the right
page here is a rather more complicated
diagram of how a system of that sort
would work and basically it would work
in to two sets of stages the sort of a
bottom-up component and a top-down one
the bottom-up component would be a
system which would look for we're
particularly targeted to try and detect
or make proposals about the presence of
certain important things in the image
such as text or faces or any other types
of objects and so for people involved in
machine learning you would have a series
of tests sort of discriminative
probability tests for example Ada boost
or methods of that sort are good you
know often very good procedures for that
which you would train on data and which
would look at certain part of the image
and say with certain probability we
think that probably is a face there or
the published text or whatever other
types of objects you want to have these
would make proposals into this sort of
hierarchical parsing representation
which would allow you to you know create
models of faces destroy them move them
around and so on up to a high level
where there would be more generative
models of faces or text which would sort
of interpret
what the low level is telling them also
the high level models could start making
predictions about the locations of
certain things in the object or making
predictions that okay you found a face
here there's certain probability that
ought to be a face over there based on
high level representation there is the
low level things wouldn't necessarily
own pixels the high level the generative
models should impose that the low level
you could have competition you know
something jumps up saying I think this
is a face and something else is saying I
think you know you're wrong I think it's
a tree but the the high level stuff
generative is the thing that really
controls it and makes everything fit
together and and so on here are some
examples of whether oh yeah in this
version it only has one but that's not
yeah this is this is a version from two
or two or three years back that was
implemented by so-and-so and Alex Chen
and so on so there's nothing in
principle to stop it having more and
some of the ones that residue is
implementing with how far more would
have far more nodes of that sort
here is an aspect which would illustrate
the issue of the the bustle of the
bottom-up and top-down so here based on
the filters we had several years ago
would be our estimates of lots of faces
and what a text in the images so okay so
it's getting these faces okay but it
also thinks this thing is a face and if
you look closely in it you know take out
the context it actually doesn't look
that different you know there's a bit
here that looks like an eye and eye this
could be a nose that can be a mouth you
know so it's a plausible thing maybe no
plausible error that the system can make
and given the complexity of images
you're you're often going to find things
of this sort you know if you had a model
of a tree or particular tree thing that
would compete that would say hey this is
far more likely to be a tree you
shouldn't have a face but
at the bottom level of these cues you
know which are so working sort of semi
independently it's a legitimate thing
for a face and over here there's another
face here etc over here text here text
here it thinks this is text well it
looks you know it's consistent this
could be a bunch of ones and so on so
these sort of discriminative bottom-up
approaches are you know are not always
consistent and they may in certain cases
be be wrong when you oh hang on here's
the okay that's this slide so here when
you have the high levels of generative
models you start resolving the ambigú
you know solving ambiguities these
things are now detected as faces but
this area here this is a part of the
general region for the tree it's no
longer described as the face itself and
many things are you removed this area
here which was considered to be possible
all text is now described by the
high-level generative model as being a
type of texture because that sort of
fits it better over here the nine you
may not have realized notice this nine
was not picked up by the lower-level
text detection but it has been found
here and it's now interpreted as a nine
okay so this is giving you this sort of
basic sort of strategy of the approach
you would have you know images would be
composed of patterns you would have
various bottom-up cues which you would
typically learn by you know by sort of
machine learning approaches they would
activate hypotheses these hypotheses
would be tested by generative models and
the generative models in turn would try
and impose a uniform interpretation of
the whole image which makes everything
consistent and happy okay now I was
going to show you this which I think I
should show on the how do I get out of
the slide presentation escape okay so I
may need help of a guy here and now
let's see if I can
put up a demonstration here okay so I'm
seeing this here drag it over to the
other set to the left okay okay
okay let me just sort of go back here go
back to the beginning and sort of make
it fullscreen okay normal mode let's see
I can only need to push it over here
move that right over
okay so this is a great low tech talk
here is an example of what one can do
now so this is some time further on and
so if you don't know vision this may not
be very impressive if you do know vision
you should I think be pretty impressed
by this so on the left hand side as
there's video taken just by a founded
simple video camera you go out into the
street you show these you know run it
around and on the right hand side it's
it's in practically real-time it's
detecting text it is by analyzing it
it's sorry all right well in some cases
that's it's some it's not it's not
keeping any memory of what is there from
frame to frame so there were certain
aspects as well as moves around us it's
coming into you know it's into view and
into focus and then it's coming out of
out of focus moving around so some of
the text will appear in one image and
then will disappear again in the other
image nevertheless I think that this is
something that is not I don't know
anything else that can do it than a
method of this type hello it's not a lot
actually it's a very Daniel do you know
the
but it's yeah and so once you know what
this is relying on is having data you
know data sets of images learning from
that methods for which the you know what
is distinctive about text implementing
them and then proceeding on to do
binarization
and from here on they go on to to do
recognition so the systems we have this
would be if one of the more practical
things things so far a bit of sort of
history or memory memory that these
things existed over time would remove
the flicker but it will make certain
mistakes and places which again you'll
see with the flicker they'll flicker on
and then they will disappear off later
on
there's no temporal combination then so
we got box could actually be crude
accuracy the total correlation will make
things yeah that would make things
better definitely you know you're using
everything here is based on cues and
each picture is sort of producing its
own cues and there's no information from
both of them yes it seems to change
forms as it flickers around is it doing
different things in that case example I
mean sometimes it's like it comes out
very readable and sometimes it's very
confused I'm just trying to understand
exactly what we're looking at it's I
mean it's the nature of the text certain
of the text is different than others and
some of that also relates to the the
binarization I mean they're two stages
in here one stage is actually doing the
detection then the binarization is
making a limit limit you know a certain
amount of hours and it in itself that
takes detection I think the detection
itself I think is working extremely
extremely well the binarization well
we're working on improving the
binarization and the certain balls we
have which are good and there were
certain cases where there are sort of
failure modes which we then isolating
and working on it
because of the way it's composed in the
single frame no memory right okay so
let's get back to this so to get back to
this thing I go grab it just move that
over make that a slideshow and well not
anymore I think here and now damn sorry
the mouse has now disappeared oh it's on
your screen okay okay so good we're here
okay so that's that was you know so that
was work on basically we've been
concentrating on finding faces and well
particularly on finding texts now the
question is really how do you go beyond
this because really the idea is you want
to you know you don't just want to find
texts and faces and things and images
you want to find everything and
everything we had before was relied on
having a certain amount of training data
you know to get the text thing to work
we had to have large numbers of examples
of text in real images and so that we
could train so we could get algorithm so
could distinguish between the text that
actually was text and the parts of
images that work could be anything else
that looked like text and the same thing
for faces and so that involves really
having enormous numbers of images
because you really have to see what
there is in the environment that
actually could coerce pond to text and
if you think about it vision is a fairly
strange subject in the fact that no one
has ever really characterized what
happens in all possible images you know
in speech you know there were certain
number of words that people offer there
were phonemes and so on there's a fairly
basic understanding of what the basic
vocabulary is what the basic inputs are
if you're an astronomer you know you
study stars and you know you know the
stars you know there were galaxies and
you know there are dust clouds and a few
other things you know
the basic elements of your domain are
but-for vision there's not really a
large amount of knowledge of that I mean
one level you know you're working with
images with pixels with intensity values
but there's very little understanding of
the whole complexities of what actually
goes on inside them and so so my
colleagues song-chun Zhu at being down
at at Lotus Hill and a certain amount at
UCLA is developing this really fairly
ambitious project which is more or less
to take a very large number of images
and sort of trying to map them and to
understand what well actually what
really goes on with them essentially
well the work I described so far was
based on you know the idea of passing
images into certain components he's
taking it further on one hand by trying
to pass all you know enormous numbers of
seams into visual components on the
other hand at the moment it's being done
more or less interactively so it first
started out a year or so ago with I
think 20 Chinese art students sitting in
front of images and hand passing them
and I'll show you some examples later on
and then it's moved more into an
interactive approach so that you can
spare the art students some of their
time by putting in vision algorithms
which confine certain structures and
then all the art students have to do is
to you know to validate that they're
correct or not or to make changes and so
certain some work we did on text was but
was made possible by having some of the
maps of data that we got out from this
this process and so once you have these
dump these representations well I think
you learnt and well you learn some very
big things by having them one is you you
get this idea of what of the sort of the
structure that images form in the in the
forms of the sort of graphical
structures you have you get the idea of
what can happen at all possible images
and then after you've done that you can
use the representations for learning and
then you can also use them for
benchmarking you can see how good the
algorithms actually perform and how well
they don't I mean if you're in the
computer vision community you'll know
one problem with computer vision is that
while it's easy to come up with an
algorithm that works nicely on a few
images and you can publish a paper on it
actually getting that algorithm to
generalize and to work in very large
datasets is a compl
different business so here is an example
of a typical Chinese image I guess he
gets some of his funding from China
obviously and so here is an image this
is obviously outside the Forbidden City
I think looking from Tiananmen Square
and this would be trying to take that
scene segment it at this level sky
building flag etc streetlights portraits
and so on then mapping these down with
line drawings line illustrations of all
these types of structures here is the
yeah putting layers what what structures
behind which other structures etc and
where is the text you know this was
stuff we were using for example labeling
these areas as being text these areas as
being Chinese characters and there
should be some here on the top of the
bus and yeah that's in in here it's it's
a use of interface annotation tools so
this would be the the Chinese art
students the algorithms this would be
the Chinese graduate students and and
then tying in with the knowledge
database and so on because the form of
the represent you know the the the
representations the image would be based
on a representation that Zhu has defined
based on a sort of and/or graph type
version of a grammar and now he's sort
of initially guessing what that sort of
grammar representation ought to be so
they have one version of the grammar and
then based on seeing how well they can
describe certain structures they sort of
have to modify it and so on so it's in a
sense learning a good knowledge
representation for the data and the
automation well more comes in you know
of course in the sense if you can
automate the whole process and you
solved the entire vision problem and
there's nothing else to do but the some
way away from that yet but still it's
interesting to find out which computer
vision algorithms are actually useful
for something like this which is hard
and where they have to work with
and which ones are not so where are we
so here is just an example of a sort of
representation so here is the you know
here's here is a small element of a
scene so of you know a boy with the
backpack you you were draw it at one
level then you would represent it this
well this hierarchical manner in terms
of these sort of parts the container the
handles the zipper and so on
the face would be represented the you
know the person the boy would be
representative in terms of face the hair
the ears the parts and so on and all
these would be encoded within this sort
of hierarchical data representation
structure that some transients people
are are doing here is I think another
illustration of what one does with I
think a chair scene so you're you know
you're labeling the parts of the chair
you're putting the you know here is the
the seat of the chair the cushion the
back the light and so on I think and the
window and the so on is in the
background so from these you can both
well represent and you could learn
methods for detecting chairs you could
also find the sort of public
relationship between certain structures
happening in the image like the chair
and the table and so on this has been
sort of talked of in in computer vision
it's something that sort of comes and
goes I remember even when I started
vision 20 years ago there were a schema
being developed by Alan Weisman and
Hanson which sort of talked about these
issues but it was practically impossible
to do anything like that then there was
no data there was no real-world images
that you could really work on and there
was no possibility that you could
actually even think of designing
algorithms that would work on those
types of processes ok well yeah street
scene segmentation Google Earth images I
guess he's got from here I I trust with
your permission where take these images
label all the cars and so on label all
the buildings and
get these representations and then
proceed to use these use these domains
to build image passing systems based on
the principles that I've been I've been
describing database I think this is the
size of this is big certainly by the
size Houston computer vision I remember
five years ago databases of a hundred
images were quite large
after that the berkeley had a database
of segmented images which was about 2000
which was considered big well here the
total number of images is something like
500 thousand so far I which sungchung
says I and annotated I haven't checked
them all myself so I'm not quite sure of
performance criteria but still it is an
amazing amount facing up by looking at
the outdoor scenes indoor scenes images
done by activities aerial images and
also encoding of various animals objects
and and so on and this is by no means
the end point the Institute has been
running now for about a year and a half
I think and five hundred thousand I
think is a big number but Hong Kong is
shooting for a lot more orders of
magnitude videos being included
yeah that's videos that there that's
yeah I'm not sure exactly on that how
much does that is done by exploiting
that yeah that property or something but
yeah apparently there was a number of
videos of Chairman Mao's speeches which
I'm not sure exactly I have you know I
spent time out there there are highly
motivated people working you know eight
nine hours a day just doing these things
that's I'm glad I'm not doing it myself
but sometimes I'm sure could give you
the figures figures of doing that I mean
it's also going to change as soon as as
soon as you pops but including the
annotation in there and you start just
having to you know prune them things out
which are bad I know you have you got
any idea
and I think also with this processes
you know there's time issues like how
long do you learn to train the students
to do this this type of thing etc and
you know so the efficiencies which come
with time
okay so this is that's sort of part of
how one could take the image passing
further you know one needs to have this
data sets you need to have these types
of representations you need to have this
for training you need to do it for
validation you need to have it for
learning all your all your types of
models okay here are just a bunch of
very variety of cats in here it gets
partly I like cats so I picked one one
example slide there and then certainly
the representations also go into 3d
scene labeling so I've just put in a
slide of that of that form any questions
on the data sets because I'm now moving
on to I guess the final part of the talk
as opposed to only specific I you would
like I'm not sure exactly a below much
the generic model as you can obviously
four grounds of interpretation I mean
and into the extent you can find
similarities and you'd have a grammar
that could you know generate a kangaroo
with one input and generate something
else with another input that's what
you'd like to do and I frankly I think
that's quite practical given you know
the fact a little mammals so you know
have very similar structures there's
been work in the past in fact work well
there's work I did was so much of a long
time ago when he was a graduate student
which in sort of a forms representation
system which involved representing
things by skeletons and so on and a lot
of other people have worked on that sort
of skeleton type representation
structure and so I think it's not
impossible to do to do that I should
also say that in terms of detection as
well you know so for the earlier system
that we had published you know we built
something which would detect faces
bottom-up we built something to its
tactics bottom-up but you don't really
want to build something that the text
you know of the eight ten thousand
possible objects in the scene
individually you want something that
brings out depends on the the
similarities between them and will
function and will output you know a
number of possible as you know number of
these possible structures there's been a
bit of work done by Bill Freeman at MIT
related to that Shimon Ullman at Weisman
is also considered that issue but you
want to find sort of commonality and
sort of build up with ideas of sort of
compositionality putting certain parts
together parts that hopefully reoccur
and can be detected individually so
anymore
and he yeah
so some of the refer to inaccurately set
of cars
oh you would have a number of yeah and
with some slides which I took out of
providers of cars you'd have you'd have
a sort of for that to whiz at least a
generic car and then a series of sort of
a more specific type cars and of quite
how that's yeah funk yeah
function dire what was it yeah what
would what was your question more
specifically I mean its base aspects of
being addressed well then you have the
hierarchical representation where at the
bottom level notes you have the
individual cars and then you would have
those boot into you know regularities
such as lows of cars and then there'll
be a no drive up which would be sort of
sort of parking
you know parking well not complete
parking lot but you know some parking
structure and so on and then the whole
note hi-low up which would be the whole
parking lot so the hierarchical modeling
is it you know intended to take care of
those those issues yeah seem recognizer
for like the map shots as you would for
a ground-level scene shot are they
separate they would be separate okay I
mean yeah
I know I mean the cues in common uh I
think yeah the viewpoints suggest two
different that to be to be done okay so
now I want to get in some other for the
last part of the talk I want to get in
some other talk with another person
called zoo but this is a graduate
student so long so not to be confused
with some showing until the Chinese
character is actually different even
though the anglicized version do is is
the same so here is an attempt to learn
probability grammars in an unstructured
way just from input images so here we're
not relying on the types of detailed
segmentations and annotations that song
chunks group has is producing because
well they weren't around when we were
starting this work and in any case when
we would like to see how far one can go
without having to have hordes of Chinese
graduate students to do that type of
work for you and so we're addressing
work in the Caltech 101 data set that I
guess the number of people may know so
here this was set up at Caltech by Fei
Fei Lee working with Pietro Perona and
she got a hundred and one different
categories
I think apparently Pietro told her to
get more than 100 so she went up to 101
and stopped there
and so this data set though you know
it's it's simple compared with what song
showing it's producing and the certain
criticisms of it but still it's it's
it's consider the good state of the art
you know it's it's something that people
are starting to use and compare what's
arts on and say there's certain was the
objects you have in it and this is you
know cherry cuckoo etc and below a
certain of our recognition results
performed on it the important thing
perhaps for us so much isn't so much the
you know the quality the performance
which is you know which is is it's
fairly good but it's more the concepts
behind it at least for this part of the
talk so you think of having a table
listing model or grammar that could
generate these objects and so the way
these things are done is you're given a
series of images and in each image who
is an object and it's you know a
you do not know where it is in the image
and you don't know what the background
is and the background is fairly
complicated so it's called I think Petra
Bona caused it
unsupervised learning so it's not quite
fair it's more sort of semi supervised
but still you don't have the detailed
knowledge of where the boundary of the
phases or the particular position or the
size or the scale so the idea is you try
and learn that grammar
incrementally from these sets of images
and so the first thing you do is you
assume that the image is just sort of
purely background it's sort of the
default model it's sort of like it's
random that corresponds schematically to
this graph structure here there's sort
of just a background generating the
image and that's it I should say the
this work here initially is done just on
feature points extracted from the image
and later on is generalized but so just
you can think of these as feature points
in the image to start out with you take
the image you run an interest point
detector on it you end up with 40
feature points something like that and
you want to explain them and they're
here attributes like the appearance
properties and so on and you're using
method like now here Brady
sift descriptors and so on so you start
out was by this default model we say
that's rather boring model but it's
where you need to start out with so then
you start seeing can you find more
structure more regularities in the image
which are sort of more likely than the
data just being generated by this sort
of independent model okay so here you
start doing it in terms of combinations
of Triplets of features triplets being
quite useful because from Triplets of
features you can get properties that are
invariant to the orientation and to the
scale so you don't need to have those
things fixed then you see whether this
model will explain it better than this
then you grow this model to a more
complex one see can you explain it
better and so on and you keep on adding
new features adding more elements to
this graph to this grammar while you are
still able to you know buy your put your
ability to explain the data goes up
you'll carry on adding extra features in
here so you all sort of grow your
grammar
over over time with data I know that
people are probably may be familiar with
Ada boost so you know ADA boost
algorithm you do feature selection you
use a certain number of weak classifiers
to do a task and then there's a
procedure which decides what's the best
you we classify to add on and into the
system well this is a little like this
except it's rather more complicated and
because you're here you're learning how
to increase the structure of a very
general public model rather than just a
sort of a classifier like Ada boost so
grow up with those things here and you
know the details of this are you know
well I can describe them later to people
who are who's sort of interested but
this gives you I hope the basic sort of
idea from that you could learn a grammar
of the object and this grammar is not a
very is a fairly simple one at the
moment because it's based on feature
points it would be comparable to the
types of models somewhat that have done
by Perona and Fergus and the
constellation models that people are
perhaps familiar with that literature
but there's a difference here everything
here is learnt completely unsupervised
and the grammars we're getting here you
know the the models appear true and and
the other people get have sort of fixed
numbers of points here with the grammar
you able to have different numbers of
points depending on different aspects of
the object so the motorbike can be
decomposed into this type of aspect
appearance for it here is another form
of appearance here is another and
depending on amount of viewpoint
variation you could have other aspects
being developed as well
another important other sort of
advantage of this price is is also once
you've learnt it how can you do the
inference and the form of the grammar is
set up so that doing the infants is very
fast so for example to so once you've
learnt this structure performing the
infants to find out if there is a
motorbike in the image or not is done in
about one second which by the speeds of
what you can do you know what attentive
methods here can do is it's very it's
very fast moving towards practicality of
real time efforts three tasks this was
little bit more this is just to bring
out the main sort of different tasks one
is once you have learnt this pub listing
model how you do infants which would
Mead involve detecting the object in the
image passing it finding its boundary
the other would be sort of learning the
parameters of the model when the
structure of the model is fixed and the
other is structure learning where you
allow the grammar to grow based on you
know based on the on the data so the
three different tasks related here the
influence is a part that's really fast
the parameter learning part is is
reasonably quick structure learning is a
bit slower of the order of several hours
but that's something that you do offline
in any case and here just to say some of
the algorithms involved you're using e/m
you're using dynamic programming subtle
point there's a certain range of
techniques sort of put in there to do
that that task so here coming up with
again more examples boosters pianos etc
faces aeroplanes and so on now this
these these methods are I say effective
his invariants for rotation scale I'll
skim over that because we're getting
short but the form of the representation
enables it to to do that think right
well here is sort of I think
conceptually interesting is that if you
don't say what is in the image it will
start learning the models individually
as different branches of the grammar so
before you would give an image which
would be to be a face in there somewhere
when you'd have that information as a
face in there now we're saying when
they're making it weaker there's an
image some of them is a face or it could
be a plane or it could be a motorbike in
the image and then it will learn the
grammar and one part of the grammar will
correspond to the plane once the face
one to the motorbike so it will start
learning that there were different types
of objects automatically without you
being told it okay
now these results here are I think
they're good and they're sort of
comparable or maybe better to the
current state of the art of the
representations which are based on
representing objects by interest points
but interest points are only a very you
know they're sparse representation of
the object and you are only getting a
limited amount of information from them
you're getting quite a lot of in you
know you're getting ability to do
certain tasks like certain
classification tasks seem to be possible
based on interest points because a grand
piano and a face you know they may look
very different just based on the
interest points but if you want to get
for more fine scale resolution or if you
want to actually find the boundaries of
the objects you have to go through a
richer set of vocabulary so the next
stage is to say okay we don't just
represent the image in terms of the
feature points you represent them in
terms of fake interest points then you
add extra features like it could be a
sort of a mask for the whole shape of
the object it could be edgelits and as
we can move forward with this we could
start putting in extra features so
eventually one would hope to get enough
features so that you could really
represent the object completely the
interest points are very good places to
start because they sort of been have
nice invariant properties there were
small numbers them they can get you
started they can get you you know they
can tell you roughly how big the object
is they can tell you the orientation etc
once you have those these other features
can be applied so over here we start out
with the basic objects just using
interest points we find the interest
points and that interest points will
start telling you roughly where the
image should be so here is a box based
around the interest points to the star
of this thing here is one based on the
interest points you found here
so the locates a certain area from that
then from this automatically you can
start hypothesizing a shape a model for
the shape of the object given again a
hundred examples and you can learn that
by doing inference of where that would
be using certain information from the
inside of the object and from the
outside the object again sort of purely
purely unsupervised and so from that you
end up being able to learn masks of the
object
which are reasonably good they were
going to screw up some places like with
the chair they're not quite getting the
legs of the chair very accurately but
nevertheless they're getting a fairly
good for most of the objects are getting
a pretty good model of the shape of it
and so you're going hear from sort of
the basic very sparse interest points to
a publicity model of the shape and then
you're adding in edges and so on so I'm
hoping that with this method by adding
more more features you can sort of
bootstrap your way up until you have a
model that generates not just the
interest points and the mask in the
features but could generate maybe the
appearance of the objects as well and so
with this you can not only do things
like classification which is what people
do on the Caltech database and get
numbers for this which are good or you
know it's like you know better or
something here than the other things but
also you can get methods for the
detection for the positions of the
boundaries and so on and I think this
would address one of the concerns with
the Caltech thing is that though there
were large numbers of objects in the
data set they're not really
representative of the domain and so if
you compare it say to the work on text
detection which we've done a lot on you
know we had to get examples of text and
then we had to get lots and lots of
examples of things that were not text in
the image you know thousands tens of
thousands even more non text things to
find out anything that you could confuse
with text and so if you take the couch
Act one and one data set and you take
the interest point models we have all
the models that other people have sure
there work quite well in the Caltech 101
data set but it's going to be a lot of
other things out there in the world that
they're going to mix up with these
objects because they have to limited to
restricted and representation of the
data you know so hopefully with what
we're doing here though as you add on
more features is use the interest points
just to get started you add on more and
more features and that allows you to
discriminate better and better between
these objects and everything out there
and then also it enables you to find the
boundaries and do every other thing that
you want to with it finding the
boundaries doing the parsing you know
saying hey that's not just a grand piano
but there were the legs you know here
are the keys
etc
so that would be well go so I guess I'm
finishing more or less on time so here
would be the conclusion slide so
generally are the group you know are
Harvard group the center of a vision
image and vision science myself and so
on we're interested in formulating
problem tasks like image passing using
sort of publicity grammars for objects
the orbits patterns with high-level
models of structure low-level models of
acting too you know it's a bottom-up
top-down method to you know to use a
low-level cues to test the hutt you know
to activate the high-level models which
then be checked confirmed and make
everything consistent and I think where
we've gone furthest on this so far with
something that's pretty good as the
automatic detection and binarization of
text with then moving on to these other
things so song song huge Genome Project
of images where with all these are
students and extracting these datasets
and getting these hierarchical
representations by hand and using them
for training and for testing etc and
then this other work I'm describing at
the end the issues of whether how far
you can go with learning these grammars
in an unsupervised way without having to
get Chinese our students to give you
clues part of the way so I think you
know hopefully these these aspects come
together and one can develop yet very
powerful methods this talk has been
mainly about computer vision I should
say I'm also very interested in how the
brain does these types of things and I
was earlier this week I was at a meeting
at NSF in Washington where you know
issues of how you would attack the brain
or model the brain from all different
perspectives sort of from physics
computer science statistics and so on
were coming out and so I guess I was
also arguing there for this as a type of
model that you could have you know we're
trying to build here a system that is a
sophisticated more or less as a human
visual system and why I can try and see
whether certain of the aspects of this
relate to the properties that we know
about the human brain and how its
organized and you know whether something
like this could be both sort of test
validated confirmed or all I could find
out that the brain is doing something
different better or possibly something
worse who knows so I think it you know
ties in here and it could also starts
using as a model for a sort of the rest
theoretical sort of new science type
model capable of of some test anyway
well thank you very much for your
attention and they could use most things
see it's only the one at the end we're
starting off for the interest points
these the things at the beginning for
the faces that was that could be patches
that could be any any type of
description that is yeah any anything
that wipe them can put in there so
there's no restriction to interest
points you really want to be able to
explain the whole image for the face
would look like which you would learn by
training data by having you know a
little bit like act for the face would
be a bit like the act of appearance
model so you'd have some sort of spatial
warp some models of the intensity and so
on the patch is there could come in as
sort of possible cues if you could
statistically show that certain patches
were highly likely to be present where
there was a face then that would be a
useful feature that you could put into a
bottom up a the boost detector but that
would be there merely to drive the
activation that the top-level generative
generative model to validate and to
confirm it
I'll call it viewpoint sensitivity
although you had an objective you know
rotation and scale invariance that was
for a given viewpoint on the original
image so there I was looking through the
kel-tec 1056 database just browsing yes
and it seemed as if very often the image
of the object always respected what
might be called the principal axis yeah
that you never saw a book face on like
this is there a precision recall
sensitivity issue in terms of the
perturbation from this train well there
isn't but all the system I described it
should be able to deal with that by
having a glamour one aspect being for
what the book would look like from front
on another for what it would look like
from sidon so at the top of the graph
given the right training data yeah this
issue I don't know if I could it's not
clearly shown here but but there's
different types but planes no it's not
there's nothing here which really quite
demonstrates it but here would be you
know this would be like an or graph or
could look like this all this all this
so if you had enough training data
it could happen automatically there is
an issue though of whether you are
whether that is the ideal thing to do or
whether there were similarities between
representation here and the
representation with other angles that by
going to a full 3d representation you
would you go away with so at the moment
yeah given enough training data this
would this sort of things should work
should deal with that but there could be
a smarter more you know not more better
representation
yeah it keeps on coming up I mean it's
an issue about certain of the machine
learning techniques a very
data-intensive you know you rely a lot
on them there's knowledge of how humans
learn you know humans seem to learn very
quickly from one or two examples and
then they generalize and babies do it
you know enormously well and so that
seems to be this interesting work being
done by Josh Tenenbaum and people at MIT
on that not directly in vision but on
the issue of how humans learn concepts
and it does seem there that in his sort
of models you've got a more structured
representation which means that you know
once you're you're you know gives you
more of the world more of the structure
and allows you to generalize more easily
from small number of examples and so I
think that's an area that I'm sort of
partly trying to push themselves in at
the moment since having enough data
works let me know how to do it I'm but
still it would be certainly more elegant
and more effective in the long term to
get at that sort of those invariant sort
of issues and that's a it's a
challenging you know scenario of
importance at the moment yeah it's how
close this is to be in production
quality if I wanted to say fine text in
Google Images what what would the what
are the issues standing a way of doing
that in a useful way
I think the finding text would work
pretty pretty well I would say since
we've tested on large numbers of things
the the finding text is a binarization
and then it's reading the text the
finding that takes I think it goes
pretty well back on that binarization
still we're sort of work you know that's
working pretty well but there were
certain failure modes at one the text
and again with more data set and more
labeled images you find ways around
those the reading of text
I mean we've sort of yeah we've taken
the apples of these things you put them
into OCR systems like a BB YY or things
like that
and it's surprising how often those
systems fail even when we give it
binarization that looks very looks you
know that looks very good so at some
level I think the limitation is you know
of those things as the
the text detection just the text reading
systems which raft were designed to be
done on sort of you know they're
designed to be done on documents like
these they're not designed to be done on
real-world images and I think the
limitations of those if you want to find
the text if you want to binarize it and
put it into a system that I think this
is pretty you know these things work
these things work pretty well for the
other aspects other things I think we
have not spent so much time on or
developed so far and they you know and
partly it has been an aspect of having
the data you know text detection stuff
we started working on I know five six
seven eight years back we started
getting images we started with you know
small numbers we tested we expanded etc
etc with these other things it's only in
the last year and a half that song
showing has got these people to get
these types of images and to go forward
also it gets into this issue of
variability text the amount of
variability is not as much as there
would be for a deformable object like a
a panda or something like that so and
the more default it is unless you have a
good knowledge representation system
that sort of is able to deal with that
the more deformities the more data
you're going to need to do it as to
stylize things like logos and stuff
where they take normal text and they
intentionally make it unique yeah I
think all those we do surprisingly well
except there were certain particular
cases where you find where you where
there are some slight problems if the
boundaries are done you know well for
example the tech stuff is based on black
and white images if you have color
images where the you know the text is
green on on red and the intensity is
almost exactly the same then the system
is not able to do that I I think there's
you know again with a bit of training or
a bit of putting in certain cues for
those things I don't think there's going
to be too much of a problem for those
infos the detection stage and maybe of
the binarization stage but certainly
yeah those have been frustrating in the
past
yeah well he showed the person if you
showed the person Google Images mixed
together with face images and just
learnt it I it would be crowd yeah we
haven't done that maybe we should try it
and see what see what happens
although Google faces and human face
well um the other issue is at the level
of interest points Google phases and
human faces are a bit similar I mean
they're different enough so that when
you learnt the model they are able to
distinguish between them but in the
stages where you're trying to learn it
without telling the difference between
the kuhu and so on it might not work
just with the interest points I think
when you start putting in the other
stuff it's going to it's going to go a
lot better but with interest points
alone you might get mixed up until
you've gone far enough to learn the tool
owner for the full model
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>