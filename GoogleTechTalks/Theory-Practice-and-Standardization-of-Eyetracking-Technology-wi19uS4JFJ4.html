<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Theory, Practice, and Standardization of Eye-tracking Technology | Coder Coacher - Coaching Coders</title><meta content="Theory, Practice, and Standardization of Eye-tracking Technology - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Theory, Practice, and Standardization of Eye-tracking Technology</b></h2><h5 class="post__date">2017-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wi19uS4JFJ4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well thank you very much everybody feel
a little as though or preaching to the
choir here you guys know an awful lot of
my light rising and I'm probably going
to say a bunch of things that you guys
are know but I'm going to put it
together in a way that sort of girls
historically and builds up how I
tracking got to where it is today
because we stand on the shoulders of a
lot of people who have done a lot of
relevant research over the last hundred
fifty years and it really understanding
what they went through makes it a lot
easier to understand where the
technology sits today I'll see
technologies it was basically founded on
hope and the expectation that I tracking
has a stunning future and that
assumption is based upon two fundamental
tenets the first is that the human eye
is always looking at what's most
important to us now and if we're going
to build the basis out there in the
world that will help us they need to
know where we are looking because they
need to know what's important to us now
and they need to be able to respond to
that so if you had a teacher in broad
areas of application or education and
entertainment there are a whole bunch of
I don't want to get into a lot of
details there but if a teacher a reading
teacher knew what a kid was looking at
if that kid was going through the text
and knew where his gaze was they give
their eyeteeth to know that information
an eye tracker will give them that
information if we divide devise
automated adaptive planning oh geez that
can teach based upon what the kids doing
now it can be a lot more effective than
it is without eye tracking and similarly
if somebody is doing a PR game if they
if the system doesn't know where you're
looking
how can
it haven't really realistic interaction
with your psyche at the time so those
that fundamental premise that humans
communicate I - I we've got to give that
same characteristic to machines and then
machines can be a lot more like humans
and that's the fundamental premise of
Elsi technologies was founded with 30
years ago
so there are ought to be an eye tracker
in every computer device and that are
tracking available ought to be available
to any application that's running on
that device so look at building our
trackers the concept of one of the eye
tracker looks like I'm going to review
three things in this in this talk first
we're going to talk about some of the
key anatomical and physiological
features of the eye and the Beast that
we want to measure and second we'll talk
about some of what the early pioneers in
the eye tracking industry have done why
they did it and what they learned and
finally we'll review a whole bunch of
our tracking performance metrics that we
have to design - so this picture you're
all familiar with it's a standard
cross-section of the eye you know all
the parts but there are two particular
features that are important but I want
to point out at this point and one is
the angle Kappa and the angle Kappa is
important because our eyes do not look
in the direction the foveal vision of
our eyes is not perfectly aligned with
the optic axis or of our eye and so when
a measurement comes on on our eye we can
see where the optic axis is pointed but
we don't know exactly where the foam
Yola is on the back of the retina so we
have to make a correction to that if we
don't make that correction correctly the
accuracy MRI tract
is out the window the second feature
that is important here is if you look
very carefully at the corneal surface of
the eye it's not spherical most eye
tracking software makes the assumption
that it's fundamentally spherical but
there are a couple of things that learnt
that don't follow that model first off
there's a severe flattening of the
corneal surfaces that I it gets out
towards the edges so that means in the
people center Corning a reflection
method there's a non-linearity that
happens out there but needs to be
accommodated in the software and it's
different for your eye than my eye is
different between my right eye and left
eye and if we don't calibrate some of
that information it's not going to track
me very well and if we really want to
hear lie tracking we've got a calibrate
people that's a pain then not to
calibrate people so but we have to do it
if we want out here at eye tracking sure
it depends upon how sophisticated your
ellipsoidal model is because the
ellipsoidal model can actually
accommodate some of that smoothing if
you make them you want elongated ellipse
it actually starts to smooth out towards
the edges networks and another really
nice thing about the ellipsoidal model
is that you can put astigmatism into it
and so that accommodates several of the
really key features the variability in
the human eye so that's that's a
wonderful thing to do
terrific thing to do there's another
feature of the eye that is extremely
important and it really allows eye
tracking to happen in the first place
and remember that eye tracking when we
when we evolve our eyes when nature
evolved our eyes are two competing
mechanisms going on one is we want to
have a very very wide field of view and
the second thing is we need to have high
resolution you can't have both
and if you have both if you had 180
degree field of view and all the
resolution that we currently have in our
central vision or optic nerve would be
about this big around would be about
nine inches across we have a half a
cubic meter of brain function in the
occipital lobe to handle all that stuff
it's not going to work
so Nature found a really nice way to
compromise on that so it gave us
relatively low res in the peripheral
region with very very high res 70 times
the intensity of cones at the center of
the eye and across the across the pupil
so in this diagram is trying to meet
because it shows that these green cones
up there show how densely packed the
cones are as a matter of fact the cones
are so densely packed in there that they
are just the receptors the cell bodies
of the cones in the central formula part
of the eye exist outside the central
region of your retina and all they do is
send the wires with the receptors to a
little green things and then as you get
further and further away from that the
center of fold Yola all the little
purple dots in here represents a
represent the cones in your eye and when
you get wet outside the region then the
cones are fairly dancing all you've got
right and rods and cones do different
things this next slide shows is a more
scientific representation of what the
rods and cones distribution in the eye
is so here's this really sharp spike at
the central region and the interesting
thing is that when you go out to the
peripheral vision there's almost as much
rod density is there is cone density and
this is the thing that people forget
when they design displays and
particularly the concept of foliated
rendering has to take into account and
that is there's not you can't make the
assumption just because you've got high
cone density in the senator welcome
density out elsewhere that your eye sees
blurry stuff out there yes it is bore
you can't make it out by homeless hand
out here and wiggle my fingers I can't I
know because my own proprioceptive
feedback that I'm pointing up or down
but you miss a lot of things but the I
said oh there's a hand out there but it
doesn't necessarily figure out at that
point there's not enough information to
figure out which direction your hand is
pointed well I think you can figure out
your hand if a hand Rob salutely still
you couldn't make it out but if it moves
any those rods are terrific it's
figuring out what's out there and moving
in the environment because it wants to
figure out that there's just leopard out
there that's about stalking you and you
need to find that thing and you need to
make out what it is so if it's moving
you see it and then you swing around
your eyes swing around and look at the
same so this then brings us to the next
stage of what's going on if we have this
nice central vision with high resolution
and low cone resolution out with
periphery that means that the eye has to
move it can only look one place at a
time if it wants high resolution and I
want to look at your right eye I can do
that but I can't see your eyes very well
I can barely make out a face over there
so that means that we have to have we
developed that nature went to all the
trouble to develop this concept of
fixations and secants
we fixate our eyes right on one place
those muscles that we have you see up
here the ocular muscles hold my
extremely still for those 250
milliseconds that it takes to get a good
enough photons to get a good image at
that foveal area and I can make out
get the detail that I want and then snap
those same muscles snap your eye over to
the next place and take a picture
someplace else
this happens roughly four times a second
and four Hertz rate our eyes are darting
around and they hold extremely still
they snap extremely fast
cick add to the next place and they
fixated so that behavior resulted from
the design of the eye to have the
balance this wide field of view versus a
high-resolution trade-off so on the
output of the eyes goes back
fundamentally to the occipital lobe the
visual cortex on the back your brain
where it does all its magic and figures
out what's out there all of us gives us
all the feedback that we need to
interact with things but I talking
there's another really important element
of the brain and that's the superior
colliculus the superior colliculus is P
would you get me my pointer on the back
of my briefcase there the superior
colliculus is that part of the brain
that in one sense adjudicates figures
out for the whole brain where the eye is
going to go in its next fixation so the
FC is that you
so the FC sits right there right the
center of your brain so the FC gets data
from all over your brain and all kinds
of all your different brain functions
want some visual input everybody starved
for visual input well who wins your eye
can only look one place at a time it's a
winner-take-all strategy and the SC is
the one that we makes the final
resolution that tells your
where you going to go so if you're
reading a book the next thing you want
to do is move your gaze to the next
chunk of text if the baby screams off
there and write someplace you know like
that part of your brain wants to go pay
attention to the baby and then if you
happen to be walking down the street at
the same time you're reading the book
this is a silly example but it it
demonstrates the point and you trip over
something your best simular system says
you better look at the ground because
you're going to be breaking a fall
so it's copy three inputs all at the
same time and it's kind of defined which
one to do keep in the back of your mind
well it's making this decision it's
making the decision what's the most
important thing for you to look at right
now because it's that fundamental
process in the brain and the fact that
we have a central vision that enables
eye tracking that's the fundamental
reason that we can do it is because that
information nature poked a hole in our
skin to let the photons in to our brain
that allows all of this to happen and
we've got to take advantage of that to
build machines that have the same kind
of eye contact that we humans do so the
SC is a really kind of neat democratic
thing what it is what it's able to do is
get these inputs from all over the brain
and if you unfold the thing it's like a
map and you can see areas poked up if
your eye is pointed right over there and
the peripheral vision sees something it
says all one of the cat and that many
degrees to go over and look at you
instead if you happen to be waving your
arms at me you do not that's what it
would do and something else the
vestibular system says something's
happening over there I want to scan to
go over there it's got this map and it's
building up these potentials of all
these important areas it's being very
clever about it because each part of the
brain says I want visual attention but I
want it with a level 6 then the your
vestibular system comes back and says I
want to
so that spike grows faster on the
surface of the superior colliculus map
of your of your retina and finally one
of these spikes pokes through a
threshold and bam the superior
colliculus fires off and that's where
the eye goes and that's that that's
where the next fixation is so the key
point here is that unlike Keizer can't
see what all the alternative options
were when you made the decision to go we
can figure out that problem that'd be
pretty cool but one thing we can see is
the final decision that it did make and
that decision is the most important
thing to your brain at that point in
time so the role of the SC and then I
try he is a really big fundamental
element of what allows us to get jeebies
goals of getting systems that make eye
contact with you so what happened how
these guys build this equipment in the
first place what's going on well you're
sure oh these are the visual pathways
the red is the purplish left eye and
notice so that goes through the the
optic nerve the optic chiasm the optic
tract back here to the pulmonary region
some signals go back up into your
superior colliculus others head back
into your visual cortex and then a from
the visual cortex they go up to the
frontal lobe and from your frontal lobe
that's where a lot of the decisions are
made feedback down to the visual cortex
they all cross over for some reason your
right eye is connected to the left half
of your brain and vice versa and one
thing I don't quite understand that's a
fabulous question is what happens in the
central region in the in
phoE vo love your brain where is it
connected is it divided down the middle
I don't know
doesn you knows who knows it Queens
University did a lot of the research on
that found out the connectivity of of
all the sort of stuff he did it with mer
keys they were nasty experiments but he
found out an awful lot of the stuff and
that's where it was verified if these
crossover things happened it was
speculated before that because
somebody's have a right brain problem
damaged right brain and the right I
wouldn't work and everybody thought well
that's interesting but it wasn't till
munis in his work we actually proved
that that was the case so early people
in the research researching and eye
tracking wanted to find out how was the
brain doing is what's it doing
and so this misguided Delabar back in
1989 built a device this is not nella
Mars device by the way but he built a
system
he took a needle and he stuck it in the
eye he took a little piece of plastic
and plaster eluded to the eye and
there's wire sticking out this needle
sticking out with a string coming off of
it and the string dragged along
something on a strip chart recorder and
he recorded all these eye motions and
you could see from the stripped our
chorus there was fixation saccadic
sessions to CAD fixation see cab so he
demonstrated all that with that crazy
equipment one hundred and seventy eighty
years ago so I Tracking's not new it's
been around for a long time long time
and then this guy shackle and there's
several other people in this history but
shackle and came along and he figured
out that there's this potential across
the eyes if you put a pair of electrodes
on right left eye to your eye there's a
potential across there it is your IRA
rotates from side to side it presents a
variable resistivity that can be
measured in measured
across the eye a lot of the hell a lot
easier than sticking a needle in your
eyes patching a needle to the corneal
surface Eri so a og came around and that
was really cool technology at the time
and then Yama's I'm sure all you guys
know Yarmuth but the keen thing that
yamas did he actually don't som need eye
tracking equipment and pass this around
this is device so this device somebody
put their head not in the in the
headrest here and these can Cameron look
at their eyes and they recorded the data
with video so video eye tracking finally
came on we'll see when these guys he was
doing this work in the 50s and the 60s
1950 and 60s so this is even at work now
is 70 years old and he was able to
collected the byte those video cameras
recorded all of it and then by hand
frame by frame they'd go through and
calculate the the corneal reflection and
the pupil Center calculate that data
that information from that person's eyes
and mat and my about the gaze trace it
was beautiful research and what he found
was well his chief purpose was not to
build an instrument he did that he'd
built a wonderful instrument but the key
thing that he found out was what people
do in visual search and they do exactly
what you'd expect them to do but he
documented what you'd expect him to do
is on somebody watching the room you'd
expect them to take a general look all
over pick up the big picture something
on the fact they're interested and
they'll start zeroing in on some
particular detail and he documented that
that's what happened so that was really
another fundamental piece that was
putting together this whole picture of
how the I behaved so then there were
these three guys merchant Mason well two
two guys really a merchant Morissette
these guys are working for Honeywell and
Honeywell had a contract they have the
site
the end that they took out to
wright-patterson Air Force Base that we
would they wanted to actually control
something with their eyes not just see
what they did the user isin the control
device so what they did was come up with
this idea of well the basic application
was they wanted to do targeting so pilot
was flying a plane he'd look over at
something he'd say that's what he would
need you'd say that's what he wanted to
do and he would indicate to a target
acquisition device by pressing about
once at the target acquisition device
actually picked it up you push the
button again and the missile would go
off and kill the target really cool idea
but they'd have to do this now they have
got to get eye tracking working in real
time but they didn't so they got they
got a camera that could capture the
image in real time and digitize it they
took all the algorithms for computing
the pupil Saturn and projecting the gaze
and they put that stuff into computer
programs and they got gaze point
calculation programmed up gaze point
propagation algorithm and when this
thing worked it generated eye tracking
data automatically and in real time
these guys were the right in my opinion
these guys are the Wright brothers
equivalent of eye tracking to flight the
right horrible Wilbur got lift power and
control all saw simultaneously to get
this plane off the ground by 120 feet
stabili and land and these guys Marchant
larson built the first eye tracker it
was automatic real-time and pretty funny
good accuracy they use the pupil Center
corneal reflection method which is a
standard today so back in 1969
these guys published their results 50
years ago and we're still using that
concept today so these guys lay down a
tremendous foundation what how I
tracking them work they use the pupil by
pupil Center method as well and LLC
technologies does that today a lot of
other companies have abandoned that
because to get coaxial lighting is very
very difficult to implement that in
hardware in a small piece of hardware
but it gives you the very good image
processing results because it gets very
good contrast between the pupil and iris
is that you can get an accurate pupil
Center measurements much better with a
bright people effect as you can with a
dark people effect so what happened
Martha personal Morrison works well
there were three big problems with this
device first off they implemented the
image processing algorithms of a pdp-11
computer that sat in the corner of the
room they were never going to get that
in a fighter plane number two the room
had to be dark
that wasn't the environment that you
were in that a pilot would be with a
county over his head and the sun shining
in no way that was going to work so
let's say there was a third pump oh yeah
and it didn't allow any head motion he
had to hold your head pretty still your
whole you're looking at my Yarmuth
device it was the same type of deal to
get a high enough resolution image of
the eye they had to use a really high
telephoto lens that only had a very
narrow field of view you could move your
eyes very much you had very much and I'm
the eye tracker continue to see you so
that project just basically died and eye
tracking sort of went underground for
about twenty years
but one thing really did come out of
those guys works and that is a list of
eye tracker performance metrics and this
is a hodgepodge list I mean the list
goes on a lot of gets boring and I'm
sure you guys are fairly firm and with
it but I detection accuracy the image
processing not only have to be able to
measure the corneal reflection in the
pupil center but if it doesn't recognize
the texture the fingers of an eye in the
first place if if it takes the corner of
my glasses it can measure what it thinks
to pupil center coordinator and
reflection perfectly you nearly ate
garbage data so you have to have a good
detection algorithm the differentiates
between I and I objects then you have to
the gas prediction accuracy you have to
be able measure those things precisely
then you have to have some kind of
freedom of head movement and then you
have to have the ability to accommodate
a whole bunch of different eye
variability many glasses and contacts
powers 2mb infrared light ease of
calibration safety there's certain
maximum permissible exposure that you
can put on the eye and it should gotta
work fast enough if I work really fast
in certain applications and then 5/8
power and cloth all those things last
and list the things that you have to do
so there while problems with industry
that's just a summary of them so I want
to just give a few examples of images
that have some of these problems that
are difficult to process reflections
caused a lot of clutter in the images
this is this is an example of a person
sitting in a room with a remote eye
tracker they happen to have glasses on
but you can see this window in the
background reflecting off glasses making
a mess of that eye image most image
processing algorithms look at that thing
they stop their hands and they say I
don't see an eye in there you know I can
see it so we know the information is
embedded in there
but we haven't yet developed enough
really small algorithms to dig that
information out of that cluttered image
and then over here is an example of
reflection of the LED look I try his own
illuminator or I can't see it here but
there's a big reflection off the glasses
itself and there's some internal
reflections up here and you can't tell
which of those is the real corneal
reflections they're a bunch of glyphs
there but how does the image processing
know how to dig out the right corneal
reflection the right Flint that has the
corneal reflection here's a case for
glasses the reflection off the glasses
are getting right plus to the M is VI
covering up the corneal reflection
completely that's not going to give you
any data over here on the left we have
contact lenses contact lenses have these
little things stamped in the middle of
the you and I can't see but since high
trackers work in the infrared region
they reflect off this thing and they
come back with this little symbol you
figure out who made the damn contact
lens that's with zero interest eye
trackers but they can get confused by
that data in an image and we got to
design algorithms that just say I got it
ignore that data and proceed on with
calculating the eye image harmine
bifocals the camera is looking at your
eye from coming up looking from below
and it typically sees your eye right
through that cut in the glasses so the
top half and the pupil seems to be one
thing you see it through the pop lens
and the bottom half through the bottom
lens where's your Center and a pupil we
gotta find out or the handle that stuff
so this the contrast on this is fairly
poor but the real corny or reflection is
sitting right there and then that
corneal reflection straddles the
perimeter but
in the pupil of the iris so if we
include this thing in the pupil are we
going to compute its pupil Center right
it's sitting right on the gradient the
corneal reflection sitting right on the
gradient between the people and the iris
is coming up where is the center of the
court of the real center of the corneal
reflection
it was processing has got to handle that
problem another big problem in eye
tracking is measuring the range to the
eye it's extremely important that we
know the distance between the lens and
the corneal surface of the eye not just
some place on the center of the eyeball
but bright to the corneal surface of the
eye because the grip you 'pl factor so
here's here's a standards let people
factor when somebody is sitting in a
normal location looking at a point on
the screen and then as the I were to
move backwards there's a
double-barrelled effect of making it an
error in range the first is if I were to
move back and you didn't know it you
being the eye tracking instrument didn't
know it the whole pupil vector would get
smaller first because that is just
further away and the whole image got
smaller the second thing is that as your
eye moved backwards that's still looking
at the same dazed point it literally
started to point downward so it's
orientation was different but if we
didn't know those two things were
happening we would project a very
different gaze when we project somebody
looking way down low on the screen so we
have to measure gaze accurately that's
one of the things that I'll see
technology spend a lot of time I was
being able to measure that we use what
we call the triangular the asymmetric
aperture method and it's based upon the
fact that the corneal reflection looks
like a point source of light out in
space and it's located right behind the
cornea or behind the corneal surface
itself 1/2 the radius behind it but
that's where it is if the camera lens is
in the right place that
all these rays coming from my cornea
reflection back to the lens and put into
the sensor surface on the camera will
converge in a perfect location but if
for some reason the eye move forward or
back or conversely an act in in an
offensive plane the camera the lens word
lengths were too long or too short you
would be dazed these mÃ©lenchon that
point wouldn't converge on either side
but there's a really cool property and
that is if we make a triangular aperture
whose shape is different upside-down and
right-side up we can look at that
corneal reflections and it will be
blurred on either side but we can tell
whether nothing is right side up or
upside down because of a triangular
aperture so if we can design a control
loop the aid that drives this thing back
into focus we're good we've got it we
can measure that and we can use the
Gaussian lens equation to calculate
exactly how far the cameras focused and
so we can make very very precise
measurements down to a tenth of an inch
in that tenth of an inch is extremely
important for getting at your gaze point
calculations going wrong direction here
sorry
large and small pupils this is a problem
when the pupil Center method or what's a
bright people method because big pupils
typically laid a lot more light into the
pupil and a lot more light back out so
they end up being very bright and tiny
little small pupils tend to just get
very very dim and we have to design
image processing algorithms that can
handle acquired dynamic range of pupil
amplitudes yes and you can see it back
there in the demo after the fact you'll
see all that stuff we've got it
demonstrated so you can actually see the
mechanism how that works
this is a particularly cool example
because there's a tear glint down here
light from the LED is going and
reflecting off the tear glint in the
bottom there's a children's at the top
there's a charity list down there at the
inner campus and there's a real cornea
or reflection and there's another
reflection that comes from an internal
thing in the lens how do you find the
right corneal reflection which one's the
real one if your image processing
doesn't handle that you're going to
produce bogus data eyelashes come down
this is not good contrast here you can
see it beautifully in my machine but you
can't see out here very well but there
are eyelashes coming in how do you map
the perimeter and that pupil because
your eye does that clutter screw up if
it does your eye track they're going to
work very well
we got dry and congealed corneas
we ll see technologies works with people
with disabilities and so we see all the
corner cases and a lot of people with
ALS for example have lost their blink
reflex not only can't they speak in the
caves their hands their eyes the ocular
muscles still work because they're
connected directly to the brain rather
than going down to the brainstem we're
all right there's loss of connection
between the brain and the parts of the
body that the brain wants to control so
your ocular muscles work but your blink
muscles no your eyelid muscles don't
there they are connected back down
through the brainstem so the data you
get there if somebody doesn't blink you
get congealed tears all over the front
of your cornea you got to be nice to
handle that in this case you have dry so
you don't break enough this is the
corneal surface doesn't get lubricated
properly
there's no corneal reflection there
there's a nice tiered wynt
but there's no corneal reflection what
is your image processing do that
if your eyes are motion you get a very
blurred image of the eye so you have to
take high-speed photography to get
images that are still enough that you
can interpret correctly one of the
serious problems in eye tracking is
squinting if you if you split you lose
the corneal reflection you don't have
bumps the corneal reflection and a pupil
Center the PCC RMS it doesn't give you
the answer you need so you need to start
putting other illuminators around at
different places that when you're
squinting it will continue to work why
is squinting a problem mostly because
when people are playing a game they
start to get intense about something the
first reaction you have is the squid you
just want to say what in the world is
going on there and you split to look at
it and if you're depending upon that eye
tracker to track you while you're doing
that if you squint you're in trouble so
we need to tackle that problem two LLC
technologies has taken a good step
toward that we did it for people with
disabilities because they sweat for
different reasons they've got ptosis of
the eyelid or what we call droopy
eyelids their their eyelid control as we
said before is working perfectly so the
eyelids start to come down and pinch
particularly on enlarged pupils like
this one so our it was processing
algorithm which is that people if it
says oh the island actually is including
the top edge of that thing and so when
it fits the pupil it doesn't just find
the center of mass of that thing it
finds legitimate apparent pupil
perimeter it fits only to that data
there's a little secret light tracking
or a concept an eye tracking it's not
very well understood and that is the
pupil Center does not open and close
precisely about a given point you've got
a sphincter muscle
constrict your pupil and radio muscles
that open it up and when your pupil
opens and closes it doesn't go around
some nice constant point there it's law
our people may drift over here a large
peopled river and if it starts to drift
around the pupil Center corneal
reflection method doesn't know that the
pupil Center shifted and so it's using
the old calibration ascending a
calibration of the pupil center of the
diameter when the Cal when the
calibration was taken and then it starts
to drift a lot of eye trackers have that
problem so we're trying to solve that
problem too calibration you have to have
a calibration if you want accurate eye
tracking it's really all I'm going to
say there there's a Elsi technologies
have built this thing that we call an
iPhone line there's a fundamental
trade-off and eye tracking between
accuracy and tolerance the head motion
if you want a nice tourist the head
motion the typical solution in the
industry is to widen out the camera
field with you good we can see wherever
you are we can get your eyes we can
track but how written how well can you
measure the people center corneal
because people back here is the
resolution goes down you can't and
there's this crazy trade-off of our eye
trackers now have a very narrow field of
view but you need a wide field of view
so camera technology will get you a
factor too maybe for in dynamic in field
of view but we want 10 15 times the the
field of view that's not available in
camera technology we can't get that high
resolution so we basically borrow the
model from now our own human eye and
that is we have a robotic device where
the eye tracking camera is actually on a
gimbal it moves around is your head
moves around and we have another camera
wide feel the camera says oh that's
where the head is
so the eye tracking cameras recording
off here someplace
in your heads over here the wide field
of view camera just like our peripheral
vision oh there's the head go up there
and boom our system will saccade over it
fixate on your eyes and start tracking
your eyes so we get always a very
high-resolution image of the eye no
matter where your head is and at the
same time we can track your head over a
large range so we've got our own little
robotic system to do that and you'll see
a demo of that in the back you don't
need that for all applications and
you're doing VR glasses that's not a
problem but form own eye tracking it's a
very good solution so where does the
technology stand today well there are a
lot of pretty simple quote good enough
solutions the systems that are out there
now they track a fair number of people
they try a fair amount of the time but
they don't get everybody all time with
high accuracy so we've the industry's
got a long way to go they'll make that
thing work to be really robust across
the entire population we need to make
them yet smaller and cheaper today
there's these little bars coming out
cool you made it smaller but you gave up
all our accuracy you gave up the
robustness and tracking that's not a
good enough trade-off today we've got to
do better than that so we've got to have
all kinds of fancy innocence I was going
through all those images showing about
how you process different kinds of
images problems in images to be able to
cover a bottle range of the population
and accuracy really remains a key
problem so the next step there are
obviously two different categories and
product design their remote head
trackers and head mounted eye trackers
so we have to design both of those for
different applications Hardware
miniaturization is really a key it's a
this ly important thing we've got to
build much smaller cameras we have to
build cameras that are adapted to the
specific purpose of eye tracking eye
taking off-the-shelf cameras and just
throwing them in there making work same
for the lenses there's more glass and
stuff in those off-the-shelf lenses that
we ever need night tracking we need to
throw most of that out but there are
certain characteristics that don't exist
in the lenses that we want this should
be built into I tracking lens that
optimization has really never been
addressed adequately in the rest of use
in the rest of the industry but the key
thing is I was mentioning before we
still need to do this image processing
we've come a long way in that area but
there's a lot more work that is a
tougher problem of all of these things
the hardware is really pretty
straightforward you get a bunch of good
engineers in a room and they'll make it
smaller we can follow those problems but
the thing that handles the diversity of
the human factor in our eyes lies in the
software and so we have to do that and I
believe that LC technologies has delved
into that problem better than anybody
else in the industry well thank you very
much you guys been a bit of fun talk and
good question</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>