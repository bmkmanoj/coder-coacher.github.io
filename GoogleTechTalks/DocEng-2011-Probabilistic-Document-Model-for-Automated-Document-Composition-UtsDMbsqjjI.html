<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DocEng 2011: Probabilistic Document Model for Automated Document Composition | Coder Coacher - Coaching Coders</title><meta content="DocEng 2011: Probabilistic Document Model for Automated Document Composition - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DocEng 2011: Probabilistic Document Model for Automated Document Composition</b></h2><h5 class="post__date">2011-09-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UtsDMbsqjjI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">your engine is a researcher in HP labs
in Palo Alto just up the road isn't a
document automation lab they're working
on work which basically supports his
paper thank you very much
so this talk will be about some of the
work that we did at HP Labs on
probabilistic modeling for automated
document composition one of my quarters
josei bentos here from Stanford
University he was an intern when part of
this work was being done so why automate
document composition and you saw pretty
good reasons in the keynote address but
essentially just to repeat that you have
lots of current publishers looking to
target audiences right so they want
personalized content there's very
specific to their user preference and
also you have to deliver this on a
plethora of device formats so the
necessity to you can't have a
human-in-the-loop designing everything
only you'd be surprised how much manual
design goes on and actually you know
delivering to pads and so forth so
technically what what is this involved
automatic document composition in a
broad sense requires the arrangement of
multiple content blocks of varying size
number and logical relationship onto
multiple pages in an aesthetically
pleasing manner now are shown on the
left side here you know a typical
content structure you know you have a
bunch of text blocks and they're
typically could be an XML I'll just
shown it as a figure four for example
and you know you have logical
relationships and you have some figures
over here and the logical relationships
here referred to you know if a text
block refers to a particular figure then
you know in the layout you need them to
be close together
another type of logical grouping is a
sidebar grouping where you know specific
text and figures that are in a sidebar
need to appear together on the same page
and so the objective is then to arrange
these in a nice manner and you know
device the size the figures and
everything make it look good
aesthetically pleasing but what is the
static pleasing that's kind of one of
the issues why automation is hard you
know just given the content structure
and each person gets on his own content
structure you just get some text and
figures and to lay it out you need to
know have an understanding of what looks
good in the end
what's what's esthetic there's no
Universal model for human aesthetics I
know there's a lot of work done and a
lot of this conference on you know these
macro models of aesthetics we can try to
try to model how many and balance of a
document and so forth but you know it's
still a hard problem and even if
documents could be scored on quality you
know there's an infinite or intractable
number of possible documents to score
and as an example in this corner and
also to introduce some notation will be
useful later on I'm gonna give an
example the carton sets C is a content
to be composed now in this case it
consists of three lines l1 l2 l3 and a
figure f1 while the set of possible
content allocations to one or more pages
is a much larger set to begin with for
example you could have you could
allocate the figure first so the first
page you could allocate 3 lines of text
to one page you could allocate 3 lines
of text to 3 pages and so forth so for
each of those allocations that are
possible you have multiple ways to
arrange content on the page you can put
the figure on the top figure on the side
you know off to the left and so forth
and for each of these possibilities
there's an infinite way to manipulate
image sizes you know image can be a
little bit smaller a little bit bigger
the white space can be a little bigger
and smaller what do we do so so
essentially you can see even for this
case technically there's an infinite
number of possible variations on layout
so if you look at the state-of-the-art
and automatic document composition I'm
just going to refer you to the two fine
review papers there to kind of go
through the pros and cons of the various
methods but I will say that our method
unlike you know XML based frameworks
like this is a LIFO and so forth and for
the advances on that like DDF it's a
mathematical framework rather than an
XML framework to do layout and it tries
to incorporate or try to unify a lot of
the work that has been done on you know
handling page edits edits on a page
pagination problems and templates
selection for layout into one framework
which we call the probabilistic document
model
now the first thing we need to
understand is a paralytic document model
is a representation of designer choice
so we try to model the choices that
designer makes and creating a document
and you know we think that there are
three fundamental choices the designer
makes in designing a document there may
be more but we model three fundamental
choices
the first is pagination you know what
content should go on page one what
should go on page two and so forth we
modeled that with this with what we call
a random set C and the less than equal
to I say is that for C less than equal
to one it's all the content up to and
including the first page C less than
equal to 2 means all the content up to
and including the second page and there
are many possibilities for that's why
it's a random set you can pick say in a
set one that I've shown here that's one
possibility you can pick something
different so you have random
possibilities to choose from to make a
content allocation to one or more pages
then you have the content arrangement
problem so even a designer thinks about
how to layer count he thinks ok how am I
going to arrange this on the page and in
his head you may have many many
possibilities how to do this but because
we can't have this infinite
possibilities we try to model that with
a template library so if you have a
large template library essentially we
model the choice of a relative
arrangement to use as a selection from
the template library so each of these
has an index so T takes the value 1 it
means you selected the first template t
takes the value 3 you selected the third
template and so on and finally you have
the notion of page edits I mean should I
not this should I make this a little bit
smaller make the whitespace a little bit
bigger and so forth so in this case we
can think of a template as being
parametrized by theta which is you know
a bunch of Thetas which are basically
parameters of the template and they're
basically all continuous that's another
compounding problem because it could be
infinity possibilities right some of the
specific things you can consider for
example how much white space should be
between elements and so forth and this
is also a random variable because for
every choice of parameters you won't
have a different different layout for
example if I made this theta 1 and theta
2 smaller the figure is going to get
smaller
from 882 3 larger than you know you're
gonna have more white space on the page
and so forth
and these are some of the design choices
that we try to model with these random
variables and the compounding factor is
that they're not independent you can't
just deal with one of them and you know
because they're all coupled you can't
make a decision about arrangement
without also having done something about
the pagination and so forth so the
probabilistic document model is
essentially a unified model of the
design choices that explicitly couples
together these design choices in an
interesting way so there's many ways to
couple this design choice when you're
thinking about layout in your head you
have many ways you can you know you're
doing pagination all at the same time
with with with relative arrangement
selection and so forth but we've chosen
a specific way to couple these design
choices together and you know these
arrows show the dependency between the
design choices and the reason we do this
is mainly to we told you why the
automation problem was hard one is the
lack of a well-defined model for
aesthetics is one and a complexity of
the problem and so this formulation
tries to you know get you a well-defined
formulation for their aesthetics at a
local level which is much better than
macro definitions of aesthetics and also
tries to have efficient algorithms that
you can solve to get the layout
computationally fast now in this
structure you can see that each of the
design choices here is associated with a
probability so and those probabilities
basically depend on the parents so for
example the design choice for the
parameters and pointing care the design
choice for the parameters here depends
on the prior design choice for the
template that you use so so what does
this mean so let's start from the
beginning so you choose the template
right so you have a design choice about
the template and you look at the
probability distribution that kind of
defines a choice and you say you if you
tend to prefer certain templates for the
first page then you know those templates
will be preferred for the second page
you know different set of templates may
be preferred so this just indicates
preference of templates and so forth the
second one the theta is basically
parameters for the template now notice
that it's now
by itself it's a conditional property so
it's a local decision that you have to
make based on given a template you know
certain parameters are more preferred
than other parameters so this is where a
designer somebody who needs to know
machine may not know which parameters
are more preferred right so a designer
would know for example given this
template you know this white space
should be between 20 and 50 points and
it can vary widely it's not not a big
deal and so forth so this preference and
notice that this preference does not
have is not depend on contact so it's a
prior preference before seeing the
content so it's like a prior probability
of these parameters finally you know
given the template and the parameters of
a template if I fix the template and the
parameters of a template essentially
I've given you a static holes in the
template right and if I then also give
you all the content that's been placed
on the prior pages then I can I'm
basically giving you the design choice
for a pagination problem basically what
I've done there and it said if you have
some content to place on the current
page then then you know how well does it
fit into those slots so I've turned that
the pagination problem out the local
decision about you know how well content
fits on the page this probably also
encodes references that you mismatch
because in the prior content that you
see over here so the prior content
actually may have some references that
are skipped from the current con so the
probability will be low or higher
depending on whether you skip references
or the logical structure is not met so
essentially this is a micro model for
aesthetics every design choice has its
own aesthetic probability distribution
but you can say did you make a good
choice or a bad choice and so you put it
all together overripe ages and so you
get we essentially get then an aesthetic
model for the whole document that is
composed of these micro choices this is
different than a macro aesthetic model
which in designing if figuring out the
harmony or symmetry or anything like
that each of these decisions can be more
or less well-defined and only the human
actually really comes in only for this
one because it's abstract a machine
cannot no machine can decide whether
things fit but a human needs to decide
you know which is more preferable so
using this model
actually generate documents you can
sample you can take a template you can
sample parameters you can and then you
can fill content into that template and
then you can evaluate and see how well
you did and obviously you try this many
times you're not gonna have much luck
it's like a monkey throwing darts right
you're gonna you're gonna end up with
bad scores but we can then ask ourselves
the question is there a sequence of
design choices that you can make that
will actually maximize this over all
possible documents and all possible page
counts all positive when I see a
document the document is nothing but a
sequence of design choices a document
composition is a sequence of choices for
range means the choices for the
parameters and charges for cartel
occasions on the page so this is what we
come to next how can we efficiently
solve this model in a very general
because right now I've not made anything
specific it's it's applicable to an
arbitrary document composition at this
point at least the model list so our
objective is to optimize this so we go
back here and do you know from high
school algebra we say this function has
variables these are the variables of the
function we go one at a time and try to
say how can i optimize it with respect
to theta you know i then take out theta
and then optimize it with respect to t
and then optimize it respect to the
other one so first thing i do is collect
the terms or involve data so these two
appear to involve theta so i'd group
them together and maximize over theta
now interesting thing pops out this as I
told you was like a fit term well how
well content fits the page and this is a
prior on the preference for the
parameters so what this tells you is
basically that in maximizing this for
theta if you have more than one way to
fit the page you're going to choose more
aesthetic outcomes because this will be
the same and then you know this will
dominate so you're gonna choose more
aesthetic outcomes another way to think
about this is if you have a prior on
aesthetics and having seen the content
the prior becomes a posterior it was
your belief about the aesthetics is
changed and and this becomes a posterior
probability and this is a map problem
maximum a piece posteriori estimation
problem which also makes sense so once
you have this in any case you've
eliminated theta and so you have these
other three variables and you go back to
your previous thing and you see you can
now I
the other variable has leftover combined
it with what I just had and do an
optimization to remove T so here I opted
for the templates and I get this other
function that is that does not include
the templates and then I go through for
each of the content allocations and I
Anna and I and I do the same thing and
essentially end up with the recursion
now what you can see at least you can
think of these as fundamental equations
coupling those design choices that you
had and you can see the design choice
for the pagination depends on the
arrangement which in turn depends on the
on the page edits and so they're all
coupled together in a nice way so anyway
when you continue this finally at some
point you'll hit when you exhausted the
whole content and see less than equal I
will be the full contents see we just
started with and then you go look and
say of all these possibilities which
which choice of page count will give me
the highest value and you pick that for
the document count I mean how much pages
the document should be laid out in right
and once you do that you have this and
the content and you can back substitute
you can go back here and say now I know
which page count it is I know this value
it's just this final value and now this
is only one variable problem and I can
solve for that variable so that's a
backward pass basically and you can when
you read the paper you can actually go
through it more detail and figure out if
it's not immediately obvious but you you
basically land up with an optimizing
value for for the content allocation you
can go and substitute that you know now
you know these two we can substitute in
the previous equation and get the
template and keep going the nice thing
about it is that we just did basic
algebra we didn't do any suboptimal
thing I didn't make any approximations
or anything like that so solution is
globally optimal with respect to our
aesthetic model and is equally the fully
exploring the infinite design space
including the parameter in you espera
meters and everything provided that you
solved you know I mean you're able to do
this maximization so what's the
fundamental that snice you got an
algorithm it's able to evaluate thing
but how complex is it that's the key
question right so here you know
basically I've just replaced those C's
you know for notational convenience this
B denotes the previous content and a
denotes all the content including the
previous content and this shows that map
optimization problem now even bees come
from a set of possible allocations as I
kind of introduced you to that earlier
that is C prime and so you know if you
can take all possible values for a and B
this will be order C prime squared it'll
be quadratic and C Prime and if I look
at this one
because I have page count coming in here
which page count is proportional to the
to the content itself because you know
the more content you have the more
number of pages this is going to be and
this essentially dominates the
computational complexity of the problem
but you can do better than that because
you know if we bound the content on a
page then you know then basically what
we're saying is on a particular page we
want no more than four figures and 300
lines of text right I mean if you know
your page you can say well that that's
that's reasonable if that's the case
then you know this B does not grow
because B is tied to aim it just always
close to it doesn't grow with the
content and so you know this complexity
you can lose the square there and so it
becomes that also C prime is basically
dominated by you know the content
squared as you can see over here as the
content gets large so this is basically
a cubic complexity in content so the
your document composition in this case
is cubic in content but we can do better
right now we've assumed that you know
for each page you have a different
preference over templates so you know
certain templates for page one or more
preferred but if you were to say there's
no such preferrence
then you know oh I mean this lose that
the dependence on I goes away and you
can do a neat trick what you can do is
you know when you're maximizing this
quote the Tao quantity with the previous
equation you can just move it inside you
know this because there's no term I here
so you can move it all the way in and
you get a recursion that is independent
of I and hence the term that depend on I
the C goes away and now you have a
quadratic complexity finally you know
this quadratic is because you know
essentially it was C prime just C Prime
you know we substitute a C prime C
squared for C Prime
now there are certain case when C prime
is equal to C plus one actually it's
pretty close to C that's the case when
you you allocate content in the order in
which it is that so you don't actually
allow figures to float of too much in
other words you know if you had you know
if you had you basically you allocate
content in the order that was received
so if you had a figure then it either
has to appear on the same page or the
next page as the prior content that was
just before it and you know if you're
willing to make that choice then you
essentially get a linear complexity now
it becomes linear in content so without
making any specific thing I mean
assumption other than you know just
coupling these design choices we've come
up with a model and also a solution for
that model that gives the optimal
aesthetic score in linear complexity
with the content which is nice now the
question is how do you actually
parameters you actually use the model
you need to actually put in some values
for the probabilities right I mean
otherwise it's just theory so in this
case what we do is you know we have a
very simple model for now and later
we're exploring ways that we can learn
the model because once you have produced
iqbal one of the nice things you can
start learning it from data right but
for this work right now what we do is
have a designer set this now we remember
theta given T is the preference it's
preference for certain parameters over
others now I'll take a very simple
example of a white space right between
20 and 60 points and if I give a mean
and variance typically designer gives a
mean variance min and Max so if I give
it a tight area then basically it's
saying that you should not I mean you
want this value to be enforced very
carefully if I give it a loose variance
then you know it can be changed you
basically design is specifying is intent
that B that parameter can actually move
freely right and if you have multiple
parameters algorithm will automatically
trade off and say figure out which
parameters it can move more freely and
which parameters it doesn't move more
freely and so forth because in code in
aesthetics now this means are lonely
they're diagnosed it's kind of wasting
this multivariate model because you know
now it's only getting a diagonal if you
just do every parameter independently
you can also encode linear relationships
among the content you know anything of
the form CT term equal to D you can put
a precision on it and and you can get
some off
you can basically write a closed-form
solution that gets them off diagonal
terms and this is especially useful when
you are dealing with aspect ratio so
let's say you want the image to be close
to its original aspect ratio but you
want to allow that aspect it should
change
this is nice because you have a lot of
image retargeting algorithms today which
are not being used that much in layout
will be nice to use them so so you can
say theta W bar theta H is approximately
a but precision row and the precision is
very high then you said you know don't
mess with my scaling right just let it
scale but if the precision is loose then
you allow the reformatting algorithm to
do some reformatting on it and we also
need to define fit so basically to
define the fitting score we we calculate
paths to the document and this is just
one instance if I'm sure we can come up
with other parametrizations and so forth
and based on this we can just turn this
graph into into into into into matrices
and basically make the computation of
fitting along each path so along each
part you're looking and saying how well
is the width width of the page is the
width of the base page being filled by
these parameters or not right and it's
the height of the page being filled or
not and then here you have the term
which I was talking about which is the
the logical structure of the document
which says are you missing any
references and you can penalize the
missed references the nice thing about
it if you put these two normal
distributions together the product is
also normal and you can technically
write down the solution in closed form
except you have some you have some lower
and upper bounds and so it becomes
actually a constraint least-squares
optimization problem otherwise the
answer just would have been a inverse B
right so once you solve the innermost
problem is sufficient to solve you
notice the innermost part does not grow
with content we're in business and then
you can actually it shows an operational
view of what you're actually doing you
know the designer encodes these
templates and styles the content comes
in and you know without any knowledge of
style and then you'll be able to churn
out the layouts here's an example input
I'm spoken about it but we can support
alternate versions
optional images can be supported you can
say I want the I want output in two to
four pages or one page two pages so
forth
here's a basic template editor is
showing the different relative
Arrangements you can generate you can
pull it in and you can set the
Precision's and the means and so forth
on different things and you can you can
play with a little bit more style sheet
for the fonts and everything and you can
also put some banners and other
decorative things and apply them to
templates it's a very simple basic
editor so you have some content here you
know more content it reformats yet more
content multiple pages pops out
different content I mean and here we
just want to compare with hardened
coatings hardened coatings of aesthetics
basically says that it's basically
evaluating fit because what it's doing
is one-way constraints say you shall
start this document I mean ten points
below there's a ten points above that
and it's only evaluating fit so in this
case we can be more flexible because you
know here the image is allowed to move
to vary to fit the content because a
prior was defined loose it's a low prior
if you had a higher prior on the image
then the white space moves up and and
and is able to accommodate the content
and in this case no matter what you said
the prior has because the prior is only
a prior the likelihood function
dominates the prior in this case you
want to fit the countdown the page and
so it just ignores the prior so that's
nice to be able to if you want to
enforce the prior only kind of informs
you if necessary so this is multiple
document format I mean the same document
because I've tied each parameter to to
the height and width of the page so it's
it's automatically adjusting to to
reflow the content and reformat it
making those design choices appropriate
for that form fact or whatever so we
also did the HP labs 2010 annual report
and that's downloadable here we were not
allowed aspect ratio changes so
basically and the main consideration was
to have images appear close to where
they're actually being referenced and
you can see a design choice here where
it actually made the image slightly
smaller scale it up so it matches with
HP labs Singapore and but it could have
put it on the other page but it made
this choice to make it optimal you know
it had sidebar and some extra stuff so
in conclusion we have a general model of
automated document composition the
specialization is not general by the way
the specialization refers to a specific
class of documents and the specific
probability distribution
but you can hope in future people will
come up with better parametrizations for
this type of thing we naturally
incorporate soft probabilistic
constraints and can be used to
synthesize layout so we got an efficient
grader synthesize layouts
we've also any I know this is Google
Talk so we also have a nice MapReduce
implementation it's very conducive to
MapReduce on large-scale clusters model
distributions can be learned from
examples which they'll probably present
it next time you know we look at more
sophisticated models like multivariate
mixture of gaussians which you can learn
with vm algorithm and so forth and right
now the main sticking point is basically
how do you model non-linear complex text
flows around images in a way that's
easily per a parameter izybelle and so
forth thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>