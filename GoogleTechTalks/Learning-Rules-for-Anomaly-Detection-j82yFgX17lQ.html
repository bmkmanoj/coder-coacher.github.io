<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning Rules for Anomaly Detection | Coder Coacher - Coaching Coders</title><meta content="Learning Rules for Anomaly Detection - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning Rules for Anomaly Detection</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/j82yFgX17lQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I just I want to gauge the audience a
little bit how many of you are familiar
with let's say the term machine learning
how many of you are actually doing
research or development work in machine
learning okay good how many you have
let's say papers or something like that
in icml or kdd type conferences okay how
many of you I is familiar with the term
anomaly detection okay all right I think
I could kind of accommodate this
audience here so we are going to talk
about a particular algorithm call
Laurette and okay there's no advancing
here so let's okay they talk about
learning algorithm color red and it's a
randomized algorithm and what-ifs
purposes just learn rules that represent
normal behavior and detect anomalies
accordingly I'll go through what a long
a detection means a little bit let's say
in a security framework it doesn't have
to be security I mean can be anything we
just differentiate what is normal and
abnormal you could be let's say a
shuttle flying up and what's normal
behavior of the shuttle or your car was
the normal behavior of the car being
driven on 101 or something like that by
the way on the way to here there was a
accident in and I almost got stuck so I
guess that could be a anomaly maybe it's
not in California I mean it happens all
the time I don't know they were
helicopters flying and stuff like that
was like I guess I'm not going to make
it so so first of all let's form a
intrusion detection perspective as in a
security perspective there are major
league two approaches of attacking the
problem one is usually what you get on
your computer with like a virus scan of
a file photo scanner mostly that
basically it knows about what the
attacks look like your story in the
database and you just match
you get with the database okay so you're
modeling a text okay you're not modeling
the bad things so generally police are
here generally for that approach you
cannot detect new attacks for example
you're very scanner when the new attack
comes what happens so you probably know
that so a number touching is another way
of approaching the problem of intrusion
detection instead of model during
attacks now we are modeling normalcy
okay what's normal behavior of the
system and then identify things that are
deviating from your model he obviously
that model can be hand written but in a
more in a CS way probably that you want
to apply machine learning or data mining
techniques to generate these models
automatically and that's the focus of
the talk today and obviously that it can
potentially detect novel text there's
the main advantage of it okay so it
doesn't know anything about attacks just
know about basically normal behavior so
anything that is deviating could be no
could be all could be existing could be
new novel text unfortunately one of the
drawback is that the false alarm rate is
high as in that is deviating from the
norm be a but doesn't mean that it's bad
okay so these two approaches are pre
complementary to each other and I don't
think that one can replace the other one
probably some hybrid pose later on would
be a good thing to do
in intrusion detection system it depends
on what kind of data you are looking at
analyzing you could be on the whole
space side that means you're looking
from the OS perspective or looking the
application perspective or you can on
the network's I that means you are
looking at network traffic packets
connections stuff like that there are a
couple of algorithms that are already in
place I'm going to go through that the
slides will be available at Tech Talks
database anyway so we're just going
through a little bit quicker here one of
the key thing that in the algorithm is
trying to approximate certain public
events which are anomaly events so for
example let's say that given some data
that has no attacks so it's all normal
voice probability of certain event X
happens okay the central idea here is
that what is that probability okay so so
given training data that is normal I'll
approximate what that probability is for
event X now if X was observed during
training then you can approximately
using standard techniques what happens
if X was not observed okay so even
though also observe what is the
probability of something that you never
saw before that's a pragmatic issue to
address so sometimes you get I'll call
it a cero free bottom you never
observed before how I even asked emmett
that probably so remember that we are
interested in the likelihood anomalies
that since things that are not normal
things that you might not have seen
before so let's back up a one-step with
a kind of a simple example to help you
understand the problem and what how we
can estimate that in the in some fashion
consider like sequence number one here
you have a BCDE bfg CH ok now we want to
estimate something that we have not seen
before ok let us say
letter Z ok it's not in the training
data so what's the property likelihood
of that occurring zero frequency plot
you haven't seen it what is probability
ok let's look at sequence to you have
AAA BBB AAA BBB AAA things like that so
that to sequence here you have the same
question why is the liking of seeing and
then seeing z ok now I'll give you like
a little question here which sequence
has a higher level of chz ok this is a
sequence number one sequence number two
and we want to estimate the likelihood
to see something brand-new that you
never saw before so it's a see you don't
we don't need to have an exact number
just by a gut feeling or you can apply
complex web of that mathematics in your
head as well so it's a sequence to our
sequence one has a higher likelihood to
see something brand-new it's a question
clear first okay it's clear so I'll give
you 10 more seconds think about it with
your vote okay whoo thanks sequence one
has a higher like equal to see something
you good how about sequence two okay
very good sequence one so someone raised
your hand and can someone explain to me
why that's the case yes please more
values already that's Marvin okay more
different values already has three the
length would also matter about the seem
to be saying that okay good very good so
basically that the first one has more
letters we can think about that and now
we're looking at you is that this is
more random than this one this has some
sort of penance 8am via repeating so you
have a higher likelihood to say i'll
expect a and B after that and so yes so
intuitively that you would say sequence
one would have a high likelihood to see
something you okay so let's say apply la
paz smoothly the at one technique
so this turns out to be don't worry
about this exact number here i just want
to show you the relative ordering is not
what we want so using that plasma thing
in certain sense is 1 over 19 here is 1
over 13 here so sequence 2 is higher
than sequence 1 in terms of observing a
new event so this is not what we want so
another way of estimating the novel
probability is all over n all it's a
number of unique values in the training
leader and it's in total number of
observed values so the more you have
observed in terms of unique values then
you have a higher likelihood to see
something new so obviously you want n to
be large to large sample so that you can
have a better approximation this is due
to with Annabelle so in our example here
the sequence 1 and 30 days 8 / 10
obviously tens I / 10 is a very high
probability but you can think about this
sequence very long like 10,000 milind
and then this would be a more
approximate a better approximate so here
you can see that in our case over n here
eight different values eight unique
values of eternity bands he is 20 / 10
so obviously sequence one has a higher
likelihood and this poli-sci with our
intuition with the sequence one should
have a high link to see something
brand-new so basically we kind of touch
on the problem of zero frequency problem
you haven't seen anything like that
before what is the probability okay this
is one possible way there are many other
ways as you're always point out two ways
so many other ways so this is a very
simple way to approximate and with the
direction that we want you so
now given that how we can that's the
basic idea behind zero frequency problem
and we want to approximate not just a
single you man but want to also
condition on certain other events not
just like a prior probability but like
conditional probabilities okay posterior
puppies let's say that we can look at
just a possibility Benghazi or you can
approximate something you've NZ given
certain other event or you can even
better that a process some even given a
bunch of other events ABC bubbled over
now in tribute the action let's say
dealing with network data let's say that
one of the attribute is IP addresses
right okay so how many possible IP
addresses there let's say not worried
about the new ipv6 yet let's say the
regular oh I p how many possible I ppl
is out there I'm good four billion a 22
32 very good so just one attribute okay
let's say a here one attribute is four
billion combinations let's say you want
to model source and destination address
so b is the best nature's in 12 asos b
is a destination IP addresses so you got
four billion times four billion this
will be 16 what I don't know trillion
culturing monitored for billions time
four billion combinations that's web
large number I think Google always deal
with large numbers here right so
probably not at the Google level yo le
know how to spell google the real right
way at geo geo what gol gol oh my so you
know what I'm talking about the right
way of spelling Google anyways so there
are lot of combinations here so it's not
easy to jump on let's say these kind of
abilities to up to these kind of
abilities okay the lot of combinations
of events
okay so then we need to do something a
little bit faster okay so we're going to
look at algorithm based on sampling
techniques that we don't need to
actually estimate everything the four
billion time four billion times four
bits and type things okay so we only
need to look at the regular occurring
events to know a bit more than our
behavior just when we pick the ones
somehow to estimate these properties we
don't need to actually estimate all of
them so where a is an algorithm try to
sample some of these some of the
important ones so to speak to
approximate before we talk about the
elephant we want to let's talk about the
output okay the output is something like
the attribute that we saw before okay
and implies certain event the Z that we
are approximately is an element of these
values ok that's a bottle blue look like
now this is different from association
true if you know about those things it
stops here we allow set of values of a
single value Association we will allow
any single value we all have different
from a classification rules because this
is not a fixed attribute if you know
about constipation we like wipro CN c
4.5 rules those are particular classes
they are predetermined to be the right
hand side consequence so first of all I
want to point out the output quoting
called the ruse a message is different
now what does a rule mean what's the
semantics of a rule so this is a rule
and actually what we are trying to
estimate is that given the antecedent
what is the likelihood that it's a new
Idol a new value that we have not
observed think about that these are the
values that we are offset the ABC
previously so it's not a movie that is a
novel battle the zero frequency type
thing that either training there so we
ask you made that by using the olive end
the little approximation formula that we
use all is the size of this set here so
I mean unique values there are ok
so n is the number of instances or
events that satisfies the antecedent so
the rule applies Liz and basically we
are trying to approximate this
particular probability okay by using our
n so the algorithm looks like this okay
in a very simple way a simple create
candidate rules okay using a randomized
algorithm we talk a little bit odd
quickly score somehow ranking them and
make a design that which might be better
than the other ones and are we going to
remove some of them okay there's a spay
on a training sample to speed up the
learning ok and here that we're going to
update the rules saying that using the
entire training set and and at the end
we will validate see what it is we are
going to be really good at protection so
we try to evaluate its performance on an
unseen validation set so the first step
is to create the candidate rules so the
first thing it does is a randomly pick
two things let's say two instances ok
and then if let's say that these two
instances out here a 1 B 2 C 3 d 4 D 6
here then we find the attributes that
are matching so it will be a B and C and
then we create a different combination
of rules that's based on the matching
attributes so in this case since ABC are
matching so we can make either a B or C
to be the consequence and the two
attributes would be in the Edit even
I'll coordinate with the same battles
and see here is a question mark later on
with because of set battles we're going
to filter than later on now the
mechanics of the vendors are simple but
why do we want to do this why do you
think that if I pick two things all of a
big pot so to speak and if they match
then I would see more
all of them why is that actually I gave
you the answer or I should have added a
different way but let's go back
wise to set in 25 seconds last five
seconds here okay think about okay let's
say you have a big part of ok Google
utilize colors right you have blue
orange yellow green okay you have all
kind of color balls in a big jar right
you okay big candy jar or whatever okay
and the candy jar is not transparent
that clear is dark you cannot see
anything you randomly pick out two balls
and they are both read what can you say
not much yet okay let's say you randomly
pick again there are two balls and
they're also read again what can you say
now let's say huh a lot of red balls all
right so if you randomly pick something
and you don't know what they are and
they match the more times have you do
the same thing again and again you
haven't more and more likelihood as
expecting that more of those things are
rare ok that's a primary idea behind the
this simple mechanism basically you
picked through things and then basically
the more you they are the same thing
then you expect more of them ok so let
me give you the back side uppy let's say
that the the jaw the candy jar you have
has like millions of candies and they'll
need to read balls let's have million
balls and two of them are red what's the
likelihood of picking two red balls very
slim right ok so the idea is that so the
flip side is that you this very small
amount it's very unlikely to pick two
things and they are going to batch so
that means that there would be more of
those
not not there will be if you are very
very very lucky you can still pick to
read Bohr's oh well 1 million other
color balls right well probably you can
win the lottery too so so you could do
that but the likelihood is much less so
if you randomly pick through things and
they match there's a like there's a
higher likelihood to see more of those
around so again if they match there are
more or less lying around potentially
not definitely so we create this rule is
to capture that potential penance I
think they might occur more and more
often okay so we use a training sample
to estimate how good are these patterns
now how many of those exist as in how
many times to red balls coming out a lot
of time so so look at these rules here
it's the same three rules now we updated
with the with the sample train example
and we calculate that how many of those
are two three four reviewing the web's
remember that it was a question mark
beforehand here's a question mark now we
fill in the various based on the
observation in a sample of the training
data so how how likely they are better
so they matching let's say in this case
and again it's a number of times it
matches so it's basically so yeah here
all n is 100 that means 100 of these
things around based on the training
example okay and all of these hundred
samples the variation is very pretty low
it's only three possible values in the
consequence so a lot of let's think
about business a color or something and
this one is they're not attribute I know
the size big or small or something like
that so it's only has few sizes that you
can see but the colors are the same and
that 100 of those okay obviously that
the bigger n is the better relative to
our so we want to end to be big that
means more times that those are matching
and that r is small because
us we don't want this to be a huge set
if they are 100 let's say 200 matching
pairs and it have 200 items here then
that is basically everything is unique
in terms of be so we want large and
small are so the larger that score is
out of our the better is so basically
the flip of the zero frequency
probability thing so after we have some
score as we made a score based on a
train example then we look at how we can
actually reduce the number of rules that
we can throw away a look at constitute
on the high quality was again basically
high quality probabilities that we want
to estimate so we already talked about
high scoring there's something called
redundancy check some rules are or
something soon the other ones so you
don't need the other ones so you need
two more general ones you don't need a
more specific ones and that's also cover
each at X and we talk a little more so
these things can eliminate some of the
things that we need to consider that's
okay we doesn't see as seen that some
rules might be more general the other
one and you don't need to keep those
most specific rules let's okay we run
into here so rude to they have their
they have the same consequence here
Andrew one it's more specific than mu 2
this is a more general condition so
route one is actually a redundant so
we've totally array we don't need to
worry about that and let's say rule
number three ah it's like this and two
and three tone really correspond to each
other so we don't worry about that let's
say roof for is something like this then
roof is more general and then we kept
that I throwaway number three so
basically just you keep the more general
ones and throw away the more specific
ones the coverage check actually covers
actually includes the redundancy check
we don't need to perform that check
separately so the last step here
actually does everything let's just want
to illustrate the idea so so first
what the first thing that we need to
understand is that you know to realize
what the coverage that is checking is it
number one one rule you want to be
covering as many examples as possible
that means a general rule that means
certain pattern exists over across
multiple many potential many instances
however a tuple or instance I event does
not need to be covered by multiple rules
you just need one to explain that
particular to polar instance okay you
don't need multiple of them so that's
the coverage test I mean that's the
major motivation of what what we are
doing the coverage test or the reason of
doing it so the other is a relatively
simple greedy algorithm we check the
rules in descending order so the higher
score one will consider first so for
each rule then we mark the currently am
up to post they are covered okay so if
the rule cover new clues to posts then
we keep it basically if there's no new
coverage we move that rule so that rule
does not help me explain the data
there's already a previous rule that
explained it data so there's no new
coverage and we can show that actually a
coverage absolutely that's so basically
the cover chapters all three things that
we were talking about a higher scoring
here we doesn't see an extra comfy chair
so that helped us minimise the rule set
one important thing that probably google
people knows is that you are dealing
with large a month data you want your
algorithm to be fast right so if you
have a huge a bunch of rules to check
against data that will take a lot of
time so we want to minimize that okay so
we basically help you Fisher see
purposes and the final training
basically saying you start looking at
the training sample now we have is
minimal set of candy rules okay
and we basically finalize what the end
in our battles are a populated
consequence how many possible values
there are so I paid a consequence to
note that develops here and up to the
score which is in over so the final
status validation so actually the
training is only ninety percent of the
available treatment later or whatever
percentage we can choose and then
actually ten percent is for validation
that means that is not available during
the regeneration phase just what we
talked about before okay so remember
that all the data is normal okay
assuming all the data is normal in the
tree that means the violation set is
also normal so if any of the data inside
the validation set the ten percent that
trigger a violation of the rules that
means that rule is not generating up
that is overfitting so if the data item
cause a false causing a violation of the
rule basically it's a false alarm
remembering the alarm detection false
alarm is a big problem so here this step
is guarantee not guaranteeing we're
trying to reduce the false alarm rate
okay so what happened to things that you
never seen even in the train later what
do you do with that so here we take ten
percent out that the training data are
the the new generation air weather has a
seen before see whether we can break
some of those rules if we if you break
some of those rules those are removed
I'm goes directly to the experiment the
operator said 90 90 was basically a
evaluation back in 99 by DARPA with by
multiple projects and stuff like that we
used week 3 as a training testing for
four and five weeks four and five acid
so that means the violations that will
be part of ten percent of week three
these are summer sizes here of the
training data
what are the attributes that we use
first of all that's a version called
Larry at TCP which look at TCP
connections okay we assemble the package
or we met we assemble the connections
based on packets you look at the last
two bites of the destination address for
advice of sausages and port number
duration lang tcp flags and payload okay
usually most of the algorithms does not
though they don't use the payroll
information only the header information
so this is one of the few work that
actually does that and for all here we
also look at UDP attributes as well so
all the attributes in TCP as well as you
DB and ICMP head of use result during
the type of evaluation they dictate that
you cannot have more than 10 false alarm
per day okay it's reasonable they were
thinking about reasonably that a system
mean would be able to handle 10 force
once per day they are totally 201
attacks 74 of them are hard to detect
different by lipman basically those are
attacks that were not detected by the
e-version participants well mostly okay
new attacks of variation of existing
attacks that the current participants
could not at that time in 2,000 that or
99 that could not detect for Larrick ecp
based on tcp attributes we detect about
fifty eight percent of those we're also
more interestingly the tech sixty
percent that the other system cannot eat
it well okay and we have similar numbers
for Larry oh that means using UDP
attributes as well the best coin call
participant during that evaluation was
called expert one okay according to the
paper and they detected 85 about forty
two percent so we did better however
mean to be a little bit more cautious
about these days
numbers are not directly comparable
because we are able to run the test
multiple times day but cannot they
basically is a blind test they submit
their results however most of these
systems are based on signature based
systems okay that means there you have
knowledge about text in our system here
there's no knowledge about text is
totally pure anomaly detection systems
so we don't know anything about text and
still detect more than they do that's
probably a significant thing to think
about now we talk about efficiency now
some of you might hurt us not as an idea
of course not how many rules our date is
not ballpark 2000 good I'll learn who's
again it's not our basically signature
rules right specifying attacks we learn
about 50 to 75 rules that's two orders
of magnitude lower than someone just
gave me an estimate of snuggling okay so
you can tell that it can be a very
efficient system okay there are some
other numbers here probably not very
relevant it because this is you have
much faster computers and stuff like
that so this are much more data and
stuff like that so so we can do it in
minutes with gigabytes of data so they
are starting with gigabytes of data here
dealing with minutes here and then we
can actually doing the detection time
and training time those are in seconds
now part of the KPD talk is looking at
doing the validation phase we actually
throw away rules remember that we want
to reduce false alarm rate which is a
very big problem for anomaly detection
so since we don't know anything about a
bad thing so so false alarm is can
happen very easily so we move so however
remember that we have a coverage Chester
minimize the number of rules so if we
remove some of those that means removing
coverage
I could although we are reducing the
false alarm race but also we might also
increase the missed detections as in
reducing the detection rate so we
figured that hmm if we spend so much
time let's watch out sometime in
reducing the set to be 50 to 75 rules in
that practical experiment by throwing
some of them away then you're getting
really few rules so that might not be a
good idea so instead of pruning all
these rules we keep 50 to 75 rules
around but we wait them as in the
samples are more important than other
ones or more trustworthy the other one
so instead of being basically pointing
that we evaluate the move decrease the
way okay not throwing them away so you
can say seamless if you're throwing away
basically starting a way of zero and any
of its conforming it would make a
increasing our confidence increasing our
trust to that rule then we increase the
way a very simple idea I'm going to go
through the all the algorithms but a
simple one called window so basically
you have two parameters alpha and beta
alpha is a penalty saying that you be
cause false alarm during the validation
phase then I'll decrease the way from
one starting from one damn so alpha is
something between 0 and 1 let's say that
if I if you confirm and saying that you
the rule is not generating for salami
increase the weight a little bit another
way we look at it is another element of
assigning ways is that we want to look
at the weight total sum for penalty and
reward we want them to be equalized as
in total amount of reward is the same as
a total penalty for all rules so we
first calculate the total penalty TP
here and then distribute the increase by
the number by the penalty so recovery
first the total sum apparently and then
reward evenly to all rules that were
good so bad rules you have some total
pedal
penalty and the good will be distributed
the same amount so more details is in
the kid ed talk in a little Tuesday on
Tuesday or something like that okay this
time we're going to look at multiple
data sets in addition to davao dataset
network and host data so these idea
values the top of you said there's a
university one I think that's the f louw
co network data set hows whys there's
also University of oil should be n UNM
University of New Mexico and also us and
university of tennessee right nostril
this is system call data and this one is
a scripting commands in windows how he
bella is based on the ROC curve anyone
familiar with OC curve okay so I'll see
curve XXX is false alarm rate y-axis is
the detection array and the area under
the curve the larger the better
basically so area under the curve the
larger the better so here these are the
three just pruning here it means the
original algorithm here the window is
the first waiting out of the second one
was the equal weight algorithm these are
all the data sets here so the last
column here tells us that how many times
that this particular algorithm is
waiting our beats the pruning algorithm
so here you can see that five times it
beats the printing I'll go down one tie
and then three times worse and here they
are actually six times letters so in
this particular experiment seems like
equal a pope of proportioning is doing
better in terms of waiting so you can
see that in general that waiting is
better than pruning in this particular
experiment so based on multiple data
sets here now we also on this try to
understand why are we performing better
in this experiment okay there are two
possible explanation one is what we were
predicting so to speak is that because
pruning we move
the rules okay it's are we moving them
we're just increasing the way we
actually based on we look back look at
the rules whether they were removed or
not and then decreasing the weight and
we find that we have additional 80 new
attacks based on this particular reason
there's also additional reason is that
although the rules are being kept by
pruning that's in they don't generate
false alarm during validation phase but
we also increase their way that means
increasing their their confidence or the
trust of that so that actually has a
little contribution so the most of
contribution is based on what we were
hoping for contribution coming from not
throwing away those rules but that but
to decrease the employers okay what's a
time oh good oh there's a clock right
there alright so another more recent
work is looking at mobile data let's say
you have laptops you have cell phones
what happened it is cell phone is stolen
okay what happened you have in a moving
attack on your mac address someone is
masquerading you okay running around
Google campus pretended to be you using
your mac address okay which is easily a
done in these days so the question here
is that can we look at your spatial
temple behavior and detect whether there
are anomalies in those behavior so
special temple means time and space so
basically when based on your movement so
it's not looking at the navigator or the
whole state that now i'm looking for
geographical locations with respect to
time so
so the clearance of question is that can
we come up with some approach come up
with some techniques to detect anomalies
based on your geographical locations
with respect to time and we look at it
if I'm a publicist a viewpoint let's say
that there's a spatial distribution that
you train during the training time ok so
d here ET here is the certain day of the
week let's say monday tuesday wednesday
something like that t is a time let's
say our like right now it's 11am or 12
or between 11 to 12 or 12 to 1 something
like that um it's a particular mobile
device so a particular device like your
device ok it's cell phone laptop ok
mobile device and this is basically
during training time so what is the
public distribution based on the train
deal now let's say during detection time
you want to estimate that based on your
current location how likely that you are
supposed to be there based on the
training so we look at look back to the
form the current location bag with some
window size called w here with let's say
double instances let's say five minutes
is one instance so you have 20 minutes
then there you have for instance or
something so you look like our window
size so LC is the current location with
respect and then the previous location
you just were so this is a joint
probability we want to find out the
current location and the previous
location where you just were before how
likely that is the case ok following the
textbook chain rule so this is John Paul
beta you can break it on you can break
it down into the conditional
probabilities and then your LC based on
the previous up to to the last one is LC
minus 1 basins or condition on C minus 2
standard chain with textbook stuff now
all right so we all know that this can
be a company Nick Torrio explosion again
because there are so many variables here
and these are not there are multiple
probabilities and multiple tables that
you need to come up with okay so there's
a lot of time and space estimate these
probabilities okay usually we make
assumptions just like other probability
people do independence assumption in AZ
basic Network and stuff like that those
use or naive bayes classifier those are
making some assumptions about depending
on certain things but not other things
so embracing classifier aiding the
independence of something seems to work
well as in that just based on it with
the condition of something else so the
joint probability can be expressed as
just simply multiplying the respective
poverty at each location not conditioned
on the previous steps basically okay so
just remember quickly so here what we
did if you make a naive assumption
basically each location is independent
of the previous location then you're
taking out this part okay the
conditioning is gone so because this is
not dependent on the previous location
we can take it out so all these are
thinking out so that means LC times L C
minus 1 and type LOL bobba so so this is
the equation based on naive assumption
so this is a probability of so of a
certain location okay based on
independent assumptions then we want to
figure out what is the enormous called
how unlikely that you are there so
basically that the lower the probability
is the higher the anonymous call is ok
if you're a very low poverty where you
guys not supposed to be there then score
is going to be high so we take the
negative log also preventing underflow
stuff like that so this will give us the
higher the number
the score is high now you saw in that
you've approached we can use the Markov
chain okay okay so you might know the
Markov assumption so basically that the
current stay is the property of current
state depends on the previous state okay
this is the first order Markov
assumption you can think about naive
Bayes is basically zero order assumption
Markov assumption so so now you can see
that the probably is based on the
previous location okay not just quickly
go back again the original equation
looks like there's that LC depends on a
lot of punch things a bunch of things so
now we just cut off all except one so LC
given else you might want else you are a
given else u minus 2 that's the Markov
assumption Oh first order Markov
assumption so just we get this and we
take the log as well and you get the
enormous scope the lower lip ability is
the higher the economic scores so rarely
it is simple now however the two
algorithms that we just discussed the
naive assumption of naive approach as
well as the Markov chain okay that the
score is the summation of all the
previous locations whether it was
observe or not whether it was normal
that you have been there before or not
so that means both observe and
underserved locations and I serve I like
like a serial frequency problem again
after an answer but we probably don't to
reduce false alarm so we're not going to
care and improve detection you're going
to look at unobserved locations only so
if you have been there we are not going
to contribute score to the anomaly
detection so we come up with two
techniques here let's test 0 is that
once that 0 is just like naive but only
the announced several occasions are
being counted that means zero frequency
type locations
that means you've never been that before
if you are suddenly somehow being there
we're going to get else go to it step
one is like Markov chain based on the
Markov first holder assumption but again
we are only looking at unknown sir
locations okay so so momentum
mathematically basically saying that if
it was not observed then we have to
school otherwise we don't have a score
we sum all of them together in the state
0 k's in step 1 case similarly that it
was not observed that we have a score
otherwise it's zero school and then
similar mathematics you get something
like this let's look at experiment's so
we presented for techniques to are
basically standard naive and markov
chain techniques we introduced to other
techniques which based on i'm also
locations only we have from a university
we have 500,000 hours of cell phone
usage data the data contains a cell ID
where the cell phone potentially is or
should be the time and basically time
location and identity of the phone so we
have these three pieces of information
we learn for experimental purposes we
don't know actually which one was doing
bad things or stealing phones or
anything so we assume all of them are
normal but you know the test our theory
that we train for each user that's a x
and treat all the other people behavior
as masqueraders so basically we learn a
model form an X and let's call the other
ones behavior based on the ex model so
basically presenting user wise to use
your eggs phone again we use our already
talked about area under curve so the
higher the number the better is in terms
of performance so we have investigated
different false alarm rate from one
person so effing equals one means the
entire ROC curve F
point 01 them is a very small corner or
actually very on the left side so we
want local foster language so okay so
here you can see that in almost in three
out of four cases that step one is doing
better than the other techniques here
these are the four techniques that we
just talked about this is preliminary
work so you still have a lot to do to
improve this another way okay is that
the Eco error rate when the false alarm
rate equals the detection array where
does it occur and we look at this that
one's at this time slower the better in
terms of our array so we do better in
this particular metric okay summary so
we presented basically two algorithms
the sub algorithms and stuff like that
Laurette is the first thing that we
talked about learning rules okay we have
the ruse America is different from the
typical classification and Association
rules and we introduced the randomized
algorithm back in 2003 we extend our
work phone network data to host data in
05 and this year we extended our work in
terms of the validation step waiting
versus pointing is Katie do seven and
secondly we discuss the spatial-temporal
a nomination problem and our preliminary
results using a cell phone they're set
currently we just obtained another data
set using laptop so hopefully you get
more reserves on different data sets
laptop cellphone and see how we can do
okay I think that is the end of a talk
and if you have any questions let me
know my website any questions I was
reminded that I need to repeat the
question yeah
yes you consider different time periods
and the oxidation on traffic volumes
like in some sides the chocolate or the
weekdays is quite different from the one
of your weekends so does it affect what
you believe to do to be the normal
traffic behavior with my team and does
it affect the false alarm rate okay good
question first of all the question was
whether they among of traffic affects
the model I seen that weekdays is higher
traffic than weekend so currently the
attributes involving the models are
based on the header and the pillow it
does not have any traffic volume
statistics in there or attributes all so
we can easily put that in if you want to
so but currently the attributes that we
look at a base on actually the content
of the traffic not the volume of traffic
so a lot of research does look at the
volume of traffic but you wanted I think
in our case we actually want to look at
the payload what kind of actually things
that you are transmitting so we don't
want to go into the volume kind of thing
we still child I've we wanted to
actually you attack so so kind of we
staying try to look at actually the
pillow as well that's the answer the
question kind of and you can we can
actually easily well I haven't tried yet
but you start using we can use both kind
of attributes right in addition to
content attributes getting at volumetric
use okay and in volume and also that we
can add like day of weeds our the day
and stuff like that so at dictionary so
right now we don't but a second part of
the question that you need to ask that I
didn't talk about but I dress apply some
of the paper is the non-stationary
effect of the traffic to speak okay so
over time the content could also change
so how do we address that and there's
some work on that based on yeah to deal
with the non-stationary nature of the
traffic yep
yes okay
yep right here so the question was that
the difference between naive up correct
let me go through that real quick so the
naive is basically considering all
locations right and multiplying them for
the stats one some of these are observed
some of these officer to realize serial
frequency internet website let's say LC
let's say LC was never observed before
the rain tree so this would be countered
but else you might as well let's say
that it was absurd ranging this is not
counted so we only look at events that
are that did not happen during training
instead of looking at both observe and
uh no sir so you know this example so
this is an officer this observe then we
don't count this is that zero basically
yes
for like very low public a false alarm
rate they are not performing differently
at all but if you look at here that's
kind of different a little bit but we
don't actually want to have really naive
and stats 0 or everyone it should be
high order so we have so again this is
very pulling me results so we still have
a lot of work to do on this so I'm just
presenting you basically a problem and
potential solutions and again very
preliminary results not published so so
correct correct but again I want you to
emphasize that we probably don't want to
go at too naive instead 0 type thing I'm
just putting this up for illustration
purposes we are going to focus on kind
of higher order stuff another potential
problem here is as the way we coloring
statistics might be off so this result
might not be totally reliable in some
sense because some of I think the
resolution that we calculate the time
instance it was like 10 seconds we might
need to go to resolution one second 10
second resolution is produce fewer
sample data points if you look at
resolution of one second we have more
data to to estimate the public's will be
better so here is based on 10 second
resolution so so I am not too good about
a little easy about that 10 second
resolution based on the data we have so
we need to increase the resolution as
well other questions yes
okay okay
correct yes that's another way stop
ruining you can agree into this back the
most specific ones we thought about it
as well it's a little bit more
complicated process we went for more
easier approach that's wait them is the
cup theory revision and seeing that you
make it more specific so definitely
there's another project we can take but
we just kind of chose to be a little bit
easier interest no that's something that
we consider yes by the way if you just
think about that we deal with a large
amount of data so if you do that that
means that first of all you have to keep
track of all those rules that you
consider before as a more specialized
somehow you have to sort them and then
you have to be evaluated them to bring
them back so to speak and put them back
as in which one you want to pick up so
you involve analyzing another rung of
training so to speak yeah it's harder
but also more time consuming the point
was in taking is more time consuming
when we're dealing with network data
you're talking about tons of traffic
just I mean you guys must know what tons
of data me so we need to have some sort
of fast might not be perfect algorithm
but I need to be fast and be good but
not perfect we want the one that would
be we don't want to be perfect and pus
no slow we want to be fast and good so
speak okay yes loads other than
analogous investigation for something
that
location if you say it's the cell all
right we're actually boredness yeah
that's a very good question so far we
have been looking at coin go discrete or
ordinal type attributes we haven't
looked at actual location like like GPS
latitude and longitude type things which
reveal values in that case that would be
totally different types of algorithms
probably because in probability we
usually deal with discrete events we
could deal with continuous events
however then you need to assume certain
distribution like pass on or normal
distribution so that does another thing
that we can do but right now we are
looking at only district discrete values
are all the novellas or we are not
looking at continuous values yet oh I
see this is matters yes actually we
scurry we don't have that kind of data
so we are not worried about it if we got
a data definitely we will look into how
distance there are a bunch of algorithms
that based on distance we can devise
algorithms that are a nominations and
based on distance yes we haven't
considered yet well one of the reason we
have in consider we don't have our hands
on data to investigate it if someone
that I can give me data then definitely
I would like to actually work on those
that's actually more interesting poem
then kind of cell data type discrete
locations actually the data we have the
cell location we don't even know that
what the respective locations of the
celebration we can approximate how far
the cell locations are from each other
so so we were thinking about losing
distance but we don't have the
information so so we can use it so if
there's data out there that i can use
that'd be great any other questions
alright thank you have a good lunch</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>