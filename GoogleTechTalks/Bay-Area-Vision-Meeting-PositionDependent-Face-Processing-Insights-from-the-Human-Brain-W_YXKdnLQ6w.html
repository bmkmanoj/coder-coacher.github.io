<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bay Area Vision Meeting: Position-Dependent Face Processing: Insights from the Human Brain | Coder Coacher - Coaching Coders</title><meta content="Bay Area Vision Meeting: Position-Dependent Face Processing: Insights from the Human Brain - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bay Area Vision Meeting: Position-Dependent Face Processing: Insights from the Human Brain</b></h2><h5 class="post__date">2011-04-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/W_YXKdnLQ6w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to introduce our next speaker
colony girls girls bector sorry she is
going to be our biological vision
speaker for today which i think is a
really wonderful tradition that we have
for the berry of vision meeting she's
been an assistant professor at stanford
university department of psychology and
neuroscience institute since 2001 her
labs research utilizes functional MRI
computational techniques and behavioral
methods to investigate visual
recognition and high level of visual
processes she's a recipient of the Sloan
research fellowship in neuroscience and
the Klingon Stein fellowship in
neuroscience I also discovered when I
was looking around her website that she
is also a brilliant painter in addition
to her amazing work so help me welcome
colony grill Specter and thank you
andrea for introducing me i'm going to
be the in neuroscience speaker for today
and i'm going to describe some work in
our lab looking at a position dependent
face processing in the human brain and
much as this work has been done together
with Rory steris who's this started when
he was a postdoc in my lab now he's at
Google and we're still collaborating on
this project and also together was is
Kevin whiner and Ryan Wendell at
Stanford University and so what I'm
going to describe today is some bits of
pieces about how face recognition might
happen in the human brain so we have an
object in the world for example a person
and like bounces of that person and hits
are I it then forms an image of that
person and inverted image on a retina
and then this information from the
retina gets relayed to the brain first
it gets processes as subcortical nucleus
called the lateral geniculate nucleus
and then transformed to the first
processing stage in the brain the first
visual area is called air
v1 and then through a sequence of
processing stages going from v1 to a
processing stream called the ventral
stream going to the temporal lobe till
there is this realization that maybe you
recognize the space for example this is
a face of sergey brin and what we're
trying to uncover as a group what are
the computations that happen in the
brain that enable efficient and robust a
face recognition and now much of these
computations are still very mysterious
to us but I'm going to address some
experiments that have dressed some basic
questions and specifically how our faces
represented in the human brain that
enable efficient a face recognition and
specifically a one of the issues in
terms of a face recognition is how does
a visual system deal with a variability
and their parents are the same object so
the appearance of the face will change
depending on your viewing position the
distance of the viewer from you or is
the object from you is the lighting and
so on yet recognition is pretty robust
as these changes and how invariant a
face recognition is accomplished is a
question of central interests in this
stock today I'm going to address only
one aspect of environment recognition
and this is it the question of position
invariants so the question is what
happened when the face moves in your
visual field it how are these
representation sensitive or insensitive
to the position of the face in the
visual field now a a position in the
visual field is actually very important
in the visual system because there is a
retinotopic map in your visual system so
for example and if you look at the world
there's a picture of a face this is
pursued picture of a Newton and this is
how the faith appears in the world and
there's a regular topic map or
topographic map of that picture
in your brain so this is a picture of
how that faith is represented in the
first cortical area or area v1 and the
human brain and there are a couple of
things to note first of all adjacent
points in the world map two adjacent
points in the brain and this is called
the topographic map or in this case a
retinotopic map because it mirrors
what's on your retina second of all this
image is a upside down it's inverted
sort of all there's what's called a
magnification stuff that's called close
to the central part of vision is
magnified of the brain compared to the
periphery and sort of all and each
hemisphere v1 is in right hemisphere and
left hemisphere codes a contralateral
visual field for example this is a right
side of v1 right visual right hemisphere
and that sees the left side of the world
and the left hemisphere irawan sees the
right side of the world so if we think
about representation is v1 we neurons
have very local receptive field each
neuron only sees a little bit of the
world and and it together this forms of
pictures that's very sensitive to the
position of the image of the visual
field for example if that picture moves
a little bit in the visual field that
image on the brain is going to move a
lot and this is why we sing that this
kind of representation is not very
effective to enable position invariant
for example face recognition and as I
mentioned in the beginning a recognition
just begins in v1 and actually what
happens in the brain is that there a
sequence of computation starting from v1
and ascending across a hierarchy so this
is a coronal slice of the brain and this
is an axial slice of the brain and what
I've colored on these slices are the
regions in the ventral processing
streams that are involved in object
recognition and there are sequence of
regions from v1 v2 v3 v4 and that all
have retinotopic maps and as we
census processing hierarchy we know that
receptive fields increase in size and in
the higher stages of this hierarchy
there are regions here that i coded them
in red and these regions are more higher
level in the hierarchy because they
don't respond to any kind of stimuli but
these regions in particular or what we
call face-selective regions in that they
respond more strongly to face is
compared to a variety of other objects
and we have three such face-selective
regions in the human brain one region is
here in the inferior occipital gyrus i
call it iog and then there are two
regions in the physique from Jerez one
in the posterior fusiform and one more
anterior in the mid frizzy form a Jerris
em and what we're trying to figure out
is what kind of processing happens
across as a processing stream that
enables face recognition and if we look
at the higher-order areas of these
processing streams for example if you
look at the activation from the mid busy
from dry versus activation here and
activation in this part of the brain is
coupled with how people perceive objects
so what we do in these kinds of
experiments we put a faces very briefly
at the threshold of recognition and
sometimes people can see them and
sometimes even though you've presented a
face people might not recognize it and
what you find is that the activation of
easy areas increases as people recognize
faces so when you show a face and the
person identify it you see the red curve
there's a strong activation in this
region when you present the faith and
the person can tell you it's a face but
cannot tell you whose face it is you get
some intermediate responses region and
when you present a faith but the person
can even tell you that there's a face in
the stimuli you get very low activation
in this region so what's kind of
interesting about these higher-order
areas that they're couple to your purse
up and you get higher activation as you
better recognize
as a face and your current recognize
who's fix it is this is typical of this
mid fizzy form region but you will see a
very similar profile of response in the
Imperial Capital G irises again you get
activation in this region is coupled
with how people perceive or successfully
recognize faces this is different from
low and intermediate-level regions of
this hierarchy for example if you look
at the signals from before which is an
intermediate area or back to v1 which is
a receiving area the first region that
processes visual stimuli in the brain
and you can see that there's actually
robust activation anytime you show a
visual stimulus but the level of
activation does not predict what
subjects perceive so we think that these
ridges are more kind of processing the
visual attribute of stimuli and these
higher areas might be doing something
specifically that's relevant for face
recognition so how does this relate to
make a computational model of faiths
processing we like to think about the
brain at some sort of hierarchical
processing in that this hierarchy is
built of several stages and you can
think about it as some sort of a neural
network was multiple layers so this is
the standard model the first model of
this kind of publishing 82 by fukushima
it's been recently updated by max res in
huber and ng eng and basically you have
an input visual stimulus and there's a
first layer let's say in your neural net
that processes local information and v1
is known to code sings like a edges in
the visual stimulus and and then you
impose some nonlinear and this network
is very simple network it set some
things up and when you and when you move
to the next HS it imposes some
non-linearity to create neurons that
respond to more complex stimuli like em
junctions and angles for example what's
happened in v2 and also what happens as
you are send this network you have
bigger receptive fields as acceptor
Filton v2 are bigger than the one in v1
and then this kind of model have a
higher stage for example this is a stage
of this network that's supposed to
illustrate the inferior temporal cortex
where there are neurons known to respond
to faces and these models suggest that
there might be beauty near on stream to
hold views of faces but one of the
assumptions of these networks is that
want to get to the infratemporal cortex
these neurons don't care about the
position of the face in the visual field
because it's supposed to respond to a
whole face so I'm going to illustrate
some results in our labs that we
actually wanted to measure how position
sensitive these neurons are in the
higher stages of this network and second
of all since we found three fates
elective regions we wanted to
characterize the properties of neurons
in these different regions in terms of
their position sensitivity and then the
end I'm going to go back to the smaller
and see our neuroscience measurements
might inform and update this kind of
hierarchical a model so the goal of our
experiments was to quantify the degree
of position sensitivity in three phases
elective regions in the human temporal
lobe and as a message that we want to
use is to use as a continuous
measurement of position sensitivity
across the entire visual field and to
provide a quantitative model to describe
which part of the visual field it
activates each ephemera voxels in the
subjects brain so what we do in these
experiments we put people in an fMRI
scanner and show them a visual stimuli
as they're going to see movies of a
stimuli in our experiment and as they
look at this timilai will record brain
activation across this whole sequence of
a brain areas so in order to sweep the
visual field we have subjects fixate so
we know where they're looking at and
then very similar to how Hubel and
Wiesel and measured receptive fields in
a cat's we just sweep the visual field
with something that looks like this it's
an aperture that continuously sweeps the
receptive field again remember the
subject is fixating and we're
stimulating different parts of the
visual field and this bar basically
indicates which part of the visual field
is stimulated only except instead of
showing a stripe of light what the bar
revealed our faces so basically we have
a field of faces and this field of faces
gets changed in six times in every two
seconds and we have this bar sweeping
across the visual fields art of every
instance it's time it revealed parts of
this phase field as illustrated with in
this example a image and the bar sweeps
as we see from right to left from top to
bottom and so so that's our basic
stimulus and and then what we want to do
is for each fMRI box will determine
which part of the visual field activates
is this brain so basically we have a
stimulus and we can record an fMRI
signal from a voxel and what we want to
model is the part of the visual field
that would predict this response and our
model of a receptive field is described
by a two dimensional Gaussian and this
Gaussian has three parameters x and y
which describes a center of this
Gaussian and Sigma which describe is the
width of the receptive field or the size
of the receptive field and we're trying
to find the best fit Gaussian that would
predict our fMRI signal in every voxel a
of the brain so this is an example time
course from a real voxel in the brain so
then this is the time this is a signal
that we measure from the brain and the
errors on the top indicate and the
movement of the bar as it sweeps of
visual field the shaded area indicate
when we have a stimmel sets on once in a
while we turn the stimulus off just so
that's a response settle back to
baseline and first thing that I want you
to note that during the shaded periods
the response goes on and off and the
reason that a response goes on on enough
that some parts of the visual field
stimulate this voxel and other
the visual field don't stimulate this
voxels so basically we're getting
position sensitive modulation in this
region that's supposed to be high level
region so what we do we research and we
find the best a Gaussian that can
predict the time course in this case
there what we estimated they receptive
field that's slightly off the center and
it's maybe about 6 degrees and maybe
prefer slightly the lower left a visual
field once we have this and a PRF
population receptive field we can
predict the fMRI signal from the voxels
so this is a black time course shows a
predicted response from this voxel and
you can see that there's a really nice
close coupling between the predicted
response and the actual response shown
here in the dotted line from this voxel
as since we're scientists would like to
repeat our experiments and validate our
results so basically reruns experiment
again and and now we want to see if the
receptive fields still predict the
second set of data and you could be
honestly see a very good coupling
between the predicted response and the
recorded response and basically these
receptive fields explain a significant
amount of variance in our time course so
this is an example voxels we do this on
every voxel in each of our subject and
in each of the face elective regions
that we record and then we get pictures
like this so basically you remember we
have the street by selective regions in
fear occipital gyrus the posterior fuzzy
form and the mid flizzy form and what
you see here is that as a part of the
visual field that is activated by each
voxel each dot here represent a voxel in
the brain and the point here represents
where the center of the receptive field
is in visual space so couple points to
note first of all as in lower level
areas their centers of receptive fields
are contralateral what do I mean that a
voxels in the right hemisphere will
respond to stimuli in the left visual
field and voxels in the left hemisphere
we'll respond to stimuli in the right
visual field so this contralateral
nature of response is still present even
in high-level visual eras and you can
see this a laterality effects in each of
the three visual and face selective
region a second things that's notable is
that the cover of verge of the visual
field differs between this region and
these two regions so in the invisible
jars you get centers both in the center
of the visual field and more into
periphery but as you go into the higher
order stages you can see that most of
the receptive fields are actually close
to the center part of the visual field
does they have more of a central
preference another way to look at these
data and is to take into account not
only the Centers of the receptive fields
but also the size of the receptive field
so basically we tell the visual field
according to the receptive these centers
and their size and we get a coverage map
of each area basically saying which part
of the visual field in each area
basically a process and and again this
illustrates this differential and
coverage of the visual field across
these three phase selected four areas
where the inferior occipital gyrus
covers the entire visual field and this
region is the fuzzy form have a coverage
that's more a central in nature and this
is important to us because now if we
want to model is these face-selective
regions we have more precise model about
which part of the visual field is and
the receptive field sizes in these areas
and furthermore because we're getting
different coverage maps across these
three regions it suggests to us that
these are three distinct stages that are
involved in processing a faces so so far
we stimulated the visual filled with
spaces and the question is do you really
need a face stimulus to drive responses
in these regions so the same way that we
have the bar sweep across a few
the faces we created another stem rules
that we call the faith scramble stimulus
what we do is we take is the Fourier
transform of this image and we scrambled
a phase spectrum and keep the same
amplitude spectrum and created an image
like this and this image and preserved
all the first order statistics of this
picture as well as the amplitude
spectrum but of course you cannot see
any face in these regions and this is
just to examine well whether just a
low-level aspects of the faces are
driving these regions or maybe you
really have to have something that
resembles the face to drive responses in
these regions and again we sweeps it as
a visual field with a bar aperture and
the bar sweepers identical across these
two stimuli so all we're changing is a
content inside the bars as a subject
sees it turns out that it really matters
through these high level areas what kind
of stimulus you show inside the bar if
when you show a phase scramble similis
you get very low signal in these regions
and you can no longer a kind of see
clear position modulation or fit a
population receptive field that explains
to you which part of the visual field is
stimulated by these stimuli so basically
in order to drive these regions you
really need to have a faith in the in
the picture and not only something that
has the first order statistics of a face
so what do we have so far we find that a
contrary to the prevailing model the
higher order regions of this processing
stream face-selective regions are
modulated by the position of faces in
the visual field but in order to see
these position modulations you have to
use an effective stimulus for example if
you're looking at face selective regions
you need to have a stimulus that has a
face in it and furthermore we find
evidence for three distinct stages of
face processing in the human temporal
lobe which have differential
differential in a coverage of the visual
field where we find that responses
become
more centrally biased and less
lateralized as your Center processing
stream from the inferior occipital gyrus
to the fuzzy form aged irish Tuesday
pistear physical and mid frizzy form air
regions and however we were intrigued by
the fact that we got different results
when we change the content of the the
bar and we ask whether the position
modulation that we see in these rare
areas might depend on the kind of
stimuli that we used to map the visual
field so so far I've shown your result
was what we've called the scale faces
this is a center stimuli we've created
two more kinds of faith stimuli one a
key I think it's hard to see but
basically is if we created a field of
faces with small faces that are
uniformly spaced and are uniformly sides
and they cover the visual field this was
one extreme of our stimulus and other
extremely use a single face and in each
case a bar actually sweeps through parts
of the single face and subject will see
in each instance of time something that
looks like this so see only part of the
face as it bar sweeps across the visual
fields during the experiments I'll see
more than one face more than one phase
field and but each time only a part of
the face what's important is that the
the bar sweep across the stimuli is
identical across all these conditions so
if these neurons in these high level
areas only care about where faces in the
visual field they care only about
position we should get the same response
across all of these stimuli because the
visual field stimulated is identical
across all these movies however if then
if the neurons care both about what kind
of fates you're showing us and where it
appears in the visual face it filled you
might actually get differing responses a
different position modulation across
these stimulus in that case
would indicate as a receptive field is
something interactive because it depends
both about position and the kind of
stimulus so we first measured v1 if you
look at v1 responses each row shows you
one of the stimuli and the dotted line
shows the response from the brain and
the black line shows the fitted stimulus
and basically this is a receptive field
that we measure in one a voxel and v1
behave like v1 should basically it
doesn't really care what kind of
stimulus I'm showing it only cares where
in the visual field I'm showing
something and consequently we have the
same responses across all three movies
in v1 and we estimate very similar
receptive fields for these different
movies so basically responses of the v1
voxel only depend on where i'm showing
steel lines or visual field if i look at
the frizzy form something interesting
actually have happens so this is again
an example of one voxel and the dotted
line shows the time course of activation
from the brain for each of the different
stimuli and what you should note that
when i showed when we show the subject
of small faces we get very punctate and
and sharp modulation but as we increase
the size of the face we get more
prolonged modulation what that means
that is more prolonged modulation that
it gets activated by faith in more
positions across the visual field and
consequently because we have different
grain responses to different stimuli we
get different estimates of population
receptive fields from the same voxel
because of the different visual stimulus
so when we showed small faces we get a
small receptive field that's as close to
the center of the visual field when we
show the small the scales faces we found
that the receptive field becomes bigger
and actually shifted more peripheral e
and when we showed the large face we
suddenly get a really big receptive
field that this is even shifted more
perfil e so we get the surprising
finding is that in a region that was
supposedly position invariant it's
actually not pose
Varian in and instead it calls both
we're in the visual field I present the
face and what kind of face it is so when
we show bigger faces it it has larger
receptive fields that are more perfectly
shifted compared to when I show small
faces so this is just one example voxels
but we can do this analysis and across
the population of voxels and and this is
basically as a result i'm going to show
you here and here i'm showing all you
all voxels from the mid fuzzy form
region again each point is a voxel here
i'm showing use estimated prf sites
population receptive field size for the
large faces compared to the population
receptive field size we measured from
the small faces and this shows identity
line and what's very clear is that for
large faces we get larger estimated
receptive field compared to the small
faces and furthermore and it's not only
that they're larger but there are also
shifted more peripheral e so you can see
that the eccentricity like where the
center of the receptive field is is more
peripheral compared for the large faces
compared to the small faces so what I've
Illustrated first a single voxel is
generalizable across a population of
voxels in the brain so we summarize the
results across a multiple regions and
across all the pairwise comparison so we
have the full field faces minus the
scale faces we have the scale minus the
unscaled then the full field minus the
skilled and there are several things to
note first of all oh everything looks
like a same color so the tall bars here
are as a face selective otherwise and
you can see that for all the face
selective are wise when you have larger
faces you have bigger population
receptive fields so the receptive field
size scales with the size of the face it
this bar here that's supposed to be
black this is a response from v1 and
basically this shows you use after
no shift in v1 so early visual koriians
don't have this interactive of faith
they just tell you where the stimulus is
in the visual field doesn't care about
the content but these higher level
regions shows scaling of the receptive
field size with the size of stimulus and
further all more the scaling seems to be
bigger and the more anterior region in
the mid fusiform regions compared to
something that might be more
intermediate in this processing stage
like the inferior occipital gyrus so
this is the effect of a size bigger
faces produce larger a a receptive field
sizes if you look at the scent Lok a
shin of the center of the receptive
field size in visual space it also gets
shifted again we have all these pairwise
comparisons and again if you look at the
tall bar Z there's three face elective
regions you get a shift of the center of
the receptive field it becomes more
peripheral as you show a larger faces
and again a v1 which is this bar which
is supposed to be black and shows no
shift in the center of the receptive
field m so this is very striking with
shukla the chosen a modulation of the
receptive field depending on the
stimulus in these face-selective regions
so what does this tell us about a face
processing in these higher-order areas
so let's go back to our coverage maps
that I showed you in the beginning so
each column reflects one of our face
processing regions going ascending from
the infinitive occipital gyrus to the
fizzy form and each row denotes a kind
of stimuli that I shows a subject so
across all of the stimuli we get
different coverage of the visual field
from then fear XML jars to the fuzzy
form where the inferior occipital gyrus
behaves more like a visual field
representation it covers an entire
visual field and these regions is a
fuzzy from tend to have a more central
representation of the visual
field but if we consider this to be the
higher stage in our processing stream we
actually see that the interactive effect
of position and stimulus is actually
larger here as compared to this
intermediate staged so it is actually
the highest level of this processing
seems that shows the small more the
larger sensitivity to both the stimulus
content and the position in the visual
field which is actually really contrary
to what you'd predict from a position
invariant a model m so let's tie this
back to our hierarchical model of a face
processing the rain so I still think
that a hierarchical model is a good
model to consider when we sing about
face processing in the brain but given
recent results from our lab as well as
other labs I really think that this
model needs to be updated for several
reasons once the initial model had only
three stages v1 v2 2 degree for an IT
and I think this is just wrong so first
of all it be nice to actually include
the right number of processing stages
when you put a model because maybe this
is important so it would be nice to have
v1 v2 v3 and v4 in our model and not
just two stages of low-level processing
second of all and a lot of data suggests
that there's more than one and
processing stage of face processing in
the temporal lobe and at least in my lab
we find three processing stages and
these stages have different properties
so we would like to include in the model
and relevant data from our brains
actually looking at these specific
properties of these higher-order areas
you might note that I put a question
mark in viessmann before because we know
a lot from the monkey work is also human
work what happens in v1 also meet you
and what happens in these intermediate
visual areas is actually much less known
surprisingly than these higher-order
area so we still need a lot of research
for your not what is represented in
these intermediate visual areas and
other aspects of the model should also
be updated for example we can measure
receptive field size population
receptive field size in all of these
intermediate errors and this should
could be used to constrain the actual
receptive field size that we put in our
computational model and but our model
also has to account for these recent
findings that we find an interaction
between a position and kind of stimulus
and this suggests to me a different view
of the representation in these higher
level areas specifically it doesn't
suggest a position invariant it actually
suggested position sensitive or
presentation but it might suggest that
there might be multiple neural
population each coding for different
sizes of faces so maybe there are some
neurons that like small faces and some
neurons that look like large phrases it
covers a visual field and when we show
small faces we're activating one subset
of neurons and when restoring large
faces we're activating a different
subset of neuron so there might be
multiple maps in this piece of brain and
basically is there some some place in
the brain actually might say oh there's
a face here and and that might be either
from top down or from some kind of fast
mechanism that goes from the superior
colliculus and and then and it either
activates one of these populations or
maybe there is some sort of kind of
adaptive filter that can tell you first
were the faces in the visual field and
then it adapts the size of your
receptive field to optimize the
recognition in this part of space where
you sing the faces so if you think the
face is big you're going to adapt a
receptive field to be bigger and if you
think that the face is far away and you
might use a small receptive field again
to optimize the computations to the
relevant part of the visual field so
that's all for today and and thank you
for your attention
hello we have time for some questions
and Holly if you wouldn't mind repeating
the questions for the video that because
okay fine yes okay so the question is
how many neurons are in a voxel so it's
quite a lot and it depends on the size
of the voxel so one cubic millimeter of
the brain has about 50,000 voxel a
50,000 neurons m and the voxels that
we're measuring here are anyway anywhere
between one and a half cubic millimeters
23 cubic millimeters so you can multiply
this and get an a range of anything
between a hundred thousand to a million
neurons of course all these neurons will
have different properties and this is
why we call this a population receptive
field rather than the receptive field
because we're measuring the aggregate
receptive field across the population
and neurons things are nice for us in
terms of mapping with fMRI in that there
is this routine atopic organization in
the brain that means that clusters of
neurons that are adjacent in the brain
will actually see adjacent parts of the
visual field so even though I cannot
tell you what a single neuron sees quote
unquote I the population receptive field
does represent the part of the visual
field that's coded by this part of the
brain across this population of neurons
somebody else said yes
this is a very good question so
basically the question was that in the
full when we have the full field so far
sweeps across different facial features
since the face is always in the same
place maybe we're getting this position
sensitivity because some neurons like
the eyes and other in neurons like the
mouse and I'm translating this feature
all sensitivity to position sensitivity
so um we think that feature sensitivity
can't explain everything and the main
reason is mm that there is a coupling
between and even those are there is a
shift in the preference of a particular
voxel it voxels that in the beginning
were more fovea was the small faces are
also more foveal for it's a big faces
and so there's actually a consistent
shift across voxels so when we shows a
small stimuli that means that that voxel
had to respond to a bunch of features in
the face in the face so we think it has
to do at least to some extent to
position it still could be that some
neurons would prefer some parts of the
face versus others are some results from
moody's ahora slab and as well as it
suggests that some neurons might like
the top half of the face and other
neurons might like the bottom half of
the face and they're also there in the
fusiform gyrus so it might be a
combination of both of these features
yes yes okay so the question is what
happens if I'd actually shows 3d stimuli
instead of Judas tonight we've done a
lot of experiments looking at
environment recognition across the human
temporal lobe and it looks like what
really affects the responses in these
regions of two things one is a shape of
the stimulus and to the stimulus is
recognized by the subjects so if I show
moving faces
or sweetie faces or line drawings of
faces or black-and-white photographs of
faces or colors faces or movies of
colored faces I'm getting get pretty
similar responses in these areas yes so
um the question was which shows that the
position modulation our face selectively
I've compared two responses to faces
versus phrase scrambled faces but your
point is well-taken this is unfamiliar
stimulus it doesn't have a shape what if
I use other stimuli so just to remind
you as a way that we defined is these
regions to begin with if I find it m it
was something like that so we compare
responses to faces compared to other
objects so this part of the brain I
haven't showed you but its response
about TT faces compared to other objects
and we actually have done this
experiment with houses as well and
houses do drive the responses in these
rare areas but not as much as spacious
and part of our research agenda is
actually exactly asking the question
that you're asking me and that is do I
get also changes the receptive field
size if I change the underlying category
so I need something that at least drives
this region is the other really good
stimulus for these regions it's not as
good as faces it's something like a body
parts like limbs and animals I know that
there's going to drive these errors as
well and then the question is if I'm
going to get the same modulation is the
subject of ongoing research yes and this
particular experiment we've run only
with frontal view of the faces in other
experiments that we've done in the lab
we looked at view and variants and and
it is the case that across all of these
regions
is the representation is very sensitive
to the view of the face and frontal
faces tend to produce slightly higher
responses and profile faces and backs of
heads are not really good for driving
responses in this region yes
so it's a very good point so basically
it the reason that you look at people to
recognize them is that you have the best
visual acuity at the central part of the
vision and this starts already at your
retina and so it's true that you might
notice a face and somewhere in the
periphery but if you really want to make
sure that you know that person you
probably are going to point your eyes
and typical people will put their center
of gate somewhere here which is pretty
much the center of the face and still it
is a case that sometimes that person
will be far away from you and sometimes
that person would be close to you and
sometimes a situation like this there
are actually a lot of faces all around
you that actually might be crowding your
face and actually might confuse me so I
actually might need to sharpen my
receptive field to see only your face
and ignore all the other faces that are
really nearby right now hitting my
retina and I want to recognize that it
to you so there are a lot of situation
like in when I'm looking at the crowd
that'll be many faces and that might be
an ecological reason why you might want
to scale your receptive field for better
face recognition right now it's still
hand waving and we really need to do
experiments to look at this but I think
there might be a collage achill reason
for that yes
okay and so I haven't told you is this
is an experiment that have done already
70 years ago and so basically what you
do is that you put so the person he
asked me a little bit more to describe
the details of his experiment how do I
know if a person recognize the faces
they don't so we have all kinds of like
a physics tricks in our back in our bag
and what we do is we put two stimuli
very very briefly and then we followed
by this noise patter like a face
scrambled image and this mask the face
so basically if you present a stimuli
let's say 430 millisecond or 50
milliseconds sometimes people can see
them and sometimes a key not this is
what I mean stressful of recognition so
what we do we puts people in the scanner
and shows them a lot of faces very very
briefly and for example in this
experiment we had in maybe they had to
recognize Harrison Ford so we had a
bunch of Harrison Ford faces we had a
bunch of other male actor faces and then
we had some scrambled faces and then
each stimulus flashes really briefly and
the subject has to say like keypad in
the scanner and they press one button
and say oh this is harrison ford they
press another button say oh i can tell
it to face I don't know it here who it
is and the third button if they say it's
not a face so what's then we go back and
sort our data not accordingly to what we
showed them but accordingly to what they
responded so we have all the Harrison
Ford faces but sometimes they can see
them and sometimes they cannot and this
is how we can get these signals that
correlated to subjects percent yes
you
okay so so the question is it falling
I'm showing use these face selective
regions and these regions are indeed
highly specialized and the question is
if the specialization in nate's or is it
learn other experiments that we're
actually on going in my lab that I
haven't described today specifically
address this question because I think
it's really important question
especially like for machine learning
like did we learn about faces over we're
born with spaces and what we doing this
experiment we scan children is so we
scanned school-aged children seven to
eleven we scan adolescence 11 12 to 16
and we scan adults and what we measure
is a degree of face selectivity actually
there are other category selective
reading the brains or face selected
regions and body part selective regions
and object selected regions and we
actually see if the specialization
develops over time and one of the
interesting finding from our lab and is
that these face-selective regions tend
to develop the latest so you'll find for
selective region and children but they
tend to be smaller and less selective
than an adult and what's really
surprising that they still develop all
the way into puddle essence it's
intriguing because if you look at
performance kids don't remember faces as
well as adults so they might remember a
small group of faces that they're highly
familiarize like there's you know their
kids in their school or the family but
generally if you just do shows a novel
faith in and ask them after five minutes
to tell you if they've seen it or not
they don't do as well as a belt so we
think that these errors there might be
some innate specialization but we think
that a lot of the selectivity is coming
from experience so so think one more
question
so you're asking what is a relation of
the second experiment to recognition
performance in these experiments we
didn't really measure recognition
performance in the scanner because we
really wanted to make sure that subjects
fixate so we can map the visual field
properly so they have a color fixation
test we did measure recognition
performance outside the scanner we
didn't do like a person recognition but
a gender just safe it's a male female
and subjects can do it even in the
periphery it but maybe if we have asked
is this Harrison Ford in the periphery
they couldn't do it but we haven't
measured it so basically needs to be
found out let's thanks thank speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>