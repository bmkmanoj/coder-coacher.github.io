<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2015: Fake Backends with RpcReplay | Coder Coacher - Coaching Coders</title><meta content="GTAC 2015: Fake Backends with RpcReplay - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2015: Fake Backends with RpcReplay</b></h2><h5 class="post__date">2015-11-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wJPfsyRRmyw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">next up we have matt garret from google
who's going to be talking about RPC
replay hey guys my name is Matt I work
at Google and i'm on team called google
frameworks we work we build platforms
and frameworks to try to make developers
more effective and what I'm going to
talk about today is a technique that my
team has developed our goal is to run
our tests and these are like the
heaviest class of tests we call them
functional tests our goal is to run them
against robust and fast implementations
and next slide the big one I found it
thanks guys okay so let's start with the
test authors dilemma so I am a developer
I'm building a new service it's going to
be called server under test and I really
want it to execute well so i'm going to
write a test for it when i do that what
happens is my test brings the server up
exercises some logic and brings the
server down right now that test is going
to execute very very fast and the reason
is because i haven't integrated with any
backends so let's do that let's add a
couple back ends now when my test needs
to run it needs to start up my server
and also to back-end servers that I'm
not really exercising code in my test is
a little bit longer at this point and
even if you start up servers in parallel
often times your test is CPU bound so
you're not going to get like the full
parallel parallelization effect mostly
because like they're competing for the
same CPU resources when you add more
backends the problem gets worse your
test is going to get longer and your
test is going to start to flake out to
common response is to lower your
expectations and say well I only expect
my test to be fast ish but that affects
your development practices because now
you're spending your time waiting for
your test
complete and nobody wants to do that and
as you add more and more backends it's
just going to get worse so the next
response is for people to fake mock and
stub out your backends it actually does
make your tests faster and that's good
news the problem is that now you're
going to maintain these implementations
until the end of time which really
stinks because for you to push a working
binary you now need to make sure your
code is in order but also that your mocs
fakes and stubs for dependent servers
that you don't actually care about are
also working so now you've increased the
workload on your developers and that's
not very good too this also introduces a
class of bugs where your server
implementations of other team servers is
different from what's running in
production so you can pass all your
tests and then once you go to prod it
turns out there was a bug in your fake
mock or stub and you've caused a
production outage that's a really
negative thing so the test authors
dilemma is this if only we could have
fast and robust implementations to test
against my team thinks that we have
found a middle ground and solved this
problem it requires one caveat that we
will talk about but basically what we're
going to do is split the test up into
two modes so the first mode looks
exactly like what we saw earlier we have
the test we have our system under test
we have a bunch of backends and when we
run the test in this mode we just write
down all the RPC interactions that
happened with the backends what this
lets us do is it lets us run a lighter
version of the test and instead of
starting up those expensive backends
we're going to start a very dumb server
all it does is read from these logged
our pcs and replay the responses that
the original servers would have
responded with the code you are trying
to exercise is still live and the
assertions in your test process are
still working against responses from
your server under test
so here's the caveat when you change
your requests to the server under test
and that server under test then changes
its request to the backends you do need
to update the RPC logs so what ends up
happening is we ask developers to update
their RPC logs when they're making
changes that affect that but all the
rest of the time they are just
submitting through this test and this
test is a lot faster because you're not
starting 10 1832 backends the RPC logs
themselves are actually checked into the
repository and there are human readable
this is important and we'll talk about
this a little bit later so the first
question that people have when they hear
about this is that sounds really
dangerous what happens when a back-end
changes its implementation and this is
this is a real issue so let's say we
have this test scenario and our back-end
their team works really hard and they
push out version 2 and it's it's awesome
the problem is that now our tests are
exercised against the logic that was in
version one because that logic is
recorded it's stored in an RPC log and
now we're exercising that logic during
our tests this really scares people um
so here's the course statement here if a
back-end change actually breaks your
request path its push will cause a
production outage so if the backend
changes to back-end version 2 um and
they push to production tomorrow and
that change broke something in your
request path no tests that you write for
your server is going to stop them from
doing that they can push to production
immediately and it will break every
server that you have that's speaking the
old protocol in production at this point
you have to rely on other teams because
it is the backends responsibility to
to completely test their API and also to
provide a migration opportunity before
they just say okay like we push this new
version all their clients need to use it
immediately and they're broken until
they do so when we actually make this
change and we upgrade the backend to
version two the change falls into one or
two possibilities the first is that they
cause the production outage and this is
no longer a problem that your test
should catch this is a problem that
their tests should catch the second
problem or the second kind of change
they can make is what we call it benign
change and this would happen if you they
like change the format of the data but
they're still sending the same data back
or if they add some new metadata and
it's not gonna break your request and
response path but you would want to
update that in logs so essentially what
we do is we run a continuous job in the
background that's consistently running
these tests and if the log generated at
the end of it differs from what we have
checked in we check it in and that is
equipped that's like equivalent to
refreshing the version fake that you're
using to test against this keeps the
backends you're testing against updated
which is a good thing the next question
people have is that aren't requests
different on each execution and the
answer is unfortunately yes servers will
do different things when they receive
the same input and here's a essentially
any requests that's generated and is
non-deterministic as we call it will
fall into that case we see that as a bad
thing we really want to remove
non-deterministic behavior when possible
mostly because deterministic behavior is
easier to understand reproduce and debug
it also happens that it makes RPC replay
nicer but these are real benefits that
we see and it's really hard to reproduce
an error state in your server if your
server does a different thing and sends
different requests given the same input
so we really push the teams that our
client teams to actually try to make
their servers as deterministic as
possible this is not always possible of
course there are things like time stamps
IDs and IP addresses that are not
deterministic and they probably
shouldn't be either if your time stamp
is deterministic all the time you have a
problem in your clock so essentially
what we do is before we write the RPC
logs down we just run a processor over
them it does some metadata stuff and we
kind of like strip out nondeterministic
fields and then before we replay them if
those nondeterministic fields are in the
response we can add them back in so a
good example would be like a time stamp
if the response needed a time stamp we
could put a time stamp back in after
we're replaying it back to the system
under test here's a big win for us as a
bonus you're protected against broken
dependencies so a very common case once
you have but 13 backends which is more
and more common in the micro service
world is that one of your backends is
broken and all of a sudden because you
have a dependency on that your tests no
longer compiles and none of your
developers can work that's really it's a
bad thing but you're not actually
submitting code against this but you're
submitting against is this and that RPC
log is effectively the last green that
all your backends had at the same
version number so you actually don't see
those airs that will cause your tests to
blow up so here's an example test
history and I've stripped out most the
information but the the red is failure
the purple is where both passes and
fails at the same change and the black
is a compile error and you can see with
our PC replay we actually don't see any
of those flakes and what's more
important we don't see the compile break
in the back end and that's a huge win
because when you're iterating on your
team you don't want to be stopped
because a back end
checked in a bad CL and needs to roll it
back as a bonus the tests are faster too
and that speeds up your iteration time
and enables you to submit more code and
write more features that's it questions
how do you handle stateful our pcs where
the same request can have different
responses we actually do that
chronologically so the logic here is
that if you send the same request and
this actually does happen so an example
would be if you write some state like
you read some state and it's not there
you write the state and then you read
the state again and it is there when we
write down the logs we just do it
chronologically so the first request
will respond with there's nothing there
because that's what happened first
during the test and then the second
request will respond with oh there's
something there just because that's what
happened during the tests in record mode
why not mock the required state so this
is actually interesting question mocks
are applicable they can be used but you
run into a situation where if all of
your test needs to mock all of the
behavior of every single one of its
backends that can be very expensive and
especially when you rely on back ends
that are like numbered in the tens if
you have to write a test and you have to
Mach ten different back-end states for
that test it's an expensive test to
write an ideally you would want to use
the authentic version of the back end
which is the one that's going to be used
in production you don't want to have to
mock it yourself you want to
automatically generate a fake which is
what RPC replay does
what false positives can you get so I'm
I'm actually not sure about the term
false positive you can get a passing
test that shouldn't be passing but
that's mostly dependent on the
assertions in your test class which are
unrelated to our PC replay at all so
there's still test assertions that are
going on in your test and you shouldn't
not write those assertions and it is
possible to write a test that is faulty
and passing maybe what this is asking
about is if it's possible to have
something that's broken in record mode
and passing in replay mode and this is
actually true and this is something that
we want to happen so things can break in
record mode because your back ends are
broken and you don't want to block your
own team submission on that you don't
want to stop writing code because
someone else checked in a bad dependency
you do want to keep writing code so you
can get a false positive in that sense
but it's intended just to follow up on
that what about when the backends bumped
their version and you have not done in
new records you're using the old logs
and now the thing is the system under
test is using old logs and thus not
seeing maybe an incompatibility with a
new back-end bump this can happen and
this is something that we actually do
alert when we run the continuous job we
do alerting on that too but in general
you cannot have a back-end that does
that because if a back-end were to check
in code that has that incompatibility
and then they push they're not waiting
on your test to pass before they push
they're just going to push and so if
they push that change that in kampala t
incompatibility is then live in prod and
backends will do anything they can to
avoid that they should be writing their
own tests what logs are recorded are
basically request-response protos that
are is it basically request-response
proto
is that are recorded in comparators or
anything else being logged it is
request-response protos and then we also
put in some metadata just for our PC
replay but as far as the recorded logs
yeah it's just request and response
protos it's also worth noting that these
are not this is not like production
traffic so we check it into the code
base but it's logs that are generated
from the test class itself thank you
very much thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>