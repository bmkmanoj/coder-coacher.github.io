<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Four Solutions to a Trivial Problem - Guy Steele Jr. | Coder Coacher - Coaching Coders</title><meta content="Four Solutions to a Trivial Problem - Guy Steele Jr. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Four Solutions to a Trivial Problem - Guy Steele Jr.</b></h2><h5 class="post__date">2016-01-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ftcIcn8AmSY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today we have with us guy Steele a name
that may be familiar to many of you he's
had a hand in a number of specifications
for programming languages including
common lisp and java which i think
there's some people in this office who
might use those and he's asked me to
keep the introduction short so here he
is okay thank you Matt it's good to be
here I see a number familiar faces in
the room including somebody didn't
expect to see here today so thanks for
coming I hope you enjoy this talk it's
about parallel programming and I feel
like in at Google I may be kind of
preaching to the choir about parallel
programming but I've got my own
perspective on it this is a talk about
given other places so if nothing else
you can hear the message that I'm I'm
giving in other places as well most
recently just at a Harvard last month
the main points this talk is that I
think that the best way to write
parallel applications is as much as
possible not to have to think about the
parallelism parallelism is something
that comes hard to our brains for some
reason and there is a need for
separation of concerns and I think the
issue is not so much thinking about
parallelism as such as just thinking
about independence of computation and
the problem is that our programming
languages today don't help us to express
or to indicate when computations are
relatively independent of each other
there's a specific Bugaboo of mine
having to do with the accumulation
paradigm and accumulation tends to be
inherently sequential and so in a
nutshell my slogan my oversimplified
slogan is accumulators are bad divide
and conquer is good I think there's an
old message I'm not the first one to
discover this but I think we need to
take it seriously
also algebraic properties are important
I've got a background in applied algebra
and I think that is informed my
programming and I think that mean making
programmers aware of algebraic
properties their code and communicating
some of those properties to the compiler
maybe you work foil so to start let's
add up a bunch of numbers suppose I have
an array X and it's a length 1 million
and I want to add up all the numbers in
X so here's some code can it be
paralyzed and the question is who cares
there's a bug I forgot to initialize
some
this is a very common bug and it bothers
me all the time okay with this code can
it be paralyzed well the problem is that
this code is already bad it has an
inherently sequential flavor to it we've
used this do loop whose semantics is
inherently sequential and clever
compilers have to undo this in fact fran
allen won a turing award for devoting
her life to writing compilers that can
undo code like this and turn into code
that a parallel computer can really run
so what does the mathematician say to
add up a bunch of things well then we've
got this big Sigma notation and with the
explicitly parameterize version it looks
a lot like that do loop as I goes from
one of a million at select X sub I and
add them up or maybe we just write a big
Sigma in front of it and leave it to the
mathematician to figure out how to add
them all up compare this to be some
intrinsic and Fortran 90 this says what
to do I want to add a bunch of things
but doesn't tell you how there's no
commitment yet as to strategy and what
I'm all about is not committing to early
to the strategy of doing something if
you can just say what you want done okay
let's go back to this do loop it's
written in a language that is remarkably
similar to Fortran I call it Fortran and
we'll be seeing some other programming
languages along the way so this is
Fortran code and what this code
describes is a computation tree for the
addition now there are some other
computations well having to do with the
mechanics of running that do loop but
essentially what you're doing is
describing how to this variable sum
you're going to add in individual X sub
i's and so we get this this copy binary
computation tree looks like this that
describes just the summations and their
arguments and you see there's got this
long list linear spine that's what makes
it inherently sequential you've got no
choice you can't do any of the upper
additions until you've done the one at
the bottom so you've got to start with a
zero add X sub one then add X sub two
and so forth
okay now that one could try to paralyze
it by saying well let's make two do be
parallel so we can do that will mark the
do parallel wonderful everything is
sweetness and light well no the problem
is we've got race conditions now it
might be that if we execute all of you
bought the body instances of the body of
the do loop in parallel that they some
of them might pick up some add in their
own integer X sub I and then do the
store and the problem is some of the
summations we lost that way and so we
can indeed Adam and
order we may lose something well a way
to fix that is to say well the problem
is we pick up the sum and we add
something we put it back let's make that
atomic great now the program is correct
again but it's not parallel we can add
things in any order but we can't add
things simultaneously and just doing
them out of order really doesn't buy us
a lot necessarily okay so this is really
the computation tree we like it allows
us to take the million things and add
them pairwise and as really we get a
tree whose height is proportional to the
logarithm of the number of things we
added up rather than to the number of
things itself that makes the thing wide
we need processing resources to make it
work but we have enough processor
resources available then in principle we
can do it in a long time instead of
linear time so that's the kind of
trade-off we're looking at and of course
this is a very simple example we're
gonna look something weird if
interesting in a few slides but first
I'm gonna turn my attention the problem
of finding the length of a list list I
have to get some listening to the stock
somewhere and I promised you several
programming languages ok so here's
here's a list that is made up of chained
records that are connected by pointers
and if you're only given the point of
the front of the list it was seen that
you necessarily need to do things
linearly you can't find that fourth
block and list without following the
pointers and here's the Lisp code for
finding the length of that list length
the list if list is empty then returns
zero otherwise look at the rest of lists
take its length and when you get the
answer back get one okay so this is a
typical recursive formulation in lists
the total amount of work is state of N
and the delay is Omega then that is it
takes at least length of the list
because you have to taste on this
pointers to find it wouldn't it be nice
if we could somehow paralyze this well
that if you had a processor at every
record there there are tricks you can do
but I'm not going to assume that instead
we could choose to represent the list in
a different way
and this points out the interest we
linearize multi-way to composition we
had no choice but to use a linear
algorithm on that list because of the
way the list is represented so data can
dictate the way code works and linearly
linearly linked lists are in fact
inherently sequential compare this to
piano or if a decree or represent the
number 5 by a 0 plus 1 plus 1 plus 1
plus 1 plus 1 we know that one can do
arithmetic with the numbers
in this way but it's so much more
efficient to use the binary form and if
you add binary numbers from right to
left then in fact you can do the
addition in time linear in the length
through representation but the length
the reputation is logarithmic in the
size of the number so you've saved a log
and if you carry look-ahead ladders you
actually get a double log effect so you
can actually add a number in two numbers
in time log log of the size of the
number okay so these are fish these are
wonderful parallel efficiencies an order
to get this you need a multi way
decomposition pair of paradigms so
here's some Haskell code for finding the
length of the list and instead of having
two cases the empty list and the other
case we're gonna have a three-way
decomposition of an empty list a
singleton and then more than one so the
length of an empty list to 0 the length
of a singleton list is 1 and otherwise
we're going to take the list and chop it
into two pieces a and B notice how that
how the code doesn't actually say how to
chop it it just says if the list is a
concatenation of two smaller things we
can take their lengths and add it up so
this this amounts to a summation problem
we're adding up a bunch of ones but how
we obtain those ones dictates the
efficiency of the algorithm the total
work is a point going to be theta of n
but the delay could be O of n and it
could it could be as big as Oh n in fact
it could be worse but it will be at
least Omega of log n depending on how
you split the lists if you managed to
split the list into approximately equal
pieces at each step you'll get a long
time if it's not approximately equal
then it could be a little bit worse than
log and this in turn assumes that the
splitting itself has constant cost so
the splitting is worse than constant
cost then that factors in the equation
here too okay so if all that is
background I'd like to tell you about
this toy programming problems which
we're going to see for solutions and I
want to thank den miss bum Steve Heller
for suggesting this I think it actually
may have appeared in some Google
questionnaires for employment I'm not
sure you can tell me once you've seen
the problem so we're giving this array
of integers and we're going to regard
the array of integers is describing a
bar chart okay so for each integer
there's a bar of that height and of unit
width and now we're going to do is pour
water over it and the question is how
much water does that does the diagram
hold it's kind of a funny shaped bowl
okay some of you are nodding your heads
recognition you've seen the problem
before
fine okay well there are lots of
different ways to solve an Emily afford
today one of them sequential and three
of them parallel and the key insight is
that we might be able to compute the
water on top of each bar separately and
then just add them up this gives us this
is a ID a this gives us B composition in
didn't event pieces this in turn gives
us the opportunity for parallelism
possibly we're to look at a sequential
solution first and the key insight is
the amount of water that can be on top
of any one bar such as this one that's
of height two is dictated by how tall is
something to its left and how tall is
something to its right that can contain
it so you find the tallest thing to the
left the tallest thing to the right and
you take the smaller of those two and
that dictates how much water can fit can
get what will be contained then from
that height of the water you then
subtract the height of the bar itself
that tells you how much water was used
to fill in that space okay so what we're
interested in is knowing for each
position in the array
what is the tallest thing to the left
and what is the tallest thing to the
right we can determine tallest things to
left by doing a left-to-right sweep
doing a max accumulation I said the
nasty word accumulation this is going to
be sequential algorithm so we start with
minus infinity and we sweep to the array
from left to right at every every step
max into that accumulation and that
gives us a sequence of values shown here
in the red at the bottom we hit this
thing high two so we see the answers to
now we hit a thing of height six it's
gonna be six for a while then we hit the
eight which is the tallest thing a
little bunch it's eights all after that
similarly we can do a right-left sweep
and find the tallest things to the right
then at each position we have the
tallest thing the less than the tallest
thing to the right we find the minimum
of those two shown here as little green
horizontal bars and now we abstract that
away that's the information so that is
the minimum height the minimum of the
tallest things let them tell us thing to
the right and now we compute the water
for each bar by filling in the bars and
doing that relevant subtraction and that
gives the answers at the top and all we
have to do then is add them up let's hit
that with big Sigma not worry about how
we do that and the answer is 35 okay so
given this outline for how to go about
it we can see some sequential code
here's some sequential code can anyone
guess what language this is
hmm okay anyone but yawn anyone for
David yeah this this is fortress code it
may look like pseudocode that's because
fortress was a programming language
designed to look kind of like
mathematical pseudocode and use
mathematical notation podiatry there's
actually actual functioning running code
or at least it was as of three years ago
last time I checked it
okay so here's code we're going to
compute the histogram I call this
histogram water because there's kind of
a histogram related application and it
takes an array X which it takes I'm
going to use 32-bit integers indicator z
32 to index the array and the array
contains elements of types type integer
32 and the histogram routine is going to
return a value of type Z 32 and we're
going to let n be the length of the
array that happens in the second line
and then we're gonna get rid of
discharge the trivial case by saying if
n is 0 just return 0 that way don't have
to worry about some some boundary cases
down in the bottom of the code and
essentially the structure is Cooke I'm
not going to go through this line by
line but I'm gonna point out that there
are three loops in your right one is
limits for the left-to-right sweep one
is to the right left sweep and one is
for the final computation and summation
and you can see the left-to-right sweep
is doing this max accumulation into left
and the the right left thing is doing
this max accumulation and the last one
has this computation we're taking a less
than a right taking the minimum of the
two subtracting X sub K itself which is
the height of the bar and accumulating
that into the result then we return the
result okay so this code is okay it does
the job it gives you the right answers
we could try to optimize it why I don't
know because optimization is what
compiler guys do and one thing we can do
is diffuse the last two loops so we can
instead of doing it in three passes
where you can do it in two passes one
left to right and one right to left and
during the right-to-left pass as we're
creating the maxes then we can also do
the rest of accumulation as well that's
the typical thing for compiler to do is
diffuse loops okay so that's that now
let's look at is there any questions
about that sequential version okay I ran
through it really fast because the
interesting stuff is coming up let's
complete consider a pure divide and
conquer approach
I indicated that we can compute the
water on top of each bar independently
once we have the information from the
sweeps anyway at least that was one way
of getting information but if we can
compute the water on each bar separately
then maybe we could recursively
decompose the graph down to its
individual bars compute the water on top
of each bar and then pairwise combined
solutions so that would sort of look
like this well it turns out a single bar
can't contain any water in fact a pair
of bars can't contain any water either
there's nothing to contain but then when
we start combining groups of two into
groups of four suddenly there are places
to contain water and then we can combine
those guys into groups of eight and then
finally into the big group of 16 I
conveniently chosen or length of an
array of length 16 precisely so the
binary decomposition wouldn't have
corner cases and that involves extra
programming but it can be done okay so
we would like to how do we do this
recursive decomposition how do we
compute the water on each single bar
well we know the answer is zero so the
meat of it is really and how do we
combine groups into larger groups and
the thing to realize is that we don't
need the internal structure of a group
of stuff I'm gonna call that group a
glob all we really need to know is how
much water it already contains and what
is its outline so there's this idea of a
bi tonic outline why do I say it's
platonic we know the word monotonic
means that something only increases or
only decreases I probably got that
backwards for you but monotonic means
it's it's monotone it just goes in one
direction by tonic means it can go up
and then it can go down or as I said of
the Monty Python bright brontosaurus
it's very thin at one end much much
thicker in the middle and then very thin
again at the far end so think of the
brontosaurus when you look at these
globs okay so in order to combine two by
tonic globs here's one on the left and
one on the right what we're going to do
is abstract them so all we have is the
outline of the total amount of water in
it we actually don't even know to the
shape of it we don't even need to know
the shape of the water just how much
water is in it slow abstract that as an
integer so here's a bi tonic glob on the
left it has 11 water and a bionic shape
on the right
as to water and we want to combine them
and figure out how much more water they
can hold and turn that into a single
glob so the trick is to find an
abstraction that we represent in real
things you can combine them and the
result is another instance of that same
abstraction so in order to combine them
we stick them together we figure out how
much water it will go in there we take a
new Bionic outline that covers the
combined thing and then we add up those
three numbers and that will give us the
new water and now we have a new instance
of a single by tonic glob okay so how
are we going to accomplish that let's
let's dig down a little deeper into the
representation of one of these globs
going back to this one we know that it
has 35 water in it oh and that that
water switched font I'm not quite sure
what's up with that okay and we're gonna
break the outline into three pieces all
we need to represent a bionic outline is
a list of the plateaus the horizontal
segments and four in order and what we
need to know for each one is its height
and its width and we could just have a
list of height width pairs we're gonna
be a little more tricky here and break
out and identify the highest plateau
because that's going to be the interest
to us in the combining algorithm so
they're in sort of turquoise at the top
is this little plateau and it is its
peak and then we're gonna have a list of
the plateaus to its left and a list of
plateaus to its right
so here are the indications of a height
and width here the height width pairs
for each plateau and we're gonna
represent this is a five element data
structure that looks like this a list of
the plateaus to the left the height and
width of the plateau at the top a list
of the plateaus to the right and then
finally the water itself we clear on
that okay I see a few nodding heads
that's good enough to go okay so here's
fortress code for that we're not going
to go into this detail but notice that
at the top there's an object declaration
a glob is a record structure with five
components left high to the top width to
the top the right list and the water in
it we've got a couple of utility
routines there's its width routine in
the second line whose job is to take the
list a list x of height width pairs and
add up the widths so for every pair of
the Q values there's this thing Phil
which takes such a list and a number M
which is the height of water you'd like
to see above it and figures out how much
water actually fills in by looking at
each bar and looking at its height P
subtracting that height P from the
proposed water level M multiplying that
by the width Q and that out of them all
up if that fills in and the bottom is
some fortress boilerplate that has to do
with making a binary operator whose
symbol is o plus and being able to use
it as a big reduction operation so don't
worry too much about that boilerplate
but notice that in the second line of
the Declaration of object law production
it says it extends Monroy production of
glob that's essentially a promise that
the the o plus operator the combining
operator is going to be associative that
in turn will justify a parallel
implementation in the third line it also
claims to be reduction with zero there
is a zero for the addition operation but
we're not going to get into the details
of that okay so to combine to buy tonic
globs there are three cases it might be
that the left one has a higher peak than
the right one the right one has a higher
peak and left one are the heights of the
two peaks are the same okay and for each
of those cases we can have a little
piece of code that handles it and for
each one we can have to figure out how
to fill in the water and then make a new
outline that essentially bridges the
water the water will extend some plateau
and we'll have to figure out how to make
a new outline the interesting case is
the one down the middle
notice that I'm we're thinking about
right and left from your point of view
now yes
notice how the result at the bottom in
the center column reuses the left-hand
part of the outline from the top but the
right-hand piece has to be constructed
of three parts the no sorry the right
right the right hand is used yes on the
left the left hand outline consists of
three parts the left hand outline of the
lower glob and the little piece that
gets constructed to span the water and
then a piece of the left hand outline of
the higher glob and in
in order to get that piece of the
outline of the hired globe we had to
take its left hand list outline and
split it into three pieces and here's a
utility operation for doing that split
it something it takes a particular
height and gives you the part of the
outline that's below that the part of
the outlines above it and the part
that's right there there might be such a
piece and given that here's the code for
combining to buy tonic gloves and yeah
it's a bit of a mess but just look at
the structure of the code here's this
operation Oh plus it takes two globs and
returns a glob that's important it takes
two things in the same type and return
something of that same type it has a
3-way case on comparing the heights of
the x height and the y height and
depending whether it's less eager
greater there are three different pieces
of code that represent the operations
that I showed you pictorially on the
previous slides for those three cases
and we'll get in the details but the
interesting question is here's a very
complicated piece of code that's
implementing a binary operator and
gloves
is it associative can we prove that well
we could take the two expressions a o
plus B o plus C and a Oh plus B a plus C
and inline the two instances of a plus
and do the necessary programs source to
source transformation and try to reduce
them to clone forms you know and that's
we could do that by hand I've done that
it's a pain if you know I should have
adhere improver to it or compiled here
improver built into a compiler even
better remember then my declaration I
promised it was a mono IDE so the
compiler knows that I intended to be
associated would be nice with the rest
of the of the proof we're done
automatically
or if unit testing can be done or
something okay so given that operation I
assure you it works you can trust me so
given singleton Bionic globs we put
water on top of them there it is none of
them hold water in the whole argument
doesn't hold water we abstract that is
by tonic outlines with zero water in
them and then we combine them pairwise
and we're going to do that using a weak
binary tree of o+ operations and o+ by
the way is associative so we are
entitled just to use a big o+ operator
because we really don't care how they're
grouped we can leave that to someone
who's worried about efficiency about
whether it's
to be done sequentially or in parallel
and so the solution is that for every X
in the array we every such V we make a
single and glob out of it
we hit it with Big O plus and the answer
comes out as a white an account line
that happens to have a number 35 in it
we read carefully reach in and abstract
that extract that last component number
35 pops out and that's our answer so as
a result with all appreciating several
pages of fortress code that I showed you
the final solution of fortress looks
like this it's one-liner what's on the
left hand side is the same methods
signature you saw before it's the only
new thing is this thing here on the
right hand side and it says that in
order to solve the problem given the
array X we're going to for every V and X
make a singleton glob that's the code
that constructs a singleton glob hit it
with that Big O plus operator that
combines globs and then reach in and
stripes extract the water component
we're done okay well it was very very
clever to be able to write this solution
as a one-liner once we build up this
very complicated data structure and its
associated operations okay
is there another approach well let's
look at that left-to-right sweep again
that's actually a familiar sort of thing
I'm going to introduce the idea of a
mono I'd cached tree to compute it and
the idea here is that we build up a tree
one layer at a time and in this case
with a combining operation and that
operation should belong to a mono IDE
which is a technical mathematicians term
that just means the operation is
associative and that it has an identity
yes so we're going to compute max
pairwise and then do it again and then
do it again and then do it again and
whereas before I sometimes showed an
operator there to indicate these are
operations being computed here I'm
actually intending this be represented
as an explicit data structure in memory
so we built this tree in memory using
this combining operator Max and so every
internal node of the tree now has the
result of combining everything that is
all the leaves that are below at
okay very simple idea but turns out to
be remarkably useful in particular
okay so here's fortress code to
construct a math.max cash tree at the
top we have the Declaration of this tree
data structure so trait cash tree hat
comprises three distinct object types
there's a null node a singleton node in
a pair node and their definitions are
straightforward and then to compute a
Max cash tree if what we have from a
list if what we have is an empty list we
create a null node and the null node
contains the identity for the combining
operation in this case minus infinity is
the identity for Max or if the list of
size one we create a singleton node for
its single element otherwise we split
the list into approximately equal
non-empty pieces then we compute the max
cash tree of each of those pieces and
notice the comment line this has
parallel recursion here forretress
allows the two element two expressions
in a tuple expression to be computed
independently so so there's a potential
for parallelism there and then those
results get assigned to the values a and
B and then we use those ami values to
construct a new pair node okay so this
very simple code will construct this
mesh Max cash tree and the point is is
that in order to do the parallel
left-to-right sweep will be first first
build a max cached
tree that which builds the tree from
bottom the top then using a top-down
sweep a second pass over the tree we can
compute the result of the are parallel
are left-to-right sweep in parallel
rather than sequentially and essentially
the trick is that as you go down the
tree as you visit each node information
is going to be passed from the left son
to the right son or the left child or
the right child must be a gender
nonspecific here and you do that at each
level of the tree and as things are
passed across it turns out that what
happens is that at the top node things
are being able to jump distance 8 you're
transferring information from the left
half to the right half and then at the
next look at each level before your
passage of left half the right half and
so in a logarithmic number of jumps the
information can reach where it needs to
go and the result will be in fact the
very same array we computed before by a
sequential pass over
from left to right now just reversing
the diagram mirror image we can see we
can also using the same max cache tree
because recall the max cache tree itself
is symmetric it doesn't have any
left-to-right or light right-to-left
bias so given that same tree we can do
the right left suite passing information
from the right each right child to each
left child and compute the same array
that we computed with the sequential
right left sweep and in fact by being
very careful how we code the recursion
we can do both at once as you visit each
node grab information and do that grab
information and do that keep passing it
down in the correct pattern and you can
compute both arrays at once and the
result will look something like this and
yeah there's a lot of details and left
out of this but I will show you the code
there it is okay so the code so here's
the here's an implementation of
histogram water in the top line what it
says is given the list X compute its max
cached tree of X then feed that to this
recursive routine process whose job is
to do these left to right right left
sweeps using this child swapping
information trick and it's not very much
code it's just a matter getting the
pattern of swapping information right
and again it breaks down into three
cases of a null node a singleton node
and a paranoid because that's the
structure of the of the data structure
and again parallel recursion can happen
there in the first implementation of
process the the first method which takes
a paranoid and if it's a para node it
will process the left tree and process
the right subtree and those two calls to
process our arguments of the binary
operator plus and fortress says you've
got a binary operator then it's the two
argument expressions can be executed
independently if the compiler chooses
and so you there's potential for for the
parallelism there so that is the end of
that solution notice it was only two
pages of code instead of four or five we
have a much simpler data structure we
avoid making that complicated by Tonique
glob thing there is no need to stop rate
on a special glob data structure to
invent this special o plus or define the
big o plus operator and it made use of
an abstraction the the mono I'd cache
tree that might actually be used
another setting so aha we've got the
beginnings of potential library routine
it might be useful in other cases okay
let's revisit that left-to-right sweep
one more time
it smells familiar
they'll smell sweeter to me it's
actually an example of a parallel prefix
operation if you're not familiar the
parallel prefix operation I like to use
the checkbook metaphor suppose you've
got a checkbook if you you can keep a
checkbook I keep a checkbook a lot of
people just use online statements and
depend on the computer to do it but if
you keep a checkbook by hand you know
that you enter deposits and debits in
the form of checks and you keep a
running balance and at every step the
running balance is the result of adding
to the previous running balance either
the deposit you made or subtracting the
the cheque that you wrote and so if you
have just a list of deposits and
withdrawals represented a suitable
puzzle negative form then the parallel
prefix sum of that list is the set of
running balances because what parallel
prefix really means is given an array of
stuff find the sum of every prefix of
that array so you find the sum of just
the first thing the sum of the first two
things is sum of the first three things
is sum of the first four things and so
on and the reason it's usually referred
to as the parallel prefix operator is
that although there is an obvious
sequential left-right implementation in
fact it's exactly our left-to-right
sweep it using summation rather than max
we know that there are good parallel
implementations this operation in fact
we've already seen one we can do it with
a mono in cash tree that's one strategy
one of several well-known strategies and
literature for computing a parallel
prefix so we're just going to abstract
that and say we're gonna take
left-to-right sweeps for granted we
using max we're just going to say that
is a max parallel prefix operation and
we're going to use this notation for it
which is in the upper left of this slide
I'll just write the operator max and
then put a left-right arrow over it to
indicate that I'm doing a left-right
sweep with it or conceptually left/right
sweep I'm not going to say let's
implement it anymore than I say how big
Sigma is implemented I just assume this
is a library operation leave it up to
the compiler the runtime to decide
whether it's essential or parallel and
certainly the right-to-left sweep we
might be able to indicate by using the
operator max to the right left arrow
going over it
so let's assume we have these in our
library then here's the concise solution
once I have those operators in my
library
no more boilerplate I have a true one
line or in fortress here and this works
in fortress I have mind you this is a
executable code and it's a big Sigma
operator and there are three parts to it
first we're going to take the list X and
the max parallel prefix of it and the
max parallel suffix of it the right left
sweep is called a parallel suffix and
we're going to compute those three
things in fact we can compute them
simultaneously and we'll leave it to
compiler to figure out whether those are
computers three separate operations on
three different different processors or
whether to maybe fuse their loops you
know various things that a compiler can
do we're just going to compute those few
things the list X itself it's parallel
prefix max and it's parallel suffix max
then we're going to use the zip operator
from Haskell which is also in Fortress 2
zip them together thereby turning a
tuple of three arrays into an array of
three tuples it's an effective kind of a
matrix transpose and then for every
element of that array we're going to
call the three elements V left and right
so in fact V is the element the single
bar of our bar graph and left and right
are the things that we would have
computed during the left/right sweeps
and right the left sleeps in the
sequential version so with this cover
was this cover expression we have now
got a generation process going for the
big Sigma operator that will generate a
triple of values V left and right for
each thing we want to do well what do I
want to do with that triple we want to
compute the sub expression which is to
take the left and the right min them
subtract the original height of the bar
that seemed out to water we got and then
the final thing we're going to do is add
them up so this is a generate step in
the yellow a mapping step the body of
body expression of the big Sigma and a
reduce operation the big Sigma so this
is this is MapReduce in math notation
and I think this is actually a really
good way to program not only in a large
but also in the small even for even for
small stuff like this
now is this a sequential solution or a
parallel solution well that's a very
good question I really haven't said much
about that I've indicated places in this
solution where things can be done
independently so for example the
computing of the max parallel prefix and
max purl suffix could be computed
independently or perhaps they could be
fused as we did when we did the clever
thing with a Monroy cache tree all of
the instances of the expression to the
right of the biggest Sigma are
independent they can be computed
independently maybe we could do that on
a bunch of different processors but the
point or they can be done sequentially
the point of this is that if we were to
take this precise piece of code and do
careful in lining of operations such as
parallel prefix and purl suffix and zip
and that could reduce if their
implementations are sequential then
suddenly sequential for loops are going
to appear in our code and we can do loop
fusion on those for loops and in fact we
can transform that into the efficient to
pass sequential solution and I don't
have a compiler it does it automatically
but I have done that transformation by
hand this is about the biggest program I
can do it by hand on and yes you can
reach the sequential solution from this
code by doing the appropriate
optimizations through source to source
transformations and I don't want to I
don't want to belittle the effort it
takes to construct such an optimizing
compiler but the techniques involved are
already familiar and out there and other
compilers more to the point from this
exact same solution if we do inlining of
definitions of the operators that happen
to have parallel implementations well
those parallel implementations are
probably gonna operate on either actual
tree data structures or at least on
computation trees if we then do
deforestation which I would describe as
loop fusion on trees then the same code
can be transformed into the efficient
two past parallel solutions that we saw
with the Monroy cache trees so this is
the main point of the talk is that it's
possible to write solutions to the
problems that don't have a commitment as
to ever to be sequential or parallel and
then downstream either optimization at
compile time or decisions made at
runtime to make decisions what whether
to be sequential or parallel and this is
an opportunity to automate those
decisions and make the best use of
whatever Hardware resources are
available the time
the thing about accumulation is you
start with an empty solution you use
each input incrementally update the
solution notice that this incremental
update operator is typically as
symmetric it takes two arguments which
are an input and a solution and produces
a solution as a result any trees you
construct this with this will
necessarily be a symmetric and they will
have the form of that long left linear
list or perhaps a right linear list so
the processing will be inherently
sequential this is great if you're
committed using one processor it turns
out the accumulation strategy is superb
for a single sequential processor which
is why we've designed processors that
work that way that's why we have CPUs
that have registers those registers used
to be called accumulators back in the
60s and before that in the card
processing machines in the early part of
the 20th century that process the
Hollerith cards the registers were
called accumulators because typically
you'd be feeding census data through
them and accumulating various counts and
and the processor registers continued to
be called accumulators probably through
the early seventies and then we switched
over the name registers for everything
not quite sure how that happened the
great thing about accumulators is that
while they take linear time they save
space you pull something on memory you
add it the solution tends you keep the
solutions small and you avoid building
data structures you can usually keep
things in very and global variables all
these variables that are local to your
routine now compare that with a divide
and conquer approach with divide and
conquer from each input you construct a
singleton solution and then you merge
the solutions typically pairwise this
may take more space may be done long
times you do this when you're trying to
shorten the time but this will be at the
expense of space and in fact
intermediate solutions may need to be
heap allocated the merge operator is for
merging two solutions is typically a
little more complicated than the
incremental update operation that use in
an insulin in the accumulation paradigm
but notice that merge is typically
associative and this is possible because
look at the type signature of merge of a
merge or of o+ its job is to take two
solutions and to produce a solution
Ravin take a solution and an input and
produce a solution just from its type
signature you can tell it has the
possibility to be associative in a way
that the accumulator combining operation
does not and its associativity is a key
to parallelism and but furthermore
there's another advantage that
identifying this associative combining
operator usually lends deeper insight
into the problem I wasn't able to see
that a Monroy cache tree was the answer
to my problem until I'd first built the
glob combining thing and the thought
okay how can I further simplify this
maybe I don't need to completely commit
to building the entire solution at once
maybe I can build a sub solution in the
first pass and then from the sub
solution build a final solution but
doing both with trees so looking for
scioscia Tiffa T in the operators of
your data structures may lead you
insight into the nature of your data
structure of the nature of your problem
in fact not just acitivity but other
algebraic properties are also important
and this is a point when I find that the
eyes of my audience tend to glaze over
or when I suspension thing associativity
commutativity maybe remember those from
from grade school particular if you took
something like new math when I get the
item Potence well item what does item
Potence nobody remembered idempotence
means then there's identity and ends the
identities and zeros for the mining
operations but it turns out these hat
these have correspond to very simple and
appealing ideas that are relevant every
programmer associativity just means the
way you group them doesn't matter and
that is the key to parallelism
commutativity means the order doesn't
matter you can swap them around and
you'll still get the same answer when
you add them up or max them up item
potent means duplicates don't matter so
addition is not item potent but max is
you take the max a bunch of things it
doesn't matter where for any one thing
with their duplicates in there or not
you'll still get the same answer if a
value is an identity that just means
this value doesn't matter you can add it
in or not you'll still get the same
answer and a zero means the other values
don't matter
I'm the king and I Trump everybody else
if you're multiplying a bunch of stuff
together if there's even one zero in
there among the million values the
answer is going to be zero the other
values don't matter
now invariance such as knowing that the
property that an operator is associative
gives an implementation wiggle room it
gives the compiler or whatever processor
working in the program the freedom to
exploit alternate representations and
implementations and in particular
associative he gives implementations the
necessary wiggle room to use parallelism
or not as resources dictate so winding
up the talk I think that going into the
next millennium we need a new mindset
now we've got we've got an old mindset
from the previous of that same
millennium and I won't be so highfalutin
from the last century during which we
developed a bunch of programming
concepts mostly since the 1950s but we
developed these ideas in the 1950s we
have polished them and honed them so
they are they work beautifully in their
intended sequential setting but the
truth is that do loops are so 1950s
you know Fortran is literally or is over
50 years old linear linked lists are so
1950s Lisp was developed in the 1950s
Java style iterator same cups came
somewhat later but they are still so
last millennium the java iterator data
structure commits Ute and its api
commits you to a sequential processing
paradigm as soon as you say well first
let's set some to zero you're already
host you're already hosted and I have as
soon as you say process subproblems in
order you lose a for loop with a C or
Java kind is probably the worst possible
way to say map it takes a lot of work to
figure out whether it's that the for
loop can in fact be parallelized or
whether the iterations of its body are
independent or not the great tricks that
sequential past don't work in a parallel
world and the programming idioms had
become second nature to us over the last
five decades as everyday tools don't
work in a parallel world they have to be
undone at least to some extent and I
found that this has changed my
programming style even when I know I'm
writing sequential code or even when I
intend to as soon as I write int x
equals zero or something like that there
are about five voices clamoring my brain
once says isn't really the right data
type and another says is X really good
name for that
to give it a better name and never says
should that zero be integer or
floating-point you know and you know and
if I matched it to the but now there's
an extra voice in the clamor that says
are you sure you want an accumulator
maybe there's a better way to do this
you know and as you write each line of
code there's be these little competing
voice if you seen Pixar's inside out you
know what I'm talking about
you know there are you know I've got joy
and anger and frustration and all that
stuff and then a course of gerd critter
critics you know criticizing every line
of code aright and I'm adding to that
list of critics so we've got these
idioms and they're built into our
mindset and they're built into our
infrastructure our programming even our
parallel programming languages are
typically designed by starting with a
well known sequential language and then
bolting on a couple of parallel features
on the side I think it's possible to do
better
forretress was an attempt to do that I
think we achieve partial success but I
think more needs to be done so I think
in the parallel we need parallel
strategies and programming language to
support them for problem decomposition
data structure design algorithm for
immunization the top-down description of
this mindset is don't split a problem
into the first and the rest instead try
to split problems in equal pieces
solve them and then combine the
solutions the bottom-up view is that if
you have individual inputs or individual
leaves of a tree coming at you don't
create a null solution and then
successfully update it instead map the
inputs to singleton solutions and then
figure out how to merge solutions and
that's usually trickier
okay I've have been I have put forward a
rather extreme manifesto and I wanted to
make sure I got that manifesto in your
face and I'm gonna back off slightly in
practice engineering needs to be done in
practice there many optimizations and in
particular in practice you need to make
intelligent decisions about when to be
parallel and when to be serial so first
of all you probably need optimized
representations of single and lists
because if you represent singleton lists
explicitly in the heap well we know down
at the leaves of a tree is where most of
the action is going on if you can just
save some storage down at the Lee's
you're saving half your storage you can
use tree branching factors larger than
two for example in the closure
programming language which is kind of a
list dialect above
designed to fit within the JVM he chose
to represent lists as 64 area trees I'm
not sure the 64 is the ideal number but
I know it's way better than two it's
also way better than 10-million you can
use various kinds of self-balancing
trees like red black trees finger trees
but most importantly you probably want
to use parallelism as you go down the
tree until you eat some certain point
and then start using sequential
techniques at or near the leaves you
might also want to use arrays at the
leaves rather than trees and you can do
things do things like the dynamically
decide whether to process those arrays
sequentially or Tepera recursively
subdivide the arrays and turn when
you're iterating over in an integer
range you can decide dynamically whether
the process that is quench lis or by
doing perl recursive subdivision this is
something we were experimenting with in
fortress so to conclude my argument I
hope to convince you that a program that
is organized according to linear
decomposition principles the kinds have
been very familiar for the past 50 years
it can be very hard to parallelize I
repeat someone won a Turing Award for
writing a pilers that do that it
shouldn't be that hard but if a program
is organized according to primarily
independence and then secondarily to
divide and conquer principles which is
one of many ways of achieving
independence such a program may be
possible to run either in parallel or
sequentially with the decisions being
made dynamically according to available
resources I'd like to compare this
proposal to the introduction of garbage
collection which by the way also dates
back the 1950s with lists and it took
some work in the 80s and 90s to
parallelize the garbage collectors
garbage collectors were feared for
several decades and it wasn't until Java
popularized them early that they became
mainstream and there is a sphere that I
don't know where the storage manager is
going to put my data I don't know what
the effect is going to be on the cache I
don't know what the effect is going to
be on all kinds of things and we finally
got comfortable with having the
allocation of storage the allocation of
data managed automatically and yes there
are some associated overhead just is
possibly not as good as we could do if
we didn't did everything compulsively by
hand but doing by hand is so terribly
hard we have better ways to spend our
time as programmers they were willing to
give up
little bit of performance and hen and
then delegate that work over to the
garbage collector and those overheads
are present they have been reduced over
time as we continue to engineer the
storage manager so they've not
disappeared I argue that the management
of parallelism is very much like garbage
collection is the automatic assignment
of processors rather than the automatic
assignment of storage it's an automatic
allocation of code to processors rather
than dated a memory and again we are
going to fear this at first because
those automatic strategies are going to
have overheads that will look daunting
at first but I believe that over the
next several decades as we continue to
engineer those the same thing also
happen with the garbage collectors and
the overheads will be reduced over time
but not disappear but I believe that in
a world of parallel computers Avaya
wildly varying size possibly
heterogeneous processors all this kind
of stuff well we've got heterogeneous
memories - we've got caches we got disks
and we've got SSDs and all kinds of
stuff and we matter storage managers
deal with that to some extent
and as we have different kinds of
processors available we can learn to
manage heterogeneous processors as well
I think there are only hope for true
program portability not to have to keep
rewriting code every five years finally
I believe that better programming
language design can help to encourage
programmers to use the kind of
independent thinking that will lead to
code that allows parallel programmers to
run programs effectively in this way so
that is my vision for the future and I
think it's hard and maybe a few you be
interested in trying to make it happen
thank you
your time for questions are the
questions there's one in the back yeah
yeah wenches it's a it's a the question
through the mic is it possible to get
some of this stuff into our current
programming languages new programming
languages seemed notoriously hard to get
accepted yes and we see that happening
already one strategy for trying to make
things independent just to make things
purely functional we've got a history of
design of purely functional programming
languages and Haskell I think is a
preeminent example of that but ideas
those languages are leaking over and
other things and we see now map and
reduce features that are being
introduced into Java for example that
were quite explicitly inspired by
Haskell and I know that the Java Design
Group at Sun and then Oracle has kept an
eye on the Haskell evolution for about
the last 10-15 years so yes it is
possible graph stuff on but it's hard
and you never really quite free yourself
from the sequential mindset so we've got
the map and stuff in there with the Java
iterators were also still there and
they're there they're tempting the
programmers so sure one can't always
start with something that isn't quite
what you want and bolt new things on the
side the question is how do you get old
things to drop off and maybe that is
best done by a process of language
evolution and letting things just fall
away in decay where I've been designing
these something new from scratch because
I agree it's really hard to get a
full-blown language adopted if it's if
it's unfamiliar new I gave fortress only
at most a 20% chances of becoming big
and making it and surprise the prize is
in the 80% but I think we learned a few
things from it and I've had some
increase recently from the eye from a
Giulia users group want to be here to
talk about fortress hoping it can inform
the evolution of Giulia which is a
language that has gained more traction
and is head in this direction other
questions
I know I'm standing between you and
lunch
three-two-one yes so the concise
solution that you had at the very end
was nice in that it was short and it was
easy to sort of automatically optimize
to the sequential and parallel
implementations but it also came at a
cost where it took the better part of
half an hour to explain how it worked to
a roomful of Google engineers and
convinced them that it was correct so do
you think that there's a tension there
where in order to have these sort of
better parallel optimisation techniques
it sort of comes at the cost of having
to solve our problems in a more complex
way mentally complex way not necessarily
code difficulty or is that just a sort
of function of the fact that we were
looking at you know sort of like an
interview style brainteaser solution
okay
yes your points are very well taken I've
got several responses first of all you
flatter me and in trying to make me
think that I've actually proved the
correctness of this program to you I
don't believe I have I think I provide
some informal justification and tried to
give you some intuition as to why it
might be right and furthermore I tested
it yes it is possibly being to be misled
by examples that are small enough to fit
on presentation slides and so you should
always be aware of that as well yes if I
have a data structure that has dozens of
components rather than five spotting or
even coding that a social event
associative operator might not be easy
to do or it might not even be the right
thing to do but in that case you might
want to break your data structure down
into smaller pieces and work on them
separately on the other hand about the
unfamiliarity of yes how many of you had
already seen the idea of a parallel
prefix before let me just take a quick
poll okay I'm seeing about 15 hands okay
now how many of you have seen a
summation operator before okay all of
you so I didn't have to explain that to
you if only we were to reach a point
where everybody already knew what
parallel prefix was then things would be
somewhat simpler leastat step of the
explanation can be emitted and yes it's
a it's an uphill road there is an
education process involved and before
that just identifying what
are the relevant primitives I believe
that parallel prefix isn't important
when there will probably be others as
well and in fact I explored something
wrote papers about them back in the 80s
when I was at Thinking Machines
corporation working on the connection
machine which is a highly parallel
supercomputer of that era but this
situation doesn't discourage me because
it reminds me a little bit of the
structured programming revolution of the
1970s when it was suggested that one
limit oneself by not being able to write
arbitrary branch statements anywhere in
your code but just limit yourself to
if-then-else and while do and a handful
of other structured things and at first
programmers were flummoxed because they
said well I've got to do this and I've
got to do this and I don't see how to
package it up using those primitives and
it took a while to develop a style of
programming in which the go-to really
was not used very often at all but there
was there's a real passionate debate
about that in the early 70s and you can
look that up there's a debated I think
the 1972 ACM National Conference and I
took part in that debate I was still a
little wet behind the ears I was a
student but uh people like bill Wolfe of
Carnegie Mellon were there he worked on
the Bliss compiler and the Bliss
compiler was one of the first systems
programming language to say we can do
systems programming without a go-to
statement and people were dumbstruck and
said wait you can't do that you know
operating systems of forcing you to go
to so it's a matter of figuring out what
are the relevant primitives and then
developing a community consensus on a
style that actually achieves the goals
of programming within that set of
primitives and so I would like to think
by analogy we can find a set of parallel
programming perimeters that are adequate
for 90% of the structuring of our code
in much the same way that the structured
primitives like while do and if then
else did turn out to be adequate for
structuring 90% of our sequential code
so I said as a grand vision it's a
result of synthesizing you know 40 years
of experience and it's not that I mean
to play the experience card it's just
that I've got an appreciation for
history and I've lived through probably
two-thirds of it and I've tried to study
the rest and so I think there's some
lessons for going forward and it's not
that I think it will be easy I think it
will be hard but I think it will be hard
in the same same way that some other
earlier things were hard we got over
those
it's a long answered your short question
yeah a quick question about recursion it
seems to me when I look at things like
the definition of regular expressions
and all of these things they have all
recursions that that that go exactly
what you say don't oh here's my starting
point and then do a bunch of those how
do we get away from that yes yeah we do
that in regular expressions and more
generally when writing parsers sorry
grammars for programming language we
tend to stick to left recursion to write
recursion precisely because we know then
the parser generators will have an
easier time dealing with it but that's
because they were going to then generate
sequential parsing algorithms if your
price or gender we're going to generate
a parallel parser you might write your
grammars and your regular expressions in
that grammar in a different way and I
speak from experience I've actually
tried those experiments yeah
just curious what is the cost of merging
to buy toning gloves what is the cost of
merging to buy tonic gloves okay let's
go back about 50 slides there it is okay
most of the cost actually is in this
three-way split operator is used to take
either a a left outline or right outline
and try to find the point where the
water is going to be so you can break
off just a part of it and use that as a
part of resulting outline all the rest
of it I'm just scanning the code quickly
yes all the rest of it is constant time
the problem is that three-way split
and that is done on the previous page
and essentially by doing a binary
decomposition of the list that is a log
time operation that's a long time in the
length of that half of the outline and I
couldn't figure out a way to make that
that log factor disappear so this
combining operation is is not is not
constant time and therefore the overall
combining time is not going to be law
again but probably the square of login
and I'd love to find a way to do better
in this approach yeah I have used a lot
of the things that you have built over
the years that I just wanted to say
thanks thank you that means a lot to me
do you found something useful
do any of the popular languages that are
in common use today show promise in the
directions that your that you're
thinking about things so you've created
this this new language you mentioned
Julia you mentioned Haskell a couple
times yes are any of those do you think
the right place to concentrate on adding
these features these types of ways of
thinking about things yeah I think the
Julia and Haskell and Clojure are
probably the what most widely used
languages that are intentionally
exploring in these directions I think
such exploration could also be done in
say Scala but the Scala community I
think does not made a priority of
investigating that dimension
so this may be sort of bias or selection
selection bias on my part but you
mentioned those languages and I don't
think of them as being very widely used
do you think there's room for things
like this in say Python which I know has
map and reduce operators and so has a
little tendril in this direction yes I'd
be delighted the Pythian community
suddenly said yeah this is what we want
to do you know but I don't
but the Python community is really big
and while there is some exploration it's
a relatively small fraction the Python
community yeah Julie has a much smaller
community but they are really pressing
in this direction and they're all more
popular in fortress you know so help me
obi-wan you are my only hope larger
language less interesting community but
yeah so I'm just going around talking to
all kinds of communities including you
hoping to find some interest and maybe
we'll take in a direction I never
thought of now question here
so you compared the parallelization to
the structured programming 40 years ago
and I think one role in difference is
that the program community was a lot
smaller back then so do you think like
because of how many programmers there
are in the world these days that there's
more inertia and it's going to be harder
for like a few computer scientists with
the debate and a few papers to like
change the way the programming is going
my initial reaction is oh yeah on the
other hand I think the programmers of
the 1970s were somewhat more fractious
and contentious than they are now know
there were a bunch of you know really
yes even in the 1960 there were stubborn
old timers know they've been doing this
no since until 1945 you know who had
experienced all of programming history
right you know except for except for Ada
Lovelace essentially and they said well
we're settled in we've got the way we
want to do it and Fortran is good enough
for the for us thank you
or COBOL and and didn't want to change
and yet small groups arose and took
things in new directions so yeah I think
there's more inertia but I think there's
less resistance today to new ideas so I
think I think the programmers are a
little more open to trying out new ideas
if it's clear that they will solve a
problem for them
whereas I don't I'm seeing a lot less of
the programming language religion worse
let's say which was a a term of of
technical art back in the 70s how does
this all apply to working with the real
world a lot of what I do is user
interface programming for example is
that is that parallelizable like what
what thoughts do you have about that
sort of thing good question well there
are different kinds of user interface
programming if you're if you if you are
handling an inherently sequential
interaction with a single user well
ensure there may be there's not much to
do and part of problems at our users
tend to be sequential flowing at
parallel users then things might be
easier but the things you're presenting
the user
I think there's I think of HTML have
been designed a little bit different
than there be a lot more opportunity to
process the different parts of web page
being presented than there are now and
we kind of struggled to try to find the
parts that can be done independently can
I process this ad while also working on
the main text you know and then for
things up but they interact in weird
ways yeah so yeah I think there are
there's opportunity everywhere then the
question is as a question of engineering
is it then worth exploiting the
potential parallelism or not you know if
you could get two processors going on
processing Erastus instead of one you'd
probably take it you know unless the
processor we're better spent better
spend doing something else it's probably
good time to end thank you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>