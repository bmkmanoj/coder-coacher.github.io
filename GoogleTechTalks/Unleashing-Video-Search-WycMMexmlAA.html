<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Unleashing Video Search | Coder Coacher - Coaching Coders</title><meta content="Unleashing Video Search - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Unleashing Video Search</b></h2><h5 class="post__date">2007-10-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WycMMexmlAA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my pleasure to introduce John Smith from
the IBM Watson Research Center he's a
Columbia PhD who's been working on video
indexing retrieval sorting he's a lead
author of the mpeg-7 standard for video
metadata markup in 2003 he won the
multimedia prize for best paper over the
previous four years from the I Triple E
he's an i triple e fellow he's here
today talking to us because he's giving
it all day presentation tomorrow was in
an all day meeting tomorrow with the
other night rippley committee currently
leads IBM's multimedia analysis and
retrieval research area and he's going
to talk to us today about how he's been
how we've been devising technology and
using them to improve video search so
I'm sure it'll be very helpful and
interesting for us thanks John Thank You
Geoffrey for the introduction and it's
really my pleasure to be here this is my
first time to google and and I've been
here all of about an hour and I must say
it's pretty impressive and when I first
was driving up and I saw all of these
bicycles and someone I thought that's
really not whole I you know I wonder why
they're needed until I need to get a
parking place and then I realized what
the bicycles were really for but it's
really my pleasure to be here so today I
want to talk about some of the work that
we've been doing and that has been going
on in the research community around
video search and so I'll talk about some
of the challenges with video search
right now and there are many many ways
in which video search can be improved
and some of some of the key ideas have
to do with trying to get a better
understanding of the contents of video
modeling the semantics and so on so I
want to talk about some of the work that
we've been doing that we published
recently at some conferences and and so
on and I want to talk about some of the
community efforts that are also going on
now around video search having to do
with public evaluations
and efforts to create ontology for
multimedia as well and i want to show
how this connects also to efforts like
the mpeg-7 standard which is really
quite different from from the previous
MPEG standards and picks heaven is all
about how do you describe multimedia
content in a standard and in this case
XML representation I have some demos and
so on that i'm going to show throughout
i think if you have any questions at any
point though please feel free to to ask
so let me just start a little bit up
front to motivate the problem although
i'm sure many of us realize that video
is rapidly becoming an important medium
of content for people to interact with
its introducing many technical
challenges though for IT the relevance
is seen in a lot of different industries
it's not just media and entertainment
but traditional enterprises are needing
to deal with collaboration assets and
conferencing and so on the challenge
with video is that it's not naturally
self-describing whereas with text
documents and so on you have something
to go on to build an index in search
video itself the pixels don't
necessarily tell the whole story you
need metadata to to effectively use it
and clearly not enough metadata is being
generated the expectations are clearly
increasing around video this is a chart
that was in Wired magazine not too long
ago in in June and you know the whole
idea is that well video isn't as easy to
search as text nobody has really figured
out the the best solution yet and you
know who will be the next Google for
video search maybe it will be Google but
to date there are different approaches
that are being taken to make video
searchable some of them are typify din
in search engines like blinks which rely
solely on the speech portion of the
video content they try to do some very
clever things to find keywords and in
terms and so on from the speech but you
know even with all of that smarts speech
is not a very good descriptor of what's
in the video and it certainly doesn't
tell you what's being shown on the
screen the completely different type of
approach has to do with trying to
leverage of information and web pages so
scraping text and so on and you know
there are search engines like search
video com this is a a a OLS true vo
search engine which you know does just
that mmm but that also has some
limitations the really interesting
approach and you know what I'll talk
about today is trying to leverage
computers to to better understand the
content visually and and in try to
improve search that way and we're wired
characterized this is still being a tech
that's stuck in in the lab but I think
that it's also poised to to make a real
impact so coming back to this idea about
metadata all video search relies on
metadata in one form or another well
whether it comes from being manually
authored or from speech transcripts or
scraped or so on but today's metadata is
not good enough there are a number of
dimensions in which it falls short
there's not enough of it when it's there
it's inadequate often the tags and so on
that get created describe whole item
levels they don't go in and say well
what's happening in individual scenes
within a video which might of course be
interesting for users and for for other
purposes it's not visual there's a lot
of ambiguity because taxonomy zar not
standardized folksonomies are
interesting but they create problems of
their own standards like mpeg-7 can give
us a way to represent controlled
vocabularies and so on in a standardized
way so the mechanisms are there they
just need to be put in in more wide
stream practice so they're a bunch of
issues where metadata is falling short
and there are different needs depending
where you are in the spectrum with
digital objects themselves of course
they're always going to be some area of
content which is your really popular
content these can be your hit TV shows
and so on that there will be a role for
manual professional cataloging of that
content to create episode guides and put
transcripts out and in all of those
kinds of things and this is indeed
happening today interestingly at the far
other end of the spectrum here so this
is a plot of let's say the popularity of
digital items consumer content and with
sites like Flickr and so on are
leveraging the digital masses to do work
to create tags and and so on but these
are really just extremes of the overall
spectrum of the digital objects that are
out there this is a tail which is very
long there are deep archives which
aren't really being exploited there
there's raw footage related to what is
being produced think independence and
and so on so it's this region here where
perhaps there's the most bang for the
buck if we can leverage the computers to
do work to make this content searchable
the quality may be less than what could
happen for the professionally catalog
stuff but that may be okay if the only
other option is to just not exploit it
at all okay so a little bit more on
where video search is not satisfying
there are studies one from emarketer not
too long ago which characterized some
dimensions where video search
is is not working well and you know
again typical video search now relies on
some simple ways of capturing metadata
that may or may not really be reliable
later for for searching if you look at
some of the search engines that are out
there and in this case there's a query
on each of them for just the just
the term basketball and we can see the
results coming back on google video and
Brightcove and blinks and AOL well
they're all completely different results
and blinks for example returns 214,000
matches and there's really little
ability for the user to refine or say
well no I want a player interview or I
want a game scene or or something like
that and the key dimensions here to
think about with with video search in
this way is the depth at which the user
can do a search and the computer can
find matches so how how descriptive can
user be in there in the query to get
matches for their search and breath a
lot of the search engines today rely on
partner content that populates their
silos so they search narrowly you know
with within just a small set of the
overall video resources that are out
there so there's really no one engine
which has gotten depth and breadth yet
yet mastered so I'm going to move a
little bit now into you know some of the
technical approaches that are possible
for this kind of problem but I want to
put it into a little bit of perspective
of you know how research disciplines
have evolved you know going back a
little bit of wave some of the earliest
work on image content-based retrieval
happened in the early mid 1990s actually
IBM was one of the pine
years in this area with a system called
cubic which stands for query by image
content and it was all about extracting
descriptors of photographs that could
then allow queries where the user could
say well give me more photographs that
look like this one and it would find
things that have similar color or
texture and certainly very interesting
technology to play with and even today
you know we're still playing with things
like that problem is users don't
necessarily want to search that way it
might be a nice add-on in a search
engine but other mechanisms are needed
to really get that going but you know
there's certainly a lot of follow-on
work around that which is to try to make
the image matching better by
characterizing spatial regions and
matching of images and smart ways based
on layouts and orientations and all of
those kinds of things and you know that
certainly you know generated a lot of a
lot of a lot of neat ideas and you know
once people started to be able to handle
digital video more routinely on
computers you know then certainly
attention was turned to well okay how
could we you know push also on video and
you know so then the work there got
focused on things like automatically
detecting shot breaks in a video and
summarizing long video so let's say you
take an hour video and you create a five
minute summary of the highlights those
kinds of problems now indeed these are
nice building blocks to to have but i
think the real interesting work is is
happening recently which is you know to
try to take the ability of the computer
to analyze features to process and make
sense out of video as a temporal medium
to extract semantics so in other words
for the computer going back to earlier
scenario for the computer to learn what
does a basketball game or scene look
like
how is that different from a baseball
scene or another kind of sport or you
know or any other kind of video that's
out there these are things a computer
can learn through through training so
we've been doing work in this this area
at IBM Research we have a system it's
you know it's been out on our IBM alpha
work site for download you know for more
than a year now and the idea there is
you know we create this computer based
machine learning approach you know by
which the system is taught what
different categories of content look
like and then it has the ability to go
out hunt that down automatically tag and
score those kinds of things and so I'm
going to go through some demos in a
little bit but let's let's look a little
bit more at what what's behind it so the
idea is with you know with some kind of
input video to try to leverage the
computer where you know it can really it
can help a lot in analyzing and you know
as I described the ability of the
computer to attract features is you know
long-established going back to that
early work on the image content-based
retrieval mpeg-7 has even standardized
descriptors for things like color and
color histograms and edge histograms and
texture descriptors there's already a
very good foundation of things there the
the the next challenge is and you know
where we are working is to go one layer
further which is to create models of
semantic content that could use this
information so when we have visual
information only you know we have a path
there which is to analyze the visual
features build models on those when
there is text information like speech
that can also be useful but typically
you need to consider modes when you
don't have any text but you know
certainly that text information can be
leveraged in models as well and the idea
is for a video stream to get some kind
of tag stream generated by the computer
that goes along with it and typically
there are scores generated for these
tags which are some indication of the
confidence level that the computer has
when it assigns that tag and they can be
very useful later for searching when you
want to rank a result or fusing together
and so on so behind this approach then
since it's a very much a data-driven
approach where models get created is is
the need to build out taxonomy that are
illustrated with example multimedia
content and this is just an example of
you know one of the tools that we're
working on but I think it helps convey
the the concept which is you know four
different categories of in a taxonomy to
create a model all you need to do is
load in example images for that category
and it can be in some cases as few as 40
examples in some cases it can be
thousands or even tens of thousands of
examples if it's important to get you
know a really full description of that
category or if it's just a very common
category in some cases it can be even
easy to get that many example of
examples so this is really the type of
starting point that's Bette's that's
taken to create the the models so I'm
going to go into now four areas of
research to make this work and their
recent publications we have on these and
you know I'd be happy to give that
information if people are interested to
to follow up but we're working broadly
on this base which is you know how to
model how to build the models for video
content and that can include not just
image scenes on themselves but even
temporal information like is the scene
showing an airplane that's taking off or
one that's landing so you need more than
simply an airplane detector you need to
know something about the temporal
information
we've been working on also combining
supervised and unsupervised
classification together so for example
computer can not only run models against
features but it can do things like
clustering and so on those features as
well and there are some interesting
things on how do you fuse the ability to
tag along with the ability to cluster
and then these these kinds of
information generated here then has a
role in search and a lot of interesting
things so how do you put it all together
in the search engine which may have
other modalities of search that can be
supported based on speech and so on so
we're not saying do away with speech
what we're saying is let's enhance it
and and do a better job as a result okay
so I'm going to start on the first
problem there which you know is in
building the models there are always
going to be issues of how do you find
trade-offs and and how can you make
classification really fast or scale to
thousands of or even hundreds of
thousands of classifiers and I'm going
to talk about something we're doing on
this dimension here which is so one of
one of the techniques that can be very
useful for building the classifiers to
use something called support vector
machines it's we can just think of it as
producing some kind of decision boundary
between your positive examples and the
negative examples for your for your
classifier and it's a very powerful type
of machine learning tool which is good
for this this space but there are there
always issues with support vector
machines in your model files can be very
big needing to scale and how do you how
do you do that one of the techniques
that we recently published at the kdd
conference just this summer is based on
something called random subspace bagging
and basically we can think of it
so I mentioned we have features that get
extracted from your from your content so
we can think of those features as being
composed in two dimensions so let's say
we take our color descriptor and our
texture and we just stack them
side-by-side and we get really a high
dimensional vector that goes this way
and then on this dimension are our
training examples here they're called
training shots because these came from
video but you know think of it is so
this row here represents one example
video shop and these are all of its
dimensions of its feature descriptors so
typically when you're creating a
classifier using something like support
vector machines you use this whole data
space so in other words you put in all
of your features you put in all of your
training you say here to the positive
here the negative and then you you learn
from that what what we found is through
this concept called a random subspace
bag which is you do a random sampling of
your feature dimensions in your training
examples so in other words you create
just an extract like this and then you
learn a model based on that and then you
repeat and you you what you do is you
create an ensemble of these unit models
as we call them and then we fuse these
together we get a lot of nice properties
the these you know models are faster to
learn there faster to run against your
video content later on the fusion is a
very simple fusion because it's just
averaging in other words by being random
each unit model is no more likely to be
good or bad then than any other so you
don't have to worry about learning at
the fusion step to say I need to weigh
texture more than color and those kinds
of things here it's everything just just
comes together very well we did this
mainly because we are motivated to scale
to large number of class
farz but when we ran this kind of
learning against real datasets we've we
discovered that we can not only get the
equal performance from learning over the
original space but we can even got
improvements in the overall
classification accuracy and you know
this has something to do with you know
maybe being more robust to noise or
something when we're doing this kind of
sampling but we're still maintaining
very good performance in terms of the
accuracy of those models by just using
these extracts the real important result
though is the efficiency gain by just
using these random subspace bags and we
can reduce our training time
significantly so you know up to close to
a 60 time speed up actually no I'm sorry
this is the reduction in training time
so you know typically on the order of 62
190 8 minutes to just 100 seconds to
learn a full set of models from a
training corpus and this is about 40
models that were were learned here and
also on consumer video and and also
again in later in the testing time so
testing is when you're running new video
through the system later and it's using
those models to classify the the
contents and I'll say a little bit more
about the track video data corpus later
on but that's you know one of the
standard test sets that that we've been
working against one just one more slide
on sort of the efficiency and scale
dimension you can sometimes learn your
models ahead of time like I just you
know showed if you know that it's really
important to have a classifier for
basketball scenes then you do the work
upfront you create that classifier it's
ready to go maybe you tweak it and
improve it over time but you know it's
it's done ahead of time in some cases
maybe a classifier
needs to be created on the fly maybe you
know there's a new information need or
user has something in particular mind
you know one example was fine scenes of
a demonstration or protest now maybe you
can anticipate that people will want to
look for that thing later so we've
wanted to support well how could you
know how could you rapidly put together
a classifier which has few examples so
it becomes an imbalance learning problem
in that you just have a small number of
positive examples it could be five it
could be as many as 10 but in the
relative scheme of things that's a small
number of examples according to our
number of dimensions in them in the
feature space so we've worked on the
technique and this was published in the
conference on multimedia this summer
which leverages unsupervised
classification along with the modeling
to say that well the computer can figure
out where the pockets of similarly
appearing visual content appears and
what we do is we take our examples and
we just map them to those pockets and
when we get enough hits we just say well
let's just use that centroid to
represent our positive classes and we do
the same thing for four negatives and
then we just learn from these centroids
what we get is extremely rapid way to
create a model on the fly and this
happens to also be very efficient model
it it doesn't contain a lot of examples
a lot of information in that it only
retains the the centroids from these
clusters so that's another work we did
along this dimension sorry question
jeffrey
are selected to be as similar to a
protest but not there there is a concept
here which can be very useful okay the
question is how negative examples can be
chosen and so there's actually a concept
which is very useful here that we call
near negatives so in many cases you can
find eget ofthat have absolutely nothing
to do like you could put you know indoor
scenes here or portraits of faces or you
know something that's totally unlike
protest so it's it's important to know
what is really not an outdoor protest
scene but there's also it can be very
important to have examples which are
close to the decision boundary to help
the computer be precise where it really
matters and these negative examples
weren't meant here to necessarily be
near negatives or you know be closed but
but we found that in practice that
sometimes is very useful to go through
this iteration stage with with the user
where you create a classifier based on
your positive and negative you run that
on some other set you see what the
results are you give back the users
examples that are close to the decision
boundary and then you take that feedback
into the system this whole concept is
known as active learning you know sort
of in the in the academic community and
so on but you iterate with the computer
on rifat giving giving human judgment
input on the examples which the computer
is most uncertain about and those are
usually the ones that are closest to the
decision boundary not the ones that are
already very far away so that's you know
that that's certainly very interesting
concept to apply here okay a little bit
more on the on the cluster labeling
actually I'll show this one in in the
demo system but basically the idea is
that so if we have this ability to run
classifiers against the content
repository we get some labels we also
have the ability to automatically create
clusters
let's bring the two of these together so
in other words the computer discovers
there's a pocket of content that fits
into a cluster and there they're like
scenes that you know there's some
homogeneity to them in some feature
space so the idea is well how do you how
do you pick your labels that may or may
not be assigned individually to the
items in that cluster an aggregate in
some way to label the cluster and the
interesting thing here is you don't
necessarily want to pick just the
dominant label in the cluster the reason
is you know typically half of your
scenes or outdoors so it's not very
informative to label every cluster as
outdoors what you want to do in many
cases is have I'll have labels assigned
to your cluster which are discriminative
so it says what is it about that cluster
that makes it different most different
from all of the other clusters so this
is a work we did recently as well okay
so I think what I'll do here is I'll go
a little bit into the demo system that
we have so a lot of these things are
coming together overall in this in the
search engine including searching basing
over text including the models and so on
okay and then I'll come back to some
more details okay the first thing I will
show here so this is this is a data set
which we obtained through our work on
the NIST Trek video retrieval benchmark
so it's put out by the national
institute of standards and technology
each year it was originally provided to
them by the BBC and these are known as
be rolls to the BBC basically you know
someone goes out onto the field
somewhere put puts out a camera they
capture something from a scene and then
it just kind of gets dumped back into
the organization there's no metadata
there's no tagging there's no speech
transcription there's there's nothing
it's just raw video
so what what we've done is we've run
this video through our system and I'm
just going to show a few classifiers
here that I think really help illustrate
what I've been talking about so far in
the in the talk but we can see it's just
you know it's a wide variety of of
content each of these is a key frame
selected from some shot in the video so
there's actually you know a few second
or can be you know even longer video
clip behind each of these these
keyframes so after we run some
classifiers through it switch to this
view we can see you know how how it's
doing its work I like to show this one
because well this is a classifier well
we just called greenery anyway it was
trained it was trained on examples of
images that show outdoors greenery type
scenes and often when I showed this to
people they say we have the computers
just finding stuff that screen and it is
that's precisely the point but there's a
little bit more to it than than than
that and we can see that yeah these
scenes do have green but there's a
certain textural characteristic to them
you know the the way the edges are
distributed and orientations and all
those kinds of things so the computer is
able to to look across this these
multiple features and find well what is
making the images in that class
different from the rest so this is the
application of you know of that
classifier if I compare this to another
one which we train for nature scenes
well it's also finding stuff that's
green but you know it's finding the
matches on how this classifier was
trained and in this case there were more
landscape scenes and you know maybe it's
you know also color dominated but
there's something about the texture the
smoothness to orientation and so on but
you shouldn't get caught up on the names
we put here greenery nature and so on I
mean this is just meant to illustrate
how the system works how it can be
trained and so on basically you can
train it however you want and you can
call your classes you know whatever you
you feel is
is best match I'll show some other
examples this is one that isn't perfect
at all but it shows the the strength in
the approach that we're taking this is a
person classifier and it doesn't use
face detection it's not looking for
portraits it doesn't know about eyes and
and and so on it's just again saying
well out of the examples that were
trained for a person classifier what are
the characteristics how things appear
spatially with texture with smoothness
with color and so on that capture
facings and it's not perfect this is the
face of a dog and this is some kind of
window scene and this is a statue but
for the most part it's done a really
good job for at picking out from this
data set across the diversity of how
persons can appear in in the video clip
one more I'll show here is this one
because this shows that spatial
information is also important this is
just our blue sky classifier and you
know it's doing a pretty good job at
pulling out scenes with blue skies and
it's not just finding blue it's not just
finding the green it's where they appear
in the image spatially and and so on so
that's a really good illustration of how
it works I'm going to switch here to a
different data set mm-hmm this is a news
data set also we obtained from NIST as
part of the I'm sorry yeah question yep
Oh
all the things you mentioned in the
beginning I'm sorry I didn't hear are
you asking a reading sound okay no the
example the example I showed previously
is not using any sound or audio it's on
it's purely a visual but everything I'm
describing here on visual can be applied
to audio as well you know in other words
extract features build models on top of
those features and create classifiers
and people have done this for things
like speech versus music detection and
so on so that's also a rich area of of
work okay another question yep
the nature
the question is do we use for the phase
classifier or the others spatial
features is that what you're saying
space spatial Oh special special okay no
we we don't so in other words we're very
interested in scaling to thousands or
tens of thousands or hundreds of
thousands of classifiers and we want to
do this without requiring a PhD student
for each one because that's certainly
one way you can you can you can approach
the problem so so there's something to
be said for trying to build on a common
you know a common standard set of
features and we know these features are
are good they've they've been you know a
lot of them have already been worked
through the mpeg-7 standard body their
performance has been measured you're
always going to find maybe new ideas you
know for how to capture information from
images but it's a pretty solid
foundation that we're building on
doesn't mean we'll be able to build a
classifier for everything that's out
there I don't think so but there's still
a large portion of the semantic space
that we can reach using you know using
the set of features that are here yes
I think you train were and it requires
human interaction for each word of which
many words that somebody might type into
a search does that mean you need no huge
amounts of human interaction for this to
train all the words so so the question
is how much human interaction do you
need for words and so on maybe I can
hold this question for a minute because
just coming up in the talk I'm going to
talk about some efforts to build out the
semantic space but the idea is you don't
let you don't train a model for each
word you may train a model for some
visual category and with that category
you may associate a whole bunch of words
later so you know there are a lot of
ideas for using things like wordnet or
using statistical techniques based on
words in the relationships that will
land you on a good set of categories for
for a query so we'll come back to that
one but I do believe though there is
some part of this which is maybe it is
an endless problem to create classifiers
but you can certainly get started on you
know some corsets and I'll talk about
some of the work to to do that yep
so do you work we come to the pictures
with 550 what I'm showing here typically
uses more than 10 descriptors and it's a
total of probably 5,000 dimensions so
that you know in other words these
vectors in the end if you added them all
together we're talking in the order of
five thousand or so and and it's easy to
find information about these things so
you know if you can get copy of mpeg-7
specification you can see you know what
does it take to describe a color
histogram and edge histogram
co-occurrence texture color correlogram
all of these things are well documented
you have any features that they invented
this video's not just one frame of image
do you run it on each panel payments for
the features a new server with this is
all summer yeah the question is you know
do we what do we do to treat video as a
video as a temporal medium as opposed to
to image am so so actually I chose I
chose not to go into much detail on that
one here today so you know we we have
done some work on on temporal and we
have publications on that topic as well
you can do a lot of things you can run
your classifiers on image frames and
then look at trajectory in the semantic
space you know which can be very
interesting one you know one example of
that is you know so I mention the one
about airplane landing well you will
have more sky in the beginning and more
ground in the end I mean that kind of
thing so sometimes your image
classifiers can be a good starting point
for learning a temporal concept okay so
one more example here of search and then
I'll go back again so this is the news
data set so the difference between this
and in the previous one is this one has
a full speech transcript which is
searchable this video comes from a
number of sources
uh us Chinese Arabic and so on but
somebody has done the work to to get the
speech portion out of all of this which
may have included language translation
as well so all right so let me just do a
query here and when I don't put any
modifier in the string it's just going
to match against the speech transcript
so I've asked it to find those video
clips that have basketball in the speech
and this is typical of what you get when
you're relying on speech as for your
video search it doesn't give a good
indicator what's visually being shown so
maybe you know an anchor person is
introducing a next segment or something
like that but not very good but what we
can do and I'm going to redo the query
here so the system is suggesting
basketball classifier and here the
computer is finding which of the video
clips score highly on that classifier
and the interesting thing here is when
we relied on on the computers ability to
detect you know what is likely
basketball not only did we increase the
precision so the results are you know
very highly relevant in the top matches
we also increased the recall because
it's not obvious if you were lied on the
speech that these scenes would have
easily have come up now in some cases
there are words like NBA in there and
the Knicks and in those kinds of things
so maybe with some really smart
processing of the text you could get you
could get some of them let me show one
more example where a lot of this
information can come together for search
so I'm going to type a query here for
weapons and you know being a news corpus
you know there's often discussion on a
topic like that and again we're relying
solely on speech and the user can kind
of hunt through and say well which of
the scenes that may be relevant but you
can also feed
back to the computer so we can say well
visually you know this is showing
something that looks weapons-related
here's another is another example so
let's just feed these back to the
computer and really have a multimodal
query so now the query the system is
going to answer is give me the give me
the matches that say weapons in the
speech and are like this image and like
this image that the user has fed back
and so you can see right away pretty
quickly in this pretty large news data
set we are getting pretty specific
results for the user's query and one
more thing i'll just point out here is
so I'm not showing how the classifiers
are really coming in to support this but
one of the ways which we can we can see
is out of the results coming back the
computer is analyzing those and saying
here are the classifiers are the
categories that seem to be most relevant
for your query it's saying the scenes
look like they're military related or a
lot of them are outdoors or showing
crowds so the give the system is giving
the user the ability to say yeah well
give me just the ones that are military
that score highly on the military
category we can again feed that back to
the system so now we're looking in the
speech we're looking at matches based on
certain examples and we're taking into
account the classifier for military
which is going to help rank them again
so you can see you know how all of that
comes together in search okay so let me
back here
okay so I showed some examples of their
the search engine so you know what's
behind the scenes here we we had a
publication just a month ago at ACM
multimedia conference what the idea is
to treat video queries as something
different from an ordinary search in in
that depending where the user is in the
process it can be more than just having
a one-word term entered into a search
box but we can think of a query as
having you know some text statement of
information need but also examples
either because they started with
examples or as they're interacting with
the system so how to use all this
information together the text plus the
examples to do things which expand the
text expand the examples expand into our
our classifiers to find out you know
what is most relevant for the user to
search and then use that and so anyway I
think I'll not go into a lot more the
details there just give a result here
and I know there's a lot of cryptic
stuff on the slide this is I just
grabbed it right out of the paper the
important thing I wanted to show here is
the difference between what's on the
left and what's on the right on the left
this is the performance we got in the
retrieval system when we relied solely
on text that means text part of tech
statement for the query text being
matched on the back end when we treated
these queries as really multimodal
queries so they have a video portion
they use classifiers wear matching based
on visual content we got a significant
jump in performance so that's saying you
really should treat video as something
different to get the best results okay I
think I'll skip this one here okay so
there was a question before about you
know so how do we how do we grow the
number of classifiers and so on and this
is a really important question you know
a lot of papers and so on talk about
this idea of bridging the semantic gap
that that's what you know that's what
we're working on for video which is
going from features to semantics and you
know how to do that and how to make the
machine learning work well and and so on
but we need to make sure to not build
the bridge to nowhere in other words
just demonstrating that you can detect a
sunset well maybe neat but really is
that what the user needs in the end for
search so it's important to to develop
these semantic spaces in their number of
dimensions that that need to be
considered to to do that and this also
maps back to the ideas about you know
wanting to have broad coverage so
sometimes you don't care about your
domain of video you just simply want to
know is the scene outdoors could be in a
broadcast it could be a consumer photo
it could be user-generated content but
in other cases you may you want to go
deeply for example within news there are
a lot of you know very specific concepts
that are relevant there or within topics
in news like weather and disasters or in
sports scenes and and so on I'm going to
talk about one effort on broadcast news
in particular called LS calm and this
was this was a an effort over about a
two year period which wrapped up last
year there's actually a good paper on
this and I Triple E multimedia last
summer but it brought together
scientists end users and library science
thai people from news organizations and
through a series of worked workshops
they designed an ontology of a thousand
concepts that are that achieve a balance
of being useful to support search of
broadcast news video but also
technically feasible that you could
actually think that within a five-year
period a computer could reasonably
detect those kinds of things this is
available for download actually it's
been
now at the at Columbia University's site
it's got a bunch of things there the
ontology of a thousand concepts the
fully annotated broadcast news data set
for these concepts and anyway just some
idea of you know the kinds of
institutions and so on who have accessed
this one maybe even more have downloaded
it since I got this described but you
know the overall process was an
iterative thing so in other words you
know started with some draft of concepts
and those were annotated and we can see
some kind of breakdown you know how many
were event activity types 13% how many
were people related location related
objects scenes programs and then there
was an effort to model those see how
well they perform and feed that back if
something was hopeless and there was no
projection that it could be done then it
was removed if something wasn't useful
then it was also not considered but you
know this was an iterative process for a
couple of years one of the places that
ontology has been used is in the trek
video retrieval benchmark this is a
yearly video retrieval evaluation
organized by nist the workshop this year
is in two weeks in Gaithersburg Maryland
if anyone is interested in going to that
but what they do every year is they they
have a public evaluation they make a
large video data set available and then
they create some tasks around that like
automatic classification of contents
video search you can see a number of the
other ones here and this one has been
getting a lot more interest in
participation well there were 70 people
last year 70 participants which means
universities or research labs and so on
I don't much were the number this year
this is an extract from the Ellis
comment ology that's part of that
benchmark so the participants are given
a training data set which
goes along with these categories they
train up their system then this gives a
new video data set and the participants
have to run their classifiers against it
and give the results back to NIST
without looking at them and NIST will
then score and produce you know
retrieval plots and so on the search
task is a little bit different there
it's similar in that a data set is made
available people ingested into their
systems and then this provides a set of
retrieval tasks and they can span the
range from looking for you know specific
individuals or you know different scene
types and all of these are multi-modal
in that not only is there a statement of
information needin in the text but they
give example video clips and so on that
go along with these categories I wanted
to mention one other here actually let
me see how I'm doing on time ok I think
I have time because you may enjoy this
one this was the first year this event
was run it's called video Olympics and
we we helped organize this one we were
also a participant this was something
out of university of amsterdam
video is quickly becoming the most
important information carrier of our
times youtube and juice shows video is
no longer domain broadcast television
only video has become the medium of
choice for many people communicating via
internet despite its popularity finding
specific video footage in a large
collection is a difficult scientific
problem today we are the netherlands
institute for sound and vision the Dutch
broadcast video archives we are at the
first video Olympics the showcase of
several state of the art video retrieval
systems
the goal in video retrieval is to
satisfy a visual information for example
fun clips of flying helicopters based on
this needs a user starts and interactive
search with a video retrieval engine
to compare systems benchmarks are
designs and which research prototype
work on the same video data sets
the nice thing is a different system Sal
are four different cities to collect as
much correct results from the video data
as possible
you are at the video Olympics we aim to
make the possibilities and limitations
of these systems tangible
yeah down
the uniqueness of the video Olympics is
that we bring systems from the lab to
the audience we do so by having systems
compete simultaneously on a common
search task I took charge of u.s.
President George W Bush jr. walking all
systems solar stuff on the same video
archive within a five minute time frame
is ready and we're on the board to get
results are there
in purnia
almost
GG
it
our retrieval results are communicated
to the audience on scoreboard in real
time good results are awarded that
results are penalized it allows for
on-the-spot comparison of the
performance of different video search
engines but it also allows to evaluate
their interfaces and ease of use don't
forget to vote for the system that you
think is really the one you'd like to
use the main aim of the video Olympics
is giving the audience a good
perspective on the possibilities and
limitations of current state of the art
video retrieval systems in the spirit of
the Olympic Games what station is more
important than winning that is why all
participants go home with a golden
retriever award but of course the real
winner is the audience
so we got one of these golden retriever
awards as well is really nice only if
you noticed you know the wine mixed with
the event I think only in Europe could
you have such a thing but it was
certainly a nice touch this will be
organized again next year in Niagara
Falls so it certainly is an interesting
activity the last thing I wanted to just
talk about here is is mpeg-7 standard
this has really been designed to support
video search and other forms of
multimedia as well what it does is it
standardized is an XML schema or
basically a set of descriptors and
description schemes for multimedia
contents and it includes descriptors at
the low level things like well how would
you describe a color distribution to
higher higher level things at the
semantic level as well as other more
typical things like production metadata
titles creators authors and in those
kinds of of things what's you know
what's interesting is when we put a lot
of these together not only the the web
the coating of the resources with the
XML you know then there really are units
of transaction that can be created and
this is where MPEG 21 comes in but
anyway I won't go into to MPEG 21 really
today but around mpeg-7 you know as I
mentioned it gives the ability to to
express in a standard representation the
description of multimedia resources so I
think this is a good sampling of how
approaches can come together to support
video search I just wanted to put a
couple of links here our tool for our
system is available for trying out on
the IBM alpha work site you do need to
log on and get an ID there but
you know it's it's it's easy to download
and just a link also to to our our
website so that concludes my talk and
I'd be happy to take any questions in
the last couple of minutes thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>