<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Relevance Feedback:  Getting the Most out of Your User | Coder Coacher - Coaching Coders</title><meta content="Relevance Feedback:  Getting the Most out of Your User - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Relevance Feedback:  Getting the Most out of Your User</b></h2><h5 class="post__date">2008-03-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/d-XngnEfDUY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm really excited to be here to
visit Google and have a chance to tell
you a little bit about relevance
feedback I understand I know quite a few
of you I know a lot of people i don't
know though and i think a lot of you may
not be as familiar with the field of
information retrieval so i want it tell
you a little bit about that but also
tell you about some of the work I've
been doing and you but all this work is
really about relevance feedback and what
relevance feedback has been used for
traditionally is to get to know your
user a little bit better it's been used
in single session so just for one
particular query but also over the long
term so those are the things that I want
to tell you about today David gave a
really great introduction I think but I
also wanted to just tell you a little
bit about my background so I am in the
school of information and library
science and some of you may say wow
that's kind of weird but it is the case
that people who study I our problems
historically have been either in
computer science schools or in
information and library science schools
and the reason for that of course is you
can imagine that libraries had lots and
lots of information a long time ago that
needed to be organized and so they sort
of lint themselves as nice institutions
places that had problems retrieval
problems to begin with so that's a
little bit about my background I also
have studied a lot in psychology and
cognitive science and so I sort of have
a behavioral science perspective on
things as well I mentioned the before a
little bit of what I'm going to tell you
about today what I really want to focus
mostly on is again sort of giving you an
overview of some of the research has
been done in I are in relevance feedback
and I'm not going to have tons of
references in this PowerPoint
presentations just to keep the clutter
down but I will go back and add some
references to this presentation and I'm
assuming this presentation will probably
be available to you so if you did want
to go back and read a little bit more
about any of the stuff that I'm going to
tell you about today you'll be able to
to use this as a reference so that's one
of my big goals the other thing I really
wanted to to tell you a little bit more
about is the use of it was called
implicit feedback and so I know a lot of
a lot of a lot of you probably are
dealing with log data
looking at signals you get in log data
for instance people's click-through
behaviors and things like that so I
wanted to tell you a little bit about
some research that I did that looked at
that from sort of the other side of the
fence that is from the user side of the
fence so that you can get a little bit
of an idea about how complicated these
signals actually are and sort of get a
little bit more information about the
context in which these signals are ridge
of originating to set the stage here I
wanted to make a few distinctions
between I are classic ir and web I are
so if you know it if you've been in the
field of classic ir but what I mean by
that is sort of this text retrieval
conference model of I are the more
academic kind of version of IR where you
know we have test collections that often
consist of newspaper documents and we're
doing experiments on those kinds of
things versus web ir which is what
probably most of you are familiar with
where you have lots of other kinds of
information you're dealing with the
scenario is a little bit different so
first a lot of what I'm saying is work
that I've done more in the classic I our
area so think about that when I tell you
this stuff not all of it is directly
applicable to the web just because it's
a different situation but I think some
of it probably might be useful for you
and at least useful for you thinking
about ways you can adapt it to the web
environment the environment that you're
working in classic I our users typically
have been of course you know library
patrons when I our first started the big
task or user model was a person who was
looking for documents about a particular
topic more recall based topic more
recall based searching where you want to
find a whole bunch of references about a
particular topic not something like a
high-precision task which I think a lot
of you probably deal with and a lot of
web users deal with where you're looking
for one small number of pieces of
information one correct answer
navigation tasks those kinds of things
so traditionally people in IR this
classic ir have studied different kind
of task but of course with web ir and
with web searching in general people are
starting to work on more and more
different kinds of tasks not just the
sort of classic task where it's very
exhaustive kinds of searching
a lot of stuff that i'll tell you about
sort of looks at both areas but again
just think about these two different
kinds of users and these two different
kinds of task that exists so relevance
feedback is actually kind of exciting
because I think and this is sort of me
making a statement here but I'm pretty
sure I ran this by David and he says
yeah I think that's true but i think
that relevance feedback actually was the
first interactive I our technique ever
systems and I are used to be ran in
batch mode and even the librarian who
would be doing searching on behalf of
some patron would actually you know
formulate the query for the patron
submit the queries in batch mode because
these are these old systems where you
time shared systems systems where you
had to pay a lot of money to do
retrieval and things like that so there
was no interaction between the user and
the actual information or the system it
was all mediated by a librarian then
over time of course when more and more
people started using the system and even
librarians researchers said hey let's
try to get some more information from
people when they're searching instead of
just looking at these queries let's
actually try to engage in some kind of
interaction some sort of feedback loop
where we get some information from
people have a chance to elicit some
information from people so I think that
relevance feedback is pretty safe to say
is probably the first interactive
technique and I arts what made I are
interactive a lot of people today in
fact even will argue that relevance
feedback is not really good interaction
that you know because now we think about
interaction we think of a whole lot of
other different kinds of things but I
think at its core it probably was the
first there I mentioned before it
originally was developed to help people
arrive at the ideal query so again we're
thinking about single sessions where
people are sitting down and trying to
find information about something and the
idea was that this this technique was
going to help them refine their query
not not thinking about profiling long
term stuff just as short-term stuff the
nice thing about it is that it operates
on recognition instead of recall so
there's a lot of work and I are that
says you know people often the
don't quite know what they're looking
for otherwise they wouldn't be searching
right if you already know it then why
bother to search for it there are of
course some exceptions to that but in
general the idea of people trying to
formulate a query was really odd because
how are you supposed to reduce what you
don't know into something that's
supposed to be understandable by the
system and so the idea of relevance
feedback was that well people may not be
would actually articulate what they want
to know or what they don't know but they
may be good at recognizing it and we
know that this is sort of a principle
that works in everyday life we can
recognize when something is good but we
may not necessarily be able to recall it
from memory so it's a good thing that in
that it works this way from the users
perspective now i want to i guess stop
here for a second to let you know that
if you want to interrupt me and ask me
questions that's fine doesn't bother me
so if you feel like asking you can ask
but here are the two types of relevance
feedback that i want to tell you about
these are the two major types that
people have looked at and researched
explicit feedback which is something
that the user actively engages in it's
sort of an activity on in and up to
itself and the other kind is implicit
feedback and this is stuff that i think
is more popular these days where you're
actually trying to mine signals from the
person's natural interactions with the
system so the users not doing anything
extra the user doesn't even know they're
engaging in feedback it's just something
that's there as part of their normal
behavior so that that's something that
I'll tell you it sort of will sort of
dive into that at the end and also sort
of let you know of some problems I think
with with using that or some challenges
challenges I think is a better way to
say it so when you think about explicit
relevance feedback you can think about
it in a couple of different ways but the
major way that it's been used as term
relevance feedback so term relevance
feedback is for instance a system
suggesting some terms to a person who
says hey maybe you want to add some of
these terms to your query so sort of the
most basic way to think about it but you
can also think about it happening at the
document level
here is where a user says I like that
document and you can imagine each one of
these kind of techniques has their own
sets of problems with respect to well
how do we use the information if the
person is making a judgment at the
document level how do we decide what
terms to show people so they all have
their they have their own unique
problems and we'll look at some of those
I want to just show you a couple of
different examples because interestingly
feedback is available now everywhere on
systems most systems most services
cannot resist be listing just a little
bit of feedback from people about
something so this is what's termed this
is term relevance feedback and this is
just an example of something from a
study that that we did and this is just
to show you sort of what happens with
typical term relevance feedback you know
persons like clicking and saying add
these terms to my query this is some
other kinds of feedback I just thought
that those were funny because you know
you can rate items and recommender
systems and things like that so that's
also a kind of feedback that's available
let's see here's another kind of
feedback example this was from another
study that we did or actually 1 i'll
tell you about in just a second but here
you can have terms and then you can have
little example sentences from where
these terms came from so this is another
way that you could get feedback from
people and then one more shot which is a
very old system that I did some research
on but it shows a couple of other ways
of getting feedback this is very busy I
know but the the reason i want to show
you this is that you can see here in a
list of search results you have good and
bad documents so this actually has bad
negative feedback which i'll say
something about a little later which is
very hard and then here when you click
on good terms from that document are
extracted and then populated in this
window and then the person could
actually take one of these terms and add
it to their query if they wanted same
thing with negative these are terms
extracted from this document that's been
marked negative and a person could take
these terms and add them to the query in
sort of a negative way saying that i
don't want this term it's not a good
term to use so just did not have a good
sorry i didn't have a good back button
for that but what people have found a
system centered research is that when i
say system centered research I mean
research that you know knows that
there's a user out there somewhere but
doesn't actually study the person making
the relevance judgments just studies the
the operations of producing the terms
using the terms and things like that
system centered research has found that
relevance feedback works in fact
relevance feedback is probably one of
the successes in classic ir and most
people believe in these classic I our
model that more is better more terms are
better now in web searching doesn't
always work out that way
but in these kind in this kind of
research that was the general idea and
it was because of the way the systems
worked you know a longer query would
lead to usually higher precision and in
general better results so when you do
relevance feedback you're making the
query longer that usually worked out
well there's something called pseudo
relevance feedback as well so this is
relevance feedback where you as a system
developer assume that the top 10
documents that you've returned for a
user are relevant you just assume right
away they're relevant you take terms
from those documents you use those to
expand the query and then you give the
user some results so the user actually
never sees the first iteration of
results because as a developer you take
that as data as feedback to build a
bigger query than to return some new
results so there's this idea of pseudo
relevance feedback as well which people
generally believe works pretty well so
the user centered research so this is a
research that actually studies people
trying to do these kind of things use
these these term you'll provide term
relevance feedback and stuff like that
it sort of had mixed results and I'll
sort of dive in now into some of the
system issues and user issues and
particular issues related to whether I
know what some of you are thinking which
is what people actually do that who's
going to sit around and you know pick
terms to add to their queries is this
something that's even feasible to do so
we'll look at that in just a second but
here are some of the big issues if
you're going to try to use relevance
feedback that people have found and sort
of studied that that are really sort of
system I'll call these system problems
or system issues first is actually how
do you use this feedback that people
provide to you what do you do there's an
infinite number of ways of using this
stuff and I think down at some point I
talk about this is a sort of lots of
parameters that's the big issue here so
how do you use this feedback that people
provide do you do waiting do you read do
you say the terms you know do you add
the terms to the query do you go out and
get new terms
if your people are providing relevance
feedback at the document level how do
you use that how do you extract
information from those documents so how
do you use this another big issue is
actually populating if you're
interacting with people populating those
interfaces I showed you so I showed you
a bunch of terms well where do those
terms come from how do you get those
terms so that's another big problem and
this is one reason why again the whole
area of relevance feedback is actually
really hard because it's there's a whole
bunch of stuff that's happening and a
whole bunch of stuff that has to be
going just right in order for you to for
instance even test whether people are
going to want to use a relevance
feedback interface you got to get all
this stuff right if the terms you
suggest to people are awful then no no
one's going to pick any of them so you
know there's a lot of issues with
respect to that I listed some of the
different parameters you have to deal
with if you're trying to use relevance
feedback or implement it so there's
rankings so again how do you just rear
ank documents based on feedback do you
show people a document that they've
already seen that a person has already
said is relevant howdy what do you do
with that waiting again how do you wait
the terms how do you wait the terms that
the user adds to their query
automatically how do you wait the terms
that you might extract from documents as
a developer to add to the query oh
there's issues with how many documents
do you use for feedback so if you're
doing pseudo relevance feedback do you
take the top 10 documents and then
extract the top 20 terms from those or
do you take the top 20 documents and
extract the top five terms from those so
there's all these different parameters
that people have spent a lot of time
trying to fine-tune and you know I wish
I could tell you that there was some
really great rule but the research seems
to suggest that there's a lot of sort of
heuristic development going on so just
trying different adjustments on the
parameters and seeing what happens it's
also the case that the parameter values
are dependent on the document corpus so
actually what the documents look like to
begin with so again lots of stuff you
have to deal with if you want to do this
and in practice and then you know how
much feedback do you provide to people
or how much to pay feedback do you list
it from people and then how do you
combine these new representations with
the old representations hmmm fairly
complicated to figure out what to do but
are there any general rules about what
does and does not work any general
principles that have come from all the
research that says this generally does
this was not always consider doing this
um I'm going to say although i can't i
can't think of any rule right off the
bat except that people generally have
thought in classic ir that relevance
feedback works period pretty much any
kind of it now if you add a bunch of
terrible terms of course that's not
going to work but people generally
believe that it's a good thing to do
people generally believe that it's
longer queries are going to increase
retrieval and so that was sort of the
target goal in this classic ir was to
try to get a bigger query from people
but in terms of these parameter settings
I none really pop out of my head they
all seem to be like here's what you need
to use in this particular situation here
is what you can use in this other
situation and I will say that I you know
I don't know everybody in here but
certainly David who introduced me is
actually a an expert in relevance
feedback and so he may be able to sort
of you know give you some more rules
about it than I'm able to right off the
bat but I think in general that a lot of
it has to do with particular situations
context collections search task and
things like that would you say that's
fair especially I think some of the
general in general you don't need a lot
of relevance feedback in order to make a
difference so even knowing one relevant
document or two can make a big
difference to you to the search to the
cord to the wind queries pushed I think
as well on the pseudo relevance feedback
again he can be very damaging to search
so when it works well it works really
well but it can it can be very damaging
so again the general rule would be not
to go too far down when you're doing a
pseudo relevance feedback maybe feedback
one two three a small number of
documents as you
to be relevant of course you know in in
web retrieval we're in some of the
ranking we're looking at an anti
situation you have to be very careful
about how many more terms you add in so
that's that's another issue that comes
about yeah that's really worth pointing
out another distinction between a lot of
this research is developed in a
probabilistic framework it's not using
these hard classic and so that changes
the the sort of that changes I think an
ability to say this is a rule that you
might try it's more a matter of here are
some things that have been done and this
might give you some ideas but probably
you're going to have to do some
adaptation just because your situation
is is a lot different than when a lot of
these studies were done and different
than the collections and the topics and
things mm-hmm what would you consider
how you measure it because are you going
to say something about that because I
mean that the whole thing is that you're
you're saying relevance feedback little
better but how do you measure that and
visit does it change depending on
whether you've got somebody who's an
expert in this field versus somebody
who's um so I'm not going to so in these
kind of models I talked we carry a
little bit about this about how sort of
evaluation is done in traditional ir
which is usually with a closed
collection that has a corpus typically
newswire some topics typically fifty in
number and then some relevance judgments
so people have made some relevance
judgments and so that's probably similar
maybe to some of your evaluation
scenarios except some small differences
and so typically what's used as a
measure of better especially in system
centered research is our measures like
average precision discounted cumulative
gain is another one I'm not I'm not
going to get into I'll be glad to talk
in fact I love to have a conversation
about evaluation measures but i'm not
going to present those here because it's
a sort of a different different thing
but that's typically how
these things are evaluated and that's
actually a problem here is that it's
really hard to evaluate relevance
feedback and it benefits of it because
things change it's an iterative process
and so you have time one where there's
no relevance feedback you have those
search results then you have time to
where you've got some relevance feedback
and so how do you actually compare the
impact of those results or impact of
that relevance feedback how do you
isolate the effects of just adding those
new terms that you added for instance
and then the other issue is with
relevance feedback of course you don't
want to give people the same document
over and over and over again if a person
has said this document is relevant well
that's great but they don't want to keep
seeing it over and over and over again
they've already seen it and if you're
trying to do evaluation do you just keep
counting it over and over and over again
or do you try to you know include some
kind of measure of diversity or how much
the thing has to list has changed so
it's actually a really hard problem here
is to evaluate and the actual impact of
this there's a really nice model by Ian
Campbell and Keith and rice Berg and
they did some work on this extensive
relevance and so in their model that
they've there's a there's even an
interface therefore for browsing in this
way but in their evaluation scenario
there is actually a decay function so
the documents that a person marks
relevant at a particular time a snapshot
those documents are considered to be the
most relevant at that particular point
if a person had marked something
relevant at the beginning of the
interaction and it shows up again later
at this particular point maybe later in
the list then those don't get quite as
much value or their value is diminished
a little bit because persons already
seen them and then the other idea is
that a person's need is changing as they
keep seeing new documents they're
learning about what their interests are
and they're changing what really is
relevant so there's that kind of
function in there as well so the other
big issue here is that the effectiveness
of this depends on the initial query so
if the person's initial query is crap
then if you do pseudo relevance feedback
on results returned from that query it's
unlikely that that
really going to help right so a lot of
this is dependent on how good that first
query is if you're using it to generate
terms to retrieve documents and so
that's a big a big issue and then
actually David did some work and talked
about this idea of query drift which as
you go through these iterations of
refining refining refining there's this
danger that the person's topic is
actually really going to start to change
the query is going to change not from
the person's point of view but just
because you're adding more and more
terms to it and so it suddenly becomes a
totally different thing than what the
person initially started with so that's
another problem these are all again
problems or challenges or things to
think about and then I just alluded to
this here as well is that the person's
need is changing over time as a person
interacts finds more documents what they
want is changing and so it's really hard
to incorporate that into the query in
some kind of way so here are some user
issues and again this is like
summarizing a bunch of research and
trying to present you all with some
bottom line stuff although sometimes
it's hard to do this but when a hard
part room is that terms especially on
these just term interfaces where it's
like here's a bunch of terms these
things are not presented in context so a
person really has no idea what this term
means you're just presented with a bunch
of terms you have no idea if this term
is being used in the way that you think
it's being used or that you're trying to
use it so a lot of times you know
research has found that well this may be
this may be a problem for people people
understanding and not not understanding
what these terms are that they're
picking and what they mean in these
particular different context um the
quality of terms also can impact the
interaction you have with the user so if
you actually want to engage a person in
relevance feedback and you start off and
you generate a bunch of awful terms the
person is going to look at that and make
some judgment about the quality of that
feature and so it's really this
dangerous game you're playing because
you can you know turn people off right
away and they'll say well I don't trust
that I don't recognize any of those
terms I have no idea what that system
doing so you can actually like those
terms can actually impact a person's
willingness to do this to engage in this
kind of activity the other two things
here are kind of related people have
said well people don't do relevance
feedback or people don't want to
participate in relevance feedback
because it's just too cognitively
demanding like people are already
engaged in the searching tasks they've
got all kind of other things going on
they don't have time to sit there and
you know add terms to the documents the
other issue here has to do with up well
so this has been an explanation for why
people may not engage in this so a
person may do a study and find that
nobody's engaging in their relative
their experimental relevance feedback
feature and they may explain this by
saying well people are using this
experimental I our system and so they
just aren't able to concentrate on
finding and marking relevant documents
and you know doing this query refinement
process all at the same time so that's
more of an explanation for why and again
a lot of these are explanations after
the fact after people have done a
research about why people may or may not
be participating in relevance feedback
so again depending on what the person is
doing they just may not have time not
time is it I'm busy I need to hurry up
and finish this but they might not have
the cognitive resources to engage in
something else they may just be able to
only focus those things on the task at
hand so does that answer your question
ok this idea of control has to do with
how much control you give the person so
you let the person actually pick the
terms do you show the person how the
terms are generated you know how much do
you communicate to the user about what's
going on and about how their terms are
going to be used versus just doing
things magically right so that's that's
this this issue that people have
addressed in these kind of studies and
again you know it's it's unclear some
people have found that you know there's
sort of a optimal mix of control
magic that people want other people have
said that you know no people people just
want the system to do good things for
them so the next one I guess is my
always my favorite one which is that you
know people are just too lazy to provide
feedback so why bother especially in web
searching why bother to provide feedback
and refine your query when you can you
know type in a query check out the
results you might see some good words
refine your query like it's just too
burdensome to actually do this so this
is a reason that people again sort of a
barrier if you wanted to use relevance
feedback something you need to think
about that you know this is something
that might be happening the other thing
here is that maybe people just aren't
able to pick the best term so there's
been a really nice study by Ian Ruffin
which said you know okay if people could
pick the optimal terms then it would
help a lot but people are not able to
look at terms a set of terms and say
that's going to be a good term and the
idea here is that well machine actually
has access to lots of statistical
information about term co-occurrence
term frequency what search terms people
have used in the past and things like
that one individual human being is not
going to have access to that so this has
to do with whether a person actually is
in a position to determine what's going
to be the most effective terms to add to
their query given a list
statistics and you only present the ones
that the Machine believes are useful
then whatever the user picks has got to
be better so I mean you don't present
things which you believe are going to be
going to be bad yes or whatever
statistics you have yeah so what a lot
of this research is found is that even
if you if you took a term and you
measured its goodness based on some
statistic that the machine had available
to it to identify as goodness even then
people will not pick these terms so I
understand your point that okay let's
say only present five really really
really outstanding ones and then if a
person picks one then it's going to be
presumably good from the system point of
view but these kind of studies have
looked at all the terms that have been
presented and have just found that
people are not able to go in and pick
people and systems do not agree on what
are the good terms and so that's what
this stuff is found and again if a
system word existed that could pick five
really awesome terms then why even
bother to show them to the user to begin
with like why not just use them so there
is this there is this idea that there
has to be some sort of not filtering by
the user but it's you know here's the
system presenting its evidence here's
the user using its intelligence to you
know add on to that evidence and the
studies here have have sort of question
whether the user is in a position has
the not intent well I guess has the
intelligence to be able to go in and
pick out what term is going to work best
because the person doesn't know what
documents are out there they don't know
the distribution of language in terms
and documents and things like that and
then the last thing here I think that
people have discussed a lot has to do
with the sustainability of explicit
feedback for long-term modeling so if
you do it having a system a lot of the
stuff I've been talking about his sort
of bid one off where a person has a
query and you're trying to do something
for that person in this particular
session in time but you can also think
about using relevance feedback as a way
of eliciting feedback from a person over
a long period of time in order to build
some sort of model or profile of that
person's interest so again a lot of the
examples I've been giving you up in like
in a single session where a person has
query or two queries but it doesn't have
to be used in that way it can be used
over a longer period of time and so
there's been a lot of questioning about
okay well explicit feedback even if it
does work can we expect that this is a
way that would sustain an evolving model
of a person's interest is a person going
to periodically you know pick out terms
and pick out relevant documents in
enough frequency to allow a system to
gather information that can be used a
long term way and can do it so that that
information is updated when it needs to
be updated yes so what I'm going to do
I'm looking at the time and I see I
probably did not time this best so what
I'm going to do is I'm going to jump
ahead i had an example of some research
we did in this field and explicit
feedback but i'm going to jump ahead and
tell you about implicit feedback and
tell you about a one project that i did
there because i think that's probably a
little bit more interesting so if you'll
just excuse me while i go through here
very quickly i'm not going to say again
i apologize for this but i just did not
do the timing very well you saw the
slide about negative relevance feedback
and i'll say something about that at the
end because that's actually an
outstanding challenge is actually how do
you incorporate negative feedback it's a
really hard problem because one is not
clear what people mean when they say
that I don't want any more like this so
if specially we talking about a unit
like a document what is it about that
document that the person doesn't like or
that makes it not relevant it's hard for
the system to also implement some sort
of negative function so what does that
look like and it can be really dangerous
because you might run the risk of
eliminating a lot of really good stuff
so the negative feedback stuff i think
is something that's sort of low-hanging
fruit it's a really hard problem but
certainly something that people really
haven't got a good handle on and users
in the studies that I've done they
they're they're really scared of
negative relevance feedback because
they're not sure what's going to happen
they say I don't want any more like this
they don't understand what that's going
to do and there's actually been a study
where a person looked at negative
relevance feedback as it's been
implemented in several classic retrieval
models and found that it behaves very
differently it's not predictable what
that relevance feedback will actually do
to a term or query if you include it so
there's a lot of stuff there that hasn't
been fully examined I think so to kind
of go ahead to implicit feedback this is
again using behaviors that a person
exhibits while they search as feedback
of what person's interests are and it's
really nice because if you think about a
person doing searching especially on the
web they generate lots of data and
there's a whole bunch of pages that they
view and that you know very nice thing
to think about is within that large set
of pages that any particular human being
looks at the pages that they like are in
there somewhere right so your problem is
to try to figure out which of those
pages in an entire stream are the ones
that that person likes the most and
again to use that information to
potentially create some sort of
long-term model of that person's
interest or even to do something with
retrieval in a particular instance in
time so these are some examples of
things that have been used and
investigated as implicit feedback now a
lot of the stuff i put select and bold
here because this is you know your
classic click through and this has been
investigated the most and as you may or
probably all know it's used a lot it's
pretty common practice now to use this
in lots of different web applications so
this is again looking at what a person
clicks on and using that as a signal for
relevance so click equals relevance in
some kind of way people have also looked
at view so if a person how long a person
actually views something so that should
actually save you time there how long a
documents displayed in a browser listen
so if you're talking about an audio a
scenario how long a person chooses to
listen to a particular thing people have
looked at scrolling as well as finding
so you can do find within a page
all of these things as potential sources
of evidence for what a person's
interests are and what a person knows
there's a few more here and just call
these retention behaviors and then these
two down here really are more about a
person sort of production behaviors i
will say these are things that may not
really happen is part of an interaction
with a search engine or even in a
browser but these are production
activities that if you had that
information available to you and you
were trying to design a search system
that kind of information might help you
understand again more about what that
person should should start so this I
think is why it's really important is
because it's it's all there and the
person's interests are in that in that
you know gigantic mess of page views and
you know your job is to figure out what
are the good things the other good thing
about it is again there's this idea of
whether explicit relevance feedback is
actually feasible ie will people do it
do it consistently even can they do it
and then this idea of sustainability if
you're looking at long-term modeling is
this any way to sustain a model over
time and so implicit feedback sort of
you know says well here's some good
stuff that we may be able to use it's
usually thought to be a little bit
weaker but it's available in larger
quantity and in general a lot of it's
somewhat easy to access especially if
you work for search engine company you
can see at least a little bit of what a
person is doing with respect to click
through anyway so what do we know about
it in general this may not be a surprise
to anybody but click-through is usually
a pretty good signal of a person's
interest and people have used that with
a lot of success and it's not always the
case and there's lots of caveats to this
which I'll tell you about but just as
sort of a lump average talk about this
this statement i think is fair to make
um the other thing which is not here is
that generally if you're using display
time display times are really very
oh so display time web page display time
is actually a positive skew there's a
whole bunch of stuff at the end you know
around one second two seconds five
seconds and then there's a whole is a
very long tail that describes how long
people describe a display web pages so
that's another general finding which
again is probably somewhat intuitive a
lot of studies have also found that
there's a positive relationship between
display time and relevance again there's
some caveats to that which I'll tell you
about a little while but if you look at
it overall a lot of times people have
found this relationship exists so some
of the big issues with this is that a
lot of studies that have looked at this
behavior have been based on what i
consider incomplete data so for instance
you can imagine what kind of data if you
were actually trying to look at display
time and you were only looking at
display time and you only could look at
it from a server perspective what kind
of display time data you would get if
you looked at it from a client
perspective if you're actually computing
this metric based on client you could
think about what kind of measure you
would get there to vary their different
measures so a lot of times it's
incomplete data or it's just not not
everything that the person is actually
doing in a particular moment it's just
some stuff that you can get that's
available at one particular location or
the other the other thing about it is
that it's usually general and so we know
how science works and this is really
nice thing about science is that you
know we often looking at big data sets
we take the average and we say oh here's
some summary information and this is
what people in general are doing and
this is really great but if you're
trying to personalize retrieval to
individuals basing your rules for doing
the personalization on what everybody
does seems a little off so you know the
idea here is like why not just look at
what particular individual is doing
instead of just lumping everybody
together because we know people are
different the other big problem is that
a lot of this research hasn't considered
contextual variables so things like task
and a person's familiarity with the
topic so these things have been shown to
potentially affect the rate at which
people display pages click through
behavior display time whether a person
prints are saves and so when people are
doing searching they're all you don't
really know what they're doing and a lot
of this data that you get just that a
server is out of context it's a whole
bunch of signals and again there's not
much context for what's really going on
and what that person is trying to
accomplish so that's been another big
issue and in fact I know Melanie is here
and she's studied some of this stuff as
well so it's another great resource I
think internally for you if you wanted
to look a little bit more into this idea
of task changing people's behaviors so
the study i'm going to tell you about
what i wanted to do was to look from the
person's perspective when to gather data
from that person's from the person's
client machine look at what they were
doing over a long period of time and try
to figure out if these contextual
variables like what a person was trying
to accomplish how much a person knew
about a particular topic how much a
person search for a particular topic if
these kind of things impacted the ones
ability to use behavior as signals
behavior that's out of context as
signals so what are some things to be
thinking about if you're trying to use
this behavior so the other thing I
wanted to do in the study was to try to
develop a method to study this because
it's really hard hard thing to study so
what I did in this study and again this
is going to be a real quick summary of a
very complex study that lasted a long
time and consumed many hours of my life
but there it is what I did was I did a
small study with seven people and what I
did was I studied them for about 14
weeks and when I did this study I did a
study a few years ago what I do is I
gave people laptop computers to use and
these were new laptops and so this was
sort of the way that I enticed people to
participate in this study and I said hey
if you're willing to participate in this
study for 14 weeks if you're willing to
let me log every single thing that you
do on the client so this means all email
this means all word processing all
interactions
the web every single thing that you do
for 14 weeks and if you're willing to
come in meet with me once a week and
give me some information some feedback
then you can keep this computer so that
works pretty well and if you're worried
about ethical issues i should point i
I'm actually am a person who is very
concerned another thing I could talk
about forever about research ethics I'm
actually on my University's
institutional review board so you know
there was some some working out of those
issues but anyway so it was all okay
from that perspective so as you can
imagine I did not have a you know no
drop out right here there's a term
called experimental experimental
mortality that's that that's the real
term for when people drop out of studies
I actually used that term once in a
paper and a reviewer wrote did they die
I said no but it's a real life term so
this is a diagram of the data that I
collected just so you get an idea for
every document that a person viewed this
is web document here I had some set of
behaviors that I looked at I had a whole
bunch of other behaviors but these were
the ones I picked out to look at I had
display time whether they printed or
saved it I had a relevance judgment for
every one of these documents so for some
people during this 14 week period they
made relevance judgments on like
thousands of documents they did this at
weekly intervals so it wasn't too much
work for them and then I collected what
I consider to be context so for every
document that a person viewed I had them
create customized list of task and
topics so the person told me what they
were trying to do I'm trying to find
references for a paper I'm trying to
plan travel so they actually classified
all the documents that they viewed into
these tasks categories that they created
themselves same thing with topics so
they'd say this document is about
traveling to Bermuda or this document is
about running a race in richmond for
instance and then for each one of these
things I collected some information to
characterize them so endurance and
persistence are sort of the same and
that's how long a person expects to be
interested or doing a task or be
interested in a topic so it sort of gets
you know is this something that they're
interested in for a long period of time
or is this one of these one-off things
where they just happen to be looking for
some you know they saw something on TV
and just wanted to check frequency for
task was how often a person actually
engaged in a particular task so do you
do it every day once a week once a month
and then stage so this had to do with if
it was a task that could be
characterizing this way were you almost
finished or were you just starting so I
wanted to look at how all of that stuff
actually affected the ability to go from
this to this so again is it a one-to-one
correspondence or does all of this stuff
really affect your ability to use that
for instance as a signal so this just
gives you a quick diagram of the sort of
the study what was going on the whole
time I had client log are going on I
also had a server law a proxy there
searching was going through a proxy so
that I could gather a little collection
of things that they view during the
sport teen week period the the legal
issues with that I'm not going to say
anything about because I don't know what
they are but anyway I collected these
little mini collections for every person
now the context evaluation at this day
one when people came in I gave them
their laptops they sort of told me what
they expected to be doing so I sort of
got some baseline information about that
each week of the study they would come
in and meet with me for one week and
they would update their tasks they might
say oh I'm doing this new task this week
or that task I'm finished with so they
might provide me some information like
that they also sat down and evaluated a
set of documents that they had viewed
during that week so they would rate the
document its usefulness so this is how I
gathered this kind of baseline judgments
and so this went on for 14 weeks and as
you can imagine I collected
lots of data oh well this shows you the
evaluation interface so when the person
came in the pages that they had viewed
during the week would show up here
there's a scrollable list of task and
topics a person could click on to
associate with this particular document
then a person could rate whether
something was useful from a Content
perspective or from a navigation
perspective and what that means is that
if a person said well the document help
me get somewhere not because there's
anything specific on there other than a
hyperlink that's why something would be
navigationally useful so it's a more a
matter of assistance with getting
somewhere else that's even more
important than the document itself and
then people indicated how confident they
were in their evaluations hmmm um so the
right so that's that's a that's a really
great observation for this day and time
and when I did this study five or six
years ago that would have helped me out
then but as it turns out is one thing I
wanted to say something about the
question has to do with whether this is
really usefulness and how do you
separate that from entertainment the
fact of the matter is in the end of the
study people do use the web a lot for
entertainment and again that's sort of
this issue is separating those instances
where a person is just looking because
they want to find the YouTube video that
shows somebody doing something stupid
versus you know trying to find a paper
that's going to help them right Sega our
article or something and that's that's
sort of the crux of this whole entire
study which is if you're just observing
signals you have no idea really what a
person is doing then how it makes it
really difficult to use these signals
effectively and again what that's what
needs to be done is sort of the the next
research challenges is trying to figure
out how to distinguish among these
different types of things people are
trying to do because a lot of stuff is
entertaining
it really is mm-hmm but it seems to me
that that's a differentiation between
classic ir versus web-based iron in
other words it's not even both are
useful it's it's more likely that
somebody that's looking for
entertainment will be looking for a a
particular thing versus a variety of
things as compared to someone who's
looking for research yeah so that's
again that's part of part of all of this
which is that there are these these two
we can talk about high precision or you
know entertainment task or something and
then this other kind of thing that
people are doing but you know we know
that people more and more are probably
doing these exploratory searches these
high recall searches probably are
starting to look go to the web more and
more to do these things so the issue is
in a world where we see more of a mix
then what can we do I mean right now i
agree with you yet and this is exactly
this you know web searching has changed
people's view of evaluation measures of
what their conceptions of about what
people are doing and what I r is for has
changed dramatically in the last 10 or
15 years and change research published
research you know it's you know it's had
a huge impact and so that is that's a
really that's that's part of it I was
going to also note that between the web
nice i are classified r is a problem
that i run into
because I'm a technical writer here
trying to help people find information
about stuff within Google that's much
more of a classic I are so defend this
company though is based on web I our
attitudes towards searching and so you
get it to run into that problem a lot so
baby a little bit more like Enterprise
is that no you know I mean even our I
don't know a lot of our enterprise stuff
but i think that that what we run into
is the difference between what i would
call technical information
versus web-based gathering yeah it's
hard again that's why it's really hard
to try to use and apply stuff that's
been developed in other domains and
figure out what's when can you say this
is a good rule in winking when is it not
and and also just having conversations
with people and making sure that
everybody understands what's in
everybody's mind when they're talking
about searching mm-hmm UI for asking
people the relevance of the pages they
visited right mm-hmm so a couple quick
questions the tasks and topics were
those generated by the subjects for by
you in observation or where they come
from the subjects generated them
themselves so these are our sources
their own tests yeah yeah was this
evaluation done at the moment or ex post
facto like a day later a week later and
like that this was done at weekly
intervals and so don't me know there was
people had no problem remembering what
they were doing and remembering the
pages but the distinction between
instantaneous relevance versus post hoc
relevance because they could be very
different thing yeah that's true and
that's in fact one thing that the
questions about how I think how
relevance changes over time is it could
be exactly the same article exactly the
same cast but after I'd read three other
articles on this all this and then
realized that was the best one right yes
and so I changed my subjective
evaluation with the road yes and this is
a problem with getting relevance
judgments from people and in fact again
I somehow magically out of time here I
don't know what I've done wrong but the
one thing I wanted to mention is that
you know we assume that a click equals
relevance but we make a lot of
assumptions about what relevance really
is and what you're saying is something
that people and I are just understand
about relevance that it changes over
time that it's relative you know you see
one thing and then you're going to
change the next they're not independent
judgments and all of these kind of
things and these are really hard
problems and if you're going to use
you're going to talk about something
being relevant you really need to step
back and think about what does that
actually
me because it changes why don't you give
yourself another ten minutes is
everybody I probably know the rooms not
been supplying anybody else okay sure
just carry on us if you've got another
yeah okay and again if you need to leave
and you want to ask me questions you can
I think my email is with the talk
announcements so let me just show you
just a little sample of the data that I
found and then I want to kind of jump to
the bottom line again i'll post all of
these slides so you can go back and look
through them if you want to see a little
bit more about what what i found but
this just gives you an idea of the
amount of data that I collected for each
person these this is the number of URLs
requested so just looking at a log how
many URLs I saw coming in there now some
of that stuff is like you know a style
sheet or an advertisement or stuff like
that so some of that had to be filtered
out this is the number of pages that i
actually got evaluations on so i also
had to make some decisions about what I
was going to show people because clearly
a person is not going to be able to sit
down and evaluate 15,000 documents even
in 14 weeks so this gives you an idea of
that and this gives you an idea about
how many task and topics each person
identified and what you can see from
this and the thing the important thing
is that people are very different right
and so again this idea of trying to use
what everybody does to make rules
especially for something like this where
you're doing something very personal for
somebody can be really dangerous because
everybody is different and so thinking
about people as individuals I think is a
really important thing to do let me just
skip over to this slide this is a really
high bird's-eye view I know this is hard
to look at but I wanted to show you all
of the display times here and I did
clean it a little bit at the end and cut
it off but you see most of the documents
are being displayed for really short
periods of time this actually is one
minute right here so people make really
quick decisions they do it really fast
the other thing from this picture that I
want to show you is that the
distribution of documents that were
rated as highly
useful so that's that light blue it's on
the bottom right here you can see this
actually goes all along this line are
all along this distribution these scores
actually are more variable so again if
you're thinking about trying to create a
rule like display time you really need
to think about the variability in this
data especially for things that are
useful for things that are not useful
there's not as much variability in the
display times they're a lot lot tighter
I'm going to just now skip over to show
you another slide that's important which
this is for one subject and this is this
date this time has been put on a log
base scale just to again compress but
what you see here is this is a different
the person's different task this
particular person you see that display
time varied greatly depending on what a
person was trying to do and again this
gets back to this idea of entertainment
versus something like looking for movie
reviews and schedules versus something
like shopping and things like that
reading the news I always like is up
there reading the news is usually
actually pretty tight too there's not as
much variability in that so you can see
how what a person is actually trying to
do at any moment in time affects what
this data looks like so here the sort of
the major findings from this Oh in
general the display times were low most
usefulness ratings were really high and
there wasn't much printing and saving in
this data so you know in terms of how
much use you can get out of that I don't
know and there was also no direct
relationship between display time and
usefulness and so again the take home
message here is that there's a lot of
stuff is going on that you can't see
that potentially impacts your ability to
use these signals to interpret these
signals and create rules and other sorts
of techniques that use this these
signals as implicit feedback or feedback
of what a person's interest are the
other thing I wanted to sort of say is
what this gentleman here sort of brought
up which is that relevance is sort of a
moving concept is a moving target we
make lots of simple simplifying
assumptions about what it means but it's
actually really complex and you know the
assumption that one click equals
something like relevance is really quite
naive and so there's a lot more going on
there that a person could potentially
take advantage over their researcher
could potentially examine and try to
take advantage of in terms of coming up
with a little bit more sophisticated
understanding of what relevance actually
means and what what's going into those
kind of judgments time and size of a
document because if some some documents
are small right he only takes five
seconds to finish but it's still useful
but some other documents are long yes so
you can normal the time here is not
normalized because most things were just
so short you can normalize this and some
studies have normalized this ryan white
did a study where he normalized display
time according to the length of the
document and from my memory of that it
didn't really help that much did really
add that much more information so I'm
going to stop here and again I'm going
to issue a another apology for not quite
planning things very well but I do want
to let you know that i'll post these
slides and you can go through them i'll
post references if you want to read
about any of this stuff you know a lot
of it you're not going to be able to not
going to be off the shelf in terms of
using directly in your research but
hopefully it'll give you ways of
thinking about things in a different way
maybe some pointers some things that
will help you solve some problems that
you're working on um I don't know is
this
video just email them right yeah so I'll
stop here and again I'll thank you all
and if anybody wants more question or
want to ask more questions I'm happy too
happy to answer more of them so thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>