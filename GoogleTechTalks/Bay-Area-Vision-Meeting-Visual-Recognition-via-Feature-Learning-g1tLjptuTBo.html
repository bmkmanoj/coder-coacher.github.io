<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bay Area Vision Meeting: Visual Recognition via Feature Learning | Coder Coacher - Coaching Coders</title><meta content="Bay Area Vision Meeting: Visual Recognition via Feature Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bay Area Vision Meeting: Visual Recognition via Feature Learning</b></h2><h5 class="post__date">2011-04-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/g1tLjptuTBo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi you sets up let me go ahead and
introduce him he is the head of media
analytics Department at NEC laboratories
America before joining NEC he was a
senior research scientist at siemens
corporate technology his research
interests are learning from relations
collaborative filtering multitask
learning computer vision image
classification deep learning and sparse
coding and I should note that he led the
first place team in the image net
large-scale visual recognition challenge
and with that look forward to hearing
his talk so how everyone today I'm going
to talk about image recognition you've
seen unsupervised feature learning so
you look at the title you can notice
there's a big overlap with the angels
talk because we both believe actually a
bunch of you know machine learning guys
really believe by learning from data we
can actually obtain very powerful
features for image recognition but just
one part actually I don't want to talk
is deep learning because i believe it's
hard to be as deep as enjoyed it so so
let me focus on as provides feature
learning so in the past two years I
worked with a large group of people
those are my colleagues from easy labs
and also we collaborated with professors
like Tom John Thomas home and John ft
and by the way we also have some secret
weapons which are our interns they are
very smart and productive so um what we
did I believe is pretty amazing so this
is why examples so in 2009 this is a
Pascal visual object class challenge um
I think this is probably one of the most
prestigious
benchmark in commit division for object
recognition so the task is to classify
images on to one or multiple of the 20
categories right so typically very often
one image here can be multi-label
because containing several different
category of objects right so in 2009 and
the task is okay given about a thousand
images for training to classify a test
instead of 6650 images so this is a
result and we achieved the number one
positions for 18 of the categories out
of total 20 categories right those here
for each category will release our
performance and the best of performance
from other teams here is the difference
right so it's a actually pretty
encouraging for us because we used to
learning features right we don't you
know combine all kinds of different
features like appearance shape texture
those features we just use a hog
features on great images right so it's
pretty standard simple feature but what
we really did is learning features from
these simple features and this is very
encouraging result it shows the
potential of learning features from data
well you might argue that okay on we all
know human can recognize tens of
thousand objects right so the Pasco
challenge is pretty small scale so we
should really you know our challenge
ourselves you know to to push the
boundary and to recognize more and more
type of objects so what's happened last
year is pretty amazing feifei and her
politics organized is imagenet
large-scale visual recognition challenge
so in this task we have one installed in
the categories of object to recognize
and 1.2 million images for training
compared to 20 categories and the ten
thousand images in Pascal challenge so
it's much bigger scale of the problem
and and the weighted really will our
distance shows the result of the of the
different teams and actually in terms of
classification accuracy we achieve the
fifty two percent for 1000 cast
classification so from even from machine
learning point of view this is a pretty
high classification accuracy right and
we all know image classification
actually is really really challenging
right much more difficult than a text
classification so I think angel have
given a quite deep inside about why
feature learning is useful so I think I
want to keep this aside and in
particular I think for image recognition
you know we also have a strong evidence
that biological visual assistance they
actually learn features over you know a
very long time of evolution right and so
i guess i'm not an expert talk details
about its the biological system but i
just want to point out our there's
simply the discovery of the visual
processing architecture in the primer of
a one-layer lemony these complex cells
and simple sales those kind of behaviors
actually inspired computer vision
researchers developed a very powerful
visual recognition systems right so for
example this is a convolution neural
networks right so here's the input image
they apply some learning your filter
right you can loot the input data and
then at different locations then you get
a response map right then to further
handle this local invariance right so we
do like pudding so for example average
protein right pull a local response
together to get a single response right
again you get a smaller sorry it gets
smaller feature free to feature map and
then again on top of that you apply
convolutional filter again and then
putting again so you can repeat this
process several times in the end put
into a quantifier right so this is
basically the architecture of a
convolutional neural network so you can
see here the convolution of filtering
the filter step simulate the behavior of
the so called a simple sales right and
the pudding step simulates the
processing of the complex cells right I
think what's amazing here is the list
the convolution on your own network
actually learn a host that the whole
structure simultaneously that means
unlike many other vision models which
kind of a first you know handcraft or
whatever together some image features
and then put into svn or boosting to to
get the final outcome and this model
actually learns the hosting from the
pixel level right so it's achieved a
state of art performance of number of
different tasks for example one thing is
a digit recognition and also I believe
on for face recognition is also one of
the best models so it turns out actually
on this model probably was developed
like more than 20 years ago right and
reasonably people this is a very popular
thin income division community
especially for scene recognition and
objective recognition which is so called
the bag of words peg of visual words
model
in particular it's a combined with
spatial pyramid matching to handle the
spatial structure it's achieved really
state of art performance on a bunch of
benchmarks for example contact 101 and
Pascal object recognition and also like
check read large evaluation but if you
look at actually the details of this
model it still follows pretty much the
same architecture for example here the
first layer you use the local gradient
you know apply everywhere across the
image and then duly kind of pudding so
by this way you you obtain sort of dance
its safety feature or dense hug feature
and on top of that you apply vector
quantization coding and then apply
average average pulling that's the way
you get the histogram right in the end
you apply linear SVM so you can see the
structure is pretty similar but the
first two years are handicraft right and
this week you coding there is someone
like a learning because it's learning
the learning the vector quantization you
have to apply k-means to a large flow of
unlabeled image patches so this is the
answer lies learning right so I think
our observations of our honest model is
popular there are several ways to to
start from here to improve this this
model so one thing is first least
learning your SVM is not scalable right
even though it's a produced very
impressive performance but we all know
in computer vision really if you want to
solve the problem we have to use a lot
of training data right so for learning
your svn you cannot probably it's
non-trivial to handle for example over
10,000 of training examples the second
why is the week you vector con
asian codeine might be too discreet but
maybe two costs not far enough right
because the we all know this the sensory
data structure is it continues right so
are probably we need a better coding
methods then the next one is average
pudding right so probably average
pudding is sub-optimal actually I think
many researchers observed this and using
something like for example or mac
supporting you can achieve much better
performance right so we also need to
investigate it what's the what how to
develop what the principles to develop a
better pudding messrs right the last
question is why don't we learn housing
together right instead of feeding you
know fixing the initial two layers of
fixed right so this pretty much
motivated our work so so our our work is
pretty much let's say that don't try to
do anything too fancy let's accept
that's this kind of architecture is the
thing we want to stay with and then
let's based on its the observations can
we do something better right so the
first thing what waited is replace the
linear SVM by simple linear classifier
then the question is how to competition
between a linear classifier with
millions or hundreds of millions or even
billions up for training examples right
so we developed the stochastic training
I mean for to handle this problem and
the second why is the codeine part so
how to develop a better cody measures
from unlabeled data right and the key
question we want to ask here is how do
we know I mean learning for an unlabeled
data actually can help you to achieve
supervised learning tasks right then a
third one is pulling to develop pretty
Messrs the last one is deep learning
means learning the hosting together
multiple layers all the other filters
together I think it is talk I will focus
on only two part the first is allowing
your coding we are asked always learning
so I think if I have some time then
we'll talk a bit more on the pudding
side so here's the notation this is the
input image then if we extract the
features in the local patches everywhere
then what we get is super dense local
features so in this case each image is
represented as a bag of feature vectors
right in a case of if the feature is
safety feature then it's 120 a
dimensional feature vector then we
perform linear mapping to map the
features into a new space right so this
is a learning year coding and typically
the new space the dimensionality is much
higher and the end the end of 0 is some
representation not just the
high-dimensional but also highly sparse
right next the way I perform linear
pulling so which is a weighted average
of the of the non linear codes together
in the end that we perform linear SVM or
any linear classifier so if we look at
this this whole structure and look at
this classifier so this is a linear
model applied on a plural read codes so
because this is summation then you can
move the linear operator into the inside
of the summation so then this is a
function on the images is a classifier
on image level and this is a function on
local patches right so in this framework
interesting insight is so learning a
classifier on a level of images actually
is closely connected to learn some
learning your function on
patches
so here's a very high level idea about
learning non linear features on the
local pantries right so suppose this is
I the the distribution of the local
pantries right and here's the unknown
learning your function on local pantries
right so we don't know is learn your
function this is something what we want
to learn so the Assumption here is the
image patches X follow a learning your
many folder structure and the target
function is moose on its manifold so
that's our assumption right I think this
is the key assumption to connect
unsupervised learning to civil rights
attorney right so that means by learning
this interesting structure of unlabeled
data so we can get some information
about a target of function right and
that's the information actually can help
us to learn the classifier ok so the
coding step is to learn a learning a
mapping from the input data to a high
dimensional space right typically this
is a high dimensional and spars right
and then the way it will learn such a
learning your function is to actually
put a linear function on its learning
American right because the the mapping
is a learning year then the linear
function here actually is a linear
function on the original input ok it's
clear yes so here's why idea we
developed which is the code a local
coordinate the coding so the intuition
is the following suppose this is the
input data right we don't know this
learning your function right and then we
want to figure out what's the function
value you know at this particular
location right so what do we do is ok
let's say we have a set of basis right
by whatever measured we have this basis
and then we find some local basis right
to chilean code for linearly in code is
to the point where those are linear
coefficients
so we know this target function is
learning year but is a reasonable to
assume is locally the year okay and if
it's a local linear then what we get is
here in a local region right so this a
function value at least location can be
represented as a linear combination of
the function values on those activated
basis because those basis is also in the
local neighborhood right so based on the
local in your assumption you have this
approximation right so this is the way
how are we interpret its target function
value is the kind of high-level
intuition how do we use the coding
message to to to learn the linear
function so the procedure is like the
following first we learn the dictionary
from unlabeled data right those are the
basis sort of element in a dictionary
right and the second step is to learn
use the dictionary to encode the data
point so the way we do is okay for each
data point between to be encoded that we
find the neighborhood to activate the
neighboring basis right and we get a set
of coefficients right typically these
coefficients is highly sparse right so
because each data point only activate a
small number of neighboring paces right
that means all the coefficients most of
them are zero right only a small number
of paces non zero so this is somehow
connected to the sparse coding idea then
the next step is to estimate the
parameters it gives the based on the the
local in your assumption we have this
approximation laemmle each data point
the function value on each test point
can be represented as a linear form
right those are linear coefficients of
the coding and those are kind of fixed
number of parameters which corresponding
to the function values on the location
of the basis so here turns out this
for those parameters are global in your
ways to be learned so this is pretty are
interesting because somehow we turn a
learning year learning problem into a
kind of a simply estimating a linear
function right estimating sound linear
parameters so we can say something about
this function approximation performance
so this is the approximation to the
function value at least location right
it turns out this approximation error is
bounded is upper bounded by a kind of
unsupervised learning objective turn
right victory is the coding error right
so how close you encoded the data point
x plus one additional term which is this
is sort of like a one regretted
regularization on the coefficient right
between courage is sparsity right and
police is weighted by the distance turn
which is the distance between the
encoded data point and the bases right
so if you look at this if we minimize
this upper bound right that's actually
push down the function approximation
error but something good is about this
term encourage local kind of local basis
are tending to be picked up right so the
key message here is a good coding scheme
should be yes should be our have a small
coding error and also be sufficient
local so based on this principle we can
develop pretty simple very simple
algorithm so let's say we use dictionary
learning like a k-means or higher
k-means to learn a dictionary right so
then to encode a data point let's
firstly ensure locality that means let's
pick up neighboring paces right so
that's similar somehow similar to vector
connotation but in week you you pick up
only one basis here we pick up multiple
basis
right in a second step we show a low
coding error that means that we solve
this minimization problem which the
constraint right then we get it a code
so we apply these measures on for
example here is Cal Tech 101 and Caltech
256 so on Cal Tech 101 it's pretty much
comparable with state-of-art message
using a sparse coding on top of a safety
feature right and on cal tech a 256 it's
much better right you can see here
performance
so the next is to another message pretty
much for followed a similar idea of
exploring the data geometry right so
this is so called a super vector codeine
so it's a let's say we want to want to
figure out what's the function value at
this location right the same as before
so what we do is find it here it's the
one basis right so are so one idea is so
let's say we let's use the function
value at this particular location right
to as the approximation of the target
functionary right and the way if we do
this for all the data point so what we
get is something like a piecewise local
constant approximation of a learning
your sports smooth function right it
actually this is pretty much explains
how the vector connotation for peg
awards presentation right does let's see
if we can do something better so again
so we want to figure out the function
value of this location and then let's
find in the nearest basis then let's
pick up the nearest bases here so
instead of doing a zero under
approximation maybe we can probe the
structure a high order structure let's
say the first order which is local
tangent right so if we do this for older
data point what do we get is a very
simple piecewise linear local linear
approximation right so we can also look
at so how how about the function
approximation error right so this error
is upper bounded by simply the vector
quantization error well that means
that's very nice right that means that
we can simply apply with you to learn
the basis
then just do a lipid twist on top of
week you then we can get better
performance right so here's the local
tangent turn at the location of the
bases so the function approximation can
be represented as a linear form so this
is the super vector code of data and
those are unknown parameters to be
estimated right so I think here is the
summary of of the three cody masters i
mentioned the first one is the popular
vector quantization coding second one is
local calling and coding the serta why
is the super vector coding so they all
lead to kind of a high-dimensional
sparse and localize the coding schemes
and they by learning from an unlabeled
data they owe explore the geometry
structure of data right but the new
coding measures are more suitable a kind
of day explore finer structure of data
and they are all suitable for linear
classifiers right so their
implementations are pretty
straightforward
so I have a how many minutes did I have
okay okay okay good okay so uh in the
next I going to talk about so since we
have this learning your coding right how
do we do pulling right I think boolean
stuff is a pretty much a kind of explore
the area oh I think they're people have
been especially in the future learning
community people have in turn sively
worked on how to learn nonlinear
features from data but a boolean step is
pretty much like in a kind of ad hoc
stage right I think that there's only
one work done by Elaine and the local in
last year icml to analyze rattle sir
ethically analyzed the circuit of
masculine which is pretty much stayed
wad messrs for for the pool instead
stage right but i think this deserve
more a higher level of interest for
further investigation so let's look at
our the hard way represent the images by
local features okay so for each image we
extract a bunch of local features at
different locations right in that sense
each image represented as a set of
features which is the feature set write
a feature set is empirical distribution
right so that means each image is the
empirical distribution like it is right
so one idea to perform imager
classification right is kind of tree
embed the empirical distribution into a
feature space well let's say a Hilbert
space right and in that space we do
linear classifier right this is a pretty
much like the kernel trick okay so let's
say this is the embedding of the
distribution I think what embedding
people have started this problem let's
say this work done by smaller
and his colleagues they consider a
embedding in a such a very simple form
right simply its average of the learning
of mapping of the local features right
so this is pretty much exactly the
average pulling right
are there some problem about this
average pudding I think fundamentally
what issue is in this embedding the the
kind of norm of the of the embedding is
not properly normalized it could be
unbounded so it seems to be a
mathematical problem but it's actually
also a issue in practice that means in
an image if some features some patterns
kind of like noise or whatever right
repeated and occurring from mark 20 them
then this component will dominate you
know the representation of the image
data right so this is kind of in an
intuition level why this simple average
pudding is not good and empirically this
has been observed by many come division
researchers right max max pudding is
much better way better than average
pudding ok and then let's look at
another way of defining a kernel between
distributions this is so called a parish
re kernel which is pretty simple fun
right this is the integration of this
like this is the square root of the two
densities right you take the integration
this is a very well performed in a sense
that's the norm norm of the awfulest
embedding is pretty normalized right so
it's too long is simply one right but
one problem Alfred is the palette sorry
colonel between two distributions is the
following here is the simple example
right suppose you have two discussions
like two peaks right somehow if you
apply this kernel on in this case or
another case right pretty much you get a
similar result because this kernel
ignore the structure of data right even
these toothpicks are close or far away
this Colonel is insensitive
right let's say if you have a vector
connotation write you another way so for
each image in the extremely simple case
you each image your animal is one pin
right so if to image and the two beams
are for two images actually they are
very close in the feature space right
and this planetary Colonel rigidities to
image is totally different somehow
ignore the distance the structure of the
of the data so then we introduce a
smoothing turn so here is the new kernel
function we defined between two
distributions right so what we have here
is a learning your Colonel between on
the feature space right it doesn't have
the problem of the the previous thre
colonel because because of it is
smoothing term actually it's
incorporated a struct smooth structure
of data right however this definition
requires to know the density which is
impossible right so then we introduce a
nano formulation which requiring only a
rough estimation of the two densities
which is the P tilde and acute further
here so this formulation induce a new
feature presentation which is to this
formulation let me yes so this is a if
we apply this message into a empirical
distribution so what you're getting is
the following so this is the empirical
distribution right the embodiment of
this empirical distribution is average
by this weighted average of the larger
American of the features right here's
the weights right the ways is based on
an estimation of the density right so
interested in seeing here is a nice
property here is the actual distance the
estimator doesn't have to be very
accurate
right so as long as the ratio between
the estimator of the density and a to
density right is bounded from above and
below and if the estimate test measure
itself converge to a limit right so
mathematically it will be well behaved
it will achieve desired properties and
we have done actually extensive
experiments show that as these new
methods perform much better than average
putting and in most of cases is like
equally well or even better than max
pudding right so let me tell you are in
which cases this is better than max
pudding actually masculine in implicitly
assumes the the coding is non-negative
right so you can't afford picking up the
most prominent response right so what
about you know if we want to just use an
arbitrary Cody message just want to
explore the geometry structure of data
right so the cooling is doesn't have to
be non-negative right in that case
sparse coding will fill right in this
case this new are putting measures would
be much better yeah so here I will
conclude my talk so so we have developed
a series of visual learning methods we
are learning from unlabeled data and
they achieve the state of art
performance on several benchmarks right
actually on I have a few demos if I have
time or maybe we can show them demo
offline oh I'm done basically
okay let me finish the conclusion yes
actually this is the last second slide I
one more time so um and we proposed a
new pulling message it's just very
interesting theoretical properties but
also empirically should better result
than average pulling and in a for sparse
coding or local coordinate coding type
of Messrs it's the same level as
masculine but for civil vector type of
proteins it's way better than not
sporting there's bunch of interesting
their directions I think we would like
to explore further so once they is
definitely deep learning you know so why
don't we learn housing you know multiple
layers off allowing your future
transformation you know together right I
think angel has given a very excellent
coverage on that topic least something
we are also working on and the second is
definitely interesting it's a fast
faster Messrs you know for feature
extraction right because we have to
think about how do we handle you know
extracted once the learnings turn the
feature learning is done how do we use
the learn the model to extract image
features for example in the level of
like 50 millisecond right so this is
still very challenging the sort of thing
I think it's a probably the most
interesting to myself is learning
powerful features from the richer
structure of data for example from the
video I think angel also mentioned from
a temporal consistency assumption to
learn to explore the kind of you know
temporal consistency but also to learn
kind of America features for example our
plane rotation of the objects
right so those are the papers we publish
over the last three years and thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>