<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Large-scale Image Classification: ImageNet and ObjectBank | Coder Coacher - Coaching Coders</title><meta content="Large-scale Image Classification: ImageNet and ObjectBank - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Large-scale Image Classification: ImageNet and ObjectBank</b></h2><h5 class="post__date">2011-05-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qdDHp29QVdw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay good morning so it's my pleasure to
introduce if they need you they face a
professor in Stanford she has done a lot
of interesting work particularly on
high-level visual recognition but since
a lot of you are familiar with her work
I'm not going to waste time describing
some of her accomplishments but instead
I'll just mention one thing that
probably none of you are aware of before
she joined started HD eye contact she
actually spent a year in Tibet doing
medicine research so really this I hope
this is a casual talk and be interactive
ask me questions
you know it's my way throw together this
talk - named at large scale image
classification but really I'm at Google
so I talked Stein he's gonna multiply
everything about a large number in his
head to make it large scale actually I
was talking to a group of you and I just
want to say that um you know how great
authors with their write novels they
start the novel and they don't know
where the novel is going a great novel
is where you started you let the
characters to take their own life and
these two projects imagenet an optic
bank is how I feel I'm not saying this
is a great novel I just feel that I want
to share with you the beginning of this
story but I don't know where this is
going and I'm super excited by how long
it can take its own life and if you have
any ideas I'd love to hear about it so
um I don't need to tell you what is
vision we actually did have a
philosophizing session earlier this
morning before my coffee but but there
is two components of vision you take the
real world the visual world there is a
part that's for
mean pictures that's you know capturing
and imaging and so on but there is a
even deeper part that is understanding
pictures or videos you take the pixel
world you have captured and how do you
make sense of it and our brain is doing
that every single second in fact that
more than 50% of the neurons cortical
neurons in our brain is is involved in
visual processing it's by far the
largest component of the sensor system
in our brain and for me and then those
of us working in computer vision we're
trying to you know do similar tasks as
the brain is doing and personally like
Thomas said I'm particularly interested
in making sense at the high level which
is really about telling stories if I see
a picture like this like this how do I
you know tell a nice story
that tells you what are the objects
inside what is the scenery what is the
layout of the scene what's happening
what's activity and maybe down the road
the emotion the intention the purpose
the goal and so on so that that's kind
of the overarching interest I have for
effort for doing vision I'm not claiming
this is all of vision there are other
parts of vision such as people do
looking at you know properties of images
properties of objects or people looking
at and segmentation issues but another
vision is what I'm interested in just a
little bit of a road map of where
computer vision has come from in this
space of telling stories or high-level
vision so roughly this is my own opinion
as as well so take it with a grain of
salt roughly speaking the high level
vision story involves three
he tasks or functionalities object
recognition Scene Recognition and event
and activity recognition of course
they're not isolated as you know but
doing this to to make things simpler and
for each of them we have different
levels of tasks and I try to make it
make them in a vertical line so going
from relatively lower level tasks to
relatively higher level tasks for
example but some of you who are old
enough like me and those that you know
for the pasta the the bulk of the
research that was happening in the 90s
or even earlier to earth early 2000 is
more concentrated in the lower-level
part of this chart trying to figure out
one of the important features interest
points feature descriptors everybody
knows the word sift that comes from
intensive research here people are
looking at tracking the videos some of
the grouping and segmentation work
understanding shape and texture and a
little bit you know this is what my PhD
would be a little bit of object
recognition
now today really the bulk of computer
vision research is has moved a little
bit upward and it's focused here we're
trying to do a lot of work in solving
the object the fundamental object
recognition problem solving the
fundamentals seeing image recognition
problem and understanding activities and
actions this is actually not
non-developed work has been happening
here and my own prediction is that
vision is post perception and cognition
we are still very busy trying to solve
the perception problem or far away from
solving it but eventually perception is
going to meet cognition and once we're
starting to meet cognition we're even
going to go up higher to think about
intention and goes functionality and
rolls and things like that
but that's just a speculation so um
in just a little bit of an advertisement
for stem for vision lab so we have been
very active in in in several areas of
research and I'm gonna tell each one of
you these papers today not just kidding
but this will give you a fuller picture
weaving very much interested in
understanding object recognition we've
been trying to work on seeking
recognition both from segmentation point
of view all the way to sing
interpretation and understanding and
personally I'm also very enamored by
verbs and activities these days so
that's kind of a over roll picture we're
now going to go through each of them so
today here I just want to share with you
two very recent work in fact I'm mostly
gonna share with you the object back
work but to just lay some ground from
for where object Bank come from and as
well as to just not never I should tell
you a little bit about even though some
of you know image that better than I do
now so that's that's the road map we'll
look at both of these these work I'll be
relatively quick about internet credits
to my fantastic collaborators professor
Kylie from Princeton University when I
wanted to do image that three years ago
four years ago
I was not able just the thoughts just
bounce me so much that it was Kyoto me
do it I have a big cluster and without
his support I would never be able to
venture into this project and the Alex
Burt is also collaborating with us and
these are the two very bold graduate
students who actually sign up for this
okay
so here's this small roadmap for image
net I'll we briefly described what image
net
yes and then tell you a little bit of
using image net to benchmark today's
state-of-the-art computer vision
algorithm and again this is just the
beginning I'd love to hear which is what
you think er or explore possible
collaborations if you're interested um
data sets and computer vision has always
been intimately related in fact one way
of looking at the discipline of computer
vision is it's driven by data sets so
without this is that computer vision can
hardly advance here is attribute to some
of the classic data sets some of you
during your PhD have worked on some of
these with all these things and some we
wouldn't be making profits as we are
today we look at these data sets today
and laugh at the small-scale the the
lack of variability and so on but they
have played a critical critical role for
example object recognition I don't know
if anybody remember coil some of you
remember coil we look at it today we we
just laugh at it but it was really
critical for first wave of object
recognition papers also also UIUC cars
that's another very classic thing is
that I didn't dare to put Caltech but
also some of the other data set like
face recognition beverage faces seeing
new faces
other than recognition some of you
working other areas of computer vision
probably recognize Middlebury stereo
datasets very classic texture dataset
and so on so so I'm in again in the
morning
philosophizing session and I was
chatting with you guys
personally I tell my students as well
we've to solve the problem of vision
you've got to understand your data it's
not just a feature vector to me and
you know it might be to some other
people but so having good data set is
one of the most critical thing in
computer vision research so in object
recognition research let me just zoom
into this part of the history a little
more we started with in early object
recognition research we started with a
data set basically that comes from
Caltech and it's a binary data set have
one class over bite and some random
images crawled from the web of things
and that already got us pretty far if in
object recognition and then people have
been working on this Caltech core
dataset for quite a while again again
this is some of many of you have worked
on this probably in your PhD if you
still remember personally uh you know
part of my own thesis was coming up with
the so called large-scale data set at
that time as the Caltech 101 by now it's
a it's a it really is an outdated
dataset in object recognition but there
is a push towards large-scale what Cup
really exciting is is the last few years
there was a lot of effort in in
recognizing the problem that if you
really want to solve object or image
recognition you've got to respect the
the true scale of this problem and then
what is the true scale I think human
scale is even different from Google
scale at least we know there are tens
and tens of thousands of different
objects and probably from Google's point
of view you have millions categories so
there were some effort in putting
together datasets that's trying to be
that towards that skill one of the nice
data set is called tiny image some of
you probably are working on that
tiny image is a great data set but it's
quite tailored towards a specific
problem of trying to have very limited
memory very limited resolution and how
do you how do you you know solve a
problem within that scope and while tiny
image data's that was going on at
Princeton we were thinking about a
different problem we were thinking about
coming up with the with the data set
that is high full resolution and it's
not a business like that reflects the
statistics of the web but rather an
ontology that that havelet's the visual
knowledge in human human understanding
so that's the that was the the
philosophy of image net and where it
started so image that in a glance today
you can browse image that online and
download it for free and if you go to
our home page last night we have rolled
out 17,000 classes of objects and scenes
and they were to these 17,000 classes
come from they come from word net so
image net is built upon that worked the
backbone of word net I assume most of
you here have heard of coordinate and in
total image now has more than 12 million
images spread over 17,000 sunset we try
to make it as even as possible given the
given the resource on the web and every
one of the images in the image net is
human verified multiple times for the
correctness we cannot claim a hundred
percent correct but but this is as good
as it gets for us and for each of the
con
except imagenet you can actually browse
for example it's randomly type the word
tiger cup and image that has 1444 images
of tiger cub and we tell you that if we
if we drank it we made a ranking based
on the popularity on a British corpus
document or purse and tell you this has
a purple arity percentile of roughly 50%
you can also you can also download some
of a subset of the categories have
features for you to download if you
never want to process it yourself we
have process some features for you we're
also rolling out something boxes for for
each of the object in image net dataset
you can also browse you can also look at
this particular concept and see where it
exists in the word net ontology look at
and browse the the parent and children
of this set yourself so any questions so
what was the rationale to remind the
label the classes with word net which is
a semantic that that's a very good
question was okay there are two
rationales the the shallower answer to
that is that um where else can I find a
huge lexicon database that can give me
labels other than weren't it I don't
know there might be there might be other
for example Google you guys have say a
list of query words but we don't so
that's one point another deeper answer
to this is where two language
where does language why do certain
object have words it's because that they
exist in the world and humans through
evolution need a label for that so in a
way you know the language labels reflect
the visual world but then there are
things like functional categories chairs
which are not they should be homogenous
yeah right exactly so nice that's part
of the research we don't know how we go
I'll show you a confusion table that
actually taps into that out question but
functional category are they completely
non visual are they partially visual we
don't know yet it's part of research so
what happens in case and noun doesn't
have it doesn't things like that
so in constructing imagenet we we have
for now limited ourself in physical
entity subtree okay so we can go deeper
into this there are 80,000 concrete and
countable nouns in words met after
eighty thousand we have done an
estimation about probably thirty
thousand of them are visible and there
are two ways of deciding whether they're
immutable one way is by filtering by
researchers like us we don't get into
these very fluffy nuts that's one way
the other one is we have decided to take
them out the disease some substrate for
example we just don't wanna get images
for disease or bacterias so we've taken
out a little bit of that but that's not
a huge portion then what we do is we
give to Amazon Mechanical Turk or I'm
going to show and we find out their
sunsets that people can never agree on
how to label these images and we take
them out as well because if people don't
agree we don't know what to do with it
so so okay so that's an overview of
Internet why you feel it's reasonable
because image is still it's pretty
different from the like the text
information why I'm seeing is reasonable
to use like the word necklace meant its
structure to all the night I mean image
may have their own hierarchy I mean
what's up yeah that's a great question
we haven't come up with our own
hierarchy so we have to rely rely on
something that's why we start with word
not do we think we're not fully reflect
the visual hierarchy I don't think so
do we think it's related absolutely
because I this is you know the
relationship between language and and
vision is very close even in human
vision yeah so you know we have things
like animals with matter or animal world
above bacteria we have things like ink
or geologic formations we have lots of
sports activities materials and fabric
instrumentation such as tools appliances
plants flowers and so on so basically
I've tested people think of a noun and
searching imagenet in general the first
few nouns people think of as last you're
not trying to break image that exists
there so okay so very quickly I'm how
did we construct him Internet I'm gonna
just be very brief about this so really
there are only two steps one is that for
each of us in set so each of the
category we collect lots and lots of
candidate from the internet and then we
ask people to clean it that's it
so everything set we go through these
two steps and collecting from the
Internet is you know you have many ways
of doing it again we don't work at
Google so we have to find a way to do it
we do you know query search we expand
the query you can for things we we do
multiple languages
do multiple search engines and yeah we
do lots of parallel downloading so
basically for each category we come up
with about 10 to 20,000 cat Canada
images that's that's the number we we
are operating with I know that you guys
have more but but that's that's where we
we stopped okay so the second step is to
verify images by humans and so what do
we so I was doing this computation let's
say we so how large internet eventually
will be I was thinking back three years
ago it'll be you know about 40,000
sunsets and we have let's say about
10,000 character a candidate images a
human or humans need to go through for
each of the sunset we should at least
have three people to look at each image
right otherwise you cannot verify it
very well and and if I do that let's
just say a person looks at two images
per second that's a don't eat and sleep
and so on and if we do this computation
to construct the entire image net
it'll take nineteen human years so I
asked my graduate student do you want to
do this I need to graduate so um so that
was impossible
thank God we discover Amazon Mechanical
Turk I didn't even discover it my
graduate student got so desperate he
discovered it and because of that were
able to cut this number almost any way
we want
even the how much
and that's we have this so-called
parallel human computing and now is the
the secret sauce for being able to have
imagenet today is because of Amazon
Mechanical Turk
so roughly what do we do in Amazon
Mechanical Turk you can you know if you
have free time you can go to the task
force day um so if you go to a page for
image that task this is a typical page
ok so this is a since I cook Delta and
they gives you the explanation it's a
low triangular area of blah blah blah
blah blah and then these are the
candidate images we give you about 300
candidate images as a Turker and then
you you look at them and you say ah ok
you click on the ones you think are good
images that contains what does good mean
we give you instruction that click on
the photos and then these are the good
ones it gets deposited over here if you
think you make a mistake you click again
it goes back so that's the basic UI for
uh were doing this task and for 300
images and you get paid for sentence we
can talk about why people want to do
this and are we exploiting chained
prisoners from third world
I can guarantee you or not so um and
then there are some things you have to
do people are people not vicious but
they do anything to get money so for
example um oh ok this is just to provide
people something or might not know what
Delta is and then not only we have
provided definition we
give them both Google thank you and
Wikipedia links to show them you know
they can just go and study it for that
for cents and then there are also you
know for example some words are
ambiguous you should see that worked
keyboard how do I know it's a music
instrument or a computer keyboard or or
some boards that actually hand keys so
you want to make sure because if you
google that word keyboard I don't know
how good it is now you might get
multiple meanings multiple images and
how do you make sure people select the
right one
we actually quiz them it's not easy to
get these two persons you actually have
to go through a quiz so we we show the
word and then we ask them to read the
definition and they click on I have read
it and then we ask them to do to
actually click on the definition that
they have read so that he's sure and
this definition comes from all the keep
this case Delta you know a word that you
Delta has multiple meanings and you can
list all of them and and they have to
read it in order to select the right one
and after that they could go into the
visual tasks so um they're also a
feedback page and also another thing I
can tell you guys since you never have
time to do this task anyway is that we
have planted evidence we have some
images we know the ground truth so we
make sure once you finish doing these
300 images we compare your results with
some of the planted images and make sure
you're not cheating us so that's that's
I don't know if you guys care about
doing Amazon Mechanical Turk that's
actually really really important teni
fetch it together we do since no you
don't get any money and you email me and
cry and say you know I didn't mean to
cheat we got a lot of emails like
okay any other questions about Halloween
- like this yeah
okay so um alright so that was a quick
overview of how we got imagenet we said
image that is an ontology some of the
properties of image that it is pretty
high accuracy because we have done this
human verification and then we have sent
what about 20 or 80 categories from all
different depths of the word country and
go through the independent verification
and measure that against Internet and
have obtained this precision it's not
perfect but it's very very high we have
look at the diversity of the images
that's another thing we cared about and
then here is an example of comparing
Caltech 101 versus image that one way to
compare it one quick way is just look at
the average images and see if the
average image you know the greater it is
the better it is if you can see faint
outlines probably means it's not diverse
enough we come up with some quantitative
measure but don't worry about that so we
also look at the distribution of image
net in terms of the images and again our
philosophy is not to reflect the real
world statistics our philosophy is we
want this to be an ontology so in 2009
for example we have more than 6,000
classes that have more than 500 images
in image net whereas another fantastic
thing is and in computer vision which is
for another purpose has only 85 classes
that has more than 500 images and only
200 classes that has 100 we have almost
10,000 classes that has
so so really we were what we hoped is an
image that can be used as a benchmark
but it's not to reflect real world
statistics okay so that was a quick
overview of image net and just very
quickly the very obvious thing when we
had image net in our hands was to ask
yourself now I have this large-scale
data set
let's see where computer vision is as a
field in recognizing this large scale
are we you know how are we doing so
here's the answer let me explain to you
what the answer is so we have work
testing recognition accuracy against the
number of categories and the accuracy is
measured by the average of the diagonal
entries in a confusion table in an EM
way can categorization and we have
tested a four state-of-the-art
algorithms you look at these four they
are just representation which is filter
Bank representation with nearest
neighbor bag of words representation do
I need to explain backup words here
everybody knows right great Bible words
representation with nearest neighbor bag
of words representation with linear SVM
and spatial pyramid representation with
SVM do I need to explain spatial pyramid
so spatial pyramid is a bag of words on
steroid so why do I call these for state
of the art why am I not you leading
constellation photos and
and pictorial structure it's because
only these four our 500th core notes
computer and cluster we can only run
these I don't know if I could go you
guys I would love to you know see some
of you testing even fancier models but
that scale it's just so ok what's the
punchline the punchline is the best dog
algorithm out of these four is spatial
pyramid plus SVM and a 10,000 way
classification gives us about seven
percent so you train ten thousand one
dresses rest SVM and the represent each
team is image was normalized to the same
l 2-norm or they were not um we use is
the vector mobilized so you have you
were just from you know different
illumination different so the energy of
each images do we use the bag of words
representation there is intrinsic
normalization if you use sift
representation because cares about
gradients so we don't need to I mean the
feature construction is consider sure
but the question is whether the ill norm
of each image is different there are any
similar sides the dynamic range is okay
but they're not identical for sure okay
so and why the choice of you know ten
thousand one versus restless be
something more robust to that
computation we don't have enough
resource or time so this takes one month
I mean it's because we don't have a big
cluster so that's basically the
punchline is it pretty sad well it
depends on how you look at it so my grad
student look at it and thinks very sad
the 7% is pretty bad
I looked at that wow there's 93% to go
who you know lots of research is random
charles is one over 10,000 so we're not
that bad
all right so can this be used in real
world no so so to me this is very
exciting because of that what would be
human performance on this just a very
good question what would be human
performance the answer is we don't know
and the question is do we care um it's
really good if it's less than 5% then
you know right because for example
humans cannot fly you still wanna make
airplanes it's true that we want to
meaning of this simple task for example
detecting this cup we want to benchmark
about against human and make sure we
computer vision can do as much very
different this argument I don't want to
digress but you're talking about a
cognitive task where the labels were
defined through a cognitive test so you
know yes human cannot fly and you will
cannot try it can miss out of the way
you know simple can you filter can do
right but we're talking here about the
cognitive maybe one thing that makes a
big difference a thousand case very good
but we do need duplication within a path
so we don't do it
duplication across path let's say a
person in a chair that image in the
person path which is what noun and Homo
Sapien whatever will only appear once
but that image can appear in the chair
path anybody need that path it makes
sense for human so artificial it does
how to measure exactly nobody knows
because it's a hierarchy before that no
one had a hierarchy so we might not be
menus so typically again it's a bit of a
digression but if you talk about if you
look at problems in regular
classification in humouring that there
aren't that many but they are there are
a few world establish then a tree gives
rise to a unique distance right between
two nodes right so typically the natural
laws or you know decision task is the
tree distance yeah exactly
and can we train with hierarchy is
another very useful thing I think it's
safe to say humans are gonna do a lot
better depending on the that statement
one needs to be careful I agree with you
it's safe to say humans can tell the
difference of many many classes but
humans might not be able to name it so
how do you even get that ground truth is
is you know a very tricky task so have a
child you haven't the metric to have a
child using using talk to to get those
variables cuz if you showed me one of
those Delta images in the way of a pilot
Delta
well the Turkish for whoresons only can
be can do it yes or no if you ask them
to type a word you per have to pay for
reasons
you're going to perception and like as
humans we have a lot of contacts we have
you know silence and you know things
that are beyond just oh yeah yeah I I
totally agree with you kid I don't even
have an answer can humans do this or how
well can human do and I get with you
human use meaning other information
human has learning years of learning as
well so um I don't know just wondering
if you add extra say for example in
addition to the images you also add a
little bit of text that they totally
improved yes I I think if you have a
good way of extracting useful features
then to the right learning absolutely
okay so alright um have you ever seen
the 10,000 by 10,000 confusion table I
just saw that so this is it it's uh so
the diagonal gives you about seven
percent um okay so outs are wide word
and hierarchy has any set that it makes
any sense
so the the categories here are ordered
according to the word that hierarchy and
the fact that we see structure that
makes sensing this confusion table tells
us there's a strong correlation between
language and vision we don't know who is
causing who but there's a correlation
because if if the computer table is a
lot more random than where's al cake
this structure is not so correlated but
the fact that the animals have blocked
confusion
and artifacts at block of Yujin and then
you can zoom in and actually look at you
know bird reptile fish and each one has
and tells us that the structure of word
that actually reflects not a hundred
percent but some of the visual world so
so that's that's another cool thing we
discovered um this is something that I
actually don't have to preach to Google
folks but in computer vision we're using
this experiment to show people that
depending on the granularity of the end
class classification the performance
varies wildly so if you look at a
hundred or 200 categories of say
vehicles or or hundred mushrooms the
perf the performance is a lot lower than
randomly simple a hundred or two hundred
classes from imagenet and the reason we
want to do this is to tell people that
Caltech 101 and Caltech 256 it's not
necessarily the right benchmark if you
really want to you know be realistic
about a large scale recognition but here
you guys don't so hierarchy how do you
take into a kind of using hierarchy to
do better to do better say retrieval and
so on we're not claiming we have solved
this this is actually really a stupid
way of just changing the cost of
measuring error depending on the
hierarchy all we're saying in this paper
is let's bring in hierarchical
information because mistaking the dog
and the cat is not as bad as mistaken
linking the dog as a microwave in most
cases and for example if this is the
query image shipwreck if you use no
hierarchical information to retrieve
iceberg you know visually that makes
sense but the mistake is kind of large
the tree editing business
whereas if you take into a hierarchy
into account you retrieve cruise-ship
still wrong but not as wrong as you
train it with your kids no no we measure
the error with that we didn't train in
this case now we're doing training so
this is not I'm not trying to advocate
our method I'm just trying to say that
hierarchy is really important and with
image that we're able to look at this
problem now yes and if you're a boat so
I'm not totally from a cognitive task of
purpose point of view maybe that mistake
you know like who is to say which
mistake is better I don't know I'm not
claiming that's why we're getting yeah I
agree it's a little bit of a looping
argument the only thing that I'm trying
to save myself is that wordnet does
reflect some well accepted ontology or
knowledge structure of human of human
concepts of semantics whether that's
completely correct or not I'm not
claiming that but based on that I think
my point is just I'm not sure that
you're saying
yeah I'm not saying anything cool all
right I think I should go on okay so
imagine that like I said it's just the
beginning we we're still we're almost
done constructing and bridges we're
really excited by the possible research
opportunity you know driving towards
large-scale recognition um one thing
that came out of image that it's not
direct but it's an inspiration is object
bank and you know I'm so excited by
object Bank that to me if nothing else
come out of image that I'm already very
happy so let me tell you um let me tell
you what object Bank is and I have to
give credit for my collaborator
professor Ericson at CMU dr. green drew
at CMU and my two students daddy who has
interned here with Thomas and and how
also that interned in Google in China
so um okay so what is it object Bank
okay let's just talk about um vision
tasks again image classification what is
image classification okay this is let me
show you this picture why wait till a
boy is a saving event you can label in
other things but at least that's one
good label this abseiling to and they
actually shares similar image structure
you know proportion of water and sky is
similar the object looks similar when
there's two boats missing but you know
roughly okay what about this still
saving at least our human eyes look at
it as saving the now color space is very
different texture is very different the
shape of the boat is very different the
background is very different the task is
getting tougher now what about this
human I still look at it as sailing but
now the image structure between listen
in Greece but yet you know we would at
least in certain scenario saying all
tasks we still would like to label all
four of these images are saving so image
classification especially semantically
meaningful image classification it's a
really difficult task and what have we
been trying to do we have been trying to
use low-level features in a better
vision to extract our image structure
and some kind of machinery to achieve
this task and this whole thing is our
feature representation is versus were
the Tosh's that gap is called semantic
gap and research in computer vision or
multimedia whatever is really trying to
close this semantic gap because we have
universally accepted these are some of
the features we'll use the third hog
color texture backup or in spatial
pyramid so on so can we do better
while we have doing better is to have a
really powerful machinery that closes
the gap but we're thinking about
something different and let me show you
the thought experiment um here's a
thought experiment so I have eight
classes of sports things and saving
growing and all this let's do the best
state of the art computer vision
algorithm spatial pyramid representation
and kernel and the performance of eight
to eight classifications about seventy
two percent all that bad but let's just
do a thought experiment let me tell you
not only I see these objects of these
images I also know the identity of the
objects I also provided you the
information of the object information
along with the scene and I get I used
both information and what do I do
how do I do I get a hundred percent so
this is a thought experiment no one can
ever tell you this
precisely
but it's telling you that these
important object level information
contributes a lot in differentiating
things and to image classification so
that's what inspired object bank we want
to somehow not only use low level image
information but we want to bring out
some of these high level representation
and visually speaking we want to close
the semantic gap by actually bringing
the representation higher so that the
task
our starting point is closer to the high
level tasks compared to the low level
representation so that's a little bit of
philosophy what exactly is object bank
is really simple here's object back you
have an image let's say somebody has
given you a sailboat detector and the
person said okay I have a sailboat
detector for you I'm not saying this is
a perfect sailboat detector the ticket
so we take the sailboat detector we run
it we run it through the image and then
we get a response back okay now forget
about this um now what do I do with this
response back well I look at the
response back when I look at it in
several different ways of cutting it
look at it as a pack of words and look
at the two-by-two amount of words I look
at it as a four by four battle words and
I just like the
cells collect a max response and I'll
only look at saleable detector in this
scale I look at it in another scale as
well because I don't know what sailboat
how big this symbol is so that's one
that's one thing now somebody says oh I
have more than a sailboat
I actually have 200 other detectors
there's bear there's water there's trees
there's chair just take it so I take all
of them and I rub all the response map I
do exactly the same thing as I did with
salable I come up with this guy can take
feature and that's code object Bank
that's that's almost it that that is
object Bank any questions here right
good question
so actually how many did I choose 200
and now you're gonna say where did this
200 come from it comes from the ZIF
small basically we looked at the 200
most popular objects or or stuff and
then we cut it off and one of these
object detector so who gave you this
object detector we just pick the
thousands from object detector anybody
everybody picks that um we can pick any
we're agnostic to the object detectors
um there are some design choices do we
have to use this three levels no we
chose it how many scales we happen to
use twelve of them again what's the
value to be put into each of the cell a
cell we use max pooling which is kind of
a universally accepted very nice it has
very nice properties but if you want to
do something fancy that's fine as well
but these are the implementation details
and if I were Google I can do a lot more
than two
so nobody stops us from using only 200
this is you know but I have to say
there's some human vision study that
tells us probably about two to three
thousand is actually so so that is the
entire object bank representation so so
why do I think of the bank record
representation makes any sense
let me show you an acne anecdotal
example so there are two images one is
forest and one is urban streets if I use
a low level feature a very nice feature
called filter Bank or just they look
like this this is forest and this is
Street not so different if I use another
very nice feature called spatial pyramid
this is forest this is mountain I mean
sorry the street not very different if I
use object back it's really hard to
visualize that gigantic vector so I'm
just showing you the response map of
each of the object detectors and without
telling you how are you a user object
man you already see there is a lot
bigger differences they're treating the
images yes the same as the objects no
the object descriptors the object when
descriptors are for now our detectors
are trained on our image that images
these two images work so okay bankers
has no overlap with image
Etha completely Victor's are not trained
on the imagenet is that correct are
trained that image that so you shot
sleep of me but when we do any testing
as you can see you have the
representation for the training set as
well yeah so so okay so this is the
object this is it for object members it
is really really simple
okay good person 40,000 40,000
dimensions
okay let me speed up because I think I'm
sorry I think I'm out of time
should I stop okay let me start talking
fast
um okay some proof of concept let's just
trying to tell you the power of optic
bank this is possible data sense we come
up you label me again deep duplicate it
so that the training and testing will
overlap these are some of the nice
low-level features this is a average
person correctness
this is object Bank here is another on
businesses MIT at endorsing 67 classes
these are the low-level features this is
state of the art and this is object back
again okay when he just run on linear
SVM we didn't use any of the nice fancy
machinery and then in fact the
state-of-the-art is even supervised with
lots of labels how different didn't have
that another dataset the sports data set
again these are the low-level features
these are the state of the art object
Bank with linear SVM so it's a really
powerful Universal representation of
dependent was not designed for any of
these data set in fact we just run it we
didn't really to any parameters and
that's what we get and more
just give this these are more
experiments on Caltech 256 again object
bank it's a lot higher than any other
okay so let me answer Kevin's question
so how large is object meant for 200
detectors we have about the
dimensionality is about 44,000 and you
can imagine for 2,000 you can just add a
0 there so there is it's highly it's a
high dimensional vector it's over
complete and it's highly redundant and
that's part of the power but um if we
can have a way to actually I'm
supplement you know to simplify this
feature and and one beauty is that by
simplification we can actually extract
directly meaningful interpretation of
the images this is what we mean so to
combat this problem of curse of
dimensionality we used two really really
stupid out of them
we we do a classification based on
logistic regression but instead of using
this this full-length feature we add a
regular riser we actually added to
regular risers one is a group lot so
regularizer
um the other one is just a simple of l
2-norm
actually the Apostles the l 2-norm plus
and plus a simple feature specification
regularized what does this do to the
logistic regression it basically
segmented while it's doing
classification it simultaneously picks
out feature dimensions that are that are
useful for this task and the beauty of
object and representation is because
it's a high level representation when
you are pulling all these features they
already have meaning you matter if
you're doing a bag of words codebook and
you pull out a couple features you have
no idea what that means but you can say
it's edge in this orientation
but you can all say this is actually
water at the scale in the spatial
location but object I can give you that
objects objects objects so that that's
one thing that reading makes me love
object Bank is that while we can do all
these high-level tasks but when you're
looking to the features and pull out the
relevant dimension you immediately it
immediately automatics automatically
give you interpretable semantic meaning
of the image and here is an example and
for example if you're looking at a beach
scene and you're looking at the weights
that are learned by the logistic
regression and you look at the high
weight that they are sky water sand
think they're just they're exactly what
you wanted that's the object detector
and the same thing goes to the mountain
class if you look at the highways sky
mountain trees Rock cloud that's another
sky high okay yeah the interesting so um
I think I'm gonna skip this so basically
I think back that I I wanna present to
you this new representation cultic Bank
which is quite it's very discriminative
very rich and can be used universally
for high-level image classification
tasks I can imagine if you have you know
bigger computers this feature can be
even more richer and and there are also
ways to specify if you care about
extracting meaning out of it so um just
some acknowledgement for the students
who are being involved in this and</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>