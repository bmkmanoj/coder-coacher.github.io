<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Measuring Noise and the Influence of  Noise Reduction on Resolution | Coder Coacher - Coaching Coders</title><meta content="Measuring Noise and the Influence of  Noise Reduction on Resolution - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Measuring Noise and the Influence of  Noise Reduction on Resolution</b></h2><h5 class="post__date">2009-01-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DrLoynC0oCI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're very happy to have deep mark
dealer it's a proprietor of a very
interesting company in Germany that does
cameras quality assessment services and
use and stuff like that you can mention
that if you want so very interested in
this topic of his talk on moyes and
resolution and interactions between
those things like that the emerging
doing yeah thanks very much yes dig
sides i'm running a an independent test
lab and in germany and we had a couple
of problems over the years with the
development technology of digital
cameras and one of the bigger problems
we had was noise and there were two
different aspects of that that appeared
and one was measuring noise so we do
most of the tests for german magazines
or for manufacturers who basically see
noise not as a physical effect that is
in the image but something that you
perceive in an image and that
disturbed's disturbs it in a way so they
want to have a measure of noise in a
kind of perceptual way the other thing
is the other aspect is that we find many
of today's cameras especially cameras
like cell phone phone cameras with a
very small sensor very small pixel size
there is of course a lot of noise in
these images and manufacturers try to
get rid of that noise instead of
increasing the sensor size because cell
phones for example have a limited size
that you can carry them in your pocket
so they want to keep the sensor size on
a very small level and then of course
noise of the signal is more visible and
they try to get rid of that noise using
image processor
in their cameras and the way they're
doing that of course also leads to
reduction of especially something we
call texture which is low contrast fine
detail and I will show you how we try to
find a way to get a method to develop a
method to measure this reduction of the
resolution due to the noise of the loss
of resolution to the noise reduction
that is computed in determination so the
noise sources we have in cameras is
basically the capture system itself so
we have the lens we have the sensor we
have all the electronics especially the
analog part of the electronics in the
camera that introduces a certain amount
of noise but on the same hand we have
something that we call noise adjusters
so everything every single step in the
image processing in that camera adjusts
noise in a certain way so if you do
color mate matrix saying if you do
shading correction and all that stuff
you change then the amount of noise and
the visibility of noise in images there
are different things to measure noise in
images different ways to do it one way
is due to measure standard deviation
that's what today's standards that we
can find for measuring noise are about
but these standards usually don't
address the luminance and chrominance
aspect of noise they also do not address
the the dependence of the visibility of
noise on the different viewing
conditions
they certainly don't look at the
nonlinear image processing so noise can
be different in different parts of the
images and it can be different on
different kinds of structures and images
and of course we find quantization
limits of the noise measurement and
sometimes we want to have an isolation
of different noise sources so especially
manufacturers say they want to know
where does the noise that I haven't in
in an image come from and how can I
would use it and of course things like
compression of images add to noise as
well so even if you look at the high I
resolution output of the of the cameras
if you compress these images afterwards
you may add some some more noise to that
so let's let's look at the standard way
to measure noise there is just an iso
standard for noise measurement in
digital cameras and that is the ISO 157
39 which basically makes use of the
target that you that you see here so
it's basically a grayscale a circular
gray scale and there's there are some
noise Paget's in this in the center here
that are used for measuring a standard
number for noise that is then recorded
or reported as the ISO total noise which
is this one here which basically is the
standard deviation in one of these gray
patches so we analyze these gray patches
calculate the mean and we calculate the
standard deviation and this standard
deviation is reported as the ISIL total
noise value and typically for
this you compute the signal-to-noise
ratio and since you can apply tonal
correction like gamma curves or aunt or
similar things to an image this signal
to noise ratio which would be completely
linear and for for a system that has no
image processing is corrected with the
incremental gain at this certain
position so we measure not only the
signal itself and the noise but also the
incremental gain of the so-called oec F
curve so the up to electronic conversion
function which describes how luminance
is converted into digital code values
then some people like to report noise in
DB and that this is how you convert that
signal to noise ratio into DB values
there were some problems with the ISO
standard because it didn't really
specify how to expose the target and how
to compute well it's specified how to
compute the noise but it made some
assumptions that are not that accurate
so therefore the standard is currently
under under reviewing and there is a
draft available already for those who
are on that on that committee and let me
just show you briefly how this is going
to work in the future so this was the
formal method of the ISO standard so
expose the target in a way that the
brightest patch back to that target that
the brightest patch here this one is
just doesn't clip or we exposed it in
our lab that it just slips because it's
better or easier to control
the new way of exposure is that we
expose the target in a way that the
background leads to a digital code value
of 118 you might want to ask why 118 so
typically what the output of most of
today's cameras is is images that are
encoded in the srgb colour space and for
srgb the average reflectance that is a
reflectance of eighteen percent known
from the former photography standards
leads to a an l star value in the lav
color space of 50 and this is equal to
the digital code value of 118 in srgb
images so that's why 118 is selected
there's a certain range I don't know
whether the tolerances are fixed now but
it'll be something like 118 plus minus
10 digital code value something like
that why do we do that so in former
times we adjusted the exposure in a way
that that the the OEC f curves looked
like this so we had different digital
values for the same luminance for
different cameras according to their way
of doing tonal internal tonal correction
so we got different signal-to-noise
ratios for for these cameras because we
had these variations in the mid gray
where the the value the signal-to-noise
ratio value was computed so changing
that to the way that
cameras use the 118 digital code value
for for the background we will get
pretty much the same signal to noise not
the same signal to noise ratio but the
same signal level for this mid gray
tones and the noise value then depends
on the noise value of the camera but you
can find different cameras depending on
their different ways of changing the
tonal range so some of them may have
highlights clipping highlight
compression in order to record a higher
dynamic range some of them might have a
steeper value here but then they will
clip in the highlights pretty soon so
this will affect the dynamic range
measurements but it will not directly
affect the noise value measured for
these mid gray values so we expose the
target in this way and then in order to
get to a good recording we will go to
the luminance where the output value of
the digital code value of the image is
145 so looking like something here so we
select the digital value of 245 and then
we look at what is the related luminance
value and then from there we go down to
thirteen percent of that luminance value
and record the signal-to-noise ratio so
this is not the OEC f this is a signal
to noise ratio curve and we select the
signal-to-noise ratio here and that is
the reported signal-to-noise ratio so
that basically adjusts for the different
tone tonal Corrections the different
cameras do so this way will be more we
will lead to more standardized
reporting's of the values so this is how
the standard is going to change but we
still have a problem that when we look
at signal-to-noise ratios we see we find
the case that we have grey patches with
the same signal-to-noise ratios or about
the same signal-to-noise ratios we've
measured but as you can see here
completely different perceptions so this
is a higher noise this patch has a
higher noise perception than the one on
the left hand side so this signal to
noise ratio measurement doesn't really
relate to what we see in the images so
therefore we were looking for a way to
measure noise based on on a on the human
perception the first wing thing we did
about that was that was quite a couple
of years ago we try to show people that
see lab color space doesn't do the jump
so we were selecting colors added noise
to that color and then we made people
look at these colors and they had a
slider to adjust the amount of noise in
different direction so at luminance
noise saturation noise or change the
view the U of the of the color and found
that it was while depending on the on
the specific color completely different
what we could find some of the levels
for luminance it was very much the same
for all the different colors but
depending on the color we got different
hue angle
settings and different saturation
settings and it's not really a uniform
space when it comes to the spatial
aspect of of color and of noise and
images so but the good thing about this
experiment was that we had a set of
images with a J and E so the people were
asked to move that slider to a position
where they just could see the noise in
that image so the selecting that
basically is a jnd and the just
noticeable difference so we had that set
of image images and that was a good
starting point for when we found
actually a method to measure the
perception of noise to prove that method
if it's valid or not because all these
images should lead to about the same
level the same value in this measurement
because they're all on the jnd line of
it so there are two different things we
were looking at and one was the visual
noise which a dad or currently is an NX
of the iso standard of the existing one
currently it's in informal informative
NX and we also had a look at the DSC lab
approach that was developed by juan del
and others and there's a very nice paper
on that from johnson and fairchild
called the top-down description of SC
lab so that basically describes what
this algorithm is about and the basic
principle of these models is the same so
if we look at the human visual system we
have an object we have the eye that that
looks as the
these objects we have the cones that
receive it we have a neural system that
works on that and finally we have the
recognition and both of these approaches
work in about the same way so we have an
RGB image we convert that into XYZ and
LMS space so that basically represents
the sensation of the human eye then we
convert that into the opponent space so
which is a one luminance channel and the
two chrominance channels and from the
opponent's base we do a Fourier
transform we filter that image based on
the contrast sensitivity function of the
of the human eye and of course that
there is or that is the point when the
viewing conditions come into play so the
contrast sensitivity functions depend on
the distance of viewing these objects
and resolution etc and from there we
compute that the image back to XYZ and
then we have the difference between the
two different methods one is converting
the image to lu v and computes the
visual noise from the AL UV image by
basically measuring the standard
deviations in three in the three color
channels l u and v and the output the
visual noise output is basically I
waited some of these standard deviations
for the three color channels the other
thing is the other approach the SC lab
approach basically converts the the XYZ
image to into la beam and then you need
a perfect image with no noise and you
need that filtered image
subtract them from each other and
basically do the the Delta II 2000
measurements for each pixel pair of
these two image images the perfect one
and the noisy image and from that you
can then derive a Delta II 2000
difference image that you can evaluate
it's not a single number on the first
hand that's coming out of the SC lab
approach but you might find a way to
come up with a single number so we just
had a basic look at these two approaches
and finally we found that that the
visual noise approach is a very good one
and actually that the basic work for
that approach was done by by poe she
hung from konica minolta and he had a
bunch of papers unfortunately most of
them are in japanese so it's hard to
have to have them translated to
understand them but yeah it was
interesting to look up so these are the
the the contrast sensitivity function
that we actually used and of course
although people don't really agree on a
single set of contrast sensitivity
functions that are used all over it
varies quite a bit we found a set which
is basically the one that is reported in
the johnson and fairchild paper that is
pretty useful so this is the luminance
CSF which is a band pass filter and then
we have the different chrominance CSS
has a low pass filter and depending on
the chrominance so this is basically the
green red channel and this is the blue
yellow channel there's a different in
perception based on these colors
and we applied this model to to our
images that we got from the first
experiment and it it worked out pretty
well so we've other the J&amp;amp;E images were
on about the same level for the visual
noise value that was up put it out of
out of this approach and well that was
very nice thing so we finally think we
have found an algorithm that really
works one problem we have with that is
that it may need some improvement for
the encoding so we're not really sure if
the opponent space is the right one we
may be able to improve the the quality
of the results but that needs some for
further experiments but for the time
being it works ok for for our
applications so we do that kind of
evaluation for three years now about
three years now for all the camera
testing that we perform in our lab and
we usually test about like 250 different
cameras per year so we've done it on a
huge variety of different cameras so far
and it turns out to be very consistent
with the perceived amount of noise that
we see in images in order to make it
work we we've implemented in our
software that we use for the system
analysis and here you can select the
different viewing conditions which you
can choose yourself these are the three
ones that we typically use so one is
looking at a monitor at from half a
meter distance that monitor has a
resolution of 96 pixels per inch this
one is for looking at a small image with
10 centimeters in height and another set
is for images with
what do you centimeters and I'd so like
looking at different sizes and this is
basically the weighted sum so the the
factors that we use for that and these
were evaluated by the quanta camino de
Guise and seems to work pretty well so
this is what these curves look like the
interesting thing is you can not only
apply it to the to the noise chart that
the ISO standard provides but you could
also do the same kind of evaluation for
target like this which is basically the
colorchecker SG that you can get from X
right and you you can find for example
with this camera that we've analyzed
here that this camera has some problems
in the light red tones with the amount
of noise that you see there okay so that
is how we measure noise today come into
the second aspect that I mentioned we
found that of course if you look at
today's cameras the amount of pixels
which we call sampling rate it's not the
resolution the sampling rate of today's
cameras do not necessarily lead to
better detail in images and we've there
was a little initiative that I started
about a one and a half years ago called
6 megapixel which is actually not about
the number of megapixels but about the
pixel size itself and the bigger pixels
you have the less noise of course you
have and for today's cameras this is for
example a 6 megapixel camera this is a 7
8 and 12 megapixel camera and if you
look at the at the details of this image
here you see that there is actually more
detail in this image than there is in
this image of this 12 megapixel camera
and that is
mainly because due to the amount of
noise that is in the the lower rev lower
right image the manufacturer had to do a
noise reduction and that noise reduction
leads to a loss of details low contrast
fine details in images similar image so
you see that these are looking a little
dark on this screen here but you can you
can definitely see that up here you see
much a much higher detail level in that
6 megapixel image then you see in that
12 megapixel image down here or also the
while the 8 megapixel camera wasn't
wasn't all that bad but this 7 megapixel
camera wasn't wasn't that good so
there's obviously some image processing
going on that leads to a loss of fine
detail in these images the problem was
that we couldn't find that loss of
detail in our resolution measurements
I'll show you in a second how we do the
resolution measurements so doing and
doing the resolution measurements a
typical way we got a good resolution
value because these noise reduction
algorithms that I used tend to leave all
the edges high contrast edges in the
image untouched so they only affect the
low contrast especially high frequency
part of it so our resolution
measurements weren't really able to
reflect that loss of fine details so we
were looking for something to describe
and to measure that loss of of detail so
to do that we looked at all the
different kinds of noise reduction
algorithms we can find out there and
there's a bunch of different
mathematical systems to do that like
averaging median filter V no filter
coring filter wavelet filters in fact in
reality most of the menu
factors use a combination of those and
they don't tell us exactly how they do
it that's usually a proprietary thing
they all work in about the same way so
you you have an input image you take the
low-pass filtered image and then you
take the the high frequency part of that
you modified in a certain way put these
two together and you get an output image
so we try to find a measure to quantize
these lusts of to quantify the the loss
of fine detail in these images and in
order to do that we use this resolution
test chart which basically has the
sinusoidal semen stars that are now
widely used and that are going to be
part of the revised ISO resolution
standard then we have a lot of or a
bunch of slanted edges at different
contrast levels here that is the method
that's described in the iso standard and
we've added some noise patches here at
different contrast level which basically
white noise and of course we added a
gray patch here as well so we just
wanted to have different kinds of
structures in there to find out what
algorithm is the best this is actually
the software that came out of the study
you can find that the detailed study
which is the seizes of a guy called
admin diploma thesis and we've put that
on the website so if you want to know
the details about that you can download
it and this describes different
approaches that we've used to find out
about this loss of detail and these are
the approaches the first one was taken
the sinusoidal Simon star and that
basically creates an
TF curve for that system and this as I
said doesn't really tell us the truth
about the low contrast fine detail so if
you look at this curve it doesn't change
that much for different sensitivities
and it doesn't really show us the
difference for the different cameras the
same thing for the slanted edges so the
edges are usually preserved by these
algorithms and what we find is of course
there is a diff difference between the
different contrast edges but they remain
basically the same at different iso
speeds with different noise cleaning
levels so this doesn't also also doesn't
tell us the the value that we that we
were looking for then of course we try
to find out about the edge a little in a
little more detail so looked at the at
the edge itself and try to find
something about the the width of find
out about something about the the width
of the edge but that didn't lead to a
good thing then we looked at the the
noise level along the edge because we
found that then the edges are untouched
by these algorithms so around this edge
you would find a slightly higher noise
level and if we could find out about the
amount of noise in around the edge it
may lead us to a value that we were
looking for it gave us some information
but not really the one we were looking
for then we looked at the noise patches
so this is the white noise patches and
we took a picture of these patches and
iso 100 with one camera and with at iso
1600 was the same camera
and you see the difference here so you
see what happens to this to the noise
and we try to quantify that using the
the power spectrum of these noise
patches so we basically did the Fourier
transform and then looked at the power
spectrum and this is the power spectrum
of the white noise at iso 100 taken with
that camera of the white noise Paget's
with the different contrast levels and
this is the power spectrum of the the
gray the uniform gray patch up here and
with iso 1600 it was interesting to see
and of course you see it here that you
implement some or that noise is in that
gray patch and of course the software or
the image processing in the camera tries
to get rid of part of that so it's
interesting to see that these power
spectrums get close to each other which
gave us well we looked at first hand
somewhat promising but we had a problem
so we did that experiment on a on a
bunch of different cameras and found
well there are some cameras that
introduce artifacts so you can see it
here already in a in a way so you see
these this black spots around here in
the in this noise patch and there were a
couple of cameras that introduced even
more artifacts to that and that of
course effects the power spectrum here
and we were not able to differentiate
between the artifacts and the and the
white noise of the of the target and the
noise reduction and that was the problem
we had with this approach so we then
look at
a different thing which we call the
grade-level co-occurrence matrix so
basically if we have if we take a
picture of a white noise patch the white
noise would assume that if you take one
pixel it can have the code values from 0
to 255 and if it's white noise you don't
know what the digital code value of the
neighbor pixel is it can be in the whole
range from 0 to 255 if it's white noise
it should be in the in the whole range
and depending on how even the image gets
of this white noise patch you can find
that there is some noise cleaning going
on in that image and this is
demonstrated here by these images so if
you would have so these are the code
values from 0 to 180 that's what the
range is that we show here but basically
it'll it'll drain from 0 to 255 and
these also from 0 to 255 and if it would
be white noise you would have a black or
dark blueish image with no concentration
at a specific position but at iso 100
100 of this camera you can also see that
there is a slight concentration towards
the diagonal line and at iso 1600
where's a higher concentration and
basically everything is getting is
getting gray you see how this
concentrates in the center of this of
this diagram here the problem is how do
you convert that into a single number
because that's what's what people
usually want a single number what is
the loss of texture in the image due to
noise reduction so it's hard to come up
with a single number out of this
experiment so that's why we looked at a
another way and that's about the same
approach so we looked at at the digital
cult values in here and in here and we
basically use the derivative of that
image in order to get rid of exposure
differences etc and what we came up was
with this graph here so the derivative
that the histogram of the derivative of
that image should have a Gaussian
distribution because of the white noise
and everything whenever it's not a real
Gaussian distribution we know that it is
not exactly white noise that we see in
that image and there is an interesting
way to measure that that is the
so-called kurtosis value and you find
this kurtosis value in automatic pattern
recognition and and these areas of image
processing so kurtosis is already used
in some parts of imaging and we just
applied that to this application here
and this is basically the iso 100 image
the green line you see is from the gray
patch that didn't have any noise in
there and the others are the noise
patches at different contrast levels and
this is the ISO 1600 image so everything
moves towards the center to ma 20 here
and the amount of that you can see for
the amount of that you can see the
kurtosis value here very calcium is it
one it's zero
yes so this is basically how it's
calculated and how we do that and this
is the formula to do it so it's
basically the fourth power of that image
so it's such statistical value but it
reflects very well how far we are of the
Gaussian distribution that should be
there and this is the the image that
you've seen already with the kurtosis
values in there so that is an
interesting approach and we've done this
on a variety of different cameras now
and it seems to work pretty well the
only problem we have is of course
depending on the algorithm that the
manufacturers use they are doing this
noise cleaning from a certain contrast
level on and it's the the tricky thing
he is here to select the correct
contrast of the white noise to have this
affected by the noise cleaning algorithm
so we in former times used to contrast I
was little too high and we're reducing
that a little more and then we will have
a very good kurtosis correlation with
the degrading of the image actually
there was a paper which I didn't into it
here in this presentation given on the
electronic imaging conference that talks
about the correlation of this approach
and the image perception there was a
like physical experiment done on images
that were degraded and it was combined
with the results of the kurtosis
measurement and there's a very very high
correlation it was point 99 so you can
you cannot get a higher correlation
value for for these things there's one
other approach that is
that was presented by the guys from dxo
labs in france and they use basically
use the dead leaves target so which is
in this case the dead leaves is the
approach so they they they took it over
from some other applications and they
simply use circular structures at
different sizes and different code
values and combine these on top of each
other in this image and then they
basically do the same thing they
calculate the power spectrum as we've
done with the white noise already and
from that of course since everybody
wants to have a single number they
compute something and that is the so
called acute ins value which you can see
here so that is also well then sharpness
as a word is one of the nessus and it
basically relates to the perception of
an image so we wanted to use a different
word actually this is coming out of the
CPI Q initiative the cell phone image
quality initiative and there they wanted
to have a measure for sharpness and what
the idea right now is to come up with
the acute ins measure which is basically
the MTF curve of our system multiplied
with the contrast sensitivity function
for a specific viewing condition and
divided by the contrast sensitivity
function itself so it's the kind of
filtering that we do for the visual
noise analysis as well and this leads to
a single value and these guys from DXL
applied that to the power spectrum that
they thought
which is an MTF coming out of the dead
leads target applied that to to the
power spectrum and wanted to have or use
that number that acute anumber on the
images as a measure for the loss of
texture in order to find out whether it
works or not we took a bunch of cameras
and photograph the same scene as you can
see here so there's a white noise patch
on the right hand side and the dead
leaves on the left hand side and to show
the differences on real scenes and real
images we put a green grass patch over
here some stones over here some bricks
and let me show you some of the results
so this is taken with a camera that had
a low resolution due to a low pixel
count so I love something rate the
measure was a low kurtosis value so it
didn't do much nonlinear image
processing and but it had a high acute
enciso the sharpness was very good but
it didn't have much texture in here
because of the low the low sample rate
so the low resolution of the overall
system that's what the power spectrum
looks like so it is quite high in the
low frequencies and then degrades but
here even at the Nyquist around the
Nyquist limit you have a certain
contrast amount of contrast visible in
the image this is a different camera
that has a higher pixel count but it was
simply out of focus so it as you see
here these are blurry images with your
false leads to a low cortosis value
because it's not a nonlinear image
processing that was done on this image
it leads to a low resolution measurement
using the semen star or slanted edge
approach and it also of course leads to
a lower cute and
value because it's not much in here and
the out-of-focus thing applies to the
dead leaves as well too as to any other
edges that are in the image oops then we
found what we applied it to a set of
camera at different iso speed settings
we found a little it's not a real
kurtosis problem here we found that a
higher noise level of course reduces the
kurtosis value because the noise when it
adds up to to everything that is kind of
Gaussian distribution and of course in
levels of kurtosis but it also leads to
a higher noise measurement in that image
so it's somewhat taken care of by the
other measurements and these have a
medium acute ins value and an average
resolution a problem with the dead
leaves approach is that sharpening of
course changes the acute ensue as you
can see here so that this was taken with
a camera at ISIL 320 and this was taken
with the same camera at ISO 640 and what
you can see here is you have a slightly
higher noise value but you also have a
significantly higher sharpness so
there's just this is just a sharpened
image it's not that there is additional
texture in there but you have a
different acute ins for these here so
the sharpening of the image effects the
acuteness value and kurtosis isn't that
much different actually and of course
you can see that from the power spectrum
so this is the power spectrum of the ISO
trillion 320 and this is the power
spectrum of the ISO 640 and you can
notice that the that you reach a
contrast level of 1.2 hear enough about
zero 1.0 in
this other image so that means there's
not a difference in texture in that
image but there is a difference in the
acuteness measure which causes some
problems so both of these approaches
kurtosis and the dead leaves are
somewhat scale invariant rotation
invariant but for both of them you have
to carefully select the tonal range that
you have in your target and the dead
leaves is somewhat sensitive to
sharpening and artifacts that are
introduced so one other aspect I didn't
mention so far is of course you can vary
the sharpness here but if you have high
frequency content in that in that power
spectrum you also do not know whether
this high frequency content comes from
artifacts that are implemented or
introduced into that image by the North
cleaning or whether it comes from the
real original image and that is a bit of
a problem so you cannot differentiate
between these and kurtosis of course the
curtain kurtosis does not really make
sense as a standalone value so you have
to look at different values like noise
and resolution in addition to the
kurtosis value to get the complete image
of what happens to texture but if you do
that the it's the interesting thing that
the croutons it only describes the loss
of the fine detail in the images due to
nonlinear image processing to noise
cleaning algorithms that are nonlinear
and this is indicated here so if you
have a bad sensor and no noise filtering
at all you will end up with a high noise
level in that image you will end up with
a typical or good resolution value and
you will end up with a local tells us
value which is good and for the power
spectrum
we also a low value or you measure a
high value for for the resolution and
acute ins maybe and if you have a bad
sensor and linear filtering you will get
a low-noise value but you will also get
a low resolution value because the
linear image processing affects the
seamen star and slanted edge MTF and you
will get a low kurtosis value because
the you pretty much keep the Gaussian
distribution of that white noise but you
will also get a low power spectrum then
if you have a bad sensor and non linear
filtering that was the problem that we
had so we had we got a low-noise value a
high resolution value but we had a loss
of texture so we had to find a method to
just measure the loss of texture for
this non linear filtering and that the
kurtosis does very well and for the
power spectrum it's a bit of a problem
because you do not you're not able to
differentiate whether the artifacts
etcetera that you have there come from
the original or from the image
processing and if you have a good sensor
of course you get a good value for for
all these different aspects yeah that's
about it what I wanted to show thank you
very much and if you need some
additional information there's a lot of
things available on that website and I'm
open for questions so it seems to me
that there's a lot of interesting good
points to all these measures Muslim
don't take into account what the target
is that you're taking a picture
that is they don't kind of measure the
signal we protect write me a little bit
of power spectrum or a noise or some
statistic that doesn't recognize what it
was there taking a picture yes would
incorporate that help you make a more
meaningful they signal to noise where
there's really a signal there or
something like that yes on the one hand
if you know the signal and if you would
have a signal in these areas it's it
might help but it's somewhat difficult
to to compute and do the analysis for
that so yeah having that white noise and
coming up with this kurtosis is kind of
an approach where we basically know what
the signal is and what it should be like
we don't know the exact position of each
digit code value of the pixels but doing
the statistics or not analysis on that
really helps and delivers the value that
we were looking for yeah and for the
SEMA start we do that so the seaman star
is the best method actually to get the
the resolution or the MTF curve the
contrast depending on the frequency
because we know for each radius of the
seaman star we know the the frequency
that we're looking at and we're fitting
in a sine curve that has exactly the
frequency that we're looking at that
added at this specific radius and that
way we can get rid of all the impacts of
aliasing instead of ayodhya frequencies
yeah same camera same lab testing but
mini smash
whether you were getting the same number
where it is very number we pretty much
get in the same number so it's always
surprises us since we produce this as
test equipment and also sell it we have
a variety of different labs using the
same approach in the same setup and it's
interesting that within the labs we have
we typically have a variation that of
the results that is less than five
percent and in our internal lab we very
often have a variation that's less than
a percent which is surprisingly good for
for image analysis and if we have a
higher variation we found in the past
that it usually depends on capture
Pacific things so that somebody has set
up the camera in a different way or
somebody was had a problem with things
so for example they didn't fix the screw
on the on the on the camera Stan or
something like that so we if we find
that and we had a very very big problem
in the past i'm still have it with the
DSLRs due to the well very very bad
autofocus system that most of these
cameras have so if you shoot for example
10 pictures in a row and you look at the
images and the variation of that images
it's well I know why a lot of people are
complaining about sharpness in digital
SLR cameras and if you talk to those
Service guys from Romney Concannon he
said well it's interesting about eighty
to ninety percent of all the problems
they have is according to sharpness and
and resolution so the focus aspect of
the camera
to switch off the camera image
processing technology and tested for a
selected range of cameras we have yes so
for most of the DSLR cameras you'd get
pretty much raw images if you should raw
and then you processes for example that
was DC raw and what you get from there
is well the image may have some
processing but all the noise correction
tonal correction color correction
etcetera hasn't been done at that point
eight bits right the Apple images are
typically eight bits the raw images can
vary from a to 16-bit and even higher
than that for specific camera so since
we are also looking into things like
cameras for security devices and and
automotive application these cameras can
have a higher bit depth even higher than
16 bit
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>