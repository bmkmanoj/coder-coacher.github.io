<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hidden Topic Markov Models | Coder Coacher - Coaching Coders</title><meta content="Hidden Topic Markov Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hidden Topic Markov Models</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vgqWMGT9haY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so thank you all for coming to this talk
and luck thanks for introducing me I'm
going to tell you today about a model
for text documents which we call a
hidden Markov model and I'll just say
that this is a joint work with Michael
Rosen's view who was a postdoc at the
University of the time within this work
and with your Weiss who is a professor
at the University and is also a PhD
advisor and this work has been published
in AI stats so let's begin I will start
by a I will start with an introduction
telling you a little bit about problems
related text documents I'll define the
problem of latent semantic analysis that
we want to solve and I will tell I will
review a little bit Oracle models for
text documents then I will present the
HTML power model HTML stands for a
hidden Markov model then after
presenting the model I will describe an
algorithm for linear coloring in this
model meaning to estimate the parameters
of the model and then I will show you
some results we really have cool results
so it's worth fighting to the end and
I'll summarize ok so you know there's
many computers today and or many texts
out out there and there are many things
that we might want to do with text one
of them is information retrieval which
is really something I don't mean to talk
about here at Google you know a lot it's
much more than me we have some query we
have a set of documents and we want to
retrieve the documents that are most
relevant to this query
another problem when we think of is an
automatic classification and
organization of texts and one example is
when we get an email we it will
certainly be useful if some automatic
tool will start whether it is there a
spamming email or whether it is
something like we actually want to read
another thing that could be handy is if
the emails that we receive will fall
into folders automatically without
without us having predefined the
explicit and exact rules for each email
to map into a certain folder another
thing that could be nice is topical
segmentation if we have a long text that
discusses several
topics it could be nice if we can just
partition the text according to the
topics that are being discussed this
also relates to summarization of
documents maybe if we could then
partition documents into several
sections then we can summarize and find
some key words to present each section
and there is also the task of automatic
translation which is very hot here again
something we know about about much more
than I do than I do and there are many
many other tasks that we can think of
and something that is related to all the
tasks that I've mentioned is that if we
can somehow capture the semantics of the
document if we can somehow really figure
out what is the topic is talking about
it might be helpful to perform each one
of these tasks so let's say what we mean
by by semantic analysis of text
documents so I have a set of text
documents and I want to figure out what
these documents are actually talking
about this means that I want to
associate with each document and several
topics that that the topic deals with
topically can deal with more than one to
a a document can deal with more than one
topic for example if we have an
interview with the president he might
talk about Foreign Affairs then you
might talk about the economy then maybe
about some other domestic issues so it
may be more than one it may be more than
one talking a single document and we
also want to figure out what's the
meaning of the words to be seen in the
corpus that we have we want to assign
each word to different topics and we
keep in mind that word can be can have
several meanings and it can be related
to more than one topic as well if we
take the word bank for example then it
can be the river bank or it can be the
place where we put our money so we have
these two problems we want to assign a
mixture of topics to documents mean that
we want to assign probability of topic
to each document this will also tell us
what is the proportion
is topic within the document and we want
to do the same thing for for the words
that we see in the corpus that the
problem that that we're talking about is
totally unsupervised which means that we
are our input is only a set of documents
there are no levels no one tells us
anything about the relation between
these documents or between different
words and we don't know what our topics
that we are actually looking for
so topics are not predefined I don't
know to tell you I adult I'm what is
what are the topics that I'm going to
find and after I perform this semantic
analysis only then I can see what are
these topics by inspecting the words
associated with each topic that's how
we'll figure out what are the topics
that they found and see if it's
meaningful or not however I do have to
in in my work I do have to specify how
many topics they I'm expecting to have
there are there are other works that not
discuss this issue and and tell how to
find the best number of topics in some
sense so there there has been a vast
amount of work about semantic analysis
in text documents and it will be
impossible even mention all the works so
I'm not going to try to I will also make
I will only mention a few works that are
related to mine so the roughly speaking
you can you can talk about an approach
in models for text documents that is
called bag of words the backwards
approach represent each document as a
account of the number of words appearing
in in these documents that means that
here you take the corpus that you have
and then you prepossessed your text
documents you can count how many times
each word appears in each document and
after you count the number of
occurrences of words in in the document
you discard a document and all you keep
is the number of words is the count of
words that appear in this certain
document so usually a document
can contain only a small number of words
from the entire vocabulary so this is a
sparse representation of the text
document which is quite efficient you
don't have to keep all the original data
you have a competent Sports presentation
however you lose are a lot of
information by with this representation
because you know no longer have the
structure of the document and we don't
see what are the relationships between
words within the document so there is a
work from the beginning of the
nineteenth called latent semantic
analysis or LSA that started with this
pre-processing they built matrix of of
document term occurrence and then they
used tools of linear algebra SPV single
operand decomposition to map documents
into the space of topics and really the
same words later on in the end of the
night means there is a work of Thomas
Hoffman called probabilistic latent
semantic analysis in which they will he
presented the graphical model a
statistical graphical model they
described a generative model to create
the documents in the corpus I'm going to
get into this work in details and a few
years later this Dave Blaney's the our
quarters presented a work called lake
induced allocation or alley Airy they
introduced jewish-led priors to try to
get the parameters of different
documents I will also describe this work
in more details and there are many
extensions to the LDA work for example
there is the work of the car wasn't even
collaborate was about how to incorporate
the author information into the semantic
analysis and there many works that
incorporate other sources of information
there's another work called that builds
on the early age and that integrates
syntax information into the semantic
analysis which is a hidden Markov model
to represent the de prata syntactic
dynamics of documents so the last work
is not a bag of words model because it
really has to follow the entire document
what's going on terms of syntax so my
work is not a backwards model and the
Magna the main motivation is that when
you have a text document then usually
text documents tend to be current when
person speaks were right to document a
it doesn't just randomly pick words to
talk about usually they're if you say
something then towards that are next to
each other or related in some way and it
will be ashamed to to ignore this
information which is what we do when we
when we use the bag of words
representation because once we discard
the original documents we have no way to
know what was the distance between two
words we can never know which the words
were just adjacent to each other so the
main motivation for my work is to keep
the original document and then somehow
to use the information that we have from
the adjacency of the words or of
sentences and because we keep the
original document and we don't don't
just count the number of pairs of words
within this document we have an explicit
presentation from each instance of word
that we actually see in the document so
we can distinguish between different
instances of the same world something
that in not appear in the same document
something that is impossible in models
that just come together and are there
instances of other appearances are
forward within a document and then you
no longer have a way to figure out
what's the difference between two
instances of the same word according to
the context so that's the main
motivation
my work builds on aldehyde and on
probabilistic latent semantic analysis
so I will describe this works in details
before I I'll get to my work so the
probabilistic latent semantic analysis
which is also called the aspect model
presents a generative model of how to
how to generate the corpus that we see
this means that I have to tell you how
to generate each word in each document
and this is the true beauty of monistic
process when if I want to generate the
word W in document D so first the one I
want to pick the document so I pick a
document D which probability with a
certain probability after I've chosen
the document and I want to generate the
word I have to I have to decide first
what is the topic that we want to talk
about so after I've I've chosen the
document I pick a I pick a topic
associated with its document we that is
drawn from a distribution a multinomial
distribution that is associated with the
document we will denote this
distribution by the vector of theta it
is a vector of probabilities associated
to the document T and each one of the
possible topics that will note by Z has
a certain probability and after we
decide what is the topic we're going to
talk about we choose a certain word that
relates to this topic and each word as a
probability to come from this topic so
for each topic Z we have a vector
probabilities Phi and Phi Z of W is the
probability to generate the word W after
we have chosen the topic C we can
describe this generative process with a
graphical model first of all I have
there is a disclaimer that's not a way
that Hawkman presented the the graphical
modeling is work but it is equivalent so
I don't know if everybody here is
familiar with graphical models so say a
few words about what this picture
actually actually says so we have the
random variables that are inside circles
for example this is Phi which is
well that's not a good example let's see
this is the topic it's random variable
so it is inside circle and the errors
between the circles represent
conditional probabilities when I have an
error from theta to Z it means that Z
depends on theta in this generative
model second fire considers parameters
and others random variables but I'm
working it this way because that's where
the way we will treat these parameters
later on in stock so if I want to
describe the generative process that I
showed you in the previous slide using
graphical model what I say is that for
each document I have to generate a theta
after a at least theta for each word I
have to draw the randomly from the
distribution theta and after I've chosen
Z I pick a word W from the distribution
v that is specified that is specified by
Z so again theta is the mixture of
topics that are assigned to a certain
document Z is the topic that I pick for
a certain word W is the word that they
generate from this topic W is shaded
here because it is observed and we know
the value of W we don't know the value
of any one of the other variables and v
vector probabilities that are associated
to the topics and they tell us what is
the probability to generate each word
after we have chosen the topic and there
are also direct angles in this picture
and the rectangles which are called
plates represent repetitions in the
process that I showed you in the
previous slide we describe how to
generate all the words in all the
documents that we have so this inner
plate represents how to generate all the
words within a certain document so we
film document I have a single theta and
then I repeat the process of picking a
topic and and picking a word and times
where n is the length of its document
and I'm Pete I repeat this all thing D
times for D documents in the corpus -
that's the outer plate and I also have
many topics if I have K topics in them
in my analysis then I have the
capability vectors Phi so that's why I
have looked like to run Phi so the
difference between this representation
and way that Hoffman presented its work
is that and the way common aid the way a
Hauptmann present his work the random
variable there is a random variable the
associated with document and theta and
fire actually parameters but I'm going
to keep this representation because
that's the way we're going to sit in
tonight in their recipes of the table
then after a after othman presented this
model he suggested to use an e/m
algorithm for Larry for learning in this
model this means for estimating the
parameters theta and Phi and the EM
algorithm at the problem of overfitting
then he suggested that a temper TM
algorithm and I'm not going to get to
any more details a few years later they
suggested the the latent original
allocation model which ties together
parameters of different documents to to
overcome the table everything and to
have a more consistent the probabilistic
model so I'll skip the verbal
description I'll just go to the
graphical model so in the previous a
graphical model that we had we didn't
have these parameters better than alpha
the part and the main difference between
this work and work of Thomas Hoffman is
that we now consider the parameters
theta and Phi not as parameters but as
run variables that means that there is a
prior placed on these variables they
come from a certain distribution so by
relating these variables interest to a
certain distribution which is count to
all the Thetas we actually tie together
of Thetas of different documents and we
have a more consistent way to describe
our to describe our
a corpus and this also helps us to to
cope with the overfitting problem
so if you want to specific we need to
specify a certain distribution over the
Thetas so a natural choice is going to
be the Jewish slave distribution because
the different distribution termites from
unity vectors and from the mathematical
point of view the distribution is
conjugate to the Manta one to the
multinomial distribution which makes all
the math much more simple I can say in a
few words that when I said that the
Jewishly is conjugate to the multinomial
distribution what this means that if you
take the posterior probability of theta
resilience multinomial come from theta
and alpha li the developer it means that
the posterior distribution of theta is
going also to be multinomial which makes
everything simpler so the specific
demand is just to tie together all the
parameters using some some parts on the
parameters so we in a similar way we
have the bezels that do flip cars on
their probability that was Phi and in
this way we just make some simulation we
make a strong interaction between
different documents then the algorithm
that was suggested in the LD egg paper
was tears burgers on am nowadays are
also other approaches there are
approaches of collect schemes give
something to to the LD a model and works
the mill on it and there are several
options of how to do inference in this
models so in the previous slide I use
again the the plates impatient which I
remind you means that we just repeat
over and over again the process of
generating generating work inside the
document I can unwrap this
representation and write explicitly the
way that that a document is generated in
this way well this model is exactly the
same as the previous one
I just unwrapped
in a plate and I have the sets of words
and topics one X to the other and you
can see that there is no way there is no
direct connection between the topics of
different words inside the inside the
document this is exactly the the problem
that we want to solve in my work we want
to relate different words inside the
same document so in the height in the
HTML in the topic Markov model we
started as a sudden introduction
documents and tend to be current in some
way if I have to concede consecutive
words or two consecutive sentences there
is a very good chance that that these
words or sentences actually deal with
the same topic so I started two adjacent
sentences have the same topic with high
probability which I denote by 1 minus
Epsilon but from time to time we do we
do have a topic transitions within
document but this doesn't happen too
often so we said that this happens with
a low probability which we denote by
Epsilon and we want to represent these
assumptions about the coherence of the
document using the Markov chain so I'm
going to represent the document with a
Markov chain over the topics and say
that there with high probability two
topics of consecutive words is going to
be the same and with low probability
it's going to be different since I'm
keeping the original presentation of a
document with the structure of this
document I now have an explicit
presentation of each appearance of each
word so I can now try to figure out what
is the meaning of a certain word from
the concept from the context of this
world
so if I want to describe the generative
model and first of all I have to draw
the parameters same as in the LDA but
that's not the focus of my talk and then
when I get to to create words inside the
document for each documents first of all
I have to choose the topic of this world
so I have a variable side which says
when I want to generate the next topic
whether it is going to be the same as
the previous topic or is it going to be
something you so the first stage it is
to decide if size is going to be 0 or 1
if I'm going to drown new topic or to
copy the previous one and this is the
loan from a binomial distribution with
the parameter epsilon with which revenue
which very low probability I'm going to
change the topic otherwise I'm going to
keep the same topic so with I probably I
simply copy the topic of the previous
word and otherwise I pick a new topic
which comes from the distribution of
topics in this document from the theta
after I've chosen the topic of death of
the word then I want to a I want in this
way we describe how to create standard
cells where other words within a
sentence at the same topic so after I've
chosen a topic for a sentence I have to
generate each one of rewards in this
sentence so for each one of the words
are just a lure word from multinomial
specified by the topic that I have
chosen in the previous stage so WI comes
from the fact that is they correspond to
the topic that I just chose
if you want to look at the picture of
this document we now have remember how
the LD unwrapped looked like I'll show
you the previous picture
that's the unwrapped LD a so the main
difference is that now we put links
between these notes will relate them
because we say that the topic of the
n-word depends on the topic of the N
minus one word so that's the read errors
and that's the the main the main point
of this work in order to perform
inference into the fennekin
consistent a probabilistic model we also
have some other variables the size that
tells that tell us whether I'm going
simply copy the topic of the previous of
the previous work or not or whether I'm
going to draw a new one and this size
I've drawn from a binomial distribution
with the parameter epsilon so again the
generative model is first of all create
the first topic in the in the document
from the distribution theta I always
knew I do I always draw a new topic for
the first word then we don't want to
generate the second word then I have to
decide if I'm going simply to copy the
previous topic or to generate a new one
to decide this I have to do a variable
psi which is zero with a high
probability and 1 minus epsilon 1 with
probability Epsilon if size 0 I just
copy the previous topic size 1 I donate
a new topic from theta so that's the way
talks are generated so we have this
Markov chain that describes the dynamics
of topics between the document and and
we now have explicit representation of
each word in document whereas in the
previous models these words were not
were not actually the instances of words
appear in the document the real they
were only the the terms of the Pyrenean
document associated with counts
so now once I have this model I can ask
a few questions one question is what is
the topic associated with each word this
could be useful if you want to do
topical segmentation of the document
because then I can see if this say I can
see what are the topics that they I
obtained and just look for the places
where there is a transition in topic of
the document and say after it is one
segment and from this point and on this
is a different segment and so on I'm
going to show you some examples for that
so maybe it's going to be maybe it will
be better explained with with the
examples another question that that I
can ask suppose they want to do some
translation and I still word it has
several meanings and I'm not sure which
one is the meaning that is the middle of
the word of this instance and don't know
exactly how to translate it so then I
can maybe inspect the topic that that I
found and choose the appropriate
translation according to the topic that
I found another question that we may ask
is if I want to do information people
maybe want to know what are the topics
that a document is Scott and talks about
and this is actually captured with the
booth in their parameter theta that
tells me what are the proportions of
their of the topics within document the
the topics themselves are represented by
these probability of actuals five that
specify what's the probability of which
of each word to come from a certain
topic so if I want to figure out what's
the topic that I I got I can just see
what are the most probable words that
are associated with this topic so this
model allows us to answer many questions
and and the algorithmic question is how
to find out all these variables how to
do inference in this model so like in
the previous models exact inference is
intractable that's why we have to
consider an approximate eternities for
approximate inference and we actually
derived the
and two algorithms for approximate
inference in this model one of them is a
collab script sampling where we sample
these quick variables which are disease
and size and we integrate over the other
variables and the other option which is
what I'm going to focus on is the e/m
algorithm and that is quite similar to
the IAM algorithm in previous works only
that we have the inference in the market
chain so very briefly the EM algorithm
is an iterative algorithm that it's an
attractive algorithm that distinguishes
between two sets of variables as it's
like in variables over which we average
and returns parameters for which we find
map estimators and in our work we have
chosen the parameters to be theta fine
epsilon and the latent variables are
Disney and size and the the results the
result that we get from the EMA
algorithm is the map estimation for the
parameters and we have a distribution
over the latent variables I won't get
into the details or just say that M
algorithm is very straightforward
the step consists of four backward
algorithm and they am step four the
m-step we have close from formulas and
the details of not are really not that
important interesting and I just show
you some results that's the interesting
part of the talk so what I'm showing you
here is there is a part of text document
and I train my algorithm on name
actually on a on a set of documents that
are papers that were published in nips
the conference a conference of machine
learning in neuroscience and and I
perform the thematic analysis error I
want to show you what are the topics
that I found so I have a section from
the beginning of a paper
and they have a section from the end of
the paper this is the abstract that
appears in the beginning of the paper
and that's the end of the discussion and
then the acknowledgments and the
references and the colors that you see
in this picture correspond correspond to
the to the topics that we found what's
that are colored with the same color
we're assigned to the same topic where
where is different colors represent
different topics so if we start reading
the abstract we see that it discusses
something mathematical it talks actually
about support vector machines so our
algorithm just filled out that all these
the words correspond to this time to the
same topics and it is time to all of
them same the same topic the same color
and this section keeps on going I didn't
cut all of it but it keeps on with
within the same document if you now take
a look at the end of their paper even
though the world several topic
transition within the document you can
see that the end of their discussion
which is also related to their support
vector machines and was also assigned
exactly the same topic that's why it is
a code with the same topic if you want
to see what is this topic then here I
have the list of 20 words with the
highest probability to come from this
topic and you can actually see that this
world represents something mathematical
it actually it actually captures then
the topic of support vector machines you
see the world effect on you see the word
kernel you see other words that are
related read supporter is also linear
separation and all these stuff that is
the trill related to support vector
machines if we continue looking at the
end of the document
we see that at the place where the
discussion ended and I and
acknowledgments began our algorithm
managed to find that there was a topic
transition and it assigned the
acknowledgement section a new topic and
that's without defined any rules for it
it was found automatically so we see
that this
action as the red color and if we want
to see what are the most probable of
words for the red column we see that
these words are actually related to
acknowledgments you see twice the world
acknowledges with different spellings
maybe even three times you see other
words that are related to this topic
such as research grant funding
institutions that give you the grant the
NSF then you see the Institute navel and
so on so it's really nice that we could
capture the topic of fair
acknowledgments even though it appears
in only some of these documents not all
of them of acknowledgments and it's a
very short section within the documents
it's only one line still managed to
capture it now if we take a look on the
on the next section we see that once the
references started then a new topic has
been assigned the green one and if we
take a look to see what's the green
topic then we see that our names of
publishers like Springer York Berlin and
other ones and you see if you keep
looking use the names of people that
published for example Hispanic vapnik
you see a names of journals and you see
that you can see the word press and so
on and this topic was also captured just
just because it it consists of
consecutive parts if we if we continue
inspecting this example you can also say
that the word support appears three
times in the beginning of the document
in the abstract and and all all the
appearances of the word support in the
beginning of the document are in the
part of the term support vector on the
other end in the end of the document we
also have the the word support but here
it has a totally different meaning here
it is a mathematical term connected to
support vector machines whereas here the
word support the word support means the
the
so help that the authors received from
from in this case from Lucent so we see
if we check what are the topics of the
worlds we see that in the first case all
the three occurrences of the word
support were signed to a mathematical
topic on the other end if we check
what's the topic of the last day of the
last occurrence of support within that
management section then this one was
assigned to the acknowledgments topics
so we see a nice example for a
partitioning of text into segments
cording to topics and for where words as
disambiguation of course this is only an
example it's not quantitative I'm going
to get to quantitative results as well I
showed you some of the topics that we
have there are also other topics and
they look very nice but I must say that
if you just look at words that represent
topics then topics then the topics that
you get from LD our piace also look very
nice so we need some quantitative way to
to compare the works a weber this this
example that you see here for the
segmentation for the word sense
disambiguation is a test that the
previous models could not could not
tackle whereas we can if i will show you
the results of a LD egg for the same
section these are the results that you
see you don't see segments you don't see
a section that that all have the same
color you know modern folk colors were
assigned to two words in the different
sections so i just show you the the four
where most the four most frequent topics
that we had in these sections and the
black words are not all assigned to the
same topic this'll just four that were
assigned to two topics different than
the ones i've showed you so you can see
that in the LDA I don't know if it's
readable with these cows but here it's
the world sounds it has the same color
as the word unique and is
we'll listen where I'm standing it's
hard to read it there is a mixture of a
mathematical terms and of other things
that are not related to math and you can
see that all the fair occurrences of the
word spot actually were signed the same
topics so we cannot distinguish between
the different instances of the world
support within the same document however
if you do take a look at the most
probable words of the LDA it also looks
very nice but it's just not suitable for
the tasks of segmenting a text and for
this and forwards sense disambiguation
so these are very nice results but it's
it's not something quantitative and it's
it country is service based on to come
it can't really serve as a measure to
compare different algorithms and we need
something quantitative to show you I'll
just say that if you want to take a look
at all the the results that I showed you
were obtained with training a set of a
set of a I don't remember the exact
number I think it was fifteen hundred
and something documents from the NICS
corpus with 100 topics if you want to
see all the only one on topics there
they're all available on the web so you
can see it all and not just the topics
that they picked for demonstration and
it can also compare it to the L di
topics
however we we want to have some
quantitative measure to compare it to
and to compare the different algorithms
and the one one commonly such quantity
is a quantity called perplexity that
that captures the difficulty of
predicting a part announcing part of a a
new document after we have learned the
parameters of the of the model from a
training set and after we have seen the
beginning of the document
so the perplexity captures the
difficulty of the problem and the batter
will learn plasticity is going to be
lower so we want to have lower
perplexity
in this graph I show you the perplexity
of three models with respect to
different number of observed words I see
a certain number of words in the
beginning of a new document and then I
compute the perplexity for the rest of
the document so as I see more and more
words from the beginning of the document
the tough season is getting easier
that's why the publicity is decreasing
and in these three graphs I show you the
perplexity of Lda
versus the perplexity of two versions of
the HTML so the LDA
works on different words and as it sees
more and more wrote it it's getting it
gets better very very it it actually
gets low very rapidly whereas in our
model the perplexity is low all the time
but it goes down very slowly and the
reason for that is that in our model
once you see the topic of certain word
then you induce a very high probability
on topics of the next words and you
don't get that much from seeing the next
words that's why the perplexity is going
down what's very very slowly that's
different between the slopes of the of
the LDA so we see that if we just see a
few words in the document the perplexity
of our model is much lower than than the
perplexity of the LDA but as we keep
going and we see more and more words the
models are getting closer to each other
still the HTM is lower Platte City but
the difference here is not very
significant yes
you can yes you can forget you can
predict something without having seen
anything do you have some association
because it's a different model you are
saying different probability if you with
the HD mm you're going to have a better
probability for something that says that
the topics are going to be similar along
the words of the documents whereas in
the earlier you don't have that so even
if you don't see anything
you still get you still get different
probabilities for the two models and
keep in mind that you also learnt the
parameters of the of the corpus
previously with different models so the
estimations that you have for these
parameters are different and if one of
them capsules better the parameters of
the models then then it's going to
influence these probabilities actually
what we do to compute the perplexity we
we we run the we'll run the same
inference on the beginning of the
document and then you get some some
estimation we when you see no words then
you actually do have a theta for the new
document from the Jewish leper and
that's what you work with for the rest
document otherwise you just run the same
inference that you had for all the
documents but you fix the parameters you
fix the parameters that you learned from
the from the trends that that is define
the epsilon and just estimate the new
theta and after you estimate the new
theta for the beginning of the new
document then you compute a probability
of the unseen part of the document
other questions okay so I told you that
I compared you to version of the HTML
and and actually in our implementation
of the HTML we didn't really work and
level of words we worked in the level of
sentences in our implementation we said
that all the words in the same sentence
have the same topic and this is
something that is different from LDN
wanted to see if the difference in the
perplexity is because of the Markovian
assumption or because we simply assigned
the same topic to all the words within a
certain sentence so in order to check
which in order to to check the influence
of this assumption there is a variant of
the HTML which we call this slide the
HTML one which does not assume the the
Merkava instruction document but it
works in sentence level this is actually
some sort of sentence level LDN because
it assumes that the topic of other words
within a sentence is the same but there
is no relation between topics of fair
adjacent Sanders's so that's a plasticy
of the be HTML and you can see that the
perplexity goes down very slow again
that's because if you see a single world
from a sentence you have a very good
idea for what the sentence is talking
about and when you see the next words
within a sentence then it doesn't tell
you that much and you see that working
in the sentence level improve the
publicity in the beginning but in the
end it does worse than a le and a HTML
so we conclude that the Markovian
assumption really helps to reduce them
to reduce the capacity and it's not all
working in their sons level
another thing we another thing we can
compare is how the perplexity looks like
when we have a writing number of topics
so here we have a fixed number of
observed words in the beginning of the
new document and we see the behavior of
the perplexity as a function of the
number of topics I should say that index
in this experiment in the previous
experiment we had the Train stat which
consists of roughly 90% of there the
data and the perplexities average of the
other the remaining 10% of the data and
there's a big variance of the perplexity
between different documents but the
results were average of the improv
laxity so once again we see a similar
picture the HTM is always better than
the other two versions and the number of
observed walls was not it was it wasn't
too big that's why the LDA does well
then VHT mm but if you let the algorithm
sing more and more words then at a
certain stage the picture changes and
then the LDA is better than the VHD mm
but but the HTM is always better than
the other two and an interesting graph
is this one where I show you what are
the values of the epsilon that we found
from the e/m algorithm and I remind you
that epsilon is the parameter that tells
us how often we actually draw a new a a
new topic versus how many times we
simply copy the topic of the previous of
the previous word or the previous
sentence so for my work to have any
value I must have that epsilon is lower
than 1 because if it is 1 it just means
that each time I draw a new topic then
it's completely equivalent to the
previous works that where brave words
works so here I show you the map
estimation for epsilon that we obtained
from the ium algorithm and I showed the
epsilon that we found as a function of
the number of topics so first of all we
see that epsilon is indeed the lower
than 1 and significantly lower than 1
and we also see
that when the number of topics is small
then epsilon is very small but as we
increase the number of topics then
epsilon is getting bigger all the time
and the reason for that is that the
number of topics actually it's it it it
says what's the resolution of the topics
or what is the granularity if I have
only five topics and I run my algorithm
on a corpus which is a set of documents
which are in research papers then maybe
I'm one topic that is associated with
math and another topic associated with
neural networks this is the proceeding
of of nips so it's neural networks and
maybe I will have another where another
topic associated with with the other
thing that is very common in research
vehicles but I'm not going to capture
the the things within each each topic on
the other hand if I have 100 topics then
each topic can catch of a certain theme
within within a field now I have a topic
that corresponds to linear algebra and a
topic that corresponds to a theorems and
or proofs and I have a topic that
corresponds to support vector machines
and I have a topic that corresponds to
the auditory system of humans and I have
a topic that corresponds to the visual
system I have many topics that
correspond to neural networks and I have
topic for acknowledgments and so on and
so on
so if I have very if I have only few
topics then there aren't that many tab
transitions because in the mathematical
section it's all going to be the same
topic but if now I have a certain topic
that talks about linear algebra and
something that talks about theorem I'm
going to have more and more cutting
transitions and that's why epsilon is
increasing as I increase the number of
topics but still see that notice that
it's always less than 1
and okay so let's let's summarize then
the main motivation for this work and
the main claim of this work is that you
actually want to keep the original
documents and to consider the structure
of the documents because it lets you
learn better parameters better
representation you can actually perform
more tasks with your model so by a very
assuming a mark of a structure of
document and keeping the original the
start of it we got topics that are
inherently different from the topics
that you get from Lda I showed you that
we have a topic of acknowledgments there
is no such topic among the 100 topics
that that will found by LDI because this
is a topic that was fun only because it
is a certain section within document and
it doesn't appear that much so for Lda
it's not very important but our model
managed to capture it and you can go
over all the 100 topics and see that
there's nothing like this in the LDA so
we have topics of inherently different
to remember the picture that I showed
you with the colors this is what the HTM
m2 topics look like you see that we have
contiguous sections that that are
assigned same topic whereas in L di you
have you have different you have topics
that have different meaning it it
assigns the topics that make sense but
they're just different to certain words
within the document it doesn't create
contiguous sections so I'm not claiming
that it is better I'm only saying that
it is different
and we also see that having the explicit
representation of the text document we
can now where this model is suitable for
tasks of topical segmentation and for
toss offer and for the task of word
sense disambiguation according to the
context and this model builds on the LDA
and it can actually be incorporated with
other extensions of the LDA such as
syntax information or author information
or other extensions of Lda and all these
extensions are orthogonal to each other
so this is just something in addition
and I just say one word about the
computational complexity of this
algorithm versus the previous ones the
many computational difference is that we
have an inference in a hidden Markov
model in this in this work but we have a
verse we have a special form for the
transition matrix in the in this hidden
Markov model and the transition matrix
actually depends on a linear number of
of parameters linear in number of topics
and not the general quadratic number of
the terms that you might have in the
matrix and therefore it is possible to
perform the third backward algorithm for
the inference in in the hidden Markov
model in linear time and not in
quadratic time so the computational cost
of considering the underlying structure
of the of the document is not that high
so it's really something that you so you
pay a little price in and you get much
more and I'll be able to answer
questions if anybody else yes
if you look at here at Baker 7 the
epsilon pointing out
this one yeah sure and so it seems like
as the number of topics grows that will
say to say 20,000 the value epsilon is
going to go high enough that the actual
transition model might I can imagine
might be useless I think that if you
have the if the number of topics is
equal to the number of words that you
have then you will assign as a different
topic to each word and by looking at
keeping a percent in syllable so assume
that we stay at the sentence level
classification so one topic per sentence
and that's never gonna happen that we're
gonna have more topics than there are
possible combination of those sentences
but my question is might we be
interested in a different kind of
transition model perhaps moving towards
the quadratic number of parameters which
would you said make your hmm I agree
it's interesting to to check what's a to
check what's what happens when you have
a more general transition matrix because
you want to capture correlations between
different topics for example if I have
the full transition matrix then I can
see that I always have references up
after acknowledgments and I can always
do something a bit more sophisticated
and to see it which place inside a
document it happens and I can use these
correlations to say something about
relationships between between topics and
it it can be very helpful for different
toughs that that we have but keep in
mind that if you have the full
transition it is interesting but just
keep in mind that if you are trying to
estimate more parameters then you need
more training data and the problem is
more difficult and there is also
computational cost so it's definitely
interesting but there was a reason why
we did it this way but that leads
directly to the next question which is
what do you think is going to happen if
we trained on a hundred thousand or a
few million documents versus a couple
thousand it's a very good question I
haven't tried so
so I don't really know what to tell you
and with regards to the computational
overhead we don't actually have to bump
up to the full transition matrix we
could try iteratively adding parameters
so transition between these two topics
right that's probability or to any other
topic with some with the remaining
probability mass so in that way we don't
have to use the whole matrix but we
could try to bump up more from this just
one Epsilon parameter you can think of
different ways to extend the transition
matrix yet not to have a full one but
I'm not sure if I understand exactly
what you're suggesting but maybe we can
talk about it more later so any other
questions
can you derive the timing can one derive
the topic here here or relations between
topics like the heav'n all-encompassing
topic which you know breaks down into
smaller ones so this relates also to the
previous question and we don't have an X
in our work we don't explicitly
represent dependencies and correlations
between topics there is this suggestion
of the previous question to have a
general transition matrix not one it is
specified by the parameter theta but one
that tells you what's the probably
different to transit from a certain
topic to a different topic for example
from acknowledgments you will always
move to references and this captures
some kind of correlation between topics
there are also works with other persons
there is the Chorio topic models and
actually there were several works about
this topic in the last year and that
have different presentation for a
percolation between topics and it can be
incorporated with this work but this is
not something that we have done work I
present and this is something these are
things we experiment with in different
ways but it's not part of this work
is it appropriate to assume that epsilon
is going to be constant over the entire
corpus for example the web probably
won't have a constant transition
probability of course it's not concerned
because you know different people and
different authors and to the different
things in but I think that actually when
when I looked at the result since we got
she definitely see differences in the
Absalon between different documents but
we we specified only a single epsilon it
could it would be interesting to learn
an epsilon per document but they haven't
tried out
and why didn't you take in the first
place not considering the for changing
matrix I mean in terms of number of
parameters it seems that you already
have foldable evasion order you already
have number of words times number of
topics in the fire
so that's true that they have number of
topics times number of words which is
way greater than number of topics word
and yet this is very sparse defi so I
think that that gives you an idea for
why you can absolutely learn it because
the the number of words that you have
there is no words in the vocabulary in
many of them don't even appear in the
corpus so that's that's the in the the
intuitive answer for why you can learn
it we got the full transition matrix
then there are two conservation one of
them is the run time and the other one
is the it's the information and both of
them are important and I think that it
is somewhat more difficult to learn the
transition matrix then to learn the
Phi's probably due to the sparseness I'm
not sure that's that's that's an idea
but I'm not sure about it well anyway we
were experimenting with it so I can't
give you a really good answer that was
the reason why we did this way but I
will be able to give you a good answer
only after where we finished experiment
with please stand I see results
any more questions
maybe someone in New York has questioned
this New York end up connecting I don't
know in that case let's thank our
speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>