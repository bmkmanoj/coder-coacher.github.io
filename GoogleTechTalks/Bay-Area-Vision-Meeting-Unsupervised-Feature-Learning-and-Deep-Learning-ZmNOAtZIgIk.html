<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bay Area Vision Meeting: Unsupervised Feature Learning and Deep Learning | Coder Coacher - Coaching Coders</title><meta content="Bay Area Vision Meeting: Unsupervised Feature Learning and Deep Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bay Area Vision Meeting: Unsupervised Feature Learning and Deep Learning</b></h2><h5 class="post__date">2011-04-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZmNOAtZIgIk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the first speaker is and ruing and he's
going to speak about unsupervised
feature learning and deep learning he's
done lots and lots of great work in his
many things I could say but he said I
should just keep it short and let him
get up and give his talk because
research interest in machine learning
reinforcement learning in robotic
control and he'll share some of his work
with us now tell you a bit about
unsupervised feature learning and deep
learning um and I know that most of us
in this room care primarily about
computer vision and but I'm going to
develop these ideas using computer
vision as well as examples drawn from
audio and text let's say you want to do
image classification right what you do
is you take an image and you feed it to
learning algorithm and it learns to
classify it as a motor cycle or not um
all of us notice pictures and really
like this what we do is we actually take
the image and we construct a feature
representation and is that feature
representation that we feed to say our
support vector machine more generally
this type of pipeline is pervasive in a
computer vision and in other domains
where does this sort of low-level
features is what we actually feed to the
learning algorithm in fact if you look
more broadly across computer perception
right envision we don't feed the images
to learning algorithms we feed low-level
features like edge detectors or sift or
other things if you have an audio
application you don't feed the raw audio
you feed instead you know various audio
features to do say speaker
identification and similarly in natural
language processing when you do machine
learning on text you feed various text
features to do the applications you
actually care about given that this
pipeline is what almost all of our
learning algorithm is almost all our
vision in other applications do the
low-level features in the middle is the
primary lens through which our learning
algorithms sees the world and so this
gives them a certain degree of primacy
and what I want to talk about today is
sort of where do these features come
so let's actually look at what we do
right now right in computer vision where
do we get these features from well in
computer vision to state-of-the-art
answer is over the last few decades our
community has spent a lot of effort hand
designing or hand engineering these
features and as a result of several
decades of work we've now found some you
know pretty good features but but it was
a long process inside with us can we do
better than this um that's vision how
about audio in audio the state of the
art answer is of the last few decades
there's an entirely separate community
of audio researchers that's been
designing putting a lot of insights into
a coding out good audio features and
some of these are incredibly clever
ideas like ever CCS - I feel is like
what 20 30 years old is actually very
hard to beat but again you have to ask
you know is there a better way to come
up with features than to have a whole
community of researchers trying to
design these things by hand and for tech
same thing right in fact a good fraction
of NLP research today is just in coming
up with
say better pauses and what appalls good
for well they're they're primarily used
as a feature into other things or name
ng extraction you know you don't
actually use that for semantic role
labeling most of these things are
designed to be used as features for
later learning algorithms um so is there
a better way is there a reserve maybe
better way to come up with maybe better
features and to do so with less of our
time and our grad students time so then
we're going to talk about takes some
inspiration from biology and I'm gonna
share with you some of that like many of
you I tend to treat biological
inspiration with great caution and often
a healthy dose of skepticism but some of
this is actually really cool so I want
to share that with you so it turns out
that the fascinating hypothesis that a
lot of human perception a lot of
biological perception can be explained
by a single learning algorithm okay this
is a hypothesis so this is not really
proof but but let me share with you some
of the evidence for that um and this
idea it's been around for several
decades of recently popularized by Jeff
Hawkins this is in
around these three decades um in that
image that red part of your brain right
shaded red that's your auditory cortex
the way you're understanding my words
now is that your ears are taking the
sound signal and is sending the sound
signal to the auditory cortex which that
little red part of your brain and that
part of your brain is about news is so
this figures out what I'm saying right
processes the sound so what
neuroscientists did throw with our MIT
did was they took a number of animals
and they cut this actually cut the wire
between the ear and the auditory cortex
and rewire the brain so that the optic
nerve or other so that the signal from
the optic nerve gets routed to the
auditory cortex and what you find if you
do this is that the auditory cortex
learns to see okay and this is actually
not controversial this is replicated in
multiple labs in these four species of
animals and these animals can do
behavioral tasks you can do visual
discrimination tones you know with their
older G cortex the same rewiring
experiment also works for your
somatosensory cortex so you can rewire
this to the part of your brain
responsible for touch processing that
will learn to see with a totally
different part of your brain and more
broadly it turns out there's evidence
that so if you look at the auditory
cortex right what that means is that the
same piece of brain tissue can process
sight or sound or maybe touch and if the
same piece of brain tissue can process
sight or sound or touch then move
there's actually one learning algorithm
or move there's one algorithm that can
process sight or sound or touch and can
we discover what that algorithm is just
other quick examples on the bottom is
example of seeing with your tongue this
is a system called brain port undergoing
a TA Charles the way it works is you
this is being used to help blind people
see right so you strap a camera to your
forehead takes a grayscale image of
what's in front of you and it impresses
that grayscale image onto a rectangular
array of electrodes on your tongue and
so that you know a high voltage
corresponds to add a pixel and a low
voltage response to light pixels so like
to the array of electrodes in your
tongue and with tens of minutes a train
we can actually you know I would be able
to learn to see about tongues human
echolocation so you can do this either
by snapping your fingers
there's no yeah this room has good
acoustics it's hard to tell um but by
either snapping your fingers or clicking
your tongue humans can learn to
interpret the pattern of sounds bouncing
off the environment as a sonar and if
you search you in the internet there are
amazing videos of this kid with no
eyeballs tragically there was a kid
whose eyeballs were physically removed
because of cancer but there's a kid with
no eyeballs who can by snapping his
fingers walk around and not hit anything
you can ride a skateboard shoot a
basketball into hoop okay um and the
example just go on and on this haptic
belt that gives humans the directions
ends you can implant a third eye in an
animal learn to use the third eye oh and
and there's actually surprising welfare
of evidence that suggests you know to
some extent you can take any of a large
range of sentences and plug them into a
brain and very often not always very
often the brain will figure out what to
do with it so the question that excites
me is is there some fundamental
principle that underlies perception and
quickly grow without is um and uses the
computer vision of course because that's
what more resolved so just the just as
summarizes when Branton philosophy a
little bit I think there so to to peer
two possible approaches doing computer
vision we all know that the adult visual
system is incredibly complicated so one
approach to computer vision is to try to
directly implement what the adult visual
system is doing so you know can implant
features to capture different types in
various contexts capture relations with
object pause 3d structure and so on and
maybe you can implement a visual system
um or if we think and we can't no one
can prove this but if you believe with
belief so if I believe and maybe I'm
wrong but if we believe that there's a
more general principle more general
computational principle that underlies
perception maybe not even just computer
vision but more general algorithm that
underlies perception may be that
discovering what that algorithm is may
turn out to be much simpler than
capturing you know what the what they
trained the divisional system
- okay so talk about about this research
agenda and of course this research
agenda you know I start working on this
five years ago but now this is an agenda
shared by many research as the machine
learning and I should say they're
probably people that will work on this
for like thirty years um so concretely
it has a problem one opposed can I give
your learning algorithm a set of images
and have you find a better way to
represent images than the raw input
pixels and if you can do that maybe I
can show the same learning algorithm a
bunch of audio and have you find a
better way to represent audio than
enroll waveforms so as a current example
just just a really you know formalize
what I mean by that here's a concrete
instantiation the problem given a 14 by
14 image patch X one way to represent it
is to use 196 real numbers like 196 our
pixel intensity values probably one
opposes can you find a better vector can
you find a better feature vector to
represent that 14 by 14 image patch then
the raw pixel intensity values okay and
we know how the hand design things but
we can you learn something automatically
um and in order to to sort of really
evaluate this framework and sort of
testing the form of machine learning
framework of let's let's actually pose a
formal learning problem so this is a
standard supervised learning problem
where maybe I give you a small number of
examples of cause small number of
examples and motorcycles and then you
learn in the test that I say here what
is that at the bottom a few years ago we
post the the unsupervised feature
learning formalism we call that
self-taught learning in which the
challenge is in addition to your labeled
examples we're going to give you a ton
of unlabeled images maybe random
internet images and the challenges can
those random images down on the bottom
left can they help you figure out that
that test set example on the bottom
right is a car and so concretely one
natural thing you might try to do if if
we can come up with an algorithm is to
take all the unlabeled images learn a
better representation for images and
then use that better representation to
learn to distinguish between cosimo
cycles okay so how do you do that um so
it turns out there's an algorithm that
was originally invented by
neuroscientists Bruno's house and David
view that actually works for this task
so Bruno and David are neuroscientist
and this was originally conceived as a
neuroscience like a theoretical
neuroscience piece of work for how the
brain for how part of the brain you know
visual cortex works and when they
invented it I don't think they ever had
in mind that this would be used for
machine learning but so five years ago
we proposed to use this the machine
learning and then let me share with you
how we actually went about doing that so
this is how sparse clothing works um
sparse coding is an unsupervised
learning algorithm and we're going to
give the algorithm a set of images x1
through XM okay so each of these images
say an N by n image patch like a 14 by
14 image what's false coding does is it
learns a dictionary of basis functions
Phi 1 through 5 K let's say 64 basis
functions so that you should input X can
be approximately decomposed as a linear
combination of your basis function
subject to the constraint that the a J's
a J's are row numbers so subject to the
constraint that a J's are mostly 0 and
then this tends to terms pause okay um
this is my only equation I on this talk
well let me let me actually really
explain this in pictures so this is what
sparse coding does we train sparse
coding by taking a set of natural images
and cutting out random patches and
feeding that to the learning algorithm
what the learning algorithm does is it
learns in this case 64 basis functions 5
1 through 5 64 shown on the Shonan upper
right and you see that those basis
functions correspond roughly to edge
detected correspond roughly to your
edges right like Gabor edge detectors
when you give the algorithm a new test
example what it does is it selects out a
small subset of your 64 basis functions
and it expresses your test example as
combination of just that small subset of
your you know 64 based assumptions okay
and so you can think of this as saying
that that test example X is 0.8 times
you know basis 36 plus 0.3 times base is
42 and so on and because we're using
only three out of 64 these basis
functions this is a very sparse vector
right most of the coefficients are equal
to zero because we're only three nonzero
coefficients um
speaking informally what this algorithm
has done is it has taken your image X
and it has decomposed your image patch X
in terms of what edges appear in the
image right there saying that your image
X is about 0.8 of H number 36 plus 0.3
of H number 42 plus 0.5 of H number 63
and so speaking loosely this is an
algorithm that has quote indented edge
detection and if you look at a vector at
the bottom the a1e up to a 64 equals
that vector of coefficients that vector
right that vector a1 through a 64 of
your coefficients this is this is now a
new feature vector for representing my
image patch and this expresses the image
in terms of what edges appear in it
rather than in terms of the four pixel
intensity values okay
just couple more examples um and and as
you can imagine by taking an image and
expressing the emission by representing
the image in terms of what edges appear
in the image rather than in terms of the
raw pixel intensity values this is a
much better representation right for you
know the various computer vision has we
want to do and just and and then so just
be really concrete right that if you
look at one of those vectors the a1
through a 64 you can now feed that
vector into your learning algorithm into
your support vector machine or other
learning algorithm instead of feeding in
your raw pixel intensity values and
hopefully this is now a better
representation to do you know stand
division tasks okay finally um other
authors I had to run down the shaft
showed that this does it the output of
this algorithm is actually
quantitatively similar on
any dimensions not all but constantly
similar to primary visual cortex early
vision in the in the biological brain so
that was a vision how about audio I'm
going to mostly talk about vision but I
think it's interesting thinking more
broadly about perception so Evans sniff
Evan was a was a white post office
Stanford with AJ McClellan but what Evan
did was a she took sparse coding and
applied it to raw audio data and this
figure shows 20 basis functions that he
learned appliance fast going to audio
what Evan did was he then went to the
cat or the trees just a cat auditory
processing system and for each of the
twenty basis functions he found the
closest match the closest analog ins of
Kathryn biological world reprocessing
system and so the blue functions are
those learned by sparse coding on audio
and if you take the closest matches in
biology that shown overlaid in red okay
and so this is an algorithm that on the
one hand gives a good explanation for
early visual processing which we know
early v1 right is doing edge detection
and it gives a good explanations early
maybe for early order crossing again not
on all dimensions but some surprisingly
many um turns out this work done by
Andrew Sachs students that work with me
what Andrew did Andrew Sachs did was he
actually collected touch data you know
following the way that animals use the
hands in the wild and actually
demonstrated that that in some respects
just the solar model is a good
explanation for somatosensory processing
flow so just mapping this back to vision
right we now have an algorithm that can
invent edge detection gives us somewhat
better representation for images than
the raw pixel intensity values it turns
out that the president is done probably
the most amazing work applying these
ideas is Caillou he's going to talk more
about that instead take this in a
different direction which is Kumiko be
on edges rather on can we come how else
can we extend this so um this let's see
so if you imagine in this diagram that
the lowest level are the input image one
level up you can apply
sparse coding and learn edge detectors
it turns out you can repeat this process
and just as you can group together
pixels to form edges you can group
together edges to form more complex
feature detectors to build more and more
complex features the technical details
of this is with either a sparse version
of Jeff Henson's DPN responsible to
encoder but when you do this let me let
me show you the result
so one layer of sparse coding one that
responds DPN lets you group together
pixels to form edges okay and again just
be really concrete about what this means
this this basis from there there are 24
basis functions here and this one on the
upper left that's an edge detector
you're detecting edges that like what 85
degree angle so what this visualizes is
this says that I have one feature that
says do I have an edge of that
orientation okay when you repeat this
process this is the result of training
the algorithm on faces what you find is
that at the next level up you find
detectors for object parts and another
level up you find detectors for more
complete models and faces so concretely
this is a feature detector that you know
fires whenever there's like maybe an
eyebrow and this is a detector that
fires whenever it sees what like the
side of the nose these are all
visualizations of very complicated
nonlinear functions but this is our
attempt to visualize the process and so
recursively applying this design of them
and you can imagine if you want to do
various things with faces these sorts of
features can be very useful and all this
is learn from a labral data right if you
train his algorithms on different object
types you get different you know the
similar sort of edges then object part
then complete object model decomposition
for example if you train this on cars
you get parts of cars and then more
complete models of cars and if you
change the sort of model on sort of what
multiple object classes then you know at
intermediate levels you get features
that share the amongst object classes
and then the highest level you get
object specific classes so this shows
that these that we can learn interesting
features but what is all good for so
let's talk about about machine learning
occasions um the Hollywood to dataset is
a standard benchmark in computer
activity video video activity
recognition and you know the tosses you
watch a short snippet of a Hollywood
movie and then you recognize whether any
of a large number of activities of the
character you recognize whether I don't
remember the exactly the things that you
know whether someone whether what two
people hug when to do is shake hands
with two kids whether someone's running
activities like that so over the last
few years you know various authors
reference here have developed and used
fairly sophisticated algorithms fairly
sophisticated features to do activity
recognition on Hollywood to win over the
last viewers in substantial progress but
using unsupervised feature learning you
know that you actually get significantly
better results than any of the previous
work um that's images how about audio so
it turns out that you can take the same
source of algorithms and apply them to
audio so what's shown on top is a
spectrogram and so what you can do is
take snippets of spectrograms and apply
sparse coding to to say spectrogram data
and if you do that then it learns to
decompose you know spectrogram snippets
in terms of sparse linear combinations
of basis functions like that and if you
look at the basis functions learned by
sports coding or by the spa's DB and all
these sort of sponsor learning
algorithms this is an example of a set
of the first layer dictionary or basis
functions learned by the algorithm now
you find it many of these basis
functions seem to correspond to phonemes
and so just as sparse coding applied to
images sparse coding applied to images
quote indents edge detection the same
algorithm applied to audio quote indents
phoneme detection apply to speech okay
um and then you can apply this recursive
idea again so this is a sparse DB end so
we'll take the spectrogram throw on top
a layer or spa's DB n nails like that
and then you know learn additional
layers of features on top and again just
be really clear right this is a
this is a graphical model and the way
we're extracting features from this is
we have as input the spectrogram and
then we're going to compute the
posterior mean of each of these nodes at
the top so this gives me you know two
numbers in this representation but these
numbers are then my feature
representation for this spectrogram
input and is you know the posterior mean
of those graphical models knows that
that gives me a vector that's what I
feed to my learning algorithm to do you
know various audio tells so um the timet
benchmark is a standard benchmark in
phoneme classification and and you know
all your researchers compete on this
benchmark I say this is not the
benchmark that audio researchers compete
most intensely on of all the benchmarks
but this is one of those places where
you know if you do point 1 percent
better on this data set you write a
paper on it so over the last decade
there was a 1.6 percent worth of
progress on this data set and a couple
years ago in a home lack former student
was able to make what 1.1 percent
progress that's like two thirds of the
decade of progress like a pretty
significant audio benchmark and then the
thing that I like about this is that so
when how mark and Peter Pham is on the
ground and I were doing this work none
of us actually know that much about
audio none of us experts in audio that
because ha but somehow laughs you know
not an exponent vision not an expert in
audio but by being only an expert in
machine learning was was able to compete
with your audio researchers on the whole
term um so what are the challenges that
face a supervised teacher learning and
deep learning
so there are DARAB so there's actually a
large community of people certainly not
just me developing unsupervised feature
learning and deep learning algorithms
let's share view one of very briefly
share of you what I think was one of the
main technical challenges which is just
the idea of scaling up
so Adam codes ran an interesting
experiments of taking different
unsupervised feature learning algorithms
shown in different colors and scaling
them up to different degrees so the
colors of different lines correspond to
different algorithms the vertical axis
is performance and the horizontal axis
is the size of these models the number
of features you learn and the trend is
actually incredibly clear right the
larger you can scale up these models the
better it does and very often even a
simpler algorithm if you can scale it to
learn more features or just learn from
more data and learn more features will
do better and so over the last few years
I should feel like one of the main
technical challenges facing unsupervised
feature learning is how do we come up
with efficient algorithms to scale up
these algorithms and I don't really want
to talk about this but concretely
proposes formalism in I guess what 2006
reject was the first to do apply GPUs to
this problem now essentially everyone in
deep learning is using GPUs and then
various other ideas for scaling up these
algorithms um I don't want to talk I
actually don't want to go into the
technical details on these today but by
scaling up these algorithms we were able
to get stated yot results on many
machine learning you know image audio
and so on tasks so as I was preparing
this talk I asked my students to
summarize for me all the data sets on
which we have a so that state of the art
without meaning of superior to the best
published result and I was actually
surprised actually quite a lot more of
these than I realized
but on audio I talked about that already
images I should say images I believe
there's one unpublished without that may
have services now but I think we were
superior on published results um video
when when quoc I tried these algorithms
out on all the standard on serve me of
many of the most standard video activity
recognition benchmarks all of these
surpass the the the Deep Web dear human
design features let's say a multimodal
classification one's going to say this
is you know I'm supposed to each other
learning deep learning is a rapidly
growing area of ICML web machine
learning so I'm certainly not Union
working on this just other say of the
archetype results I believe I'm not 100%
sure I believe yawn liquid may have the
best pedestrian detector not hunt
residential all that phony recognition
we were the first to get to
state-of-the-art result for
or audio timid more recently geoff
hinton has also gotten a very gutter
nasteria result on on a different phone
recognition task and kai has pre-modern
perhaps more than anyone else gotten
really amazing results on images and
leave them to talk about that when I
give talks that we're kite isn't
following me I usually have one or two
slides on this world but but I won't do
that today
um so that's not the state of uh yeah a
lot of what what we and other groups are
doing in unsupervised feature learning
and deep learning um in the loss of what
how much times I have 10 15 whatever
half-hour 30 40 50 minutes okay okay
thanks thank you so in last 10 minutes I
want to talk about some more speculative
future looking you know exciting maybe
pleading yourself is the idea of
learning recursive representations and
this is one direction of several the way
that we're trying to extend these ideas
um so what I'm going to do is actually
develop these ideas in the context of
text and then I promise I'll take these
ideas and actually apply them to
computer vision images again okay so it
turns out let's say I have an English
sentence the cat sat on the mat and I
want to learn a feature representation
for this sentence the cat sama lamaa the
the standard you approach with maybe
start as follows first we're going to
take each word and you know just encode
each word using a feature representation
um you can think of this as a one hot
vector representation or one on vector
indicating which word it is we usually
use other representations like an Lda
representation or there are few people
you're cheb NGO jason WestEd marshal
koala bear done very exciting we're
learning so distributional similarity
type ideas but you can basically taking
each word and represent each word VAR a
feature vector yeah and if you want to
think of this as a one hot feature
vector well think of it as an Lda or a
PCA vector but there's a knot over there
there are clever things by don't talk
about that now what you can do is throw
on top a standard you know generic
structure right graphical model or auto
encoder in your network
whatever um but you look at that no say
it turns out this sort of generic
hierarchy itself doesn't really make
sense for text right because uh that
node where the arrow is pointing it sees
the input cat sat on and so is sort of
the job of that node to represent the
phrase cat sat on but the phrase cat sat
on it's not even the proper English
phrase it doesn't mean anything right so
you know how do you represent it doesn't
make sense to try to represent the
phrase cats at all it just just doesn't
mean anything in contrast because text
language has a natural hierarchy what we
like to do is learn a representation
that respects the natural hierarchy of
language and concretely we know the cats
as a noun phrase the matches are noun
phrase and so on and what we like to do
is learn a feature representation that
respects the natural pause tree or the
natural hierarchy does imply by this
English sentence and if you look at that
note that's called Pepper's
prepositional phrase there's a job of
that note to represent the phrase on the
mat and now the phrase on the mat you
can imagine something sitting on the mat
you can imagine you know on the mat has
a meaning we can think about how to
learn a feature representation for the
phrase on the mat and concretely just as
we were taking images like image patches
like for and and coming up with a
feature vector to represent an image
patch we'd like to do the same thing we
want to take any English sentence or
take any English phrase and learn the
feature vector like just drawn as 8 3
there's just an illustration we'd like
to learn the feature vector to represent
you the quote meaning of each of these
phrases
oh so just 1/2 a slide on how we do this
the basic unit of computation in our
learning algorithm is a what's called a
recursive neural network and that's a
neural network that takes this input the
two children notes and it outputs the
feature representation of the parent
node in this hierarchy ok so Richard so
sure implemented is and so allows you to
recursively compute the feature you want
for the parent node as a function of the
features you have for the children
and you can train this a large-scale put
these train alders in one you know
discriminative e train framework with
one optimization objective and run the
whole thing and it converges so we can
do is apply this idea to pass sentences
I should say this algorithm actually
discovers the structure of the pass tree
and the representations of details so we
can do is actually apply this to pass
natural language sentences so given a
sentence like you know a small crowd
enters the historic church you can now
take as input say a representation for D
and representation for historic church
and use the recursive neural network to
compute a representation for the parent
right again each at each node we have a
feature vector that represents that node
and it turns out you can apply the same
idea to images so given an image like
this there's a natural hierarchy right
of this image so you think of it the
composition of the image into parts for
into object parts and subparts so at the
bottom is a super pixel segmentation of
the image and there's actually natural
hierarchy in which you know this part of
the building where my mouse point is
where my mouse cursor is pointing
combine some of a window to form like a
little bit bigger chunk of the building
these things combined with a roof right
to form the building and so on and so
this of a natural hierarchy view of
parts and objects sub parts in many of
these images and you can apply the same
recursive neural network formalism so
that you have feature representations
for the low level parts and you can
recursively combine the children in
order to learn representations for every
node in this hierarchy ok so the cool
thing about this is that each node in
the hierarchy it has a feature vector
representation right we started off with
features of the bottom level leaf nodes
and as we merge the leaf nodes to form
larger and larger channels of the image
were recursively computing a feature
vector for every node in this tree what
you can do therefore is selectively node
or select a node in your tree and ask
what other nodes have features similar
to this note that I just selected right
and this is a figure that shows that
so on the leftmost patches you know we
select to the node on the tree and
looked at the sumac so or union of
subpixels or whatever in one node of the
tree and then we look through a large
database of images and said what other
image patches get mapped to the most
similar feature vector as this one and
the nine images on the right are the
most similar neighbors and so you kind
of see that you know somehow right
semantically or maybe visually similar
things get mapped to similar feature
vectors so for example in the third row
right that fragment of a car well what's
most similar to that fragment of a car
was these other fragments of calls more
importantly you can apply these sorts of
features to standard vision tasks I'll
just work as much more with all this
work a summer preliminary but Richard
showed that using these features you can
now perform you know many hand design
features that you actually pretty
sophisticated hand design models on
multi class segmentation this is work on
the stand for the background data it
wasn't collected by me this is data
collected by Steven Google as part of
his thesis work and you actually had a
pretty darn complicated model for its
thesis work and then these learn
features I'll perform that um just to
wrap up just applying the same idea so
text we can you know each sentence has a
feature vector representation goes again
right this sort of featured recursive
neural network idea applies to text too
so what we can do is pick a sentence
which I'm calling the center sentence in
the in the second column and we can
compute the feature representation for
the center sentence and ask what other
sentences get mapped you know two
locations similar in feature space in
the center senses and that's shown in
the right column right um so this
example and so this example the bottom
the things are similar to Columbia South
Carolina are Greenville Mississippi and
unknown Worth Maryland so even though
you know UNK is the pauses annotation
forward is never seen before so even
though it's never seen this word before
it knows that unknown word Maryland is
probably a similar sort of thing as a
Columbia South Carolina
let's regal just a few more examples so
the sentences are similar to has
declined to comment while the fourth
more sentence similar sentences coastal
wouldn't disclose the terms where it has
declined to comment it has no words in
common what the phrase coastal will then
disclose the terms but we learn the
feature representation for these two
phrases that says that you know these
two sentences are similar to each other
it's a useful useful representation for
other now details finally you can
evaluate this this sort of algorithm
you're more using more traditional NLP
metrics like how well works as a powder
right now we're what we're we're
respectable positive but also not
superior to the state via best hand
design pauses but some sort of ongoing
somewhat more preliminary work okay so
um one last slide and then I'm done what
I've done is I've described a research
agenda in unsupervised feature learning
and deep learning and over the last five
years I've been going around talking
about this you know I've heard frankly
many weaknesses and criticisms of this
work and there are weaknesses of and
criticisms of this agenda of Allens just
mention some of them and acknowledge
them and maybe try to address all of
them as well
one criticism that often here is this
you know your algorithms are trying to
learn everything um but we know a lot
about images and so it's better to
encode the prior knowledge we have about
images um turns out there was a similar
debate in NLP in natural language
processing about 20 years ago 20 years
ago there was a linguist who wanted were
building pretty sophisticated linguistic
theories and then they're like the dumb
machine learning guys you know guys like
me right dead like just want to apply
learning algorithms a ton of data and
see what happens 20 years later is
actually really clear which side has
carried the argument in NLP and what we
read what what I think you know NLP
researchers what we realized was that
language is an incredibly complex
phenomenon and there's no way for us to
encode everything we need to know about
language there's just language is so
complex you can't write down everything
you need to know about language and if
you approach the solving NLP
is by hand two new features Louisville
at the time or if your approach to
solving NLP is to write down every
single phenomena and then manually add
one graphical model know for every
phenomena you can think of in language
right you're basically dead there there
is language so complicated if you want
to do this one graphical note at the
time or tuning one feature at the time
is just just never going to get there um
I think the debate is still playing out
in computer vision and and maybe vision
is a simpler phenomenon than language
and maybe it is possible to you know
build models look complex enough to
capture all the phenomenon there are in
images but again it depends on what you
believe is true for images versus for
language over the years actually let's
see over the years I've also heard a lot
of criticisms of unsupervised feature
learning of the form you know
unsupervised feature learning cannot
currently do X where X is many of those
things on that list below um so it turns
out all of these were actually are valid
criticisms many of these were true at
the time but are not true anymore and
you know I think there is certainly
still work to be done there are still
things at the bottom this list as we
strike things off the bottom this list
of course we ourselves as well as our
colleagues and friends helped us to add
more things to the bottom so this is not
which is the way it should be
but the certain thing work to be done in
the 30 problems we have not yet solved
and and finally one of the criticism I
hear that actually is a signaling
problem is that we don't understand the
learn features and we apply these
learning algorithms they learn something
yeah they learn some hardly complicated
nonlinear function and you know there
are people working on visualization
tools to understand these things but we
sort of don't understand them and this
actually is a serious problem I think
many vision features are also hard to
understand especially when you still
have used like these multiple kernel
learning type methods to combine tons of
other features you know what is the
output I don't know but but I think I
think the fact that we don't understand
the output of these arrows actually is a
problem okay um just to wrap up I've
talked about this agenda of unsupervised
feature learning and and you know let's
let's try to learn the feature Rosen
tations
but of course future learning is just a
narrow technical formalism of a problem
this problem isn't really about learning
features right is about understanding is
there some fundamental computational
principle the underlies perception the
underlies maybe not just computer vision
but the underlies perception more
broadly and can we discover
computational principles that underlie
perception and implement that because
that might be a much simpler way to you
know solve vision or make progress and
vision then to narrowly just implement
things that we think the vision system
should do top response coding and deep
learning variants which is proved very
successful for various tasks and finally
I know this talk has been light on
technical details and if you're a grad
student your video or if you're a
researcher and if you want to use these
ideas the best way to pick these things
up is actually not by getting the
details from from from like a talk line
line but the best way is probably to
actually look at the tutorial and work
through you know prank V through some
technical things and v4 Pro exercises
and so on Stanford students and I have
been working pretty hard on setting up
online tutorials with the goal of
actually bootstrapping people up to
really apply these techniques to your
problems there's an extremely
preliminary version up at that URL but
we're working on the most sophisticated
tutorial with like they're like
seriously hopefully a serious
well-designed tutorial with readings and
programming sizes to really to really
get people up to speed as quickly as
possible on using these things yourself
on your problems
so email me if you'd like to point to
that and certainly happy to share that
actually I was actually designing this
for Stanford students actually selfish
reason I was designing it but happy to
share it with others as well and then of
course unsupervised feature learning is
a large community so you know if you
know any of the other guys working on
this you should definitely amnon so
they're not the only proponent we should
take ideas from this community in the
plan to work thank you very much
so questions what happens if your data
is imperfect so humans can interpret
noisy images with blurry and so on so
how balance wise feature learning so I
think it's a broad problem that you know
holds true for all of computer vision
not just and supervised teacher learning
I not aware that we have a huge
advantage or disadvantage but but we're
driving people work on these problems as
well yeah burns question is you know we
know that we want things like scale
rotation translation invariance so
should we put these things in their
algorithms to get better scalability so
the answer that is satellite is sort of
a good idea in fact using convolutional
nets for example hard code translation
variance we do see better scalability
that's not a yes answer here's a no
answer which is that what I see many
people do right this is so this notion
of these invariances it's actually very
seductive they're like five or six types
of Endurance's that we know about and
can hard code there's a translation
rotation scaled luminance you know maybe
one or two more right scale did I say
that and so when I see many people do is
you can actually turns out as possible
to embark on like a six month or two
year research agenda to take these types
of invariances and code them one of the
times your learning algorithm and you
can write papers you can keep going but
then after you've encoded these five or
six Endurance's everyone has the same
brick wall right so
I think in the short term you cut these
things up too to encode these invariants
disaster aid but there are these other
types of invariances like out of plane
rotation so if I rotate my face this way
I'm still the same person right all you
know more complex like deformable parts
like the fact that I move my arm and
deformable away I'm still the same
person
those types of variances we don't know
how the hand code so there's this
trade-off do you want to code up what
you know and then maybe hit a brick wall
and then you need to learn things anyway
or do you want to try to learn
everything and I think they're actually
valid things with both approaches but
just be aware if you're on the agender
of coding up the small list of
invariances that we know how they called
up everyone has the same brick wall all
the questions
yes oh right so one of the reasons for
sparse coding is energetic efficiency in
the brain is much more energetically
just repeated question reminds much more
energetic expensive to send a spike than
send a pulse electricity then did not do
so and this was I think Bruno and
David's original motivation for spawns
coding so the company context we don't
let this constraint can we relax it so I
I don't know I think on speech learning
is the relatively new field and we're
all trying lots of different ideas
sparsity seems to work however make our
algorithms work much better and for
those of us you know doing vision and
audio our main motivation for doing
sparse coding has been the performance
of the algorithms I don't know maybe
there are other principles that could do
even better but actually temporal
coherence or slow feature analysis one
of hypotheses or many groups high pollen
so don't don't don't really know it
turns out that there's some fascinating
work on computer hardware where it turns
out to be more energetically efficient
to and if we end up using those types of
hard sorry is a yeah don't open whew
oh yeah right so I draw fair enough so
in other words you're taking issue of
the word unsupervised and the fact that
we encode you know the local topography
of a pack so um I uh I'm happy to encode
at amount of prior knowledge so what
we're doing is were essentially telling
the algorithm which pixels are adjacent
to which other input pixel so this is a
type of prior knowledge that we didn't
code into the algorithm we haven't
really explored algorithms you know that
it turns out that the algorithms are
much more scalable if you actually tell
which pixels are next to which pixels
there's you know evidence in the brain
that suggested that the way your brain
gets images actually captures the
adjacent information as well it turns
out these elements work just fine if you
make some local reshuffling of the
pixels actually surprisingly robust to
that but I agree this is a this is a
type of information that we coded to the
algorithm and with that I should wrap up
and pass it to Kai so let me just say
again thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>