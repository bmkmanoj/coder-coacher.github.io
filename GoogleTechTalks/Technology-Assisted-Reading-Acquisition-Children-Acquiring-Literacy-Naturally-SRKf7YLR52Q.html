<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Technology Assisted Reading Acquisition: Children Acquiring Literacy Naturally | Coder Coacher - Coaching Coders</title><meta content="Technology Assisted Reading Acquisition: Children Acquiring Literacy Naturally - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Technology Assisted Reading Acquisition: Children Acquiring Literacy Naturally</b></h2><h5 class="post__date">2013-01-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SRKf7YLR52Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today I appreciate all of you coming to
the talk and I'll try to go through
fairly quickly so you'll have time for
questions as human said I'm I was
trained as an experimental psychologist
majoring in mathematical psychology and
I developed an information processing
model of perception memory and learning
and I wanted to apply to real world
domain so I got interested in language
and so I've been working on speech
perception and reading the last 40 years
and I want to give you this background
primarily as a road map to where I ended
up in the idea that kids could learn to
acquire reading naturally without
instruction so I'm going to talk a
little bit about our work on multimodal
speech perception and then a a project
we did on embellishing face-to-face
communication and then talk about
naturally acquired literacy so as you
know language has been thought as being
special particularly led by Noam Chomsky
that it doesn't follow the rules of
perception and memory and learning and
we took the opposite view that indeed
speech perception and reading can be
understood as prototypical pattern
recognition how to make sense of the
world around us and so what we got
interested in is how much the face
contributes to speech understanding so
here's a little film excerpt that you
can take a look at
ok so it's particularly important that
the man of the audience picked that up
as I tell my students if the guys missed
that see me after class because it's
important for the survival of our
species so to study this problem of
speech perception we use synthetic
auditory speech we wanted to also vary
the visible speech that we could control
it exactly so we developed a computer
animated talking head called Baldy
spelled with I rather than a why because
he's from California so here I'll let
Baldy describe himself to you there is
very little bit when my attractive
exterior see there is only a lawyer
frame underneath I live through computer
animation and text to speech synthesis
my visibles each is accurate so we
approach the problem is speech scientist
we wanted to make the visible speech as
accurate as possible and we develop
baldy that he could be aligned with
either synthetic speech or natural
speech we also made Baldy multilingual
by looking at the unique characteristics
of different languages and programming
the appropriate mouth and face movements
in baldy hyeondo in yemen reunion kijiji
ocean college from behind you in how was
that Sherman so bold II can also be
aligned with natural speech ok so
basically you can think of all the as a
puppet on a set of strings and we're
manipulating those strings moment by
moment to produce the acrid visible
speech such as jaw rotation mouth
opening lip roll and zipping and so on
by adjusting the wireframe model had
each time period my speech is accurate
because it is based on real speakers and
I have a lazy son just like they do I
can be texture mapped with the image
other you person who said I'm not real
see you around so we also modeled the
tongue because the tongue is very
important for example if you have
Japanese trying to learn are in L in
English you don't see much on the
outside the face but the tongue makes
very different movements so we use
electro palate ography in which you put
a a pallet on the roof of the mouth and
with sensors that picks up where the
tongue it's the roof of the mouth and
also ultrasound picking up how the
tongue moves so we know that in speech
production people speak with a lazy
tongue the way we articulate one segment
is influenced by the segments that
precede it or follow it so you can't
really just use a key frame method where
you have prototypical mouth movements
and then you just interpolate between
those so we developed a dominance
constraint so it's each control of baldy
has a certain amount of dominance and if
they have equal dominance here is shown
in the upper panel than you do a simple
interpolation but if you take some
characteristic of production like the
protrusion if you say the words do you
notice the the lip protrusion of the you
comes much earlier in time and
influences how you say the s and the tea
so the s in the tea in stew is very
different than it is in steep for
example and so we assign higher
dominance for the protrusion for ooh and
therefore when we interpolate we see
then the lip protrusion then comes
forward into the S&amp;amp;T this is a co
articulation argue algorithm that's been
tested in many different experiments and
has been shown to hold up pretty well in
the sense that it produces accurate
visible speech our gold standard here is
to get baldy to give the same kind of
quality of this
speech that a real talker gives now many
at some of you have experienced the
McGurk effect anybody okay so what you
want to do here is you want to look at
baldi and he's going to say a set of
syllables like Bhagavad ah ma simply a
watch Baldy keep track of what you hear
and then we can talk about it ok so if
Baldy said four syllables did you hear
the syllable changing from syllable two
syllable some of you some of your not
it's pretty small but in fact for those
of you that did hear the auditory
syllable was always Bob but the mouth
was going ba da da da so if you were
hearing different syllables in fact the
visible speech was influencing what you
hear i can play it one more time you can
either look at it again or close your
eyes now i've been looking at that for
much longer than i want to remember and
i still get the illusion ok that the
visible speech has an impact and so
based on this we developed this a
multi-sensory pattern recognition scheme
called the fuzzy logical model
perception in which we have these very
simple stages a process model where you
evaluate the two sources of information
in terms of how much they support the
different alternatives you integrate
them together to get an overall metric
of support and then you make a decision
and a learning occurs with feedback so
that you can modify the evaluation of
value given the feedback that you get
and this turns out to be what we call
the fuzzy logical model perception
spacing fuzzy logic it turns out to be
it's mathematically equivalent the Bayes
theorem which is an optimal method for
combining multiple sources of
information to make a decision I should
mention if anybody has any questions
please feel free to interrupt
clarifications and so on and if I'm
going too fast or
too slow let me know okay so given that
we had support for Baldy having good
visible speech again compared to our
gold standard Baldy does almost as well
I won't show you those data we thought
that there would be value in baldy being
a virtual tutor so who better than a
bunch of deaf and hard of hearing kids
that are behind in vocabulary simply
because they have degraded hearing and
we thought that Baldy might be a good
tutor so here's a little part of a
prime-time special that evaluated
baldies instruction of vocabulary for
these kids just how fast ball he can
work with primetime had teachers create
several new vocabulary messes for tennis
afterwards he had never spoken before
let's talk about what you see first
Baldy checks his knowledge click on the
bowling balls then Baldy shows Timothy
each item correctly these are bowling
balls followed by a drill of speaking
the words for the very first time what
is this it's not easy click on the
tennis rackets no that's not right Tim
Roy mispronounced and misidentified nine
out of ten objects but just three weeks
later we retested it okay Timothy you're
ready for the final test what is this
soccer ball this time he got nine out of
ten correct and his pronunciation
improved dramatically what does this
baseball okay so it looked like the kids
were learning vocabulary but we wanted
to make sure that indeed it was a baldy
intervention that was responsible so we
did some experiments in which the kids
are learning three sets of words we're
testing on all three sets every time but
we're only training on one set so the
idea is that if it's the baldy
intervention that's important then
they'll only learn that set of words and
not the other sets this is called a
multiple baseline procedure
so sure enough there the dark circles or
squares our comprehension and the open
ones are production and comprehension is
always a little easier than production
but you can see that sure enough this
particular student learns a set one
words but not to set two words when they
start training on the set to they learn
those items and then they learn it for
set three so this is an accepted
procedure that is accepted by peer
reviewed journals it says yes your
intervention was responsible so we were
happy with baldy being an effective
virtual tutor for learning vocabulary
and we look to the autistic kid
community and thought that Baldy might
help there to assist the kids like
constancy in baldy is always constant
his emotion can be controlled exactly
and we can also teach grammar here we're
teaching singular versus plural with
these autistic kids and I'll just give
you a quick look at Tony working with
baldy let's practice with the ladybug so
he's learning singular dresses plural
where be over and he's clicking on the
right answer but he's not saying them to
the kids really resume your to baldy an
ER doctor they come in high ball they're
loving Baldy and so on and then correct
sure enough the Baldy was also effective
with the autistic kids and so more
recently we decided to port Paul D to a
tablet and here we have Teheran working
with baldy on a tablet in a simple tile
matching game where you make match two
tiles so they disappear yeah are seeing
Cuban expression
yeah autistic kids tend not to look at
the face and in fact we taught kids to
look at the face and found that they
could use that information integrate it
with the voice in the same way that
normally developing kids do so that's a
good point but they tend not to look at
the face but we were able to get them to
learn how to lip read better with baldy
than they had previously and sure enough
they could look like normally developing
kids in terms of integrating the two
sources of information one of our month
mantras is that people naturally
integrate multiple sources of
information even autistic kids so thanks
for that so here's Teheran working with
the tile matching game
so you can see that little game can be
engaging and the nice thing about a
virtual tutor it's available all the
time but there is a downside so these
programs to write are expensive they
require some maintenance and one could
argue you know it's 2d media they're not
getting personal interaction so one of
the reasons I talk about this is because
this kind of intervention is coming kind
of late in the child's life and one of
my mantras is that we need early
interventions just to anticipate some of
the things I'm going to say okay so now
we're going to switch gears to a second
project in terms of embellishing
conversations and we have one out of ten
people across the world deaf and hard of
hearing and we all lose our hearing
particularly the men as we become
chronologically gifted and therefore we
depend more on the face but the face
gives some information but it doesn't
give complete information so for example
you can get place of articulation from
the face you can see the difference
between the and da for example but you
can't really get things like voicing so
well like the difference between ba and
pas so what we thought is that if we had
hard of hearing people that we're
getting degraded hearing maybe we could
embellish the signal by giving visual
cues about those things that aren't seen
so easily on the face and this would be
a wearable appliance that people would
wear like a pair of glasses and it
wouldn't take much sophisticated
technology you could just have a couple
of LEDs on the corner of your glasses
you're looking at the person lip-reading
getting some degraded hearing if it's
available and then integrating these
cues with it so we decided to develop a
scheme where we would represent these
characteristics of the speech visually
so voicing would be indicated by a blue
dot vacation by a white dot and nasality
by a red dot so what we did is we
developed
a neural network model that tracks your
speech so you'd have a microphone on
your eyeglasses you would be tracking
the speech of the interlocutor you're
having a conversation with and then
these cues would be showing on your
eyeglasses and the idea is that you
would be integrating these cues with the
face and the voice to understand the
message and so you can see like
remembered red is nasal like men so you
saw the red in the blue which is voicing
and then you can say things like assists
where you get the furcation the voicing
and the furcation right or men where you
get the de nasality okay or church so
the neural network actually does a
pretty good job it's doing it in real
time we're only lagging by a these are
10 millisecond steps so we're only
lagging the speech signal by about 50
milliseconds and it does it does about
ninety percent correct and so that's one
half of the problem the other half of
the problem is for people to learn the
cues so if you see fan okay if you just
see it on the lips then that could be
van because you don't see the voicing so
well but if you get the cue that it's
voiced let's see if this works live
demos so so if you get if you don't get
the white queue then you're okay fan so
I saw it went white blue white where if
I say van then you see you don't get the
white Q so that could tell you the
difference between fan and van so you
just have to put those two things
together so that's so what we want to do
is teach people and we had we didn't
have a whole population of people of
deaf and hard-of-hearing people so we
depended on you
adversity kids that came in every day or
maybe three or four times a week for an
hour so a day in which they practiced
integrating these cues with the face so
Baldy would bollywood mau the word with
the ques the student would write and
indicate what they perceived and then
Baldy would give feedback
alright so these kids were very heroic
they came in for over a year every day
and we're learning the cues and they did
a pretty good job but what we found is
they did pretty well with single words
or maybe even two or three word phrases
but they would law were lost in
continuous conversation right think of
trying to track what I'm saying with all
these cues that's a tough one so we also
made the observation that it's hard to
change behavior now we know first of all
people that require hearing aids they
have very little patience most of them
throw them away don't don't really use
them you probably have experience with
people have done this and similarly we
thought there's no way we can convince
people to spend a year learning these
cues that might help them and the other
thing is as you can see when we r we
don't hear something so well our natural
tendency is to move our ear to the
source and lose the visual information
and so this woman who has hearing aids
is gaining about 3 decibels to
understanding but in fact if she were
looking at the face she would gain about
12 decibels but so how do you how do you
teach people to do that it's hard hard
to change behavior so this is another
lesson in terms of where i'm going with
in terms of the naturally acquired
reading so what we did in this project
was that we thought well okay why don't
we just do full-blown speech recognition
and then communicate that way so the
hard of hearing person can get all of
the cues of you talking and then also
get the words so they can get the
parallel mystic information and the
linguistic information and then have a
conversation
weather
today okay so so the idea then again if
I'm talking to someone that's hard of
hearing I can ask them a question we all
know about speech recognition open-ended
alternatives and so on but do you know
if there's a starbucks nearby in this
neighborhood so not bad did you see the
frost on your roof last night so you can
do a pretty good job now this recognizer
only runs locally if you had access to
the internet it could do a lot better
and it could be faster to simply because
if you know about speech recognition is
a much bigger database and more
computational power and so on so this is
where we ended up on this project where
we just were doing the full-blown speech
recognition ok so we're making pretty
good time that name going to too fast ok
so here's what you all came for I guess
and that is again I wanted to tell these
stories because i want to show you how
this took me in this direction I should
have showed shown some research also
that we've done in reading just in the
same way that we did in speech
perception that people integrate
multiple sources of information and
reading like putting together
information about the letters themselves
the orthographic structure that is how
the letters go together in words and
syntactic and semantic constraints for
example and so about three years ago I
arrived at this idea that kids are
immersed in spoken language at birth why
can't we immerse them in written
language at birth this has never
happened because we haven't had the
technology but the technology is getting
there why don't we immerse the kids in
written language at birth and the idea
is that they'll learn to read naturally
without instruction
and this has huge implications for the
way society is structured today okay
what are we up against here well the
current belief is that speech and
language as I said our very special
things and that they're more or less
like instincts whereas reading is
artificial its artificial because it's
only was created a couple thousand years
ago and we created it rather than some
extraterrestrial influence or something
and the Mary Ann Woolf here represents
the neuro neuroscience community and
when she says in her book here unlike
its component parts such as vision and
speech which are genetically organized
reading has no direct genetic program
passing it on to future generations so
there's something special about reading
that it's artificial and speech is
natural and so that's what we're up
against okay so maybe I can have someone
from the audience would you be willing
to participate so what I want you to do
is there going to be a set of pictures I
want you to name the object and the
picture okay and just go from left to
right and across the two rows name the
picture
nest eggs baby rabbit rink okay now you
can do the next one this is this is like
the Stroop effect right where you're
trying to name the color of the print
when it's spelled in words of a
different color right so it makes a
point that we did learn to read but we
look we do once you learn how to read
you can't help but read that's why
advertising is so effective and that
it's just one more similarity with
speech we can't help but hear someone
mentions our name and we can help but
orient our attention to it so the way I
thought about the problem is well what's
needed for a child to acquire spoken
language and does that same child have
that same stuff to acquire written
language and so for spoken language the
child's got to do some kind of signal
analysis they have to hear the syllables
combine the syllables in different
orders form categories that are
associated with meaning and most
importantly they need an early exposure
we know from those few sad cases where
kids haven't had language until
adolescence or even six or seven years
old they can't acquire language and
what's really impressive you don't know
this maybe this is why you're tired in
the evening if you have kids in a giving
year say from one to two years old a
child hears about a thousand hours of
speech which is about a million words
okay so that's a lot right so they need
that early exposure and they need a lot
of it now what's not needed well some of
you may have heard of the theory of mind
this is something that kids don't get to
about their three years old and that is
that they have some kind of
understanding that they are them you are
you and you can engage in something like
a dialogue and you might have different
beliefs than they have and that you can
change each
beliefs well that's not necessary to
learn language because by age three kids
are incredibly sophisticated language
users even though they don't have a
theory of mind as the car talk guys
might say it's unencumbered by the
thought process okay so the kids acquire
the language without thought now one
convincing piece of data was you all
heard of konsi who can do amazing things
with language in fact how did konsi
learn to manipulate these symbols and
eventually that were associated with
speech and eventually learn how to
understand speech well here's konsi on
her mother's on me well matata while
they were teaching Matata these symbols
alrighty so they spent a year or so
doing very formal instruction of the
adult Matata to learn these symbols and
and cons II was just there nursing and
having a good time and so on well it
turns out that Matata never learned and
then simply one day some deer
serendipitous discovery they found out
that konsi had learned all the symbols
okay so that's pretty wild not much is
made of that in that literature but I
see it really important sit and this
agrees with the idea that there's this
incredibly explosive brain development
that occurs in the first few years of
life that where the brain is getting
bigger it's making more connections
pruning connections that aren't
meaningful and this is another support
for the idea that you know it's
important to get in there early when the
brain is plastic now all these companies
are telling us that our brain is plastic
through our lifetime and we can learn
and lower your chronal chronological age
and so on but I'm kind of skeptical that
it's a little plastic but not like what
the young kids bring to the table okay
so that's more or less what's needed for
speech perception and we can ask similar
questions about reading so for reading
we
to do some kind of signal analysis we
have to learn letters combine the
letters associate those combinations
with particular categories and like
speech that we need early exposure and
we need some kind of language and
reading the immersion in the same way
that we have it in spoken language well
how many hours do we eat we don't know
but we know right now the kids are
getting next to nothing right you know
I'll expand on that so first of all in
terms of the signal analysis babies come
equipped with the vision that's very
sophisticated very early on here's a
baby at three weeks old where you can
see her making he saccadic eye movements
tracking the toy that's it just three
weeks a week earlier she couldn't do it
also their visual acuity is very good so
that at one month they're real good at
you know arm's distance looking at you
but by eight months they pretty much
have the vision that we have so they can
do about as well on the eye chart as we
can so the baby seem to be equipped to
process visual information and sure
enough they can form categories so at
one month old infants can see the
difference between a square and a
triangle whereas at one month old they
can't see the difference between the
square and the triangle if they're
embedded in a circle but at two months
they're able to make that distinction so
again there's this rapid development of
what's necessary to form categories and
it turns out that babies are incredible
Association engines they learn
statistical constraints like nothing
alrighty and developmental psychologists
have made a cottage industry of this
behavior showing that kids
can very quickly learn associations in
speech in music in objects so a typical
experiment might be the experimenter
sets up probabilistic constraints among
objects here you can think of these as
peer Avadh pairs of objects three pairs
of objects one object always follows the
other object but between pairs they
follow each other only a third of the
time all right so there's a set of
constraints there if you take a seven
month old infant and you give them two
minutes of this sequence they get bored
out of their mind they habitually okay
you change the statistical properties of
that sequence the objects are still the
same the infant's wake up this is how we
can tell what infants know about the
world around them they get bored very
easily we change the world they wake up
and we assume that they noticed a
difference okay so indeed and they can
do this pretty impressive and what's so
interesting again even though the
developmental psychologists have made
this a cottage industry they never
thought of studying letters and letter
combinations somehow reading seems to be
off the map for them so we propose an
experiment of the following kind that we
use the same kind of constraints but now
we do it in letters rather than in
objects and the hypothesis would be that
indeed kids were would learn this so
maybe someday someone will get some
funding to do it so we start learning
about how kids a process written
language now we think that kids are
going to be able to do this it's going
to be a no-brainer why it turns out it
seems that if you do a topographical
analysis of the alphabet and alphabets
of the world they have the same
characteristics of the world around us
whether it's a geometric architectural
world or whether
to pastoral world they seem to have the
same property so did the argument by
consiga there is that the alphabets
actually developed not because they were
easy to write but rather to make it easy
for the visual system that's already
prepared for that kind of information in
the environment so as I mentioned then
we want to have this critical period of
development we see it's true in the
auditory system the visual system speech
and sign language and we're saying that
in reading it's the same thing that we
want to have reading being learned at
this critical period of development
obviously it doesn't have sharp
boundaries but the fact is the earlier
you can get in the better so how do we
immerse kids in written language as I
said this has never been done because we
really haven't had the technology and we
still don't have it but we can have
successive approximations so we can
think of picture books picture books are
really important for kids and their
inculturation in a in the world and we
all read picture books to our kids and
it turns out that you think Oh picture
books you know there's writing in
picture books right look at the writing
well obviously when you take I've
movements of kids in picture book
reading on ninety-five percent of time
they're looking at the pictures and not
the words writing and as you can see but
the graph there that the artist makes
the fonts real funny and so on to please
the adults reading not for the kids to
read so we developed this app where we
put in at all the popular books that we
could find we put them into our app and
so the caregiver can choose a book that
they're reading and then supplement it
with nice visual letter
is that the kids can read easily so here
I chose barbapapa barbapapa at the zoo
and you could be reading along the kid
be looking looking the book when they
were having a party after the fire
barbapapa heard cries for help and then
a fierce leopard had escaped from the
zoo and you so what you can do is um
okay you can then dictate this a fierce
leopard had escaped from the zoo so now
the child is getting nice written
language that supplements the picture
book reading and they should then you
know get some of this a written language
that they need for the development of
learning to read okay so when we have
all the books in memory and we know what
book the caregivers reading it makes
speech recognition a lot easier but
someone can't really read our book
without having the real book so we're
not really hurting any copyright so
here's another little one so you could
see it was kind of difficult for me to
negotiate holding the book and the I the
iPad so there's the nice features you
can buy we can put your tablet right in
here right and then it's easier to
negotiate
there's he and looking at the word see
and you might wonder about this format
that we're using where one word occurs
on top of one another but in fact this
is called rapid serial visual
presentation and psychologists use it a
lot in experiments it turns out that
kids even third graders do better with
this vet rapid visual rapid serial
visual presentation format than they do
with a page format and again we grew up
reading at our own pace and so on so we
wouldn't like that but in fact just like
we're pushed in listening to speech we
can be pushed into reading and we can
actually then read faster more words per
minute with better comprehension in this
kind of presentation than the standard
presentation of course if you can see
this gives the visual quality much more
value to the kids because they get nice
big words and it wouldn't work very well
in a page format so here's Nathaniel's
mom reading Goodnight Moon he's geez
what is he four months old or the eight
months I'm sorry
you see he's really looking and he does
pick up on it and the thing that we
don't realize is that we're talking to
our kids all the time and they don't
understand a damn thing right that's
where that that's what that set and it
keeps going it gets worse it gets worse
okay so we're looking at other ways to
embellish the written world for the
child and one is right my world where
you can then write what the child is
experiencing so people you might think
that's kind of artificial but in kids
learning sign language the caregivers
are faced with the same problem they
have to get the attention of the kids
it's a little different from speech to
watch the sign so that the caregivers
might actually sign over the object or
get their attention look at me and do
the signing and kids learn sign just as
easily as they learn spoken language so
this other application we have here is
that the child will carry around the
camera and zero in on the world around
them and we have barcodes so here we can
distinguish 24 different things with
these barcodes and then we can present
whatever we want we can talk about it we
can show the visual information and we
can also present it so it's always right
on top of the object taking into account
the camera angle okay so that's another
way that child could walk around and do
this well so here at Google you've got
your ideas about interactive digital
signage so that eventually we're going
to be surrounded with a literate world
right so there's no reason that that
digital world can be available to young
kids as it is available to our adults
that's certainly indicating that you
know the technology is getting there in
terms of having the child immersed in
written language and of course kids can
have robots the European community is
looks like they're going to fund a 10
billion dollar project on robots as
companions and there's no reason why
these companions can provide written
language as well as spoken language to
two kids so here's a little concept
video that was done in 2009 about a
Danish farmer that goes out with these
intelligent glasses goes out to his barn
and he looks around the barn and of
course it's doing object recognition and
he sees that the roof needs repair here
this cow has to stay on medicine for
another two weeks and so on and so these
are very valuable things and he comes in
the house and the recognition system of
course recognizes his spouse and tells
him hey tomorrow's your anniversary you
better get a present okay so in our
patent application I proposed this
heads-up display where there are two
things that you want to do you need to
understand the experience of the child
and represent that in written language
so there's two basic ways to do that one
is to do speech recognition so the
speech that's being said is very
predictive of the child's experience and
second is to do object in action
recognition worth recognizing what the
child is doing is another idea of what
they're experiencing and both of those
would be associated with written
language so the caregiver says you did a
fine job the heads up display might just
say fine job so it would read it okay so
you all know about google glass and the
point here is that the wrong person has
it on all right that the baby should be
wearing the glasses that give the
information
about the mother so although she put
some toy glasses on the baby here to
take a picture that it's really the baby
that should be seeing what's going on in
the world eventually it'll all be on a
contact lens right so that will make it
even easier but for now we just have
these portable tablets that kids seem to
be attracted to naturally they love the
touch system and I was telling human
that they could do something like a
swipe on the tablet to interact with the
written language that way so we're
winding down here what are the benefits
of early reading well the idea would be
that illiteracy would be no more
frequent than speech impairment would be
now whereas now is obviously much more
frequent it would reduce the cost of
reading instruction and there are
advantages of written language we did an
analysis of the kind of language you
find in picture books versus the kind of
language adults have when they talk to
each other and we found that the
language in picture books was much
richer and being at a higher pitch a
more unique vocabulary more complicated
kick grammar and so on so it's good to
get kids reading as quickly as possible
because they're going to be faced with
more difficult language I could that
only can be beneficial so if we're
successful then this is going to change
how we allocate resources it's going to
have incredible implications for the
deaf and hard of hearing community and
it allows us to rethink schooling so
right now if you look at the public
spending on kids as they go through from
birth to adulthood verses of their brain
growth you see this inverse function so
we spend all our money on kids after
they go to school whereas the brain
growth all occurred before they go to
school so we need to have a realignment
of some public spending before schooling
rather than after school and sure enough
our Nobel Prize winner James Heckman
showed that the return on investment is
much greater when you
invest in preschool kids relative into
school age or adult kids so you get a
good return on investment there for deaf
kids most def kids read at a fourth
grade level and that's primarily because
19 out of 20 kids that are born deaf are
born to hearing adults and the adults
want to hear them talk so they these
kids don't learn sign language very
quickly with our method these kids could
be bootstrapped with written language
right so that that would be their second
language in addition to either oral
language or sign language so the written
language would be a real great modality
the bootstrap kids that are deaf and
hard of hearing and then finally we can
read we can envision schooling after
kids are ready for reading at the age of
which they would normally go to school
and so rather than we spend something
the way I figured it out we seem to
spend about ten thousand dollars per kid
for a year of schooling so we could save
a lot of money if these and we have the
3 R's reading and writing the children
would have both of those so it would be
something that could help the economy a
lot but it would also allow us to think
of schools as do we did where we would
have communities of scholars that with
certain interests and they would
congregate together and pursue their
interests rather than sitting in a desk
and and learning a litany so this is the
conclusion i usually give to the group's
i talked to because behavioral and
Social Sciences is still pretty
conservative right so I don't have to
give that conclusion here that you know
it's clear that science and technology
impact life and that we have to be open
to disruptive ideas so I'm happy to
entertain questions now and
we can open the floor to you all thank
you kids are small you show the brain
growth and the importance of speech time
you get a call so I have my son you
didn't speak to lose three so there was
a lot of time it really couldn't
communicate so the other side of the
breeding early thing for me would be be
able to communicate effectively with my
son the reading as opposed to speech and
hopefully bootstrapped speech or maybe
get out early thing so have you thought
of it and so the reverse way that's a
nice that's a nice idea certainly one of
the reasons that we we try to make a
behavioral science case for learning to
read in the same way that you learn to
understand spoken language and one of
the things you see our kids that are
late talkers so-called late talkers your
son was when it one of these could
understand hundreds of words but didn't
speak but once he started speaking did
he yeah yeah did get him to speak but
the point is he did understand so
there's a there's a we actually believe
he hadn't figured out that crazy noise
you're here actually had meaning to it
we just thought it was just like no way
okay what I can talk more about you that
lately there's a scale you can fill out
that says how many words your kids
comprehend versus how many they produce
and kids always comprehend much more
than they produce and that's a normal
trajectory and so the point would be for
reading that could be also the case the
kids could be reading much more than
they're able to write but one
alternative that people have chosen that
in similar situations are baby signs
that babies are able to
make signs that they use to communicate
so we can talk more about that later
yeah well far less sophisticated or
consistent on Sesame Street would
frequently show the words flashing you
know with the object and precisely how
you have it as the word is spoken you
see the flap word flashed and the
picture at the same time are there any
studies that showed the effectiveness of
that on our literacy for cheap watch
Sesame Street versus those who didn't
yeah that's a good question i don't know
of any but that would be something to
look into so we had that ? we don't know
how much written language the kids need
and so that you know the few times that
today the show is brought to you by the
letter l you know that's probably not
enough but but it's a start and it would
be nice to determine that there are a
couple anecdotes the gentleman mentioned
in india they're showing the written
language subtitles with the spoken
language to helping in literacy when i
visited denmark i was impressed that
they show Sesame Street without dubbing
so the kids are hearing English but they
show Danish subtitles so my idea was hey
these kids want to learn to read Danish
because they want to understand what the
hell is going on in sesame street so
that's kind of kind of need anything
yeah I don't know of anything systematic
that's been done with that people have
looked at picture books and seeing what
effect that has picture books has a big
effect on language but not literacy
because again the kids simply aren't
looking at the looking at the words and
there's some controversy about that to
what extent kids can learn from 2d media
but obviously reading is 2d media so I
know I think that's a no-brainer of
course they can learn yeah
teaching kids phonics you need to be
better um so okay so um many years ago
we wrote a paper it said the trick about
see so how do you teach a kid to read by
today's schooling well what you do is
learn teach them how to decode what does
decode me it means that they're able to
map the written language into spoken
language and the way you do that is that
you teach them phonics right and so a
colleague of mine just shared an
anecdote with his granddaughter the
other day she had a picture of a cat and
she went and the word was written
underneath and she went uh cut cat and
then the next picture came on it was an
insect and she went hmm but bug bug
except it was it said aunt okay so
that's a great anecdote so one of the
things we sweep we wrote in 1979 was
that one of the benefits of phonics
might not be with respect to decoding
but drawing the kids attention to the
orthographic structure that means the
spelling constraints in the language
what letters follow other letters and
where they occur in words and we showed
that indeed people are sensitive to
these constraints even though they have
nothing to do with spoken language all
right they there's just certain
constraints in the written Ling some are
dependent on spoken language would come
on we're also sensitive to those so our
idea is that written language has the
same constraints and a deaf child could
learn written language independently of
spoken language when we heard the
picture book reading we we have an
option you can either have the voice on
or not you heard it on and that can help
bootstrap the child she already knows
the speech but it doesn't have to be on
that the deaf child could learn it that
way so to get back to the original
question I'm sorry about the long answer
is that we think that the child can
decode into spoken language and then
it's a no brainer right they can
understand the spoken language but in
fact I'm not so sure that's the case
even if they do decode successfully it
could be the case that that decoding
takes so much attention and cognitive
processing it distracts them away from
the understanding of the message and
therefore if we had the early literacy
they would comprehend the written
language directly without going through
the spoken language and not needing the
decoding process that's right so you're
not going to teach them the alphabets
yeah and it would be learned naturally
what the idea that is we saw here that
though and let me tell you about the
baboon study these investigators in
France just did this study they had
these baboons in an open player area
they could go up to the computer any
time they wanted they probably were a
little hungry that they could get this
work this food and they saw four letter
strings and some of those letters all
the half of those letter strings were
words and the other half were non words
that were composed of the same letters
but they were not words so they had
different orthographic structure these
baboons were able to classify these
items as words versus non words they
didn't know the meaning but the point is
they were using the structure to
discriminate those two what letters
combination occur in what positions and
so on so they're picking up these
constraints so the idea is that when
kids see whole words are going to learn
about the individual letters because
they are separate objects and they'll
learn about them in the constraints and
tone no the correct answer brought food
they had two categories I'm sorry I
didn't explain it clearly they said word
or non work by two levers and when they
were right they got food yeah sorry you
were first
differences in languages or are you not
a quick sell the Chinese symbol in some
sense anyway well like Sherman will tell
you the Chinese that you're really
teaching them something like pinion
first where they're getting something
closer to an alphabet ok now so again in
my system I don't think I'd want want to
throw characters at the kids but rather
maybe something like pinion but like
human said earlier you know kids can
learn anything and maybe they could
learn the characters too but there's no
reason why do we have to give them the
characters we could give them the pinion
and eventually they would learn the
characters in Spanish pronunciation and
the actual letters are much better match
than an English word that's right you
can pronounce or differently right yeah
right yeah so that's a whole other
dimension of the reading Wars of whether
a nice direct mapping between the
written language and the spoken language
makes you a better reader and there's
not much evidence that it really does
and again my argument would be that
you're going to pick up the structure of
the written language regardless of how
it's mapped into the spoken language so
it could be a deep orthography or a
surface orthography it doesn't matter
good question so I know that big a sign
of trouble reading too and but the big
thing about reading is that if once you
get it you get it you might get a couple
of years late but once you know how to
read you know I already in decision you
make your vocab word and it's actually
know how to
that's right it is that somehow is that
true for speech too once you know how to
there's actually like visit for adult
readers you the thing is that your until
reader obeying you know he can be 40
years old order Delphine and just you
have a change yeah that's that's
actually a good question because I don't
know how easily we can test it because
again most of us learn to speak very
quickly and so it's hard to know that
they now you you can understand the
language right and reading again you're
waiting till they go to school and then
they learn the decoding and by fourth
grade and the teachers are saying yeah
they decode but they can't really call
the handy so again I don't know when you
say that someone you know someone's a
reader fake that might be they do the
decoding and again it's a result the
Colt readers you can actually read
things yeah yeah they did your test and
then try to make sure the church well I
think I think again it's a gradual
process in the sense that the material
you're reading so as I mentioned the
reading materials much more demanding
than spoken language material and so
that that plays a big role in it hey
there their websites where you can put
in a passage and i'll tell you the
reading level and you can see that
reading levels can differ a lot i'm
sorry we're going to have to close here
and i can talk offline yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>