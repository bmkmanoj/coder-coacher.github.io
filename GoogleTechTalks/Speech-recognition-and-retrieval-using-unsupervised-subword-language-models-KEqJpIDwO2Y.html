<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Speech recognition and retrieval using unsupervised sub-word language models | Coder Coacher - Coaching Coders</title><meta content="Speech recognition and retrieval using unsupervised sub-word language models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Speech recognition and retrieval using unsupervised sub-word language models</b></h2><h5 class="post__date">2008-02-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KEqJpIDwO2Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">nee-sama venue and it's my pleasure to
introduce you Vico kuriboh
so because I met Miko about eight years
ago I think he was with me at the EDF
Research Institute in Switzerland I
think doing a postdoc I was also doing a
postdoc at that time and then he went to
Helsinki and I think he's been there for
the last eight years
he's now visiting SR I for a couple of
weeks so well Big O thanks so much for
the introduction and it's nice to be
here and to have so many so many
audience so I'm topic today will be
speech recognition and retrieval using
unsupervised sub work language models
and the content of my talk today I first
give a short introduction where I'm
coming from group and what kind of
speech recognition system we have it's
kind of the backbone of the whole thing
and then I explain the topler language
smothers and speed three three laughing
and at the last point is to advertise a
little bit our more for challenge
competition that we have been running
for a couple of years so here is where
I'm coming from
it's about here that's sort of far away
from here
at Helsinki University of Technology
which is abbreviated as DKK we have this
Center Research Center called adaptive
informatics research center which is
divided into five groups big groups and
I'm the head of this multimodal
interfaces group which is about 20
researchers and inside that group we
have the space group which is about 10
people and we start listed here and so
of course I haven't done this whole
thing myself it's mostly verified by my
colleagues and and students here I know
so to Matthias Kreutz who was a previous
student of mine who has also visited
essa Ryan I don't know maybe not school
go but he was here about two years ago
so what is our our research topics we
are specialized in continuous peds and
unlimited vocabulary and what we try to
do is we try to develop efficient
acoustic and language models for that
and the torchic target applications are
dictation audio indexing and speech
translation and here is our webpage so
next I will explain a little bit about
our system because I think that's
important to understand when you think
about this language modeling units so
the basic system this is quite standard
you have feature extraction first then
you have a haiku stick models it's also
fairly standard then we have our own
decoder which is one pass decoder for
especially for long in chrome language
models using crossword fry phones and in
the construction we have tried to be as
language independent as possible so that
we can just plug in any language where
we have trained the models and fine so
far we have tried Finnish and Estonian
Turkish and English then the language
models since we are targeting the open
vocabulary recognition we don't have any
fixed lexicon we just have a lexicon of
sub word units which we can use for
building any any words and because the
sub work units are shorter than word
units when we need often long in crumbs
and then we have developed our own own
toolkit for for growing the variable
length in crumbs I will talk about that
more later
and here are about the more about the
target applications so it's not only
dictation but also indexing indexing
large audio and video archives or
example what's coming out from Finnish
TV or other languages and then also some
multimodal interfaces things are
interesting and I'm speeds of different
languages mixed together and then we
have a little demo in the web that you
can try pay a basic idea is that your
speak there in Finnish and you'll get
the transcription out by email well we
could plug in the other language systems
as well the actually thing this is quite
interesting because even if you don't
understand anything is you can't just
read the text I pronounce every letter a
secure phone name and it probably
recognizes you're quite nice maybe fifty
percent error rate or something like
that but you could see the system
working then we have also a little
translation demo which is basically more
just for fun but the idea is the exactly
the same interface but instead of
getting the only the text back you will
get the translation in English and also
the audio in English okay then for the
main topic today is this software
language models so why are we doing this
this is what we call vocabulary problem
here you can see here are a few
languages if you increase the size of
your corpus and this is for thousands of
words and then this Excel is the amount
of unique word forms that you will find
in your caucus so you will see that
finishes among the worst ones but it
seems to pay even every time you get new
condo kunis you'll get new words as well
which is a bit difficult if you want
have a fixed lexicon then there are
other languages like Turkish which is
morphologically quite similar very also
add things and make long works Arabic
and then in this comparison English is
the lowest especially spontaneous
English but of course English has many
other difficulties for recognition as
you know but actually this problem is
not only for speech recognition it's as
you know it's also for information
retrieval machine translation all the
applications where you need to need to
have some lexicon and then if you take
languages which have a lot of works and
you're in problems so this Estonian
Finnish Turkish and also Hungarian death
they are known as agglutinative
languages have an example here what is
the arcuti native more for watching so
it means that that you keep on adding
things after each other and then you get
very long words like this one is called
in your lacking it's not an even area
one it's quite common the idea is that
it's it's almost like a sentence if you
translate it into English oh this
nuclear crisis down there from our
current solution so it's just several
words just stuck in the one so there are
prefixes suffixes and also compound
words
so when you think about making a lexicon
of some sub word units then here are a
few questions that I thought that might
characterize the problem so what you
need to do is you need to find that kind
of units that you can use for predicting
what's coming next
so they need to have some meaning in
them the units and then in speech
recognition you also need to be able to
define the pronunciation for each unit
so you can't arbitrarily split let's say
English words or Finnish you could but
for evenings not and then what is often
the case for example in audio indexing
you will have a lot of lot of different
stuff there also from other languages
again if you think about Finnish then we
often use words that are the same as
some other like borrowing them from
English or other languages so if your
system is completely customized for one
language then you just don't know what
to do with the foreign words or just
forget them or something
which is not good if you think that they
might be the most important words and
then again in speech recognition is the
important but if you are recognizing the
soft words and your recognizer gives out
just kind of continuous flow of sub
words but that's maybe not what you want
to do if you want to dictate something
then you need you want to have the kind
of real output so you need to glue it
and tack into words so that they look
nice so you need to also to do that and
then when you have developed system for
one language of course you won't like to
try it in some other language so that
system should be easily portable to
other languages
and here is a bunch like motivating our
approach what we talk so if you think
about automatic understanding and
production of over language then you
need to know or understand something
about the inner structure so that you
can produce words so our approach is
that we took an unsupervised learning we
didn't want to be developing any any
rule-based systems so the idea is to
find the optimal segmentation of each
source text into morphs sound or
software units kind of morphemes we call
them more because they are statistically
defined and we define some cost function
and try to minimize that cost function I
won't talk much about this I just
explained that to look at what we have
you can try it there is a demo also into
that you can type in any word and you
will see how its split in the sub words
so this is fully data-driven
unsupervised machine learning algorithm
the idea is that you have a lot of text
data in the language and then you just
go through the data and see that what
kind of phoneme strings or or letter
strings of an appear there on and make a
reduction that that might be a good
suburb unit so the result is
surprisingly that it will look quite
much like the linguistic morphemes which
is kind of a nice thing we think so
and it's on language independent so
there are no morphological rules so
annotated data needed here is the code
available and it also includes programs
to segment and align all possible works
what you give them
then if you think about speech
recognition and this is only one step is
how to split the words into units
something you need to develop the
language models and last you know that
if you have very large language models
like the state of arts systems they
often have gigabytes of language small
as they are kind of a bit bottleneck in
many many things that you want to do
either you want to teach them or use
them more or just analyze them because
they are very big even for big computers
not to measure small ones if you want it
to have you recognize or in a mobile
phone or something and you didn't want
to have a large language model and also
in large language model there's
typically many rare words and because
you are estimating the probability based
on your training data you don't have
good estimates for the real words so
it's one difficult thing of course you
can build compact language models by
first training a big language model and
then starting to prune it taking out
things that are not so important and of
course it's in principle it's good okay
it's probably you might even be better
than our approach but in practice it's
difficult because then you would need to
first collect the statistics and very
trained a huge model so our approach was
to grow the Engram context from
unicron's starting from the electrons
like this example you have a unique ROM
recognition then you would like to CRO
with your things like what verse before
it would increase the information
content so you may be seeing speech
recognition character recognition there
are common things then maybe work on a
large vocabulary speech recognition is
common so you build your model by adding
this kind of pieces and this is our
toolkit that it's also public domain in
our webpage
so it's it's bit similar like SSRI
language modeling toolkit that I suppose
almost everybody is using nowadays for
training and grams except that this has
this ability to crow language models
from a short slant so you don't need to
call it huge studies they can make the
big models first and also we have
applied this a certain age more thing
which is the state of a smoothing
algorithm a it's not so straight for for
how to do it for crowing models because
you need to keep track on all the
statistics not affect to the smoothing
but now we have the toolkit and what we
get out of it is a compact set up
properly sports variable order in grams
there is one example in English you will
see that if you are splitting the words
into morphemes this is kind of end of
word tag here you'll quickly add come
into into long long engrams
and here the cost function is that kind
of how many engrams you will have in the
model and what is their probability and
the whole system now we have talked
about this two piece is the more Fester
algorithm and then this Laurie K an
algorithm is that you can they are
actually well they appeal for our speech
recognizer sort of it's the big
beginning idea was that they would be
compatible with each other but of course
you could use other language models and
other units as well system and about the
performance what we have tried so far we
have tried in language modeling for
speech recognition and some information
retrieval spoken document retrieval and
machine translation tasks in several
languages in his Estonian Turkish
English German Egyptian Arabic Hungarian
and we have been told a lot from other
people that they have tried other
languages as well so the list is growing
all the time and then we have organized
these competitions that I mentioned
where of this more faster and more okay
orders have been used as a reference to
compare what what the participants get
and also to give them first to try try
the system how because they are public
domain so they can download it and try
to solve the system try to solve the
task with our system first and then see
if they can make it better
here are some error rates sorry that
this slide is half-finished maybe you
can understand so this is our different
automatic speech recognition tasks for
different languages there is finished
Estonian and Turkish and here is the
word error rate the yellow bar here is
the most based approach that we are
suggesting and this is the third based
approach the blue bar here the idea in
this while is to show that what we see
is almost like a constant improvement in
different settings so different language
situation
this doesn't mean that finish is easier
than Estonian but it's just that we had
different tasks the finished article
speaker dependent thread speeds so I
procrastinate type thing and then
Estonian was a telephone speeds with
thousand speakers so it's more difficult
fast you get higher rhetoric but the
improvement seems to be almost constant
and it's quite significant if you know
about space error look at alright then
about using this sub words units in in
speech retrieval so how does it work in
practice well you have the transcripts
of the speeds and you've just split the
transcripts and the queries what you
have in two morphemes and then you do
the matching in the morpheme level not
in the word level and this is especially
nice for recognizers like ours that
already produces some words so you don't
even need to do the splitting because
the recognizer gives it out this is
quite important for speech retrieval
Picasa there are some rare words that
are difficult to recognize that often
are the most interesting for example
names of people or some foreign works or
something like that after you can't
recognize by your speech recognizer
you'll fail always but if you split them
in some way into sub words you may be
able to maybe not to recognize all the
software's but at least some of it so
you might be able to index the word
correctly and another thing is that once
you have the software taste recognizer
you don't need to do any staining or
other word processing for the indexing
because you already have that kind of
idea units coming out from the
recognizer so you kind of avoid this
possible source of errors and actually
what we have found how to achieve the
best performance is to add both of these
two so you do the normal word based
recognition word page indexing and then
you do the software taste recognition
and in the
and then you add the both indexes this
is kind of straightforward thing to do
in indexing you have two indexes and use
them both so some words that are easier
to recognize fiber-based recognizer than
you do see other links and then some
words that are not be able to recognize
then you use the other and this seems to
build
very good results of course near you at
all avoid all the speech recognition
errors but like many of them and then
with a soft word based index you can do
all the usual tricks that you have done
with word based indexing love and
semantic indexing query expansion
document expansion and using lattices
and and best lists there is just one
example of how to use confusion Network
for indexing so this is one one word
which is recognized like a sub word
units so first this third symbol then
about the other states and finish and
then this is skipped over skip and
bender this other
inflective suffix so the first result is
this one and then the correct result is
something different so if you would just
recognize such the index the first
that's based on the first test result
then you would miss the word of if you
are indexing based on the whole
Confucian network with this
probabilities you might get the correct
word as well indexed
and the last thing what I wanted to show
here is this more for challenge
competition that we have been having for
a couple of years now
so the idea here was that we give this
task advertise it and then try to get
other people to participate and then we
evaluate and compare who would have the
best algorithm so we had two
competitions one starting from 2005 and
one from 2007 2005 was the simplest form
just implement unsupervised algorithm
that segments all the works in that in a
corpus we evaluated based on the
linguistic code standard morphemes and
then statistical language modeling a
speech recognition then the second task
that we had last year was a bit more
complicated so not only split the verse
but find the morphemes meaning that some
of the splits might have the same
meaning or they would be some kind of
Allah morph forms of the same morpheme
so that you would also need to do some
clustering there and then we have
already did by information retrieval
also so the motivation was not only to
test our own own algorithm but pistols
on other people's algorithm and see what
kind of algorithms work the best for its
application and then also we wanted to
find out what would be a good Mexican
units for different applications like
machine translation speech recognition
information retrieval and one aspect
that was important from the very
beginning was that discover approach it
that would be suitable for wide range of
languages not only for English or
something that everybody works on
so this was the first challenge so if
the challenges are organized by this
particle network of excellence which has
been now running five years in European
Union some funding but it's of course
participation is open to everybody and
free of charts and actually most of the
participants nowadays are outside from
Europe from the US
so in this we had two evaluations what
we call two competitions first was to
compare the morphine stat the
participant scale to clean mystic
morphemes and using F measure measure
that we saw the closest one maybe this
is not relevant for applications but
it's interesting the linguistic point of
view and maybe give some explanation
about the behavior of the algorithms and
the other competition was testing speech
recognition experiments in other
language mothers were trained using the
units that the participants submitted
here are the linguistic results so the F
measure means that the higher the
percentage here the closer it is to
telling we speak morphemes and we see
here that these two are our own baseline
algorithms and these are the
participants algorithms so that actually
about half of the participants got quite
nice algorithms that were close to ours
and then the other half not so good
algorithms so different colors there are
different participants results and then
we using use the same things in language
modeling trained language models and
different languages so the lexicon size
was not fixed here so of course it
depends on the participants how what
kind of software units they are
proposing some are proposing shorter
units which are so many of them and then
some are proposing longer units that you
have a loss of lexicon so we let it vary
3 luckily our tools were able to handle
even million words of lexicon if
necessary only thing what we fixed was
the water size of the language model so
that it should be something like 10
million
Ingram's for each language so that it
could be fair for the different
algorithms they same amount of three
parameters
and then the speech recognition
evaluation we did the Finnish evaluation
by ourselves
and then all Turkish Turkish partners
did the evaluation of this recognition
and this is also two different systems
because our system was completely
homemade which is quite good but the
task was quite easy then a Turkish
switch then was kind of using the
standard HD k and a diety language
modeling tools and they try to make a
fast system that would be running almost
real-time so error rate was higher so
here are the results for finish these
are again the different participants and
now there's an opposite the more errors
you make them worse system you have so
here was our baseline algorithm is the
more faster ones and you see that some
of the participants still are quite
close to it so it seems to be quite
through robust that if you have kind of
meaningful sub word units and with our
language model toolkit you can get good
language models but not all of the
participants be able to do this
okay the full report is here you can
still find the webpage it's running and
you can even try the talks the data is
still there and we can evaluate if you
want to send us an email and we will do
it readings okay and this was the latest
thing
last year the nortonís 2007 this was
again part of the Pasco Network but we
also get help from the clef which is
another center of excellence in in here
it's kind of corresponding the track
here in u.s. so this class means
rustling wall evaluation forums for
information retrieval so why we joined
the cliff as well was that they have a
lot of the information retrieval
evaluation tasks and data so we didn't
have to collect them ourselves
and again participation was open to all
and three
and here the rules were the similar so
again you only need two we gave out of
list of words and you need to supply the
subversion it's how the splitting of
each word into software's and then we
would evaluate it
comparing to the linguistic morphemes
and then do the information retrieval
experiments using the southwards units
as index terms training data was here
for different languages and then we also
gave out some cool gold standard sample
for its language 202 the so the people
what kind of stuff there is we can't
expect that everybody understands
Finnish and turkeys so here are some
examples of different language what
could be the more for morpheme analysis
so not just splitting the words what
splitting then the words into some kind
of morpheme classes or all the morphemes
but of course you can imagine that if if
you are having an unsupervised learning
task then you don't have these labels
because you just try to cluster works
though there's a problem how to evaluate
this if you don't know the correct
labels so we solved this problem in a
way that that participants are free to
give whatever labels they want to their
morphemes we only check that those pairs
of words that share the same morpheme in
the gold standard and see that if if
those words said there anymore female
also the participants systems so if they
if those two words share the same
morphemes then we would get the point
for them but if those two two words
didn't share any any morpheme then we
think that there is an error so we took
a point of and and of course we computed
some matches in a large random sample of
the earth pairs
well those had a common morpheme so we
can again compute the precision and
recall and and then the eighth measure
so the profession great precision would
be the proportion of the success Network
pairs that the participant is suggesting
that also have a common morpheme in the
gold standard and then the recall would
be the proportion of sample from the
code standard that also the participants
such as tea that they would share a
morpheme and here are again the results
the higher inertia is the better and
here we see that there was one
participants full course it means you
significantly better results than what
our base one algorithm it was good I
think and then several participants got
almost as food and some and also because
the data was about the same size and you
can see here that the Finnish Finnish
language had about two million different
word types so if you would use the full
vocabulary you would need two million
words attend ok turkeys will had a bit
little less late about Germans I've had
a equal amount of data they only have
three 1 million different word types and
English it's only four hundred thousand
different work types if you think about
that what's the results of it closer
here you see that they actually this is
for German quite similar than for
Finnish not the same algorithms were the
best but there's algorithms that work
good in Finnish they're also good in
German and vice-versa
and English as well
and now the most more interesting
evaluation was the information for three
well one to see that which which of
those morphemes would work in practice
as well
here are the cliff data that we were
using documents from newspaper articles
and the interest test queries and then
binary relevance assessments so we are
very thankful to left because especially
those relevance assessments they are
difficult to get and it's a hard manual
labor to do them well the comparison
then we buy curiosity we made a bit more
different kinds of reference networks of
participants so of course the more first
two applications two versions of more
fester and then we apply the dummy which
means that we just put the words there
without any morphological analysis and
use them as index terms and then
chromatic all indexing which is that we
would use the actual code standard
segmentation that were given by the rule
based algorithm and use that and then
the porter spending we could try that as
well that's what the information
retrieval people usually use and then we
had one participants also tested the
hybrid method based on our our baseline
algorithm and some rules that were
trying to improve it and the evaluation
was made by the standard L'Amour toolkit
and we and there was another problem
most of people are usually using some
kind of stuff list in the information
variable but if you have your own own
units like morphemes then how would you
define the stop list that would be the
same for everybody so we solved this
file just fixing a frequency threshold
of Units that if morpheme is more common
than the frequency threshold then it
would be included in the stop list and
the frequency thresholds in our task was
something like 100,000 and it seemed it
seemed to be quite robust so if we
increased it to 200,000 or decreased it
to 50,000 results were not much
effective but outside that range that
would be important and then another
thing that we didn't first thing that
would be would be important whether out
of a couple are reverse meaning that we
had different training data for training
the word splitting and then the
information retrieval test data and of
course in at least in some languages
serve a significant amount of words that
were not in the training data and we
first throw that well we just skip them
don't care but then we ask the
participants again but maybe they could
cause a provide analysis for those words
so that is to check that does it really
make a difference here are the results
for finish
you will see here that the first bar
this is again its mean other it's
persistent which is the kind of standard
evaluation metric for information
exactly about the first bar is the one
which has the Oh out of vocabulary words
analyzed and the second part is the same
participant without the auto focus Rory
Virge
so you see quite a difference here in
Finnish and other languages there were
not so much difference and you see here
that the same participants that had the
best results in the morphological
analysis also at the best results in
information retrieval and the more
faster baseline came second then some
other participants were almost as good
then you see the for finish the porter
stemming and the dummy approach he
doesn't work very well that's all german
it's kind of significant see similar
behavior here as well maybe the
difference between of using autoflow
couple of works was not so big for
English for English you see that the
porter stemmer is the best but actually
the this unsupervised systems are pretty
close as well which is kind of
interesting thing that you can without
any rules you can approach you can get
almost desperate approach but also you
see here probably at Google you know
this but it's not evident for everybody
that if you are just using the words as
such as index terms in English the
result is not very good so you need to
do some some morphological analysis or
spending or something so this here's
it's the dummy approach so almost all of
the at least a better algorithms were
able to beat it another interesting
thing still is this grammatical so you
would think that if you're using the
gold standard algorithm for
morphological analysis then that should
be the best morphological analysis for
information retrieval as well but as you
see here
it's this one so there were a couple of
algorithms unsupervised algorithms but
better and the difference is
significance here and the explanation
that what we thought is that especially
for the real works not very important
there is always a kind of problem of
coverage based on this rule based system
so they were not able to probably model
yeah
yes yes that's actually it's not the
optimal solution probably it would be
better to split the verse using the
context
but it's kind of difficult organizing
this competition we would then need to
give the whole corpus to everybody it's
more easier to us to just give the list
of words for different reasons but
that's actually one thing that we are
now changing for this 2008 competition
that we would give the whole context and
then people could use that for splitting
that's on the length of
yeah of course it depends we can think
about these cliff queries they were
usually very long and quite specific so
it's kind of good context but if you
think about queries that it was using in
Google and then this different certainly
Hey
conclusions so you currently have about
ten different groups that are
participating in these things when which
is a Finnish Turkish English German and
then again the website where you can
find out proceedings presentations lies
from the workshop and all results and
everything and then some future
directions so for this to this year we
are adding a new language it's either
probably either Hungarian or Arabic
depends on how we get the data and then
we have been thinking about some new
tasks as well in addition to speech
recognition and information retrieval
so maybe question answering type of
thing or machine translation or
somewhere alignment this was kind of
smaller problem inside the nothing
translation anywhere to have our next
workshop maybe we could get some new
supporters so it would be that would be
interesting and then of course to get
some new and improved learning
algorithms from the participants
yep let's go glutes my talk and there
are lots of times where you don't just
break words into sub board units
we're definitely ends is why now that
was it why on the world
yes their system is it able to just take
that into account and say this is an
x-ray concatenation of support units but
there
things that happen if the batteries of
those humans for example
now our system is actually quite simple
so it only understands kind of prefixes
stems and suffixes and then whether
actually it seems that it's the
simplicity is kind of good thing because
it then in some other ways it can cover
those things so if words some words have
different stains a couple of different
stems then the system will find the
stems and maybe cluster them together
and here we are or if they have some
inflections for example like this floral
thing in English that would be several
different florals but the system could
probably find then other questions
better use this
so I actually have two questions so
first one would be so you're now
assigning probabilities to strings of
morphemes as opposed to words so that
means that you may start generating
things that are actually not legitimate
words right you're more for tactics or
to relax there so how do we ensure that
you actually get correct word sequences
out of the recognizer to to measure the
word there it and everything we don't
but in practice that's not a problem I
think yeah and also if you have the
language models in crumbs then the
income probabilities would be higher for
those softwares that make the full world
there are kind of the normal words or at
least common words but it can be like in
Finnish that you can just connect soft
words get a new word that is not in your
dictionary but it's still the correct
well but nobody just uses it I see
so so this is a good thing for finished
but I know if it would be a good thing
for English yeah so another thing is do
you know what the incidence rate of this
more for spitting is if you take my text
so what correctly so for example if you
take running text in English or Finnish
or German and then you run your
morphological analyzer on it how many
words in the original text actually gets
split because I would expect different
yeah that's actually we haven't computed
that at least I don't remember it but
but a star system is kind of statistical
it's often things that common words that
have morphemes it's not worth splitting
them because the words asshole
appear in different connections so it
I'm not sure if it's good or bad thing
in at least in speech recognition is
sometimes it's good to have long units
in Paestum for the acoustic modeling so
that they are not so confusing to each
other but it seems to pay that it's
maybe also for information retrieval
canopy in some cases they might have a
bit different meaning versus in
different forms I don't know for very
common things do we have other questions
sort of comment on what he had said
leading into a question so an analysis
factual human speech in any language any
language you want to pick the what you
would consider perhaps the error rate
what's actually said not an
interpretation not the another computer
side but like the speech production
itself has probabilistic error rate
attached to it and this is how like over
large periods of time you see
unification of languages and evolution
of new words and shift in pronunciation
and whatnot
so it's not really accurate to say that
the underlying language which you're
modeling has strict boundaries because
it's it's if all it's not just evolving
but the actual individual speaker is
making regular errors now towards that
end the error rate for a particular
individual speaker is very dynamic it's
something that's going to be a function
of some constant error rate that they
experienced plus their stress in the
current environment their level of
distraction and having some kind of
model of the underlying rate at which a
particular speaker is making those
errors would perhaps give you a better
tuning parameter for deciding what
they're actually trying to say which is
something in practice that actual human
listeners do we try to sort of model how
frequently someone is miss speaking and
broaden our search for what they're
saying
and spend some more time on that and you
know if someone's making a lot of
frequent errors we start dropping the
probability that there's using
improbable words we start focusing on
the much smaller set of probable words
but doing a lot more searching in that
space and so in that regard have you
looked in that direction for ways of
possibly dynamically tuning how you do
the morphological search based on what
seems to be the doing error rate that
you're encountering yeah that's that's a
good point yeah we haven't haven't
really considered but I think it could
have some reporters but of course it's
also different and here we have been
mostly dealing with flange speeds or or
read speeds or something but when we
turn into spontaneous speech then those
errors start to appear questions
so you said that your speech recognition
system is based on crossword try phones
I was wondering if you've had the idea
or done the experiment to use your sub
word units that you've learned and and
make a sort of a whole word model it's
except using those sub word units
basically trying to learn the
pronunciation or the the sound of each
piece of those sub word units to try to
model the that context the context
that's happening within those units does
that make sense yeah well we haven't
tried that I'm not sure if it if it's a
good idea because there are still kind
of tens of thousands of of those units
and yeah I don't know
and the other questions did you have a
sense of the decoding speed changes
between using word units and
subordinates and the fact that you have
to use longer context and stuff like
those in practice is it really changing
there are these we don't recognize all
right no no it's well we are not really
really real time so it's anyways it's
maybe two or three times real time so it
doesn't make a difference but often with
the toes along in crumbs you can
actually forget forget the past quite
quickly you will see that the best
hypothesis will only differ a little r3
southward southwards and the rest will
be quickly fixed so it doesn't really
seem to cause any delays
no more questions okay so when it
studied the speaker again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>