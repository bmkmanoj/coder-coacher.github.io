<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Vivek Kumar Rangarajan Sridhar- RESEARCH SCIENTIST -- NEW GRAD | Coder Coacher - Coaching Coders</title><meta content="Vivek Kumar Rangarajan Sridhar- RESEARCH SCIENTIST -- NEW GRAD - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Vivek Kumar Rangarajan Sridhar- RESEARCH SCIENTIST -- NEW GRAD</b></h2><h5 class="post__date">2008-07-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vwhdo-kMgrc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everyone thank you for
inviting me to google my name is Vivek
you don't have to pass the rest of it
I'm from university of southern
california and the theme of my talk
today is deriving and utilizing rich
representations in spoken language
translation so before i go into the
details of my talk just a brief
background about myself i'm from chennai
and i joined us see in 2002 after
completing my bachelor's in india and
i've been working in the speech analysis
and interpretation love with the
professor Sri corner and since 2004 I'd
also like to clarify that the the talk
is predominantly going to focus on the
work that I did as part of my
dissertation but I also worked on I've
been fortunate to have been involved in
several other projects funded by several
agencies I'd be more than happy to
discuss about any of these projects in
detail after my talk okay so this is
going to be the outline of my talk so
i'll start with the general big picture
motivation of why this work is important
and where it fits into the larger scheme
of things in state of the art and speech
processing i'll then formally the
problem of using these so-called rich
information in a speech to speech
translation framework after which i'll
step back and tell you about the various
techniques that we've developed for
automatic enrichment of automatic
enrichment specifically talked about two
aspects one is prasad information in the
other is disclosed information after
which I'll demonstrate the utility in
speech translation and conclude long
with some directions for future work so
let me start with an example so in so
imagine a doctor-patient interaction
scenario where the doctor is talking and
this is the output of automatic speech
recognition okay so the question is
beyond this string of words what more
can we infer so there are a lot of
things that you can say so if you had to
do speaker recognition here you can
identify this as speaker 1 and if you
were to find out from a database you can
index speaker 1 as being the doctor you
can also say that the prominent words
either syntactically prominent or
prasada CLE prominent words in this
particular utterance our chest pains and
for two days you could also imagine
chunking this utterance in two different
prosodic faces or syntactic faces and
further say that the conversational act
of this utterance is a yes/no question
so these are the kinds of
rich information beyond words that we
are interested in in this work and I'll
talk about these things in further
detail so more from an application
perspective I would also like to
motivate the problem from what are the
different applications that this
research is useful for I'd like you to
keep these applications of the back of
your mind and see how it relates to the
rest of the talk as I progress so one
typical scenario is a customer call
application so typically in call centers
you receive a large volume of calls with
accurate detection of what the the
customer is saying you can either read
out the calls or use it as a quality
assessment tool so if for example if you
get a large number of questions you can
see that there's something wrong with
the system another area that is of
increasing interest in especially to
Google from text is information
extraction so you're interested in
finding out what is the important
entities given particular text so I'm
going to play you a video here the point
I want to try to make here is going
beyond lexical information and syntactic
information you can also extract rich
information using prosodic aspects in
acoustic aspects of speech so the output
that you see on the right side is is not
real time but the output is the system
that I developed as part of my PhD work
also tax returns the Clintons have
finally made their so as you can see
here the words that are marked in green
are actually the prasada CLE more
emphatic words as well as the
parentheses indicate racing but prasada
creasing using post duration and other
kinds of information so this is one
application the third application that
is of particular interest to us is an
enriched speech-to-speech translation
framework so conventional applications
treat speech-to-speech translation as a
compartmentalized task so you have a
source signal being converted to source
text using a sr source text being
converted translated into target text
using machine translation and then
target X being synthesized on the target
side using text to speech synthesis so
as you see a lot of rich information
that's presser in the source side is
lost during the translation
process so the idea here is if you were
to take the hypothesis on the source
side find out certain rich information
about these utterances or word level
information such as say the intention of
this particular sentence is a yes/no
question you can not only use that
information within the speech
translation framework but you can also
use it to augment the translation so
I've given an example here the
references is there's a painkiller for
this sentence in Farsi the hypothesis is
this is a painkiller even if you look at
the hundred based hypothesis you always
get this as a painter on the other hand
if you read a transfer the intention
from the source site of the target side
as a yes no question you can either use
it for disambiguation or sentence
reordering on the target side so these
are some of the applications that the
talk that I'll be describing is going to
be applicable to in terms of a research
goal the basic research goal of this
work is to develop statistical
frameworks to detect an extract which
information from speech and text and
this pertains to both representation as
well as modeling of these events and
then we are interested in exploiting
these annotations in spoken language
processing specifically in applications
such as speech translation and speech
understanding so the question that pops
up is why do you think it's important so
we believe that extracting and detecting
this information can provide valuable
complementary information that can
inform several linguistic modules in
speech processing so having given a
general motivation let me formulate this
problem so as I mentioned a few slides
back conventional speech-to-speech
translation system has these three
independent sub-blocks so you try to
optimize the performance of each block
individually say the ASR with the word
of the rate metric machine translation
blue score on this score the TTS with
some mean opinion score some subjective
evaluation metric the problem here is
that the focus of the translation itself
is the content so you do not utilize a
lot of rich information that means
basing on the source side of the speech
such as what is being said in the source
side how it's being said and as I
mentioned the compartmentalization by
itself introduces a lot of noise and
channel so I hypothesis here is to
convey not only what is communicated but
also how it's being communicated having
said that how can we improve current
systems so there's been a plethora
work recently especially in the empty
community and using a lot of linguistic
information in translation specifically
things like syntax as well as morphology
so in contrast to these approaches what
we are interested in is to augment or
supplement speech-to-speech translation
systems with these rich so-called rich
annotations these can range from either
/ sonic prominence phrasing discourse
information to simple things like
disfluencies topics and so on so these
have been previously used in both spoken
language understanding an automatic
speech recognition in several nest
evaluation tasks but we are interested
in applying it for speech translation
okay so this is a basic problem
formulation I don't want to go into the
details but contrast this conventional
approach with our approach so
essentially you have a source speed
signal and you want to get to the target
speech so the compartmentalize approach
converts the source speech to talk
source text using ASR either the one
best hypothesis or a lattice on the
source side to the target text using
machine translation and then synthesized
is finally on it using the tts so in
contrast what we are interested in doing
is let's for now assume that we either
have a channel tations on the source
side or on the target side so at the
output of a SAR what we are interested
in doing is inferring these rich
annotations from both the speech and
text information using these annotations
within the machine translation framework
as well as supplementing them for the
TTS part so it's it's it's in spirit
different from conventional approaches
so I would like to clarify that even
though there are a lot of possibilities
that you can use this framework for we
restrict ourselves to two particular
kinds of rich information one is
prosodic information the other is this
course information captured through
dialogue tags so I'll explain what these
are in the forthcoming slides so let's
say that we did have access to this LS
and LT so-called rich annotations either
on the source side or on the target side
the question is to use it within
translation how do we detect these
automatic alee and we need to also
detect them robustly okay so I'll just
give you a brief background about what
what kinds of prosodic aspects we
address in this work and what kinds of
discos information and dialogue
information
read address so the two aspects of
porosity that we are interested in is
prominence and facing so let me explain
this with the help of an example so you
have the sentence here that says the
Pentagon reports fighting in six
southern Iraqi cities so if you want to
look at the more prominent words here
you would say this as the Pentagon
reports fighting in 60 then Iraqi cities
so prominent is nothing but these are
properties such as intonation rhythm and
lexical stress patterns that confer
emphasis intent and emotion to certain
linguistic units you could also imagine
raising this are trends in a particular
way you could say that the Pentagon
reports fighting in six southern Iraqi
cities so that's way in which you boss
and other durational aspects so that's
call is facing so essentially you're
chunking the utterance in to Posada
phrases which helps you an appropriate
interpretation of a sentence so the
problem with detecting these kinds of
prominence and facing information is
that you need some kind of reliable
intermediate representation to to
facilitate robust supervised learning so
for that what we use here is a
categorical representation of prominence
in tracing using the scheme called as
stones and break into since framework so
I won't go into the details of it but
again give you an example just to
understand what it is so essentially if
you have a sentence and the pitch
contour of the sentence there are two
labels namely the pitch accent and
boundary labels that represent phrasing
so each word is a sign or a syllable is
assigned a particular pitch accent
boundary tone the presence of that label
means that that word is accented and the
absence of that label on the word means
that it's an unaccented question at the
same time for racing what it does is it
assigns a number from 0 to 4 higher the
number the larger the perceived
disjunction between two words so we
essentially assume here that we have
access to human annotated data that has
have these kinds of labels and we can do
automatic supervised learning with this
now the problem here is if you look at a
typical corpora typical corpus you have
a really large collection of labels okay
this is almost like 28 of 32 labels so
do reliable learning on such a large
vocabulary label vocabulary is quite
difficult so in literature what people
do usually is to either to knock these
labels
to intermediate categories or into very
coarse category so presence of any of
these labels means that it's an accented
version of the word absence of these
means that it's you know there is no
action of the word similarly for the
break indices so these are the kinds of
categories that we're interested in
automatically detecting on the discourse
side what we are interested in doing is
detecting dialogue our tags which are
nothing but labels that represent
communicator works so if you have an
utterance what you're interested in
saying is this utterance is a statement
it's a yes/no question or an
acknowledgement and things like that and
typically in English as well as in
several languages such as spanish and
dutch these kinds of dialogue tags
highly correlated with lexical syntactic
as well as international recuse so i
won't go into details of this because of
time constraints absolutely for both of
those is a reasonable agreement in terms
of you know if you get somebody to do
the labeling so the internet agreement
for this one is about eighty four
percent and so that's a couple score of
about eight which is point eight that's
reliable so the porosity part is it's
depending on the corpora it's almost
from eighty to ninety percent okay hello
greetings greetings okay can you guys
see the slides no
alright
okay chekko it yes ok so again to just
give you an example these are the kinds
of dialogue acts that we had automatic
sorry I'm sorry with the question you
said 85 said agreement that was on the
labeling for for the dialogue and I like
that cross any part of us it's it ranges
from eighty to ninety percent depending
on the corpora so for spontaneous speech
it's slightly less accurate for red
speech it's much more accurate so the
experiments that I'll lesson in the
latest slides I've done it for both red
speech and spontaneous speech and I'll
compare the accuracies that have
obtained with inter 90-day agreements
okay great thanks okay okay before we go
into schemes for automatically detecting
these these kinds of rich information
there are some challenges one is to
choose a reliable representation that's
what I was talking about so you need
some kind of human annotated data that's
reliable with high interlayer agreement
the second thing is that the Q is the
lexical syntactic as well is super
segmental sorry Q's occurred typically
at different time scales so there's
usually no consensus on a generative
model and they also highly overlapping
features so you need to do smart ways of
feature selection before you use these
features in a framework another problem
is how do you infer an in-form these ace
or an empty in lockstep with this
information so you are essentially not
interested in doing an offline
computation but also an online
population so you get these information
and use it immediately in other
linguistic modules so the proposed
solution for this part of the work is a
maximum entropy technique so i won't go
into the algorithmic details but tell
you more of why we used it for this take
for this particular task and the details
of the implementation so we use a task
essentially here because it's a nice way
of handling overlapping features and
it's not hard by strong independence
assumptions and we also figure that it
was a robust way of combining different
pieces of diagnosis contextual evidence
we've also done it with crs and other
things but it doesn't scale as well as
the maximum entropy taken so essentially
the formulation here is given a sequence
of linguistic units these either could
be words so the porosity labels we are
interested in predicting at the word
level the dialogue act label very
interested in predicting at the
utterance level so given a linguistic
here and a particular label who cavalry
we are interested in estimating the
distribution using the marks in
technique so the features here that we
use are the lexical features and nothing
but the words in terms of the word by
grams and trigrams the syntactic
features that we use or window are part
of speech tags which automatically
extracted the label said is 47 tags from
the penn treebank we also use a shallow
syntactic structure in terms of super
tags super tags are nothing but their
talks with encapsulate the predicate
argument structure in a local tree so
these I obtained from the extract tools
if you pen so that's the syntactic
feature part so we also use acoustic
features in this one so the conventional
approach of using prasada core acoustic
features for this kind of work is you
would take an entire segment extract the
pitch contour of that segment and
extract as many semitic statistics as
you want such as the slope range mean
minimum and so on just throw everything
into a philosopher and let the
classifier learn what the important
features are so in contrast to this it
was more of an approach that we strayed
at sra and it's still being used we are
interested in exploring a different
alternative so what we do here is given
a particular segment once we get the
prasada contour we normalize the
sequence of values with the speaker
specific means and variances we quantize
these acoustic prosodic observations in
terms of some quantization precision and
then we extract n-gram features out of
this acoustic basaltic contour so
instead of getting a summative
statistics we are interested in more of
a fine-grained approached by the extract
engrams of this acoustic Posada values
so I will tell you and also the next
couple of slides why this performs
better and give you some numbers so what
is the normalized pitch contour I mean
did you go is that stiff occurs no
that's just the pitch contour value this
is a bitch extracted at every 10
millisecond intervals ok what is zero
mean or 0.25 that that's compared to the
mean for this speaker Oh over here yeah
so it's basically it's a xenon okay it's
a xenomorph the value that you obtain
here so every speaker has a speaker
specific mean and variance so you take
the xenon and that's the 0 okay and then
you quantize that value into some
position that you want so depending on
the amount of data that you have in the
performance that you are interested in
you can quantize into into bins
then extract Engram features the idea is
to the way in which marks it uses
lexical features you're interested in
using acoustic features in a similar
fashion and then you want to compare it
with the conventional approaches that
use all the slope and pain and those
kinds of things and see how well it
compares pizza ok so you're using see
norm if it's voice then if it's unvoiced
what do you do so if we first perform
linear interpolation smoothing and
everything and then only then we take
the contour and then apply the signal so
we have a pre-processing step that we do
the linear interpolation in smoothing
linear interpolation by envoy sales
there will be regions of speech that
would not be boys so we do linear
interpolation for that so if you have
two segments of voice and then we just
linearly interpolate the contour there
he's earning interpolate the contour so
that oh there's a value for every frame
value 5 reframe exactly and there's
seven numbers there is that do though
and that's just an example nothing over
here yeah those seven numbers right yeah
that's just basically a 70 millisecond
interval so it's every 10 milliseconds
every time I say that's time ok ok ok so
having given the motivation so let me
just explain what we are interested in
detecting and porosity so given an
utterance we are interested in
classifying each word into a particular
category so it can either have an accent
or no accent so I have just indicated
knocks and with a star because who is to
break otherwise and either a boundary
tone or a break index oh and a larger
number for break index means that there
is a phrase back there ok so these are
the kinds of super labels that we
interested in classifying each word s
and then they are as a they can be used
as a function of either prominence or
facing ok the data that we've used for
these experiments are range from in two
categories to genres one is a red speech
corpus that's essentially we use the
Boston University Daily News corpus
that's been used for ages since 90s for
experiments and porosity we also used a
unique spontaneous speech corpus that
Mari was kind enough to gone gone for
our particular purpose essentially it's
a subset of the switchboard corpus which
is
completely human annotated so I don't
want you to get over well with the
numbers but the point I wanted to make
in this table is that for the BU corpus
we use for speakers and the validation
is performed as leave-one-out speaker
validation similarly for the bdc corpus
and for switchboard corpus we do a
10-fold cross-validation it has about
128 speakers so the point again being
it's different genres of speech it's
different vocabularies and also much
larger training and test sets ok so
before I go into some of the results I
would also like to see say that the
results are all reported on identical
data sets used in previous work so
they're exactly same training and test
splits used in more classical Johnson at
UIUC so we borrowed the training and
test blitz and they have several papers
on performing this automatic detection
so if these are accuracy numbers a means
by using only acoustic features s is by
using syntactic and lexical features and
a plus s is using all the features
availability dopest so if you look at
the numbers for both the Patuxent and
the boundary tone scenarios our
accuracies are reasonably good and the
Patuxent accuracy is also slightly
better than previous work but the
problem here is that if i were to
extract the acoustic features for a
given word i need to know where it
starts and where it ends so i need to
know the time index ok so essentially
you need to know some kind of force
linemen knowledge so that's not always a
present so we went back to the drawing
board and what we did here it was more
of a speech approach so what we did was
we trained continuous density hidden
Markov models to represent these speech
pitch accents and boundary tones
essentially instead of the 39
dimensional NSCC feature you have the
pitch the energy the energy value as
well as the Delta and oxidation
coefficients so you train a hidden
Markov model over this ok that also
leads to a very general question can you
know that maximum entropy model is very
good for using lexical and syntactic
features the hmm model might be good for
using the acoustic features is there a
nice way of combining these two models
so what we came with is essentially an
FST implementation for this so what the
syntactic model the max and model does
is gives you a posterior probability
table
so here each word has either an accented
version or an unaccented version can
that's the posterior probability so what
we did will be converted this into an
FST where each pair of links each pair
of nodes have two links one is the
accented version the other is an axe
integration on the other hand which what
the acoustic model gives you is simply a
lattice what we go ahead and compose
these two lattices and then find the
best path through this lattice so
essentially it's a way of combining the
output of a discriminative model with
the generative model then here in the
acoustic model you don't have to rely on
the word boundary information so these
are again some of the results using this
approach as you see by going from the
mark sent acoustic syntactic model to
they combine maximum entropy place
hidden Markov model the results that
we've got using both acoustic and
syntactic features are all better than
previously reported work on identical
training and test sets so it is also
good in the sense that since we decouple
these features so if you had access to
only speech recognition and only the
acoustics you can use this acoustic
modem if you are doing it more for a tts
purpose you can use only the lexical and
syntactic features and if you're using
it for things like speech understanding
or dialogue modeling you can use both
the acoustic and syntactic features so
as I mentioned we've also done this for
phrase break detection just to remind
you of the problem essentially at each
word you want to detect what the break
indexes and higher value of break index
means that there is a phrase break there
so you would say the sentence is the
Boston School Committee religious and
community leaders have already come to
loggerheads over the issue so again I
just don't want to go into delve into
details but we've compared it with also
state of the art out of the box labels
such as festival tedious you know
festival as well as 18d natural voices
toolkit have these kinds of small
modules that can predict these so we
tried it on those things too by using
the same approach that I proposed in the
previous couple of slides we get
absolute improvements of almost four
percent or previous work for face break
detection also and we've also tried it
for spontaneous speech so there was this
j qi workshop in 2006 with
processor may be helped and they were
interested in structural event detection
so we did it on identical test pits as
they have done and even for that we are
results almost stead you know if you
perform statistical significant tests
they are not very different from the j2
results so in summary I would like to
summarize the part where I've done the
porosity recognition thought this is
more of a chronological sequence of
things that I did in my graduate school
life so we exploited simple local
lexical and syntactic features and we're
being able to reliably detect prosoniq
prominence and facing so in comparison I
don't want to go through previous work
there's a lot of efforts that have a lot
of hand label data so the kind of
features that we extract or come at a
very minimal cost we're just getting
words syntactic information and the
acoustic information from the pitch
contour and the Engram feature
representation seems to be a nice way of
characterizing intonation it's kind of
different and it's not been done before
and the results are also close to the
internal data agreements and as part of
this particular subset of my work I've
also developed a tagger that can work
both on red speech and spontaneous
speech so given not friends it can do
all this stuff that I mentioned here and
give you spit out the values okay so
let's move on to the dialogue act part
suggest again to remind you given
utterance what we're interested in doing
here is either detecting this utterance
as a very detailed category such as it's
a statement opinionated statement non
opinionated statement to guess no
question or is a very simplified
dialogue our tax ID so in a lot of
applications we don't want to find a
very detailed level but you just want to
say if it's a question a statement or
acknowledgement now there are two
challenges that are prevalent in
automatic dialogue after detection one
is we all know that okay so if you say
yes no question typically your pitch
races and things like that but how do
you meaningfully use this kind of
information in an actual application so
they've been lot of efforts at sra and
other places which derive summative
statistics of the pitch and energy
contour as i mentioned a few slides bar
so can we do better than that that's
question one the second question is how
do we exploit dialogue history so in a
lot of cases you know that if you say
something that's an opinionated
statement the other person on the other
side is either going to give you a neck
mentor you know he's going to refuse it
at the same time if you ask a question
the other subject is most likely going
to give you an answer in terms of a
statement so the classical way of doing
this in previous work is to use an hmm
like approach where essentially the
states or the dialogue add tags and the
observations are nothing but the words
so instead of building a language model
over the words you build a discourse
language model or the dialogue and tags
okay so before I tell you why our method
is different and why it works better
what are the two corpora that we've
tried it on so we've tried it on the
switchboard damsel corpus which is
essentially a generalized stock said you
can think of it as being applicable
little variety of tasks it has a 42 time
vocabulary but not in all cases you
don't need this such such a deep
vocabulary so you just need a very
simple vocabulary such as you know you
just want to find out whether it's a
statement acknowledgement letter it's an
abandoner trains agreement question
appreciation so this is based on
previous work we've also tried our
experiments on the not marked as corpus
which essentially is more of a task
specific domain so I have some numbers
first for the results for using only the
acoustic information so what I want to
compare here is previous work that have
used some rate of statistics so the
numbers here have absolutely no
knowledge of lexical or syntactic
information by just using acoustics how
well can you do so by using previous
work and a very simple decision tree
classifier the accuracy is about 45
person by using the Engram acoustic
features that I described in this work
we are able to get almost a six percent
improvement over previous work and by
using these features in a maximum
entropy framework you can get a little
bit more juice here so this is all just
using acoustic features no lexical or
syntactic features can you go back to
the slide yep when you say you're using
n-gram features are you talking about a
generative model where each of the
different no this is not a generative
model this is the maximum entropy model
so given a particular autrany say oh so
these are all in the
maximo yeah these are all in the maximal
so the other thing that I mentioned was
exploiting dialogue history so if you
had to do again an hmm like approach and
to do optimal decoding you would have to
do a viterbi forward backward or
decoding the Disco's Ellen you
essentially have to build up the trellis
and you have to wait for the entire
conversation to complete so there are a
couple of approaches to Oakham this one
you can do a greedy decoding approach
where at each state you make the
decision based on only the information
the previous state but in contrast to
this what we are interested in doing is
coming up with an ox and based online
dialogue at target essentially so for
example for utterance three if you want
to make the decision what we use here
are features from utterance three
lexical syntactic and Posada features
and only features from previous offenses
so we don't want to wait to find out
what the dialogue ragtag of the previous
references but just dump all the
features from previous occurrences and
make a decision at this particular point
so it's more like you can do the
decoding in an online fashion okay so
how well does it compare with the greedy
decoding so the blue bar here for map
task and this is for the switchboard
jamsil corpus with 42 tags and the
switchboard damsel corpus with seven
tags so obviously seventh are cases a
subset so it performs slightly better
it's less confusing the green or the
blue bar represents greedy decoding the
green bar represents the maximum entropy
technique with oracle knowledge so you
essentially exactly know what the
previous three dialogue are tags are the
orange bar here represents max n with
only features from three previous
occurrences so you don't assume any
knowledge of dialogue acts but you have
only features from previous offences so
if you see for all the three corpora the
drop in performance is not that
significant so you can still get really
good performance of dialogue attack
detection almost as well as knowing what
the previous dialogue Arctic is and the
immediate question you know then I give
this talk is why did you try only three
previous appearances you know you could
have tried four five six so there's not
much information that you get even going
beyond two previous offenses so the most
knowledge is present in what the person
said previously compared to what the
person said six turns before
so just to demonstrate that I'm not
tried it only on reference transcripts I
also tried it on SR output so this was
actually a separate project that I did
in grad school so this was an acoustic
model that I trained just a speaker
independent model trained at about 250
hours of Fisher corpus and you know the
usual tricks they were speaker after
training on the first bus lattice and
things like that and the language model
was also quite robust with no publicity
so I tried the same test set with a sr
output so they were the rate for this
switchboard corpus that I sure who chose
is quite high it's almost thirty-four
percent one every so if you look at the
performance again for greenie decoding
with a sr output with mark sent by
assuming that you know what the previous
three dialogue act tags are and with max
end with only features from the three
previous appearances there are gained
quite close so overall in terms of
performance there's almost a ten-percent
drop from reference transcripts to ASR
output even with you know when the word
rate is quite high so you can get almost
seventy to seventy-five percent accuracy
for what the dialogue our tag is at the
output of asa success heavy maybe it's
an expert the talk but or are you going
to ever feedback this information into
the ASR process and maybe change the
acoustic models or the language models
given conditioned on the yeah actually
got logs date yeah well elect I mean the
rest of the doc will be about how to use
this in speech translation so okay there
was this work that I did for about six
months as part of graduate school life
where I was trying to play around with
these features feedback in the haze are
and use it for restoring the lattices
and things like that I wasn't getting
substantial improvements and always with
something that people had grinded out
for a long time so it was more
challenging and using these kinds of
things in translation cut it but I did
try those things so in summary for the
recognition of dialogue acts so the
Ngram features seem to be capturing the
pitch patterns better and exploiting the
features from just previous occurrences
is almost as good as using a separate
discourse model and as this subset of
this work I also developed a dialogue at
Agra which essentially will take the
utterance either will just take the
speech signal to a sore get the features
and give you the dial
based on what corporal you want so if
you want yet model train in the map
tasks corpuscle you give your labels
pertaining to the map thoughts corpus or
to switch both got this so just as an
offline point offline optimal decoding
by constructing the entire lattice going
forward backward algorithm gives you
about 71 percent accuracy on the
switchboard corpus this is results of
stalled que tal from a sorry by using
the online decoding framework it's about
seventy two percent so you can't
directly compare these two numbers
because it takes sets are different they
had about a 4k tested and we had about
twenty thirty thousand or princesses it
tested so it's a much larger essay and
it seems to be performing reasonably
well okay so I've told you the tools
that have developed for automatic
detection of porosity and dialogue add
tags and fortunately they've also
performed reasonably well in comparison
with other efforts in literature so the
question is how do you exploit this kind
of information in translation so that
was the question we were trying to ask
how do we detect these things so I've
told you how ok now I will tell you how
to use these kinds of information in
translation so first I'll talk about
dialogue add tags and how do you explore
them in translation so we have two
approaches here one is free space
translation that I extended and the
other is an approach that we express in
it by straining boss bangla at all at
AT&amp;amp;T so I took their system and try to
extend that system to incorporate
dialogue act acts so i will talk about
free space translation first so everyone
knows this visa not basics so given a
parallel corpus of you know sentence a
line parallel corpora we generate word
alignments and then essentially generate
the face translation table from the
world lineman's and the heuristics so
essentially you have the source source
faces and then the target faces so in a
typical phrase-based translation system
you have the torah source text being
converted to target text now let's say
you can also find out what the source
dialogue our targets so that is Ellis so
if you were to decompose this using base
decomposition it essentially leads to a
dialogue a specific phrase translation
table and a dialogue act specific
language model instead of just having a
face translation table for the entire
data and a language model now you have
dialogue act specific translation and
language
now you can do translation with this
kind of model but the problem is if you
have a really large face translation
table and you're splitting it into seven
or eight different translation tables it
leads to a lot of out of vocabulary
faces so one particular table might not
have the phases present in other table
so you need to come up with with smart
ways of of backing off to the original
translation table so what we do here is
essentially build a translation table
with the entire data well a dialogue a
specific translation table and then be
interpolated with some page so basically
this is a backoff factor you can think
of it essentially as a language model so
you want to adopt your language model to
a topic specific language morning so
let's take this actually use an example
which will make it much clearer so if
you have this fire in corpus a english
chinese and you have n sentences here
what you do is on the source side you
down each sentence with a dialogue out
tab so it lead to different translation
tables so if you have seven different
dialogue tags still have seven different
translation tables you built a phrase
translation table for using the entire
data you've been the face translation
table for each of the dialogue act
specific translation tables and then you
interpolate the phrases here with the
baseline in translation table with some
weight so you optimize that weight on a
held out set so that's the system so
I'll tell you how it works in
performance but I also like to describe
before that what the bag of words
approaches so the Tong so it's I hope
it's clear what the phrase space
translation approaches and what the
extension make me the bag of words
approach is essentially given an entire
source utterance okay what you want to
do is independently detect each target
word so if you have n words in your
vocabulary you want to detect what is
the probability of seeing that seeing a
word X in the presence of this entire
source sentence so you detect each word
individually and then you take that bag
of words and reorder them with a
language model
so essentially given an entire source
sentence you form in grams of this
source sentence and detect each word in
the target vocabulary independently so
the idea here is you can use n static
binary classifiers because whether the
presence of a word in this context is it
whether it's present or not you can do
it independently okay now the problem
here is doing it is not difficult but
the reordering is a big pain because all
that you get is a bag of words how are
you going to pull me on this bag of
words so if you have a really large
number of words as 20 words you can have
25 to real combination of these words
okay so what we do here is we take the
bar before its approach and also assume
that we have knowledge of what the
dialogue art art is so essentially
instead of getting a bar code words in
target we are getting not only we are
not only using the information of the
dialogue tag on the source side but we
are also transferring to the target side
so instead of just using the backward
grams on the source side you also know
what the dialogue tag of the source side
is so you factoring it within the
translation and for reordering the
advantage is that when you try all
permutations of the words weighted by a
general language model now you have
access to a dialogue act specifically
which model so you can hope that a
language model for statements is going
to perform much better than you know a
language a general generate language
model or if you have a language model
for questions it can perform better than
a generate language model ok so we've
tried these experiments on several
different corpora we've tried it on the
farsi english corpus that was obtained
as a part of the trance track effort
it's basically doctor/patient mediated
interactions it's it's microphone speech
the japanese english corpus is the homie
I help you corpus that I obtained from
AT&amp;amp;T which has telephone speech and the
Chinese English corpus is the I wasn t
corpus that is usually released for I
wslj evaluations so again the point I
want to make with this big table is that
these are different language pairs the
training data substantially changes it's
about 8,000 offenses here 12,000
utterances here and 46,000 or twin
sister
the Chinese English case okay so these
are some of the results for each
language better for the face based
translation approach for the bag of
words approach this is the blue score
without any dialogue attacks so if you
were to just do plain translation this
is with the dialogue our tags with the
seventh our case and the 42 attack case
whatever half here is the F score f
square is nothing but it's just the
intersection of number of words in the
result in reference so you take the
precision and recall and then compute
the F score so it doesn't take into
consideration the phrases but just how
many words you get right in your result
so if you were to look at the approach
for all three language pairs we are able
to get improvements in the blue score
for the Farsi English case for the
Japanese English case as well as the
Chinese English case the improvements
are a little bit more pronounced for the
Chinese English corpus as well as the
Farsi corpus compared to the japanese
english corpus and similarly just
because of the nature of the model you
can expect because we are not interested
in generating phrases but only in bag of
words the Bhagavad model gives you much
better lexical selection accuracy so the
F scores are much better than using a
phrase-based translation approach but
obviously the blue score is not that
good because you're reordering is not
very robust way of reordering so how do
these numbers compared to you know
competitive systems that don't use da
tags on these that are only trained on
these tasks is this all day of the earth
I wouldn't say that so I don't know
about the farce English corpus but for
the Japanese English corpus it's
definitely as well as good as the FST
framework that AT&amp;amp;T used in the prior
work for the iwi Sedley coppers it's
quite competitive so then your
improvements are better than in promises
is done on these tasks yeah the I
wouldn't say completely it's almost
close to their previous best results but
what we have done here the point that's
not made by the table that I would like
to make in the next slide is we've not
only improve the blue score and these
kinds of objective evaluation metrics
but we've also given this be facilitated
the transfer of knowledge from the
source of target side so that's actually
not evaluated here so if you know that
as a source of transverse
statement we've also transferred the
target returns as a statement so you
know that knowledge but that is not
evaluated here to do that you need human
evaluation so we did a small-scale
experiment in the lab by giving people
not only the improved translations but
also what it meant on the source side
and you know I don't have published
results for that particular experiment
as these but almost nine and ten people
would prefer having what the intention
of the source speech was rather than not
so they would rather have that
information then not have that
information so if you get imagine a
two-way communication with a speech
translation device you would rather give
that information in a dialogue scenario
so that's not computed here so that's
that's something that conventional
systems not offer so we also did some
analysis of results in terms of where do
we get all these gains from so we did
you compute the blue score without
dialogue arcs and with dialogue act tags
for each of the dialogue actor category
so as you can see the blue score
improvement comes most from questions
acknowledgments and appreciations not
that much from statements so it's it's
actually in pewter because statements
don't have kind of Q faces that are
characteristic you can say anything that
you want in a statement but there are
certain q faces like are you going or is
it is it something that you want you
know these are the kinds of Q phrases
that seem to be captured by the dialogue
act specific translation table used for
questions that is not captured by the
statements and the reason I have put the
distribution of the number of dialogue
on taxes you cannot just understand this
tables without looking at this because
the total number of statements is almost
ninety percent the number of questions
is quite small the number of other
categories is also quite small so you
need to look at these improvements and
results commensurate with the number of
instances they appear in the training
data so as you see statements are really
high but we're not getting much bang for
the buck they're given the questions are
quite low we get a much improved blue
score there so what we did so we also
analyzed the 42 toxin are you I don't
have the space to show what were the
most important tags in the 42 third case
but things like WH questions yes no
questions and open
questions those are the kinds of
dialogue attacks that were very
important how am i doing on time great
did we have I'm sorry 12 minutes left
again some time for questions right so
in terms of the using dialogue tags in
translation by using the phrase space
translation were able to get better blue
score for the bag of words translation
we were able to improve the lexical
selection accuracy but again the point
is these are all objective evaluation
metrics people believe only when you say
that you know you improve the system by
seven so percentage but we've also kind
of i know i've not seen work that is
kind of transfer the intention from the
source of the targets I people are
always interested in improving as many
words as possible but you also
transferring the intention so we've
shown improvements in the objective
metrics for the language space and it's
a novel technique that can potentially
you be used for disambiguation as well
as in a dialogue scenario to facilitate
cross-lingual communication okay so
let's move on to the part where we can
end which translation with prosody so
before I go into the details of it so
let me show you conventional system so
you have a sr a machine translation you
have a hypothesized noisy text okay so
it could possibly be erroneous so if you
had to detect the porosity labels from
this target text you are using the noisy
texture it's a very inefficient way of
doing it in contrast what we can do is
factor the frosty labels within the
translation framework so there's this
whole effort of using factor translation
models recently where people are
interested in incorporating linguistic
knowledge such as morphology and lemurs
and other kinds of things so can we use
the porosity labels within the
translation framework to get enrich
target text and hope that this enrich
target takes is much better than doing a
post processing approach so essentially
you want to couple it with in the
translation system so I go over these
slides actually quickly I don't want too
much of time I thing
ok so the proposed framework here is at
the output of a sr what we are
interested in doing is instead of just
the source text going to target ticks
they are also interested in factoring
these labels on the target side and then
hopefully using it for texture speech
synthesis so one easy way of doing it is
considering each label on the target
side is a compound token so you have the
word and then you have a particular
velocity label associated with it and
you do the entire same free space
translation that you want now for a
source word and a compound to open on
the target side now the problem with
this is say this particular Japanese
were goes to call but you have two
different millions of call you can
either have call as an accented version
or you can have call as an unaccented
version so different words might go to
the same version of call but with
different compound tokens so that's
essentially a problem there so what we
want to do here is use a factor
translation approach so this is the kind
of this is something that I took from
Philip Cohen's work so it's we want to
factor other kinds of rich information
either on the source side and order or
on the target side and improve the
translation between the words okay so
this is our fractal translation approach
essentially what we want to do here is
instead of going from the words two
words in the target side we want to
generate words as well as prompt city
labels on the target side this is model
one the other model is we go from words
to words on the target side and then
generate this so essentially you have a
generation table where for each word you
know what prosody label you're going to
generate okay now the advantage here is
that you can build the alignments in the
word so you don't have to build a
London's and the compound tokens and
have some problems and you can
essentially factor the porosity labels
within the translation framework okay so
these are some experimental results game
so I've shown you lexical F score and
the Blue score and something called as a
price adik accuracy sopra sonic accuracy
is number nothing but the number of
labels that are correctly detected in
the intersection of result and the
reference
divided by the total number of correct
words that you get in the reference so
if you look at the prosodic accuracy for
the post processing approach and compare
it with the two different factor
translation modules you get almost an
eight to sixteen percent improvement in
assignment of these labels so by
factoring these labels within the
translation framework and not doing it
in a post processing approach we're able
to get much more bang for the buck here
okay so I didn't want to go in depth
into the porosity part but just to give
you an idea this is recent work at ICS
NP the factory translation models can
end which target output at no cost or at
the word level performance so we don't
get a degraded blue score of the word
level but we get a much better prosodic
assignment accuracy and models can also
be particularly used for tonal languages
such as Chinese where you have to detect
the lexical accent on the target side
and you need not just use paucity but
you can potentially integrate any other
word level rich annotation in the
framework okay okay in conclusion what
we did here in this stock was I proposed
a novel framework for enriching speech
translation which focuses on the
transfer of prosodic as well as
discourse context along with the content
we integrated both the source and target
rich annotations and we were able to
achieve some promising improvements in
translation quality on the other hand I
also proposed some innovative techniques
for automatic Crossley recognition as
well as dialogue our classification and
we were fortunate that the results that
we obtained here in red also
state-of-the-art and these are also been
developed as targets which I hope to
make public soon so more in terms of
future work in plants so the rich
representation part I'm very interested
in putting the work on costly labeling
to other languages so I have done
everything only for English so I want to
see how well it performs in other
languages the other problem here is I'm
relying on supervised data to do all
these things so I've been doing some
work recently on doing semi-supervised
adaptation so given a new corpora how
can you bootstrap the labels based on
the model that you have in hand and in
terms of using the representations in
speech translation of your ultimate goal
would be to develop
that you can unify speech translation so
essentially have a sr and empty as a
couple system and use all these
representations both with an ASR as well
as the machine translation system I've
also done some work recently on so I've
all only described the pitch accent part
ferocity but I also done some work on
using prosodic racing for input
segmentation as well as reordering so if
you get these big indices that I
mentioned on the target side you can do
a lot of reordering on the target sites
improve my performance okay so you know
this is more in terms of future plan so
hopefully I can use all these skills to
solve interesting problems in speech and
language processing and makes a
meaningful contributions in both
research and developing prototypes so I
also worked on several different
projects especially on the more on the
ASR part but my thesis focused on this
so i have a quick video actually this is
on using human-like speech processing by
using articulatory information so what
you see here are being it's basically
the sagittal view this is the upper lip
lower lip and this is the tongue contour
so this is a incest or this is the
tongue tip tongue body and the tongue
Dawson and this spontaneous speech so as
part of this is a separate work in
graduate school life I collected cleaned
up created this database as well as
round lots of experiments for improving
a star performance using articulately
features so let me take this video
so essentially the sensors are recording
the movements of the articulate is and
you can use it along with the mfcc
teachers some of these publications
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>