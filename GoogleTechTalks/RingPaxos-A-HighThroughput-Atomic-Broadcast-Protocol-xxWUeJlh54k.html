<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ring-Paxos: A High-Throughput Atomic Broadcast Protocol | Coder Coacher - Coaching Coders</title><meta content="Ring-Paxos: A High-Throughput Atomic Broadcast Protocol - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Ring-Paxos: A High-Throughput Atomic Broadcast Protocol</b></h2><h5 class="post__date">2011-03-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xxWUeJlh54k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">&amp;gt;&amp;gt; 
SCHIPER: ...his research is mainly on the
dependability and software base replication,
so.
&amp;gt;&amp;gt; PEDONE: Thanks, Nicolas. Thanks to you
all for attending. So, in fact, before I get
into the technical talk, I thought I'd tell
you a few words about the place where I come
from because you may not all have heard of
this place. So, I'm a professor at the Università
della Svizzera italiana, which sometimes is
translated as a University of Lugano. Although,
it's really correct because there is also
a campus in Mendrisio, another city, and the
Canton Ticino. The university's at least is
as beautiful as the view, this is a mountain
of Lugano. Just in case, so Lugano is south
of Switzerland. In that trip, University of
Lugano is part of the Swiss university system
with ten cantonal universities in two federal
universities. So, you have your ETH and EPFL
and all the others are cantonal universities.
And so, the computer science department or
informatics, as you call it, is located in
this campus. It looks like this big hard disk.
And it's the fourth faculty of the University
of Lugano, and it started actually recently,
in October 2004. We were six professors back
then and eight PhD students. And it has grown
quite quickly today. We are 21 professors
and about 100 PhD students. And so, in terms
of fundings, it's the third largest after
the two polytechnic schools, ETH and EPFL.
So, Nicolas, so we have bachelor courses in
informatics and computational science, seven
master programs, and a PhD program. And then,
Nicolas said it already, so own my research
at USI, it's on distributed systems and distributed
data management systems. We do mainly systems
work and also a little bit of algorithms these
days. And my group has five PhD students.
Okay, so that's the end of our introduction
mainly. We can now jump to the technical talk.
So, I'll tell you about some work that is--we're
researching in the group in the concerns of
high-throughput atomic broadcast protocols.
So, this also is a joint work, a demo from
my PhDs, Parisa, Nicolas [INDISTINCT], and
another student that's whose now is working
at Apple in the U.S. So, this is an outline.
We're going to introduce this work and then
tell you about the system model or the systems
we make, and then--so Ring Paxos, itself,
the protocol that we will propose. A lot of
related work because this is a very--so many
works have addressed similar problem. The
performance evaluation of the performance
and then I'll conclude the talk, so. And so,
I'll start by making a statement, right? I
can use that to a very few who are trying
to refute. So, replication is important for
fault-tolerance. I'm not saying it's only
for fault-tolerance but in the context of
this work, it's mainly for that. And if you
are doing replication, there are two base
techniques to actually keep the consistency
so I need to show a link in the replication
of strong consistency. So, although, there
are many different algorithms, they all boil
down to different kind of models. Active replication,
sometimes called state machine replication,
or passive replication with primary backup.
So, this talk is about state machine replication.
So, in a few--with a few of this slide, to
define state machine replication, all the
replicas they start with the same initial
state of the service in replicating. And that--and
you apply all the same set of commands in
all these replicas and in the same order.
And then, therefore, they are going to proceed
through the same set of states. So, although
I don't have it in the slide here, one of
the simplest of state machine replication
is that the execution, the commands, is deterministic.
That's why executing the commands, the same
commands in the same order, is going to leave
us with the same set of states with each replica.
Or it turns out that these two requirements
here to applying the set of commands, same
set of commands, and in the same order, I
can characterize this by a communication primitive
which is sometimes known as atomic broadcast
or total order broadcast. So, this is basically
is an abstraction. It's one way of encapsulating
the two properties that I mentioned before.
So, atomic broadcast, it requires--it guarantees
two properties. The first one is agreement.
So, if a process delivers a message and also
review as--one of the replicas executes the
command, then all the other correct processes
or all other replicas are also going to deliver
or execute this command. And total order basically
means that all the replicas are going to execute
these commands in the same order. So, the
goal of this work is basically to implement
a high-throughput atomic broadcast protocol.
It's how to guarantee these two properties
that I present before with very high-throughputs.
So, knowing that I just told you about what
we are trying to do, right, I'm going to spend
some time telling you about the environment
in which we did this work. So, we got them,
it was started by this work. The first assumption
is that we consider communication by message
passing on, which basically, this means that
we are not were doing this, we do not assume
that there's a physical shed that the memory
shed by the processes. And so, in the communication,
the message passing communication, we assume
two different kind of primitives. One is that
allows us to do one-to-one communication and
the other one is one-to-many, right, which
basically translates into IP multicasts, so.
And since, they're not strong with respect
to the communication itself, messages can
be lost, they can be reordered, but we assume
that they cannot be corrupt. And then, we
focus on the crash-recovery failure model
versus it can crash you can recover, but what
is in this, we assume it is in this property,
is that we are not targeting Byzantine behaviors.
Or the processes either they do, or they are
supposed to do or they don't do anything at
all, right, that they don't try to be malicious
and break the protocol. And the system is
partially synchronous, by which I mean that
the--so, to guarantee that the former properties
I don't, in safely that they are violated,
I don't not have any synchronous assumptions.
They can assume that my process is executed
in speed and the message can take another
try--a long time to be delivered, if they
are delivered at all. But I can only guarantee
that something good will happen in the system,
that messages get delivered, if there are
some [INDISTINCT] about the synchrony, which
basically means how long it takes for our
processes to execute some step and how long
it takes to actually deliver a message. So,
there's only--again, these are only needed
to prove largeness properties, determination
of these, for the protocol. It's not merely
for safety. So, if there's synchronisms that
are violated, there's different points not
going--that it behaves incorrectly. Meaning
processes are not going to deliver a message
in different orders. Okay, so let me tell
you about Ring Paxos, the protocol that we
have developed. So, Ring Paxos, it's based
on several--on Paxos, right, networks and
protocol order, protocol. And in several very
practical observations. So, basically the
deal was, if it was you took the Paxos algorithm
and then we apply simple practical observations
to turn this algorithm into a high-throughput
point. So, what are those observations? The
first one is, so if you compare IP multicast
versus Unicast, and they have a single sender,
one process sending messages to a set of destinations
and these destinations varying from 1 up to
25, one can see is the following. So, in this
graph here, on the XX, is I have the number
of receivers and the YX is I have the maximum
throughput that each receiver can get. So,
if I have, and there are--so there are two
lines like one is, I'm doing the communication
through IP multicast and the other one is
through Unicast. So, it's pretty obvious that--so
the dashed line is Unicast, of we're saying
to a single process I can reach throughputs
that are close to the maximum allowed by the
network, it's one gigabit in this case, but
as soon as I have a higher number of receivers,
I have to distribute my ongoing bandwidth
and then basically the processes are going
to receive less information. Now, if I do
this with IP multicast, and this could delegate
it to this reach of the infrastructure to
actually send you up the messages and that
it can sustain much higher throughput, almost
regardless of the number of receivers. So,
this has a number of fine prints, right, it
assumes here that all my receivers are connected
to the same network switch and the switch
can actually--it's good to know, to have your
IP multicast in an efficient way. So, that's
the thought observation. It's quite simple.
So, in these experiments here, we're both
getting too much. And the experiments we managed
to get more than 900 megabits per second,
while with 25 receivers it goes down to 35.
So, that's the thought observation. IP multicast
is preferred over several Unicasts. So, IP
multicast is unreliable, right, in this space
on best effort. So, the question here is really
how can we minimize message loss? As I said
before, our protocol, it can tolerate message
loss. But the performance is really is best
if they don't happen because if message get
lost some mechanisms will need to actually
to retrieve, to recover, this message. So,
with that, avoid message loss if you can,
although the protocol can do the right thing.
And then, what you do here is some, pretty
simple flow control and some management of
socket buffers. I'll say more about this later
in the talk when I mention this experimental
setup. So, that's one of the things that was
done to reduce the number of lost messages,
but not only that. So, if I take multiple
processes doing IP multicast, what I observe
is the following. Since there is no flow control,
if I have multiple processes IP multicasting
massages, what may happen is that, and you
pin it at the rate at which they do this,
right, it can have conflicting message being
sent at the same time and there's a danger
that my buffers get full and message gets
lost. So, what we did in this experiment here
was to run--so at similar experiments, that
the sender, one or more senders, try to send
as much information to the receivers as possible.
So, number of receivers here is fixed. I don't
really remember how many we have but I think
it was like 50. It doesn't really matter.
But what happens is that the more receivers
I have simultaneously IP multicasting messages,
if I want to minimize my number of lost message,
I have to be conservative and reduce the rate
of sending from these, of these processes.
Now, I don't have that problem for a single
receiver, that he can really send as much
as almost 900, a little bit more than 900
megabits per second. And then, only after
that I start having some lost message. But
I have lost messages much earlier if I have
more senders. So, and I would get to send
me at a rate 400 milliseconds, it would be
a minute per seconds, or it doesn't really
there how many receivers I have. But there's
this increases, I actually, I stop having
more lost packets. And as I said before, the
protocol can handle it but it's going degrade
the overall throughput. So, the third observation
is one multicast sender. I can actually higher
throughputs is I have a single multicast sender.
And finally, the other optimization, the other
aspect that Ring Paxos is taking into account
is the flow rate if I have multiple senders,
right, sending these more messages to a single
receiver. And once I present you the protocol,
it's going to be clear why this is and important
aspect. So if you have n-senders, one receiver,
and I want to minimize the overheads in the
receiver is that I can communicate these in
two ways. So, I can actually have each sender
sending it small message to the receiver,
which is not good because the receiver gets
to actually this, some more overhead involved.
The receiver has to receive all these three
messages. Or I can do it in the following
way. I can have one process send me this small
message to the next, which is going to then
pair its own message to the next and so on.
And eventually, the last one sends the whole
thing in the single--in a single message.
So, what I'm reducing here, I'm actually reducing
two things I just derived here. I have single
message being received, right, there is less
overhead at the communication level and there
is single context switch in the process that's
receiving all these messages. So this pattern
of communication is preferable to the first
one. So, the ring overlay reduces the overhead.
So Ring Paxos basically is all these observations
put together in the context of Ring Paxos
algorithm. That's pretty much what it is.
But it turns out that by doing this we can
reach the levels of throughputs that were
unseen in Ring Paxos implementations. So,
let me tell you a little bit about the Paxos
algorithm, really about the merge, but only
after I can explain it more and tell you how
Ring Paxos works. So, Paxos is--it decomposes
the processes in three agents. So, we have
the proposers, the process that sends the
messages, the commands in the state machine
replication if you would. And the learners,
these are the processes that receive these
commands and executes all the state of the
application. Actually, through that idea,
the proposers are like the clients of the
service. And then we have the guys in the
middle which are those who define the order
of this, of these messages. So, for example,
in a single--so Paxos is a single, is a sequence
of instances. In a single instance, what happens
is it gets the message from all the proposers
and decides to accept and decide one, one
with these methods, and they are sent to the
learners. So, the similarities between the
Ring Paxos and Paxos. The most important one
is that Ring Paxos, it shares the same reliability
as Paxos. And this is an important thing because
Paxos is an optimal algorithm in terms of
communication steps, in terms of receiving.
So there's no other protocol that can tolerate
in the same assumptions, can tolerate the
same number of [INDISTINCT]. So, it's the
same to reliability as Paxos. So, we also
have learners, acceptors, and proposers. And
there is also the notion of a fixed coordinator,
which is basically, very quickly, is one acceptor,
right, is chosen among the acceptors. And
this is the process that defines the order.
It's one process that tells the order and
the other acceptors they basically they confirm
the order with the coordinator. As I said,
the coordinator is an acceptor. So, let me
compare here the two algorithms. And what
I'm going to show on the Paxos execution is
really is an optimize implementation of Paxos,
as proposed by Les Lamport's original papers.
So, we have the proposers, we sent their values
to acceptors. So, actually to the coordinator,
one of the selected acceptors. The acceptor,
the coordinator, gives the order of this message.
They send this order to the other acceptors.
This is known as a phase 2A message. Okay,
it's 2A because there is a phase 1, which
is basically initialization, if you will.
Its only need is you change the coordinator,
right? So, in normal execution, you don't
actually have to do this, we can execute before
you are sending the messages. So that, they
don't come from a throughput, it's only initialization.
The acceptors receive this message and that
in most--in the normal situation, they confirm
the order that's proposed by the coordinator
and they tell the learners that they have
confirmed this order. As soon as the learners
have a majority of acknowledgements or phase
2B messages, they can deliver the message.
So, the condition here to deliver a message
is receiving a majority of the 2A message
from the--in the acceptors. So, that's how
Paxos works generally speaking. So, Ring Paxos
works in a slightly different way. So, the
important thing here is, for example, if you
implement this using IP multicast, right,
you'd have two way IP multicast, one for acceptor,
if you want to maximize the throughput; which,
as you saw before, is not good. So, Ring Paxos
work like this. Proposers, the outer, sends
their values to the coordinator and the coordinator
instead, it sends a 2A message to everyone,
to the acceptors and also to the learners,
right? When the learners get this message
here, they don't really know what to do with
it because the message, the order, hasn't
been confirmed by all the acceptors. The know
though that there's some good chances that
this order, that this message here, is going
to be delivered in the order it has received.
Because as I said before, unless there are
multiple coordinators, [INDISTINCT] can actually,
Paxos tolerates, the order that is--the coordinators
propose is going to be confirmed. And then,
and that's where the Ring comes into play.
Once the acceptors receive the 2A message,
with the message content, the first acceptor,
after the second acceptor in my list, it sends
a 2B message. Not to all the learners or to
the coordinator but it sends to the next acceptor
in the list. And the next one is going to
send to the next and so on, until eventually
the message reach the first one, the coordinator,
which we know that now a number of acceptors
has agreed to deliver this message in this
order and then all that, what the coordinator
does, it sends the decision to everyone using
another IP multicast. So, what do we achieved
here is, so there's a single process doing
IP multicast, it's the coordinator. The communication
that these acknowledgements here, which are
usually a very small messages, it's just saying,
&quot;Okay, we agree with this order,&quot; they do
not cause many interruptions at the coordinator
because we are sent through a ring, and then
these messages here actually carries information
saying, &quot;Well, all acceptors have agreed with
this message. We can go ahead and we can send
the decision.&quot; So, there's some other optimizations,
which I'll discuss later. But as you can imagine
already, so Paxos is a sequence of these instances.
So, after this decision here, what's going
to come next is another instance of 2A message
with [INDISTINCT]. So, we can actually batch
all these things here. And in reality, I don't
have a special IP multicast with decision,
I have them batch with my next phase wave
messages. So, let me show you--so actually
here, so there's a single IP multicast. So,
this is a small animation that shows actually
how this--how the protocol works. So, we have
the proposers sending the message to the acceptor,
the acceptor gets it and then assigns a sequence
number, and then stores the message in a buffer.
The reason why it stores the message in a
buffer is because in addition to all of the
things that I told you before, we also batch
multiple messages if there is more. And then
IP multicast sends the message to everyone,
they all get it, and they store it in a buffer.
Now, they store it in the buffer because they
don't really know what in this order is going
to confirmed or not. But as I said before,
the first acceptor in the ring, or the one
that precedes the coordinator, is going to
send its acknowledgements to the next acceptor,
which is going to add to this [INDISTINCT]
module and eventually the message reach the
coordinator. When he has enough acknowledgments,
he knows that his message can be delivered.
So, it waits for the next message to be received
from a proposer, it's going to assign a sequence
number as before, buffer rate, and then it's
going to take the message and the acknowledgements
for the previous message and send to everyone.
So, when the learners and acceptors receive
this message, they can deliver the previous
one and then they store the more current one
in the buffer and then it goes on. And that's
how the protocol works. So, it's a very simple
idea, actually. But it turns out that it allows
us to get very high throughput as I'll show
next. But before I get to that, I'll just
comment on some of the properties of Ring
Paxos. So, the coordinator, it's the one that
forms the ring, so someone has to take the
list of acceptors and define this ring overlay,
it's basically overlay. So, they're going
to communicate, this is delegated to the coordinator,
and it is the last node in the ring, for obvious
reasons, right, it's the one that's going
to actually send the notification, the decision
of the [INDISTINCT] at the end. So, the message
that are exchange among the acceptors in the
ring, they are actually just an IDs. Obviously,
I don't send the message contents because
these ones have been sent already originally
by the coordinator. This is sometimes called
indirect consensus. And then that it serves
a multicast to all, okay? I mentioned that
already. Now, another thing is, so I briefly
mentioned that Paxos is an optimal with respects
to resilience, it requires a majority of acceptors
to define the order of the message. So, if
you have N acceptors, you need N+1 over 2
to accept it. Obviously, you only place a
majority in the ring, you don't have to place
all of them, and this will reduce the latency
of the protocol a little bit. So, the proposed
values, that actually are the message contents,
are the disseminated through the phase way
is the first message that the coordinator
IP multicasts then the first acceptor in the
ring is the one the starts the 2B message
in acknowledgements. So, number of optimizations,
so I--phase 1 can be done. So, phase 1 is
a part of that. I didn't show in the protocol
is this initialization that is done whenever
I change the coordinator. This one can be--can
be best for a number of consensus and done
actually right after the coordinator is elected.
So, that's why it doesn't count for the latency
of the protocol. So, we don't claim any number
in here, this was discussed by Lamport when
he actually proposed the Paxos algorithm.
But we use it. Now, we do batch for phase
2 messages, as I mentioned before. So, not
only batching multiple values in a single
phase 2 message but to also batch the receivers
together the phase 2 message. So, I'll remind
you what we are trying to optimize throughput.
We don't degrade in latency too much but the
really--the goal here is to maximize throughput
and therefore, there's no way to avoid batching.
We also have multiple concurrent instance
of consensus. We don't have to wait for one
to ten minutes to start the order. You start
them all a sequence of instances to also maximize
throughput. And I committed these already,
but these are piggybacked to the next IP multicast
value. So, you might be wondering, so this
is--I'm describing actually the best case
when everything goes right but actually the
thing is not always good, right? And we have
to handle coordinator failures for example.
It happens--what happens if the coordinator
fails? So these are detected by the acceptors
which are going to elect a new coordinator.
And then because--so ring paxos doesn't change
paxos in any aspect that's fundamental. The
safety is guaranteed even if they are multiple--at
some point there are multiple coordinators
in the system and instead of [INDISTINCT]
though the protocol is just too resilient,
nothing bad is going to happen and this is
due to the--actually, the original paxos properties.
I can also have lost messages, right. It can
happen that the message gets lost in the ring
from one acceptor to the other. In such a
case, what's going to happen is the coordinator
is going to wait for some time until these
technologies from all acceptors [INDISTINCT]
for a second time, it simply assumes that
some message was lost by the acceptors and
it resends it. But, of course, if you--the
coordinator also suspects that one of the
acceptors has failed, it has to redeploy the
ring and then resend the message. Okay. And
so they send. Now, there are also some chances
that messages got lost at the learners, right?
So the way this is handled is if the learner
sees a gap, so it receives message number
13 but doesn't receive message 12, it suspects
that, well, something wrong happened. And
the way we handle this in ring paxos is by
having learners ask some of the acceptors
for a retransmission. So we try to avoid contacting
the coordinator because as you may guess by
now, a coordinator is really the most model
process in this system. I have number two
for later. So instead of asking the coordinator,
we ask some acceptors. And you can also distribute
the learners among the many acceptors. Okay.
So, that's it for the protocol. I'm not going
to give you anymore details. You can clarify
doubts later if you want. Let me tell you
a little bit about the related work and I
will do this in a quick way. So there's been
a lot that's done in the context of atomic
broadcast protocols. It's a theme that has
been largely studied. You can--I guess if
you try to guess there must be more than 60
different algorithms. Now, very few of these
have been actually implemented and they try
to optimize different things. They can be
actually classified according to a number
of parameters like, for example, unknown classification
is what I'm showing you, kind of groups that
within--in different ways. So, to a certain
extent, ring paxos, it would be like a fixed
sequencer. Although it doesn't really fit
that very well any of these categories. Now,
what I have here is some known algorithms
that either they are in this list because
they have very good throughput as well as
the case of LCR. The not--well, relatively
recent compared to the others. It has very
good throughput, right? But it's different
than ring paxos in the number of ways than
is discussed from experience. Another very
famous or maybe known protocol is Spread.
Maybe some of you have heard. So Spread is--it
exists as open source and is used in several
projects that rely on replication. It's based
on a Totem single-ring approach. Now what
are other approach that are somehow similar
Ring+FD which is--so we are excited to hear
because it's also used a ring. And the last
two are two implementations of paxos. The
first one is LibPaxos, it's only based on
IP multicast and the last one is all based
on point to point communication. So there
are two extremes of the spectrum. So a quick
comparison between all these algorithms and
the ring paxos here is at the bottom. What
I can see is that--so in terms of communication
steps, for a certain number of failures, it's
not necessarily there in the other protocols.
I mean, if you have then some [INDISTINCT]
failures. And in terms of number of processes,
it's also not necessarily the best protocol.
So LCR can do with fewer processes or you
can get F as--F is the number of tolerated
failures in my system. So LCR can do a fewer
process than the ring paxos. The difference
is that LCR is based on much stronger assumptions
than ring paxos. What I mean by stronger assumptions
is basically I cannot tolerate any failure
suspicion mistakes, so I'm assuming stronger
synchronous assumptions. I cannot--again I
have to determine with certain precision how
long it takes for a process to execute and
how long it takes for a message to be transmitted.
And if I get this wrong, I may actually deliver
messages in the wrong order. So the assumptions
here are stronger than the unit here. Well,
so I guess what really matters, we took these
protocols and we actually implemented the
ones that were not available and used the
ones that were to going work--product available
and to compared it to ring paxos. So the comparisons
that I'm going to show next, we assume there
is no disc writes because that's the way most--some
of these protocols work. Ring paxos can be
turned actually to use--to write or use either
synchronous or asynchronously. But the data
that I'm going to show you next does not--does
not--does not write to disc in none of the
protocols. I'm assuming also a best case which
is hopefully the most common case, no faults--no
faults. So, the experiments are running from--through
the [INDISTINCT] in the average. For ring
paxos, I'm assuming packets of 8K and also
a buffer to keep all the message as 160 megabytes
and the network at one gigabit. So this is
the first--the first--the graph in kind of
its throughputs, so bits per second with respect
to the number of receivers. So I'm comparing
the five protocol that I showed you before.
So the graph kind of separates these protocols
in three groups. So we have ring paxos and
LCR from the top. Then we have Spreads, so
these are--this--the Y axis is large scale.
So, we have the Spread coming next. And then,
finally, I have Libpaxos and Paxos4sb, so
these are the paxos implementations. So there
are few things you can see from this graph.
So, the first one is that the only ones that
we get close to the--to the theoretical limit
are ring paxos and LCR. And they are pretty
stable, right, with respect to the number
of receivers. LCR, the particularity about
this protocol is it places every process in
a ring; not only a subset of them and it doesn't
use IP multicast. So the only way to actually
get this level of throughput without relying
on IP multicast is by placing our processes
in a ring for a very simple reason, right,
because I can use the incoming throughput
and outgoing throughput of each process and
in doing that, can actually get you close
to that. But if I don't do this, the only
other way is by using IP multicast. It can
actually be seen from the other systems. So
a Spread does not use IP multicast and so,
it performs the grades as increased number
of receivers and the paxos work system [INDISTINCT]
doesn't use it either. Now LibPaxos does use
IP multicast but every process--so it's an
implementation that's closer to the original
paxos protocol. It was a fact that every communication
is through IP multicast. So it has pretty
stable throughput but because we have several
processes actually sending this message, I
cannot send message at that very high rate;
otherwise, we have too many message loss.
That's why it is basically the lower end of
the--of the graph. I can also--I can also
look at these algorithms not by the number
of bits per second but also the number of
message that each one sends. Although, in
order--so we tried to fine tune these protocols
to get best performance for each one of them.
So they have used different packet size. As
I said ring paxos uses the best result to
obtain with 8K packets. LCR, for example,
to get the best results, we have to increase
the packet size. So, a consequence of that,
okay, I send fewer message. And it is the
same for the--the pattern of the--of these
curves here, it closely follows the previous
one. But it only sends--it only--but it shows
the size of a message that actually gives
us the best throughput as performed first.
Now, what I'm going to do next is I'm going
to concentrate on ring paxos and on LCR because
these are the two ones that actually reached
the highest throughput. Okay. So this table
here basically sums up all the protocols by
showing--so by using a metric that we call
MTE. It's the maximum throughput efficiency.
This--basically, what it does is it tells
you how the protocol is doing with respect
to the capacity of the network, right? So
since we're running our algorithms in a one
gigabit per second network, the closer you
can get to one gigabit, the higher the MTE.
LCR and ring paxos got 90%. So LCR is a lot
constant as I've showed before. We've got
here a [INDISTINCT] vary which is a little
higher than the ring paxos. All the other
protocols, we have a much more maximum throughput
efficiency. So let's have a look at the scalability
of these two protocols. And this is another
difference between ring paxos and the--and
the competition. So with respect to throughput
the--up to 25 receivers, they have a similar
throughput although for a small number of
receivers, the LCR is there. But the problem
with LCR is--besides the stronger synchronous
assumptions is that CC+ it places all the
learners in the ring, right? The latency of
this--so the average latency third learner,
it increases linearly to the size of the ring,
right, which is pretty obvious, which doesn't
happen with ring paxos. There is a degradation,
a little bit, and this is basically due to
how the switch handles IP multicast. Now,
we have run a whole bunch of other experiments
and I'm going to tell you briefly about this.
The first one is the impact of message size
and it's clearly--and this concerns only ring
paxos for bigger message so we can get actually
higher throughput and the latency doesn't
change much. So, we set--that's why we set
all the--in other experiments, the message
to 8K. So this graph here, it's basically
a little bit more complicated from the Spread.
The experiments are the same as the ones that
I showed in the previous graph. And so as
I have fewer messages, so the other messages
are smaller, right? I need more messages to
be sent in order to reach a higher throughput
as the size of my message increase in the
right, I need fewer message to be actually
sent per time unit. And the same here is--so
here's the size of the message with respect
to the size of a single packet. So we're sending
packets which are 8K. If my messages are of
size 8K as well, so I'm sending one packet
per message, otherwise, I'm sending fewer
packets per time unit because I have to actually
form a bigger message until it is sent. There's
also some influence from the socket buffer
size although--so it does have an impact on
the--on the throughput and the latency. Those
that can see the scale here, it's from 800
to 900, so the difference is not as big as
you would think first if I look at the picture.
But there is a difference if you set a smaller
socket size--socket buffers, you're going
to have less throughput. This is in part due
to the fact that they have more lost message,
they have to do more retransmissions and the
latency as well. So what we did is we got
an externalization before we set the socket
buffer size to 22 megabytes. Now--yes, the
more important fact is really how CPU is used
in all these--in all these protocols. So we
have the four--the four roles. So I have the
Proposers which basically send the messages
and in their case, they have--so they don't
do anything other than sending those messages.
So they use 37% of the CPU. The Coordinator
is the one that uses most of the CPU, so 88%.
Acceptors don't seem to be a big problem and
the Learners are not a problem either. So,
for the Coordinator is really the part to
concentrate. So that's why if you would send
message, for example, from the Acceptors to
the Coordinator using other communication
pattern, chances are that you wouldn't actually
reach the same level of throughput. So to
conclude--so ring paxos has constant throughput
based on the ring on IP multicast and it doesn't
have much in terms of latency. Besides, it
enhance from pax--the resilience of paxos
which is, once again, it's an optimal protocol.
You cannot do better than that. Now it has
this--the system has been implemented, actually,
it's [INDISTINCT] as open source, it has received
some interest since it was--it was not published
that long ago, and had a number of downloads.
What we found is that it was a good portion
of the downloads come from China. I don't
know if that means anything. So that was what
I had here. I don't know if you realize I'd
be happy to answer any questions or clarifications
of things.
&amp;gt;&amp;gt; The acceptors, why do they have to agree
on a certain sequence? Because this seems
to be the whole problem that's a group of
machines has to agree on a sequence.
&amp;gt;&amp;gt; PEDONE: Yes. So they have to agree on the
sequence number which is--in other words is
they have to bring the order of these messages,
right.
&amp;gt;&amp;gt; Yeah. And why it does it--why do the acceptors
have to know about the sequence number. The
coordinator could just push the data to the
learners in some arbitrary sequence...
&amp;gt;&amp;gt; PEDONE: You know, that's just the--so they
have to agree on the order. They have to actual--they
have to agree on the order of the messages,
right, to make sure that if a coordinator
fails and another one is elected, the new
one is not going to contradict what the previous
one did. So that's pretty much what they are
doing here. And the reason why a majority
is because if there is a majority, right,
you can--it can crash these acceptors in any
way but it's always going to be one that remembers
what the coordinator did. And then a new coordinator
can actually learn about the--the new one
learns what the previous one did. This is
actually--it's what happens in phase1 which
I was very quick. This initialization for
each coordinator is to learn what the previous
one did by means of the acceptors. So that's
why...
&amp;gt;&amp;gt; What type of CPU [INDISTINCT] than multicast?
&amp;gt;&amp;gt; PEDONE: Yes. So [INDISTINCT] wasn't multicast.
Let me show the protocol because I think it's
a... Okay. Yes. So, you can have--anyone can
move a message here, right. If the Acceptors
move a message here, then--so the first acceptor
moves the message to never send the phase
[INDISTINCT] because it doesn't know about
the message. So the second is our unit and
then the coordinator is never going to receive
the confirmation, right? So the way you would
handle this is after we resume--this message
here starts a timer, if it doesn't receive
a confirmation after some time, then it resends
the original message.
&amp;gt;&amp;gt; To everyone.
&amp;gt;&amp;gt; PEDONE: To everyone, yes, through an IP
multicast. It could be because it actually
doesn't know who doesn't have it, right? This
must have been received, the confirmation,
so it resends. So that's for the Acceptors.
We can have--you can have messages lost by
a Learner. So this message here, what's going
to happen is the next time the Learner receives
a message, it can detect that there is a gap
and then it's going to ask for a retransmission
from the Acceptors. Yes?
&amp;gt;&amp;gt; What happens if one of the machines, an
acceptor one, goes down and how does the system
respond to that failure?
&amp;gt;&amp;gt; PEDONE: So you have to--you have to constantly
monitor it. So the coordinator has to--has
to monitor the other machines to know whether
they are working or not. So if the Coordinator
suspects that one of the Acceptors went down,
it has to replace this one by another one.
So, and...
&amp;gt;&amp;gt; Back to phase1 and redo the ring?
&amp;gt;&amp;gt; PEDONE: No, not necessarily. So here, we
only have a majority of Acceptors in the ring,
so it means that we have two other spare machines,
right? Because it's--the protocol is 2F+1.
So you--because I have to tolerate two fares
I need five machines. We don't have to place
all the five in the ring. Actually, you don't
even want to do that. So if it suspects that
one has crashed, what it does simply is it
redoes the ring, right, with some local information
and then re-multicast the message. So it doesn't
have to go through phase1 again because it's
the same coordinator. So phase1, only is executed
if the coordinator change.
&amp;gt;&amp;gt; And in the event of a failure, one of these
others--what's--how long does it take for
the system to get back to a stable state?
&amp;gt;&amp;gt; PEDONE: It depends on your time outs. Now,
this is--there's no--there's no mention [INDISTINCT]
if we have a tradeoff. You can be very aggressive
for--with your timeouts and try to react very
quickly with failures. But then you make mistakes
because maybe that is too slow and then you're
going to actually redo the ring and resend
the message. Actually, what we are really
losing here because redoing the ring costs
us nothing because it's a local commit. The
real cost to here is resending this 2A message
because it thinks that one of the acceptors
has crashed. And it can be made arbitrarily
short and then risking more retransmissions.
So in reality, this messages here, they are
rarely lost. And that's the reason why you
can get to throughputs which are close to
one gigabit because most of the time using
it, to which they don't typically lose the
message. The lost messages are basically due
to buffer overflows because they are second
to emerge and the process didn't have time
actually move the info--the buffer. So in
reality, it happens very rarely. So we accept
here we experience a time of kind of large.
Now, I didn't--I didn't say--nobody asked
what happens if this message here gets lost.
So nobody detects it right? So the way to
handle this typically proposes the [INDISTINCT]
some learners. So they propose a message,
they also set the time out and the waits--a
timer and they wait to eventually [INDISTINCT]
this message. But if they don't do it, they
deduce that, well, the message was lost at
this spot here and then they retransmit.
&amp;gt;&amp;gt; So you said that the learners can identify
a small [INDISTINCT] in sequence numbers?
&amp;gt;&amp;gt; PEDONE: Right.
&amp;gt;&amp;gt; Which, in theory, would apply that the
acceptors keep an infinite number of factors
because...
&amp;gt;&amp;gt; PEDONE: Yes.
&amp;gt;&amp;gt; Do you have any idea or numbers--I guess
you have. It matters there as well, so if
you don't...
&amp;gt;&amp;gt; PEDONE: Yes. That's sure. We have--we assume
a monotonic increase in sequence number. But
with--at 32 bits or 64 bits for the sequence
numbers which you can send quite a lot.
&amp;gt;&amp;gt; I'm not sure but I have any idea of how
frequent these losses are [INDISTINCT].
&amp;gt;&amp;gt; PEDONE: Yes. That's what I said before.
So the losses--the message losses, they--in
environments like this, like, this is--it
wasn't for a certain local area network and
you can step the switches if you want and
if your system administration allows to do
IP multicast but it's a confined environment,
it's not the Internet. So messages, they get
lost because the proposers, they're not fast
enough to remove the message from the buffers,
right. They get overwritten otherwise, they
don't really get lost because [INDISTINCT]
switches...
&amp;gt;&amp;gt; Right. Right. But for the learners I could
expect that these are in practice, these are
machines that [INDISTINCT] you build for maintenance
and stuff like that.
&amp;gt;&amp;gt; PEDONE: Right.
&amp;gt;&amp;gt; So there I could imagine that actually
the downtime is much bigger than the acceptors.
&amp;gt;&amp;gt; PEDONE: Yes. So that's a different kind
of problem. So, if the machine goes down for
maintenance and then takes some time, then
we're talking about a different kind of recovery,
right, which this mechanism here, this is
not really what you want to do.
&amp;gt;&amp;gt; So, you'll need to do kind of full state
recovery.
&amp;gt;&amp;gt; PEDONE: Right. Exactly. You also have to
do--we have to take check points from time
to time.
&amp;gt;&amp;gt; Sure.
&amp;gt;&amp;gt; PEDONE: But otherwise, these buffers for
these [INDISTINCT] whatever.
&amp;gt;&amp;gt; [INDISTINCT].
&amp;gt;&amp;gt; PEDONE: But you're right, at some point--so
yeah. So if it's a lot less than you actually
take up an image and then replay it for image
[INDISTINCT]. But you have a point, so yeah.
Any more questions?
&amp;gt;&amp;gt; So thanks again.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>