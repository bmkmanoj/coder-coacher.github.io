<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Fast &amp; Memory... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Fast &amp; Memory... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Fast &amp; Memory...</b></h2><h5 class="post__date">2012-02-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/u3hqUfIwQnE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and thanks for the organizers for
inviting me and also being flexible with
your schedule I have a leave a little
early so I thought about fast and memory
efficient and memory efficient is
actually we get memory efficiency by
being spots so it's some sense it's the
right title for this workshop but i will
not induce varsity in a way you might
think by using a regular so we will see
how we actually look in force varsity
this is joint work with my former
postdoc bunch of students and some other
faculty members who work in computer
networking and Traveller convolutional
okay so this work was originally let me
motivated by analyzing the goal was to
analyze very large social networks if
there are other networks that I need to
smoke for example we have you're working
on a problem where we will get
biological networks where we have needs
between genes diseases phenotypes from
different species and what we want to do
is link five links between genes energy
today I will focus you can think of the
sample of the prototypical application
as social network analysis so they're
huge scalability is a big issue and one
needs to solve various predict a problem
so you need scalable algorithms so you
know I probably will not present
yourselves today on networks of size 715
million notices what we will do is for
what for today we will consider networks
of about 34 million vertices 100 million
that's the largest weekly too but we can
run on algorithmic cargo and we can
hopefully scale up okay so when I say a
large it's about that August management
34 million vertices 100 million
anticipating books period so around
social networks nodes represent people
typically edges represent relationships
and this is a small network pepper study
by sociologists in the 60s or sixties or
seventies I forget is the Karate Club
network the nodes represent each of the
34 people if the karate club and don't
one I believe was the lead instructor
and no 34 was the student father of the
character and these are communications
between the people outside of the
correctly so these were recorded and it
turned out that they were reflective of
conflict in the correct and as and
turned out that our wire the club
actually split up into two and if you do
a structural analysis of this network
you would actually 5.com so social
network analysis today I am just
concentrating on structural properties
so what one might want to do in social
networks well one way one try and
estimate proximity measures between
people or between the nodes if
they may not be directly connected they
may be at varying distances you may want
to detect committed communities you may
want to try and distinguish between
relationships that are strong
relationships that a week the people in
sociology who'd studied this these
things and then the predictive task says
by travel you know many of your property
on facebook and you have friend
recommendations on face so predicting
who will become whose friend in the
future if you're in the biological
application you may want to predict
which gene is likely to be linked to
which human musics and so there was so
this is are just a small cartoon of a
link prediction problem i'm not spending
too much time here because I want to get
to the technical part soon so here you
know at time instant one only Alice and
evil friends then David well become
friends and Alice becomes a friend with
Bob the network evolves and the question
is at the next time step you know you
want to what recommendations do you give
to Carol as to who or like your friends
might be at the next time okay so there
are many models that exists companies
like Facebook normally look at common
neighbors which can actually be computed
using the square of their Jason tickets
we of course don't just you look at the
static graph they have many other
sources of information like profile
information so they can augment a square
with that information and then there are
some more involved evolved involved
measures like the Cassidy's so there was
actually a nice paper exactly about a
year from now that was published in Siam
with you which said that you can look at
Jason see matrices and then you can look
at functions on these matrices and that
can give you information about the
network problems so our adjacency matrix
is w IJ if there is a
between North inj so this could be
overrated adjacency matrix for now we
will assume that w IJ is positive but
there are cases where w IJ may be
negative when you actually have normally
friendship relationships but Emma me on
dis + relationships that does occur in
fur jacket or so like I said a square if
you look at a square you look at the IJ
element then it counts the number of
blocks of length to which are between my
engine so that's the number of common
neighbors that idea and you can extend
that to look at a higher power okay that
gives you a length or longer a walk of
longer so then you can do things like
you can look at the matrix exponential
which is our series and you can combine
a paths of different lengths and then
you typically down weight them
exponentially so the most famous one in
sociology is this measure which is
called the caps measure the cakes paths
of it one two three four wha'd amps them
expansion and so this is what is called
a caricature and just start up with one
of the authors of the first author of
this paper is particularly high on the
matrix exponential I don't think well
and then what the paper says is that you
can look at things like centrality and
you can try and approximate it by
looking at the matrix function and
looking at the diagonals that tells you
in terms of how important the critical
notice and then you can look at
communicability which is how much
information flows between I&amp;amp;J as look at
the matrix function so remember this is
a matrix function so the input is a
matrix the domain is a matrix and the
output is also a matrix F of a is itself
a matrix for example 9 minus beta
inverse the resolving or the way in
technology so you can try and compute
this matrix and then look at the idea
that element and then you can try and
compute a quantity of so sharp but that
a lot of people who do social network
analysis are interested in something
called betweenness and that you just
perturb the matrix a little bit by
removing a node or an edge and you see
how the matrix function the changes so
that's very nice but of course if I have
a network of size 3 going then it would
be very hard to try and compute F of a
full okay because even though a is a
sparse matrix FFA is going to be a
definite there is this anecdotal thing
where you know we are all 66 degrees of
separation between oh so if you think
about it e to the power 6 is going to be
nearly 10 points they are very no hope
of trying to compute it exactly so what
should one do well not simple thing to
try a new one first thing one might
think of doing is think of a spectral
process so here assume that he is
symmetric we look at the largest k IM
values and eigenvectors largest
magnitude like in balance and then the
nice thing is that these matrices are
orthonormal we think is a tall thin
matrix it just has the top k i cant
vectors and i can approximate eat to the
power a by v e to the power lambda agent
so the function goes inside because
he is an orthonormal matrix and so this
one can do in one we uses an algorithm
that's it like the ranchos algorithm
okay then one can confused 43 million
vertices you can let say compute k is
200 or 300 you can do that it might take
a fair amount of time but after you've
done that pre-processing you can
complete these matrix functions or
proximate and without that much
difficult but disaster right into this
we know that the SVD is optimal in some
sense right there is the so-called young
a cocktail that says that if you look at
all right came in DC's the K truncated
SVD gives you the best nike
approximation you need a respective
normal cookies but let me argue that in
this case it's not the best thing to do
so you know my eye since I started doing
research on this I said you know I
should probably try and get on facebook
in this me go so I go a little bit of
what's my god so I went on facebook and
but then I never wanna spend too much
time on and so I have two friends of my
wife and then her mother sent me a
friend request so i can say so i have to
but they were saying hundred and fifty
with a people of this my wife over the
other hand was I think a year or two ago
was annotated to have the most number of
friends that she could get so she
contacted a friend fair when she was
friends with him in Thailand and she was
able to connect but she has 400 friends
after that 400 or 500 you think about
500 friends versus 750 million people
this matrix is extremely stops okay
there are many many many zeros to just
be so but so the SVD you can think of as
an alternate representation of this
sparsely but the SPD the
directors are going to be tense if there
is not going to be that many zeros in
the signal vector and that in some sense
comes about from the very nature of
singular I get back to the secure but if
there is this nested or talk analogy
property and then orthogonal property
many time in most cases comes up because
of cancellation not because of structure
so here is an example graph that we took
from livejournal 3.8 million vertices
the average peevly 17 so very small it
requires about 1 gigabyte to store their
jason c matrix if you for example loaded
us in matlab and the sparse matrix but
if you do a rank hundred approximation
using their speedy it takes hundreds
even of 60 degrees so it's clearly not
not a compression okay but I don't
necessarily just want the best
impression I want to be able to
approximate matrix functions so the
problem is that the SVD allows you to
compute matrix functions approximations
for them but does not reverse force it
so what should we do so here is in some
sense you know one of the key slaw
slides of the storm I think you know
that destiny is the optimal England okay
so if I fix all right let's say 500 and
I just get the reconstruction error I
think this is in forgiveness norm ok I
have the SVD if I have any other method
no matter how clever I am that has 9 500
my the method will leave an
approximation error that is worthless so
the SVD necessarily in some sense has to
be in the works there's a theorem and as
a proof we start but if i change the
game if i say i'm not going to measure
the contactless of my representation
by the rank but I'm going to measure it
by the amount of memory that I used to
store the representation then we have a
method which I will talk about that can
substantially increase or make better
the approximation okay and so this say
that the SP it shows that in that metric
where you're trying to measure you know
someone said well someone else me is
this optimum and I don't know and the
question is whether this can actually be
somehow characterized information to
Erica we're not only do you want good
coding but you also want to be close to
the set of Dolan's so the SVD has a
particular rank we are also the new
method also gives Lauren but our basis
vectors are going to be spots and hence
we gain we are better game I'm serious
file which is person it is higher but is
much do you know where is the reason not
even here does it compare well you are
so right yes yeah we know that yeah
we've done some of those experiments and
we are better in this menu but but we
need to do some whatever we need to do
my friends do method was that for that
matter yeah the petitioner is
yeah but you know so our method for a
certain rank why don't I go to monitor
and then it takes yoga so this is sort
of just giving you an F so there are one
of the core components key components of
the method is we are dealing with a
large social networks there is a
community structure in these networks so
the first thing that we will do is do a
very very fast clustering of this neck
and when I say closely guarded a few
minutes and I did worry about five years
ago with some students of mine and we
developed some software that can handle
very large efforts very quickly so that
is going to be the key ingredient oh so
here is for example this is actually the
actual split in the car development if
you use most reasonable clustering
algorithms they'll recover this except i
believe for no.9 no.9 ended up being
male then the whole story behind it he
was he or she was three weeks away from
killing a black belt so he or she stuck
with the construct so how do we do graph
restricted well there are various ways
to cluster the graph there's something
called ratio cut which came up in the
circuit community more recently
normalized cut was used for image
segmentation close but she and Moloch
and these are in some sense weighted
graph terms so they are trying to
minimize the cut between partitions but
at the same time they don't have an
explicit restriction on the note sizes
but that is brought about implicitly by
having some knowledge so the
normalization over here is by the sum of
all the earthquakes that are hence been
approached so traditionally what people
would do for example she and Malik the
method they suggested was to you
eigen vectors of the normal isolation
depression and use that to try and try
and optimize these are all in regard to
optimize what we did a few years but
that can be a little bit expensive
spectral max okay we know that also
requires computing a capacitance what
we've done about well five years ago
four or five years ago was to develop a
software which was in the same spirit as
a package called wheels so leaders in
the package that you slept very heavily
in circuit applications and other
applications and what it does is it
takes a foot draft very quickly Corson's
it by merging thickness so the book is
then we have n vertices at one level
course event we get n divided by two
that's the book anywhere we're cool and
keep on doing it until you get to a
small graph then you do an initial
thrust frame or partitioning over here
and then you somehow Liz this
partitioning keep on lifting it up and
up we just used Canadian Berlin
algorithm with lot of twigs and then you
get a final system what meanest was
trying to do was to minimize the cut
such that the number of so that the each
partition size is of the same same size
in terms of number of words but that is
not natural in social networks so what
our method does this method that I refer
to before it actually shows that for
example the normalized cut objective is
equivalent to something called weighted
colonel Chinese n minus a equivalent I
mean exactly mathematically prove it the
objectives are attended with a circle
cover and then what we can do is we can
use the same strategy we can force on
the graph and then when we lift the
partitioning so we
initial rustic when we left the
partitioning we run waited crunk innings
and what that does and it takes an input
partition and improves it's gone so and
the nice thing most veterinarians
extremely fast and that was in some
sense a requirement that I wanted to
have before we started this work because
we were not handing graphs of a lot size
so here is an example I forget how big
this is this will maybe over 400,000
vertices so if you use our software the
spy plan originally with no ordering
spike large if you order using our
software you can see that the cluster
there are different sizes and here is
meters you get just reserved all the
seams and so you could reorder your
jason c matrix according to the
partitions in which day and what will
happen is that the diagonal box a 1-1 to
acc if you've just ring to see clusters
will be heavy in the sense that will be
more edges within these motions but not
that they are being in significant
amounts are over here let's see eighty
percent are within the partition but
then twenty percent are obsessive so
here is what we do so we will then
reserved to the rocks and whatever
condition system the edge is eighty
percent of a science yeah this is one
overall budget buster
so what we do the method is radiation to
stop okay so the method is the following
so we did cluster the graph very quickly
in to see clusters will compute a
low-rank approximation of the things of
the sub graph okay and we can either use
the SVD or we can use stochastic do I
approximations okay and once we have
that we will then try and compute the
relationships between the different
clusters okay and so one way of looking
at this is that and so this explains why
we're all memory of so suppose I have
clustered into three clusters and this
is let's say the SPD so let's suppose
this is one way in and this is 10 ok and
this is also 10 but this is not going to
be 3 times 10 so we have a rank 30
approximation it's really gets a rank 10
approximation but we have zeros over
here so it is a basis which is far like
instruction and as a result the memory
that is needed to store this is exactly
the same as for asleep the only extra me
pay is for this diagonal matrix because
sorry this inner matrix which is no
longer diagonal there is going to be
guests and we form it by will become the
best least square approximation one so
that is the algorithm so this SI j is
compute the relationship between the
customs so i don't think i have time but
what one can show is what what do you do
within each cluster you can either use
SVD or you can use Tok astok low-rank
approximation you can actually do
anything within each partition and if
you use stochastic low-rank
approximation and you start with
and also matrix you can extend the bombs
that you get in that case too when you
just so there's a little bit of analysis
I'm not going to go into the detail then
you can bound the error in expectation
so here are some results of 3.8 million
vertices 65 million edges if you use the
SVD the relative error is given over
here and for you sir and gently customs
that the reason why these numbers are
slightly always because we keep on doing
the clustering until each partition
becomes below a certain sites each sub
graph is below 60 k so he can see that
we are actually on the SVD we have not
yet and that's weak in terms of it is
now you may have asked you know what is
reconstruction error and how to do with
the application so let's go to that but
before that networks you can say why
stop at one level because network
service hierarchical structure I can
take each cluster each of graph and
repeatedly trusted and you can see that
there is a nested structure within these
networks so one can actually do this
trick in a hierarchy where I can think
of these as the to diagnose box of in my
matrix a and these are the two diagonal
blocks of my matrix the top left corner
and then I need to get an approximation
at each level so suppose I get this
effect is over here then if you know I
will actually have to trust you Lauren
two approximations one which is obtained
from this level and one which is
obtained from this one so this would be
level one is what we call a soup 1 and
then level 2 is going to
of this as having fortresses and then I
can run my approximation algorithm for
each of these feelings the issue is you
know how do I actually compute these
quantities so if I have a parent and I
have two children of the parents and if
I am giving a low-rank approximation
over here and over here how do i compute
this so what is you can say i'm not
going to test the basis i'm just going
to take a direct some of the pixels but
then the rack will increase the rank
will become to get all the other extreme
is that i can just do a rank aspd right
starting from them but it turns out that
neither is a good idea because the if
you look actually at the subspace which
is obtained from the sub graphs then it
actually is not too far away from the
subspace of the fulcrum the when I talk
about the subspace I mean the rank a
truncated SPD of the full space so I'm
not going to be quickly so I you just
use that word spaces of us but we don't
use the dresser so this is me out with
some good videos so given the subspace
an approximation for the children you
basically just run you know Q steps we
really need q equal to one of subspace
iteration followed by orthogonal
projection onto the resulting it's
actually the exact same procedure that
trough martinson and how to use and
their papers except they start with the
Gaussian random matrix but really have a
good approximation of the subspace and
so we start with this and it turns out
we have results that show that if you
start from random and you in five across
five powers we are still able to beat
that by using the approximation that the
children but much more interesting is
that instead of
just using so so I could think of that
as just computing an approximation for
the entire be rough but you can actually
get multiple approximations off the
graph which are at different scales okay
so a knot is the whole matrix our craft
a one is the approximation obtained from
the first level a2 is approximation of
tape from a deeper vibrantly 80 years
approximation obtained from one still be
provided so what can one what one can do
is actually combine this to do a
multiscale relations so you can combine
all these different multi-skilled
representations that you have off the
graph and then combine it for example we
give you the catskill so let me just
give you some one example so I showed
you the karate club network it has 78
edges so suppose I do to a leave-one-out
expect so leavin out means that i will
remove an edge and then i will do some
predictive method and then see where the
edge is wrapped in my coach so
traditional method like what facebook
users if it just have the graph would be
in common neighbors okay so let me just
explain this the y-axis is number of
hits so the x-axis is top cake so this
says I have 78 experiments because i am
doing 78 leave Iran experiments and this
say is that this is 9 so this say that
in nine of those 78 leavin arms the edge
that I deleted was night first and this
is top k and so this is the number of
fruits so obviously higher the curve the
better so if i use a comma neighbors now
get this graph if i use if i did not do
any
instigated lusting and just divided
things up in the pirate a random ok then
I will get actually something works if i
use cats which looks at walks of
different lengths and combines them by
forming i minus beta a inverse then i
get the grade group if I just use our
method but just with one level of
partitioning or plus claim then I get
this little this bar this plus but if i
use this multiscale prediction so what's
think about it think what it does if
partition that we use three level a
partition the graph the karate club into
two four eight and then it formed an
approximation from each of these
partitions so I have representation of
the approximation of the graph it at
three scales and then I use that and
combine it like I could set in the last
one and as a result we get much better
12 better and robust more robust
solutions ok so that was of course of
twine network this is 30,000 nodes 684
thousand edges and this is the false
negative rate which false positive rate
and you can see that our curve the new
method is four so it's better larger 1.7
million norms 84 million it is and we
have different time steps so we are
trying to predict in the future oh the
friendships would be again the monkeys k
relation I don't have time to talk about
it but I am just going to say me this
method is in some sense very easily well
not very easily but it's made for
parallel computation but I take my big
graph I divided it up into different
parts I do something local
in each car then I can find me so once
I've divided the graph up everything is
easily paralyzed the question is whether
you can paralyze a division and get it
okay and turns out we can paralyze well
they're getting together part very
easily the partitioning actually does
not take that much time so just to give
you a sense of the times involved let's
see 3.8 million vertices 128 processors
takes about a minute to do it and we get
speed ups off about you go to a larger
graph about 10 million vertices it's
done under eight minutes but the
speed-up was not so it turns out that
the in the partitioning software is very
fast but it doesn't necessarily scale so
as we get bigger and bigger the scaling
of that part needs to be is that the
bomber the partitioning
no because those peanuts so here is a
solemn sort of plot of the times right
so for example when you do let's see so
on on 128 processors the partitioning
time is the most in the combination time
we like to know them so that doesn't
start but we are still able to handle a
hundred million government what did I
say ten delinquencies so okay so that's
a plan I'm sorry I had to rush through a
little bit given this talk in an hour
but so what I've done is tried to the
goal was to try and compute but
approximate matrix functions so how can
you get a dimensionality reduction
through that allows you to do matrix
functions and at the same time the basis
is somewhat spots because that helps in
memory efficient spoke so the approach I
took the first veto was instead of
trying to use an optimization based
method where we try and have a
regularizer and then run an optimization
out we explicitly enforce the sparsity
by doing Fritz that's so it's no by no
means optimum ok but it does care to the
size of problems that we want so it is
maybe efficient it is paralyzed well
I've said something in the future work i
presented use to some preliminary future
work this notion of doing a multiscale
approximation of the graph and then
using multi scale prediction will get
better results that seems to be
we quite powerful light and so we pursue
that in the future and if you really
want to scale up to 750 million mercies
we have to develop some veteran
partitioning out okay that's the end of
my talk and there are some references
out here not all the mine but if you're
interested in network analysis you know
I read this book by easily a climber
it's a very well written or not well
kept a-rollin Booker so if you're
interested in that area I would say you
should be okay talk faster yeah at some
point I believe the size of the matrices
that we've done so far I mean I feel
right that we should do some experiments
where we start getting worse results but
right now we don't have any model
selection right so we should question
yeah so the indecision
the later service nutrition non other
metrics are construction er yes you try
that easy just SPS Bob yeah because I
think so this is a great question so if
you look at my thoughts the amount of
there is no natural place where i can
actually make it lower so there is no
sharp drop-off in the cigarettes so some
might even question why are you using
the language but it turns out that maybe
two experiments like prediction accuracy
then we actually perform better than
losing the traditional means but then
what can say is that the initial matrix
itself is not like why did I you know
not accept your friend request and
accepted someone else in that case you
could say that the initial process
itself what someone might so you would
think that this multi scale
more</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>