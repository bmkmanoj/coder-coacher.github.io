<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Multiview Geometry for Texture Mapping 2D Images onto 3D... | Coder Coacher - Coaching Coders</title><meta content="Multiview Geometry for Texture Mapping 2D Images onto 3D... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Multiview Geometry for Texture Mapping 2D Images onto 3D...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZjY2YR0MBjs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today we were fortunate to have
privilege or Warburg from cuny in New
York to talk to us you may know him for
his work on the warping he has a book
that is very famous about this core
digital image warping that led to he had
a presidential young investigator award
in 91 and in two thousand even got the
mayor's reward excellence in science and
technology from region region and today
is going to talk to us about new
algorithms for mapping 2d imagery and
two one two three models and range data
thank you hmm I'd like to thank Luke for
inviting me today to talk about
multiview geometry for texture mapping
2d images onto 3d range data the
motivation of this work is to develop 3d
photography software and by that i mean
the ability to import the geometry of
large-scale urban environments so it's
very easy of course to bring in images
into the computer the digital camera but
how easy is it to bring in geometry and
so that's the problem that we're facing
and we're trying to distinguish our work
by having models of high geometric and
photometric accuracy and so the thrust
of this talk will be about how to do
automated texture mapping for photo
realism so we'd like to position our
software's as basically an Adobe
Photoshop of 3d photography if you will
and by that I mean a comprehensive
software toolkit to be able to do the
modeling and do the texture mapping as
automated as possible okay so we have
two sources of inputs for our work range
scans and ordinary photographs now as
far as the range scanning goes we are
currently using a leica 2500 spot laser
scanner it's of the time-of-flight
flavor ok there you know the laser
scanners can come of time a flight or
phase based and we use time of flight
because that allows us to hit a maximum
range
of 100 meters so for large-scale
structures like buildings we need to be
able to image it from further away this
particular one happens to be a little
slow it's about 16 minutes 41 million
points phase based scanner is always
much faster but they are less accurate
and they can only reach a distance of
about 25 meters of 25 to 50 meters so
you know where as time if light can go
100 meters even a kilometer away if you
want if you're willing to tolerate a bit
more error the error we get here is
about six millimeters per range point so
as an example of 3d photography let's
take a look at the image on the upper
left that's a photograph of the Thomas
Hunter building in New York City and on
the lower left we see an example of
several range scans that were seamlessly
tied together okay so this is done
without human interaction every range
scan is shown with a different color so
we have we have to take multiple range
scans because of course we have to get
at all sides of the building and the key
here is that the registration is done
automatically here's another example
this is the view outside my office this
is Upper West Side Manhattan this is the
shepard hall building on the campus of
city college in fact if you look close
he could see Yankee Stadium in the
distance right there here we took 24
range scans around the building and what
you're seeing here is just a point cloud
of the 24 range scans all stitched
seamlessly together so we didn't do any
meshing work we just collected the
points and display them as a point cloud
of course if you go very close to the
point cloud you will see gaps and their
different rendering ways like Q splat if
you want to render point clouds or you
can just directly mesh this ok so here's
an example of where we're trying to go
with this you see that building that's
modeled as a collection of point clouds
and you'll notice these white spheres
they represent the automatically
recovered positions of the cameras that
we took to take photographs of this so
remember I said that there were two
sources of input range scans and
photographs so in order to get the
or color and texture for this we have to
walk around the building collecting
photographs and our method automatically
can recover the positions of the cameras
and each one of these white spheres is
connected to a green sphere which shows
you the optical axis of the camera so it
shows you not only where we were
standing when we took the pictures but
also how we were holding the camera
which orientation so that's all part of
the software toolkit that I'll describe
today ok so the applications of this
work is to come up with photo textured
models for google maps and other similar
products so for instance you know right
now you have Sketchup you know as as a
method for creating lightweight to low
resolution models that you can put on
satellite maps question is how would you
get higher detailed models it's not
going to be done through Sketchup it's
it will be done through the use of laser
range scanning and on and also
photographs that can apply photo
textures to it easily and the key point
here is not can it be done but can it be
done automatically ok because obviously
if you put enough you know man power
behind any problem you will solve it but
the question is will it be solved
efficiently and and will it scale up
well and this is the work we're trying
to do to make it scale up so another
application of this work is 3d auto
navigation systems the next generation
of navigation systems for cars and that
ties in with video games because
sometimes that looks like a video game
when you drive through a city and you're
trying to make a digital city out of
this so realistic sets for movies and
video games urban or military planning
virtual tourism modeling theatres and
stadiums so that you can get virtually a
assisted seating selection you could see
what your seat would look like if you
were to buy this ticket at this location
ok so we will address several problems
here first problem is feature-based
texture mapping now currently that
requires extensive human interaction for
feature selection among to the images
and 3d range scans another problem we
have to address is what happens if
there's a rigid attachment of camera
onto the scanner there are many laser
range scanners that are sold that have
cameras built-in but unfortunately that
constrains the time and the place of
image acquisition in the example of
scanning buildings it may take several
hours maybe the large part of a day to
scan a very large building and we don't
want to use the photographs that are
acquired from that laser range scanner
because they will vary in shading from
sunrise to sunset so we would like to be
able to have a model on the one hand and
and photography that could be mapped to
it taken at different times of the day
so you could show what that building
would look like at sunrise or what it
would look like a noon or near sunset ok
so that's another problem we have to
address and also the seamless fusion of
geometry and photography which currently
requires manual interaction so the
solution that we're proposing is to
integrate multi-view geometry and 2d to
3d registration in order to text your
map on to 3d range data so as I said
before when we'll have two sources of
input for this problem the first one is
dense 3d point clouds that are derived
from laser range scanners and the second
source of input are just ordinary 2d
photographs taken from arbitrary views
by off-the-shelf cameras so the outline
of this work is to produce sparse point
clouds from photos by multi-view
geometry ok so the one hand will get a
sparse model a low res model then we
will project the 3d points from the
dense point cloud that we obtained from
the laser range scanner and from the
sparse point cloud that we obtain from
the camera we will project those 3d
points together into a common image
which image the images that we took as
we walked around the building ok and
then comment to the points between them
will establish correspondence among the
point clouds and we will then therefore
recover the rotation scale and
translation to best align the dense and
the sparse point clouds together and the
purpose of doing that will be to bring
all the photos that we took around the
building into a reference frame of the
dense model so that we can do texture
mapping now that's that's a lot of
you know high-level outline it's
probably best described with this slide
so if you didn't understand what I just
said hopefully this slide will clarify
it on the on the top part of the slide
you see a collection of 22 images that
we took around the building now we took
those images and we go down the right
branch that you see here where it's a
structure for motion we use those images
to create a sparse point cloud just from
those images alone and alongside with
that sparse point cloud we can also
recover the positions of the cameras
that took those pictures okay so that is
one branch of this work the second
branch is using laser range scanners on
the left side and then using these
images to figure out where do they live
with respect to those dense point clouds
derived from the lasers and as you could
see here you see five white dots and
they represent only five images namely
the ones that are outlined in blue and
only those five images out of the 22
were able to successfully be mapped to
that dense point cloud in other words
only those five images we were able to
figure out where we were standing when
we took them but that leaves the other
17 images to basically provide us with
no useful information there which is a
waste so what we would like to do is
merge the benefits of this of this
branch which has the dense point cloud
and only five photographs that can be
projected to it and this branch which
produce the sparse model but has all 22
positions that are identified with it
bring them together and if you bring
them together that means we will have
the benefit of projecting the the color
information through 22 photographs onto
the model rather than just from five
photographs now we're trying to do this
automatically okay there's no manual
intervention here so this line of work
the structure for motion is pretty well
known in the computer vision field okay
stems from original work dating over 10
years ago by fosho raw and his
colleagues at inria and the work on the
left branch was presented last year by
yiannis thomas and his student ling
young lu who was actually a summer
intern here this this year and that was
presented last summer at a CVP our
conference 2005 so what I'll talk about
today is the contribution of this work
of bringing the two together merging the
benefits of structure for motion and 2d
image 23 derange registration so that
you can come up with the results shown
in the lower right which is a nice photo
textured accurate model of the building
okay so if you look at the left branch
into the image of 3d range registration
we can describe that using this slide
there are again two sources of input
range images and to the images in the
case of range images what we will do is
undergo a process of 3d feature
extraction which is shown in the in this
box on the left the 3d feature
extraction consists of four steps 3d
lines extraction followed by range
registration followed by 3d line
clustering and 3d cube extraction the
purpose here is to be able to match up
the or to be able to extract the 3d
lines that are present in the range
scans now we're fortunate when we look
at buildings that there are a lot of
linear features and buildings and all
these man-made structures they are
linear features so we will exploit that
information we will extract it from the
range scans and we will figure out what
are the rectangular parallelepiped or
what are the cubes that we could extract
from the point cloud now alongside that
we will look at the 2d images and see
what are the rectangles we can recover
from there so we obviously can recover
to the lines linear features off of the
photographs we can use that to recover
vanishing points and we can use that to
help calibrate the camera and rectify
the lines that we just extracted so that
we can come up with 2d rectangles now
these 2d rectangles might consist of the
windows in the building or other you
know features that are prevalent in the
building so we are going to do the
matching between the 2d features in the
photographs with the 3d features in the
range scans so that we can figure out
how to bed
match the photographs to the range scans
in essence how to do this texture
mapping automatically without having to
go and by hand and say that this point
in this image corresponds to that point
in the range scan which is the current
state of the art so just to look at 3d
feature extraction a little bit more
detail on the upper left you see an
example of range scans and in the second
row there you see 3d lines that are
extracted from those range scans and
then we use those lines to be able to
connect one ring scan to the next in
other words to align these multiple
range scans together and register them
which is shown on the bottom left then
we can cluster lines so that we can find
that these lines law among three major
directions you know the x y&amp;amp;z axes and
we can use that 3d face extraction and
then extract rectangular parallelepiped
from the building ok so we are using the
man-made features that are prevalent in
the buildings and using them to extract
rectangular parallelepiped or keep yours
you know small cubes if you will and
then alongside that we do the same thing
for the 2d images so we do to the
feature extraction and we take
photographs like you see in the upper
left we extract lines off of the
photographs and we use those lines to
figure out vanishing points and then
those vanishing points are useful for us
to calibrate the camera and to rectify
those edges so that we can have a
rectified edge map a rectified set of 2d
lines and then we can extract rectangles
off of that so then we will map the
rectangles off of the photographs from
the rectangular parallelepiped off of
the range scans and we will be able to
bring these two different domains in
sync so you know here we see that slide
again and we just you know briefly
overview the 2d image 23 derange
registration which is that left branch
over there what we need to do now is
then go over how do we get this right
branch how do we derive the structure
and the camera positions directly from
the photographs alone
order to do that we use what's called
multi-view geometry and it goes by
several other names like structure from
motion and slam which is simultaneous
localization and mapping so if you look
at some of the computer vision
literature you'll find these three
different names but they're all the same
thing the idea here is to recover the
camera positions from the images to
bring one camera into the range scan
coordinate system and then all the other
cameras will follow as a rigid
transformation but in case there are
some errors we use bundle adjustment to
best distribute the errors along with
across all of those camera positions and
what we get out of this is that it
facilitates automatic texture mapping
from the photos taken at arbitrary
positions and that's key we don't want
to limit anyone you know where they can
take pictures from and we can use just
regular off-the-shelf cameras to do this
so here's the kind of high-level
overview of how this is done okay we
start with frame 0 on the left and we
use sift which is a scale invariant
feature transform to derive features off
of that image okay these features are
such at the art scale invariant so we
can assume that there is a
transformation associate with that frame
namely k times I the identity matrix and
0 for translation so in the square
brackets basically it's a composite of
or partitioning of the rotation matrix
and the translation vector initially we
will assume no rotation no translation
so that's for frame 0 everything will be
anchored around them k r is a matrix
that stores the intrinsic parameters of
the camera so we will find these these
black dots here which represent the
shift features and then we have to do
that for all images and we can use
correlation to basically say see where
this feature lies in the next frame
where does this feature law in the next
frame we do that for all of these
features now because we know the
essential matrix we can recover the
show are one and two when the initial
rotation and translation that that best
takes the camera from frame 0 to frame 1
so we will use essential matrix to
determine R 1 and T 1 and that means
that when we know this point maps to
this point we can use that information
to triangulate and find out where does
it lie in 3d because we will have
recovered the initial rotation
translation using essential matrix so
the initial pair can be resolved that
way and we can basically get a very
small point cloud out of that so if we
have let's say 10 features that are
found in frame 0 and frame 1 that means
that they will correspond to 10 points
in my point cloud but then i have to
propagate that information to frame too
so I also do correlation for instance
this point that was found you know
connected to this point here in frame 0
and frame one that point can also be
connected to frame too so I look at this
pair and I look at the other pairs that
have connections and for all other
frames we have to compute the unknown
rotation and translation using a
six-point ransack algorithm that's
random sampling consensus and the
purpose of that is to help eliminate
outliers you know because you may have
some errors so you want to make sure you
don't consider outlying solutions so
that basically helps helps you because
that means that the 3d point that was
connected or that was due to this
connection here that 3d point is also
shared to that feature point here and so
what we are doing is progressively
adding more and more points to our point
cloud so as we consider you know from
two to three two to three hundred frames
we are building up the point cloud with
contributions throughout so this is how
structure for motion works which means
that if we are taking pictures of things
that don't have features like let's say
this wall right here which is a plane
wall there's no there's very little
features to track onto structure from
motion will not work it only works when
there are features to grab onto okay so
in the example that we had with the the
church like building the neo-gothic
building you know it's a stone building
there were many features to grab onto
because as many
interesting visual points on that stone
building so then we have to apply global
bundle adjustment to distribute the
error of the camera posing structure
because otherwise we accrue error as we
go along and we want to distribute the
error uniformly so what we do is we
minimize the summation which tries to
minimize the distance between little m
which is the an extracted 2d feature
point and capital P capital m where p is
a projection matrix and Capital m is a
3d point so in other words the
projection of a 3d point should land
very close to its corresponding position
in the image little m and we try to
minimize that distance by varying p very
by varying m capital m i should say and
by varying p so we will basically
minimize this rear projection error and
we can solve this efficiently using a
technique called sparse bundle
adjustment so as an example here is what
we get let's say we are walking you know
with the 22 photographs that I took
before remember we don't have GPS on
this we can't trust it you know in urban
canyons you're not going to get good
results with GPS anyway so what we do is
we walk around the building and as we
walk around the building we take these
pictures remember we don't know where we
took them from but that's the whole
point of this exercise structure for
motion is supposed to give you that
information and that information is
displayed here as those little white
dots okay so those little white dots is
some of the results we get from the
structure for motion where did we take
those pictures from and simultaneously
along with that we also get the sparse
points that you can almost make out that
building there right from the sparse
point cloud and those two come together
simultaneously because you need you know
you're basically balancing both the 3d
position and the position of the camera
so you aren't getting the sparse point
cloud you are getting the recovered
camera positions okay now if you were
only to project the photograph from
let's say that last photo which is shown
here on to this structure
then you might get something oh wait
hold on a second let me back up a second
okay let me just go here okay I'm not
going I'm not going to show you that
part here well if we were to project
this photograph on to this point cloud
you will only get a subset of the pixels
rendered and then as you add more and
more of the photographs onto the point
cloud you will get the more richly
painted structure but what I'm showing
you here or what we've just reviewed is
how to go down this branch how do we go
from these images to this sparse point
cloud for which we know all the camera
positions now the next step is to bring
the two together the next step is to say
I've got my sparse point cloud I've got
my dense point cloud from the lasers how
do you merge them okay so what we do is
we figure out how to merge it and notice
what's happening we take our little
sparse point cloud for which we didn't
have the right scale and we bring it
into the right scale of the dense
because remember that all of that work
with structure from motion with scale
independent you know no one said that
the size of this brick is you know
however many inches okay it's all it's
all modulo scale so we have to bring
this into the same scale and reference
frame as the dense and that's the next
part here which is to align the the
range scans and to align the the sparse
structure for motion point clouds
together so how do we do that okay what
we do is we take those images remember
we had five images for which we did find
correspondence between the image and the
dense point cloud we take one of those
five images let's say this one here and
we consider what happens if we project
the 3d point from the structure for
motion on to that image and also project
the 3d range of point on to that image
so we're looking a little subset of this
image here and in this image that you
see in a lower left you see what happens
when we project the point from the
sparse point cloud on to that image you
get a collection of those those colored
dots there now look what happens on the
lower right you also project the dense
cloud on to that very same image and you
get this collection of dots now for
every dot that you see here projected on
the on the lower left what we do is we
look for a closed position here on the
other image so what we do is we
basically consider a candidate that this
point here has a candidate match in this
vicinity here so two points our
candidate match if there are two deep
objections in this image are near each
other okay so what we're trying to do is
figure out who is a match to whom and we
can do so if we are if a match is
considered compatible if their scale
factors are approximately equal so going
back to this slide if this 3d point that
gave rise to this 2d projection is
closely associated with that one then
that means that the 3d points from which
they came should be the same they should
be corresponding but how do we know that
that's true so what we do is we go
through this set of procedures to find
that and we're exploiting the fact that
scale factors should be the same between
both of those point clouds remember that
when we recover the structure from
motion point cloud we didn't know its
proper scale but we can try to find out
what is the best scale factor to bring
it up to the other point cloud so we
consider a point from the structure from
motion point cloud minus the center of
projection which is the camera position
and divide that by the corresponding
point that we're considering from the
range image- its central projection that
will give us a value s 1 then we do the
same thing on another point on some
other point that landed in these two
images that will give us s 2 and then we
take the difference between X 2 and X 1
from the structure for motion / the
difference between some other candidate
y 2 and y 1 from the range and the idea
here is that the confidence of a match
is based on how many compatible matches
you
have in terms of a scale okay so it's
almost like a like a voting scheme where
a lot of the points will vote and you
will see that there's that they converge
on one optimal scale factor s sub opt
and if we assume that that is our
correct scale then we can come up with
the list c of robust matches such that
the match between the corresponding
point in the structure for motion and
the sparse model and the point in the
range model satisfy that scale
difference and they satisfy it for all
the points okay so once we recover we go
through all this effort just to recover
an optimal scale value s so once we
recover s then we can go ahead and
minimize this equation that says you
know what is my unknown rotation and
translation that best brings these two
point clouds together so we went through
all this effort just to figure out what
is the proper scale to take that sparse
model to the dense model and once we
figured out that scale then we solve for
the remaining two parameters which is
rotation and translation so if we go
back to that video that you saw here we
see if we could get that to work right
if we go back to that video this this
makes it almost look like it's
incremental but it's not the idea here
is that we found the right scale to go
from the initial configuration to the
final configuration and if we're in the
right scale then all these counter
positions were carried along with it
including the you know and they match up
with the five that we recovered using
the old method based only on the dense
laser skin point cloud so the ideas that
we carry over the known camera positions
that we knew from structure for motion
and we bring it into the same reference
frame as the dense point cloud data that
we got from laser range scans and this
is what enables us to do the texture
mapping automatically the fact that we
were able to know where the camera
positions were and bring those
photographs into the same reference
frame and it's not done manually you
don't have to say well this point
corresponds to that point you know the
good old-fashioned way but it's done
automatically through this this process
okay so compare what we had before with
the dense 3d point cloud and only five
camera positions and then compare that
with what we have now which is a whole
set of additional camera positions that
we were able to bring along and that
just serves to make this more richly
textured model you know it's better to
use 22 photographs rather than just five
so if we look at the result let's say
from owning that viewpoint imagine that
we have the camera here we took a
picture from this viewpoint and
projected onto the point cloud this is
what you would get you would only get
part of the building that is properly
textured the rest is unknown but as you
consider more and more cameras and you
project or splash those colors onto the
model then you get everything else okay
so that's how you begin to color
everything because you are using a
greater number of photographs in other
words you're using wall 22 that you had
available for you rather than just the
five that we were able to figure out
from the 2d to 3d registration solution
so this is a big step forward okay and
as you could see here these these
spheres represents the recovered camera
positions and the these lines represent
the angles in which the cameras were
oriented and you know this is what we
were able to get if we're missing some
other parts of the building it's because
we didn't have a view point from which
to take it from normally we would like
to be on a tall building across the
street from it so we could look down or
perhaps get it from aerial imagery which
we did not have available to us so
here's the animation again so this is
just a synthetic rendering of the
building it is animated here again you
can see the overlay camera positions
that we automatically recovered and the
orientations
and you could take photographs that many
different times of the day and then
create different animations with
lighting at different times of the day
so that's the benefit of decoupling the
camera part that the photography with
the model generation using laser range
data ok so this example of shepard hall
used about 12 million 3d points derived
from the lasers and in this example we
will look at this column here we use 22
images from which we generated 45,000
sparse points in the point cloud so when
I talked about the sparse structure from
motion 3d point cloud it was sparse
because it only had 45 thousand points
instead of 12 million so 12 million is
what we got from the laser 45 thousands
what we got from the photographs alone
and if we were to use the 2d to 3d
registration alone we would have only
known 5 camera positions instead we now
have the benefit of using all 22 when we
project those images onto the building
and this whole process took about 19
seconds ok and this is on some standard
dell computer bout three gigahertz
nothing fancy we also did this work for
National Geographic and this work aired
in December they did a documentary on
the history of Grand Central Terminal in
New York City which is a very large
structure and so we went in there on two
nights yeah you know the only time you
can go there when there are no people
between two and five a.m. and at least
we were able to get that time because if
we were at Penn Station it's always open
this message access ok so we were able
to collect about 44 million points
through laser range scans we use to
scanners we use the time-of-flight
scanner especially for the ceiling part
which is very far away and we used also
phase based scanner for some narrow
passageways that were closer to us ok so
here's an example where we took just a
few images here projected them onto the
3d laser data and you get something
that's photorealistic
and that was the purpose of this work so
that you could basically have something
photorealistic enough you could fly
through in physically unrealizable ways
with a camera in Grand Central Terminal
ok and this work is currently under
consideration for an Emmy Award I guess
we'll find out in three weeks if we got
here if we were nominated or not okay so
that's work for Grand Central so let me
wrap this up what I spoke about today
was the integration of multi-view
geometry with range registration we use
2d to 3d registration for a subset of
images that contain a sufficient number
of linear features and the purpose here
is to bring these images into alignment
with the dense 3d range model that we
acquire from laser scanners the concept
of bringing in multi-view geometry
exploits the 2d point correspondences
they allow us to bring all images into
alignment and produce the sparse SFM
model and then we go ahead and we align
all the images with the dense 3d model
so as a result of this work we can have
accurate texture mapping on too dense 3d
range data okay so I'd be happy to
entertain any questions you may have yes
when you scale the structure for motion
camera of 3d point data to
range data appear that you started
immediately with the point cloud and
when you brought those two images into
skin with each other obviously the
counter is lined up as well wouldn't it
be a good optimization to try to align
the cameras for at least two get really
radically close to the final solution
and then apply a Poincare patch okay let
me repeat the question on the in case
let me go to the slide that's relevant
hear from you see here okay let's say
let's say this live here right okay so
the question is you know the related to
this problem of scaling the sfm model
and aligning it best to the to the dense
model we use the points directly to
determine that you're suggesting that
perhaps we could have done this directly
from the camera positions here because
you have five right it was like it would
be easier to try to match that five in
your set of 19 so 52 52 22 yeah so we
couldn't match the five to the 22 what's
up this five all right already my right
so what you're saying is you could use
that to them give it a full scale right
so the point is why not just directly
map 25 feet to the 22 rather than
project these points into a limited to
project these points into an image and
then work off of that scale information
as I started all right as I started
playing I suppose that could be use as a
starting point the problem here is that
how do you vow to the accuracy of these
five and these these 22 by using the
abundance of points that we recovered
from the point clouds that would help us
minimize the error okay so it's it's
more for error minimization but I
suppose that would have been a good
addition guess yes you have a point
there's only one min max is going to be
in five somehow we'll go get up to
FEMA do register there's also if you're
lucky you can five you're not lucky you
could have more we can use this together
ok so another point that was raised is
that you know in this case we've got
five but that's a function of what
images you are collecting my cat 5e my
cat 19 22 so if you only have one
obviously couldn't do that so if you
need at least two but you know a put
serve as an initial estimate it could so
that that point is well-taken talk to
the dance day
would do that throw it away for example
taking a photo you can't very well
so you get data from just the 2d images
and
recover some sparse state and from that
stairs to being his life
ok so the question relates to welcome a
flying water plant for was a plane
overhead we're going to get the energy
will not get range data from that
because we're not going to get this type
of accurate laser range data you could
fly lidar but then it's very inaccurate
you know like there are some overhead
light eyes and could use but they're not
really worthwhile for this purpose for
that application we could get another
source of 3d points from the air by
using what's called push moon mosaics so
an interesting area future work would be
to just use imagery to get an ass to get
a reasonable estimate of the 3d data by
using what's called push moon mosaic so
that by that means that you have an
image and you consider one scanline
after you consider to scan line it's one
that's forward-looking on that's
backward looking and you think of them
as slits and as you move from frame to
frame you are creating the mosaic of
forge you do know the baseline distance
between those two slits so you can
derive what what in effect is stereo
stereo imagery which gives you 3d
information by just flying overhead and
and then you can use that as your
estimate for deaf instead of the laser
scanning results so do you use those at
all to add those into you guys ok so the
question is do we add into the 3d data
points that we got from the splice
pumped up in addition to the dense and
the answer is no we don't need to we
don't then give us additional points but
they're not as accurate as dense one as
laser ones something is better than
nothing but would be good enough let's
say to do this whole exercise without
any laser points in other words kind of
just go down this web branch and be
satisfied to get a result like that
enough in the low
right and you know often case it's not
enough it's in kitchen partly it seems
that your 3d that state is facing some
errands that you can after that's
something you say that the 3d density is
missing them to all those lower quality
sports but that's not really the case
dance is so much time 12u points
compared to 45,000 yes but if you apply
it flying over you get images I which
you can use distraction from
okay right okay so let me let me repeat
that point you're right that overhead
when you fly ahead you will get move
data you know the 45,000 whatever it is
that will be better than the no data
that it was required from the ground
using lasers and then we can pull those
two together and that's correct we
haven't had that access to that aerial
daily yet so if you have access to and
would love to share with us we glad to
do that yes basis for building
right okay the question is how do you
deal with the faces of villains for
which we do not have access to and the
problem is we can't right now we have to
go ahead and you know possibly edit it
by hand so that's where maybe Sketchup
will be helpful to augment while this
working on sketchup the rest you know so
that you can kind of fill in the gaps
there's nothing to do for parts that we
all know I mean unless we can do model
fitting unless you think you know that
now this this part of the building has
like a triangular roof top or some other
you know then you could try to fit
models and that's almost like a kendama
like application where you can kind of
fit primitives to existing structures
and you know that might be an
interesting application but right now we
can't do anything with these holes if we
can see if we can't gain access to it
yes the 32 3d range registration what is
it that limits the number of images that
you can actually register why do you get
them five instead of all set okay so the
question is well done with us let's say
to get five such points instead of more
what is the problem here did you have a
comment you want and say that there
okay 3d range registration you're only
getting five images presumable your
tribal what what's the problem with all
the problems that other image have done
support you now linear features for the
2d to 3d registration so we can register
the image of is a freedom bottle but in
the wrong way so we cannot use that for
affection I think so too something or
not on features place so the only
sometimes there aren't enough common
features necessary to match up the 2d
images with the 3d range scans on all
those pairs except for the five at her
shoulder and it's just a scene dependent
problem depends on where you were
standing when you took these images
where you were standing only took these
laser range scans do you have these
features in common and and hold the
purpose here is to decouple the active
photographing the building from the
active scanning it with a laser scanner
and in doing so you run the risk that
you may not have good enough vantage
points from both to match them up so it
will probably would have helped if if
you record the positions from work she
took those laser scans and then pretty
much come later with the camera and
standing on that same spot that would be
greatly simplifying the public we didn't
do that we allow you to freely
arbitrarily walk around a building
taking whatever pictures you want yeah
normally when you try to text your traps
a model or building
in fact that reason through the building
is kind of a deal
as honor again from here
and a ladder there's
okay so the question is how do we deal
with outliers of trees and things that
we that we're taking pictures of the
building but these trees and cars or
whatever get in the way the way we
resolve that is the fact that we're
taking so many images that from some
vantage point you know you will avoid
seeing the tree so it's just you know
the greater the abundance of imagery you
take the easier this out water removal
becomes can it be automatic well you
know you can automatically figure out
some of the Deaf positions of these
outliers and then you can if you want
just establish clipping planes you know
to just cut it out I'm not saying that
it would be automatic I mean somewhere
you have to establish what this is meant
to be unfolding or not I suppose it
could be if you just follow the contour
of the facade and then you cut out
everything outside of that you could try
to automate that
yes you get relaxing
go to catalyzing
okay so the question is do we get
different signals because the laser
range scanner the camera operate in
different parts of the spectrum infrared
or not if it and we haven't noticed any
problems with that generally the edges
line up you know when we have the corner
in the building or some other facets the
edge that the laser scanner finds is
NSYNC with the edge where the photograph
is so we haven't found and you haven't
hit any problems with that whether
you're right there could be some slight
displacement but besides at this level
of texture mapping you would never see
that you know texture mapping was
originally introduced to cover up
artifacts with low geometry with
something that's not virtually modelled
and so that type of effect would not be
even visual visually perceptible
any other questions
okay so I'll be around for a while
afterwards i'll be happy to entertain
any further questions you may have thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>