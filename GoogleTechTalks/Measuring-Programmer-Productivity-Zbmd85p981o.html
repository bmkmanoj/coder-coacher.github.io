<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Measuring Programmer Productivity | Coder Coacher - Coaching Coders</title><meta content="Measuring Programmer Productivity - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Measuring Programmer Productivity</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Zbmd85p981o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everyone thanks for coming to this
talk my name is Vikram I'm an intern
here for the next week or so which means
I'm really stocking up on the Naked
Juice this is my colleague veeral this
is a bunch of slides from the research
we do in our group at UCSB and that's
the campus the talk is about lessons
that we've learned when measuring
productivity of programmers and
specifically we are looking at the
programmers that write high productivity
high performance code so parallel code
in UPC or things like that
but as we go on till the end of the talk
you realize that a lot of the stuff you
can do even in serial code that you
write a small clarification about the
abstract we don't personally have 30
years of experience programming what we
were trying to say was that in computing
there is about 30 years of parallel
programming experience this reminds me
of a bunch of good lawyer jokes but I
think I'll keep them for the questions
so onward to the talk let me tell you
about the motivic the motor fish
motivation for this work what are we
trying to do and why is it important
machines are becoming cheap everyone
knows that machines are becoming fast
everyone knows active and you know that
because performance of hardware is
fairly easy to measure you've done a
bunch of programs on tomorrow's machines
hopefully they'll be faster in today's
and and that's how you get to know that
hardware performance is getting
increasingly better but also hardware
performance is getting sidelined by
program or performance this is certainly
true of serial programming and I don't
think anyone can dispute that now
machine code was replaced by assembly
and on to Fortran and C and now everyone
uses Python Ruby pearl what have you and
even if the code that you write in in
Perl or in Python is some X slower than
machine code or hand-tuned
C code we think that the gain in human
productivity is worth it we are willing
to make that change we're willing to
write it in a language which allows us
to get the program written quicker so a
10x difference doesn't really matter
that much but the thing with human
productivity is that it's really hard to
measure how do we compare the
productivity of a person using eclipse
to him using IntelliJ for instance
people live and die by camps and there's
not much you can do to convince them or
how do you judge a group of a fairly
qualified list hackers from with a group
of fairly qualified say Python hackers
right we don't really know moreover it's
not only the final outcome that's
interesting to us but also the process
by which you solve the problem if the
problem is to write a program to do
something you're not only interested in
how long it takes you to write that
program but what were you doing while
you were doing it what was your what was
your process of solution like for
instance if two people take the same
time but the first guy sent spends it
all in waiting for his machine to boot
up then we know how to speed it humm
very quickly we know how to boost his
productivity fairly quickly this is this
is not a new problem a lot of people
have looked at it before there are there
are a lot of interesting studies in
literature comparing serial languages
there is there is also very good
anecdotal evidence about productivity of
programmers varying a lot based on which
language you come from most famously
Paul Graham's articles about via web are
quite familiar and they've convinced
many of us to start learning Lisp and
scheme again yes I see Michael there
agrees
however what we really lack are metrics
we we say that well scheme is better
Lisp is good unless me write stuff on
the palm pilot because it's so small but
we don't really have a metric which
which lets us compare stuff we do have
some metrics source lines of code is a
metric a lot of people use but it
measures so for people who are not
familiar with source lines of code it
measures how many lines of code you've
written and it does tell you some things
in particular it's directly proportional
to the number of bugs that I in your
source code because the number of bugs
is proportional to how long your code is
it's also directly proportional to how
much effort you put in for instance you
can write your own matrix vector
multiplication routine and you'll have
great amount of productivity on the
other hand you could just know about a
couple of libraries which do it for you
so it's a good measure of effort it's a
good measure of bugs but really is it a
measure of productivity so you have
source lines of code which is a metric
and you could use it but the important
thing and and I want to emphasize this
again is you want to learn the process
by which the programmer came up with the
solution I mean it's just like computer
programming you want to find out what's
the loop that takes 90% of your time so
you can speed up that loop to get the
most gain in productivity
so if compilations only 2% of the entire
process you don't want to speed up
compilation you want you don't want to
say my language compiles to object code
faster because it doesn't really matter
it's only 2% so we're interested in the
process we're interested in the workflow
of the programmer as part of the HPC s
program we are involved in finding out
what effect the language and the
platform have in high-performance code
development and in order to gather this
data we perform classroom studies in a
class taught by John Gilbert at UCSB
both of us are TAS for that class and
and the goal of this is to figure out
what kind of machines high productivity
stuff should be written in what
languages should it be written in should
it be done on a linux cluster with MPI
maybe maybe
so let me explain a bit of the setting
that we're working with a bit of what
our hamster cage looks like it's a
graduate level class it's called
parallel programming something something
something it's taught over a quarter
which means it's 10 weeks of time but
you really get about 8 weeks give take
all the holidays it's required for a CSE
emphasis but it's not required for a
graduation so a lot of the people who
are taking this are taking it out of
interest not because they have to it
typically starts with about 20 people
and reaches stable equilibrium at about
10 there are three homeworks each
homework takes two weeks and there's a
project the project is open-ended so we
don't measure it because there's nothing
we know about it the project is a group
project the classroom homeworks are
individual work each homework is roughly
the same effort we try hard to make it
the same effort and if we find that
students are having a tough time we kind
of scale it down but at this point we've
got it down to two weeks of good effort
and there is a lot of variety and this
is what I want to stress there are two
programming environments see with MPI be
used to support Fortran with MPI but I
don't think we do anymore they're not
too many people writing for kind of MPI
anyway and we have UPC we also have a
bunch of different hardware so we have a
cluster which is at UCSB which is just
homebrew there's an IBM machine in San
Diego and there's a crane alright
students who take this class are fairly
good serial programmers they know how to
write serial code they're not they're
not newbies but they haven't done very
much parallel programming they might
have heard of MPI they might have seen a
bit of MPI code but they certainly
haven't done very much themselves they
come from a wide variety of backgrounds
they come from CS Mac chemical
engineering even mathematics
and and so we've claimed that this this
makes our average case fairly
representative of new parallel
programmers it's not just CS people so
why though the homeworks are quite small
and and well contain the process by
which you arrive at a solution to the
homework it's still representative of
what you would do if you're writing real
parallel code so the homeworks are just
two weeks you don't use them again ever
in your life most like all graduate
stuff you ever do and so the way you
solve it you know the way you solve
these problems is you start with a
framework and you say well I want to
write a conjugate gradient which
paralyzes well you write some sort of
serial implementation of it well or
something which works in parallel but
not too quickly and once it's working
you start figuring out okay I need to
speed it up and then you go speed up a
program this is fairly representative of
how you would write real parallel code
the program's themselves are not
representative you wouldn't write a
conjugate gradient I don't think anyone
would pay you for it but the process by
which you write it is quite similar
again homeworks are individual no
collaboration and all right so that's
the that's the experiments we have as
all good scientists do we also need to
get lots and lots of data from this so
let's look at how we could gather the
data there are there are different
methods you could use to gather data in
a study like this there's a group which
is gathering data by asking students
what they think they're doing so their
compile time questionnaires and every
time you compile it says so what do you
think you were doing one is I was I was
editing something trivial two is I was
taking out I don't know X amount of time
of speeding it up three years I was
debugging I found an error in my code
and it's multiple choice otherwise
really you can't do anything with it if
you ask them to write an essay level
answer every time they compile they'll
probably stop compiling all together and
and and of course there are shortcomings
to this partly because all good hackers
will figure out how to rap a call to the
compiler in a shellcode which answers
all the questions for them but but the
student himself might not know what he
is doing he might you know the
definition between debugging and
optimizing is hazy for a lot of people I
don't know I was i optimizing probably I
was I was doing a bit of both so maybe I
was optimizing maybe you might have just
been debugging maybe your program was
not gonna run before that so asking
students to answer these questions is
well it gives you some idea but I'm not
sure if it gives you a great idea on the
far end what you could do is you could
you could gather all the data there is
you could hook them up to a video
capture unit you just see everything he
does all the mouse clicks measure the
temperature and you know all that stuff
and you would get a lot of data you
would get way too much data I don't know
what you do it so both ends of the
spectrum are bad you probably want to
find a sweet spot in the middle and what
you want in the data that you collect is
that it should be simple simple in the
sense that you want to get only the data
which is relevant I mean you could
calculate or you could keep the guy's
body temperature but is it going to be
that useful we want the we want the data
to be effortless for the student so it
shouldn't involve a lot of effort on
their part to put in because otherwise
they'll just figure out how to write a
shell script if it's if it involves a
lot of effort and the guy is very
motivated and he takes him like a minute
to go through this he want to compile
less so you will change the system
there's some uncertainty going on there
and you want the data to be repeatable
if you capture the data and later you
decide oops I wanted this feature and I
haven't got it there should be a way for
you to go back and get it and the data
collection should be robust I mean it
should work in one classroom experiment
it should work two years hence other
people should be able to use it and
stuff like that excuse me and since we
were the the subjects under test for
humans you have to be careful it
shouldn't impact their grades you don't
want to give them better grades if the
answer the questions and stuff like that
so there's some stuff to do with that
students are given a fairly nice
framework that you that they have to
code in and they are allowed to edit
only one function in most cases so
everything is set up for them they have
a data generator that we provide they
get the data and they just have to do
the conjugate gradient at the end of it
they're told well it runs it's correct
because it's easy to verify so so that's
kinda nice because you contain the
problem down to to the very essence of
parallel programming you don't have to
give them the framework you don't have
to have them write the framework all
right and what we gather from this is
every time they do a compile we get a
time stamp on when they did a compile
there's the CVS log so we know what
source code they are compiling so we can
go back exactly to that point and look
at their source code and compile it
again later on in time and see wow that
was a correct run because we can verify
we also timestamp runs so we know how
often they did runs and grams are
important because if a run takes 5
seconds and it's followed by 2 or 3 runs
after that for different values for
different parameters you know that the
guy
is getting timing results and he wants
to and he wants to speed up his cool all
right so so with that I'll talk of how a
program of programs and what workflow
they have the workflow of a programmer
can be decided can be a directed graph
where nodes are the activities they're
doing and the arrows are their
transition from one activity to the
other and a formal model for this is a
timed Markov model with nodes
representing States and transitions are
timed the timing is important so we know
how much time they spent on the node so
if the mean time spent on an edit node
is three minutes you know that on the
average three months of spent editing
the filter code and once we do have such
a model we can change just one variable
holding all of those constants so you
can change a variable like how did the
play programmers do versus the IBM
programmers or how the UNIX guys do
versus the Windows guys all right before
I pass out here I'll turn the talk over
to Burrell who will cover the workflows
the models in our findings so thanks a
lot again Vic there was a great
introduction and so I'm going to
describe the modeling process from now
on we've described the experimental
process how we we set up the experiments
how you know the kind of kinds of
homeworks the students do the kind of
data that we gather and now comes the
part where you actually try to make some
sense out of this data will say what's
going on and like Vikram said we're
using time Markov models in this case
this is one way to do it and I'd be open
to suggestions from from all you guys
here feel free to ask me questions or
interrupt me at any time but rather this
be more interactive than just me talking
all the time so just a formal definition
what a time Markov process is for those
of you who might not know about it it's
it's basically a Markov process the
states of transition so state a could be
a rate and state B could be
let's say run so we put two numbers on
each edge transition which is the
probability of transitioning from state
a to state B what's the probability that
you're going to debug after editing your
code or what's the probability that you
might let's say to optimize your code
after this particular edit and the time
which is which is the 12-time how much
time did you are you going to spend
editing before you might debug or
optimize for instance so so this is the
framework so this is basically a time
Markov model that we're gonna use for
the mall we're gonna use gonna fit our
observed data to we have two years worth
of experiments to look at the first
experiment was in 2005 and just a brief
run-through of what that was it was a
parallel sorting homework in C++ and MPI
this was the one in which students were
asked a question every time the compiled
we didn't trust their answers so we took
all the code snapshots rerun it and use
simple heuristics to assign reasons for
compiling so for instance if you ran a
program and if you compile it it ranked
it compiled correctly we assume there
are no compile time errors if it if it
compiled we ran it if it ran correctly
we use the correct run if it crashed
there was a good chance they're gonna be
debugging after that specific run if the
program is faster than the previous run
there's a good chance that what they're
gonna that what they're doing is
optimizing this code so really just
simple heuristics nothing very fancy and
we put it on this model so so here's
finally actually a quantitative model
with numbers on it what's going on in
this model so the students start in the
formulate stage you know you can't
really see what they're doing at that
point they might be thinking in the
shower or they're thinking while they're
riding their bike to campus or something
they start programming at certain points
it's a two week long homework so at some
point they start programming and there's
essentially two cycles here so there's
the top cycle is what we call the debug
cycle and the bottom cycle is a compile
cycle so just like you know
imagine yourself writing a code you know
you write something you compile it
and maybe you run it right and there's a
good chance in the beginning it's not
gonna run it's you know it's not even
gonna compile if it does run you might
have to go back and debug it so you kind
of keep doing this until you get
something that is working and let's say
you're happy at some point with which of
it you know with what you have at that
point you might say okay I'm I'm gonna
I'm gonna optimize this program because
in this class students are graded on the
performance of their codes so you know
you have something working correctly you
go down to the bottom cycle and you
start optimizing your code so let me
just point out a few interesting things
in this in this in this model out here
for instance if you look at the compiled
to the test transition you know you can
only test after you compile I mean you
have to run your program so the
probability is one out there which is
which is the first number on the edge
the second number is 49 seconds it means
that it's taking on average students 49
seconds to compile their code this
includes the time that the machine
spends in compiling the code as well as
the time that it takes for students to
get rid of any compile time errors
question backing fall here so
refactoring would for for instance fall
in one of the transitions that goes from
debug to compiler optimized to compile
depending on where you're doing it so
that's where the students are really
writing all their code okay do you have
something to say so if the refactoring
after the code has compiled and run
correctly so if it's a conjugate
gradient and it's run correctly the
first time then we consider them to
refactor in the optimized stage but if
they've if it has never run correctly
the program has never produced a correct
output then it's still in the top cycle
so you use a very valid point here I
mean this is a very simple model this is
one student writing one program over two
weeks this is not how we develop
software I mean you know you typically
have the port software poor tools yeah
you know you have more testing you might
have specs you might have all kinds of
stuff
so this is
to be a simple model in which we can
think about things without really having
too many things going on some expected
things be but to compile if you look at
that transition it takes 380 seconds for
you to get something
it's 380 seconds while you're editing
the code so that's approximately like 6
minutes there when you're optimizing
it's like 10 minutes so again I mean
that's not completely unexpected right I
mean it you'd expect that you know when
you're working harder you're not just
getting rid of bugs but you're actually
thinking at a higher level maybe trying
to make your code faster the one times
if you look at the run to optimize and
the test to debug arrows the edges out
there you're spending about the same
amount of time running your code in this
case okay there's not much going on
there
the key point I'd like to point out in
this diagram is is if you look at the
test the test state so there's from the
test aid you can either have a working
program and going to the optimized cycle
which is great right I mean you want to
be in the bottom cycle you don't want to
be in the top cycle or you could go back
to debug you know your program didn't
work correctly or basically essentially
going that in the top side in the
topmost cycle there's a 95% chance that
every time you run your code you are
going to go and and debug your program
instead of making progress and and
compile your code and so you optimize
your code so when we're designing a new
programming language is what we really
want to do is maybe you know make a
programming language that that probably
allows us to stay in the bottom cycle
spend less time debugging more time you
know doing useful things so question I
mean if you're if you're using instead
of C and MPI let's say you were using
dynamic programming language with you
using you know something like Python for
parallel programming or one of the newer
languages that the vendors like Sun IBM
and clear proposing for HPC s with this
matter how different would the numbers
be just to give you an idea of some kind
of sensitivity analysis from these
number
it turns out that if you can change the
timings which is the dwell time let's
say I write a new language which in
which you know it turns out that instead
of 380 seconds it's gonna take you only
90 seconds going in debugging is that is
it a good thing and it turns out that at
least with these time Markov models what
you really want to do is is affect the
probabilities on these edge transitions
not the timings smaller probabilities
mean that you can have a nonlinear
effect in terms of the gains that you
might get whereas it we we believe from
our simulations that changes in time
have only a linear effect on the
increase in productivity so this was you
know data from the first year or we keep
emphasizing that this process has to be
repeatable and robust and simple and all
that so can we do it again and so we did
it again this year and it's a different
homework this was a class in which
students actually use - programming
language so now that we're taking a step
forward we're trying to compare two
different programming languages the
homework is game of life which I'm sure
a lot of you are familiar with it's
essentially 2d board that you have and
you essentially are just filling in
filling in cells in the board using some
very simple rules what makes it hard is
that this is a parallel program and you
just don't have too much time to write
this program so many data collections
nothing really has changed they're just
getting better at our data collection
and so we can we capture more runtime
information now again the hope was not
to replay everything I mean it takes a
long time to replay all these to encode
so we didn't want to spend time doing it
but every year it turns out that you
know we miss some data that we want to
gather and so we end up replaying things
in this case we spend 950 hours of time
on data star just replaying all the
student effort I mean every single
compile and run that the students the
students made and what came out of that
okay so it's MPI and UPC MPI is a
message passing interface it's sort of
the assembly level of parallel
programming where you say Oh
send a message from you know processor
one processor to get a message from
processor to processor one or you can be
fancy you can do non-blocking things
it's very message oriented you have to
break down everything into sort of very
small pieces of code N and sort of worry
about each section of code and messages
and everything you PC is unified
parallel see it sort of tries to get rid
of this message passing thing and gives
you a nice view of the language gives
you global arrays and sort of one-way
addressing for your programming language
we never passing messages you just copy
things around in memory so what happens
is you optimize it and if you look at
the run to debug transition here you
know you're optimizing your code and you
get a bug in the code so you're back in
the debug cycle and a lot of times what
happens is students are like oh my two
weeks are up now so just submit what I
have and so that replace represents the
low probability transition from going
from test to finish there are only ten
students so there are only ten finished
states in these codes I mean so that's
why the numbers are so small on those
the to look at is that MPI and UPC are
not that much different some I'm just
kind of going to flip between these two
to make things a little clearer of
what's going on if you look at the
compile States the time spent is
approximately the same you know it's 68
seconds and MPI
in the run a cycle 278 in the compile
cycle the UPC programmers do better with
their compiled timings they're spending
only 191 seconds in their optimized
compiled state if you look at the time
going from optimized to compiled how
much time you spend optimizing your code
you know editing your code in the
optimize cycle it's 83 seconds for MPI
763 seconds for UPC so maybe it's easier
to tune UPC programs if if we are to
believe these models but let's look at
the all-important transitioning in MPI
going from test back into the debug
cycle 92% here 92% of the time you're
not making progress
UPC it's 95% okay maybe MPI is not that
bad after all maybe what's going on is
that they're both equally bad or maybes
is that these students are not good
programmers at UPC an MPI because this
is really their second homework and and
they're not used to writing this program
codes in these languages question so I
need to repeat the question so that the
guys that you talk in here are so the
question is there were only five
students in each class and is this the
variability across across the two sets
of students is that the question
we have avoided doing any further
statistical tests at this point we are
just aggregating numbers and taking
averages putting them on these models we
haven't I mean you're very right I mean
there is there is actually a very high
variability in these students out here
let me get this question and then maybe
I can answer both
so that's another very good question and
the answer is no I mean there are often
students who spend very little time
writing their programs getting much
faster results I mean they you know this
kind of information you do not know how
many times so so here's one thing I
could do right this I think might answer
part of your question as well I could
make this model for each student now and
and and compare individual models and
maybe what I'd like to also do is a gora
maybe have a little number saying you
know student Eve and only ten times and
around the optimized cycle where student
B did that 100 times but maybe student a
got a faster code or then student B but
maybe it's called failed when I scale
from eight to thirty to process you know
all these kinds of things we have all
this data I mean and we see all these
things that that you say but we haven't
quite yet figured out how to put them in
the model how to reason quantitatively
about that and that is one of the
reasons I'm here to give this talk is to
get more ideas and feedback from from
guys from guys here so this is really I
mean this is really a very nascent
effort at this point only two years
worth of data I mean we don't even know
if we could make these models so so
these are sort of fresh off the you know
the fresh of the oven kind of results
here the next step would be exactly you
know comparing programmers see what the
variability in each group is we going
back to the question about variability
we they try to make sure that the bad
the groups are equally balanced in that
they were you know about the same number
of engineers and computer scientists in
each group and looking at the previous
one what we kind of try to make the
groups more equal so that the skill
levels are about the same
the UPC UN FBI we assign them randomly
yet initially but we did some swaps to
account for sort of both groups having
the same skill level from what we saw in
the previous homeworks this is and the
validation was that the speed ups that
the students got in both the groups was
about the same
although the variability was quite high
there were some people who couldn't
finish the homeworks and there were some
people who were packing things into
little into into bits and then getting
bit level parallelism out of their sick
girls you know code running on each node
so conclusions this is you know it's
repeatable it's systematic this is I
believe this is the first time that
someone actually put a quantitative
model trying to compare two environments
we do not have conclusions on what it
means it's too small a sample size
there's too much variability we need
more data and we need to figure out what
the right things to measure are not
restricted to high performance computing
this is something that you could do for
anything and everything and that's
something that is in the works we can we
can I are writing a little package which
would allow allow you to do it for
Python Perl MATLAB Java or anything that
you want to write in you write your unit
test frameworks beforehand and it will
tell you you know where you're spending
your time so you know I mean people like
me who do not like to use debuggers and
rather prefer print statements maybe
even I'm writing my code using this
thing and it says oh you're spending too
much time debugging I'm like huh maybe I
should really try and learn how to use
the debugger after all printers are not
that good or maybe maybe printers are
good who knows methods are not
restricted to the classroom you can
build up more models for other for
larger groups of people working together
for more complicated workflows that
actually happen in the real world people
delivering real products but the goal is
that feedback driven decision making and
development should allow us you know in
general to write better programs and
ship within time and budget and I think
that concludes our talk we're open to
questions
if you have questions and I really doubt
it but you might just have questions
that are proprietary to Google keep them
at the end of the clock so it can be
taken off the video I guess when you use
Markov models you gotta include some
assumptions general in what you States
to be so what I'm thinking is here how
much do you think here is something
about what the states are advising the
results for example it seems like you're
implicitly making some assumption that
no one ever divorced while they are
optimizing coaches or optimizes well the
debugging which you know a good
programmer should but so actually that
is so you are right in the sense that
our choices or States bias the data that
we gather although I would like to point
out that we do the the possibility of
debugging while optimizing and
optimizing while debugging is captured
because you can track transition between
the two cycles in this model but in
general you arrived at I have something
to add so you write the model that we
have right now gives it gives kind of a
rigid structure to what we think the
person is doing we could be we could be
entirely wrong but since you've done
data capture and a nice way since you
have all the source code we can run it
again and say you know we give a talk
and there's a person called Mike Barrow
who says well maybe you should do it
like this and you say wow yeah you're
right you know but we can actually go
back and test it and you never know it
might just be much better so you're
right we do have probably a problem with
the models but at least we can fix it
and acknowledge it yes
disciples you can regret this craft as
each cycle being one note and then
summarize the transitions from down
there
question so repeat the question the
question is and correct me if I don't
get a question right is collapsing the
cycles into actually individual nodes
and then trying to get the time spent in
each cycle and the answer is yes we do
have the data and for doing this but we
haven't done this yet and I think this
is an excellent suggestion and I think
that would be a very good way to do so
he could sort of just Club the top cycle
into one big node called debug and the
boring cycle into one big node called
optimize and just see how much time is
spent and it gives us higher level data
from this model
so the question is whether the students
were given canned sets of benchmarks to
test their programs and the answer is so
we have two years of data the first year
where students did parallel sorting they
were not given a canned benchmark they
were just told that we will test you
know we will be the adverse or if you're
just gonna give it like the worst case
data that we can think of almost
everyone used random data sets for their
testing and anecdotal I was the TA and
that's what I saw them doing the next
year we figured out it would be better
if we give if we gave them unique taste
and say you know you have two weeks
these are your test cases just get it
right on this and and that's what
happened the next year so so there was
more control in sort of the second year
of the experiment which is which allows
better data capturing
secondly was the code base that they
were working on
together before they were
some question one was about what other
parts of the lifecycle we would like to
capture and the second part is were they
working individually or not and let me
answer the second one first yeah it was
the homeworks that we measured students
were in fact working alone we do not
know if they collaborated be sure hope
not
now the first part what other lifecycle
parts of the lifecycle for writing a
program do we indeed want to capture and
as far as the homework school I think
this is okay but we actually want to
move on to larger programs where you
know I mean performance is really not
the only criteria how readable your code
is I mean do you write maintainable code
or you know or do you not metrics for
coding readability etc so it's a it's a
good thing to measure it would be great
if you could measure where does this guy
write good code when he's put in you
know X team and Google but the thing is
it's hard to do because you don't have
any control you don't have five people
doing the same thing also it's very hard
to know what you're trying to measure
and and towards that I say it's it's a
nice idea to have you know test-driven
development where you have a set of unit
tests and then you can give it to
different people probably working on
different projects and say that all
right all these tests are about the same
amount of effort and then compare so I
suppose it can be done
but we haven't looked into it yet
this might be a good way to actually
measure if pair programming works and I
believe that this is this is sort of a
new thing that has been recently going
on where you sort of have two people
working on the same computer scheme
working simultaneously on a program and
the hypothesis is that this works well
and and you're absolutely right I mean
we should use a model like this to
actually quantitatively prove one way or
another do you have any preliminary
results on that no not yet we hope to
start doing that for student projects in
the class next this year and so maybe
with a little something questions
this is a good start for measuring
productivity in the small but what would
we be doing this in the large or not
it's a good question we would very much
like to do it in the large I think we
have gained enough experience from doing
classroom fraud projects and this the
classroom experiments are going to keep
going on
irrespective but we would really like to
start doing this on larger scale
projects we have some ideas which a lot
of these things like replays are not
possible you know I mean we can we can
replay every snapshot of you know the
Google search the Google you know some
big project at Google but maybe we can
have plugins that actually go around
carrying subversion or CVS databases try
to be in information from them and maybe
take a guess at what's going on so so
this is something very much that our
group would like to do and the HPC s
program in general wants to do the
program was set up not for figuring out
how productive students are but for how
producting highly-paid BOE programmers
writing weapons simulation codes or of
course I mean you don't want to get
these highly paid programmers so writing
on a questionnaire so what did you do
this hour and you know so that's how the
classroom experiments started you know
the guinea pigs but but that's very much
a goal for us to go for I think I think
it is possible to do it in the large if
we if you sit down and think about it I
think if you write down you know like an
eclipse plug-in or something which which
gathers enough data about what you're
doing and you give it enough information
so you give it a unit test or you know
you you give it a fair amount of stuff
that it knows about your work then it
can probably tell but that will require
you require quite a lot of discipline in
getting the framework set up
the question is how do we how would we
capture logical versus physical design
issues logical and physical logical
meaning the thought process the
formulating that goes on so in one of
the slides we actually said that there
were several things that you could hold
hold constant and try to try to measure
the effects of things that change so one
thing I guess in in this experimental
setting that might be possible is is not
give students these harnesses that we
give them but just you know give them
problem description and see how they go
about doing these things there might be
other variation but there might be one
way for us to kind of capture people
actually you know are they writing
modular packages are they writing
comments in their codes are they you
know organizing their file systems well
or not I think again I mean this ties in
with your previous question is that this
would probably have to be on a larger
scale because in a small problem like
this these effects are unlikely to
actually even show up question
gave you about what they've been doing
and during each step you ended up
throwing that away and replaying it did
you compare how
so the question is people actually did
that we actually in the 2005 experiment
we did not use the the data provided by
the student questionnaires and that it
would have been a good idea to go back
and and correlate it with what we
guessed as opposed to what they provided
that was very much something that we
wanted to do our group hasn't done it
there is a group at University of
Maryland that has been trying to sort of
put the two together and see if students
actually report the correct activities
that they're doing we didn't have enough
people to do this at that point but
that's not a good reason to not do it
another reason why we did not do it was
also that we just saw a large we saw
most of the students just hearing one as
the reason for almost everything
like are they getting really frustrated
with the test trying to do or not
and how does that affect the diamonds
like if someone's really really
frustrated are they working less and
less efficient so the question is maybe
a reason to do something like having a
questionnaire along with these automatic
inference systems is to figure out if
the program programmer is actually doing
what he thinks is doing or she thinks
what she's doing and or if they're
getting frustrated while development and
maybe they should be going back doing
something more productive and coming
back I think that would be that would be
excellent I mean if wouldn't it be great
if your computer would just say you're
too frustrated you're locked out if you
go get a soda or something here that I
think that would be your interesting
question
say for example which students are a bad
pleasing and you have a heavy state
between sit and debug which is coercive
TV you are good out in the pub
how does this like how do you climbing
estimates handled people just taking
breaks and coming back to the next day
and we account for people going going
getting grabbing soda are going ease the
toilet or going to get dinner what we do
is if we notice more than I think thirty
minutes of inactivity VJs truncated not
not the best way to do it but there's a
good chance that the person was another
terminal we we do have more logs being
collected now you know with log outs and
shells and stuff but these are more
intrusive you need system administrators
and stuff
yeah you wanna take it yeah the question
is what we're really doing is we're
breaking down the the workflow into
timeslots of off sometime and christos a
question that is that what you want to
do oh sure okay
yeah the terminology of a session in
that we're we are allowing the person to
work for 30 minutes at a time as a
session it's interesting all right thank
you very much for being here we didn't
expect anyone to show up so we're really
quite pleased really good questions -
thanks so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>