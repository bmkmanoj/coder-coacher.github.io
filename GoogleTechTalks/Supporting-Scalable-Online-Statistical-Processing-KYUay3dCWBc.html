<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Supporting Scalable Online Statistical Processing | Coder Coacher - Coaching Coders</title><meta content="Supporting Scalable Online Statistical Processing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Supporting Scalable Online Statistical Processing</b></h2><h5 class="post__date">2008-04-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KYUay3dCWBc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome Chris remain here this comes
from Florida he is a decorated recently
attained professors right I'm going
through the time not quite there yet
slim fellowship NSF career everything
assistance river visa and the best paper
were that Sigma last year well thanks
nice introduction well thanks everyone
for showing up I guess I was warned that
the group might be small so this is
actually great okay so in general I'm
going to be talking about databases okay
and I don't know how many of you
actually have a background and in
databases so I'll kind of aim this with
a general computer science audience so
as a database person I do wonder the
first argue that databases are a key
computer science success story and some
of us well they are a key not leaking
not be key notice that I didn't say that
and so some of us database people go as
far as to claim that some or much of the
1990s economic boom was due to the wide
deployment of relational database
management systems even Alan Greenspan
might agree with that although people
blame him for everything now so maybe
that sounded great endorsement and so
how can I come here and claim that
database performance actually needs help
given all the successes that we've had
and my claim is actually that
performance might be great for
transaction processing but it's not so
hot for analytic processing and so first
alcohol spend some time talking about
what exactly analytic processing is so
analytical processing is big queries and
databases where the queries and the
databases are statistical in nature and
so here is actually an English version
of one of its kind of a key benchmark
query in something called the t PCH
benchmark now in case you don't know the
t PCH benchmark is it's a benchmark or a
set of database queries and an
Associated database that's put out by a
consortium of hardware and software
vendors and so this is something where
the database guys then go and
show how great they are by running their
queries on this benchmark and so here's
an example of an AP or a no processing
query so what you have here is something
that's statistical okay so you're asking
for actually a sum over a numeric
expression and then you've got a from
where your listing several tables so
there's actually a joint and these
tables are quite large maybe not as
large as what you guys have here at
Google but for database people they're
large and then you have operations like
group by and a sort over here uh well
actually that's a very good question so
I'll hittin that in a minute but large
you know the largest database is an hour
on the order of say hundreds of
terabytes what is you said there's
specific tables that are put out from
the specialized yeah watch there's a
data generator okay and so what you do
is so the way that it works is that you
are connected system and you say hey I
want to architect my system so it's
going to work at 100 terabytes of data
and then you actually type in a hundred
terabytes and press return of the data
generator spits out 100 terabytes of
synthetic data so so you actually see
benchmark results published for anything
from say 100 gigabytes up to around 100
terabytes so that would be the range
kind of three orders of magnitude button
so as database people we consider a
petabyte to be huge i think and kind of
within that order of magnitude is our
limit okay okay so this particular query
is equivalent to the following so it
says it says given database tables
wide-eyed n orders and customers so for
each group induced by the group by order
key order date ship pirate priority
clause and by the way if you don't know
what a group by clause does well it
essentially applies a separate selection
predicate to everything in this cross
product you can imagine that we got
these three tables what we're doing is
we're looking at all possible
combinations of tuples or records from
those tables and then we're basically
saying that we're going to break this up
into groups based upon those subsets in
the cross product that have the same
value for order key order date and ship
priority and then for everything in that
cross product you basically compute the
result of this function over the tuples
on the cross product of the records on
the cross product and this f of loc
returns 0 if the work
is false for a given LOC combination in
the cross product otherwise it returns
the value of this aggregate function and
so basically essentially what you're
doing is you're doing nothing more than
a big huge nested loops computation
right okay so that's analytic processing
okay so now here's the fun part and the
question is so if this is the kind of
query that we want to be able to answer
the question is how bad our current
analytic processing solutions well you
can actually go online and look at the
benchmark results and it turns out that
if you take a quick check of the recent
qzh results which you find is that
somebody can spend almost a million
dollars to store 300 gigabytes of data
and actually do this in 17 terabytes of
36 gigabyte disks and it turns out that
one of the benchmark inquiries still
takes 18 minutes to answer in the
throughput test in the g bch bunch park
and the throughput test actually kind of
tests a bunch of things concurrently now
if you've got 10 terabytes so let's kick
it up by a couple orders of magnitude
well it turns out you can spend almost 7
million dollars and then you're going to
wait four hours for one of the queries
to execute okay so that's not good I
think by any measure and actually quite
interesting in the sense that these
companies actually proudly display these
results okay so do you think these
decent they think that these things are
great so my conclusion is that current
analytic processing solutions are not
there they're not very good and the
result is that there's much frustration
from the industrial user base and people
are actually starting to complain about
this so they're complaining that Oracle
and IBM are not really solving their
problems in this arena and the result is
that there actually several recent data
warehousing and ap startups out there
that are really trying to compete in
this space well so actually I have a
couple students who just graduated and
they're going to green plum so that's
actually here in Silicon Valley then you
got like so Stonebreaker is starting
vertica and there are several companies
in the Midwest so just showing up ok so
well I'm going to talk about what I'm
interested in doing is helping this out
by actually applying the idea of
approximation to this particular problem
and so my ideas are my use of
summation and in helping this bad
performance is based on three key
observations so number one analytic
processing is almost always statistical
so if you're asking statistical queries
where you get sums and averages and
standard deviations and such it's not
clear that 1.37 45 million always
differs from one point three seven five
seven million now quite often these
numbers are very different right there
are certain queries where if you have an
an error out for places than decimal the
past the decimal that's a big deal but
in a lot of applications it's actually
not a problem right and actually a good
argument for this is that you know is is
along certainly knows a lot of the data
that gets into a warehouse as a result
of a dating creative data cleaning
process which is inherently erin AF
anyway and so why are you actually
spending all this time getting four
digits of accuracy to get the wrong
answer right and so this is key this
observation that maybe you don't need
four digits of accuracy is key because
it's it's often the case that you can
get something like 0.02 percent error a
lot faster than you can get zero error
so why do you spend all the time going
for the four digits of accuracy when you
don't need it and so given that I'd like
you to imagine the following database
management system so in this system you
ask any AP style query and the system
comes back and gives you an immediate
guess this guess is bracketed by a
confidence region and by confidence
region i meet something that is
statistically valid and says that the
answer is actually within these brackets
this confidence region should drink
strength throughout online the
computation and be 20 by the time you're
done so we got 0 with a query completion
and the total execution time if somebody
actually decides that they want to run
this thing all the way to completion
should be exactly the same as it is for
a classic or tpms so you should get this
approximation with no extra cost okay so
that's kind of my vision that's what I'd
like to go okay so any questions so far
by the way for an AP style query um well
no I mean there isn't there's not like a
canonical definition
so what I would say is that I can tell
you what the characteristics are that's
the stuff i am that i kind of outlined
before ok so it tends to not be very
selective it goes through a lot of
tuples in this big huge cross product
the answer is almost always statistical
right along with some grouping and kind
of ordering and so on typically what you
don't do and what databases are actually
very good at now what you don't do is
drill down to a specific record or
subset of the records right you're
talking maybe say one one millionth to
ten percent of all the records and a
huge cross product are going to be
useful ok so it's big it's a lot of the
subset of the data other questions yeah
is that an absolute or is it like a
ninety-five percent conference yeah it's
a statistical confidence interval okay
yeah I'll actually be talking about that
a lot more detail okay anything else
okay so the question is of course hasn't
this been done before well people have
actually been batting these ideas around
for a like actually almost 30 years now
and there's actually a very good example
of some of the guys at Berkeley and IBM
almaden worked a lot in this problem so
they had this thing that they called the
control project and this was really what
i think is the first kind of complete
attempt to foray into this area and
these guys got the error guarantees they
got online computation for some queries
but they what they didn't get is they
didn't get complete execution execution
from start to finish they didn't get
scalability and they get it they didn't
get generality and I regard you that
because of these problems this really
never took off so it's not clear to me
that you're ever going to have a viable
system that can only do this online
aggregation or this approximation for a
few seconds and then runs out of memory
right I think that you've got to have
something that you can basically plug in
as a surrogate for oracle or for db2 not
get any kind of hit in terms of the
performance and have it seamlessly work
so the whole time you get your
approximation right I think that you
can't expect somebody to give up
anything you've got to give them
everything they have now but then give
them a little extra ok so our work so
our goal is actually more ambitious than
what the control people did so we
actually want to redesign the database
from the ground up based upon
randomization
and approximation is kind of the key
design principle so we still want to be
able to do this and have scalability
before exact exact answers and this is
actually challenging because
randomization and fast computation of
exact answers are often at odds right if
you're randomly throwing stuff on the
desk then a lot of times you get
problems right trying to locate it
quickly and the resulting system that
we're in the middle of prototyping and
evaluation is something called dbl okay
so how to do this well given an
implementation plan like this one the
question is how are you ever going to
guess the answer from start to finish
and again since many of you are probably
not database table you might not have
seen something like this but this is
like a graphical representation of
something called a relational algebra
expression okay and what this is is you
can think of it as like a numerical
algebra expression except that it has
operations that work on sets and we've
got your some of the key operations that
you tend to find in an APA style query
so you got something like a join right
here and so what a joint does it says
find all S&amp;amp;S tnt combos where there's
some link right so join might take two
big tables and Link pairs up where the
say employee ID is the same you got
things like an empty joint operation
where it actually says find in this case
find all st and s join t where there's
no you and you where st b equals you see
in other words that's basically
subtracting the sets okay so you get a
query plan it's pumping all this data
through these operators and somehow you
want to look at everything that's going
through the operators and be able to
guess what the final answer is going to
be okay and so this actually seems
pretty hard to do I think at first blush
and so my observation is that Hassan
Hellerstein in 1999 these are from the
control people they gave us something
called the ripple join and this actually
works pretty well for this particular
part at least in very limited
circumstances so I'll start my
examination there ok so the ripple join
some of these basic ideas are really
what our system is based upon okay so
what the ripple join does is it says I'm
going to start out by throwing the data
on disk in a statistically random
fashion so both
the input top of tables in this case we
got two input tables are going to be
randomly permute adan disk these things
might be very large and so you can
imagine that the way you would do this
is by first associating a statistically
well what's something that's a pseudo
randomly generated with every single
double then you do an external sort
based upon these pseudo random numbers
that you glue to the topic so these guys
are randomly placed on this and so what
I'm going to do is I actually want to
come up with a guess for what this
answer is as I scan the data okay well
since the data is statistically random
on disk it turns out that if you simply
do a sequential scan of em in a
sequential scan of sales then all of the
tuples or records that you've read so
far are going to be a statistically
random sample of all the tuples in both
relations because of this it turns out
that there's a very nice unbiased
estimator for the final enter this query
all you do is you take a look at all the
temples you've read in so far you
actually compute exactly this query on
that subset of the two input relations
and in this case we actually you can
feed them you get a zero and the reason
is that Jan and Tom don't match up in
this case ma would be like the name of
the person ok and so this is a and this
is a they don't match up so what we get
here is we get a zero on the subset of
the data we read in so far right now if
you just took your zero this would be
biased for the final query result so
what you got to do is to unbias it by
multiplying the zero by the inverse of
the sampling fraction from each relation
and so you actually get a 36 x 0 and
turns out that the 0 is unbiased for the
final quarter result by the way in case
you don't know what unbiased means it
means that if you actually randomly
shuffled these input tables an infinite
number of times reran this and this
process an infinite number of times on
every single instance then on average
you would get exactly the right answer
ok ok so now the next thing that we do
it's a little hard to see on the screen
as we read in another couple from each
input relation do the joint and now we
actually get a hit right so you find a
three so the three gets put in here you
scaled up by the inverse of the two
sampling fractions and we get 3 times 3
times 3 or 27 as a guess and now of
course the guest has gotten a lot more
accurate because its variance or its
inaccuracies and has now decreased
okay so you read in a couple more tuples
so we got Tom and Joe we got it Sam and
John now we've got two hits do you take
3 plus 12 you multiply by the inverse of
the sampling fractions you get a better
guess and so you keep on doing this and
eventually once you've read in all the
data you get something that's a cool on
to the final answer ok so this process
has a lot of nice properties that's
consistent which in statistical terms
means that as you get more and more data
the variants of the estimator is
guaranteed to decrease you can't do
worse by having more information
actually it turns out that the error is
normally distributed there's a central
limit theorem for this sort of thing so
it's got a lot of nice properties ok so
great thing because there's a really key
invention in and I would say even in the
history of databases at least in last 20
years so any questions on this a so
there are all sorts of of simple
arithmetic or algebraic properties that
if you take a sample of them so if you
if you if you know one over some
normally distributed estimate isn't
going to be the estimate of one over the
random value and I'm trying to convince
myself that that you're not going to
build in that sort of bias with your
estimates yeah so okay so you're right
so yeah I don't want to go too far down
that path ok let me just say that this
is accurate for the simple case of a
some query or some arbitrary expression
ok so you're talking about there's all
kinds of theories behind ratio
estimators sort of a she owes are the
ones that come to mind right now ratios
you actually get it's actually possible
to unbox and actually the simple thing
for ratio estimator is actually
asymptotically unbiased it actually has
love it only has bias for very very
small sample size but it turns out
actually that the tricky thing in that
case is you actually have to use like a
delta method to get the variance and so
on so there's a lot of statistical
issues that's fishy with let me just
back up and say you're right and that
gets very complicated very quickly and
it's beyond the scope of this talk
but catch me after we can talk about it
okay this one make sure you okay people
are anything about this yeah yeah so
this is I'm trying to keep it simple by
saying we're doing a sum over some
arbitrary function in this case
everything I said is true okay so the
issue with all this thing is well it's
great okay and you can actually extend
it to other functions although it's
challenging only the hashed version of
the ripple joint is practical so what I
mean by hashed is the way you actually
implement this is that when you readin
up another temple from em you do two
things first thing you do is you probe
the hash table for sales that stores all
the hash tuples from sales to find a
match if you find a match you update
your aggregate so far right then you
have to take that tupple that's just
been probed and put in its own hash
table right then you read another couple
from the other guy pro over here put it
in sone hash table you get on doing this
back and forth and all works great until
you actually run out of memory right and
it turns out that this can happen in
only a few seconds on reasonable
hardware so nowadays you're probably
talking about a 70 megabyte per second
sequential scan rate right five disks
you know you you have a simple raid with
five discs so now you're up to about 350
Meg's per second right and so in only a
few seconds you're going to overwhelm
even a very large memory for a single
machine and so what this means is that
your estimation process works for a few
seconds and everything dies because you
can't store these hash tables in memory
so the issue is that while this horrible
join might not be directly applicable to
our particular problem the idea of
always adding more and more randomized
data and looking for lucky result tuples
so these kind of hits that will actually
contribute to the final aggregate is
central to query processing in our
system okay so first of all is everyone
clear what the problem is right you got
to have some way to do this probing back
and forth and as soon as you have to
page these things the disk you're going
to be in big trouble okay so that being
said so how does dbo deal with this okay
well imagine that we have a really
complex query plan so in this case we
have something that looks like an
eight-way joint okay on just eight
arbitrary tables and by the way we'll
all assume for the time being we have a
bushy query plan and not something
that's deep like with a spine to the
left or the right okay so what happens
in our system that we're working on is
that first
all of the bottom level table scans are
processed concurrently so all of these
table scans they start reading data from
disk and this concurrent processing of
the table scans is called a level I step
okay so the first level why step at all
times checks for lucky answered tuples
so what I mean by that is that as the
data are flowing through these table
scans the table scans are essentially
chattering back and forth and telling
each other what they have what they've
seen and so if they happen to get lucky
and find an output couple that actually
is going to satisfy the where clause of
the query and contribute to the
aggregate then this or actually if they
don't get lucky they get lucky or not
get lucky this actually allows you to
come up with an i guess it's actually an
unbiased guess for the final queries and
so the way we characterize this as a
random variable which will call n 1 and
it's one because it's the first level
why step ok so our guest is going to be
characterized as a random variable on
expectation the value of the random
variable is exactly the query result but
there is some variants or air so you're
the developer of you associated final
question query yesterday's query or is
it one for you separate random variables
from the sub queries the little one ok
good question yeah so what it is is it's
a random variable that corresponds to
the guest produced by each of these
individual table scans chattering back
and forth but it's a random variable
that on expectation is actually equal to
the final query result ok and the reason
is kind of intuitively the reason you
can do that is that at some point
everything that's going to flow through
the top of this query plan has to come
from right here right and so if you kind
of get lucky and happen to run into
those things as you're doing the table
scans you can imagine that it's possible
to make it guess for what the final
answer will be used because you sure
have the joint but we don't know yes
what is actually going to flow to be
counter to the top yes you know is it
going to be something out of B or a but
soul there's some ok well let's back up
and kind of again talk about what an SQL
query does ok we're where the database
query does fit effectively ok at least
in the case of something where you don't
have like an anti join your subtraction
what you do is you can imagine doing a
cross-product an eight-way cross product
ok of all these things then everything
that comes out of that cross product you
feed it through a boolean selection
predicate that either accepts or rejects
the tuttle ok and then you take the
result of that and feed it through a
final aggregate function that maybe
computes your total revenue or something
ok and so what's going to happen is that
again each of these guys are flowing
eventually through cross products I mean
that's what you can think of these
things is doing and so it turns out that
at every single couple that's going to
come up through the top has to have its
constituent parts come through the table
scans right and so if I ever see that in
memory I get lucky and all the
constituent parts for a given tupple are
in this black box at the same time right
then I have at uppal that's actually
getting contribute to the sum up here ok
and by seeing those things are not
seeing them it allows me to come up with
this random variable Emmel yeah each
woodhaven scan is not only ace papers
kind of big time could be but yet you
got ok guess it now ok ok so yeah so
this current value of the guests
corresponds to random variable and the
use of this n 1 plus an estimate of the
variance of N 1 plus a central limit
theorem over these kinds of cross
products actually gives us statistical
balance as central limit theorem
actually applies in this case and it
says that the air is normally
distributed ok so eventually this level
why step completes and all the lowest so
all these these lowest joints are done
with and n1 is frozen ok so at this
point what you've done is you've
actually run the joins table scans are
completed they'd fight all their data
into the joints and now we've actually
materialized or at least we don't
actually have to materialize them but
you can think of materializing
things all of these intermediate tables
are now sitting there somewhere okay and
so we're partially done with evaluating
query plan and we've got this guest now
get the guests could have possibly seen
all the tuples because it's seen out the
data that went into them but because the
timing could be off and you didn't
happen to get really lucky and see
everything at the right time of course
you're going to miss a lot of stuff okay
so while n1 is now frozen it will not be
updated because it's particular set of
table scans or its level white stuff is
done it still has some in accuracy okay
so what do we do well not surprisingly
you recursively do the same thing so
level I step 2 begins and this level I
stuff has its own estimator called n 2
now it's not good initially because n 2
hasn't seen much data when it first
fires up when it soon becomes more far
more accurate than N 1 and the reason is
that you can think that the a and the B
have kind of been condensed into a B and
so the the density of information there
is greater okay so the likelihood of
seeing any particular result couple gets
a lot higher in this step so while
initially you haven't seen much data and
n2 stinks very quickly it has far more
accuracy than n one because it's seen a
lot more data that's relevant to this
guy appeared okay so the thing is we
want to use both and so what we do is we
actually create those n which is nothing
more than a weighted sum of these two
end ones and then tues and it turns out
that it's very easy to argue
mathematically that by doing this very
simple optimization problem by putting
them together you get an estimate whose
variance is lower than either the
constituent parts and so yeah so we have
this n and this is actually what's
displayed to the user and it gets better
because it's got more information okay
so not surprisingly eventually this
level white stuff finishes and n2 is
frozen as well now at this point we're
going to fire up yet another one of
these things right so you fire up
another one you now get an m3 in this
case well I've step three begins and in
general you're going to incorporate this
new guy in here via a simple
optimization problem and n is again
continuously getting updated and given
to the user okay then level why such
step four fires up after
you down to one last table right and
this thing actually gets piped through
this guy directly and eventually the
variance of this n 4 is going to go to 0
and the reason for that is that
eventually every single result couple
has been pipe 3 aggregate function there
is no correcting for bias right there's
no sense in which you could have ever
missed anything so eventually the
variance of n 4 goes to 0 and then the
way to some has to go to 0 as well okay
so this is a very high level how dbo
works to give these guesses from startup
through to completion you got so any
high-level questions on this that makes
sense yeah so it's actually it actually
turns out there's a very simple but
Rajan optimization it goes on here okay
so what you do is you basically just
choose the weight so as to minimize the
overall variance okay and so
continuously before you send an update
to the user you recalculate the weights
based upon the variance estimates for
each of the N 1 through n 4 update those
weights and then that updates the
estimate okay because like eventually
right so let's say right 1n four starts
up for the fourth oh boy stuff starts up
n 4 has seen no data so it's got high
variance but very quickly it's very
insurance and so the weights adapt to
that computer based on the initial sizes
of ABCD up to 80 on the greatest
specifics or combination of there yeah
good question so they're updated off of
an unbiased estimate for the variance of
N 1 through and for okay and that itself
is an extremely complicated thing to
deal with estimating the variance so
actually if you yeah so you basically
just kind of trust me that we come up
with a statistically unbiased estimate
for the variance and that's what goes
into this optimization problem changes
that we is yes towards the end and for
will have the rest of that we go in the
beginning yeah right and the engine that
powers all of this in the end is
actually coming up with some kind of
estimate or a statistical
characterization of the process that
allows you to get an estimate for the
variance okay one last question the
initial estimates for for the weights is
based on the credit all the data or
combinations of them it's actually based
upon the as the weights are always based
upon the variance estimate which
themselves are based upon the data well
actually I don't want to say that so
it's actually yeah it's this based upon
the data and the query those two things
okay so some properties of the query
actually so in the end right you can
imagine there's some huge nasty gnarly
recursive formula for the variance okay
and then we have even huger nastier
recursive formulas that estimate the
variance based upon what we've seen so
far and these things actually have is a
component the data and they actually
have as a component properties of the
query okay so I think your intuition is
correct there okay okay so details on
this because there are several seiki
system design changes here compared to a
classic relational system and by the way
is there a clock around oh there we go
ok so in DB oh ok so all these
relational operations have to
communicate right they're always
searching for lucky result tuples and so
in a classic relational database system
you think of these things as being
basically black boxes a join does its
own thing it has no interaction with the
outside world except for the fact that
it's probably competing with resources
but here in dbo they're actually doing a
lot more than competing for resources
they actually participate in a carefully
choreographed dance that i'll be talking
about and another key thing is that we
always need a semi randomized output
from all the relational operations
because if you don't have some sort of
randomized output it's impossible that
statistically characterize what's going
on you can't argue for EM biasness you
can't say here's what the variances if
you can't statistically characterize the
order in which things come out and the
easiest thing to characterize is to say
hey this thing's random or at least
something random okay and so of course
we need lots of analysis I'll skip over
this today and the things that I'll
focus on is this whole process which i
think is pretty interesting of how all
these operations actually communicate so
that dbl looks for these lucky result
tuples
okay so basic data flow and dbl okay so
here's a simpler query for plan so you
can imagine that we've got table scans
they're operating over a b c and d so
there's a sort of pipelines that are
bolded here and in a level i step the
data flow between table scans or
relational joins through these pipelines
ok so the brain of dbo is a software
component call it controller and what it
does is it blocks and taps these
pipelines where tap is basically a
forking operation that carries copies of
the tuples coming through each pipe into
this other software component here
called the in-memory joint ok so the the
brain the controller is responsible for
putting blockages in the pipes ok so
what it does is the controller by
unblocking one of these pipes or a
moment's it allows a double or multiple
tuples to flow through the pipe one copy
goes to the end memory join and the next
copy goes the relational operation ok so
basically you can imagine the controller
is just basically pulling these lovers
right for all these things that are
allowing tuples to blow through the flow
through here ok so to actually
facilitate randomization in this process
all of the tuples from relational opera
supplied an order of a random value from
0 to 1 is associate with each double ok
this is what we just call this for no
reason a timestamp ok so in this case
let's say that we've got relation a and
it's got two attributes a and a dot B so
what we do is we somehow associate a
timestamp with each of these tuples and
conceptually you can think of the
following process what you do is you
stamp a timestamp with the couple then
sort all the tuples based upon their
time stamp this is at least logically
what's happening and then these guys are
sitting there ordered by stuff on their
time stamps guys so now you got all
these guys sitting here sort it on their
time step ABC and D so what happens is
that the controller looks at all these
timestamps and it chooses the guy with
the smallest timestamp and it pulls the
lever to unblock this thing so they got
the smallest timestamp is 0.04 comes
through it flows through an idiom memory
joint ok next then the controller looks
at this one unblock sake
I unblocks that guy on blocks that guy
keeps doing this and so on so that's how
it sends thing through ok so what the
in-memory join the scene is basically
seeing something that looks a lot like a
global randomized order of all these
tuples okay so that of course begs the
question of what's happening inside the
in-memory joint the control is directing
copies of these guys to the end memory
joint so the job of the IM j is to look
for these lucky combos of tuples that
are going to satisfy a where clause and
to use us to estimate an aggregate so
for a simple example say we're trying to
answer some over our for G where G is
just some attribute ok and we've got a
where clause that effectively just links
relation to one with relation to
relation to with relation three relation
three in relation for ok so you can
think there's a chain of links so an
example in the real world might be that
you have employees you have sales and
you have supervisors okay and so you're
basically saying that I want to know say
the sum over all employees you've been
supervised by someone in this particular
division of the company okay so you've
got a link employee with their
supervisor then you've got to link the
employee with the sales that they've
made and then an aggregate over all
those links is going to give you a
career zone that's exactly what we're
modeling here okay so who's our question
either on either I'm just catching up or
maybe I'm stepping ahead i'm not so much
it is so at any given point you have a
set of in the in-memory join you have
the set of tuples from for all of the
variables that are you're concerned with
and you can see does this satisfy
whatever properties then you get at
uppal at random knocks out none of those
tuples and this is presumably another
random sample which in some ways is not
independent but in the aggregate sum how
can you can basically use this you know
when you get the next a and then knocks
this a out that's you now have another
full set of tuples and memory join just
builds on that yeah I mean I think
actually you're stuffing ahead so that's
exactly the kind of stuff we'll talk
about so yeah so you can you yeah so
you're anticipating exactly what I'm
going to talk about so in a few minutes
ask that question again okay so that's
actually I great at least it shows I
something soft correctly ok so to deal
with this particular query okay the
in-memory join would have three hash
tables to actually implement this it
would implement this by having three
hash tables and one list where the list
it is a list of discovered aggregate
values okay so what I've got is I've got
a hash table for r1 r2 and r3 right so
the black circles there show with I'm
hashing on and I have actually got my
our 4G which is the guy that I'm
aggregating over okay so i'm going to
show you the process of what goes on in
for stuff flowing it into the in-memory
join to answer this particular query
okay so what you do is that when a
couple is piped into the in-memory joint
I'm going to try to build chains of
partial tuples from the left to the
right okay so the first thing that
happens is that tuples from the leftmost
relation that come in are always added
okay so no matter what happens at uppal
from that leftmost relation that comes
in detached into this hash table and
it's going to get hash it's our one a
value because you're linking r1a with
are to be and so when are to be comes in
you got to be able to probe over the d8
table right to see if you can build this
pain so everything from r1 immediately
goes in its hashed over to here ok so
then a 5 comes in and these guys are
coming in fully globally randomized
order based upon this whole time stamp
ordering okay then the three comes in
okay so now when a couple from another
relation comes in its only added if it
can join with an existing chain so in
this case I've got our to topple comes
happens to come in so it's going to be
added if it can join with an existing
chain and now I've got a chain here a
chain here in a chain here that I'm
building on and so what you do is you
basically say okay our QB well you probe
this hash table you find the match this
guy and then the r1 or those are two
Nazi detached over here because you've
actually got a chain that's going from
here over here okay yeah I don't
understand the vertical positioning
there oh this is just a hash table so
this is rand okay yeah so just some data
structure I mean you the hashing is an
important you could use a tree or
whatever
okay so now we've gotten our 30 our 3d
it gets probed over to here let's just
say by randomly this is the hash bucket
that it's it should find a mate in does
it find it so it gets crossed out next
another are three comes in it does the
probit find something the eight gets
added another are for probe misses it
this guy gets killed our to be oops i
just hit n by the way this is all kinds
of extra material so don't think i'm
going to subject you to it bad to have
the end button right next to the page 10
okay so i think i was somewhere around
here okay anyway yeah so are to be okay
so basically we got this probed right
here and then the seven got added okay
another are two comes in probe miss are
three this guy comes in gets added over
there now we got two eights and our four
comes in here's the interesting one hour
for F probes now the or 4G which is 13
actually gets added over here that and
so now at this point we've actually
discovered some tuples that are actually
going to produce the result now we're
going to contribute to the result okay
and so on right so this is the process
at a high level is that the new tupple
comes in time stamp ordering right
there's going to be some set of links it
has now in this case it was a simple
linear chain but it can be more
complicated and there can actually be
more complicated boolean predicates
going on but the high level happens as a
new guy in you probe some hash
tables if you find that that he actually
adds in then you hashem do its own thing
so you can build these chains up
whenever a chain mixed all the way to
the end this tote variable is actually
updated okay Soto is actually a random
variable that again it can be
statistically characterized okay so
using tote for estimation yeah now
though you mentioned the question of
filling up member okay so let's suppose
I actually if you don't mind I to
operate there because you're totally
you're anticipating something that's
coming on again yeah so you're you're
right on baller okay yeah so if I don't
address that then ask the question about
two minutes okay so using toad for
estimation okay so assume that the
recent timestamp the most recent time
stamp to the in-memory join us p and
there are n relations okay so then the
probability of this process discovering
any output temple is exactly p to the n
over n factorial okay and the n
factorial comes into play because you
actually discover an output couple if
all the constituents come in first guy
first second guy second third guy third
and forth and so on okay and so that's
where the factorial comes in and this
happens because in order like if you've
seen half of each input relation and
there are four relations those are the
one in 16 chance that you would have
seen all the constituents if the sweep
line or the p-value is 50% in this case
okay so that's the poor wheel so then an
unbiased estimator for the query answer
is precisely this right here okay and
getting the variance of this thing is
actually quite challenging and beyond
the scope of this talk it's actually a
very very interesting question in this
case okay but you can get it okay so
some additional details okay so note
that actually the ordering of the
relations matters okay and it's actually
quite interesting how it matters it
actually has no effect on their
statistical properties of tote okay
because that formula that I just showed
you and actually the variance formula
does not change no matter what the
ordering intuitively actually not gonna
make sense when you think about it
because it doesn't matter if I put this
one first then this one or this one
first would switch them around right
there's still a one in n factorial
chance that I get things in the right
order and there's still a 1 over P to
the N or actually a P to the N chance
that I've seen everything ok so the
ordering actually does not change the
system properties of total it actually
affects the size of the various
in-memory join data structures okay it
actually fundamentally exercise so y
will imagine that we want the average
GPA of all people pop by some professor
in the University of Florida's yes
department okay so we're basically doing
a joint over these four tables so you've
got prof the class they they taught
classes that students took and then the
student info
got so what you could have is this would
be one possible ordering student where
you append took to it append class to it
and appending off to it or you append
class the prof and then took the classes
student it took okay so here's a quick
quiz see if anyone's paying attention
which one the left or the right would be
a bad choice yeah student being first
right when you think about it makes
sense right because student is probably
a lot bigger if this is all the students
in the university well you f is one of
the largest universities in the country
40,000 students right this is bad
because you look at a student and you
don't even know if they're a CS major so
they might contribute to the final
result but prof. okay our department has
like 30 or so professors right so you
put proffers and you got a much smaller
number here so you change in this order
you have a much smaller data structure
than a key chain in this order so this
is actually requires optimization guys
you have to choose an order it's
actually a very interesting optimization
problem some were related to the classic
query optimization problem it actually
has quite a different structure for
reasons that actually I'm happy to have
you asked me about later but I have to
move on it's estimates it actually
doesn't it's actually quite interesting
work interesting to me painful yeah grab
this this way into it's actually
interesting so the intuitive reason is
interesting what I just kind of told you
a minute ago on but in the end what you
have to do is you have to come up with a
formula for the variance and then you
look at the terms and basically
does this depend on the ordering and it
turns out it doesn't ok so mission
additional detail so even with it
carefully chosen ordering the in-memory
join size can be huge it can be
arbitrarily large in fact if the
in-memory join discovered all the output
tuples and then there would be no reason
to do all this because you would
actually be able to answer the career in
a single pass and IBM and Oracle been
doing stupid stuff for 40 years or 30
years now ok so of course you can't do
that ok so to deal with this and then I
didn't mean to say USA a hardbound to
control i'm going to use a hardbound in
a soft bound on the sides invites of the
memory joint so what happens the way
this works is if the hard bound is ever
seated I'm going to choose one relation
to subsample so that the in-memory
Jordan size does not exceed the soft
bound okay so you're soft mountain might
be like I'm going to give say 10 gigs of
ram to this process okay so if my in
memory joy never gets higher than 10
gigs then I'm going to do is I'm going
to choose one of the relations to take a
random subset of so that the total size
of my think my data structure now falls
below my soft bound which say would be
eight gigs okay so what you do is you
subsample that relation you take a
subsample of it in any chain that is
chained off of a couple that gets
removed from that relation gets killed
okay so that controls the size so the
in-memory joint is actually always
shrinking the in memory footprint is
actually shrinking or actually grows and
then shrinks grows and then shrinks okay
now um so this subsampling of course
increases of variance because you're
killing information and so there's
another optimization problem here which
is which relation do amount get a
subsample okay and we actually do a
greedy thing here where we choose the
relation to subsample in such a way that
we want to minimize the variance of the
resulting estimate okay and that's even
not as easy as it sounds because you
actually have to figure out how much of
the relation you have to subsample in
order to get yourself under the soft
bound okay but then once you add this
thing in the new unbiased estimator is
actually this right here where you've
got this guy where the alpha J is the
fraction of the Jade input relation that
is actually present in the in-memory
joint okay so right now right at uppal
is going to be in the in an in-memory
Joyner in the in-memory join if the
factorial ordering was correct that's
one case one and the sweep line happened
to pass over it ok this time stamp was
ok and it didn't get subsampled out guy
and so this formula actually takes care
of unbiased and for all those
possibilities ok so these alpha you
select the off the j that you're going
to take as your master yes
minimize is a very good and that gives
you an alpha man you're going to this
decimate the other one the other ones in
accordance to the pool you have for the
chain right right and that will be
evolved for the others yes well actually
no no actually everything you said was
right until that will give alphas for
the others okay so so the deal is that
if some guy if there was like a chain
that now it gets is just got killed
because I got sliced through the middle
okay so now if the only reason that that
chain was in there in particular like
everything that kind of committed after
it that have been chained onto it right
if it doesn't have a reason for being
there anymore because what you're
essentially saying is I never saw this
temple you're pretending like you never
saw it it doesn't affect the sampling
fraction for the other relations but if
anything else got chained after it then
this guy right here this factorial thing
would have never allowed that chain to
grow okay so I'm not explaining very
well but let's say I have r1 r2 r3 and
r4 okay i decide to some sample are too
there's a temple from our two that got
killed where there was an r3 and r4
couple of tuples that chained off your
it what you're effectively saying is i
never saw that are to temple pretend
like i never saw it okay now without
affecting the subsampling fractions of
any other relation okay if there was an
r3 and r4 that got chained after this r2
you would have never had that chain in
the first place in the in-memory join so
you're basically saying you're
pretending i never saw it so it can be
killed and it doesn't affect anything
else is that ok so so very quickly i
guess is providing the timestamp
ordering ok so all of this requires some
time stamp randomization it's actually
no problem at the bottom level because
the data are actually stored in
randomized order on disk and you can
actually attach timestamps using samples
from a beta distribution plus a little
mathematical magic ok so you can
simulate in a pseudo-random way those
time stents not a problem it's actually
more challenging up here and the reason
is that if you actually implement your
wings as hash joins or sort merge joins
stuff comes out in order that is not
random right it comes out in the order
it wants to come out and so it actually
turns out that UX you need to modify the
operations so that they actually provide
for and are randomized output ordering
okay and providing a time stamp ordering
so like I said so most algorithm
actually be modified to reduce at least
if somebody random output order so an
example would be a sort merge join the
way you can do this is that rather than
sorting our one on so actually let me
kind of back up and say in case you
don't know what a sort merge join it
guys this is a standard way to implement
these joint operations the way a sort
merge join works is that you sort one
where you saw our one on the joint
attributes you sort are two on the joint
attributes using a big external sorting
algorithm then you do a one-pass merge
on the tube ok so to randomize rather
than using a lexicographic sort order
which would be the classic while you do
it you instead use a sort order based
upon a strong hash of the joint
attributes ok and so what this gives you
then is it actually gives you the sort
order or the merge order that you get is
actually randomized now it turns out
it's only semi randomized because if you
join on r1 and r2 on SE R 1 B equals our
to Z see you might get something that
looks like this ok so this might be
actually the order where this is the
first and the second and the third and
the fourth tupple right but here's one
more last quiz for you why might this
not be a great randomized ordering or
why isn't it perfectly randomized if you
look at it you actually see that it
doesn't look very random any when you
get an idea what's going to happen with
this whole process if what I do is I
basically say rather than watseka
graphic sorting and merging I'm instead
going to hash and then sort an emerg
yeah and and so you you know the board a
mushy come as that is not exchangeable
level yeah I gotta lose yes that's
exactly right that's what's happening
right so in this case right here like
the basically what happens is the
merging attributes or the joining at you
reach come out in clumps ok
inside of a clump these things are
actually indivisible right but across
clumps they're actually truly random and
so this is yet another thing that you
have to take care of it actually turns
out that this doesn't change that
doesn't induce any bias in the process
got biases really hard to it's hard to
induce bias in the law cases right but
it's very easy to crank up the variance
of something or at the very least you
actually change the calculations that
may be done so this is yet another thing
that makes the variance calculation even
more interesting okay so actually I'll
skip over this but we can handle other
types of queries okay and so here is
just a very simple example okay and this
is actually a bad example it's old we do
much better now but anyway so here's an
example of one of the t PCH careers I
forgot which career this is and so what
I've done is well actually our system
our prototype system actually implements
this query and runs it to completion and
26 minutes as opposed to 43 minutes on
postgres so basically we can do all this
randomization all this extra stuff and
beat post gross now I want to be really
careful because when I give this talk a
lot of people yell at me and they say
postgres stinks it's not a good system
why are you crowing about beating
postgres and that's actually not the
point here okay the point is that
there's only some evidence that we're
not killing ourselves by doing all this
stuff and I actually have better numbers
now that I didn't put in this talk that
are more exact and actually what we've
done is we've taken our system dbo and
ripped out all the estimation stuff and
compared it against itself okay and so
on the same query plan we basically get
like a ten percent hit somewhere in the
order of ten percent is the price we pay
for doing all the randomization in the
search okay so that's the point there
but then an interesting thing is that
what you're about like okay well on this
particular query like I said we actually
do much better now but at least in the
distant past this what this does is it
plots the duration of the query is a
function of the bound with actually the
other way around the bandwidth is a
function of the duration of the query so
what this means is that i'm guessing the
answer to this query and basically
saying how why are the confidence bounds
as a fraction of the current estimate
okay so you can think of this as being
basically what we're saying is we
saying so at this point right here after
four hundred seconds we're saying that
well we've got about ten percent error
at the confidence of ninety-five percent
okay so there's a ninety-five percent
chance our errors within plus or minus
five percent okay and so we actually get
a very nice decline in error okay and so
what these line shows actually shows
switchovers and the various level why
steps okay because this guy has four
steps that are required and so here you
got a switch and a switch and the switch
and by the way this guy right here this
axis is the is logarithmic okay and so
I'm actually quite interesting that what
you see is it's like I argued before
right when you switch over a level why
step then the information density is
much higher right and so you actually
get a kind of a drastic fall in the
bounds with okay and so the rest of the
talk is not going to be used so are
there any questions but didn't get you
got the one that I'd asked
so this this s tuition work for things
that query for exceptions affection so
look some is is sort of a i would say an
easy example because you could have a
nice continuous growth in or drop off in
your error guru but if somebody says
find me some condition where this
condition is the max of something all
the men of something or if somebody said
I want to find the top two percent of my
customers you're never going to know the
answer until the last last ten per
second or something like that okay so i
would actually i agree with some of the
things you said and disagree with others
okay so actually i could tell you so
there's actually a morse like this is
probably some of the stuff that you're
you're talking about right there I mean
there are differ we actually worried
about these kinds of things okay now max
and min in general are very very
difficult because they talk about one
couple okay we actually have had some
success on simple queries by using
Bayesian methods okay but I because
intuitively the reason is that you can
say things about Max and min if you may
have certain prior information about
distributions okay so going Batian is
one solution okay now but what I fully
admit is that I think at this point I
think we've decided that except in very
simple query that's never going to
happen so if you're talking max and min
that's the one thing that kills us okay
now that being said okay if you want
something in the top two percent right
like let's say I want the sum of all
employees or the sum of the salary for
all employees who are in the top two
percent of our sales force that is a
fundamentally different thing than max
or min the reason is that if an employee
generates say 10,000 sales per year okay
even after I've seen a small fraction of
the database right I might still have
enough information to say something
statistically about the chance that
someone's in the top two percent right
in the ranking factor yes but if but
there are other things but you might
want by the thing that you're computing
is dependent on computing the entire set
it sir like for instance somebody says
show me the top let me try to
reformulate perhaps a
so if let's say the thing in the
computing is based on on the sum of
everything that you've seen so far
hitting somebody says find me
find me the people who take more than
twenty-five percent of all the vacation
taken by everybody in the company okay
thank so for that you have to compute
the average number of days that the
person has taken vacation let's say for
the whole set and then you would have to
find who meets the criteria that you're
looking for okay so the thing is the
thing I'm getting to is the last filter
printer that says yes this is trouble
that I like or don't like won't have
enough information and let's see all the
records go get all their data but but
you'll have an app you can ultimately
you can s right Yugi long as you're
going through the records you can
estimate the amount of the data that
you've sampled and therefore the amount
of vacation that employs takes I think
you can that is it buddy wants to work
goals in different ways so actually I so
I agree with exactly what you start okay
so actually we can do it but let me just
say that so what you're talking about is
essentially correlated aggregate query
okay and it like for example the t PCH
there are several of these okay we think
we can handle these that being said it's
an extremely complex thing okay because
you actually have a cut-off exactly what
you're talking about this cutoff whether
or not this guy's in the top 25% or
whatever is actually at sulphur random
variable and then you have these other
things are not conditioned upon that and
actually including people are not
including people in the final result set
now the inclusion and exclusion is
actually correlated with one another
because they're actually all conditioned
on the same aggregate that itself was a
random variable okay and so you actually
get all kinds of now you have to worry
about covariances between things and so
on okay we actually have a paper well
actually we submitted a paper and plug
some of that stuff into and reviews came
back and said this is too complicated
take it out but we've actually worked on
that okay um yeah so it's a great
question it I think I think the
take-home message here is that there's
it's a cool problem right because you
you end up rethinking everything like
that right and there are only a few
things that are really really impossible
to do and Max and min I think are in
that
that category
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>