<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>HealthMap: Digital Disease Detection | Coder Coacher - Coaching Coders</title><meta content="HealthMap: Digital Disease Detection - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>HealthMap: Digital Disease Detection</b></h2><h5 class="post__date">2008-09-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/J31z8_dzp54" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Cory Conrad and I work with
google.org predict and prevent
initiative we're predicting and
preventing emerging threats and our
initial focus is on emerging infectious
diseases and in that light I am thrilled
to introduce you today to health Mapp
health Mapp is one of our early grant
recipients and initial partners in
earlier disease detection and they're a
really cool organization focused on
detecting diseases as early as possible
through a multi stream real-time
surveillance platform continually
aggregates reports on new and ongoing
infectious disease outbreaks health map
launched in September of 2006 and has
since been featured in a number of
mainstream media and scientific
publications
these include Wired News The Lancet
Nature and Science
they've also published peer-reviewed
articles in a variety of journals with a
recent publication in PLoS Medicine
which is definitely worth checking out
if you haven't read it already today
we're joined by co-founders dr. John
Bronstein and Clark Frye Feld and their
colleague dr. Michaela Keller John was
trained as an epidemiologist at Yale
University and is assistant professor of
pediatrics at Harvard Medical School
where he has joint appointments and the
Children's Hospital informatics program
and the division of emerging emergency
medicine Clark is a research software
developer at the Children's Hospital
informatics program
he studied computer science and
mathematics at Yale and his main
interests include web-based UI design
data visualization text mining and
technologies for developing countries
and mieka is a computer scientist
specialized in statistical machine
learning she's focused on developing
approaches to enriched text
representations by incorporating
knowledge gained across large unlabeled
data sets
before joining chip the Children's
Hospital informatics program where
health map resides is a research fellow
she was a PhD candidate
Ecole Polytechnique fédérale de
Lausanne in Switzerland so please join
me in welcoming health math as they give
a tech talk today thank you so much for
having us today we're really delighted
to talk to you about our project health
map org which is a global disease alert
map designed to scan the web for reports
early reports on disease outbreaks so
I'm just gonna start by giving you an
example this is a recent salmonella
outbreak that we had that we experienced
in the US over the past summer we all
know the serrano pepper was implicated
and we can see the spread of this
outbreak emerge through the US starting
in New Mexico but eventually making its
way to over 40 states now you would
think that this data was actually coming
out of an organization like the CDC or a
public health department in reality this
data came to us through Google News and
in fact the first report that was
identified that we could find across the
web was not a CDC report but was in fact
out of las cruces news Sun news and it
really indicates to us the potential
value of mining the informal news media
and other resources for the purpose of
Public Health and in fact if we look at
Google News versus the CDC reporting the
eventual CDC reporting they actually
line up quite nicely there are some
places where the outbreak is actually
reported first in certain states in the
news compared to the CDC and in fact if
we look at the epidemic curve the first
reports that we can identify across the
web really came out of the informal
reporting and then and well before a New
Mexico reported on the outbreak and even
even further before the CDC report came
out but this example is not necessarily
a link to a US as US specific if we
think about the reporting of SARS and
what happened over the time period in
which SARS developed in in China in fact
the first reports here once again were
not actually out of out of public health
agencies but in fact out of the news
media one examples very early on a
pharmaceutical company released a
financial report discussing cases of
atypical pneumonia followed by Chinese
media
language media reports coming out about
about a cluster of cases happening in
Guangdong province as well as an astute
physician on a mailing list called Pro
med that was discussing problems
emerging in schoolchildren and this was
well before the w-h-o was actually
reporting on this information globally
and in fact really their official report
came well after the the Chinese report
came I mean that the report of the the
the outbreak in Mahon Congo tell that
eventually led to the the spread
worldwide so in fact the sources of
outbreak news that are that are in fact
go on to be verified by the w-h-o really
come out of these types of new sources
these wide informal data sources that
that really are much greater value than
some of the other more traditional data
streams that one might think of when you
think about verifying Adisa disease
outbreak around the world and so when
when we think about the types of data
the traditional surveillance sources are
some really important limitations
especially when you think about global
surveillance the issues of lack of
infrastructure and training and gaps in
coverage and especially when you think
about information flow as what happened
in the SARS outbreak then when you think
about informal surveillance of the
surveillance of rumors and news media
and other data streams that we can find
in in mind on the web we really see that
this is an abundant and cheap resource
that provides detail the near real near
real-time reporting and is really less
susceptible to political pressure so
we've really focused our attention on
thinking about these types of data
streams and how we can we can really
harness the power of what's out there to
to better understand the global burden
of infectious disease but this concept
of internet-based searching is not a new
one other organizations have looked into
this concept but what we saw as we
developed health map was that there was
a tremendous amount of data out there
and really no comprehensive system so
and one other issue is that this
information is often unstructured and
free text and it's not categorized in
any useful way it's not brought together
and each data source has its important
gaps in geography and expertise in
population and so really what we are
trying to do is think about how to
integrate all this information into one
common platform to create that
sort of synthesized view of global
health so this is why we created health
map it's that health mapped org and it's
essentially a system designed to query
and mine and and scrape the web for
reports and bring this all into one
common interface so that someone anyone
can get to this site and and look at
look at ongoing emerging diseases so
today I'm going to be talking to you a
bit about the system overview and then
followed by that Clark will actually
give a technical description of really
how the system works
Mieke is actually going to go on to talk
about ongoing research objectives and
then I'm just going to end off briefly
with with some research directions
future directions for the system so what
are really our objectives I think I
already outlined that but essentially
the idea is that we want to enhance
surveillance through integration so data
fusion is the big term right now so the
idea of bringing Newswires RSS feeds
mailing lists anything that we can get
our hands on that we can then
automatically scrape and categorize and
filter and provide into a common
interface and the idea is that we
integrate all information about a given
situation to produce a meta signal or a
combination of all the information about
a given a given outbreak and so the idea
is that we can achieve this unified and
comprehensive view of health in both
space and time mapping and really one of
the major objectives of the system is
that we limit information overload so we
all know that when one outbreak gets
reported there's an AP story this gets
flooded in the media and really the idea
is to limit the duplications filter out
the data and really just make the most
important information available to the
user and really at the heart of health
map is a free and open system so we
really rely on open source technologies
an open API is like the Google Maps API
and so really the idea is that this
system remained free and available to
anyone who wants to use it so when we
think about who wants to use it well
there's the public health so public
health agencies we know that you use a
system like the UN and w-h-o for keeping
abreast of what's happening in the
population but on the other hand we also
have a real interest in in developing a
resource that's that's of use to the
general population so we want it to be
available to someone who's thinking
about traveling abroad and thinking
about how to prepare
how'd it what what to do in terms of
vaccinations thinking about individuals
who just want to know what's happening
around them so it really has a purpose
for anyone really who's interested in
public health so in terms of our usage
we that we have tremendous variability
but essentially about a thousand to ten
thousand unique visitors every day about
over half a million since its launch and
really the top visitors are in fact
right now from government agencies like
the CDC whu-oh homeland security these
are some screen shots actually of
current implementations of health map in
command centers like in health and HHS
and the European Union and in fact right
now we're doing surveillance for the the
upcoming national conventions in
Minneapolis and Denver and so we're
providing feeds of our data to the those
the ongoing surveillance activities so
how does the system work Clark's gonna
get into really the nuts and bolts of
the system but the idea is that we come
up with a report this is a news report
of bird flu in China our system is able
to detect and scrape the specific
keywords whether it's China bird flu and
then specific language around the types
of - the type of disease and specific
local localization of that particular
outbreak as soon as that article gets
posted within the hour it would end up
on health map so if someone was
interested in avian influenza they would
filter by avian flu and then be able to
then look at all a V and current avian
influenza reports that they weren't
specifically interested in China the
zoom in there and then get access not
only just to that report I showed you
but all other ongoing reports that are
happening about even influenza in China
and so someone can get a very broad view
of all ongoing alerts and articles about
a given situation in fact actually just
five minutes ago we just implemented a
new view of health map which is that
using the the Google Earth plug-in so
now we have a even better visualization
tool to actually start really focusing
in on specific areas and getting
additional data layers and another view
of the system is actually a situational
awareness tool so we known only provide
the map interface but the idea is that
we can put all the articles together
about a given situation whether it's
avian influenza articles happening in
China or just other outbreaks
happening in given location we can put
all this information together into one
common framework so so someone could get
all access to that information right
away so in terms of the amount of data
that we're collecting right now it's at
about over 200 outbreak alerts per day
with over a hundred thousand collected
so far so we're talking about a fairly
large amount of data and compared to the
fact that for instance the WHL only
reports maybe one outbreak per day over
a given year you can see how in one
particular day we're already collecting
more than what the w-h-o might do in a
full year and our alerts are global so
we're getting right now we've covered
about 201 countries over 175 diseases
and right now we're searching in five
languages so in terms of the geographic
representation as I mentioned we have
fairly good coverage across the globe
obviously there's a bias in the news
media towards places with more
technological development so we'll have
a bias towards instance outbreaks in the
US and the UK and other places important
to note is that we do have a gap in in
in in parts of Africa and that's really
important to us to figure out how how to
start to fill in those holes whether
it's through languages or through other
types of data that that obviously is not
available electronically with the Kiba
ideas like SMS messaging and so other
ways to fill in those holes so we
recognize that there are other types of
data that we need to be we need to be
pulling in terms of the disease's we're
collecting we have a broad variety of
infectious diseases and once again we
have a bias towards avian influenza
things that are right now in the media
spotlight and so that those are the
types of things that we need to account
for when we think about how to properly
treat our data but so there is a focus
but we do have a fairly broad range of
infectious diseases that were that we're
reporting on one thing that we're also
interested in is the types of sources
are reporting on this data so is we are
hoping to attention find certain sources
that are the early reporters that really
the great new sources that provide us
the key data but in fact what we find is
that there is a there is no one source
that there's many many sources that are
the first to report on a given outbreak
we do have particular resources like all
Africa calm or shed
that are regionally specific so we get
very good regional coverage but but what
it really comes down to is that we
really need to be monitoring all these
sources of information what we're also
interested in this reliability of new
sources and this is a very difficult
question and fairly difficult to answer
one way we've tried to look at this is
how does a news story begin to get
replicated in the media and if a news
story is taken from one source to
another is that maybe an indication that
a potential source is more reliable and
so we're building in reliability scores
for the data to really assess how much
confidence we have that an outbreak is
is happening and this is important from
the perspective of the issue of for
instance inserting a false report into
say Google News somehow that would then
get fed back onto the map so we're very
concerned with how to properly validate
the system and I don't have time to go
into too many of the details around many
of the different studies were involved
in but one of the areas is around
thinking about how to how to think about
the timeliness of our reporting and it's
very difficult to actually get a a good
gold standard of all ongoing outbreaks
that are happening around the world at
any given moment in fact though the
w-h-o does do a very good job of
reporting on avian influenza cases so
for us what we can do is begin to think
about when and when a particular case
began and think about the time in which
then it was reported in the news and
then followed by that reported by the
w-h-o so one thing we're doing is for
instance this's cases a cumulative case
count of avian influenza over 2007 and
the light blue area represents the time
in which it was the case was started the
date in which the case was started the
dark blue represents the time lag to the
actual report in the news media and we
see it still takes a bit of time before
that case reaches the media but yeah
nine days but but what we've seen is in
fact that lag is still smaller than what
we get reporting in for instance from
the w-h-o where the lag is still a few
days after that point in time and I
think a lot of our work is going to be
directed at thinking about okay what is
the baseline reporting and how do we
keep trying to improve our our mechanism
how do we how do we keep shifting that
timeliness
and make that smaller one other way that
we're trying to evaluate the data is
since we've only been in existence for
two years is to think really
historically and and one way we've been
doing that is looking back in time this
is an historical time series of of
influence over the last 70 years and to
think about how to match that with data
that we can still mine from Google News
archives and so if we in fact look at
the the the time series that corresponds
to that this is in Google News archives
that these these time series are
actually incredibly well matched both in
terms of their cyclic behavior as well
as in terms of their magnitude and so
it's giving us some perspective into how
the news media reports and in fact the
quantitative aspects of the news media
volume track very very incredibly with
the actual population level burden so we
can do things like track the news media
in terms of this is the 1918 pandemic
thinking about the news media is
actually analogous to public health data
and and in fact we're doing a variety of
investigations to think about how the
data this informal data really is is a
great adjunct to to traditional Public
Health surveillance this is just an
example of a specific city level burden
and use reporting out of Google News
Archive and how this particular data
does match up very well with the actual
burden that was experienced in those
particular cities so I know that it was
the sort of quick overview I'm gonna
stop here and then turn it over to Clark
who's actually gonna talk really about
the system and how it works and really
get under the hood of it so I'll turn it
over to Clark my name is Clark by
feldheim a software engineer at health
map and I'm gonna sort of pull back the
curtain a little bit and run through
some of the technical details behind the
system I'm gonna present a lot of
details and numbers and things apologize
for sort of moving quickly through but
obviously if you have questions you can
certainly ask just to give you an
overview of what we're trying to
accomplish with the system you can
consider the the sort of the whole
universe of available data basically the
entire Internet and then consider the
small subset of that which is health
related reports of any kind and then
another subset of that which is the data
the reports that how
SMAP system is potentially capable of
collecting and then again the subset of
that which is the the reports that are
actions that we want to display that are
actually relevant to our mission of
tracking disease outbreaks and then
finally within that the very small
circle is the subset of reports that you
as the user might be particularly
interested in so there may be you are
interested only in reports that have
international significance or only
reports for a particular location or a
particular disease so that's the idea so
the the first step of the process is the
acquisition of the reports through news
aggregators such as Google News we hit
roughly 20,000 sites every hour on the
hour the next step is extracting the
primary location and primary diseases
for the report the next step is
categorizing the reports into these five
different categories really what we're
primarily interested in is that first
category breaking news
so that's any report that refers to an
actual currently ongoing outbreak what
we call a warning is generally like a
disaster situation where no actual cases
have yet been identified but disease
outbreak is considered to be imminent
old news is like a follow-up report
about a previous outbreak like a lawsuit
or a Senate hearing etc context is kind
of a catch-all for things that may be
health related but are like sign out
comes of scientific studies pandemic
planning vaccination campaigns other
types of news and then not disease
related I think is self-explanatory the
last step is aggregation so as John
mentioned when a big story hits the news
many different sources will report on
basically the same event so what we what
we have is some algorithms for
collecting those reports together so
that on the map you see basically that
first report only rather than being
flooded with with all the repeats so
these reports are collected they go
through that process of automated
algorithms they get posted directly to
the map at the same time we have this
administrator interface which allows us
to review the reports as they're coming
in and make corrections to
the potential misclassifications of the
algorithm it also so so it improves the
output of the system obviously but it
also allows us to track the performance
of the algorithms and improve them as we
go another sort of prototype feature
that we're working on is email alerts so
you can get a customized subset of
reports delivered to your inbox and then
there they're also links embedded in
that email which allow you to click
directly through to the administrator
interface and correct misclassifications
as they occur so getting more in-depth
on the acquisition phase we collect
reports from various sources mainly news
aggregator is also mailing mailing lists
of infectious disease specialists so one
key area of our research is looking at
which which are the best keywords to use
we want to do searches that will give us
a high yield in terms of the maximum
number of articles that refer to actual
ongoing outbreaks while also minimizing
a number of reports that are kind of not
what we're interested in so you know
those pandemic planning etc etc so and
then another challenge we have with that
is as we expand into other languages
other than English we're currently
operating in five languages how do we
again find those words in different
languages because for example use the
word outbreak in English works pretty
well in terms of pulling pulling out
those relevant reports while other
languages don't necessarily have a
really close synonym that for outbreak
as we use it in English so that's a
that's an ongoing process in in each
language so them so we extract various
data from the from the reports as they
come in and we also pull out the actual
textual content of the article using a
neural network system that Nica
developed actually so again as I
mentioned we're collecting reports in
five different languages we're sort of
just getting started with some of the
non-english languages but that gives you
basic
breakdown of what we're doing with that
moving on to the extraction step this is
where we pull out the disease and
location for the report so it's what we
call dictionary based approach or
gazetteer so what you see in the upper
left there is an extract from the
dictionary file consists of a list of
text patterns matched with a list of
ID's for the particular item we have
about 600 different locations worldwide
so that's all the countries and then
states and provinces for several
different countries and then 200
different disease categories and then
the dictionaries consist of about 7,000
different text pattern which map to
these 600 locations and 200 diseases so
just for the those technical people in
the audience basically we load this
dictionary into the into memory into a
tree like that and then process the text
word by word matching it to the elements
of the tree and if we get to a leaf
which is an ID than then that means we
have a match so there's certain
advantages to this approach one is that
we can get started right away without
having to build up a training set of
human curated reports because we can
basically just take a list of countries
generate a dictionary from it and get
started the performance is pretty good
because it's it's linear on the on the
amount of text that you're processing
the articles are pretty short so and and
then and this is the it's particularly
important because we're expanding the
different languages so we need to be
able to generate that dictionary rapidly
as we start with new languages there's
obviously some problems with this with
this approach it's fairly rigid because
it's rule-based we don't really have a
good way of disambiguating place names
so like most of the time
London refers to London England but
there's also London Ontario in London
Georgia
and so the system doesn't really have a
good way of distinguishing those
different types of situations Mika is
actually working on a more flexible
machine learning based approach to to
addressing some of these issues and
she's going to tell you about that
shortly
so in spite of those drawbacks the
performance is is reasonably good
basically won't get in too much detail
but basically we can look at well out of
the alerts that we collect excuse me how
many of them required some sort of
correction to the classification that
the system did and from that we can
compute what what the accuracy of the
system is so another area that we're
exploring is going not only improving
the accuracy of the location extraction
but also getting beyond that list of 600
locations obviously there are many
millions of local places that are named
around the world and this is a tool that
we've developed for building a training
set getting into more richer location
databases okay third third stage is the
categorization to the categories as I
described this gives you a picture of
the what the distribution of them and
it's basically 50/50 between braking and
context so that's really kind of the key
distinction that we look at analyzing
the system so if we look if we consider
a breaking news alert as a positive and
a non breaking news alert so essentially
context as a negative then we can
compute sensitivity and specificity
statistics for the performance of the
system so basically just to review maybe
if you're not familiar with the term so
sensitivity basically gives you a
measure of how well you're getting the
true positives and then specificity says
how well you're you're identifying the
true negatives so that's the that's the
rundown of that
final step is the aggregation collecting
duplicates together again I won't get
into the details of exactly how it works
but basically we do textual comparison
of the different elements of the report
one key component of it is that we
actually consider the the frequency of
words
so if two different reports have a very
rare word in common then it's very
likely that they're referring to the
same event whereas if they have a very
common word in common then it could be
that they're just that they're
completely different reports just
happened to have this the same we're
gonna common and then the other the nice
thing about this system as its set up
now is that it pretty much works out of
the box for new languages so we don't
really have to do a lot of build-up work
to make that work so again do some
analysis of this by assigning a positive
and negative so basically a positive is
when one alert is truly similar to
another alert negative is one of when an
alert is unique so again those are the
those are the numbers for that and then
we can also kind of step back and look
at the overall performance of the system
so we basically consider what we call
visible alerts versus invisible alerts
so what's visible is something that is
breaking and a primary so not a
duplicate of another alert and then we
look at well how many changes do we have
to make the system that that would be
actually visible to the user as opposed
to kind of updating metadata behind the
scenes and that sort of gives you a
rough breakdown of what the system is
doing so the green circle is the set of
alerts that are visible and it ends up
being about 20% or so of the total
reports that we collect
and then again if we take visible as a
positive and invisible as a negative we
can compute the sensitivity and
specificity statistics so looking at
this you might say well the specificity
is obviously pretty high the sensitivity
is not so high why would that be when
this is a disease outbreak tracking
system you think well it's it's okay to
have a few jump reports coming to the
system because it's really important to
be able to detect you know the next
outbreak of SARS so why not sacrifice
some specificity in order to gain
sensitivity so the the answer to that is
basically because there's so many
reports that we collect you can see that
raw numbers there that that are actually
junk reports if we sacrifice the
specificity then the volume of junk
alerts that's going to flood into the
systems actually is actually quite high
okay so so that's the basic rundown of
what we're working on what we have
running currently there are obviously a
lot of different areas where we're
looking to improve all these algorithms
in terms of acquisition we're expanding
to different sources trying to fill in
the gaps where you know as John
mentioned we have a heavy focus on the
US and other industrialized countries
because those are the ones that have the
most active electronic media so we're
working to fill those gaps we're adding
in clinical sources like a Geo Sentinel
Network which is a network of travel
clinics adding more languages and then
improving the algorithms that we apply
to the incoming alerts as I mentioned
getting getting beyond the 600 locations
that we use also extracting other
aspects from the report like the number
of cases the number of deaths etc and
then we're also bringing in people
people from the outside to make use of
that administrator tool to sort of get a
communicable community around this and
kind of spread the work out of building
training sets and correcting the
algorithms and then finally we want
working on improving the front end so
may create a richer visualization better
customization and and linking up with
other systems like Pro Madden and John
will speak about that more so with that
I'll turn things over to mica thank you
I'm doing a postdoc in the house PAP
team and I'm going to talk about so
we've been interested in in looking back
at the data we've collected for one year
for obvious reason for example for use
it as a training set to be used to
improve the outbreak detection and
characterization but also because we
want to be able to compare the data we
obtain with data that W Cho that was
verified as being true outbreaks and
also because we want to make analysis of
epidemic want to discover if there is a
epidemiological patterns on calling each
year and so for example in the case of
the the timeliness verification you kind
of need to single out each outbreak to
be able to compare it with the verify
outbreak and this is not so trivial to
do because the outbreak we detected or D
outbreak I mean to do it automatically
because all this information is
extracted from free text which it's
always a little bit ambiguous and
because how is an outbreak characterizes
an outbreak is characterized by the
ongoing disease the location where the
the outbreak is taking place when is it
taking place and who is involved in this
disease the patient in for example then
the number of cases or the sex or the
age of the patient
and so in traditionally in natural
language processing this is a this is
described as the task of information
extraction automatic information
extraction and so health map already
does information extraction as Clark
explained for example the DZ is the
location and the time windows are taking
into account by by in by health map but
we would like to have a more precise we
would like to assess that the fourth
point and also have a more precise
geographic location and so what I'm
going to present is a is a sub tasks
that can be probably generalized to to
the this other information extraction
problems but I'm going to apply it
particularly to the to the geographic
location extraction so I'll first will
present the dis sub task which is the
geo parsing then explain the method I
have developed and then some experiment
so the the end task is called geo
indexing which is in fact associating to
a document Geographic index this usually
involves somewhere other a gazetteer
which is a list of geographic keywords
with their associated index which could
be for example latitude longitude index
or in the case of house map it's a
country code and somewhere else in in
the process there's also what I called
your parsing which is called your
parsing which is the extraction from the
text of the the geographically relevant
keywords and phrases
so finding into the sequence of the
sentences where are the location phrases
and so that that's what my system is
going to be about okay so how do humans
do Geo parsing when you see this
investigation into measles outbreak in
in Baliga district reveal the case is
originated from penance living in crap
Eden and sanitary conditions I don't
know any geography about Malaysia so
then I look it up but before when I
looked this sentence I'm still able to
find out that non guru and they like our
places but that penance is not a place
and that's because we rely mostly on the
on the context not only on the words we
don't rely on the keywords but in fact
in the context and the context in terms
of syntax theorem of capitalization
maybe also on global distribution since
often the location are foreign words so
they don't have the same vowel
distribution as the surrounding words
okay so that's something that we want to
exploit the party so automatic geo
parsing it's as I say an information
retrieval problem and recent approaches
to information sorry retrieval
information extraction a problem and
recent approaches to information
extraction problems such as named entity
recognition semantic role labeling have
have been to have so the statistical
approaches have been to rely on heavily
annotated data set to learn that what
we're extracting information and so
these data sets are expensive to obtain
and take long time and you probably
google those and have these kind of
problems when but for your a group of
three people you want to think of
something else before going into
labeling data sets and so what we have
is that health map is already working
and that we have a purposely crafted
gazetteer which is I'm going to show you
a little bit an example in a moment it's
really designed for the kind of tasks we
were trying to do to solve and also as
Clark said
we're mapping to 600 places but in fact
in the in the list we connect we we have
access to 7,000 location in fact because
it's much bigger okay so an example of
the health map gazetteer see here there
are country places country words
computer references city references but
also colloquialism such as aussie
announced that australia also some
really medical related terms of japan
bangladesh hospital is somewhere in
bangladesh and not in japan and cdc when
we see cdc by itself on an alert so
there is a big chance that that's
american related disease or interim is a
french related disease and in the same
time we also have negation like brazil
nut is not a place it's canada goose is
not in canada okay so that that's what
the the gazetteer looked like and so the
idea would like to do that we develop is
that we will use the gaza chair to to
label data set then augment augmented
the information by adding linguistic
information such as the part of speech
is it is this were the very business
were the subject is this word things
like that also highlight the the
capitalizations that status is this word
in capitalization not and if we stop
there and just use this that i said to
do training we will just learn by heart
the gaza cheer which is not what we want
want to be able to generalize to words
that our outside of the casa tude and so
for that we we play like hiding the
information like like you see in this
example i'm hiding
new caledonia but if you read only the
the second sentence you still know that
in this place they must be a location so
that's what we also give
to our learning algorithm okay so now
I'm going more into detail on how this
was implemented I hope I don't bore you
so first how we we integrate all the
different information that we add to our
words so we represent words as a big
sparse vector in which the first
components are for the lexical
information dictionary index the second
part is the it's reserved for the part
of speech index and the last part for
the capitalization status now how do we
implement the hide in the words for that
we in fact take advantage of the natural
distribution of location words into the
more general words and so what you see
in the in the graph is that if I if I
hide the words that appear less than
five times for example the first
paragraph that that's concerns five
percent of of the words but twenty
percent of the location references so if
I cut my dictionary removing all the
words that appear less than five times
in the in the data set I'm hiding twenty
percent of the location so that that's
the way we we managed to implement this
hiding of the lexical part and so we
label our data set using this these
things I show you before and we use it
at a set to train a neural network which
outputs the probability of being a
location or not given a word and a
window surrounded this word so a context
of several words and so I'm not so the
this architecture is in was inspired by
a language model presented by pendulum I
know and also more recent work by
collaboration force on a semantic role
labeling
and so yeah I may not go too much into
detail but there's two phases one which
is only mapping the words into a more
rich face more rich representation and
then using this new representation to
actually answer the question of whether
or not this is a location okay so now I
quickly go through the experiments so we
built a training set from the disease
outbreak alerts that were retrieved by
health snap in 2007 2050 our hundred
alerts which in fact it's one more than
1 million words to tag so it's not that
small and I've trained several neural
networks fixing the size of the context
and fixing the size of the
representation layer soon then some were
a lot of hyper parameters others and
tested several cutoff of the dictionary
so just to understand what's happening
there so in the validation what what the
results you're going to see it's on a
separated validation set so in a
training set you know are you trained at
the the neural net with knowing the
answer in the validation you don't know
the answer the answer is apart and you
look at what would be the answer of them
granite and match that the true answer
with the granite answer and so what you
see here the plain blue line is the
performance of the neural nets in terms
of f1 score which the only thing you
need to know is that the higher the
better
so when I hide so and the x-axis is I'm
hiding more and more location so I'm
making the problem more and more
difficult and so the the the
score decreases which is normal and
making the problem more complicated but
if I then decompose this error on
looking only at the words that were
hidden that so that the neural net
didn't had the lexical information
hidden nan guru was out of his
dictionary then that's the red line and
the more I trained that the the neural
net with hidden words the better I am to
discovering words that were out of my
dictionary that's the more I'm
generalizing while in the same time I'm
not losing too much performances in the
world I have access to which is
encouraging to to see that I'm able to
discover new words that were out of my
dictionary kind of given a hint that
that's a good idea and that's also to
show that the context is a it's worth
something
okay so when I increase the size of the
context I increase the size of them and
increase the performance so that's it
so I've have shown a method that
transferred prior knowledge encoded in
the rule-based approach into a
statistical learning approach this could
be generalized generalized to other
information extraction problems and have
shown that this methodology helps
recover discover Geographic references
based on the context only and there's a
bunch of other tests that must be done
but we're doing them now
John back return
great okay yeah so we just gonna quickly
wrap up just telling you about what the
future vision of health map is because
Nikko really outlined really an
impressive way that we're trying to
advance the technology or what we're
also trying to do is engage users and
really think about the public health
element as well so as in parallels we're
developing the technology we want to
think about how we engage users and
really the human network and that's been
has proven value in in other systems
like the promed system and really that
the the combination the adjunct of of
the automation plus the the
collaborative filtering approach really
is sort of the ultimate goal for the
system because there's only so much of
it that a technology can really do and
to have people on the feet in the field
validating and reporting information
that's sort of the way we see the vision
of this system going and so right now
really the reality of the system is that
health map is out there sucking as much
data as it can and bring it out there
and exposing it but we're not that we
don't have the linkages back into the
field and that's why we've really trying
to develop collaboration especially with
organizations like chrome ed and others
like tefa net and other organizations to
produce an automated feed and really
help them in terms of their work and
thinking about how they can they can
prove their job but also at the same
time bringing data back into the system
whether it's through new posts or
through filtering and editing of the
information that we're already
collecting in health map and this is
added value for a number of reasons one
being that we could just bring in new
content to that we can improve
timeliness of reporting so by bringing
in and bringing that information but
also helps us in our precise
classification annotation verification
and finally as as as Micah said we
really do need a training set to keep
improving our automated technology and
so building high quality trainings that
would also be the outcome of such an
approach so I'm gonna wrap up here I
know that we want to have some time for
questions but really I mean I think we
see some real value in bringing all this
data together and really an approach to
integrating and creating a real view of
the world that complements traditional
public health surveillance and the
multilingual collaborative environment
and really the idea behind health map is
to provide this customized real-time
intelligence view of ongoing outbreaks
for the broad scope of of all
international Public Health efforts as
well at the individual level so I'll
just think that the people from our
group and a bunch of the volunteers that
have actually helped us develop a lot of
language specific capabilities obviously
big thanks to our supporters at
google.org and Marc's mullinski Cory
Conrad Larry brilliant with without that
support we really wouldn't be where we
are right now so thank you so much and
we're happy to take questions Thanks
the category of on categorized disease I
think that was are not yet categorized
seems very interesting is that something
that you're planning to refine were use
you know for other organizations and so
I mean I yeah you want to go for it go
huh yeah yeah that's a good question so
basically as I said we have this
dictionary well we have 200 or so
disease categories and then the
dictionary of a few thousand text
patterns that map into these disease
categories basically when we caps um a
lot of times we'll capture a report that
refers to let's say very rare disease
one that we don't have in our list of
200 it's not at the dictionary we still
want to be able to put that up on the
map so we we just use this not yet
classified sort of catch-all category
the other way we use it is we actually
have a list of very rare diseases that
we just they don't occur often enough
really for it to be worth it for us to
put those into our list of 200 but
certainly as we develop the system we're
gonna you know be adding one or more
diseases and looking at the relationship
between disease types and symptoms and
definitely improving that I mean the
goal is to have as little of the not yet
classified you know as possible so what
about things like SARS which are if I if
I remember correctly when it was
discovered he wasn't Ryan Ryan exactly
exactly so we definitely need iagnosed
care well we definitely want to be able
to pick those up so that's part of the
motivation behind well you know ok the
algorithm didn't pick anything up but
that doesn't mean that this this alert
you know is no good we and as John
points out you also have a categorize
ease category called undiagnosed so a
lot of times we get reports you know
saying mystery illness you know in
Vietnam so that maps to to undiagnosed
so that's another
categories we can also take questions
from those who have vc10 so if anyone in
San Francisco has questions feel free to
unmute your mic and ask I'm actually
just kind of wondering what effect
Health map has had at any yet I'm kind
of the content producers like the
reporters that write the articles that
you kind of scan I mean it seems like
them knowing that you're kind of
searching this might be might provide
some sort of opportunity for
collaboration where there could've been
some sort of tag for you like analyze or
something of that sort
yeah that's that's a great point I mean
we've actually been linking up with some
disease-specific reporters around just
giving them feeds of information there's
a reporter by the name of Helen brands
well in Canada who's following the avian
influenza story and so she's getting
right now an automated feed of health
map data so that she can keep abreast of
what's going on and that can help her in
her content I mean the other aspect is
thinking about how we can collect better
information from the field and from
reporters and that when the idea of
really developing a true collaboration
with a news agency like Reuters C and
then thinking about how to then
potentially get more structured
information from those reports because
really we're trying to figure out you
know all these elements in in the news
report but if from the beginning a news
reporter put in had all these elements
he structured content then it would
really solve a lot of the problems that
were actually trying to overcome and so
that to us would be you know an ideal
situation as well and the idea that we
could do that for all news agencies is
probably unlikely but to partner up with
one of the big ones would be an ideal
situation
I don't know if there's anything over
that
great well thank you all for joining us
today and if you have any further
questions about health map or the work
of the predict and prevent initiative
through Google org please feel free to
reach out we look forward to bringing
more tech talks into continuing our work
with health map so thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>