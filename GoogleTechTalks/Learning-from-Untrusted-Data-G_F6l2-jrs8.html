<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning from Untrusted Data | Coder Coacher - Coaching Coders</title><meta content="Learning from Untrusted Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning from Untrusted Data</b></h2><h5 class="post__date">2017-05-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G_F6l2-jrs8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">those legs ambien from Stanford he
turned his PhD from Howard input socket
and Microsoft irreparable bogey and I
only had a bunch of girl neither what we
in want to be relationship sacrifice
we've done some great work in killing
algorithm police repellers how to make
sense of for Yelp review data even after
a bunch of malicious not enough emotions
which are given six things exam and the
word I'm most excited about is my Google
faculty insertion words thanks to
whoever did that happen and so I wanted
to come and and talk about the stuff in
part because it gets interesting but
also because maybe you guys test them
some other in a few points on the
question set that sit here and maybe
even some data that we could try out
these out with doing it so I'm feel free
to stop and ask ask lots of questions
especially since I think the slides are
going to be a check your I so I can talk
about a few things based on based on a
couple papers so papers are joint work
with different combinations of Jacob
who's probably graduating momentary 16
she stood Michaela McHale is a master
student and wrote this um one of the
papers that will mention that Nick's
last year and other the papers become
more of the heavy because it's the
coming out of that stock this year and
then there's actually two more papers
that are in submission and I won't
distinguish which results are how much
papers it'll be more just about what
what columns vary so this work is
motivated by the question of so to what
extent can you hope to develop
algorithms for estimation for learning
for optimization
that are robust to a significant
fraction of untrusted APIs so so the
setting that we also think about is you
have n data points alpha end of them for
some crown droughts are gone from the
distribution that you that you actually
care about and then we make no
assumptions on the remaining one minus
alpha and it works please so these
remaining data points maybe they're
biased or you can do noisy maybe they're
arbitrary or maybe they're can
maliciously compliant it has a function
of the alpha and good data point what
can you see in this regime and the goal
is going to be to kind of answer the
questions we like to answer I'm almost
as accurately as if we actually knew for
the alpha I'm going to do which is
general the family rule is dead right if
you actually knew which are the data
points that are drawn from your care
book and you're going to just run your
usual learning algorithms on these
enough that this is a set of year will
be this um okay I probably don't need to
belabor the motivation but here's person
would have able so then you remember
this tweets this is the Associated Press
tweet from in 2014 that's breaking two
explosions the White House Barack Obama
injured so this is a the state stress
tweets from 2014 and this is what the
the Dow Jones Industrial Average this is
what happened when the treatises was
made and presumably so what happens
right so intact the Twitter feed of
Associated Press it probably shorted it
went to socks it was the complete and
then they they offloaded their crusade
um I'm not sure what it's what the
timescale is I mean things were fixed
with him I pin it any notes most people
they see this and then check a few other
websites and they realize that they
realized what was happening I realized
that well you know you have lots of
automated algorithms sitting on news
feeds and Twitter and all this stuff
automated trading algorithms
trying to trade rapidly based on
developing stories they see this they
also created me they short a bunch of
stuff by the time people realize have
happened that people realize this is
state then the damage has been known and
then thank you very okay so I'm you
can't actually really make money this
way I mean so trading exchanges they
they roll back the cage that happened as
a result of this so just discourage the
sort of thing one reason to do that is
because of the loss of machine learning
things sitting on top of this is a
rollback I'd be able to make um so as I
learned relatively recently
ah when you sell stocks they like sell
and then there's three days later that
you get the money and in those two days
presumably they're doing something which
is check making money on it probably but
among other things they presumably want
to be able to to reverse okay I don't
dinner and I bought a house stuff last
year and we didn't realize that take so
long to get money out for the wrong man
so um this is just an illustration of uh
for example house even a tiny bit a
single account malicious data point can
cause haywire so um
anymore you know seriously looking
forward um there has August security
vulnerabilities of the future retire so
if you have more IOT devices the next
nuts up in the tourney self-driving cars
it's my understanding you guys maybe can
detect me maybe um you know on the
self-driving cars are based on data
that's being being backed by the for
example like they lost the Tesla's on
the river presumably they're giving back
data to to Tesla and you know you could
imagine if you
they first sight is Flemish or Tesla
stock I cover my garage with video
screens and you know speed that can
observe data that makes it seem like
every plastic bag that's closed in the
middle of the road is a big rock and now
maybe you know whatever learning
algorithm Tesla's music using will end
up breaking every time you get back in
the rotor book so you can easily imagine
if you have machine learning things
sitting on top of data that you don't
really know where it's coming from
this could be a big security
tolerability um they're they're they're
more kept mundane or system of things
but um let them have you in for you know
online community behavioral standards
Facebook Twitter certainly things that
they have been wanting a good way of
enforcing vehicle standards but
communities and the data will allow you
to have like a life or a flag button
well people will choose that well how
can you be really busted the sort of
stuff you parents who are see the
curation peer grading these are other of
application sites so some of the hoops
and experiment is that peer grading
where you shuffle around every term
homeworks students and you ask them to
great things and then you always have a
big coalition of 20% of the class that's
all from the same the whole block of
polluting entities all trying to save
their grade okay
even more broadly there's so often some
trade-off between the amount of data
that you can collect in the quality of
the data if you have some fixed budget
for question data you can either get an
enormous amount of garbage data or a
tiny amount of very good data that's
worth understanding of the value of the
value of cleanliness in your data okay
so and it's worth being able to
understand this without really needing a
model over what the garbage hated right
you'd like to conduct you don't want to
assume that the bad data is just random
noise Oh
so a bit of context of the service so
it's been a long history of work in the
robust statistics community looking at
exactly these sorts of questions what
happens when you have some days are gone
from a reasonable distribution and the
rest of the data maybe is arbitrary this
is a will most of the work from the
rustic it's extremely and some looked at
very specific metrics of what happens
for you when you have that sort of
settings so there's no east of the
breakdown point of estimators which is
basically how much the cutonium
estimator of some some property you care
about the great standpoint of that that
estimator is the fraction of bad agent
that you can tolerate such that if you
have more than that amount of that data
the estimate can become arbitrarily sir
so say that say ab a-- meets a mean of a
bunch of rice what's the breakdown point
of this all it takes is a single data
point that's absurd and you can make the
mean of a set of things as funny as
possible the very center point of the
mean is just basically zero all you need
is a tiny little bit of a single data
something like the median what's the
breakdown point with tax like because
yeah if you perturb more than half of
the data you can make the median absurd
if you perturb less than half of the
data no matter what you do to that pass
you can't make it arbitrarily absurd
this is no sin the breakdown point
there's been a lot of work studying you
know what estimator is a pretty great
down point right have bad a great down
point this also the notion of influence
which is how much influence in a single
data point have on estimators so this is
good work gold work um you know ideally
we would have a much much more more
accurate and nuanced and holistic
understanding of what happens as the
different creation them as the amount of
bad data in pieces okay so the step
makes misses a lot of the pick fixture
sorry okay so we'd like to understand
you as you increase the amount of bad
data information theoretically how
accurate for me hopes to and how do you
do this in combination officially
difficult for the learning theorists as
RS and there's this image named agnostic
learning where you're saying that we
make no assumptions about the data and
we just ask can you find the hypothesis
could use as good as possible for this
data so maybe you're learning in linear
separator you're not just making
assumptions that your data so nearly
there really separable but you're asking
what's the best linear separator for
this data this is from a very the
diagnostic learning is a very different
two point and many of the punchlines are
that agnostic learning becomes entity
part is so there's other nice nice
results of more it's part and I'm an
encore Moitra that so suppose I have
data and high dimensions that lies
they're all lies in some hyperlink
except 10 percent to the point don't
line that hyper a bunch of points on
some hyper planes ten percent of points
aren't on the hyperplane how hard is it
to find the type of thing that goes
through ninety percent to the points it
can be n to be hard okay so that's so
easy
that means absolute command determinant
that comes out of these exhausting
there's been some very recent progress
in kind of sending that group up I think
that we'll be talking about the 92
newspapers like ridiculous and friends
and lie around us and they both table
has thought about this problem of
robustly learning a Gaussian
distribution and high damage and
they're thinking about robustly learning
this distribution in a case where the
fraction of good data is more than 1/2
okay so 80% or 90% of the data comes
from the nice kallithea what can be done
when 10% of the data's arbitrate so I
wanted to illustrate how the intuition
for why in many cases you should be able
to do well when the majority of data can
then go on so say our goal is to learn
the di Parma this is very precise
example but they are comments that the
goods distribution is some sphere say a
radius 1 and your job is to learn the
center this year you've been to Lourdes
narro or yeah I mean this is not
available Gustin you know all those
results were going to yes so uniform
over a spherical disc your job is to
learn the middle and a third of the data
it could be arbitrary the other 2/3 are
talking so what can happen so in one
dimension this is the this little strip
is our one dimensional uniform
distribution so what can adversary do
well you know sometimes the worst thing
that I'm sorry I can do is can put all
its maps here now I have no idea
you know is the truth a spherical just
here here here here
I've no idea within two dimensions well
you know what is the adversary so this
is the truth this is that this is point
where the garbage data points lie so
this looks absurd but this was the case
you stable this is obvious the garbage
this is what we want you know gasser so
what time the worst thing that the
adversary can do well the worst thing is
sending something like this right it
just kind of moon chasing and now we
don't know where the center is in the
truth of the Nu and you have a three
dimensions is that all this
ah but in essence basically the
intuition for both the results the
previous results die conical is a lot
around Kampala um and it becomes a
situation in this sense if the adversary
all data points are have too weird
then you can identify them and remove
them and the intuition is that they
should improve has a higher dimension
pure provided under the sufficient care
of it oh nice so this is this is my
summary of the suppose of those two
fetus previous thinkers you know they're
big papers they do lots of stuff
this is intuition behind and those okay
so we'll come back to - why you care
about this other setting but consider
attending where the majority of the data
is actually untrusted this may only a
third of the data comes from the same
you care about it in some sense is this
coming obvious impossible right so say
this is a good thing and become
arbitrary or adversarial data from these
other three spheres there's obviously
nothing we can hope to it we can't tell
which is which and you know there's no
way we can always figure out the good
right so this is kind of its obvious
impossibility with dealing with the
majority of untested data which is one
reason why the statisticians go down the
stack okay so let's try this let's how
to handle this edge anyway yeah so um so
they're two notions so one is this time
list covering learning notion which
becomes variant of it was proposed by
ball pendulums on call uh no II I'm kind
of a clustering setting
think of it this way so so again we have
any other points out of fashion are
drawing the thing to care about no
assumptions of the remaining ones and
now suppose the goal is to return a list
of maybe one over alpha answers so in
this case you can set it to time to
return one answer maybe where the goal
is to return three
answers just at least women in the
spring and there's kind of tribute in
this case a quandary those are three
answers at least one of them okay so at
least conceptually if the only hurdle to
learning in a case where you have a
minority of good data is that you can't
decide among a few different options
that doesn't do if this isn't decision
me different way of phrasing this is is
it the case that the arbitrary visa can
really hide the structure in the good
stuff
but can it simply you know try to
confuse you by adding other copies of
things that kind of look like this
that's what this question I'm getting
like Delia okay so second setting which
we're proposing which is maybe more
relevant to UM the impasto headings so
again you have to think that up
additionally we're allowed to take K I
think it's a a tiny amount maybe
constant amount unverified data points
that are drawn from the distribution
think of this as we have our huge sturdy
data set to mixture maybe my minority of
it is good data and then we have our you
know highly paid workers who collect a
very small data set that you really
believe in now you're asking how can you
use your k a small amount of custody to
point to expect and rich people and at
least since you know this to the example
is trivial right all of the good data
points will be from losing and then the
data so we're going to do is become
conceptual questions how can this kafele
bill give a tiny amount of Trustee data
helps viewer accept information to a
larger um okay so i wanted to
different motivations for the setting
where a minority of the data distrusted
the business I kind of described as as a
data prism or database prism and think
of the following slide caricature so we
have very large data sets of customer
product evaluation you have M products
I think of em as thousands they each
customer evaluates some small number of
product so maybe customer evaluate five
products so you have this huge data sets
sitting somewhere and then suppose you
come back to the season you want to
understand what does the student
demographic actually like what is a
vector of average preferences for
students so again this is like we have
our Lorenz dataset we want to pull out
the student demographic so you can think
about this as you know you can map this
into the previous framework but you're
just saying okay students correspond to
the good data everyone else corresponds
the arbitrary data can we actually
extract the properties of the student
demographic and in this semi verified
model this correspond saying well
additionally what what you're going to
do you're going to take a big dataset
you're going to hire a very small number
of students pay them each to evaluate
some very small number of items use that
little verify data set to pull out the
student demographic so so here I wanted
to give a flavor of the results for this
site so so there's all simply if you can
higher yet constant number of students
each one will reads your constant number
of items this is our verify data then we
can use this together with the large
data sets mix between demographics to
pull out and address terms of the
student demographic preferences on all
of the items you know provided the
fraction of students in the original
data that isn't time and we actually get
a
so if everyone evaluate five products
we're okay as long as the vaccine
students is at least one over cousteau
hot which is which is pretty human
approval students around originally yes
yeah yeah so that's it and the constants
defenders in the fidelity of rest of my
choice yeah so uh right so the amount of
noise and the good stuff comes into this
but it's you still have this M naught
exponential thing on yourself so again
the resetting the motivation for this
instead some of maybe some of the
medical settings where like you have
tons of medical records and suppose you
figure out that Oh II know 5% of the
population has a certain mutation and
now you know yeah maybe you can find
that if 100 people who have a mutation
and look at their medical records but
there's no hope of getting or figuring
out which of these people in your
billion personality medical data set has
a mutation it's the questions how you
use your small amount to verify data to
pull out information about some small
cohort or some small fraction of the the
big datasets without actually having
those labels so yeah so if you had this
do these labels become okay are we okay
with the motivation to them and this
movement of students in the standard for
being able to sample beautiful minute
right and from whatever's on the light
of that pretty magical out that's yes so
right so okay so when you're formally
phases so in this case but informally
say the theorem will be assuming that
there is some vector preferences for
students I mean each student is going to
independently with some noise but never
have an unbiased being unbiased estimate
of this I mean that is a so this sort of
the students at bimodal or something
that's already the week I'm like you
know good save ourselves um I'm going
got a simple caption for your on your
binary setting or in a numerical setting
as long as each thing is unbiased to
reach for it yeah but that applies even
look at some we go through some accident
um yeah that's a good question okay okay
so going back to the general settings
okay you have M items any viewers each
reviewer evaluates our items think of
our small 510 some constant actions and
also factually give you noisy but
unbiased ratings and we make no
assumptions about remaining reviewers
and then after the fact we gets a
constant number K of K random reviews
for the demographic interest and use
again might be doing it but our unbiased
so the question is basically you know
how many items each person is a big
dataset need to review how many verified
trusted reviews do we need in order to
be able to get pretty accurate estimates
of all of the items so okay so on one
observation which is not intuitive so I
claim it R equals one so if everyone in
the big datasets evaluates a single item
then there's nothing you can do you
really do need to obtain verified data
for every so why is it the case well
because this business intuitively make
sense so what
we just know which item each reviewer
chose to say we randomly assign people
to read you I do okay you get and you
gather their value for each item it's
binary for babies belong to five scale
or and you know - substance by
definition leave it simple are giving
you on my senses because the mean my
definition is the average ratings for
that item among that demographic and the
bad people can you do an arbitrary so
sorry with everyone just evaluates one
item and there's really nothing you can
do you need to evaluate all of these
items yourself using this constant here
and if I said you know if everyone
evaluates two items can you do better
300 wait so we have to two heroes so um
the first thing is that it's okay to you
know with describe the review and why
they can do anything and each person
reviews some constant number of items
independent of the number of items
dependent only on the fraction of good
people and the cup error is excluding
care about then it's okay to have just a
constant number of verify data points so
you can do this so with each review one
item you can do anything if each person
viewing some constant number of data
points you can do it with just a small
accomplice size custody and it's okay to
have the number of people reviewing
things beyond the same order as the
number of item Street okay so this n is
the size of a good a to sub a so this is
extremely sparse Pacific perhaps
Mendelian mmm so you can Mendel Missoula
by because if you have more more people
reading items with desperate to find I
um yes yes I so it's uh it's okay even
if you just have in a suitably large
consolation data okay
yes well it content needs me state and
then second theorem ISM that you know if
you really care about having each person
the big dataset evaluate very few items
that is okay to have the number of items
to the number of items each person the
big days that reviews the action
logarithmic in this fraction of good
people as long as you have lots and lots
of data points in the original dataset
so if you have credible and squaring
people reviewing end products it's okay
if every distribution problems and
you've done you'll be able to accessible
so these are kind of three different
bookends
so one is the regime where the total
number of items is comparable to the
total size of your big data set and the
other regime is kind of where you have
many more people evaluating things size
are very creative and in both cases it
okay if every values council number
things and you needed constant number of
bareheaded exclusive em to the arisen
and lambreaux survives at the Nemesis
are today yeah maybe I'll start by
defining this by describe you can look
first there and then maybe we um okay so
um but think about our Jeff imagines a
matrix of customer reviews
it's an incandescent like that's like
the Yelp so each person is rated a few
items on a scale from one to five each
column represents an item each row
represents the evaluations of some
companies so each row represents a
customer each column represents item and
this is a matrix of ratings so ignore
this for sure okay so again please
that the good reviewers are
independently in a noisy fashion but
independently giving her unbiased
estimate series product then what
happens well if you were to look at the
sub-matrix of it consisting of the good
reviewers again you don't know which
where they are but imagine someone told
you which the good rumors are I think
that this sub matrix would be it would
be very smart to be very noisy
participate sickly kylo Ren why because
in some sense each of these rows is a
very first very noisy representation of
the same underlying set of crime
so each of the good rosette are knowing
these parts the copies of the same
underlining rope and this is basically a
noisy sparse red one and then the
question is to what extent can be
arbitrary extra roads that are
interspersed destroyed it would be far
Spokane quickly realized suppose if you
have this nice it's nice part of the
matrix that kind of the coherent your
the other stuff that trying to hide the
sparks to what extent can hide
the answer is own to some extent you
can't right so there is a nice sort of
range one is component what can the rest
of the matrix two while it can make
other nice ring fun components you can
say Oh below
cheers and I've ever advice rate from
one component here is another nice rate
rank one computed this corresponds to
kind of you know maybe there are
different sets of preferences for
slogging few different form components
then at the end of the day you'll use
your verify data to figure out which is
which is the one you care about it might
be slightly surprising that this is all
the adversary can do you need to be a
little careful about how to actually
pull this pull this back out
so how do you actually do this um
okay so you'll follow this convex
program all kind of the fabric what is
it so you're going to basically say come
up with estimate of the underlying
probabilities for this matrix to
actually fill in this matrix with all of
them with with ratings so as to do the
following the way you fill in the matrix
it should be as similar to the actual
matrix of of ratings as possible so you
want to come fill these things in
without changing much but subject to
subject it's a found on the nuclear
normally except you come up do you not
me so intuitively what would Mike you
want because you might want to say find
me a low-rank approximation to the
station that's a little too sensitive to
web another garbage or CIL's so instead
we're basically going to say yeah come
up with some matrix that's similar to
this but has smallest people your norm
and also you need to make sure that no
row of this new matrix to come up with
instead of too powerful why do you yeah
why do you mean nope they're not um so
if you in serve it so what is it what
are the other options
so other options are you give you a cash
flow be Gnaeus or turned on so between
once we can desperate so on for I mean
between zero one thank you some
reference to total yes I think you'd
like to do the euro and this is M not
not an option
I'm it's thought obviously the projects
my and can do the new heroic M doing as
opposed to focusing on viciously
feasible so
I think lien or maybe this is Atiya some
of the singular values can use instance
even tighter bound months yeah all right
anything have anyone can buy
construction with every right although
yeah I mean yeah you want to trade off
kind of the discrepancy with the
original thing and how much power it has
here I think using as yeah longitude I
think it works this vendor studies
actually a group yes yeah a few other
things here were a convenience of so the
good Revere's have a low types of matrix
because you say assuming that they all
have similar valuations for the
different is about nine one six
what is the money more so is it as long
as it is bounded and each of the good
people stay unbiased independent or
right to the biggest I how much noise
are you I thought I mean as long as
found is say here it's bounded between
one and five on your sing this with it
as long as in my library and again so so
it's not actually low ranked race if you
have a random they have a single row
probabilities and have a matrix where
each row I pick a constant number of
entries and then flip point that those
probabilities and have a random sparse
matrix this would actually be you know
that low rate it will be where you can't
you can't reconstruct the original
probabilities using naive floating
approximation although once you can
prune out everything and so on and you
can Jensen
so this is like random graphs with
constant degree spectrally has some
issues pretty conviction for getting
between like logarithmic sparsity and
complex
you can do it a little hard that's
wrapped into so yeah if you weren't more
careful I think you might need
logarithmic to the one spear a little
bit I tend to fun to talk about this at
a high level but at the highest level
yeah we're going to come up with a
surrogate for each rope which is similar
to the Rose been subject to kind of
having Lois shrink and making sure that
nothing that were not acceptable to any
one data frame coming out too far away
and that's all ability in it we did
leave this me so also on other reviewers
aren't allowed to pollute no no they are
silver and it's guess if they are
allowed to code and the point is how
would they pollute the only thing they
can do is they can form their own lower
range subspace over there so that's fine
you'll take our constant number of
verified thing and figure out which of
these alluring subspaces were actually
and oh and if they try to collude to
have kind of blurring thing that's what
we're showing up cat hat so I think you
should is that if you were to actually
get by with it so rank then one of the
components will be D correct
constantly never yeah so you have a
biggish lowest rank robust component at
worst you could have a few of these
constant number than one over alpha well
then you can figure that out so far is
you can you're done okay so it's
important family with your more unknown
list which one over epsilon squared
because R itself is only one of x16
grandpa are you federal because it blows
up well the nearest addition I mean
we're the design of matrix Y that you're
coming up great which isn't going to
have any sparsity registry coming up
with the Dennis matrix we're saving
though yes you read it I'll item you're
getting everything that you would have
read it
we're trying to do this to the nuclear
or unlike nah
we're trading off it did you notice norm
with subject ably a smallish norm you're
trying to have it as similar to rating
right but if absolutely no you're near
the pencil please want to reference for
those I suppose though you're just using
one what this is like whether the
bounded yeah
so X is 1/3
okay oh I think weather yeah no I mean
what did it do I mean you just thought
warrior for I think it's been me we
started off and what they want to read
one two four and then maybe we did it
the third getting it up to a squared a
chart value well that's like one of our
support efforts so much please after you
do this you end up with the matrix of
how to get to the veteran training and
the guarantee will be that all of the
roads corresponding to the good pupil
are actually close to number like the
long time dying truce and then you know
you can use the verified readers or
assist recovery model you can example if
your things very clear them how many
people here know whether it's good
robust pca stuff that and this and those
in code is yeah so that's a slightly
different setting there you have a low
rank matrix with sparse noise you're
trying to determine the orientation this
is a little different sometimes we have
a some things you have a low rate matrix
but then it's not fart noise that you
have a smaller for a loss of
Bruce returned completely arbitrary
sometimes doesn't have strength a more
general and the techniques book is
similar but a little a little more if
you had a confidence rating on
interviewer instead of being in salt XTX
sort of you are already different that
because I think most likely do suppose
you have some reviewers who they know
yeah
I'm noticing your fascinator so
differently I mean right so I think you
would end up either maybe you read
agreement more for those ones but then
you can also allow those rooms to be a
little more powerful I'm not sure how
the socials are then but what can you
use that to further on the bad roads um
so again there's this distinction that
we're not necessarily trying to remove
the batter's because maybe the divers
aren't bad maybe their end of it just
like that maybe the food demographic is
definitely getting around okay so if you
at the end of the date maybe you don't
know who's going to who's bad if you get
handsome Knight to the good okay um so
right so imagine keeping the same
framework we're trying to do much more
vicious learning and optimization so
I'll say something if it make sense
creation if it doesn't everyone this is
a kind of thinking of getting ratings
from many people imagine be observe n
convex function of which somehow a
fraction are drawn from some
distribution over comic culture be stars
that's the ideal discreet automaton make
no assumptions about the remaining one
myselfi on the function and the goal is
to find some w that minimizes the
expectation of a function on from the
thing you care about of f of dollars
we've sampled a bunch of convex
functions we've we're trying to find a
point that kind of minimizes the
expected the thing that is actually and
again maybe we also have some verify
function that we know of in your honor
so I think that this is actually a
straight generalization of a lot of the
problems that we've but well of the
problem is thought about okay so you can
think about SFI is kind of the loss
function corresponding to some data
point that you stopped so in the case of
readings
that's the previous example yes we're
given these data points at one direct
end but you can interpret one of the
data points as corresponding to this
loss function so that I see the point
SFI of some value W is the square
distance to W and X of I what point
minimizes so kind of went amazing what
point minimizes this loss function over
the data set you add reject I right
so this log function just that idea so
we have a bunch of random points and we
want to find I mean that's the same as
you each random point as a sample of
this loss function the distance apart
comment function and now the problem of
you know minimizing that finding
doubling that minimizes expected loss
function is the same as finding the
average mass which is the same as like
aggregating the drink famous time you
would need the mean radiant vector okay
but we could put different convex
function filter so instead of time to
find it mean we could also ask about
doing in a linear regression or other
kinds of optimization that's not ready
until you kind of clock the ability to
put a generative model in the previous
setting you were able to secrete we were
able to get around the your bounds by
assuming that he knows that are good and
actually coming from of excess tribution
we saw
and the question is what are the
properties that we need about the
distribution P star distribution over
genomic function that led us to respect
and sorry so informally the thing we
need is that kind of the matrix
ingredients of the of the function genre
class and have nice concentration in
meaning that they're not kind of it's
not like one of these functions if no go
over here and other than no go over here
but they're all right let me actually
strip so the way to prove results for
this is basically the natural intuitive
extension of the way secures in this
other set and then we have our end
function and we're basically going to
get grid points double use for each of
these functions so we want one w which
is good for all the good ones
considering each point will get its own
W and we're going to come trade off the
objective function value for each of
these things so we want w1 to be good
for the first function f1 we won't tell
you to be good for the second function
but subject to these double use most of
them kind of agreeing with each other
okay so how how do you capture this
theorem agree with each other well good
morning at requires at least L views
they all sort of uh are contained in
like a smallish ellipsoid who looks sort
of shape with a smallest trace we're
just thinking about this is yes so we we
have all of these different convex
function we want to find one solution
that's good for most of them something
for most of them but that's good for the
good one one coming really good data and
it says you can't have a answer for each
of them subject to them not seeing them
but not being thinking deliver them
but it so I just going back
okay let's see some fun is going to be
something okay okay so I love all this
so what's what's the punchline so even
if this kind of more general
optimization regression settings we can
get some strong positive results in yeah
you can do regression these things even
if you have a very small fraction of
working small concepts of good data um
how good are the guarantees good not
great so we have this very general
result for any specific setting you care
about see you only care about regression
is probably much sharper sort of result
you could get especially if you really
found your assumptions about the
distribution of data so think of this as
more thing you should be able to do
computation efficiently you should be
able to do something good for whatever
learning kind of you care about the
actual results you get is probably more
of a theoretical they're interesting
thing rather than some of that you
actually would good rocks maybe I'm not
going to describe this stronger result
of how to get fewer ratings and a deal
with tiny fashion this inverse
exponential amount of media that you
amount of the fastest students really
really small I won't tell you how to do
that it said more common toriel
algorithm um okay consider this is a I'm
sorry
so uh um women general especially like
disco where you're at the computer
science we need chemistry machine
learning there are a few different paths
you can take so one of them is become
earth-based analysis which usually end
up being how to pessimistic in this idea
you can't do anything for everything is
to NP hard and this account
kostik learning setting in many cases to
just computation anything very hard I
different often means you can kind of
start pursuing maybe average case your
distributional assumptions they're
capable if knowing is it's random and
this is the super moves like this and
you can climb to get your wildest dreams
in terms of results um and then in some
cases your prizes to practice and you
get in the garbage and in many cases
these things are very very brittle to
the you know to the assumption to me and
this one is step on this every bus that
we verifies is a learning um you know
one poop is that it gives strong
positive results computation and
information theoretically and it seems
to require relatively weak assumptions
that yet to making assumptions on the
good stuff do not make any assumptions
on the bad portion of the data and you
know it remains to be seen whether these
things will translate well mix practice
but I'm pretty optimistic in part
because the sorts of algorithms you get
out of this seem like they're kind very
robust things that sounds no it's not
right okay maybe I'll enter this is the
small ones will be just right
how does a medium of this effect that's
a kiss ball oh so you're trying to
evaluate I know some real data yeah so
um we had some we had some really
terrific
oh yes all the hilde that we have it has
all been not like don't start up like oh
hey agus we could do this I'm at
someone's they're like no no actually
you can say anything about the data so
it this hard time people willing to
share some say propofol
or yes it's about the other thing it
would be good to have a setting where
you actually have a true underlying the
duty to understand the underlying truth
and this is also yeah you can take reels
which academically up publicly Red Cross
um yeah so again so okay so so I know
the G can stay down with this a little
bit so yeah so please you think about
crossing things this does tackle and it
does a good job when they're it tackles
the kind of stranger malicious behavior
of you know your yells at the restaurant
across the street keeps giving you a bad
rating louis seat open service yes
because it wasn't no no yourself kids
kids began to would break help picking
up a long time and hitting some crazy
things you know say trigger is that I
know yeah top the Michael Byrne painted
a lot of crowd sourcing stuff and some
of the popular how do you accurate gate
get a bunch of crap Sorcerer's who are
raised to our time to label your dataset
and you know half of them just don't
click together and have them have five
other aliases together but anyway yeah
so so I'm not sure what the real how
much you're the real killer allocation
of this work but it's for trying to get
algorithms that you really want frugal
your Excuse work if you have a car
self-driving car you really want to
ensure that it's not going to increase
think you get arbitrate a later the much
of the Fiat stations are security facing
versus if you're cleaning it is be like
a simple God is also kinda picky because
they're somehow players are actually
really important and you want it against
it right is that
and we have any idea that like the
product you get a sense or something at
country close it better on some
opponents center sometimes okay yeah let
me know if you get</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>