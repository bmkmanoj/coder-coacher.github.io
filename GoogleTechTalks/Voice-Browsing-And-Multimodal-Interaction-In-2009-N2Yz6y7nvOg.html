<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Voice Browsing And Multimodal Interaction In 2009 | Coder Coacher - Coaching Coders</title><meta content="Voice Browsing And Multimodal Interaction In 2009 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Voice Browsing And Multimodal Interaction In 2009</b></h2><h5 class="post__date">2009-03-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/N2Yz6y7nvOg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for coming everyone I like to
introduce Paula bharya father from the
window and I've worked for a number of
years with paola envoy standards the
especially when I was active in the
voice browser working group work at the
w3c father specializes in pts but also
knows a lot about ASR and multimodal the
faces and all that good stuff um he's
focuses in his own work on the
pronunciation Mexican work at the in the
voice browser working group as well as
SML I let him this is actually a
continuation of the talk that he started
when he gave when he visited us last
year where he sort of give part 1 of the
stock and so this is part 2 of the talk
and for those of you who missed part one
its fill up on youtube so take it away
Paula thank you very much ramen yes this
is a second part we already gave the
first last year that was mainly
concerned about the speech technology
this time we will enter more in the
context of standards I will try to give
you the picture of what happened in the
last year's and what's going on today I
will mainly talk of two standard two
groups one is the voice browser working
group and the other one the multimodal
interaction both of them are part of the
w3c only two words on lock Wendell
Oquendo is a technology producer company
that create engines for ASR TTA speaker
verification in multiple languages we
have today 26 languages and 62 voices we
are based in Italy in Torino in the
northwest part of Italy but we have few
offices around the world one also in the
US our technology are recognized to be
very good first of all the tds but also
the ASR and all the other one are
catching up okay now we start
the two main standard body that I'm
following are the w3c and the ITF both
of them have groups interested in the
exploitation of speech technology of
multimodal there are also industrial
form the major one is the voice 6ml
forum this is the arrow of time that I
showed last year that shows that
everything started at the change of the
century at that time there was people
that were thinking why not we change the
way we use the speech technology and the
voicexml forum was the one to do the
proposal of the voicexml 1.0 standard
and that was the start but in the
following years including now we are
completed the war that was started there
so in the 2004 voicexml 2.0 and grandmas
and proms then updates in 2007 and last
year my word that is the pronunciation
lexicon the pls and few weeks ago M that
is the first release of multimodal
interaction working group so if this was
the picture that Jim Larson draw in two
thousand let's show an abstract
architecture of what there is inside the
iris system you should see there is the
SR understanding interpretation dialogue
then planning the response and the TTS
this is how is the picture today the
large majority of the specification are
now w3c recommendation that means you
can rely on them the red blocks are
module that you can completely control
using the standard I expect by the end
of this year 2009 that also the one is
the call control will became a
recommendation so the whole picture is
full of standard so we have reached the
goal of completing these first release
of standards so what does it mean that
means that before the voice browsing you
had to rely on everything proprietary
each technology vendor gave you their
own API their own way of encoding
grammar or prompts and also the
development of the speech application
was completely proprietary with the
advent of voice 6 ml of voice browsing
what is happening is that like in the
web you split the things into part all
the voice application all the content is
in a web application like the web today
the platform becomes a general-purpose
voicexml platform that takes download by
HTTP all the content and interpret in
order to realize the voice application
so this is the value of those standards
Oh from proprietary to standard platform
from proprietary development of
application to the reuse of what the web
has today in order to create the voice
application and the SR and tts and so on
so this is the story now we will do a
tour we will enter a little bit on the
standard just to give you few ins on
them with impossible to go in detail on
all of them because otherwise you will
take edges but I will try to give you an
idea we will touch ASR TTS lexicon voice
examine and also the call control the
first is a sr the ASR today is based on
law knowledge you need to give knowledge
or what need to be recognized this is
usually done in normal application
writing a grammar a context-free grammar
so what the w3c did was to have two
couples standard one is speech
recognition grammar syntax and the other
one is semantic interpretation for
speech recognition one take the syntax
the other one the semantics as the
syntax is concerned you have two
different way of encoding the grammar
but the interesting thing is that they
are capable the same one is called a BF
is a textual form another one is XML so
you can do the thing as you like and you
can do grammar for voice and also for
the old IVR DTMF application the SASR is
how to build a result so you recognize
words but you need to create a meaning
you need to create a data representation
of the result of the recognition this is
the SASR that is pleated in a literal
and in the script let's take this very
very simple grammar with only a single
war that is Tori know the place where
acquaint is based as you see there are
the XML on the left column and da BNF
you see the a BF is a little bit more
compact all the rule management is more
compact on that side while the other one
is XML with a container grammar and then
other elements inside the red part is
the semantics on the top row is the
literal semantics that is one-to-one
mapping I recognized during high return
code is doing for simple things the
bottom part you have the wool atma
script so you can do everything you want
you can have function and so on so it's
very very powerful way of returning
results
so just to summarize you have a powerful
sin taxes formalist that can cover both
voice and EMF and it was widely adopted
all the ASR engine today support these
two standards 12-2 syntax is good
because if you need to embed the grammar
in a voice excel page you will use the
xml format if you want to do very
complex grammar to save typing is good
to organize them in a more concise way
but you can transform a grammar between
the format and the other one without
losing nothing also this makes open the
road to have tools but unfortunately I
don't see too many on the web today that
helps to create the grammar to validate
them maybe to test them on Brittany
input to test the semantics so there
would be being that is a standard the
option to have some tools that are
independent of the technology vendor
only a few little issues you need to
declare the semantics that you are using
in the grammar so if you declare it fine
the point is if you not declare that the
spec is not clear so that might be
dependency on the technology you use
about the script semantics you need to
take care to pop up the result in a
grammar you cray you create almost ruled
another embedded rule and so on you
create your own language if you choose
the script you need to be careful that
the correct semantics result pops up if
you use the literal while the literal is
very poor is only useful to do a single
rule or one more rule with one to one
mapping like war list you have list of
city you want to transform the end in
zip code and so on is used only to do
those simple thing so using a surge ESN
sighs are you can create many grammar
linked together like what pages I would
say with fewer uri so usually the
literal will be the leaf and the script
will be connecting the result produced
by the other garments in this is our
specification there is also description
how to cleanly transform the result of
your grammar in XML and XML is becoming
more and more popular as a result of the
grandma also because I'm a standard we
will return is an XML encoding you can
also do complex fing like return arrays
and other night namespaces and so on if
you are interested you can read that
section so as I said the dist under were
immediately adopted there is only one
thing missing that the pls was finished
last year so there is not a normative
statement inside Sergius and also one
feature that is used for disambiguation
is not doable with the assessor GS today
so we will need a tweak on acer GS or as
a GS 1.1 also the w3c booshit to adopt
XML 1.1 and IRI instead of uri so there
is no major missing from the grammar
side so obviously I didn't said before
you are free to interrupt me in any
moment to us clarification if something
is not clear or I was too fast or
something was missing now with speech of
synthesis those are the steps that as
TTS is doing take a text try to
understand the structure even if it is
an email or an SMS you need to
understand better how is done then try
to normalize because one is what you
bright and other one is what you say if
you write a date in a certain way
a then you would need to speak it in a
different way after that you need to map
the text you have normalized into funny
because for speaking you need to use the
phoneme that are the base of the spoken
language then you can have higher level
thing like to give emphasis to add
pauses and so on at the end you produce
the sound that you will listen the SML
cover all those steps with XML elements
that add the author of the prompt to
embellish them to make them to sound
better for instance the speak element is
the container with a tribute of
namespace and so on but you can also
specify the language and in that we are
using what the people did on language
tags then you can structure the document
with pns paragraph in sentence you say
yes if you know what you are talking
about if it is a date or to add tweaks
for speaking better using phonemes or
substituting text but you can also
change voices using different criteria
emphasized put brakes and also try to
change the prosody that means to rise
the volume increase or reduce the rate
change the pitch and so on all those
things that are not related to the war
but are in general for the wood sentence
you can also add audio and these issues
if you want to mix pre-recorded or music
with the TTS so TTS is recommendation
but then after doing that people ask new
things the most important one was
internationalization people from China
especially of Japan Far East said that
there were thing that they could not do
so W preceded free worshop the first in
Beijing then
another one in Europe and the last one
in India to try to collect input from
all over the world about what we have to
do and we discovered that also for
Semitic languages for instance for
Indian language would have been useful
to know a better what was the
transliteration schemin use or lived
were vowel or not and other things so
this was the Moines the main driver to
continue the work on SS a man another
driver was to better fit the SS ml
inside voice browsing for instant to
have a good better error handling having
attribute to clip or to do other things
on audio and a person an expert on
engineering of audio said that the scale
that we had in SS ml that was linear was
not at all good for voice because the
voice is logarithmic in nature so you
need to have a logarithmic scale for
modifying the volume otherwise you will
modify only on a tiny bit of the scale
and you will do nothing on dress so we
change the semantics of the volume a
tribute oops I jump to the last line I
apologize
so the SS ml 1.1 is something more that
was possible to that to do in that
directions okay this is the right one a
new element was introduced for heavy
market on wards especially for the
languages that do not have were boundary
so they were missing an element for
changing what they want to change that
for instance for Chinese was the tone or
other features also that the lexicon was
more developed and the idea is that
being that the SML is a standard you
should use standard also for giving the
pronunciation that is not the reality
today all the Phoenician know in a
language that is IPA that is the
phonetic alphabet reduced by the
International phonetic association so at
the moment the w3c's mandating IPA but
is in the same time creating a register
for registering new phonetic
pronunciation for instance pinon is
highly requested for Chinese because
it's the most one known and used there
and also other for Japanese and so on so
in parallel there is this activity ok
this is TTS now pronunciation this is
something that I'm the editor of the
specification is something I know a
little better so you create an
application then you discover that the
people do not pronounce the word in the
way you expected or you are
mispronouncing something so you won't
change the pronunciation to do that is
important to factorize everything in a
single document this is a lexicon is a
set of change of pronunciation then the
important thing is perhaps to share the
lexicon between ASR entity s if you are
working in the same domain
you do the things one re use for the two
technology this is pls is a single
document language with lexicon and a set
of lexie the lexeme are the one that
changed the pronunciation with a part
that is the graphene for the written
spelling and another part that is the
element for name or alias for doing
phonetic transcription or subtextual
substitution today the pls is modeling
well this is not a big problem because
Baathists mln s RGS can include multiple
lexicons so you can include multiple
lexicons if you are doing multi lingual
things so this is an example you see
there is lexicon there are two lexeme
one for a name with the pronunciation of
the name this is a writer and the other
one is you have an acronym and you want
to expand the acronym into words this is
how you will use it in TTS you will add
the lexicon element and then the lexicon
will include this is an example of a
movie in Italian definite easy action
for the Italian of those words it the
same you can do in a grammar you can put
the lexicon element and then to have
this lexicon things inside so the
lexical specification covers many areas
like multiple orthography yo mofo nooo
McGrath I didn't enter it in the detail
but you can easily find them also in
Wikipedia or in a couple of presentation
I did the last one was this week in San
Diego for The Voice Search conference
there are still open issues on pls the
first is the most important one is the
engine takes time to support the
standards so at the moment they are
still implementing but I think there
will be short to release them there
is the point of the registry to have the
other phonetic alphabet operational and
then to complete a Sergius to allow the
best use of pls i skip this and I enter
on the voicexml the other important
point was the voicexml language itself
is a rich language the clarity one for
defining menus or forms to be filled by
voice so they cover input by voice or
DTMF output by voice TTS or pre-recorded
speech variables and so on events
telephony stuff and many other things
one big point is that the voicexml how
it was the finest synchronous so you
have not many ways of interrupting it of
a ve to send events the execution is
always in a single point called a
dialogue either form or a menu all the
menu arcuate and then when a transition
in a point when you can accept the input
it will start to speed speak and collect
the the input from the user Daenerys
definition of events and so on in
voicexml 2.1 new feature were added for
doing things one is to do recording for
instance of ruled colored all the
recognition is was not possible another
one is at marker and the third one
feature is to have an XML data exchange
when the voicexml page is loaded then
there are other thing more related to
the telephony so the point is a drop
deck as I said is that everything is
synchronous and in order to change
content you need to change the voicexml
page that had an additional cost with
the 2.1 feature
can load the page and then fed data
inside the page and so you can change
the application on the fly and this
makes more smart voice application the
data element is the one for assessing
external data is a synchronous one again
element and then the for each is for
looping for iterating on the data you
have received either to real a berating
it or normally to put them inside the
prompt these are details on the data
element but you can find them in the
specification and for each element so
the voicexml and the other standard I
presented you allow you to do standard
voice application today and is widely
adopted all the platform now today I
lost voice exam any input another point
is the call control also the call
control that means to accept call being
deceived or telephony called can be
managed using a standard language
without having to do with proprietary
things this is the reason for the c6 ml
called control examine language you can
do many things as accept call do
scripting and variable and so on sending
events starting avoid 6mm dialogue
transferring the call doing our bound
call or doing conferences those are the
elements I will give you a few example
of the 6 ml war because it's less known
than the voicexml everything is based on
transition you have transition with an
event so when you call for instance you
do a sip invite to see 6 ml system it
will rise a connection alerting an event
from then you can program whatever you
want each event contained the data
that you can access in the body of the
transition when you elaborate that for
instance when you receive an incoming
call so you receive a connecting
alerting you can do some criteria on the
number that you received or internal
perhaps that your application irani well
and then you will accept or reject so
you can implement your acceptance
criteria very easily in 6m n other
simple thing is when you receive an
alerting you try to prepare a voice 6ml
dialogue if the preparation if is fine
you will accept a call and when the
skull is connected you just say start
and the voicexml we start immediately to
talk to the person other thing you can
do is transfer the call obviously when
the dial want or when your logic want
you can transfer the call and then
restart continue the voicexml at the end
of the transfer call you have many
different kind of codes of transfer call
you can also do think that in the past
were not possible so the c6 ml can
receive the HTTP HTTP event basic and
inject them in the running 6 ml section
and also send events to external
entities so you can control what is
going on inside the c6 ml in an easy way
using HTTP wipsy 6mm you can inject an
event that asked to make a call when the
call is done you can prepare that
dialogue and so you can do outbound call
it very easy easy way when dialogue is
terminated you receive an event and you
can decide if you want to start another
dialogue or disconnect the call because
everything is finished so is a flexible
mechanism
or doing call control this is about
fetching document is less important so
just to give you an idea this is a 6 ml
page very simple the SIS XML has an even
processor that is the transition each
transition as an event and other
attribute that we will not enter in the
detail so with this simple 6ml script
your septic all you start the dialogue
when the dialogue is finish or the user
in gaps you close everything so the c6
emily is to program things they disperse
much more complex for doing conferencing
and other interesting scenario today is
the not standardized as i said before is
a last call working draft i expect it to
be very soon published as a candidate
that means there is a test suite to show
the implement ability but being that low
Quentin many other company already
implemented it I pinned the transition
to recommendation would be very fast so
aspect in the course of this year this
summer or this fall that also the c6 ml
becames aw Felicity recommendation this
means that you see the picture
everything can be standard based I think
this is the value of the war the work
done so far in the voice browsing was
working group so if this is a voice XML
platform if you look inside is more
something like this that is our own
platform you have the network today
mainly voice over IP that called the
platform which activate the c6 ml first
and then the voice exam l if there is
the need of voice technology and then
there is another part that is Anna mercy
pserver MRCP means media lose control
protocol there are two version of MRCP
the second one version 2
is under the way in ietf and we'd be
richer than the previous one because we
link would also speaker verification and
other advances just has been done is
important to know that MRCP v2 is
completely sip based sip and RTP based
so what does it mean MRCP means that you
can have a platform and plug technology
from different vendor in a standard way
or you can create your client your your
fancy web application that interface a
standard server that contain is our
entity s so it's another advance this
time inside the application to make this
ploy tation speak technology simple Dave
bark that is your colleague because he
move it in Google in UK is the author of
the first book on MRCP if you want to
know MRCP i suggest you to read his book
so this is what there is in a call
center today there are different way if
there are seat base they go directly to
the platform it there are no sea bass
today the best way is to have a gateway
a voice gateway that involve a voice
browser a voice platform and the
platform then using HTTP will be
connected with the operator daytime so
on to run the application but the
interesting thing is that also inside
the telcos the voice over IP is taking
place so they are pinking to a
convergent network where IP and
traditional mobile or fixed telephony
are merging together in this new
architecture that is called IMS there is
a place for the voicexml the voice
browsing platform and not these in this
new network when you call the signal
egos first to an application server
so everything we drive by an application
server that will contain the logic that
he need to apply and then will involve
for instance voice platform for doing
voice things so is changing a little bit
parodying using the power of the voice
over IP okay this was the first half
about the voice browsing so we have done
a tour of water is today and the status
of the things that we have today now we
can jump in another interesting area
that is multimodal so we voicexml you do
mano model application that means speech
on the application but with the reason
asst of device that there are around to
be able to use more than one modality to
voice but also touch and gesture and so
on might be interesting so we need
multimodal but multimodal means many
many different modalities that can be
audio but also Tosh also the
accelerometer that is in the iphone i
don't know if it is in your phone or not
so there are new device that are coming
in the keyboard and so on the speech and
the visual are good friends because they
are very very complimentary the speech
is transient so you need to listen
otherwise is lost is linear you need to
listen everything the visual is
persistent you can look at that if do as
I other thing you look is always there
but the species useful when you have at
the hands but for instance you are
driving all your jogging you're doing as
other thing you can use the speech while
and the visual need that you use the
eyes and the speech software noise so in
a noisy environment is not working well
but the visuals of our lives so if you
are on a beach perhaps you can
see very well so to have different
modality helps to use the best modality
for the given condition being that a
social condition or an environmental
condition or to do the thing in the way
you better like so if we started with
DUI and GUI graphical user interface
then the boys get the view I now we are
on the verge of a new acronym that has
my or are we call in boyshorts movie
that is the multimodal user interface so
is a new way of doing the user interface
that integrates multiple modalities I
will give you a little bit an overview
of the complexity that there is behind
so you have many mortality and you need
to control them so it becomes complex to
do the interaction manager that is the
art of the multimodal application so the
w3c multimodal interaction working group
did these define a unified
representation for all the modality the
input modality this is M extensible
multimodal a notation so with my uses to
provide things because you have a
uniform way of all those different
engine that are talking with your
controller your interaction manager emma
is an xml language with a container the
interpretation is a semantic result from
one modality you can have one off for a
belt to having invest to having
alternative and so on you have different
elements but the interesting point is
the resistance of the annotation you
have a lot of information that you can
put on em this is an example if you have
that
GUI to do a flight
enquiry you can say i want to go from
boston to denver on march 11 what you
will do is that you can generate the
same enma either you speak or you use
your GUI interface so you simplify the
application but also if you have the way
of doing writing recognition you can
create the same em that will have the
annotation which is the modality that
produce at the input also biometric be
that on picture or own voice can create
the semantic result in EM so all
different modalities can generate em
result for speech recognition if the end
based are popular to have a Valtor
natives you can also have a graph of
alternative that is much much more
powerful this is called lettuce and in
Emma you can represent the ladies so
emma is the way of representing things
in the multimodal and also voice word so
what we did is to have what was drawn in
this picture that all the modality can
produce Emma but also the engine can use
a new semaphore instant to have a log of
what happened to compose the input of
different modality and do other complex
ping so this is accomplished because I'm
is a recommendation and the people can
start to work on top of that another
standards for writing this is called in
camel is a data form for presenting
digital ink being that pen or stylus or
other different device and allow the
processing of and brighten gesture to
sketch and so on and that is possible
today in tablet pc in PDA and so on so
this can
became a standard language widely used
in income L you have tracy's but also
much more richer a notation that can be
produced by a device not that if you
classify if you are able to interpret
the end writing then you can generate m
so in camel and Emma can be composed one
inside the other this is still not stand
there is last call work in rat but the
people are working to create an
implementation reports so it can became
a recommendation in one year or not much
more and r is i would say an increase in
interest by even big player to adopt
this standard instead of doing their
proprietary things for digital ink and
another important part of the multimodal
is the multimodal architecture in this
case what the multimodal interaction
working group is doing is to creating a
standard for the architecture the base
is to a Valentine framework this the
basic infrastructure where you will have
module inside runtime framing the
interaction model will control the
interaction then you will plug-in
multimodality components that will
exchange life cycle with the interaction
manager to exchange data together so
everything will be event-based this kind
of architecture is a largely coupled
architecture that is done also for
dealing with distributing way of doing
application there are today already
prototypes done by company one who is
done by Deutsche Telekom the person in
the w3c what they did was to use s 6 ml
we will briefly enter in that later as
the language to do the interaction
management and then they tested gy in
HTML and GUI invoice examine so what
they did was to do a HTTP yo processor
for exchanging using Ajax way of
programming inside Oghma scripted data
with the modality and in one case the
modality was a voice plat one with c 6
ml and by six men so there is still a
lot to do is not close to became a
recommendation there is work on creating
profiles or now to start app and
register the modality component in the
architect or with transport and many
other things so is this something that
will take a little bit longer to be
accomplished another another new area
that is emerging is related emotions
that is the Wikipedia definition of
emotion so the idea is for automatic
things to be able to register a motion
capture from voice or the behavior the
face on the other side to convey emotion
to move from the plane TTS or a face
that is not able to express a motion to
be able to control them talk of emotion
is a bit complex today there are many
theory this is one of researcher
professor of the University of Geneva is
not simple to define what means happy
angry and sad so
he proposed it to have two different
dimension and could to combine the value
of these two dimensions so from a
research point of view there is still
work to be done on debt to find the best
representation of this emotion in Europe
for in sunder was a European Network SLS
then now is an association so if you are
interested you can go to the URI there
and you find all the researcher there
are exchanging work and do in progress
there so from a recognition perspective
to deal with the emotion means to be
able to classify today there are tens or
more different classifier two most used
our gaussian mixture model and support
vector machine they are the two one more
pro my promising to be able to capture
the I variability of the emotional state
of the person that is speaking on the
other side for instant for the DTS that
we are talking about before one-way
simple would be well the TTS is based on
recorded voice so I record the voice in
many different emotional states this way
is not scalable might work but make the
voices to become larger and larger a new
interesting way is to say okay i'm using
a neutral style voice and then using
signal processing techniques i try to
modify the voice to make it sound more
joyful or more sad or doubt and to
express something of higher level so the
second one is what all the research is
concentrated in this moment i give you a
couple of example of the neutral voice
that changes to sad from sad one case to
epi this is a male voice speaking I look
forward to doing it again soon
I look forward to doing it again soon so
you see changing the intonation tins in
a different way let's try with a female
voice let's see the next one let's see
the next one you see it became more
colorful so there is a lot of work
behind that so I mean the technology
needed to find the best way of enable
the user to use the emotion also in the
face in the research on the face there
is a lot of work in order to express in
a fashion way with gesture the emotion
in order to have embodied agents that
show more emotions also in this area the
people started to create an incubator
and make a first proposal in w3c and in
the next multimodal charter there is a
chance to have this work to be there so
what was produced in this incubator is
first idea of the language in XML
language for expressing the emotion
today is still more research sheesh fing
with all the option that the researcher
want to use obviously being the star
dermis became more useful for final user
for the industry so there will be a
discussion there so this is an example
in a prompt to have the new language
with the prompt to express the emotional
and the intensity that you want to
change the emotion and the same thing
may emerge from the ASR encoded in Emma
that is the way of representing the
result of the ASA could be reached with
the emotional state of the person
speaking using this language not that to
have a standard language also as the
research because a lot a lot our
notation if you have a commonly agreed
and notation way
will speed up the research to exchange
data to improve the model that they are
producing today so also this work is
part of the multimodal interaction
working group so we covered what was the
speech browser what is doing the
multimodal and now we are reaching the
end that is what the voice browser is
doing today he's doing many two things
one is a language for controlling the
interaction that is a 6 ml and a
necessarily is a voice examine that will
be voicexml 3.0 i will give you only few
ins on this area so we will be fast but
this is the work in execution these days
inside the voice browser stay charts
David Harrell defined the mathematics is
a mathematician of state charts to make
them very very efficient there is a book
unfortunately is out of train but I'm
almost sure that is inside your library
by David Ayer and station where for
instance adopted by the UML for
programming interaction what the voice
browser is doing is to have an xml
version of the state chart that is more
simple to be used in a web-based word
the language is done of states is a
state chart and transition with the
transition that are drawn by events and
the events can have condition and so on
then you can have a stage are inside
another one and the stage' are very
powerful to be implemented in a very
efficient way so in principle they will
run everywhere also in device is not
storage and powerful you can also have
parallel stage are working at the same
time that they can be synchronized one
we
the other so there is an interest in
both voice browser by dark also the
multimodal for the parallel things to
control in parallel different modality
by other w3c groups that are interesting
to have an interaction language that
encode stay char and then University
industries and so on and there are
already available to free they are
increasing open-source implementation if
people won't start to use the state
charts this is a an example of the
parallel you have in the topmost part of
you I in the bottom on the jewel GUI so
you can implement them and then
synchronize them for instance in a
multimodal application so the idea is to
have that dsi XML is the language for
controlling the interaction then this
language will call and control the
different modality being voice visual
acélar ometer gesture and writing and
so on in the case of voice only there is
still the use case because what you want
to do is to separate what is the
application logic from the dialogue so
the 6 ml can be the top part that
control the interaction on a more higher
level then we will involve both the
voice only like routine for doing the
little piece of dialogue so this is a
way of using the state's art for voice
application only there are still things
to do the main is to decide on a data
model being the eggman script or not or
open to all the possible data models and
another big work is on profiling these
two have different profiles because the
states are will be done to run on mobile
devices perhaps or more powerful one
more scalable so to a proper files will
help
to use the stage are in the proper way
so the last point is invoice 6ml 3.1 the
driver to have another voice 6ml are the
one listed there one is to be
well-founded usually in the language you
describe the syntax of the language not
semantics so the voicexml 3.0 will
describe the sumantti so what would
there is behind the kind curtains 12 to
have a more flexible and extensible
language then you can easily plug in new
modules being that speaker verification
that is not there rich media also simple
thing that is kind of VCR control to go
back fast-forward and so on on media for
instance and abs to be motorized to
awesome boy 6ml create profile someone
perhaps for rich media might be standard
on the media but not so standing on
other voice feature that are not
interesting another way of making voice
examine more flexible is in the art of
voice 6ml that is the form
interpretation i got'em the one that
control how you move from one dialogue
to the other one in principle that
should be pluggable or customizable back
by the user so a 6 ml will take part of
the logic part and the voice 6ml of the
voice interaction but in a better way
well done in different contexts and the
first work in rat have been published in
december the second will be published in
few weeks so is there is at the
beginning the people can read and
contribute on this world because all the
work done in the w3c can be published we
publish and receive comment from the
public
so this is the end of the tour of the
standard of the void browser and
multimodal if there are questions and
here otherwise we can close thank you
yes
well the things are rapidly changing I
was a few days ago in voice search
conference the advent of rich device
asked for multi modality so that will be
the place is not the browser per se on
your pc that has not been the driver for
the multi modality but to have a small
device that can be a very powerful
driver today there are many different
voice search one is your voice search
and to have voice as an input but also a
device for doing and best selection for
selecting different hypotheses in a
different modality is becoming possible
and useful so i personally think that
this is the time that the multi modality
will move from the laboratory in the
real application and if it is done there
obviously can also be done in a larger
browser in in a different context so i
think multimodality stead many years
there but i'm sure that in the next one
or two year we will see the thing change
from that point of view so are the
device that are asking for multi
modality that give you good use case to
use the multi modality to complement the
modality to speak on the car instead of
touching writing and so on so i mean
they are not there but there they might
be there in the next year or the next
two years
ramen do you have any flavor
consideration from your point of view
you were part of the of the game yeah
the point is also how you will do multi
modality because I'm talking from the
w3c point of view then there are other
different one today is very popular to
do your own application so you do go in
the google app you download in then I a
post or you download your application so
you do all of them writing code only
this is one way is very popular today
but certainly the way of using a browser
we're on the device you have a browser
and the application are using more the
web being that there is pass more
content eva team or I speed connection
also inside the device i think that is
another way of doing the multimodal
think that from my point of view could
be more scalable and if not you to
download or all the things separately
than to a million of things not
interoperable but try to integrate them
in a certain way that I think is another
way today we don't see that but that
might happen like it happened on the web
okay thank you very much for your
attention
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>