<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>CrowdInside: Automatic Construction of Indoor Floorplans | Coder Coacher - Coaching Coders</title><meta content="CrowdInside: Automatic Construction of Indoor Floorplans - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>CrowdInside: Automatic Construction of Indoor Floorplans</b></h2><h5 class="post__date">2012-11-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JuaVAHDxQm0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">
MALE SPEAKER: Please welcome
Dr. Moustafa Youssef.
I know Moustafa from my
last year in college.
He taught me in my
senior year.
And it was an honor, actually.
And he came here, he got
his PhD from Maryland.
He worked as an associate there
in the University of
Maryland for a few years.
And then he decided to go
back to Egypt to help
the students there.
And since then, he's established
three wireless
teams in the different
universities.
And this is his current
university.
And since then, papers have
been flowing to the top
conferences, like MobiSys
and MobiCom.
So it's actually a great
pleasure to have Dr. Moustafa
Youssef here today.

[APPLAUSE]
MOUSTAFA YOUSSEF: Thank you
for attending the talk.
I'm Moustafa Youssef from
Egypt-Japan University of
Science and Technology.
And today I'm going to share
with you our work on the
CrowdInside Project, where
we target the automatic
construction of indoor
floor plans.
So CrowdInside is a part of a
larger project which we call
the Ubiquitous Indoor
Localization.
So before I go through the
details of CrowdInside, let me
give you a brief introduction
to the Ubiquitous Indoor
Localization project.
So as we know, GPS is considered
a ubiquitous
outdoor localization system.
It works virtually everywhere
around the world.
However, there is no equivalent
technology for
indoor localization.
There's no technologies that I
can turn on my cellphone and I
get my location in any building
around the world.
And any building is
the key word here.
So if you think about it, we
believe that there are two
reasons for such technologies
not to exist till now.
First one is that there is
no technology for indoor
positioning.
Some of us may think that Wi-Fi
is ubiquitous, it's
everywhere.
However, in order to get
reasonable accuracy with
Wi-Fi, you need to do
calibration or training of the
site of interest.
So it is not the case that you
turn on your phone and your
location is there without
using any overhead or
calibration.
The second reason is that there
is no worldwide indoor
floor plan database.
The knowledge I always I give is
that if you're outdoors and
you are getting your location
using GPS, if I give you the
latitude and longitude
coordinates, it's a couple of
numbers that makes no sense.
Without a street map, these
coordinates are not
useful for the user.
So for indoors, if you don't
have floor plans for every
building around the world, even
if you get your position
through some technology,
it's useless.
I need a floor plan to display
this position on.
So why are these floor
plans not available?
Maybe they are not available,
for example,
in developing countries.
Or even in developing countries
in the US and
Europe, they are available, but
no one is willing to take
the effort to upload them.
So as far as I know,
[INAUDIBLE]
Google the maps for
indoors have been
released early this year.
And in order to make the
technology work, you need to
upload the map manually and you
need to do training, at
least till now.
So not to mention that if you
need to keep this floor plan
updated, someone has to take
this effort [INAUDIBLE].
There are also some
privacy concerns.
So the target of CrowdInside
is specifically this second
point, how to automatically
construct the floor plans for
virtually any building
around the world.
And the approach we take here
is to leverage cellphones.
Cellphones are with us almost
24/7, and they come equipped
with an array of sensors.
They have initial sensors like
accelerometer, compass,
gyroscope, and they have
cameras, microphones, GPS
capabilities, you name it.
So they are sensor-rich.
You can think of them as
ubiquitous sensors.
So if I can leverage these
cellphones that are available
with the users who use a
building, either permanent
users, like in Greece, for
example, in Google, or
visitors like myself are using
this building and we're having
our cellphones with us, if I
can use these cellphones to
get traces of motion inside the
building, if I collect all
these traces from different
users, I can get an idea of
the overall floor plan shape.
So the goal of CrowdInside
is getting these
traces in the top.
I want to construct the overall
floor plan shape on
the left, and I want
to identify
the points of interest--
for example, like elevators
or stairs and escalators.
And finally, I want to get the
room shapes and numbers, like
the figure on the right,
corridors.
So in the rest of my talk, I'll
start by talking about
UPTIME, which is another system
we developed as part of
the ubiquitous indoor
localization project, which
targets constructing or
achieving accurate trace
generation.
How can you achieve
accurate traces of
users inside a building?
And based on this, CrowdInside
leverages these accurate
traces to generate
automatically
the floor plan process.

So the goal of our time
is to get accurate
and ubiquitous tracking.
By ubiquitous, I mean it works,
again, virtually in any
building around the world
without requiring any
infrastructure.
And the approach we do is to
leverage the cellphones.
Again, sensors, especially
the initial sensors, like
accelerometer and gyroscope,
because they are available in
almost any smartphone, and
they don't require any
infrastructure for
calibration.
We use a dead reckoning-based
approach.
The challenge, of course,
is that they're noisy.
The sensors on the phone
are cheap, and
hence are very noisy.
And there are different
phone orientations.
A phone can be in your hand, in
your pocket, in your bag,
you name it.
So we need to have an approach
that handles all these
different orientations and
placements of the phone.
Finally, we need to handle
different humans
and different devices.
So different devices can have
different characteristics.
Different humans can have also
different behaviors.
So how can we capture this in
an ubiquitous and accurate
localization system based
on dead reckoning?
So theoretically, if you have
the acceleration form the
accelerometer, you can integrate
it twice to get the
displacements of movements.
However, since the sensors are
very noisy, double integration
leads to a huge amount
of error that grows
cubically with time.
So previous work has reported
that the error can exceed 100
meters after 30 meters
of movement.
So if you move just 30 meters,
your error can be as large as
100 meters.
And in our own experiments, the
figure in the bottom shows
that, after 14 seconds of
operation, the error can reach
up to 20 or 25 meters, which
is significant, cannot be
reasonable for indoor
localization, where you need
to talk about a couple of meter
accuracy at maximum.
So previous work in the area of
dead reckoning, of course,
we are not the first
to do that.
What they did is that they
used special initial
measurements units that
are highly accurate.
However, if I want to leverage
the cellphones as a virtual
sensing device, sensors in
the phone are cheap,
and hence are noisier.
So we need to handle
this noise.
They always use the step
detection technique.
So instead of integrating the
acceleration, what you do is
you try to detect the number of
steps of the user, and then
count these number of steps
and multiply them by the
average step size to
get the actual
displacements of the users.
For this, they assume that there
is a constant step size,
and current techniques usually
use sensors or accelerometers
or sensors that are mounted on
foot or different bodies of
the human person.
In contrast, what we want to do
is to use the cellphone in
an arbitrary orientation.
It doesn't have a fixed
orientation on the body.
We need to use it in any
arbitrary orientation.
So UPTIME, to highlight what
we are trying to do, uses a
step detection approach.
We need to detect the different
user steps.
Based on standard cellphones,
that can have arbitrary
orientation placements.
And also we need to detect
the user gaits.

The step of a user moving
slowly is different from
someone who is running.
So we cannot just use an
average step length to
estimate the displacement.
Otherwise we'll get
a large error.
So in the rest of this part of
UPTIME system, I'll talk about
the step detection module, and
the stride length estimation
module, and then we'll provide
some evaluation results.
So UPTIME has two
main modules.
As we mentioned, the step
detection module, that takes
its accelerometer reading and
tries to identify the
boundaries of each step.
Then we move to the Stride
Length Estimation Module,
where it's based on
classification
of three main classes--
moving, jogging, and running--
based on some features that
I'll elaborate on later.
Let's start by the Step
Detection Module.

So the figures, that one, what
you're seeing here is the
acceleration pattern you get
when a user is moving.
The figure on the left is when
the phone is in hand, and the
figure on the right is when
the phone is in pocket.
And the y-axis is the magnitude
of acceleration.
And the point of using the
magnitude of acceleration is
that, as you can see, the
pattern is similar.
It's noisier when the phone is
in pocket, but it's basically
like a periodic signal
of going up and down
as the user is moving.
So if we base our detection and
the feature selections as
a magnitude of acceleration,
we are to some extent
independent of the phone
orientation.
So this is the way we
handle the different
orientations in the phone.
And as I mentioned, when
the phone is in
buckets, it's noisier.
So the second thing, given
this figure, I want to
separate the different steps
in order to count the
different steps and to
get the displacement.
And for this, we use a simple
finite state machine.
I'll go into the details.
Basically, it tries to
detect the [? most ?]
of negative peaks.
And it has some states
for handling the
noise in the signal.
So let's take it in action.
So this figure shows a
typical user step.
We'll start by status 0, which
represents the stationary
state, where a user
is stationary.
And actually, if a human is
moving, you will find that
between each step, there's some
point where the user is
completely stationary,
when he's moving or
she's moving her foot.
So we start with this state.
And if the acceleration
magnitude goes above a certain
threshold, we enter status 1,
which corresponds to possibly
moving state.
The moving state is confirmed
when a user reaches another
threshold which is [INAUDIBLE]
peak detection.
Similarly, you have to go to
status 3 and [INAUDIBLE]
5 to detect that a negative
peak has been reached.
And finally, the state is
confirmed when the magnitude
of acceleration goes below
another threshold, which is
[INAUDIBLE].
So basically, using this simple
finite state machine,
you can detect the boundaries
of a step accurately.

While we do that, we need to
estimate the stride lengths of
the movement.
Is the user moving slowly,
jogging, or running in order
to get the actual displacement
within this step?
And for this, we use--
before I go into how we
do it, let's see what
if I don't do it.
So this table shows, for
100 steps, the error in
estimating the steps.
So if the user is walking,
which is a reference, the
error in estimating the
step is almost 0%.
However, if the user is running
and I'm using a fixed
step corresponding to walking,
the error can be up to 130%.
So it's significant.
So what we are trying to do is
to identify which class of
these different gaits the user
is doing, and then use the
corresponding step.
This is performed for each
user independently.
So each one of us will have a
different stride length as
compared to others.
We estimate this
for each user.
And maybe you can touch later
how to estimate it for a
particular user.
So what we are doing is we use
a standard support vector
machine classifier to
differentiate between these
three classes-- again, walking,
jogging, and running.
And it's hierarchical
in the first level.
We separate the slow gait
from the fast gait.
And the second level,
we separate the
three different classes.
And of course, the main thing
in any classifier is the
features, what features you use
to differentiate between
the different classes.
So we have a number
of features.
For example, the duration
of the step.
Remember that we separated the
steps using the first module.
So based on this separated step,
we take features like
the duration of the step, the
variance of acceleration.
The higher the variance,
it means that the
user is moving faster.
The peak of the acceleration.
Again, the faster the user
movement, the higher the peak
of acceleration, along with
three other features.

OK, next.

So here, I'll show you how when
we perform using this
technique for acceleration
of the trace generation.
These figure flows compare our
technique, which is a finite
state machine, in blue to two of
the current techniques for
displacement, or dead
reckoning techniques
[INAUDIBLE] crossing, which
basically or simply counts as
the number of times the signal
crosses 0, which is, if you
think about it, similar to
detecting the [? most ?]
negative peaks in a
simpler manner.
And the local variance technique
depends on just on
the variance [INAUDIBLE]
estimate the displacements
of the step.
You can see for different step
sizes, our technique in blue
significantly outperforms the
other two techniques.
And the [INAUDIBLE]
if you want to quantify it,
this figure has the three
different techniques
for two different
placements of the phone.
You can see that our technique
has less than 9.3% error in
both cases, and on average
has less than 6% error in
detecting the number of steps.
The other technique has
significantly more error.
And the reason for that, when
they were designed, they were
designed for sensors attached to
the foot, or different body
of the person, not for different
orientations.
So that's why we can achieve
much better accuracy.
This table shows the
classification, the confusion
metrics for the three
different classes.
And you can see that our
accuracy is more than 97% in
detecting the actual
gait of the user,
which is very accurate.
We apply this in a real
environment where we moved
around our campus.
So this experiment at trace
lengths is about 600 meters.
And for over the entire trace,
the error was less than 7%
over all in outdoors.
Similarly for indoors, we had
another experiment where the
error was less than 6%.
Of course for the [INAUDIBLE]
sensors, it doesn't make a
difference whether you are
indoors or outdoors.
It's mainly the trace lengths
that makes a difference.
To summarize this part, UPTIME
combines a finite state
machine approach to detect
the number of steps.
And for this, we can achieve
less than 6% error in
detecting the number of steps,
and then uses a support vector
machine classifier to detect
the step lengths.
And for this, we can achieve
more than 97% accuracy.
Overall, combining the error in
the step detecting and the
step length, we can achieve
less than 7% error.
Even though this is good and
much better than state of the
art, the error was still linear
with displacement.
So the more you move, the
absolute error is
7% of what you moved.
So of course, as you move away
from your starting point,
initial point, the error at
some point may become
prohibitive for indoor
localization.
So what CrowdInside does is
that it takes UPTIME and
builds on it to even enhance the
trace generation further,
as I'll show in my next
part of the talk.
So let's change gears and
move to CrowdInside.

So CrowdInside has four main
modules, the Data Collection
Module that collects the data
from the user, and then the
Motion Traces Generator that is
based on UPTIME, but it's
extended further to enhance the
accuracy more, and then
the Anchor Extraction Module
that the main function is to
detect the point of interest
inside the environment, like
elevators and stairs
and escalators.
And finally, the core of the
system is the Floorplan
Estimation Module.
So let's talk each one of these
modules one by one.
So CrowdInside collects three
main types of information.
The first one is an initial
sensor information, the
accelerometer, gyroscope,
and the compass.
And also, it collects Wi-Fi
information that we'll use
later to separate the
different rooms.
And finally, it collects GPS
information to detect the
transition from outdoors to
indoors, and detects the
building entrance location.
However, as we know, GPS is a
very energy-hungry device, so
we run it as a very
low-duty cycle in
order to conserve energy.
Once we have the data from the
user, CrowdInside uses UPTIME,
the technique we just described,
using the finite
state machine and the stride
lengths estimation to estimate
the user's step.
However, as we mentioned, the
accuracy is linear with time.
So in order to enhance this
further, CrowdInside combines
the trace generation with the
anchor-based estimation to get
more accurate estimation.
So this makes us move to the
Anchor Extraction Module.
The basic idea is that if you
can detect that you are at an
anchor point or a point of
interest, you can use this
anchor point to reset
your error.
So for example, the blue curve
gives you the accuracy of
UPTIME or the error in UPTIME
as you move away from your
starting point.
So initially, you start at
a known position, so
your error is 0.
However, as you move
away in time, the
error increases linearly.
If you detect that I am at
a point of interest-- for
example, if I am in
the elevator--
you can reset your error to your
position to the position
of the elevator, and
instantaneously your error
drops to 0.
And this is what we show
in the purple curve.
So initially, the error was
increasing, but about 100
seconds, you detected that you
are at an anchor point, so you
reset your position to
this anchor point.
And then your error increases
linearly again until you hit
another access point
and so on.
Two things to note here.
First, when you hit an anchor
point, it is not just your
correct your current position,
because you can go backwards
in time and do interpolation to
correct your entire trace.
So it not only corrects your
current position, but corrects
your entire history
of movement.
So if you want to do
something offline--
in real-time, of course,
this is not helpful--
but if you are doing offline
analysis, that can be
extremely helpful, especially in
our technique, because all
of the analysis in CrowdInside
can be done in offline.
The second thing here is that
the point of interest doesn't
have to have a physical
meaning.
So as I mentioned, it
can be escalators,
elevators, or stairs.
But in general, for example,
in our work, we use turns.
A turn itself can be a point of
interest, or a unique point
in the environment.
So whenever you make a turn,
if you use map matching
techniques, you can now
reset your error
to the correct location.
We have another piece of work
I'm not showing here that we
published in MobiSys this year,
where we tried to learn
the anchor points using
unsupervised learning
techniques, where you want to
determine unique points in the
environment without knowing
what they refer to.
So we don't need to understand
what they mean.
What you need to know is that
this is a unique signature in
the environment.
When this increases, we found
that, in indoor environments,
you have a lot of these anchor
points or points of interest
that can be used to reset
your error and
gain significant accuracy.
As I will show you later in the
evaluation, our accuracy
using these error-resetting
techniques based on the anchor
points can reach less than three
meters in the indoor
environments, which is very
good for our own purposes.
So for the next couple of
slides, I'll show you the
different kinds of anchors
we extract and how
we extracted them.
Basically, you have two kinds
of anchor points--
initial-based, based on the
initial sensors, and GPS-based
anchor points.
So let's start by the
initial-based anchor points.
Here we want to differentiate
between three main classes--
escalators, stairs, and the
elevators, in addition to
being stationary and walking.
So we have five different
classes to differentiate from,
just based on the initial
sensors and other
sensors in the phone.
So let's see how we can do it.
Let's start by the elevator.
The elevator has a very clean
and repeatable pattern.
Whenever you want to get to
the elevator, you have a
period of silence where
you are waiting for
the elevator to arrive.
And then when you start moving,
if you're moving down,
you have a period of weight loss
when it starts moving,
and then you have a period of
constant velocity, or zero
acceleration.
And finally, when it stops
again, you have an opposite
period of weight gain.
Again, this pattern is unique
and repeatable and very clean,
so we can use a simple finite
state machine like the one on
the left, not only to detect
the [INAUDIBLE]
elevator, but to determine the
direction of going up or down.
In addition, you can use the
time spent in the elevator to
determine how many floors
you traveled
inside using the elevator.
So by this, we can separate
the elevator.
Once we do that, we want to
differentiate between the
other four different classes.
For example, for the escalator,
what is unique
about the escalator is that
when you are using the
escalator, it has a
constant speed.
So this can be detected using
the accelerometer.
However, if you are stationary,
you are also
having a constant
speed of zero.
So in order to differentiate
between being stationary and
using the elevator, we use
another feature, which is the
variance of the magnetic
sensor.
And the main idea here is that
when you're using the
escalator, there is a large
motor that is running this
escalator, and this motor
affects your magnetic field,
and you can capture this by the
variance of the magnetic
field, as shown the right.
So the blue curve is being the
variance of the magnetic field
when you are stationary.
And the red curve is the
variance of the magnetic field
when you are using
the escalator.
So we can use this feature to
separate the escalator from
being stationary.
The last initial anchor
point I'll talk
about, using the stairs.
And the problem with using the
stairs is how to differentiate
between using the stairs
and walking.
You can think that, if
I'm using the stairs,
I'm moving in 3D.
And actually, this
is the main idea.
But the feature we used that
we find that can separate
these two classes of walking and
stairs is the correlation
between the zed- and
y-magnitude of the
acceleration, where y is the
acceleration, [? x ?] is the
direction of motion, and z is
the direction perpendicular to
the plane of the phone if
the phone is in hand.
If it's not, you can use the
gyroscope and compass to
reorient it.
And again, the basic idea here,
if you are moving in 2D,
you are not using the stairs.
You are [INAUDIBLE]
the displacement in
the y-direction.
[? y, ?] there is no positive
direction in the z-direction,
therefore the correlation
is almost 0.
However, if you are using the
stairs, you are having both
displacement in the y-direction
and the
z-direction, and hence the
correlation is much higher
than the other case.
So based on the correlation
between z- and y-acceleration
magnitudes, you can detect or
separate the two classes of
walking and the stairs.
So this decision tree shows
how we can differentiate
between the five different
classes.
Elevator is separated at the
top using its unique
[? pattern. ?]
And then the four different
classes are separated based on
the accelerometer and magnetic
sensor, as I explained in my
previous slides.
So let's move to the other type
of the anchor points,
which is GPS-based
anchor points.
So we can use the GPS to detect
the building entrance.
And the idea is if you're
running your GPS continuously,
the point you detect that
there's a loss of the GPS
signal, you know that you
moved from outdoors to
indoors, and then you can
determine the door position.
Of course, as I mentioned,
[INAUDIBLE]
is the energy consumption.
You cannot run your
GPS continuously.
So what we do is that we run the
GPS at a low-duty cycle.
And if you do this, you detect
the loss of the GPS signal
after some time, so your
estimate of the door position
would be core signalings.
So whenever we detect the loss
of the GPS signal, we estimate
the door position at the
midpoint of the last reported
GPS and the time for the
position where you lost the
GPS signal inside
the building.
So it's represented
by a blue across.
And if you repeat this multiple
times, [? theorem ?]
of large numbers tells us that
you will get, on average, the
location of your average
old traces.
You will the position of
the door accurately.
And I'll quantify this later
in the evaluation section.
So to summarize, the Anchor
Extraction Module is useful
for two things--
first, to reset the error.
So whenever I hit an anchor
point, I can rest the error in
the trace to the current
position of the anchor points,
and hence I can have better
trace generation.
And the anchor themselves are
useful to enhance the
semantics of the map.
I can have a higher level of
semantics on top of the map
using these anchors.
So let's move to the next
module, which is the core of
the system, the Floorplan
Estimation Module.
Once I have accurate traces, how
can I use these traces to
estimate the overall floor plan
shape on the left, and to
determine the room boundaries
and the corridor
shapes on the right?

Of course, there are details in
the paper, just to go over
all the basic ideas.
So we start by the traces.
What we are showing here on
this figure is 300 traces
collected from four
different users.
Then we get a point cloud, where
each point represents a
user's step.
So for each step from these
traces, we're represented by a
single point.
And then we apply alpha
shape to capture the
shape of the building.
Alpha shape is a
generalization of a convex hull.
And you can see that the grey
area represents the overall
floor plan shape accurately.
I have a demo at the end.
I'll show you this in action.

So in order to get the room
shapes, just not to stop at
the overall layout, what we do
is that we segment the traces.
So we start by the trace itself
and segment it into
parts based on the turns.
So when there is a significant
change in direction, we break
this trace into a new segment.
Then we classify each segment
as either in a
corridor or a room.
So we need to separate the
corridor out so that the
remaining segments would
represent the different rooms.
And for this, again, we use
different features, like the
average time spent per step.
The main intuition here is that
when you are moving in a
corridor, usually the user moves
faster than when he is
inside a room.
So these different features
try to capture these
differences between being
in a corridor or a room.
So if you have more time per
step, it means you're moving
faster, so most probably
you are in a corridor.
The segment plans, again,
a longer segment.
It's expected to be
more in a corridor
rather than in a room.
And the neighborhood segment
density, a corridor, again, is
used by more people than the
people using a room.
So mostly you will have more
density of traces inside a
corridor than as compared
to a room.
And you can see in this figure,
the blue c represents
the segments classified as
corridors, and the black
segments are those representing
segments inside rooms.
So you can see that we can
accurately detect or separate
the corridors from rooms.
Once the rooms are separated,
what we do is that we try to
cluster these rooms.
And for clustering, we use two
features, the spatial distance--
and of course, this is not
enough, because nearby rooms
cannot be separated
by just distance.
So the other feature we
use is the Wi-Fi we
collected from the user.
And the idea here, again, is
that two adjacent rooms,
because of the walls separating
them, will have
different signatures.
So the Wi-Fi signals can be
used to separate different
adjacent rooms.
Once we do this, we have
clusters, each cluster
representing a room.
And then we recursively apply
the alpha shape to individual
customers to get the
shape of the room.
You can see that we can
accurately, to some extent,
capture the shape of the room.
We can, of course, do smoothing
to enhance the shape
of the room, rectangular
shape, to
have a better display.
However, we believe that many
rooms may not follow the
rectangular shape, so it can
be based on the user
preference.
So what remains is a small
detail, which is detecting the
room doors.
If I can detect the rooms,
detect the corridors, can I
detect also the location of
the door of the room?
And for this, we use, again,
a simple technique where we
detect the points that moves
from a segment belonging to a
corridor to a segment
belonging to a room.
So in this case, you
would highlight
these different points.
This will give us a cluster of
points, where each cluster
represents a possible
location for a door.
And then we use the center of
mass of these points to detect
the room position.
And you can see that we can
accurately detect the room
position using this
simple technique.
So let's quantify.
I just showed you some
visual cues.
Let me quantify the accuracy and
the different parameters.
In order to evaluate our system,
we used different
Android phones and two main
testbeds, one in a shopping
mall, mainly to test the
accuracy of detecting the
points of interest--
stairs, elevators,
and escalators.
And we use a building, one floor
in our campus building.
We collected [? 1,000 ?]
traces, and the trace contains
many signals.
So we have about 300 segments
for these [? 1,000 ?]
traces.
These traces were collected by
four volunteers in [? 12 ?]
different rooms and
all the corridors.
And for this, we used the lowest
sampling rate, which is
the user interface.
[? Today, ?]
I don't believe if we used this
sampling rate, our energy
consumption would be lower,
because the phone is running
in the background the
accelerometer chip with this
rate in order to detect the
change in orientation.
So if we base our sensing on
this rate, hopefully our
energy consumption would
be much lower.
So this if figure shows the
number of samples required to
detect the door position.
So here we are trying to detect
the door using sampling
and doing averaging, the theory
of large numbers.
And we can see that using just
100 samples, we can detect the
door entrance location with an
error or less than one meter,
which is very good.
This figure shows the required
number of samples to achieve a
certain accuracy for detecting
the building entrance for
different duty cycle lengths
for the GPS sensor.
And we can see that, even for
a very low-duty cycle of six
minutes, we can achieve
one-meter accuracy using about
1,200 samples.
I remind you that these number
of samples are amortized over
the number of users that
use the building.
So using a building like
this, you have hundreds
of users and visitors.
You can get this accuracy in
a couple of days, if not a
couple of hours.
And again, we are doing this
in a distributed manner for
all buildings around
the world.
This is the key point.
This table shows the confusion
metrics for the five different
classes of points of interest,
the initial points of interest
that we are trying to detect.
And you can see that for 170
traces, the accuracy, we can
achieve 0.2% for a positive
[INAUDIBLE]
rate, and 1.1% for its
negative rate.
Again, this is for each
point of 170, it's
representing a case.
However, if you have a large
number of traces, a large
number of users using this
building, you can reduce this
further to almost 0% error,
because you are doing
[? central ?] fusion from all
the users that are using this
building over time.
This figure shows the CDF of
distance error for the trace
generation.
That black curve is using
just our time.
And you can see that
the median distance
error is about 30 meters.
This is for the entire
testbed area.
If you use UPTIME and then do
anchor-based correction, your
error drops to about
eight meters.
And if you do anchor corrections
and you take turns
as anchors, the error drops to
less than three meters, which
is very accurate for indoor
environments.
Finally, this figure shows how
many traces we require, for
segments, in order to estimate
the correct number of rooms in
the building.
And you can see that about 283
segments are enough to
estimate the actual
number of rooms.
Again, this is amortized over
the number of users who are
using the building.
So to conclude, I hope I showed
you that CrowdInside
leverages ubiquitous cellphones
to automatically
estimate the floor plans.
It uses a novel technique of
error-resetting based on the
points of interest inside the
area of interest to get
accurate user traces.
And the nice thing is that it
doesn't require any special
infrastructure to generate the
user traces or the indoor
floor plans.
And it can work in
parallel for all
buildings around the world.
Of course, this is just
the beginning.
There are a lot of challenges
and open problems, including
inferring higher level
semantics.
So can I differentiate between
a meeting room and a normal
office, for example, based on
the patterns of the user
inside the room?
Can I differentiate between a
coffee shop and a bookstore
inside a mall, again,
automatically based on the
traces I collect?
Can I identify the person or
the owner of this room?
All of these are different or
higher level of semantics that
are still open to investigate.
If you go back to the ubiquitous
indoor localization
project, once I have an accurate
trace generation
tool, like we're combining
UPTIME with CrowdInside, I can
use this to automatically
save the fingerprint for
Wi-Fi-based localization.
So while the telephone is in my
pocket and user is moving,
he's automatically constructing
the Wi-Fi
fingerprint of this building.
So I can use it for accurate
Wi-Fi localization without
doing any explicit
calibration.
And as any other
crowd-sourcing-based approach,
user incentives is
a challenge.
Energy and privacy concerns also
are problems still that
need thoughtful solutions in
order to address them.
And this concludes my talk.
I hope you liked it.
If you need more information,
you can go to the project
website, where you
can find papers
and other media coverage.
And I'd like at the end to
acknowledge that this work is
sponsored by Google
Research Awards.
Thank you very much.
And if you have questions,
I can have them.
Or I have a demo.
I can show it first, and then
go to the questions.
[APPLAUSE]

MOUSTAFA YOUSSEF: So this is
a tool where we have traces
collected in an offline.
I'm replaying the traces we
collected that we are
using in the paper.

So let's start by
showing the--
if I just use the convex
hull, [INAUDIBLE]
traces, you can see that the
convex hull simply doesn't
capture the overall floor plan
shape because it has a concave
area inside.
So the convex hull is not the
right solution to use.
Instead, if I use the alpha
shape and do the same thing,
you can see that the alpha shape
can capture the building
layout nicely.
There are some areas that were
not covered by the traces
because we didn't have access
to them, so they are not
included in the overall
plan shape.
But for those areas that were
visited, you can see that the
alpha shape nicely captures
the floor plan.
Of course, alpha shape
has parameters.
So for example, if you
set it to infinity.
It gives you the convex hull.
That's why the alpha shape
generalization
of the convex hull.

What else can we show?
Let's move to the individual
room g
So in order to show this, this
is the point cloud responding
to the traces we collected.
The blue represents the points
classified as corridors, using
our classifier and features
I showed you
in a couple of minutes.
And the red represents the
rooms, the points that were
classified as rooms.
If I do clustering on them,
actually I can show it as
segments, too.

However, if you go to points
and then do clustering, you
can see that just using distance
for clustering, what
you can see is that the rooms
on the top have one color.
They cannot be separated.
This is because they are
close in distance.
Similarly, there are rooms
in the bottom.
So if I showed the overall floor
plan shape using this
clustering, you can see that the
rooms cannot be separated.
It measured a couple
of rooms together.
However, if I use Wi-Fi
as another feature for
clustering, you can see that
using Wi-Fi has separated the
different rooms as required.
And if I wanted to get the final
floor plan shape, you
can see that here we can
capture the rooms.
And the rooms using alpha shapes
closely represent the
different rooms in the
area of interest.
Finally, to get the door
locations, what we are showing
here is the black dots represent
the points where we
move from a segment representing
a corridor to a
segment representing a room.
So we can get these
small clusters.
If I get the center of mass of
these different clusters, I
can get the room
door position.
And this is the final floor
plan you can get using our
CrowdInside in this particular
building.

I think that's it.
Any questions?
Please?
AUDIENCE: Are you required
[INAUDIBLE]

MOUSTAFA YOUSSEF:
No, actually.
AUDIENCE: [INAUDIBLE]?

MOUSTAFA YOUSSEF: So what we do
is that, whenever you enter
the building, you detect a loss
of the GPS signal, so you
know your starting position.
And once you move indoors,
you're just using the initial
sensors for detecting
your location.
Of course, this was a controlled
experiment, so for
all traces, we know the starting
point, and then we
get the displacement inside
the building.
In general, what we are thinking
about is, you can use
other users as resetting
points.
So you have a trace from one
user that intersects a trace
from another user, so you can
take this intersection point
to reset the error on the
less accurate trace.
This clear?
So for example, a user has just
entered the building, and
he has started moving, and there
is a person who has been
moving for a longer time.
When they intersect, you can now
correct the longer trace
using the shorter trace.
Also, the loss of GPS signal
doesn't have to be at the door
entrance only.
It can be near windows.
So for example, here at least
you can have this loss of GPS
signal, so you use these
GPS-based reference points.
More questions?
AUDIENCE: [INAUDIBLE]?

MOUSTAFA YOUSSEF: Do
they have to be?
AUDIENCE: [INAUDIBLE].
MOUSTAFA YOUSSEF: So again,
it doesn't depend
on the trace length.
It depends on the density
of the anchor points.
So if the trace is long but
you're resetting your error in
the trace frequently, your
error will not grow
significantly.
And this is what we notice
by the [INAUDIBLE]
[? pattern ?]
[INAUDIBLE], that [? your ?]
increases and then decreases.
And as I mentioned, it is not
that correct your error at
this particular point.
You can go back and correct
the entire trace.
On average, how dense we found
it indoors, I don't have the
number in mind.
But we have found that--
again, we call these explicit
anchor points,
where you have templates.
You learn the anchor points
using these templates, like
the template for the elevator.
But in our MobiSys paper,
you are welcome to
take a look at it.
We have also unsupervised
anchor points, where we
learned anchor points
that doesn't have a
template on the fly.
And then we found that it's very
frequent that your error
is almost less than three meters
in indoor environments.

Please?
AUDIENCE: So it seems like
you're detecting steps with
[? jogging ?] and so on,
based on [INAUDIBLE]

rather than [INAUDIBLE]
looking directly at
the [INAUDIBLE].

MOUSTAFA YOUSSEF: Right.
So currently, we are using just
the number of features.
Some of them have been
used before, and some
introduced by us.
We haven't thought about
automating the detection of
features themselves.
But at least we thought about
automating, detecting the
stride lengths of the user.
And the basic idea, if you are
moving outdoors, for example,
you are getting a
reference point
which is two GPS locations.
And then based on the distance
and the number of steps within
this distance, you can get that
the average distance,
[? better ?] walking
step of the user.
If you take less time, it means
that you are moving
faster, so you can get an
estimate of the average
jogging step of the user.
But for the features themselves,
no, we didn't have
automated ways for detecting the
features for the different
stride lengths.

Please?
AUDIENCE: [INAUDIBLE]?

MOUSTAFA YOUSSEF: Right, but
you basically can be also--
because as I showed in the
figure, the main ideas that
you detect are positive peak
and the negative peak.
So it's the segment is periodic
to some extent.
So actually using periodicity
can be used as another way of
detecting steps.
But we haven't looked into this
in this particular work.
But it's a good suggestion.

Please?
AUDIENCE: [INAUDIBLE]

estimating them, how do you
feel about [INAUDIBLE]?

MOUSTAFA YOUSSEF: Right.
So the question is a chicken-egg
problem.
In order to reset your error,
you need to know the location
of the anchors.
But initially, you don't know
the location of the anchors,
and in order to get the location
of the anchors, you
need accurate traces.
So it's both ways.

What the plan to do it, or
actually what-- it is not
shown in this work, but what
we are doing currently, is
that we use a [INAUDIBLE]
approach, where you're
simultaneously detecting the
motion trace and detecting the
location of the anchor points.
The point I mentioned, that
whenever you hit an anchor
point, you can go backward and
correct your location, also
helps in that.
So the answer is, yes, you can
do both of them concurrently,
and the traditional theoretical
technique is using
something like [INAUDIBLE],
for example, to do it.
AUDIENCE: [INAUDIBLE]?

MOUSTAFA YOUSSEF: Exactly.
So basically, we do Wi-Fi-based
localization for
the anchor point.
So based on the anchor point,
you can now separate whether
it's the elevator of this
part or the elevator
on the other part.
Another question.
Yes?
AUDIENCE: [INAUDIBLE]
one trace [INAUDIBLE]
the building there
[INAUDIBLE]?

MOUSTAFA YOUSSEF: So actually,
I cannot separate the traces.
So here, we are having the 100
traces, I believe, that we
talked about 106 traces.
But I cannot separate them.
But what is your question,
particularly?
AUDIENCE: Well, [INAUDIBLE] load
them again [INAUDIBLE]

another point there
is [INAUDIBLE]
another hypothesis being in
another room [INAUDIBLE]

one anchor point [INAUDIBLE].

MOUSTAFA YOUSSEF: Right,
and the turns.
AUDIENCE: And [INAUDIBLE].
MOUSTAFA YOUSSEF: Yes.
And then this small.
So the question is, the traces
are very accurate, and the
reason is, the error-resetting
technique, it is not just
based on the stairs, it's
based on the user turns.
And actually, here we are
using also the Wi-Fi.
You can use Wi-Fi themselves
as anchor points.
AUDIENCE: So [INAUDIBLE]
output [INAUDIBLE]?
MOUSTAFA YOUSSEF: No, this
is not the output of the
[? slam. ?]
This is just based on the
error-resetting and going
backwards and correcting
the traces once you
hit an anchor point.
AUDIENCE: [INAUDIBLE]?

MOUSTAFA YOUSSEF: The stairs
are there, turns are not.
So both the stairs aren't.
Yeah, so for here, the ground
[INAUDIBLE] it's known.
The stairs' position
are known.
We are not doing
the [? slam. ?]
And the turns, of course,
is detected by the
change in the compass.
And the [INAUDIBLE]
is done using Wi-Fi.
AUDIENCE: So [INAUDIBLE]
good instructions [INAUDIBLE]

liability [INAUDIBLE]
middle of [? nowhere ?]
[INAUDIBLE].
MOUSTAFA YOUSSEF: Right.
So again, this was a very
controlled environment where
people were instructed to--
actually, having the
[INAUDIBLE]
for these different traces
wasn't easy, because we didn't
have a way to get [INAUDIBLE].
So basically, the users have
to click where they are
standing while they were moving
in order to get the
ground [INAUDIBLE].
So the traces wasn't an
arbitrary movements of a
person [? who's ?]
having the phone and
moving all day.
It was a controlled start
from here, try to
cover the entire area.
Yes?
AUDIENCE: For the quality, were
they aware that they were
gonna [INAUDIBLE]?

MOUSTAFA YOUSSEF:
They were aware.
AUDIENCE: They were aware.
MOUSTAFA YOUSSEF: Yes.

AUDIENCE: [INAUDIBLE]
when you [INAUDIBLE]?

MOUSTAFA YOUSSEF: Right.
AUDIENCE: And then how were
those associated [INAUDIBLE]
with the [INAUDIBLE]?

MOUSTAFA YOUSSEF: [INAUDIBLE]
[? meaning ?] the ground
[INAUDIBLE]
or the error-resetting?
AUDIENCE: The error-resetting.
MOUSTAFA YOUSSEF: So the
error-resetting, of course,
when you have these, then we
have the positions so that all
this is done offline.
So you get the traces on the
user, and then you do offline
processing in order to correct
the traces using the
error-resetting and to go
backwards in time and to
correct the user trace.

Other questions?
Please.
AUDIENCE: So how [INAUDIBLE]
expect it to be if
[INAUDIBLE]?

MOUSTAFA YOUSSEF: So the
question is, if they don't
take a turn, it wouldn't
be reset?
AUDIENCE: Well, so every turn
is like a [INAUDIBLE].
MOUSTAFA YOUSSEF: Right.
AUDIENCE: How will you expect
it to be if users were
[? cutting ?] turns and turning
sometimes slowly,
sometimes fast?
Would [INAUDIBLE] be able
to detect that.
And do you expect it to work?
[INAUDIBLE]?

MOUSTAFA YOUSSEF: So actually,
they can get to the second
level, which is giving it to
normal users, moving in an
arbitrary manner.
It's a challenge, of course.
In many cases, it wouldn't work,
but we believe that the
basic idea here is that you are
having hundreds of users,
if not thousands that are using
the building at the same
time, at different times of the
day, even the same user is
using the building at different
times of the day.
So if you fuse these all
together, you can easily
detect outliers.
And you can throw a lot of
these idiosyncrasies.
However, we haven't tried
it, so still it's open.
It's one of the challenges, of
course, sticking it to the
real environment and seeing how
a normal user and having
it in pocket will behave.

Other questions?
AUDIENCE: Yes.
So when you have [INAUDIBLE]
original trajectory and then
[INAUDIBLE]?
MOUSTAFA YOUSSEF: Right.
AUDIENCE: So then there's some
process that has to cluster
those together [INAUDIBLE]
together.
What [INAUDIBLE]?
MOUSTAFA YOUSSEF: I didn't
get the question.
This is for detecting the turn
if it's anchor points?
AUDIENCE: Once you detect the
anchor point, then how do you
attach the anchor point
[INAUDIBLE]?
MOUSTAFA YOUSSEF: To do what?
AUDIENCE: To attach the paths,
tie the paths together.
MOUSTAFA YOUSSEF: So in order to
correct the user trace, is
this what you mean?
AUDIENCE: Yeah.
You're adding a [? constraint ?]
[INAUDIBLE].
MOUSTAFA YOUSSEF: Exactly.
AUDIENCE: [INAUDIBLE] some
process that has to
[? identify ?]
those anchor points?
MOUSTAFA YOUSSEF: So basically,
you are generating
a trace without any correction,
and then you
detected a user made a turn?
AUDIENCE: Yes.
MOUSTAFA YOUSSEF: So in this
particular point, you are
trying to search what is the
nearest anchor point to this
particular location.
AUDIENCE: So [INAUDIBLE]?
MOUSTAFA YOUSSEF: Exactly.
And it's basically based on also
on the Wi-Fi signature.
So at this particular location,
I'm [? hearing ?]
some access points.
And at this particular anchor,
it's registered that this is
the Wi-Fi signals that's
captured at this point.
This is the way you do the data
association for gluing
two traces together.
AUDIENCE: But is there
a global [INAUDIBLE]?

MOUSTAFA YOUSSEF: We haven't
tried it, but again, we are
currently working on this
[INAUDIBLE] thing.
Whether it's related or not, I
think it will be hard to do
[INAUDIBLE]
on a global scale, because as
I mentioned, if you take a
large mall, for example,
there'll be a large number of
anchor points in this building,
or this floor.
But no, we haven't
tried it yet.
Other questions?

[INAUDIBLE]
one more question.
AUDIENCE: In terms
of [INAUDIBLE]
room, would this particular
[INAUDIBLE] test, are those
anchor points there, or were
the-- the anchor points, you
said, were chosen beforehand.
But would there be
any [INAUDIBLE]?
MOUSTAFA YOUSSEF: For
rooms, I think
mainly it's around tables.
And maybe in this particular
environment, the authors, the
people or the students were
instructed to make 90-degree
turns to make it cleaner.
But as far as I believe, it's
mainly around the tables and
furniture inside the room,
if I am not mistaken.
And actually, you can see inside
the room, maybe it's
[? escaping ?]
or maybe not accurate as in the
corridors because of the
randomness inside the room.
Great, thank you very much.
And I'll be available after the
talk if you are interested
in discussing more.
[APPLAUSE]
</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>