<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2015: Large-Scale Automated Visual Testing | Coder Coacher - Coaching Coders</title><meta content="GTAC 2015: Large-Scale Automated Visual Testing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2015: Large-Scale Automated Visual Testing</b></h2><h5 class="post__date">2015-11-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/euJ2OrlBEqQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and we're going to kick it off with Adam
Carney talking about large-scale
automated visual testing thank you thank
you hi everyone I'm Adam Carney I'm the
co-founder and VP R&amp;amp;D of applitools
we're a company that provides a cloud
service for automated visual testing
large-scale automated visual testing is
a reality today there are teams that are
running executing thousands and tens of
thousands of visual tests on a daily
basis and this talk will focus on the
key tips and guidelines that are crucial
to succeeding with visual testing at
this scale so for those of you who are
not familiar with visual testing I'll
start with a brief introduction I'll
explain what visual testing is and why
it should be automated I'll also explain
how automated visual testing tools work
and the remainder of the talk will focus
on these scale-up tips and we have time
at the end to answer any questions that
you may have so what is visual testing
it's a quality assurance activity aiming
to verify that a graphical user
interface appears correctly to users now
this goes beyond the traditional
functional testing that you used to do
with tools like selenium and appeal
where you test the functionality of the
application through the UI what are we
what we're focusing on here is making
sure that the UI itself appears right
and correct to the end-user we want to
make sure that each UI element appears
in the right color shape position and
size and it doesn't overlap or hide any
other UI element this type of testing
has become increasingly difficult to
perform in recent years many due to the
explosions of browsers devices operating
systems and screaming solutions that
applications are expected to run on
today so here you can see an example of
a visual bug that we found in the
Microsoft Azure management portal you
can see here how the graph exceeded the
expected bounds of the page and this is
how the Amazon website looked like for
several hours for some customers
during the Amazon Prime day three months
ago so I'm sure you've all seen these
type of bugs and maybe in your own
application and you understand their
severity they can be very embarrassing
and damage the brand of the company but
in some situation they can completely
cripple the functionality of the
application and end up costing a lot of
money which probably what happened in
this case so why should we automate this
type of testing there are many reasons
but the most important one is that the
test matrix is just too big to cover
manually think of all the different web
browsers devices operating systems of
screen resolutions that your application
is expected to run on we all know that
website that looks good on Chrome
doesn't imply that it looks good on IE
and an application that looks good on a
wide screen doesn't mean that it would
look good on a smartphone so we have to
test the application in all these
different environments which which can
be very ineffective and costly to do
manually if your application is
responsive and most modern applications
are then you also need to test all the
different layout modes in all these
different environments if it's localized
then you also need to check factor in
the fonts resources images and content
that is specific to which language and
of course even if you don't change a
line of code in your application still
you depend on third-party upgrades that
can introduce visual bugs to your
application an example for that is a web
browser that updates every few weeks and
whenever it does it can introduce some
incompatibilities with your website that
may be manifest as visual bugs so you
have to test a lot of things many times
and it's just too ineffective to do
manually what makes things even worse is
the fact that release cycles keep
getting shorter and shorter so companies
that are doing continuous deployment
today which is the current type we're
listening to production several times a
day and with such short release cycles
there's hardly any time to do any type
of manual testing that along like making
sure that your application looks good
and so many execution environments so
let's take a look at how these tools
actually work
so there are many automated visual
testing tools out there and they all
share the same workflow with these four
simple steps in the first step you drive
the application on the test and take
screenshots in the second step the tool
takes those screenshots and compares
them with baseline images these images
defined the expected appearance of the
application and in men in the majority
of cases these are simply screenshots
that were taken in the past and approved
by a human tester in the third step the
tool generates a report following the
test run that includes all the
screenshots all the baseline images and
any differences that were found during
the test and in the first step tester
has to look at the report if any changes
were found and decide for each change if
it's a bug in which case he opens a bug
or if it's a valid change to the
application
he simply approves the new screenshot to
be used as a baseline images for
subsequent ones here you can see an
example of of the first two steps taking
screenshots and updating them with an
open-source visual test automation tool
called webdriver CSS as you can see
javascript base and it extends the
webdriver i/o webdriver and this test is
very simple you can see that it starts
by with the instance of the webdriver
i/o which initializes it which causes
the browser to open in this case it
would be a local Chrome browser then it
navigates to the example.com URL and
then it performs a visual validation
point with the webdriver CSS command
which takes three parameters the first
is a name for the checkpoint that's how
it will appear in the report and then a
list of elements that you want to
validate in this case there are two
elements header and hero and they are
both defined with CSS locators and the
third parameter that is a callback that
is called if differences are found there
you can assert and make sure that the
differences do not exceed any mismatch
tolerance that is pretty fund so you can
see here how easy it is to
an existing functional test in this case
webdriver test and just turn it into an
automated visual test with a few lines
of code here you can see an example of
the two last steps viewing the report
and updating it with a tool called
gemiini also an open-source tool you can
see here that you have a list of all the
checkpoints the one that has differences
are marked in red and you can see for
each difference that is found that you
have an image of the baseline it's
called reference in this tool the
screenshots that being validated and
also a diff image which is basically the
screenshot with all the different pixels
highlighted in pink so it would be easy
to identify the change and the
maintenance operation that you need to
do if you decide that this is not about
it just click this button over here the
arrow and this would be placed and set
the screenshot as the baseline for
subsequent ones there are many visual
test automation tools available so if
you want to get started with them you
have a lot of options to choose from it
is important to highlight that the
majority of tools all of them almost
only work for websites they won't work
for mobile applications the only two
exceptions are Facebook's sorry about
that Facebook snapshot test case by
Facebook that we can test our OS
applications and up little eyes that
will work for any mobile application
that can be driven with appeal and that
concludes our brief introduction to
visual testing let's proceed with the
scale updates and the first step is
naturally use a robust image comparison
engine and if you allow me to quote
boromir then one does not simply do a
bitmap comparison and the reason you
don't do it is that if you do it is you
get a lot of false positives false
positives in the context of visual
testing is a case where the tool tell
you that there is a difference but you
as a human being cannot see it or it's
so minut that you don't care about it
and there are many many reasons for this
these false positives if you just do
simple pixel to pixel comparison and
let's take a lot of some of the most
common ones so the first one has to do
with with an image processing effect
called anti-aliasing and here is an
example of it you can see we have at the
button of the the bottom of the image
navigation bar and the playlist tab is
magnified you can see at the bottom that
the playlist text appears in white and
probably that's what the color of the
text was in the Dom or whatever but you
can see in the magnified image the Ducks
are actually many of the pixels are not
white they are they have different
shades of blue this is anti-aliasing at
work okay basically the rendering engine
which is the graphics card or whatever
is responsible for rendering this image
is painting certain pixel along the
border of the of the lines in different
shades and intensities and colors and in
order to make the text appear smoother
to us
the problem with anti-aliasing is that
if you run your tests on several
machines which aim which makes sense if
you have more than one tester in your
team or if you're running your tests in
a lab which makes sense if you are
trying to scale up is that you'll get
different rendering engines painting
those pixels which means different
implementations of the early
anti-aliasing algorithm so this for
instance is how anti-aliasing will look
like on a different machine I will
toggle between the two and you can see
how different the pixels are not that
different how they're different they are
in position in color then again you need
a very sophisticated image comparison
engine in order to be able to identify
that these different pixels are actually
invisible to the human eye and just
ignore them otherwise your test will
fail a similar case has to do with pixel
brightness when run on different
machines or even on the same machine
which is connected to a monitor with
different capabilities you can see how
different the intensity of the pixels
can be this example has to do with the
positioning of text so this is you
probably recognize it is the dropbox
dropbox website and you can see that the
upgrade
element is moving the reason that it's
moving is that to the right of it there
is a username and whenever we run the
test there is a different username it
has different lengths and because of
that the abet account element is moving
so we would still want to visually
validate it so one simple solution to
this problem would be to just capture a
screenshot of the update account element
and compare it to a baseline that just
for that element and so it doesn't
really matter where it appears on the
page that's a very good idea but if
you're doing pixel to pixel comparison
it would still not work because although
it seems that the a great account
element moves as a whole actually each
individual letter is positioned on its
own so you can see how the a and C and
all letters are positioned in different
pixels and then again you need the image
comparison engine to be able to realize
that all these different pixels are
actually look they actually look the
same to a human the last example I'll
show you has to do with human scaling
which happens all the time in every
application what you see here is this
car here and the small magnified box
here and what happens is that whenever
you have an image element in your
application and the source image that
it's showing is not exactly of the same
size the rendering engine will actually
scale it so it would fit the target
position and then again two different
rendering engines would use a different
scaling algorithm and that would
actually result with extremely different
pixels as you can see here so again
extremely different pixels but
completely invisible to us as humans and
again this is something that the tool
needs to be able to take care of and
there's more there could be arbitrary
pixel offsets some could be small like
text or paragraph displayed at one pixel
offset some could be more severe like
column of a table being one pixel wider
and that would move the entire page one
pixel to the side resulting with a 70%
pixel wide change of the image but still
completely invisible to us he
to be able to deal with dynamic content
such as dates and ads and usernames
moving elements like the AdWords account
samples as we saw before comparing
images of different sizes and of course
the image comparison engine has to be
extremely fast otherwise your test will
take forever to run and there are those
that always get mismatches and there's
nothing you can do about it but
seriously visual test automation tools
have come very very long way in in very
successfully it can very successfully
handle this type of false positives
today but before you start working with
a visual test automation tool you need
to make sure that it works very stable
for your app because if you won't be
able to get away from these false
positives there is no way that you'll be
able to scale up your tests tip number
two use a single baseline across devices
and browsers so the idea here is to
reduce the amount of maintenance that
you have to do especially with regards
to cross-browser and cross-device
testing by reducing the amount of
baseline images that you need to
maintain and this is how you do it you
keep a set of baseline images for your
application on a space for a specific
execution environment let's say Chrome
on Windows and whenever you have a new
release of of your application you use
the same baseline to validate that new
release on the exactly the same
environment basically you're doing
visual regression testing you can find
unexpected changes you can use a very
strict matching algorithm to do that
because you know your application is
running on the same execution
environment and so it should render
exactly the same once you are done with
that process and you have updated your
baseline to reflect the new version you
can then use the same set of baseline
images to make sure that your
application renders correctly in other
execution environments such as other
browsers other devices or form factors
by making sure that their layout or
structure is consistent with that of the
baseline and I'll show you an example of
this to clarify it using ugly too sighs
okay so the first example is for
cross-browser
test you can see the baseline image here
on the left hand side and the screenshot
that we're validating on the right hand
side you can see that the baseline image
was taken on Chrome and the screenshot
was taken on ie if we toggle between the
two images you can see how different the
browser renders these images you can see
that their phones are slightly smaller
or bigger they change position the text
here wraps a bit differently but when
we're performing a structural comparison
all of this is fine that the images are
consistent in terms of layout and
structure we have this radar button here
that we can click to find any
differences that we did find and we can
see that we have a different at the
bottom if we zoom into it we discover
that we actually did have a missing
element in IE so this is an example of
how you can use structural matching to
find bugs between across browsers
the next example shows a cross device
test this is Twitter on two different
mobile devices and we have two
violations here so the first one in
course here the baseline image dictates
that the paragraph should be aligned to
the right of the image and this is
broken here here at the bottom we have
an image but it is missing in the
screenshot so again all these structural
changes can be caught and if you see
when I toggle the the screenshots the
two tweets in the middle although they
have different images and text in them
are not highlighted is different because
structurally they are equivalent another
very powerful application of structural
layout matching is monitoring and
testing dynamic applications so in this
case we have the Yahoo a website
that screenshot and the baseline were
taken in a 24 hour difference you can
see how different the images are and and
the articles but still structurally they
are equivalent and no differences is
reported if I would change this to a
more strict match you can see that all
the dynamic parts are highlighted and
the fixed ones aren't but if I would do
pixel to pixel comparison on this page
you can see that actually everything is
highlighted and the reason is that there
are everything here is different only we
cannot see it and strict his strict
matching in this case is powerful enough
to ignore all of that back to the slides
okay tip number three baseline
maintenance should be codeless in the
majority of cases maintaining the
baseline involves looking at the
screenshot and clicking a button to
approve it as a baseline in some rare
situations you also need to mark one or
more regions on that baseline that are
dynamic and you don't want them to be
matched
during image comparison so actually
there is no real value or benefit to
doing this kind of maintenance in code
on the other hand if you do it in a
cordless way there is a huge advantage
first everyone in your team can do it
even those members of your team they
don't know how to code more importantly
they can do it immediately when they see
the change so just think about huge
difference if you have hundreds of
baseline images to maintain and when you
see a difference instead of just
clicking a button and accepting it you
need to open up an IDE find the test
code related change the code there so
and making sure you don't introduce new
bugs or even worse find the guy
responsible for the tests open a ticking
for him to fix the test it could be a
difference of hours and hours of work
for every test one there's another mini
tip here that is a bit counterintuitive
and it is to prefer full page validation
over elements
validation the reason this is preferable
it because it's allow you to avoid
maintaining those element locators in
code that tend to break when the UI
changes tip number four maintain at the
test suite level rather than at the test
level now this seems a bit trivial but
actually it's a very important tip there
are situations with visual testing where
there is one bug or one change in the
system or very few changes that are
captured by multiple checkpoints across
many many many of your tests
just think how frustrating it would be
if you would have a bill that fails and
the report would show you 200 tests that
have failed and now you have to go over
one-by-one choosing the first test
looking at the difference resolving them
going back to the list looking at the
next test it's so on and so forth what
is what is much preferable is just to
see all the changes that happened during
that build and immediately have the
ability to resolve them and completely
ignore the tests in fact what you would
really want to have is the ability to
see only the unique changes and not all
of their occurrences and this leads us
to the final tip which is automate your
maintenance and in order to explain to
you what automated maintenance means
I'll show you another demo without
beetles eyes another demo
right so for the purpose of this demo I
prepared a small test suite for the
g'tok website and I ran it
I created the baseline I simulated the
difference and then I run it again and
let's see the results so in this case we
see that the test Suites or batch as we
call it is called jitta it ran and it
appears as red meaning that it found
differences a small summary here tells
us that there were 28 tests there they
covered three environments those would
be ie Firefox and Chrome on a Windows
machine three form factors that
correspond to the three layout modes
that the website support is a responsive
website and in total we had 91 check
points that all all of them found
differences so let's look at the first
image click the rider and see and we can
see that we have a small difference here
at the footer let's zoom in and see what
we found we toggle between the the
images and we say that we have a plus
symbol to change to a minus so let's
assume for the
let's assume for the sake of the demo
that this is not a bug
but actually it's a rebranding of the
product and and so we and so we want to
approve it but before we do let's look
at the other images so we look at this
one and it's the same place it's
probably the same change right and this
one let me go back here and this one
probably the same and this here this one
at the bottom again the same so but it
makes sense right we just change the
name of the product it probably appears
in all the images so we are really
tempted to click this button that says
accept all I'm not going to do that I'm
going to click on another button and
I'll explain why I'm not clicking that
button accept all basically means
overwrite the baseline with all the
screenshot that I found and all the
visual test automation tools allow you
to do that but actually it's a very
dangerous thing to do because what
you're actually doing is skipping
maintenance so you could introduce bugs
real bugs into your baseline and then
prevent the tool from finding them later
on so if I would was free to choose a
name for this button it would be skip
maintenance and the tooltip would be I'm
too lazy to do my job basically what I
just did is I asked the tool to do this
automation or this maintenance operation
for me in an automated way I asked it to
find all the similar differences and the
result is that from this bill that had
all these failures I only have two
images to look at right now 90 of the
checkpoints have exactly the same change
although they ran on different browsers
and they were any different layout modes
and form factors still applause change
to a - and that's it so I can just go
ahead and approve that because that's
what I want to do I have another change
here and let's see what we found here
and although it's subjective I think
that you can agree with me that we found
a bug and so we won't accept it but we
ejected and with that we're done
maintaining our test run so there are
many opportunities with visual testing
they do this type of automatic
maintenance as
can realize it goes a very long way in
allowing you to scale up your test
without introducing more overhead to
your maintenance and with this I
conclude my talk and I hope that you
learned something you enjoyed it and I'm
be happy to get take your questions now
well thank you Adam you have earned your
free lunch by fighting bugs on our
website yeah so for the first question
how does your baseline strategy work in
a continuous build and release model so
basically it would work like it has
nothing to do with the continuous
release or build model so basically you
run the test there are failures you look
at them you are if there's a failure you
don't release immediately you wait -
until someone approves it so it's
basically it fits right in there and
it's actually the safe way to do
continuous deployment because instead of
just releasing stuff without looking at
it you know when there's a change how do
you minimize the cost of writing and
maintaining a new set of tests so there
are different strategies the different
teams used to do visual testing some
would write new tests very simple tests
to just browse through the application
if it's a website usually they'll just
put in a set of URLs of one I mean a few
lines of code and it will just go over
the entire set of pages on all the
different execution environments and
it's you're done other teams will take
their existing tests and just add
validation points where they want to and
others will go to the test
infrastructure usually you have some
kind of obstruction around the test
driver and you would just hook it into
some operations that would automatically
trigger validation points so one of the
best practices is to do a validation
point immediately after you change to a
different URL and the other one is just
before you click a button or a link so
that covers like 80% of everything you
need and it's just centralized at one
place and you
down in your helpful coverage for
whatever test you already have running
so how would you use the tool to test a
website or mobile app that has not yet
launched can it be run locally so
basically the if you're asking if the
question is specific for applitools or
for any other tool so what's interesting
okay so basically the open source tools
all of them they run locally so there's
no problem using them locally especially
with up little specific than we support
on-premise deployments and also private
cloud and public cloud deployments but
anyway the test always runs behind the
firewall so there is no need to open up
tunnels from the outside into your
organization we use for instance with
selenium the webdriver to get the
screenshot and we send them outside so
if you can Google out of your test
machine out of you that the machine that
is running your test then you can work
with the service you don't need it can
be behind the firewall changing order
what's your defendant is it public the
defendant is something that we've been
working on for more than three years
today it's not public that's our secret
we don't patent it as well so we don't
need to expose it it has it's not open
source and there is nothing out there
that we build on it's completely from
scratch to do to handle this type of of
work do you use these tools as
acceptance tests rather than as test to
find bugs using this tool as an
acceptance test is a very very effective
way to work with the tool by the way if
you don't want to bother yourself with
each build and update the changes
although this becomes an amazing
collaboration tool in the team because
immediately you see everything that
changed not bugs even if you think that
your PM doesn't like the color of the
button that the developer just changed a
minute ago or ten minutes ago we can
immediately give him this feedback but
if you don't want to do that then you
can just use it as an acceptance test
run it once before we listen to
production or use your staging
environment
dynamic baseline to match your new
candidate to test it make sure there are
unexpected changes that would be a great
way to do it
so for acceptant testing that's great
what was the other one I forgot where is
it I don't know that is more than that
question and I think there was but then
I feel there was looked at okay what
about false positive for example things
are expected to be different visually
across runs or browsers or platforms
then you can just have a different
baseline for each of them so that's fine
no problem</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>