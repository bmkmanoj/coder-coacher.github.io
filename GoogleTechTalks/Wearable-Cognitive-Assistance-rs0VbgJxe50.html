<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Wearable Cognitive Assistance | Coder Coacher - Coaching Coders</title><meta content="Wearable Cognitive Assistance - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Wearable Cognitive Assistance</b></h2><h5 class="post__date">2015-07-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rs0VbgJxe50" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's a good morning today I'm delighted
to introduce professor Satya from
Carnegie Mellon University he has a long
history of exciting and impactful
projects in mobile computing mobile
systems areas including open AFS coda is
our diamond and I was also pioneered the
concept of cloudlets
with the number of projects and today
he's going to talk about the wearable
cognitive assistant which is an FR a
funded project and combines cloudlet
concepts with google blas please thank
you very much for the introduction Roy
and thank you all for coming what I want
to tell you about is work that we're
doing on an idea that it's very cool and
I'm very excited about it I'd like to
share it with you it's work that's
partly funded by Google I'm both in
terms of a research award and also
Google glass devices which you guys gave
me so like so much else
this is collaborative work with a long
list of collaborators to whom are in the
room with Roy and Nigel certainly the
concept of cloudlets is something that I
developed in there in the in
collaboration with them and it's part of
part of the foundation for this work so
we live at a very very unique moment in
time when three completely independent
innovations are coming together and so
let me just take a moment to explain
what these three independent innovations
are on the one hand is wearable hardware
stuff like Google glass Microsoft has
hololens there are other wearable
devices on the market there's going to
be a big fight over in the marketplace
over the correct form factor the correct
devices etc etc etc there is no question
that the time has come for wearable
devices so that's that's one force the
second force is the convergence of
mobile computing and cloud computing so
as we speak there is a commercial
product from Nokia call racks radio
access control system a cloud system and
it's a commercial product that they sell
where there's a Z on computer right at
the transmitter in September in London
there is a conference the mobile edge
computing conference which is by cell
tower operators for specifically for for
computing at the edge of the Internet so
this whole concept of cloudlets that I
mentioned which are small clouds close
to the edge close to mobile devices is
another force that has happened the
third force which is not not Hardware
but really software deals with
tremendous advances in what I broadly
characterized as cognitive algorithms
these are things like computer vision
face recognition object recognition
language translation and so on all of
these Google certainly has many many
innovations in this space last year's
publication at cvpr the computer vision
conference by face book researchers
showed a face recognition algorithm that
was as accurate as human beings and last
week I believe Google researchers showed
one that was even better so this is
happen so the research that I'm about to
describe to you asked the question what
can you do at the intersection of all of
these there's tiny little red with the
intersection of wearable devices such as
Google glass leveraging cloud resources
to extend the computation to do big data
computations if needed
combined with the progress in cognitive
algorithms what could you do
so what you can do is something that is
really quite science-fiction - lets stay
Google glass since all of you are
familiar with it when you wear it all of
its sensors see what you see it hears
what you hear it
sensors what you sense in terms of
accelerometer gyroscope etc imagine
processing that sensor input in real
time on a clock and getting the result
back faster than the person can think
within the inner loop of human cognition
and if I can constructively give advice
to the user saying here's something you
could do or here is here's an action you
could take that would be something that
has no equivalent today so I think of it
that's really a new modality of
computing and the emphasis as the slide
mentions at the bottom is seamless
integration into the inner loop of human
cognition
now what's hard about this what's hard
about this is you have to be very very
very fast you have to be able to process
the sensory inputs produce the output
and then have time left over to do
something constructive on behalf of the
user
within the time that the user has to
process the same data and if you look at
the psychology literature and the
cognitive psych literature the the
typical speeds are in the hundreds of
milliseconds range down to tens of
milliseconds so some things are pretty
amazing humans for example can tell the
difference between a human sound and a
nonhuman sound in four milliseconds you
can see how evolution might have the
cook does with this if your child is
lost in the woods and you're in the old
you know Ice Age or something that might
mean the difference between being able
to rescue the child and not so so
there's debate over exactly how fast is
fast enough and I honestly don't think
we will know the answers until we build
these things experiment with them etc as
a working guideline for our research
we have used the number few tens of
milliseconds so few is obviously soft 10
milliseconds I'm certain is very good 20
might be ok once you start getting much
further than that
we start getting into periods where at
least some of the cognitive measurements
that people have reported start to be
larger start to be comparable to these
numbers so people have used the term
tactile Internet to to suggest a
cloud-based system that is so reactive
that it responds in the way that I just
described so think of the tactile
Internet as a platform or a medium for
wearable cognitive assistants so how do
we do anything with this kind of
timescale 10 milliseconds is very little
time right really very little time the
best way to do it by far would be to do
it all on the mobile device no question
ok unfortunately doesn't work here is
optical character recognition using
tesseract which is a google open source
OCR on google glass
that's the standalone measurements 10
seconds to do the image recognition and
roughly 12 joules to recognize it of
energy if you are able to ship it ship
the image from the camera to a nearby
cloud processing entity even after you
pay for the cost of transmission and
reception it is 10 times faster 1.3
seconds roughly and about an order of
magnitude less energy so in general
wearable devices especially it's true of
all mobile devices but especially true
of wearable devices weight form factor
heat dissipation comfort these dominate
over processing capability and so if you
asked a user would you like a Google
glass that weighs half as much
I can process twice as fast it's very
clear what the answer will be they will
want one that's half as heavy
perhaps cooler and the battery life is
longer okay nobody will say give me a
more powerful Google glass
so standalone applications don't work
and in fact just waiting longer maybe
next years a software I'm sorry next
years hardware from Qualcomm or whoever
might be faster this graph from Jason
Flynn's synthesis lecture on on cyber
foraging says that waiting is not going
to help if you look at the columns they
show you the improvements in compute
power over a roughly a 13-year period in
server hardware and in mobile hardware
in each column the improvements are
dramatic but if you look horizontally at
every level of technology there's a
substantial gap and I refer to this as
the mobility premium this is what you
pay to make something mobile you have to
give up something at any given level of
technology to meet the human
considerations that I just mentioned
earlier heat dissipation battery life
weight size etc ok then if you can't do
it all standalone then the right thing
to do is to offload it to the clock
that's Google Cloud there's Amazon lots
of cloud services the problem is you
aren't going to get an answer back fast
enough the measurements reported in the
literature on the average latency
between a random mobile device and the
optimal Amazon Cloud is of the order of
75 to 80 milliseconds that's just
getting the packet there and back zero
processing so if you add to this any
processing and then you need time left
over
to do some constructive guidance for the
user you're dead
so offloading to the cloud doesn't work
and so the answer here is the concept of
cloudlets which are data centers data
center caliber compute but located close
by located one wireless hopper would
where they are located is somewhat
independent of the rest of this talk
they could be in a base station they
could be in a cell tower they could be
in the in the in a machine room in this
building land connected all of those
would be perfectly acceptable ok but the
important point is that the round-trip
time is significantly less than 10
milliseconds which means that you have
many milliseconds of budget for
processing the sensor data and then
composing some constructive feedback to
help the user yes
yep I have numbers I don't have it right
here I've many numbers many measurements
maybe at the end of the topic and dig
them out for you so the question was
what is the average time for for example
shipping image data to the floodlit and
back to a first approximation it's the
last hop wireless bandwidth which if
it's Wi-Fi today it can be as good as
400 megabits per second alright and then
you have to add to it some latency that
is true even for a zero length packet
but it's typically in the single-digit
milliseconds okay for even a 1080p video
frame from from a mobile device it's of
the order of single-digit milliseconds
okay so this picture shows you our
vision of cloudlets the talk is not
about cloudlets it's merely a cloudless
merely happened to be an enabler of the
class of applications that I'm
describing to you so our vision of
cloudlets is that someday soon maybe
within the next decade you're going to
see these things not just in buildings
but in automobiles next to your battery
and your spare tire is a problem
remember for the foreseeable future
the best connectivity between an
automobile and the cloud is going to be
4G your connectivity to the cloud line
is going to be a lot better inside the
automobile in an aircraft same story I
had Wi-Fi on my flight but it's the
worst Wi-Fi internet connection I've
ever experienced
ok latencies were horrible when it
worked so we see a world in which
cloudlets are widely dispersed and
everywhere and that's the underlying
assumption for the rest of the talk so
what can cloudlets do for you they can
do many things this talk is about taking
advantage of latency things I'm not
going to talk about or how running
analytics for example
computer vision analytics or video
analytics at the edge on a cloudlet can
actually save the tremendous Ingrid band
ingress bandwidth into the cloud privacy
one of the biggest barriers inhibiting
the Internet of Things is people's
concern about the private data sensor
data leaking out into the web if users
had confidence that a trusted piece of
software that they trusted it's not
somebody else's software that they
trusted ran on hardware that they
control and did initial analytics on the
sensor data and only the redacted data
was exposed to the rest of the world
that confidence is likely to be a big
enabler in terms of acceptance of IOT so
AC cloudlets playing an important role
that is a privacy you can think of it as
a privacy firewall we think of firewalls
as preventing bad stuff from coming in
this is about preventing well depending
on your viewpoint good stuff or bad
stuff from leaking out okay in the other
direction a last point is cloudlets can
also be useful if you can't get to the
cloud if I'm in Nepal and an earthquake
hits or here and an earthquake hits or a
flood happens hurricane sandy hits or
something like that there's so many
national disasters that happen in those
sinuate scenarios as well as minute
military scenarios getting to the cloud
is not always possible so these are all
the unique values of cloudlets
in this talk we're only going to focus
on the first of these which is it
provides you low latency and to complete
the thought about cloudlets this is not
hype shown in the slide are pictures of
pieces of hardware you can buy today
real commercial products they call Micro
data centers they actually used for a
variety of purposes but they're not
being used in the way that I'm
describing them in the stock
they're being used primarily as private
clouds rather than second-level cloud
entities so the vision that I have for
cloudlets is that you have today's cloud
unmodified all of the major vendors are
up there but you have at the edge what I
consider to be second level cloud
representatives these are cashing sites
which cache virtual machines state from
the cloud which may have data cache from
the cloud you can blow one up role in a
new one it reprovision x' from the cloud
and provides you the services you need
so that's the vision and notice how its
vendor agnostic so you don't think of a
Wi-Fi access point as being a Android
one or an iOS one the same way I think
of cloudlets is being bent and neutral
that they are completely agnostic
regarding the specifics of the vendor so
think of this like a CDN except it's for
computation what Akamai did for data we
need to do for computation so on top of
cloudlets to build the kind of
applications that are introduced at the
beginning of the talk wearable cognitive
assistants we have built an architecture
called Gabriel Gabriel is an angel so
it's like a little angel that sits on
your shoulder and offers you helpful
hints as you go about your business so
let me walk you through I'm going to
point at the picture unfortunately I
don't think there's any way well maybe
there is if I can use my mouse Candace
can you see my mouse moving none on
screen no all right I'll this is one
case where the people in the room will
benefit we have here the sensor data
being screened from the video device
from the Google glass it goes to a
collection of virtual machines that all
together constitute
a ensemble that's working on behalf of
this user this is pretty heavy wait stop
turning on a data center a small data
center close one
each of these thick black boxes is a
virtual machine and each one encapsulate
some piece of very complicated software
such as object recognition or speech
recognition that may have been written
for diverse environments some may be
written for Microsoft Windows others for
Linux some have been written in c-sharp
others in Java the reason that you we
use virtual machines just allows the
greatest flexibility in mixing and
matching these almost in a slide-in way
plug-in work the decoding of the sensor
data is done in in by one entity and
then there's a publish/subscribe
interface a bit like a bus
the disseminates a sensor data to these
cognitive engines as we call them the
outputs of the cognitive engines go to
an entity which is the entity that
understands what the user is trying to
do and to help him or her all those the
user guidance VM so the thickness of the
line is roughly the amount of bytes that
are transmitted so you can think of the
raw video data coming in and disseminate
but after your object recognizer says
that's a person there well there's a
rock there but there's a dog there
that's very few bytes okay and the
synthesis of this and the giving of
advice to the user is the essence of
what wearable cognitive assistants is
all about when you need to do this whole
loop from the frames decoding the
recognition the interpretation in the
context of whether the user is doing and
then any helpful advice
this whole loop has to be the order of a
few tens of milliseconds and the reason
you need all the strung on the platelet
the
your your destroyed if you take 70
milliseconds to even get your packets
then okay so this always sounds great
what's the killer application what what
what is it people will really spend
money for will actually want to do you
know they will google lots of ives and
microsoft hololens survives for a long
time what are the kinds of applications
in which it will really really make a
difference let me walk you through an
example so cooking all right here's a
guy wearing Google glass doing cooking
one of the earliest apps that came up
for Google glass was the recipe up so if
both your hands your kneading dough or
your both your hands are dirty you know
you can still still see what to do just
looking up at your display really
helpful but let me tell you what this
recipe app does not do just what it
doesn't do a good friend standing next
to you would say no don't put it in yet
the oil is not sizzling enough that's
something that's because the recipe app
has not is not leveraging the sensor
inputs from the wearable device now all
of you in this room with our exception
have used a system which works like what
I just described okay except you don't
think about it in these terms at all
think of GPS navigation it knows what
you're trying to do you told it where
you wanted to go
it has exquisite detailed knowledge
about the task it's got all the maps
already worked out it uses sensor input
only one GPS it doesn't use video
doesn't use accelerometer gyroscope
audio no just GPS pretty crude
but just that combination is already
useful to you it tells you step by step
what to do and if you mess up like you
didn't take the exit the voice may get a
little panicky but it recovers and says
take a legal u-turn at the earliest
opportunity or whatever the fix is so
there is an example of exactly the kind
of system I'm talking about
except of course it's not wearable
doesn't need to leverage cloudlets it's
a highly optimized one-of-a-kind system
but a metaphor is the right metaphor so
could we generalize this metaphor what
what are the kinds of use cases one can
think of let me just share some
hypothetical ones and then I'm going to
show you some real ones that we have
built ok these hypothetical ones are for
the future think of assembling a
consumer product you go to Home Depot or
Lowe's or whatever the local big-box
store is here you buy a grill and comes
in a big kit with instructions and god
help you that weekend ok if you're like
me I have putted two grills together in
my life and I made a mess of both oh I
care
you got to buy furniture there and you
know you get the stin printed sheet with
no words just pictures and of course
after you put piece a and piece B
together three steps later you discover
you use the wrong piece B you actually
should have used a short one not the
long one and of course once they're put
together you can't separate them so now
you have to call IKS 1-800 number
somebody has to field your call they've
to mail your replacement etc eccentrics
think of how much easier it would be if
I could put on my wearable glass and
IKEA gave me an app for that specific
gadget and just as I'm about to insert
the wrong part
it says no not that one right a specific
closely related situation to this is an
industrial troubleshooting if you look
at companies like Siemens ABB you know
any of these large industrial companies
a huge amount of effort goes into
industrial troubleshooting the amount of
expertise needed is a lot and they're
very few people who possess it and the
ability to share this through an app
kind of mechanism that I just mentioned
would be very powerful closely related
training of doctors so in this picture
the person lying down the patient is
actually a mannequin not even a real
person so this is how the earliest part
of a doctor's training typically happens
yet you need the expert who's standing
next to him because there's no other way
to explain to the doctor exactly how to
do whatever he needs to learn how to do
the scarce resource here is the
experienced doctor to the extent that I
could replace him by an app of the kind
I just described maybe you can't do it
for real patients but certainly during
the training phase with a mannequin
right you don't have to worry about
malpractice lawsuits very simply related
is instruments especially for elders
where they have to take either blood
pressure glucose all kinds of health
care the instruments have to be put on
correctly otherwise that data is invalid
for a senior person who's living alone
unless you have somebody checking on
them to see whether they're doing it
right this kind of self instrumentation
doesn't work very well and the guidance
on these devices is very poor there's
very little incentive for the
manufacturers to give you clear good
instructions but have a good user
interface once it gets FDA approved
you're done so this is another case way
step by step guidance correction from
error etc would be helpful and then my
favorite is of course you know just as
about as you're about to do that evil
act of putting a calorie rich donut in
your mouth
it stops you and says stop don't do that
okay so this all sounds great but we're
a long way from them the amount of work
still needed ahead to get to do any of
these seriously
seriously challenging but also valuable
tasks this is some distance ahead but we
have some initial evidence I would call
these proofs of concept and that's what
the rest of the pot will be how many
minutes do I have Roy fifteen perfect
okay so we just reported a workshop
paper with the most recent Mobius
conference and Florence so then if you
are interested in many details I'm going
to gloss over in the fifteen minutes I
just go take a look at this so about
nine months ago about September or so
SEP 2014 we put together what to the
best of my knowledge is very first
example after such a cognitive
assistance applications let me show you
once the tasks loaded our system would
provide both visual and verbal guidance
to the next step
welcome to the lego task as a first step
please find a piece of 1x4 green brick
and put it on the board
excellent now find a 1x1 white piece and
add it to the top of the kernel
good job now find a 1x1 black piece and
add it to the top right of the current
model
great now find a 1x2 white piece and add
it to the top of the current model
you
now find a 1x1 model once you did
something wrong or system or tell you
how to fix it okay and it goes on so you
get the basic idea that you know this is
what goes on how does it work
here's what happens and and since it's
the first one we built you know we
looked at what we built and then we
built a couple more and the pattern
that's emerging seems to be seems to be
quite similar they seem to be two phases
to the whole processing the first phase
is taking the raw sensor input in this
case the video frame it only use this
video we didn't use the audio or any of
the others but future applications would
you obviously use all of them now this
input is completely unstructured it's
the students office it's sound lighting
like this kind of fluorescent lighting
it works equally well if I do it in a
conference room with a big window and
sunlight so the lighting levels lighting
colors there's no careful control of
this so somehow the regardless of what
clutter there is and what kind of
variability there is in the input you
need to extract this piece that's all
that is relevant to the task and in fact
the way we represented internally is as
a simple matrix step where the number
tells you what color like zero might be
no block one might be white block two
might be red and so on so that small
matrix contains all of the useful
information relevant to the task from
this many many pixel video
frame so we think of this almost like
analog to digital conversion you have a
huge state space up here you were much
much more constrained space space
relevant to the task in the extraction
of that in some sense an A to D
conversion is the hard part and then
there's a separate step where you
compare the extracted state to the
expected state and the difference
between the two guides you office you
guidance now how exactly do you process
that frame that turns out to be tricky
so just you know this is not a talk on
computer vision but just to give you an
idea of the processing that has to
happen per frame guess what happens you
take the input image you basically do a
black and white transformation you find
out where the board is then you find you
know that the board is rectangular so
you find a quadrilateral and from the
quadrilateral you can estimate what
perspective correction you have to place
on the board to make it look normal now
you have to find on the board the little
part you typically do that by edge
detection finding out the the connected
component that has few edges and then it
turns out now that if you aren't careful
the fact that this isn't really
two-dimensional these blocks have some
slight thickness it turns out to mess up
the color detection later so we actually
have to add the detection of the sides
and then make it upright and as close to
final step you have to determine the
colors of the pieces and the magenta
areas indicate uncertainty and what you
do in the final step is then use a
weighted averaging technique to then
convert it into the matrix once you have
that matrix generating the synthesized
image is trivial so it's very much like
from here to here is analog to digital
version from here to there is
digital-to-analog and as you all know
the latter is usually trivial it's the
first one that's the really hard part so
it took us none of us was computer
vision expert so Rahul soup dunker who's
a Google employee worked with us
remotely he helped us and guided us took
about four months of our time to write
this this just this part not the rest of
the application so let me show you two
other applications and then I'll wrap up
so we we built this application from
scratch so we asked is there some
off-the-shelf application that we could
take that's already there we didn't
write it somebody else wrote it they
found it useful could we extend it with
Google glass in the approach that I just
described and create a cognitive
assistant out of it
so it turns out that we found this
program called the drawing assistant it
was written by folks at in Rio and
France it's published it whist in 2013
so it's about three years four years old
not ancient
pretty recent and here's what it does in
figure a you can see that the user is
drawing on a tablet this is a a mouse
like thing connected to the computer and
he is shown a drawing on the screen and
his goal is to try and mimic it and
there are many ways in which the system
can help the user sketch so for example
the blue lines are one kind of technique
there's a skeleton kind of technique
just as if you were using Kinect then
there's another approach using grid and
this application offers you the choice
of whichever one you want
um the interesting part is when you draw
on the tablet if you don't do it right
you'll get lines like the red line there
which guide you and say something is not
at the right angle or something is off
any and you can if you understand the
feedback then you can correct it so a
question was to use this today you have
to be tethered if we connect it to a
desktop you have a tablet etc etc could
we make this hands-free in fact if I hit
a surface like that wall on which
presumably I can draw courageous use
that could I use canvas and oil paint
crayons by completely untethering an
existing application and so in fact
that's what we have been able to do so
this is an example of the back end of
the application is unchanged
what we do is we change the front end
this is the camera image from Google
glass the symbolic representation this
is the A to D conversion is extracting
that piece and if that piece that
digital representation is exactly what
the application already expected then
all of its complicated logic works just
as before so let me just show you this
in this application who have viewed a
drawing assistant to help you practice
drawing by observation techniques for
any target object it offers blue
construction lines for a user to follow
and provide corrective feedback
the video because it's too slow
otherwise
once the users sketches are detected our
system compares it with the target
altering our database and show Sunglass
how a user can improve the drawing the
red parts of the feedback outline
indicates erroneous drawings the red
dashed line
illustrates in correct alignment between
corners of the outline polygons
now the user has followed the guidance
to fix the right part as you can see the
resulting outline is much more accurate
our join assistant is also able to
provide guidance when user destroying
with other media for example the user
now is drawing with the marker on
whiteboard the application again
recognizes the users sketches and shows
feedback on the glass
you
okay so this is interesting is an
interesting approach
how many complex logic interactive
systems are there where you could
replace the front end with a system like
the one I've been describing and
completely untethered the user not just
in terms of away from the computer
but even in terms of the medium that the
person uses to interact with so both
these examples you know the latency
argument is there but what happens if it
was really slow now the user might get
forward the user might get a little
irritated but it's not going to be
devastating
so we trying to come up with a use case
where latency was absolutely essential
and so what we came up with was a
ping-pong ball guide to help you play
better ping pong the assumption is you
already have decent hand-eye
coordination
but maybe you aren't good at strategy
about where to hit to the left or to the
right now we're not trying to help a
blind person play or any such fancy
stuff this is this is assuming you're a
normal person and so what we have to do
is detect the ball
detect the opponent in two successive
frames and detect what is happening
which way is the ball headed which way
is your opponent headed and then whisper
guidance before you hit the next ball so
there's very clearly latency
considerations okay and the ping pong
ball detector in the interest of time
I'm going to skip is non-trivial but
here's a few seconds of this
Google spaced ping-pong assistant helps
the user to play better ping-pong by
whispering whether the user should head
to the left or to the right the hint is
based on the observed ball position and
opponent position as you can see we
always find a user to hit the ball to
the direction that is harder to defend
tons of refinements improvements making
the whole thing real but but just think
about that ping pong example there's a
case where latency really matters
telling you the perfect tactic after the
ball has passed you by it's not very
useful okay one last point I don't have
a video to to do this but but we knew
that those are ton of YouTube videos on
on how to do useful things how to make
the perfect souffle or to make the
perfect omelet how to fix your
carburetor how to fix a broken washer
and today to use one of these things you
google for it finds the right video and
then search for it and one of the
interesting questions was could a system
like this just by looking at what you're
doing
suggest the right video it wouldn't be
closed loop it wouldn't be like these
three examples in that we're not sensing
the environment after you start viewing
the video so it wouldn't correct you but
it it's part of the apart way there it's
not close to loop so the way it works is
use a live video stream and then you use
feature extraction and extract keywords
to describe what you might be doing okay
and then you use that to an index
database of YouTube videos
to suggest ones that might be useful
okay so I don't I don't have a live
video of this it actually is a bit too
slow at the moment because this stuff
each extraction step is actually
definitely not real time if you're
looking for a performance problem to
attack there is one that would make a
huge huge difference all right I will
close now with just two thoughts I
started out by explaining many
real-world use cases which I think await
us I'm very excited about this work I
think is opening the door to a very new
use of wearable devices and cloud
processing in a way that we've not seen
before so my prediction
you heard it here said in the next
decade the killer app of mobile
computing will be this class of
applications and the kind of low latency
high computational demand processing
that you need to make this possible will
not happen unless you have pretty
capable small data centers close to the
mobile devices thank you there is a Dory
associated with the calendar item so if
you don't want to speak up I just want
to type in your question then please add
it to the Dory
but if not we can take it over VC link
and in the room we have a question
there's a connection there and
when I think back to the really
tour-guide stuff that we did that felt
like and we try very hard to make people
believe that it was a imagine it's like
your friend you don't need a map people
stressed immensely about this and in the
end we we I was kind of curious with
this temperature it you think it's going
to be doing active you know so dis I
don't know if everyone heard that sure I
repeat the question condesa okay the the
condensed form of the question is so the
person who asked that question was Nigel
Davis who built a system all the tour
tour guide system this was in the mid
90s if I remember correctly was before
GPS systems became popular so I I would
so the question that Nigel had was in
his experience people were reluctant to
trust the guidance from the tour guide
that they felt the need for independent
validation I submit to you that part of
the reason is because they hadn't used
GPS systems before as you get more
comfortable you stop carrying the map
okay so it is possible that during a
transitional period when applications
like this are still buggy people may
still want the printed instructions from
Ikea but if we make these reliable and
good there's so much better that I don't
carry maps today in GPS I trust okay and
it has led me astray on occasion but the
occasions are rare enough that I'm
willing to overlook them so I suspect
that's part of part of what it is but
you're the bigger point is how much
reassuring guidance should be displayed
it for exam
in the Google glass display to confirm
to the user that the system does indeed
understand what the heck is going on and
that its guidance to pick the object X
up is based on on credible logic and and
it's not totally lost I don't know the
answer to that I think we'll have to
build more of these gained more
experience we haven't done any user
studies you know it's a small group my
group that is doing this so I would love
if people at Google were interested in
working with me on this
after all it's using Google glass other
questions so let me
the person at the back who asks a
question said that so far our guidance
was predominantly audio and one could
imagine much richer feedback for example
augmented reality feedback where the
images overlain you're absolutely
correct we have talked about it we
haven't built any simply because it's
taken all the cycles we have just to
build these understands that building
this has not been easy oh we would love
to collaborate with you we can discover
the insights together
you
this is annex another excellent question
so let me repeat for the benefit of the
people who are remote the question is
the examples I gave had very tightly
focused narrowly scripted tasks one can
imagine much much richer experiences
such as instead of teaching somebody how
to draw that precise dog that's on the
screen you want to give them some
artistic freedom and give them some
latitude and give them some long rope
and only rein them in when they seem to
be getting lost these are all excellent
points and the short answer is I believe
you are correct I think those are
legitimate people will want them we just
haven't we just haven't gotten there
it's done right in fact let me build on
that question because maybe this is the
other question you had in mind our
assumption in these initial examples
that we're working on is you actually
want perfect guidance it's like GPS
gives you detailed instructions even if
you have been on this route before it
doesn't learn that you already know much
of it it doesn't cut back on the
guidance you can imagine a tutor whose
job is to teach you to atone and
autonomously do this task without help
you would then need guidance
the the analog-to-digital part would
remain the same but it's the task
guidance part which would have to be
much cleverer it would have to build a
mental model internal model of your
cognitive state and then decide he's
going down the wrong path should I tell
him now or should I let him cook for a
while okay and the first time around
maybe it tells you right away but the
second time around maybe lets you go for
a while modeling the state of
frustration of the user exact
this is a huge area of learning
technology and I have colleagues at
Carnegie Mellon this whole area is
konakawa learning Sciences and they're
learning tutors but what is very
interesting is for example that there's
there's a a commercial product called
the algebra tutor which has been used in
elementary schools in the United States
in LA I think in the Bay Area etc and it
has been shown to improve the grade that
the caliber of the math done by the
students but all of it has to be done
online like the drawing program imagine
we could take the back end of it use
Google glass and now on a whiteboard or
a blackboard or on a piece of paper the
student does the math and he's helped
though now quite you could argue is that
good or bad
is this a skill people should need or
not we could debate that but I think
from a technical point of view will be
very cool
thank you a lot of what that's another
excellent question
beautiful question and it's a question
that has occurred to us like so many
other great ideas and and all those
questions that were asked we are limited
in the resources in terms of people and
time and so frankly our my personal
belief is this is a huge area it's far
more than one research group can do I
would love to have many other groups
work on this in parallel we can carve up
the problem space in different ways the
question you asked is right on target
all computer vision typically involves
ambiguity and resolving that ambiguity
is not easy if I had embedded sensors in
a room like this and I could supplement
the first-person view point which is a
camera from from Google glass
simultaneously with additional
viewpoints a lot of that ambiguity could
be immediately resolved same kind of
observation is true when you do sensor
fusion if I have say an accelerometer
reading a bechara scope reading I've
audio and I have the video and at the
intersection of those you can eliminate
a lot of ambiguity and and those are
things we would love to do but it's more
than a small research group in the
university can do so I would love for
you guys to work with me on this we
should just check see other any
questions are in any other remote rooms
you may need to unmute if you have some
doesn't sound like it I'll just add one
final question then from the room here
so one of the things that you talked
briefly about was sort of the resources
needed for this kind of system and
the one that occurs to me straightaways
is the issue of power and you having to
stream all this this video and there's
the some sub redundancy probably and
what has to be sense where you can
actually do the image recognition but
the benefit of not streaming so much is
that the glass will last longer what are
your feelings about how to approach this
problem an excellent excellent question
from Roy you're right on target they're
the best the best processing you can do
with an image is to discard it because
you have discovered very early on that
there's not enough interesting in it to
even ship it to the float and so one of
the interesting insights we got from
doing exactly what you said which is
trying to both speed up and lower the
energy consumption was this discovery
that in many many cases you can tell
quite easily in a task specific way it's
not you know an arbitrary way but in a
very task specific way whether you
really need to ship the next frame so
for example let me give you a simple
idea think of object recognition it
turns out that if you aren't careful
when a person turns their head the
frames that are captured by Google glass
alert that's that's just the way it is
once you have a blurred frame shipping
it to the cloudlet doing object
recognition on it is all worthless
because it is going to give you pretty
bad results so if I could use my
accelerometer a gyroscope or other image
centric approach to know pretty early
look this is likely to be a blurry image
let me not even bother shipping it it
turns out to be a net win even better so
we published a paper on this at hot
mobile this year and we call this
concept offload shaping that is you're
basically using a small amount of
additional computation on the mobile
device like Google glass to decide
whether it is in fact worth
in further there's an idea called
perceptual hash which is you take two
frames of a video and you generate the
perceptual ash and of the perceptual
hash is close by then the chances are
very good that a human looking at it
would say that they're quite simple and
so the key insight is computing the
perceptual hash is actually quite energy
efficient okay so it's worth doing that
on Google glass because then I can skip
art of 30 frames per second if I can
ship very few because not much
interesting is going on you win huge so
so those kinds of techniques are going
to become very very important but what
is important is also to recognize that
what is important and what is not
important can change abruptly because
the task state could change it could be
that up to now the only thing with
mattered was a certain kind of object
but once you reach a certain task state
it could be now that there are many
other things that become important and
so there needs to be control of the
early discard on the mobile device that
has to be controlled by the higher-level
processing on the floodlit so again
another deep area worth investigating
very deeply okay thank you very much
sachit please</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>