<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Helping Domain Experts Build Better Algorithms | Coder Coacher - Coaching Coders</title><meta content="Helping Domain Experts Build Better Algorithms - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Helping Domain Experts Build Better Algorithms</b></h2><h5 class="post__date">2011-08-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bA-XwDStjng" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hello and welcome my name is Thomas
Roman I work here at Google in search
quality and I want to introduce Frank
hotel I've known Frank since my
undergrad years in the Technical
University of darmstadt in 1998 and we
worked together on a final project that
involved writing in AI for the board
game mancala and since then Frank has
worked on a variety of other
applications in AI so for his master's
thesis in 2004 he worked on so hostage
local search for solving the most
probable explanation problem in Beijing
neck networks and he also worked on
using stochastic local search for the
problem of RNA secondary structure
prediction and all these stochastic
local search methods require a lot of
manual tuning tunings or more recently
Frank has been looking into developing
methods for automatically tuning
parameters for algorithms so for his PhD
thesis in 2009 at the University of
British Columbia and Vancouver he did a
thesis entitled automated configuration
of algorithms for solving hard
computational problems and this thesis
won the doctoral dissertation award for
the best thesis in artificial
intelligence at the Canadian university
in 2009 Frank has also worked on a
system called zet zilla which does
portfolio based algorithms in the
selection first set and which one the
2010 best paper prize for HK / jay-ar so
today would Frank will be talking about
helping domain experts build better
algorithms please welcome Frank to
Google thanks for the kind introduction
right this work is based on while this
is based on joint work with my postdoc
advisors Olga who's and Kevin Layton
Brown and also with several others in
particular my colleague lin chu who is a
main guy behind zed zilla so I usually
like to start this talk by pointing out
some hard computational problems in both
industry and society we're building
better algorithms actually really can
make a difference for example in the
optimization of industrial processes or
the verification of formal properties of
software or hardware and applications
and security or in the sustainable
management of resources that our planet
has to offer but I figure here at Google
you really know that there's important
problems out there and that building
better algorithms is key to making
progress on them so let me jump straight
to the central theme of this talk so
your domain expert in whichever I domain
that you're working and there's problem
instances you work with you trying to
build better algorithms for solving
those problem instances now there might
be a lot of algorithms out there already
and you're trying to figure out which
one is the best to use for your
particular instance set or you're
working on a particular algorithm of
yourself and you have some parameters of
that algorithm that you can change
that's just different heuristics or
different optimizers you could use or
different numerical parameters you name
it and all I'm going to require from
these algorithms or parameter settings
of an algorithm today is that you can
actually run that algorithm on an
instance and measure the empirical
performance and the talk is about
automated procedures to model that
performance and to optimize that
performance so notice that this is
really domain-independent there is some
work on metal learning so if you have
learned machine learning algorithms here
and you have you learn about them that's
called meta learning if you have
optimization algorithms here and you
optimize them that's called meta
optimization but this is really a little
bit more general than that we can any
type of algorithm down here it could be
a sorting algorithm or whatever and you
model their performance or optimize
their performance so I'm going to go
three instantiations of these automated
procedures the first one is while just
empirical performance modeling so you
fit a model that takes as input a
algorithm or a parameter setting off an
algorithm
an instance and spits out something
that's approximately the performance of
that algorithm on that instance the
second one is algorithm configuration
where you have a potentially large space
of parameter settings in some
applications we have a 10 to the 47
different parameter settings and we're
trying to optimize in that space so
that's really a high dimensional
optimization problem try to find the
best promoter setting for particular
distribution of instances while of
course avoiding overfitting in orbit
tuning and the third application is
algorithm selection that's that's
elaborate where we um try to find a
mapping from instances to algorithms so
we're given the new instance we extract
some features and say for this type of
instance that type of algorithm should
work best okay and again all these
methods are really domain-independent
and it works on any kind of algorithms
problem instances and also on any kind
of performance measure what you could be
optimizing is run time of the algorithm
or solution quality or prediction error
or memory of your algorithm used or even
energy consumption and configurable data
structures it's very generic of course
in order to demonstrate that my methods
work I need to ground them in some
applications and today i'll talk about
the propositional satisfiability problem
which is the core problem of course and
computer science and applications in
software verification Hardware
verification and also about mixed
integer programming which is a very
prominent np-hard optimization problem
that's used in industry a lot and i'll
talk about applications in industrial
process optimization and computation and
sustainability now here's an outline of
the talk first I'll give an approach for
algorithm configuration a fairly simple
one that nevertheless does really well
and give two case studies one for
optimizing a Sat solver run for
optimizing your mid solver so that we
kind of know
data that we're talking about and then
talk about empirical performance
modeling not using those empirical
performance models in order to do
algorithm configuration better in order
to find good parameter settings using
these models and about using these
empirical performance models for
algorithm selection all right so you
might be wondering what these algorithm
parameters are that I'm talking about so
I have a very general notion of what
constitutes a parameter so it's really
any type of decision that you make while
you write your algorithm that is not
hard coded that's an option so it could
be a real value threshold or it could be
a heuristic you use for a certain
problem you could use it therefore Urist
'ok you could use an optimizer or a
different optimizer if that's an option
that's a parameter there's different
types of parameters of course from
there's these continuous parameters but
there's also integer and ordinal
parameters and categorical parameters
and these categorical parameters that
those are the ones where you have a
small the big mo fine i typically small
subset set of well you set the perimeter
can take binary parameters are a special
case of that but you could also have
more values such as user ristic a B or C
the parameter space also has structure
for example if you decide to use an
optimizer sub optimizer a then that
optimizer has parameters of itself and
those parameters might have sub sub
parameters so really the space spanned
by these parameters is tree structured
and well also there's a lot of
parameters so this really gives gives
rise to big space of algorithms for
these configurations of of the algorithm
are different instantiations of all the
possible parameters and these
configurations can often yield
very qualitatively different behavior
two different Parmenter configurations
of one solver might be much more
different than the default of that
solver in the default of another solver
so that's why I like to think of this is
actually different algorithms and that's
why we call it algorithm configuration
as opposed to while some people call it
Parramatta tuning when you just optimize
your continuous power matters but it's a
little bit of a different problem okay
um here are some examples of algorithm
parameters I'm going to throw a lot up
on the slide I'm not going to talk about
them here they are so you can look at
your favorite area I don't know if its
control optimization or machine learning
etc search quality there is power meters
and all over the place typical
parameters that that occur very often
are these pre-processing like clean up
your data how do you clean up your data
how do you smooth it do you have what
type of data structures do you use then
which type of optimizers and which
parameters of these optimizes etc etc so
there is applications virtually
everywhere one type of application
that's that's somewhat different but
very interesting in itself I think as
well are these um subroutines in algebra
or different data structures where where
you kind of think well that's a solve
problem how do you multiply two matrices
or something but different
implementations do differently well on
different platforms and once you write
an algorithm you don't want to re
optimize the algorithm for every
platform but maybe you just want to push
a button when you move to a new platform
or as part of the installation of a new
system you want the system to say okay
which platform am i running on let's
optimize for that okay oh and I should
say if there's any questions where I
lose you please raise your hand let me
know i'll try to answer it if it gets
out of hand will defer to later right
questions at this point
good so we want to optimize over this
parameter space here's a more formal
definition of the problem so you're
given a runnable algorithm its
parameters and the domains you also
given the distribution of our problem
instances and a performance metric and
you're trying to find the parameter
setting call that configuration off that
algorithm that optimize a symmetric over
the distribution note that this is not
just a black box optimization problem
but it's a noisy optimization problem
because we only we can run the algorithm
the algorithm itself might be randomized
so the performance metric of running the
algorithm twice will be different with
different random seeds and we only get
samples from these from this instance
distribution so if the algorithm does
well on one instance we don't
necessarily know that it will do well on
the other instance and typically we need
to gather a lot of samples in order to
really be sure that the algorithm is
doing well with a certain parameter
configuration in order to avoid over
tuning the motivation for this problem
is of course to well build these really
general frameworks of algorithms
versatile algorithms that you can kind
of take for that you can use for
different application domains maybe
different platforms etc so you have a
very large space of configurations and
you have instance that a you create a
parameter setting for in instance at a
and instance that be you create a very
different one that does well on that
type of instances and this is fully
automated as opposed to well the typical
manual tuning that people often do and
you can optimize for any type of
criteria maybe you get a very different
parameter setting if you care about
memory then if you care about speed or
accuracy ok so what type of methods
would one use followers in configuration
well there is this discrete parameter so
discrete optimization method
would be a natural point to start for
example sarcastic local surge simulate
kneeling etc or population-based
original genetic algorithms evolutionary
strategies etc but there's also this
noise so you you could also use methods
such as racing algorithms abandoned so
in practice it's going to be some of
both worlds but there aren't really many
many approaches out there some people
few people have studied this problem in
well very narrow domains based on hill
climbing and beam search in the early
1990s but it has really been and there
hasn't been any general method for
general permit optimization with many
discrete parameters until two thousand
seven we first wrote an iterated local
search for it and since then the field
has been picking up and there is more
and more approaches coming out and
people are really realizing that it's
much more convenient to push a button
than to sit in front of your computer
and optimize these parameters yourself
and we know that we know humans are not
good at high dimensional optimization
problems attuning these discrete
parameters many dozens of them but
people still do so this is why people
kind of like this approach where you
just push a button and it's done
automatically that's what it should be
okay so I'll talk about this par my last
rule and about a second work of ours
this map tool first one is param LS and
I'll always for each and a sub-project
I'll have these overview slides and
acknowledge all the co-authors on those
so this is a local search for algorithm
configuration and this is motivated by a
simple manual approach for algorithm
configuration you start with some
configuration you modify a single
parameter time if performs improves on
your benchmark set you keep that change
otherwise you revert back and you repeat
that until no more improvement as
possible or the solution is good enough
or you're out of time and
well notice that this is really nothing
but a manually executed local search in
this parameter space and what Parma less
does is to automate this and to put an
iterated local search on top the divides
getting stuck in local minima so the
iterated local search runs into a local
minimum by the same approach that we saw
before then does a perturbation to get
somewhere else in the configuration
space and then there's another local
search to get into another local minimum
and then it decides which local locally
optimal parameter configuration to stick
with for the next iteration so it
performs a biased random walk over
locally optimal solution and if given
enough time it converges while it finds
the global optimum because a global
optimum is also a local optima now what
I haven't told you actually is how do
you evaluate each the performance of
each configuration here we have two
instantiation of this framework the
first one is well really basic you use
the same fixed end instances to evaluate
each configuration and we have a simple
variance reduction scheme where we use
while also the same seeds etc the second
instantiation is called focus LS and
that only runs many instances for the
good instance for the good
configurations if you have a poor
configuration there is no need to run a
thousand instances on it it is really
really really bad after a couple of
instances you know this is not going to
go anywhere I rather move on to some
other configuration that might be much
better and also for focus LS we can
prove a theorem that well as time goes
to infinity it converges to the optimal
configuration there is one more speed up
trick I'd like to talk about so if you
minimize runtime then you can terminate
a runs for poor configurations even
earlier this is easiest to see in an
example so if the best configuration
takes say 20 seconds to solve an
instance how long do we let this other
configuration run well definitely not
more than 20 seconds because we know at
this point it's going to be worse then
you
configuration seen so far so no need to
run it further this would also work if
you care or optimizing memory or energy
consumption and it probably doesn't
change the trajectory of parham LS but
in practice yields quite substantial
speed ups okay that's all i have to say
for powerless i think the coolest thing
about it is that it actually works and
there is a lot of different applications
already out there and many people from
different groups around the world are
asking us questions about it and how to
use it for their domains um today I
can't can only talk about that many so
i'll talk about sat and MIT those are
two case studies i mentioned so the
first is the SAT solver for forum for a
formal verification and the second one
is a commercial solver for MIT no sad is
the prototypical NP hard problem and
therefore of course interesting from a
theoretical perspective but it's also
important from a practical viewpoint in
particular state-of-the-art methods for
software and hardware verification are
really under the hood based on sat
solvers typically these days so we use
two types of instant sets one for
software education from the my black
garbage and Elle who and a bounded model
checking instance set from IBM the SAT
solver we worked with what's called
spare state-of-the-art research solver
and it has 26 parameters so 16 numerical
ones but then also 10 categorical ones
three of them boolean and seven these
heuristics which can take up to very
many values up to 20 values and these
parameters give rise to over 10 to the
17 possible configurations and that's
what we're going to do optimization in
we're running each configuration on an
instance can if it's a hard instance and
a bad configuration take hours so the
pretty daunting problem and when we
started this reburn quite sure whether
this is going to work but it actually
did where
in the end particularly because when we
find good configurations then they're
fast so not all these runtimes are that
slow so what we did is just run focus I
less for two days on 10 machines on a
training set from each of the
distributions and that gave us one
configuration for software verification
and one for Hardware verification and we
compared those to the manually
engineered default by the algorithm
designer and he spent about a week
performance tuning it manually compared
on previously unseen test instances to
avoid over tuning of course and here's a
result for software verification so the
way to read this this is a log-log scale
and if we below the diagonal the
optimized setting is much better than
the original default in particular there
was no instance that it couldn't solve
within 10 seconds whereas the default
sometimes ran out of memory or run out
run out of time after a day so this is
despite the fact that the algorithm
designer here wrote the algorithm and
generated those instances with his own
Callisto started static checker so even
though he had domain knowledge and new
as algorithm he's still um well it's
still a huge high dimensional
optimization problems and humans are not
good at that notice that for some of the
easy instances actually his tool his
default did quite well actually better
than the optimized setting and that is
because well in his tuning he focused on
the easy instances because while the
heart instances take a long time and
that's what many people do you use a
fairly small set it really easy said in
order to avoid these really long tuning
intervals but these automated methods
actually have the patience to also look
at harder instances and to look at many
instances for the good configurations
right so overall this was a five hundred
fold speed up on average
and based on that spare actually won a
gold medal in the 2007 satisfiability
modulo theories competition for the
hardware verification the results
weren't quite as impressive but still
very nice 4.5 volts beat up with the
same trend of better performance for
harder instances all right so much for
sad let me move to myth so yeah um they
didn't do all that well the the default
was not all that bad it wasn't actually
great I must say I'm especially for the
software verification said it wasn't all
that great so using this set for here
did quite okay but we also did one study
where we put the sets together and
optimized on that and well that did
better than the default on the
combination but worse than the specially
configured versions of course so the the
more narrow your instance distribution
is the better performance you can get of
course by optimizing on it yeah this one
here is a fixed configuration /
distribution so we optimized on make 300
instances wrote software verification
got this one configuration ran it here
on 300 other instances and it did well
yes
sorry what is the scale this is seconds
so 0.01 seconds to a day and um I I
think mostly the latter one I think you
we just need automated methods for doing
this I don't think my local search is
any grade I mean it's it's a state of
the art but it's still it's so simple
and there must be better methods out
there and I'm working on developing
better ones but it's a tough problem but
even like even a random search did
actually pretty good random sampling in
the configuration space did a lot better
than the default if you automated so
yeah good question but what you need to
worry about the noise right you can't
just run random sampling and each
configuration you run on three hundred
instances or so that's maybe going to
look at three configurations in two days
and it's not getting anywhere so yeah
yeah so here what we optimized isn't in
fact the average across the instances
well actually the the average very often
works for the worst case because the
worst case really dominates performance
here I mean if you take the average of
100 runs where one takes a day and 99
takes zero seconds the day is still the
important one and you're optimizing for
that but if you really want to drop
demise for worst-case performance you
could do that to it you would just use a
different performance metric as an input
all right let's move on good um same
kind of study for MIT so myth is mixed
integer programming it's very much like
linear programming this is a linear
program except you have some integer
constraints on some other variables so
this combines the power of linear
programming to deal with very many
variables and the representative power
of discrete variables and that's why
it's it's very heavily used in industry
what we worked with was the commercial
lip solver cplex which has been the
leading solver for the last same thing
like 15 years and it's a real piece of
soft grades used by like thousands of
universities and corporations including
a third of the global 500 and um
nevertheless it has all these parameters
and the user manual says oh it's pretty
sensitive to them you might have to
experiment with them to get good
performance despite the fact that they
have all these customers so there is
over 50 categorical parameters the space
is even bigger 10 to the 47
configurations and we gon optimize in
that space we use six different problem
distributions including industrial runs
and some from computation sustainability
and first we minimize the runtime to
find the optimal solution got nice bdubs
factors consistent sometimes not very
large sometimes just a factor of two
because while suplex is actually really
well tuned much better than spare was in
the beginning but sometimes up to 54
problem instances from this wildlife
corridor application instances that must
be somewhat different than what cplex
had looked at before because it just
didn't do very well and the scaling was
again much better for our configuration
we also optimize the min the optimality
gap that you could reach in a fix
runtime so this is a gap between a lower
bound and an upper bound and once the
lower bound hits the upper bound you
have proof that you have the optimal
solution so this is something you
actually care about while you run and
yeah we we also get got nice reduction
factors for that optimality gap there is
a tuning tool that's built into cplex as
of 2070
cause they recognize that they have all
these parameters and customers don't
like that but it's fairly simple it just
evaluate some good configurations and
picks the best one for your particular
instance set and so typically you get a
plot like this you have the default and
after given time the tuning tool spits
you out a configuration so this is your
budget over time and this is performance
and you want to be lower power less is
at any time algorithm and so over time
it gets better and better and it also
beats the tuning tool and this is
actually our worst result and this is
our best result for those wildlife
corridor instances and it turns out that
the cplex turning tool actually doesn't
improve at all over its default there so
these must really be instances that are
somewhat different than what they had
looked at before okay um but
nevertheless this makes the point that
in this space that they created with
with their parameters there are
configurations that can handle these
instance as well and if you
automatically search for them you can
find them good um ya notice a locks
guilty of course now um let me get back
to the tar um so I talked about
algorithm configuration and gave the
case studies now i'll talk about
empirical performance models and now we
know the data so we're done with the
data on empirical performance models and
then using empirical for these two
applications good one one question we
often get from Al Gore's and designers
is well now you found this great
configuration and it's doing really well
why what's important what which
components do I need to work on to
improve that we can actually answer with
these empirical performance models
that's one example application so here's
a definition of the problem your give
them the configuration space and some
feature space so you have some features
for each instance that characterize your
instance somehow and I'll assume that to
be given I'll talk a little bit about it
next slide a thing
but mostly in this talk out just assume
it to be given and you have some data
having run the algorithm with different
configurations on some instances and
collected the performance metrics and
you're trying to find some function that
map's from the joint space of
configurations and instance features to
while something approximating the
runtime or the performance metric in
general the motivation here is we want
to explain the empirical algorithm
performance and we'll also want to use
these in order to better optimize the
algorithm and in order to pick the best
configuration or best algorithm model
for instance basis there is some
previous work here so these empirical
performs models actually extend word
from kevin leighton brown you gentleman
and europe show which focused mostly on
predictions in this instance space so
the type of instance features that they
proposed were some they had a long range
of like a big range of features i'm only
giving a few hear something like problem
size you can for a SAT instance you can
look at different representations of the
instance different graph based
representations variable graph class
private variable across variable klaus
graf you can importantly run different
algorithms you could run a local search
for a second and extract some statistics
how quickly does it hit a local minimum
how quickly does it improve you can run
late research how many unit propagations
does it do how many clauses does it
learn etc so you can in machine learning
these are called landmarking Urist
landmarking algorithms and so you can
quickly gather some characteristics
about the instance and these are all
polynomial time procedures that you run
and it's it's fully automated takes a
few seconds typically if the instance is
not too huge to get some knowledge about
the instance and the models that they
used while they tried a bunch of
different models but what they ended up
with is a fairly simple model rich
regression which is just linear
regression with l2 regularization um
but with some bells and whistles so they
had a quadratic feature expansion and
all these features and not all of them
are important so they really had to use
some sort of feature selection technique
and for with regression you can actually
implement a linear forward selection
very very efficiently with rank one
update so that's what they used and we
compared that to different other models
so first retrogression we put in a 1 and
k encoding to handle these categorical
parameters and compared it to gaussian
processes gaussian process is I had to
put in a few bells and whistles we have
really large data sets so like a hundred
thousand data points or so so I'm
Gaussian processes which are cubic kind
of don't like that too much so you need
some sort of approximations i use the
projected process approximation and i
had to put in a different kernel for
handling these continuous and discrete
paramedics but it all works fine now so
we can scale to two very large data data
set and then I also use regression trees
which are well known to handle discrete
inputs well and since you're a tree
based approach that can also handle
heterogeneous data well because they're
there directly I'm hierarchical right
and finally random forests which are
just sets of regression trees would but
typically have a much reduced variance
so since regression trees and random
forests might not be all that familiar
to you I'll just give a quick example so
here's some data we have some way of
parramatta one feature to permit a three
and the runtimes so here's one run
another run another run etc and we want
to build a model that maps from these
parameters to that runtime so what a
regression tree does it splits the data
on a single parameter a time it selects
a parameter by minimizing some loss from
greedily so here we first split on
parmita three the red data points go
left the blue and green data points go
right and all you store in this note is
the split right here on you used so that
if the perimeter was red go left that's
all you store and then you recurse into
each of the children here we first split
on feature two then you only store the
split card hearin then you're in a leaf
in the leaf all we do is store the mean
of the runtimes so 3.7 here we don't
have to recurse all the way we can make
this a leaf already taking the meaning
of this and r occurs in the other sub
tree etc and we get a regression tree
now you can use this regression tree in
order to do prediction very easily
that's an example here is a new data
point we just propagate it down the tree
so first we look at parramatta s3 is a
dread oh it's indeed red but your left
then we look at feature two that's 4.7
we go right predictions 1.65 no magic
it's pretty simple and actually
computationally very efficient and it
also works well as we will see now
random forests are just sets of these
regression trees they're simple to build
you the subsample the data t times and
for each subsample you build a
regression tree and you build that
regression tree in a somewhat randomized
fashion for prediction you predict with
each of the tea trees and you get the
mean and variance across the trees done
now how do these methods actually
compare in terms of predictive quality
so I'm going to show some runtime
predictions for running cplex the suplex
default right now on those wildlife
corridor instances on the x-axis here
there's a true runtime and this is
predicted runtime so we want to be on
the diagonal if we're on the diagonal
everything is perfect below the diagonal
be under predict the hardness above the
diagonal be over predict and this is
richer aggression it's not doing all
that badly actually compared to all the
other methods on this data set it's
doing just as well and this is very much
in line with what Kevin and his
co-authors saw before that's why they
stuck with
regression however what we found now is
that once you go to more heterogeneous
data sets where you have a mix of
different domains then retrogression
starts to kind of not do so well so
here's one example compared to for
example random forests it has these data
points here where the easy instances it
just doesn't realize they're they easy
this is two orders of magnitude here in
terms of hardness and it also has a
couple of outliers actually these here
are I only plotted them at 10,000 but
two of them are predicted to be 10 to
the 50 or something a linear regression
it just is not very robust in that sense
whereas random forests are a lot more
robust and can deal with these
heterogeneous data sets because they're
hierarchically splitting the data so
next we'll look at the scaling with a
number of training instances sometimes
we don't have a lot of instances to
train with so here I'm plotting mini sat
on some sub competition data and
plotting the correlation coefficient
between the true runtime and predicted
runtime and we want the correlation
coefficient to be as high as possible
one is perfect and indeed as we grow the
number of training data points we get
better and better predictions this is
actually rich regression and random
forests are out there regression trees
and projected processes are somewhere in
between and this is actually a picture
that we see quite consistently across
domains here's another picture for cplex
on these hetero geez heterogeneous data
set and they're richer aggression really
doesn't do anything until it gets a lot
of the data points because the the
instances are so heterogeneous yeah I
said that now we want to do predictions
in this joint space of instances and
configurations so I looked at data set
where we had a thousand configurations
that we sampled uniformly at random from
this big configuration space and we had
383 instances and I ran every single
configuration on every instance up to
300 seconds and gathered these straight
383 thousand data points and here's a
plot of them this is just the data no
learning yet so there is the 383
instances here ordered from easy too
hard and a thousand configurations here
ordered from bad to good and each plot
each dot here is a runtime so darker
dots are easier so the instant easy
instances are solved by everything
everything black the hard instances
aren't solved by anyone the good
configurations well they're blacker here
then then the bed configurations so
that's the data and as training data
first I used a subsample of 10,000 of
these data points so and just had the
approaches fill in the matrix and here
is the filled in matrix with a random
forest it looks very much like the
training data so that's great but we
can't take that for granted not all
methods actually do that well even
though it's on training configurations
and training instances it still needs to
generalize to other combinations of them
regression trees are just a little bit
have more variance the GP starts to
think some of the heart instances can
actually be solved by the good
configurations and richer aggression
because kind of doesn't really pick up
on how important configurations are it
does pick up on the easy instances etc
but random forests do best here now how
do they do for predicting your new
instances and new configurations so
training data just as before and now the
test data was all first data on training
configurations of training instances we
saw that plot already now we look at
test configurations so these are
different configurations and these but
the plots
look very similar so you can generalize
to different configurations than you
have trained with and you can also
generalize to different instances and
you have trained with and you can
generalize to the joint your instances
and new configurations now this is of
course not always the case here is an
hour worse result for cplex on these
water corridor instances we still pick
up well at least that some instances are
really easy and some are hard and maybe
a little bit of this trend but certainly
not that the good configurations can
solve some of the really hard instances
here so it's not always perfect of
course it's just the model but the model
is pretty useful typically so let me get
back to this domain here because I have
an example for that for quantifying the
importance of the parameters of spare on
this data set I'll have one slide out of
math I'll go very quick will rewrite the
model predictions this is a model
prediction for its inputs 12 k as a sum
of components as the average response
across all the data points you've seen
plus the main effect those are effects
due to changing one parameter only plus
interaction effect between parameters
plus higher order terms and we can
compute these components actually very
efficiently and then we can decompose
the variance we can say how much of the
wiggling of your predictor is due to
which component so we can explain these
well all this variation where does it
come from does it come from parmiter 10
permit that you are from the interaction
of parameter 1 / meta 2 etc so you go
about this by just writing down the
variance the variance by definition is
the predictor minus a predicted mean
squared
and integrated over all the inputs and
then you do some math and outcomes come
some terms for each particular effect so
here's the effect of parameter one
integrated over just parameter 1 squared
plus the effect of parameter 2 squared
etc plus interaction effects plus higher
order terms and so we can say how much
of the variance is due to each of the
parameters and importantly for this span
algorithm on IBM 93% of this variance
was actually due to one single parameter
which in this case was the variable
selection heuristic and people know that
the break we'll select mystic and SAT
solvers is really important so what we
can do is plot the average predicted run
time for each of these options and we
can only plot this well this really is
it's an exponential some it's the
average run time its average over all
possible values all other parameters
could take so it's an average over 10 to
the 17 terms but we can compute that in
linear time in regression trees actually
it's really nice by just doing some
operations on the leaves and filling in
that data but let's get back to this
actual example so these heuristics here
these options 027 they were
activity-based heuristics which look at
how often is a variable used and how
often does it occur in learn closets etc
and all of these heuristics are pretty
good they give you average run times
except this one Urist 'ok well that
actually was picked the least active
variable so really these activity
heuristics are good here and that's
that's useful information for the
algorithm designer there is some
occurrence heuristics here that there
are just static heuristics about how
often does a variable occur in the
formula and then some other heuristics
that are miscellaneous now let's compare
this plot to the same plot on another
data set
you have a very different picture for
the software verification set these
activity-based heuristics while some of
them still do well others don't do very
well anymore and in particular you have
this one parameter setting here that one
heuristic that does well for software
verification but does not well for
Hardware verification this was a
heuristic that just took the variables
in the order they were created variable
1 2 3 etc and that's a really trivial
heuristic how could that work well it
can work for some encodings because the
order of instances actually has meaning
and nevertheless a domain expert we
talked we had really no idea which
heuristic is going to work well here it
was like well one goes top down versus
the encoding graph the other goes
bottom-up I don't know which one would
do well the other one the reverse one
was actually your a 617 which did the
worst and he didn't know before half
right so after gathering this data we
can learn a model learning the
regression of the random forest on
10,000 data points maybe takes a minute
or so and then we can generate these
plots in about a second and tell the
domain expert here this is ninety-three
percent of the variance explained that
that's where it comes from and this
heuristic here is the best and that's
that's really useful information for
domain experts um this is really new
work but I really hope this this is
going to change a bit the way that that
people deal with algorithms that they do
more experiments over time and just look
at plots like these explaining them
where does my performance come from all
right I should hurry up but actually
these these are only gonna take like
five to ten minutes so i'm going to use
empirical performance model for outdoor
some configuration i already talked
about algorithm configuration that is
find the best parameter setting for a
set of instances and now we're going to
use models for doing that this work is
motivated by bayesian optimization
methods in particular efficient global
optimization ego algorithm by joe
Charlie Welch which works as follows so
it's for optimizing some black box
function here's a function I can only
plot a 1d function so here's a parameter
X we want to minimize this respondents /
parameter X using these models now so
ego starts by a sampling by taking an
initial design typically let in hyper
cube design and getting the performance
value is of that black box function and
it's a black box function so really this
is all you see you have some values and
you're trying to optimize this so what
you do is fit a model here for the
random forests model and then you
compute a so-called expected improvement
criterion that tells you which witch
configurations are predicted to be good
so they're predicted to be good or
you're unsure about them so you you have
an automatic trade-off between
exploration and exploitation exploration
of new unseen regions of your
configuration space and exploitation of
low predicted runtimes for example so
you max you optimize this empirical this
expected improvement credit hearing over
the parameter space and edits optimized
at its optimum you pick a new point and
your run run the algorithm with that
parameter setting you get a new point
and you've done a lot of work just to
get one new data point and you just keep
on doing that now you build a model
again here where you got the new data
point the model changed a lot now you're
going to pick this next and so on and
you iterate in adapting that to
algorithm configuration there's quite a
few challenges one challenge is well
typically these phasing optimization
methods work for a very few numerical
parameters less than 10 and they only do
maybe 100 function evaluations or so for
us for algorithm configuration we have
up to 76 parameters and they're discrete
and continuous and
we do ten thousands of runs maybe and so
the models really need to scale and the
computational overhead for us is
actually part of our budget so if we
have two days to optimize the algorithm
we don't want like one and a half days
of that spent to learn the model so we
really need to be cautious about
learning quickly and using and
optimizing this expected improvement
function quickly the noise is highly
non-gaussian it's completely normal for
a local search solver for set and also
for three search shoulder you run it
once it takes a second you run run it
against it takes and ten seconds you run
it again it takes 0.1 seconds that's a
kind of noise we're dealing with and
there is a lot of noise from coming from
instances as well so we need to use
these instance features in order to
marginalize across instance space
there's conditional parameters so this
parameters structure and if we cut off
algorithm runs early then we only have a
lower bound on the run time so we really
need to work around that and there's not
really a lot of approaches for example
for GPS but yeah we have an approach for
doing that was random forest but I don't
have the time of course to go into
detail there but i'll be very happy to
talk about any of these challenges
offline in the end we have this
algorithm smack it starts very much like
ego it performs some initial runs and
learns a random forest model to predict
the algorithm performance it uses that
model to pick a set of configurations
now not just a single one and compares
each of them to the so far best
configuration using a similar Aristocats
in focus LS and doing many runs for the
good configurations you repeat that
until your time is up and return the
best known configuration we can prove
the same theorem as for focused is that
as if we have enough time this converges
through the optimal configuration we
compared this experimentally to
Parma less and GTA the other two methods
out there gave them the same time budget
and smack perform best only by a small
margin but still up to two folders so
because para el s already found really
good configurations but this can also
find the same configurations faster and
this is still under active development
and we're pushing pushing it further and
further to reduce the overheads of
algorithm configuration we don't want to
wait two days if we could do it
overnight for example all right there is
algorithm configuration finally
algorithm selection we want to select
for a new given instance the best
algorithm that's basically the
definition and the motivation behind
that is we want to take that instance
run it extract the features quickly then
run our algorithm selector and pick well
the best algorithm for it why do we want
to do that well a lot of algorithms have
different strengths and weaknesses some
algorithms work well and structured
instances of the well and random
instances etc and we want to exploit
that and that zilla is one way of doing
that so what's a dilla does during
training it just built an empirical
heart of empirical performance model for
each component and then the test time it
predicts the runtime of each of the
algorithms and picks the best one simple
enough this core approach actually was
introduced by a group at Stanford when
Kevin was still there prominently
featuring Eugene noodle man who is also
a google actually and their supervisor
you have Sean and when Kevin ubc we
continued that work what we did is to
put all kinds of bells and whistles
around to make it more robust for
example we put in pre solvers our
feature computation might take a lot of
time for very large instances and we
just run once all very quickly to
already get a bunch of the easy
instances out of the way where we don't
need to run feature computation some
solvers crash and you still have time
left you
I want to run some backup solvers in
that case and we we have a fully
optimate automated approach rageous
sticking out ger isms and instances and
out comes a portfolio of solvers also
having too many components all of us can
hurt you if your model is actually not
too great for them and by by dropping
some algorithms for which your
predictions of that you might actually
improve the performance of your
portfolio so we have an automatic
procedure for that and most recently be
put in a new classification method based
on random force that also exploits these
hierarchical yet that is good on
hierarchical to her rightful model good
on heterogeneous data sets sorry zed
zilla did really well in the SAT
competitions already the chords our
Zilla did quite well and in 2007 and
2000-2009 we won like 10 miles in
together and 2009 for the first time
that actually included one goal in each
of the categories so in industrial in
handmade and in the random category and
nobody had done that before of course
this just utilizes the strengths of our
components always which were in turn
good solvers for those types of instant
set I in 2011 we only submitted it to
the evaluation tracking on in order not
to steal all the medals from the real
sad ogres and developers actually there
were a bunch of other people holding
portfolios in so half of the metal still
went to portfolio solvers but zed zilla
would have won all I'm cat golden all of
the categories using the portfolio
solvers from 2011's and it closed like
eighty percent of the gap between the
best solver and then oracle picking the
best solver for each of the domains for
random fifty-five percent and
thirty-three percent for the other
categories
finally you can combine this with
algorithm configuration if you only have
one strong solver you can run your
feature extractor then select the
parameters to use for that instance that
is non-trivial because the space of
parametric configurations is
exponentially large but what we do is to
run algorithm configuration to find some
complementary configurations and then
selection using those configurations and
this always improve performance at least
somewhat and sometimes very much
depending on how heterogeneous your data
set is okay let me wrap up I talked
about automated methods for modeling and
optimizing algorithm performance three
methods empirical performance models fit
this model I rossum configuration found
the best configuration for particular
problem set and others in selection pick
the best solver for each instance
individually we can use these
performance models in order to quantify
and visualize on the importance of each
parameter fire was in configuration I
showed two methods primal essence Mac
and two case studies for Saturn myth
where we got orders of magnitude
improvements and in algorithm selection
we looked at zed zilla and hydra which
did really well in the SAT competitions
and can combine others in selection and
configuration in future work I have a
lot of ideas for algorithm configuration
you can do algorithm configuration on
the cloud parallel algorithms have a lot
more parallel or algorithms the non
parallel algorithms and you can also
exploit the massive parallelism in order
to do algorithm configuration faster
finally I like to collaborate more with
domain experts and we've started a
variety of projects and to improve the
state of the art in various various
areas more generally I think really more
and more complex
distance will have more and more
automated methods for optimizing their
parameters because computer time is
getting ever cheaper and human expert
time is not so I really think that
automatic configuration and automated
methods in general will be a growing
field and I'd be thrilled if there's any
applications at Google for sure so is
that thank you for the intention and
thanks to my co-authors any questions
yeah yeah so so the question is on two
types of parameters one top level type
of parameter and then subpar mattress is
that the question or okay yeah oh okay
yeah right I see ya very good question
so the question is you have parameters
that act in certain stages of value
algorithm is running it's not just your
algorithm runs it's a black box but
really it's a it's not a black box at
the grey box you can kind of have checks
over the course of the algorithm and
some parameters only come in later we
haven't looked at that my intuition is
that yeah of course it would be a lot
cheaper to optimize the later parameters
because you don't need to do as much
work and smart methods would exploit
that and what exploit all the work you
have done up to there and not redo that
work went once you evaluate a new
configuration that only changed one of
those parameters
um yeah interesting problem right so the
comment is all algorithms are kind of
like that and you have control over the
algorithm if you can control the
compiler I haven't thought into that
level of detail of yeah really really
changing your compiler options i mean
there's actually work out there in
optimizing compiler options as well yeah
yeah i think that would be an
interesting challenge for algorithm
configuration that you know with
potentially massive gains because you
would say free doing a whole lot of work
but i would have to think more about
yeah
right so the question is have you looked
at the log online who may be online
change your parameters and well what
we've done is a bit of that we have we
have looked at the lock to actually
generate features we run cplex for a
short amount of time and extract
features from the lock but so far it
hasn't been an online process we get
some features one so the beginning and
we use that but there is work out there
in controlling your algorithm parameters
while you run but it's a lot more tricky
because now you don't have a clean
learning problem anymore clean
supervised learning from you have a
reinforcement learning problem really
because you change your actions and and
really if you solve an instance in the
end it's now you have to attribute that
success to the changes you have made and
so you really you don't have this
configuration did this well but it's
this configuration first then that
configuration then that then that that's
off the instance in this time so that's
harder right
mmhmm yeah that that is interesting and
and that in fact does seem like like um
a clean learning problem where you just
you collect the data while you run and
then just predict how long will you run
from here you're well you have to have
features that are time invariant right
or you have to have different models at
different time intervals you probably
don't want to do that you probably want
to have time invariant features it then
it's really about how much progress am I
currently making there have been some
approaches for trying to figure out how
much progress is being made next sat o
meter for facade but yeah I haven't seen
any any breakthroughs and for local
searches gets even harder to really make
sure how much progress you're making
yeah
that's a good question have you tried
polynomial problems not so far really I
mean we really starting to look at them
we had one student looking at sorting
but you know that there was more just
under grad it was the summer project but
I think very very similar methods should
work for home for polynomial time
problems and actually some other people
have done some work on on polynomial
time problems more more in terms of
selecting the best parameters on a per
instance basis but you know all these
configurable data structures or
something that's all polynomial time and
I'm just not a domain expert in it and I
haven't really had a lot of interaction
risk domain experts for those polynomial
time problems that also have a lot of
parameters I know a lot of theoreticians
but they're like yeah I have one
parameter and maybe actually one of them
is getting really excited about this
work because configurable data
structures are getting hotter and hotter
because for example for energy
consumption on your android phone you
might want to have very different data
structures than on your computer and you
don't want to go to and them everywhere
you want to have an automatic method and
they're starting to put more parameters
in but they don't have all that many at
this point thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>