<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Collecting, Analysing, and Exploiting Failure Data from... | Coder Coacher - Coaching Coders</title><meta content="Collecting, Analysing, and Exploiting Failure Data from... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Collecting, Analysing, and Exploiting Failure Data from...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TIqcp0yR8X0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">first of all thanks for the invitation
to come here seems that there's a lot of
related work going on here so we are
excited to be here and chat with you and
so what I'm going to talk about here is
work that I started about a year ago
when I started working on my postdoc
we've got and the basic motivation
behind this work is actually almost
trivial and I think I'm preaching to the
choir here sis reliability is we
important answer some reliability to be
important for several reasons first of
all failures can be very expensive even
a short time of downtime can be
expensive for businesses also system
flakiness is a major source of user
frustration so this is really nice study
that has been done in Great Britain of
college kids and what the study says is
twenty five percent of college kids in
the survey have seen peace taking their
computer and two percent claimed that
they have hit the person next to them in
their frustration at some point so
system reliability is important and it's
a major problem in today's systems the
bad news is that a lot of people expect
this problem to become more serious in
the future and the reason is that the
current trend in building systems and
you will know this pretty well i think
is to scale out instead of scaling up so
systems have many more components and
many more components means that many
more components that can fail
potentially so if you take one example
this data that we get from Los Alamos
National Labs for from one of their high
performance computing clusters this is a
class of that has around a thousand
nodes standard commodity hardware for
way as a piece and they observe about
100 node outages per month so this is
the scale they currently have they are
working right now to get up to petascale
that means 1,000 times more components
and the question is what petascale
computing that is mean instead of 100
outages per month when we have 100,000
outages per month that we need to deal
with so system reliability is a big
problem and a lot of you fear it's going
to get worse
so what makes it so difficult to make
systems reliable so first of all if you
want to make systems more reliable you
have to understand what makes them
unreliable so what do failures look like
what's the most common root cause of
failures what does the physical
properties like the time between
failures and unfortunately phase are not
very well understood and we're not the
only one complaining about this there
are quite a few researchers that have
said in the past a lot of work and the
area of system reliability is based on
anecdotes or simplistic assumptions so
why is this it's not that researchers
are just oblivious to this problem or
ignorant it's just that there's hardly
any data publicly available that you can
use so when there's a companies have
data but it is sensitive 8 and they
usually don't share it so researchers
are stuck with with anecdotes and
simplistic assumptions and that's
exactly what motivates our work so our
research agenda that we started working
on last year is basically tried to
collect data from a lot of sites and
making it publicly available to create a
public failure data repository and the
long-term goals are to analyze this data
to learn more about what do failures in
the real world really look like and then
how can we exploit this data so other
any statistical properties for example
in the data that we can exploit to make
systems more reliable or to manage
systems in a better way for higher
availability and I will talk briefly
about all the three topics I'll focus on
the second topic but as you want to give
you brief overview over the first and
last topic as well it's a starting
starting with a public failure data
repository that we want to build so what
we've been doing in the last year's busy
talking to a lot of sites and trying to
convince them to give us failure data
and we have been successful with doing
this at three different sites and we
have data that spends 26 large
production systems at those three sites
and some of this data is actually at
this point already publicly available in
the in the long run we would really like
to see this hosted by usenix useless he
has agreed to collaborate with us and
that help is hosted at one neutral place
where we can put all this data so that
is accessible for everybody so giving
you a brief overview of what data is
that we have
so the first data said that was made
available to us it's really a cool data
set that comes from Los Alamos National
Labs and the reason that it's really
cool is that first of all it spends nine
years of data it covers a lot of systems
that covers 22 high performance
computing clusters with 7,000 nodes in
total and it contains a record for any
no doubt which that they had during
those nine years on any of those systems
including good cost information so
that's the first data set we got me
really excited about that we also got
data from two other sites now we got
data from the Pittsburgh supercomputing
center on one of their high performance
computing clusters and from one internet
service provider once remain anonymous
and the data that we got from these two
other side so it's actually almost be
hard to replace with logs and describe
replies blogs so we have been working
with those data sets over the last
couple of months and what I really want
to do in the end of this talk is gives
you a brief overview of what we found in
our analysis of this data and as I said
they're different types of data sets and
I will break down the analysis it
accordingly so I start by looking at
what do cluster node outages look like
Mets the Los Alamos data that spends all
their classes is over the last nine
years and then I look at specifically
storage and hardware failures because
data from the other two sides contains
this type of no nation and in the end I
look a lil bit is statistical properties
and it turns out that many of the
statistical properties are very similar
no matter whether you look at class and
no doubt it is for Hardware data and
that's why i'll do it in in one topic
together okay so let's start with with
the Los Alamos data on cluster node
outages so any if you brief overview of
what those systems look like so that it
actually makes any sense of what data
means so as Alamos has basically two
times two types of clusters these are
all large clusters and they either are
made of standard commodity components to
a four-way as and peas or they're made
out of pretty large individual nodes
numa notes so the interesting part news
I'm telling you this is the majority of
the systems falls into this category so
large clusters made of commodity
hardware so although their usage of
those systems high-performance computing
applications might mean not so
representative of many other systems the
hardware is actually pretty standard
hardware that you see many other
contexts so what's the data that we got
for those systems so I said already this
data covers nine years on 22 of such
large clusters it concise records for
all node outages during this time and
it's a total of twenty-three thousand
records so when we start to analyze this
data one of the questions we had was
just what you fail your rates look like
if you are the administrator of one of
those large systems how many failures do
you have to deal with is it like one
failure per week one page per day a
couple per month so what this graph
shows you here is basically the average
failure pages per year for each of the
cluster system so each of these bars
corresponds to one cluster and the color
coding refers to what type of hardware
was used in each cluster so we cannot
release any vendor specific information
fact we don't even know vendor specific
information but we know that all the
blue systems use the same hardware the
same hardware comes from the same vendor
and the same CPU memory models so the
blue / systems for example heaven t-34
way as in peace that were introduced in
2001 put into production in 2001 as you
can see they're quite different systems
they are also for some green systems
that are made of Numa nodes and they're
much older they're put into use in 1996
so that's a large variety of of
different systems and if you look at the
failure rates per year for the systems
but you know this is addressed a very
big variability right there are some
systems that have a couple of pages per
year there's some like this one that
have almost three failures every single
day node outages every day now what's
interesting is that even within one type
of system within only the boo systems
there's a big variability and that yes
exactly so the reason said that those
classes do a very different sizes so
this lasts more than four thousand
processors this has only 128 processes
and this happens to be the smallest
cluster of the blue clusters and this
happens to the biggest cluster so to
make more sense of this what you really
want to do is normalize by the number of
processors by the size and if we do this
we get this graph what you see now is
that first of all the failure rates are
much more similar across the systems
even though they have different size but
if you normalize it seems basically that
the favorites Rose somewhat linearly
with the system size but if you
normalize the rates are pretty similar
what's also quite interesting a thing is
if you look at a different types of
systems you can across different systems
the failure its a very similar not only
within one type of system I'm even the
red and the blue and the green systems
have relatively similar failure if you
normalize by the size of the system so
so this data was all on how to failure
rates vary across the different types of
systems another question is if you look
at all the nodes in one single cluster
how to favorites vary across the nodes
in a cluster and we assume that all the
nodes have exactly identical hardware
because that's what's the case for those
systems and if you look in the
literature that that that's research
with these type of systems and run
simulations the common assumption said
all nodes are equally likely to fail
these identical nodes so they're all
equally likely to fail so we took the
data for for one of the systems we
checked it for all of them but I'm
showing data for one of those systems
and this is a number of failures for
each of the nodes in this particular
cluster and read some statistical
analysis and we find not the fighting
that they're clearly some outliers here
you can see this without doing any
statistical analysis they like their
kids are outliers like notes 21 to 24
have much higher failure rates than the
other ones and the question is why is
the case they are really identical nodes
in terms of their hardware so we went
back to the Los Alamos we will talk to
them and what they told us is that
those outlined notes are used in a
different way by most of the nodes in
the system are actually compute nodes
those notes I either visualization notes
used for visualization applications or
front end nodes so what this seems to
indicate is that the payer it does not
depend only on what hardware using it
depends on what work will be running on
the notes so what is also interesting
was there a question the y-axis if you
are failing about once every three years
how many have three hundred fans or even
though
so this is number players during the
entire lifetime of that machine up yes
that machine is that happen to believe
this video word system that was produced
in 296 let's still being used today so
yes normalizes by nine years so what's
interesting is that even if you remove
this clearly outlier knows if you look
at only the compute nodes there still
ask you in the distribution so it seems
notes are not equally likely to fail and
for the computers only we don't have a
good hypothesis at this point why this
is why some know just it is may seem
much more likely to fail so those glass
were all dealing with failure rates and
how to failure it's very across
different parameters this data also
included information on the repair times
so how long does it take for for the
different types of systems to fix the
problem so this is exactly the same
graph for those 22 systems showing the
media and repair times in minutes so
again what you see is there's a lot of
variability right there are some systems
that have repair times of 10 minutes
there are some systems that have media
repair times of more than 300 minutes
the trend is different he announced that
within one type of system the median
repair time seemed be very similar but
if you look at the rates across systems
if you compare for example the red and
the blue system those are both systems
of body the similar age similar hat
whether they are toward for way as in
peace respectively still the pair times
the median repair homes are quite
different and the question is why is why
is this the case so one yes so does the
video finally
so the definition of failure for this
part of the talk is no doubt it so
something went wrong in note that
personal to go down and somebody to come
in and fix it it will not take into
account any flakiness that didn't cause
no to go down anything where you know
the application top as he continued
running and the failure that could be
masks would not be required here repair
walkability bothered including the
system would or does it need replacing
the faulty hardware amazing it means
anything amiss anything and that's why
I'm giving media and repair times McMinn
repair times there's a very very very
bility because some problems are fixed
by reboots and other problems are fixed
by if I have replacement that might take
some days yeah yes the one thing what we
were wondering is why for example
between the blue and the red systems
wiser such a big discrepancy in terms of
the repair times since you seem to be
relatively similar systems and one
hypotheses that that the Los Alamos
people told us is that simply priority
so turns out that those red systems I
used to run applications related to the
nuclear weapons program at North Alamos
so it seems that that has put higher
priority on fixing those problems fast
so one really obvious question is if
there's a failure what was the root
cause so what went wrong why did this
mode go down the nice thing about this
data is that it actually does have good
cost information it's for every node
outage the root cause is best buy one of
six categories is hardware software
network environment human and unknown
and if you look at the relative
frequency of those what you'll find is
this graph here so this is broken down
by type of system hardware type of
system and the first overall trend that
you see is that in each of those bars
the green fraction is pretty big green
is hardware and makes up about fifty
percent for all of those systems so
fifty percent of all failures that they
deal with in those environments are due
to hardware another pretty big component
with Ron twenty percent ready to be
consistently software is the yellow bar
unfortunately there's also a relatively
big fraction of blue area and blue our
cases where the root costs unknown on
what this means is we cannot really
conclude that network problems
environmental problems human error are
negligible because there might be in
this blue area but we can conclude is
that hard work seems to be a really
significant factor and software dev me
too and if you look at their
contribution to the repair times instead
of just the frequency the relative
frequency you see the same to an even
stronger even more yes
actually they are quite if you have a
problem maybe I should clarify what
hardware problem includes they are
actually all sort of hardware problems
that can be fixed with reboots because
it turns out at so there's actually more
detailed information they're not only
the six high level categories there's
more detailed information so if it was a
hardware problem what was the hardware
problem what a CPU memory disk what was
it and it turns out that a lot of the
problems there are due to memory but
it's not memory dimm being fried and
having to be replaced it's very often
parity problems so the ECC couldn't
correct the number of errors that has
occurred so you just reboot and those
problems will go away so actually even
within the hardware category there are
quite a few problems that can be fixed
or reboot do you have data on how the
unknown problems were fixed so we in
general we don't have any information on
how any problems were fixed so the so I
can what I just said for example the
memory problems were often are fixed by
reboots and their parity related that is
information obtained by talking to those
people so the data includes information
on repair times which allows you may be
to conclude some things like the repair
was three minutes music was reboot or
not to have a replacement the graph do
you have there
hardware software that's not something
and quite reliable because there's no
temporary by 22 sandy assist America of
twenty percent anyway it's true so so we
cannot say anything about human
environment and network problems because
they could be in there what we can say
is that for most systems at least fifty
percent of problems are due to hardware
and at least twenty percent editing
software so for their applications so
they do it for a lot of cases they
didn't give extra information on
software and what it was and it seems to
be the case that operating system
software problems or scheduler software
problems or hiya system problems seem to
be the major contributors but they are
also a lot of entries that just say
software generally where it's
unspecified what software so I cannot
put percentages on on that exactly
this is all classes at Los Alamos oh
this is 22 clusters at Los Alamos no 140
not for the Internet services we have
hyper replacement logs so we see only
hardware failures and for some of the
assets we know which type we have more
than just disk failures we have also CPU
memory other types of failures but it's
all hardware only I was going to make a
suggestion of related questions of
clarification questions are so much
interesting data here then we could make
this last a couple of hours I have
planned the top for being attractive so
don't worry you would be here forever so
this was a really very high level
overview of our analysis of the cluster
node outages from the Los Alamos data
what I want to do next is look at
storage failures specifically and just
to remind you we have storage data from
three different sites towards failure
data actually most of this data includes
hardware failures in general not only
storage and these pens different types
of rice skazhi and fibre channel we
don't have SATA drives at this point so
we were industry or do you have to save
given the counts of drives and I'm of
ratchet is included usually is on the
order of a few thousand for most of
those data sets and usually the duration
of the data sets is on the order of a
few years so for us that's it's very few
large data sets i know that for for lot
of vendors or people that run the really
large systems this maybe not so large
but glasses was still very interesting
so starting by a very general question
so if you look at this face how frequent
are there complete are they compared to
other hardware failures so the data set
we have our hardware replacement logs so
what what we can look at is which
components are most frequently replaced
among all the different hardware
components so what these three tables
show you is for three different systems
that we have this information for the
top ten most frequently replaced
components and to make easier for you
this is where the hard drives are for
all those three systems
I drives among are among the top three
most frequently replaced components and
in some cases there they make up up to
fifty percent of all components being
replaced so this means this credits are
relatively frequent compared to that of
do you've to the failures of other
components another question is what is
their actual failure rate so if you have
a certain population of disk drives say
a thousand rise what percentage do you
expect to fail every single year so one
way to to estimate this would be all
drives come with data sheets and those
data sheets include a number that this
mean time to failure and for more on why
is it that tends to be around 1 million
hours so I mean time to failure 1
million hours would roughly correspond
to an annual failure rate of point seven
percent point two point nine percent
okay now what we did is we look at the
data and we look at the annual
replacement rates so how many drives
were really replaced in the field for a
given population every year what
percentage was replaced so this is the
different data sets and the AR are in
percents so what the solid in the dashed
line you tell you is the expected annual
failure rate that your computer from the
data sheet the bars show you the annual
replacement rate and what you can see is
that for all datasets basically the
replacement rates are much higher than
the failure is that you might expect
based on the data sheet and PDF typical
values seem to be around three percent
which currently trying to get data from
other sites to confirm it seems that the
three to six percent rate is very common
we have some data sets where failure
rates are where replacement rates are
are even higher on the order of row 62
up to twelve percent we're currently
still trying to verify why this might be
we're trying to verify for example we
know for all these sites the drives were
less than five years old up to five
years old we're not sure at this point
whether these data might contain data
for drives that are older than than five
years
so what this says is the field
replacement process really is quite
different process from what you might
expect based on data sheet mean time to
failure the disks are not replaced
there's no failure there's a very good
question because depend on the
definition of failure right so we know
from all the sides that they replace
this so he went to them the drive seems
failed but how do you decide what the
drive is failed so what you do is you
perform some type of test and you decide
if based on this test the drive seems to
have problems you discard it so what a
lot of disk manufacturers tell us is
that the rates that they see are lower
and this could be for example because
sides are pretty aggressive when
replacing drives because they err on the
positive side why manufacturers maybe
make different assumptions then then the
sides do so basically what this says is
this is the percentage of cases where
people running those those systems
decided this drive to us looks like it's
failed I'm wondering if the disk drug
manufacturers have actually told you
that you can expect a lower annual
failure rate than you're seeing I'm it's
not obvious to me that the MTBF and the
annual failure rate are actually
inconsistent and they're different
things right so first of all the so the
MTTF breeches are made under certain
assumptions so the drive manufacturer
assumes that for example operating
conditions are at a certain level for
example temperature space within certain
thresholds certain handling procedures
are followed and the discrepancy might
just be because for example sites using
the drives abuse them in a different way
than that manufactures were expecting it
during something even more basic than
that just as a matter of statistics is
there a discrepancy oh there is yes
I mean we did statistical tests if you
make the assumption that this data that
we got for any of these data sets you
make the statistical sumption that this
data that's come from a distribution
that had a mean time to failure of 1
million hours you can reject this
hypothesis with very very strong
significance levels I'm reading 99.999
something its significance levels
because I mean some of these data sets
contain like 40,000 drives it is highly
likely yet over a sample size of 40,000
you see a mean that's so far off the
redistribution is mean so one thing that
people often complain about when it
comes to this mttf value that specify
the data sheet is that what the MTTF
value does not capture is that the
failure rates might vary as a function
of the age of drive for example so it's
commonly assumed that that failures for
hardware components follow this bathtub
model where you have some time of infant
mortality then you have a stable life
for a while and in the end you wear out
and four disks it's it's often assumed
that he have infant mortality in the
first year then you have four years with
relatively low stable failure aids and
in the end you see we're out so if you
look at the nominal nominal lifetime of
drive which is five years you expect to
see something like this so one of our
data sets nets the data from the
Pittsburgh supercomputing center at it
does spend exactly five years and it's a
homogeneous population of exactly the
same type of drive that is measured over
five years so we said right so we can
compare what the real replacement rates
look like compared to what you would
expect in terms of failure rates and
this is what we get so what these two
bar graphs show you is the number of
replacements per year for the drive and
the compute nodes and the drives and the
fire system nodes and if you compare
this to the theoretical model it
obviously looks very differently what
looks different is first of all there's
hardly any infant mortality in the real
data we don't see any signs of infant
mortality and
it seems that we're out starts much
earlier than you would expect it seems
that the replacement race clearly are
steadily increasing over time and it's
pretty large factors of increase Minh
even going from the first to the second
year or going from the second to the
third year you almost double your
replacement rates the drives were brand
new and yes yes so the advice were in
2001 all brand new drives of the same
model that were put into the system and
they have been there for the last five
years unless they were placed because
the PC decided they are broken and you
can look at this at a different
regularity also this is the same graph
showing the dead by month of operation
and then you see maybe a little bit of
infant mortality for this data said but
we not much so it seems that the
important factors are really early wear
out and not so much infant mortality if
you want to model failures as a function
of age okay getting to some statistical
properties so why most people think
statistics are pretty boring so why do
we care about statistical properties and
most work whenever you want to evaluate
the reliability of some system design
you may have to make some assumptions on
on what for example the time between
failures look looks like and the common
assumption is that first of all the time
between failures is exponentially
distributed and the second assumption is
that failure her independent so we
decided we'll take a look at the real
data and see how realistic those
assumptions we are so starting with the
first assumption the time between
failures follows an exponential
distribution so what we find for both
the cluster node outages and the
described failures the data differs
significantly from an exponential and we
statistically testing with it
distribution fitting but really what
defined as they two key properties that
make it different is first of all the
real data has a much higher variability
so there's variability with square
coefficient of variation of up to 12 and
exponentially has a sweat provision of
one comparison and we find decreasing
hazard raids so what does that mean
just as a reminder basically hazard
rates tell you the probability of a
failure as a function of the time since
you've seen the last failure so if you
look at the expected remaining time
until the next failure on the y-axis and
on the x axis ii of the time since the
last failure if you're at time zero that
is you just have had a failure what
would you expect you would expect that
the expected time until the next failure
is just the average of the distribution
so for this particular distribution this
is a thousand minutes so how does this
change if you have lived for say six
thousand minutes without any failures
how much longer do you expect to go from
the 6,000 minutes so for an exponential
distribution would still be 1,000
minutes one thousand extra minutes it's
a basic property of the exponential
distribution so how does the same graph
look like you look at the real data and
this is the data for the cluster node
outages so what you can see is that the
graph looks very different from an
exponential and what happens is that the
longer you have been living without any
carriers the longer you expect to live
so if you have been living for 6000
minutes without any problems you expect
to live an extra four thousand minutes
so this is one of the key properties
that makes the data very different from
an exponential distribution at the same
holds for this could place me later if
you look at the time between New
placements so the second assumption is
that failures are independent and we did
different tests for correlations we find
all kinds of correlations there's maybe
not surprising that failures I'm not
really independent in the real world you
find autocorrelation find long-range
dependence that's one example this shows
you the autocorrelation graph for
independence where auto place would
always be zero for the real data we have
much stronger levels of curation up to a
correlation effective oh point seven
maybe a different way to show that the
same data is so what does
autocorrelation mean autocorrelation we
tells you what's the correlation of the
number of failures in this time period
as a function of Omega is it as in the
previous time period so if you look at
the number
failures in on the weekly level for
example if I have seen in the previous
week a small medium or large number of
failures how many failures to expect in
this week as a function of that
obviously if the data is independent you
would always expect the same number of
failures for the real data this is the
disc replacement data the number of
phases that you expect in a given week
is very different depending on the
number of failures you've seen the
previous week so there was a small
number of phase in the previous week you
expect point one phase in the next week
compared to more than four failures in
this week if the priests who has a lot
of failures this is basically another
way to look at Auto correlation so there
are strong levels of correlations
basically what we find so the
interesting question is what is all this
wood for so we have this data we've
analyzed it may have found that may be
common assumptions are not are not
necessarily realistic the real data is
not exponentially distributed their
correlations can be exploit this in any
way and there's something that we've
just started to work on this very much
work in progress and that is true you to
ideas that we have looked at where we
think this data makes a difference or
knowing that the real data looks
different from what people commonly
assume so the first example that I want
to give you is high performance
computing systems and the impact of the
fact that the time between failures is
not exponential so first of all if you
talk about high performance computing
systems how do they implement for
tolerance so what they do is there are
these large-scale applications that run
on a large number of processors and
these applications use periodic
checkpointing so every fixed X minutes
they're right back a checkpoint if only
one note that this application is using
fails the entire application has to roll
back across all processors to the most
recent checkpoint so the performance of
this whole thing depends critically on
how you choose this fixed checkpoint of
El you check one very frequently you
have a lot of overhead because you spend
a lot of time writing back those
checkpoints on the other hand if you
have really long checkpoint niveles your
checkpoint very rarely this means that
indicates you do have a failure you
lose a lot of work you have to look very
far back in the back in the past and
people have some methods for for how to
deciding depending on what the type of
the expected time to failures how they
choose is fixed interval so what we were
thinking is is it really the right thing
to choose a fixed checkpoint about so
why do people have this fixed interval
they basically have this fixed in the
hall because they assume at any time the
probability of the failure is the same
so we have a constant failure
probability so we use always the same
checkpoint about what we have found our
work is that so for example depending on
how long it has been since the last
failure the expected time under the next
failure is very different so maybe we
should use this information and we try
exactly this so we try to update the
checkpoint interval based on the time
since the most recent failure so if it
has been very long since the most recent
failure will make the check one two
valves bigger and we've tried this and
we get only moderate savings in the
checkpoint overheads it's around seven
percent one of the reasons I think for
why these savings are some water it is
that you really gain when you're in this
end here when you haven't seen a failure
for a really long time and these are
relatively rare data points so we didn't
get too much they're somewhat another
way of doing this would be instead of
just looking at the most recent failure
and updating check for novels based on
that what if we take the whole last week
into account I look at the whole last
week and I see how many fears have I
seen there and I base my check button to
bail on that and we actually find that
you can have big savings if you do that
you can save sixty percent in your check
for overheads so going back to the disc
replacement data and what might be the
impact on on how we deal with these
carriers so one common way to protect
yourself from beta loss is use a raid
configuration and when you put together
a certain rate configuration you want to
know what's the expected time until I
lose any data on this particular
configuration so betting what this
depends on is the probability that
if you have lost one disc that while
you're reconstructing the data from the
other disc in the rate that a second
disk fails because if you have single
parity as soon as you lose two discs of
out of raid you lose data action you
cannot reconstruct anymore so so the
question is if you estimate this
probability the probability that you
lose a second disc while you're
recovering from from another disk
failure how does this probably depend on
what assumptions you make whether you
use the exponential champion or whether
usually date on whether you take
independence into dependence into
account or you do not so starting with a
simple way so a simple way of estimating
with probabilities taking the data sheet
mean time to failure making the standard
assumptions that failures are
exponential and independent and the
computed based on these assumptions the
probability that within the first hour
of reconstruction you have a second disk
failing and that's this tiny beauty bar
here so this the probability is point
one times ten to the minus three so we
ought to look at the real date and we
did the same estimation based on the
real data and that's when you get this
long green bar which is actually two
orders of magnitude higher than the
purple bar so this simple estimation
wasn't working very well and you could
argue it's because we have seen already
that the data sheet n PDFs are not very
representative of replacements so I
should maybe take the real data and
they've the mean time to failure based
from the data so we did that so we took
the mean from the rear data we still
assume exponential time reading failures
and independent failures and if we do
that we get the blue bar so we're
getting closer to two greater but it's
still it's still pretty far away is
there a question ok so finally what we
did is we we match different
distribution to the rear data and we
found that the weibull distribution is
one good fit so we did the same
calculation with weibull distribution we
take a weibull distribution meta to data
and estimate the probabilities based on
the weibull distribution and that's the
orange bar that you see so you're
clearly doing much better of you if you
make those more realistic assumptions
and that is the K is not only if you go
one hour reconstruction so
the case if you look at the first three
hours of reconstructions or the first
six hours of reconstruction so there's
this consistent trend that if you want
to have realistic estimates of losing
any data in a right configuration it is
very important that you take the
realistic properties of failures into
account and you don't make simply
simplistic assumptions so to conclude
basically in our initial analysis of the
data that that we've been collecting in
the past year what we see is that first
of all enough common assumptions are
maybe not realistic based on this data
the question here are so obviously this
gives you motivation for future work but
what should the future work be so first
of all how do we read more realistic
models if those assumptions we're
currently using and not not realistic
what should realistic models look like
so obviously you could build really
complex statistical models that capture
every single detail but then you lose
all the intuition about your model so
what's the right model that captures
enough of the real word but it's still
such that you can deal with it you can
work with it the long-term goals that
the dream sidin is really how can we use
this data so how can we exploit the data
to build systems in a different way and
some of the things that come to mind are
so can we exploit some statistical
properties like we're looking into
checkpoint in the two slides it at us
just showed you can we do better
checkpointing if we take statistical
properties into account and the long one
can we automate things and do this
management more proactively so if you
have a lot of data can you look at the
data and can you predict that affairs
coming up and that's something for
example it with it you guys in some way
have looked at by looking at smart data
and disks and this could be done it at
very different levels in the systems but
that's something we're just starting to
work on and the basic concludes my talk
yes so some of you have more failures in
the past to do that they write the
future right but and you have another
legend of something Sandoval I think
therefore I keep them do have a true
provision put together it is identity of
the dating your nose that you know I
have covered those that say of this way
next week I like to see won't also there
and how many hundred notes there is we
have you experienced so what i have done
is i looked at the autocorrelation at
two different way so i look at it taking
the entire system like this entire
cluster and i looked at the times that I
see any failure in this cluster and that
shows autocorrelation I also looked at
only individual nodes so if you take
books on one of those fondant nodes
which has a higher failure frequency and
you look at the auto collision only in
within this data stream only focusing on
one of those nodes and you still see
auto correlation and in general also the
statistical properties are relatively
similar if you look only at an
individual node the the average pay rate
is higher for example for the fondant
node but a lot of the basic statistical
properties like the decreasing hazard
waves the high variability
autocorrelation they seem to remain the
same
so if you showed that the user video had
a presence lots on veteran now was
really commentated heart murder yes they
were so soft yes maybe you know one
knows maybe use more complex software
additional software maybe they used it
earlier there's many short requests many
users logging in for short time doing
something and then going away rather
than running these long running one big
application but we don't yes we don't
know that so we're trying to get more
information on what exactly the workload
looks like and what they provide us with
so far is job logs and they do contain
information on for example the cpu
utilization and who the job came from
which is the job came from how many
notes was in things like that but we
haven't really had a chance to look at
that and trying to call her correlate
which which factors of usage to affect
the failure rate
one was getting those any mass scale
deaths are both temporally and spatially
because most of these analysis
yes we do see that so I looked at
trentham supra note note outages in the
HTC clusters I look at correlations
between notes and I mean obviously for
example network failures you have a
really high correlation because network
pages do 10 who do they got a couple of
notes this quite some correlation for
software failures as well and I don't
have a good explanation for that and it
tends to happen in 28-day period so it
might be something that these are the
days in which they upgraded software but
I don't know that for for this price you
definitely see many more cases were disk
fail almost instantaneously then you
would expect it but just statistically
random you fail I don't have an update
at this point to say why some of the
disks fail almost at the same time where
we have gotten enough for summer
temperature data from the pc and we have
gotten data on one day and power outages
because i think that the power spikes
that come with power outages might cause
some of his behavior
one of the slides you had early on
several
it opens
looking at the usage of the customer I'm
very hungry
yes yes so we have a different
indications of how usage might affect
thing one is looking at funded notes
versus compute nodes and they have a
very different behavior but it's true
that over time usage behavior changes
and we actually do also see other didn't
show this in the graphs that the number
of node outages in a cluster that's
changed quite dramatically over time and
it's not the in the intuitive way
necessarily we expect infant Metallica
and it goes down no for some of those
classes actually it starts low then it
goes up and one of the hypothesis that
this is because of usage that as the
system gets used more also the failure
rates go up five years of dinner how did
they learn specific about the root cause
and in pigs and that caused the village
to go down so I don't know any specific
anecdotes I know for sure that they
learn more about root cause analysis as
time goes on if you look at the
breakdown of root causes the fact of the
age of the system in the beginning in
the first couple of months almost all
the root causes have an unknown mark
there they couldn't attribute it to one
particular root cause and as time goes
on they are much much better able to say
what was that would cause the case of
the table he provided he was like one
big bucket for hard work hey what I'm
asking is because their regularity there
that's do CPU memory hard drive power
supply mother so you're asking whether
this break down the motorcade breakdown
changes over time because they it may
change to the system to fix things
anything for most of the high
performance is very hard to do anything
about it so for example let Allah most
is at a relatively high elevation so
they have more cosmic rays and they have
made more bit flips and others and
there's what causes for example more
memory and CPU promise you to this this
issue but there's not much that they can
do about it other than deciding that
maybe next time they use a different
hardware because they have for someone
cpu model that happened to be very
susceptible to this kind of problems
because it wasn't protected very well
the next system they use a different
type of cpu but it's hard for them to
fix anything when it comes to those
problems
any periodic oriented operation
maintenance repairs there were like the
Los Alamos data that's contained
information on maintenance work and we
didn't count on to just due to
maintenance as out of the study include
any of the making its work here it would
be entering though to maybe correlate
how the time points of time when they
did maintenance how they correlated to
the known outage that they observe and I
haven't done that analysis yet or is
there any cell system that we're making
upgraded versus Thursday right stay
there for the whole night I haven't
specifically looked for that but I
wouldn't think so so I think that those
systems are managing a pretty consistent
weight so it's the same policies that
are used for all these systems so I
think that this is ms Woods would see
the same maintenance policies for
example but I cannot say that for sure
diagnosis tools for detecting errors
change over time for change
so my impression is that a lot of the
diagnosis tools that they use our tools
that they built themselves and obviously
all these systems come with tools that
the vendor provides and these will
change because they use different
vendors and if their new system those
tools are different but we just get some
error logs for example out of them that
were collected by some of those vendor
vendor delivered tools and it turns out
that they don't use this data for
example at all they seem to really rely
mostly on on the software that they
build themselves to monitor systems and
I don't know whether they've changed
anything in there like policies of what
they wanted her and how they make
incisions over time but I could find it
out obviously effects of what they
consider as for example advanced
advanced tools on when it comes to those
high level policies like how do you
attribute a certain outage to one of
those categories they have used
consistent ways of doing this throughout
the years and thankee consistent
policies also cross all the cluster
system so they they have weekly meetings
where they operate and staff and
administrators me and they decide these
are the categories and this is our
procedure for deciding which cat we are
given no darkness falls
is any religion to be the see patterns
and in this video seats I would like to
know the answer to I don't know because
in this data we don't have information
on the usage of drives and likely
coalition with a positive place so
correlation with power supplies might
might be there so we know we know that
anecdotally power outages have a
correlation with failures I haven't
verified this but I have the dates of
power so i can do this now let's have to
find the time to do it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>