<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intelligence in Wikipedia | Coder Coacher - Coaching Coders</title><meta content="Intelligence in Wikipedia - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Intelligence in Wikipedia</b></h2><h5 class="post__date">2008-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cqOHbihYbhE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning it's my pleasure to
introduce my friend and colleague Dan
weld the towels like the next 15 minutes
to introduce Dan I've got a lot to say
you all know him everybody here pretty
much knows Dan mostly from his long
distinguished yay I career so I thought
I'd spend a few minutes talking about
his work in databases that see very few
of you know about he did some work in
databases as well Ashley dan has done
working so many fields and computer
science is pretty amazing now he's into
HCI and and stuff like that so he's
quite a very broad guy Dan comes from
University of Washington he's an
entrepreneur in his free time or used to
be an interpreter in his free time he
founded net bot and then a very
wonderful company called nimble a few
years ago 10 years ago actually he's a
fellow of the triple AI ACM and I'm just
a great guy so I'll let him do the rest
of the talking today thanks um thanks
salon it's nice to see you've mellowed a
little bit in your inner time and thanks
for not being quite so so candid
although I did enjoy enjoy working on
the database projects with you I hope
you'll come back to teach me some more
so I wanted to talk to you folks about
the intelligence and Wikipedia project
which is going on at UW and as you can
see from sly there there's quite a few
people who've been working on it but I
wanted to highlight Fay woo and Raphael
Hoffman who've been doing most of the
work in fact I see that I'm supposed to
put some pictures in there to tell you
who they are but apparently I I forgot
to do that I was working on the talk
last night in this coffee shop and they
they closed now earlier so i finished
the talk in my rent a car in the parking
lot and it was a little awkward place to
place to work so just before i start
today I'm going to be telling you a
little bit about some of the work having
to do with the web but there's two other
projects that I'm involved with quite a
bit as well once collaborating with some
folks at NASA Ames on automated planning
and the other one that
refer to just a second ago is on
intelligent user interfaces so not too
much talk about those in this talk
although there's a little bit about the
user interfaces so the motivating vision
here is probably apple pie to you guys
and it has to do with trying to make
search quite a bit smarter and I guess
the premise behind our work is that we
really have three additional components
to make search smarter one is
information extraction another one is
ontology and the third is inference and
I hope after the talk you guys can tell
me how how we're wrong on on this but
we're interested in trying to answer a
wider variety of queries where there may
not be a single web page that has the
answer so given a sort of a little toy
query which German scientists started US
universities you might find some page it
says Hein Stein was a german-born
theoretical physicist another webpage
that Einstein was a guest lecturer at
the Institute for Advanced Study in New
Jersey another page says New Jersey
state and northeastern region of the
United States put these together and a
human obviously knows that Einstein is
an answer to that question but can we
make our search systems come up with
those kinds of results as well so coming
back to the to the three the three main
components information extraction what
we're imagining is being able to go out
and crawl over these web pages and
extract tuples from the natural language
text there we also want to build a
taxonomy for example realizing things
like physicists or subclasses of
scientists and finally we want to be
able to do inference to put all these
facts together one key component there
is realizing that Einstein referred on
one page the same individual as a
science fun and another page and
furthermore we want to do this at web
scale and so while self supervised sorry
well fully supervised machine learning
works fine for information extraction in
many cases it doesn't work we want to
scale this to the many thousands of
relations that we need to to get to sort
of encompass human knowledge ok so our
premise is that what we should do is
start with Wikipedia and there's quite a
few reasons why one is it's a
comprehensive information source is
high-quality and most importantly
there's a lot of useful structure that
makes it much easier to extract
information from Wikipedia then from the
web as a whole so one key thing is every
important concept or entity has its own
unique identifier which is the URL of
the page describing that on Wikipedia
and whenever another another article
refers to that end a day typically it
links to that page giving you a unique
reference another very important fact is
notion of info boxes and just out of
curiosity how many people know what an
info box is okay well I'll give an
example in just a second um there's also
category and enlist entities on
wikipedia which gives sort of
rudimentary tagging information in fact
this know this information is really
noisy so it's not as helpful as we would
like there's also a bunch of other
structure for example every Wikipedia
article the first sentence is very
stylized and tells you an awful lot
about the object being talked about
redirection pages are a source of
synonyms disambiguation pages we're also
useful for for doing coreference finally
although about twenty-five percent of
the pages in Wikipedia English in fact
there's over 200 different languages in
a few just focus on the top 10 to 15
most popular languages you find that
much of the most important content is
written in several different languages
and we believe we can harness sort of
the correspondence between the pages in
those different languages to do
extraction with a higher precision and
then finally there's a revision history
so every change made to Wikipedia gets
recorded and by looking not just to the
current version of the page but how the
page has changed over time gives us
quite a bit of important information
about it so it would be nice to be able
to do complete extraction from the web
and in fact what we believe is if we can
learn to do extraction and get a lot of
information out of Wikipedia we can then
use that to bootstrap ourselves to the
web we'll come back to that point a
little bit later in the talk but we
argue it's much much simpler to do it
from Wikipedia to start with
okay and then there's a whole bunch of
things that make this difficult one its
natural language text to even those high
quality there's a lot of missing data
inconsistencies and most importantly
there's low redundancy so quite a few
strategies for doing information
extraction rely on the fact that any
given fact is repeated out there on the
web many many many different times and
that's not really true and Wikipedia
itself because most things are stated
once and just and just once okay so
here's an outline for the talk first I
want to tell you about our self
supervised extraction scheme then I want
to talk about automatic taxonomy
generation and talk about probabilistic
inference how we can take these things
and put them all together and it's sort
of the eye line for the talk as you'll
see i'll be skipping around a teensy
whip but i'll come back to this slide to
try to orient you okay so i'm going to
start by talking about the self
supervised extraction and the corner
store the idea there is to train on
infoboxes so let me motivate that so
with traditional supervised information
extraction or use supervised learning so
we start out with raw data and we get
labels for those pieces of data for
information extraction we might have a
sentence saying currently based
Microsoft is the largest software
company since we're interested in the
relation headquarter of a company in a
city we would label the sentence with
there's a city name and sorry a company
name and a city name and so on and we
could use that as a training example we
also have a bunch of negative examples
saying that sentence doesn't have any
information about that particular
relation so we take that training data
we feed it to our learning algorithm
when we get in an extractor and the
problem is where do we get the label
training data it takes a large amount of
work and in our case we don't want to
even fix ourselves to a fixed set of
relations at the beginning so if we
haven't if we want to be able to handle
sort of an unbounded number of relations
how the heck are we can get this
training data we certainly don't want to
go out there and ask people to do it so
is there some other way and our
technique is basically to use heuristics
to generate a noisy training set so our
heuristics come from this particular
Wikipedia structure so what we want to
do is go from infoboxes to a training
set and it's very simple so
most many many different articles in
Wikipedia have what's called an infobox
here's a little picture of one it
typically occurs in the upper right-hand
side of the page is a little tabular
display of the key attributes with the
relevant values for that article so here
we have an article on Clearfield County
Pennsylvania and we find out what the
population is when it was founded with
the land area isn't a bunch of a bunch
of other things like that and by doing a
simple heuristic match we can see ok
well the value of the date that this
county was founded is 1804 and we can
find that in a sentence and if we can
find a unique sentence in the article we
can automatically mark it up and say
well that's probably a positive training
instance for the relation which
corresponds to this attribute of this
class so in this case the class is us
County and we've got a bunch of
different attributes so those are our
relations and we get our training data
so pretty straightforward there's a
couple challenges one is this
inconsistency so sometimes we're going
to miss training examples because you
know somebody updated the language of
the natural English text without
updating the info box or vice versa also
you can see there's cases here we got
28.2 km/h din the info box so there's a
number of you know a little challenges
to make this whole approach work but the
basic idea is pretty straightforward so
taking that idea the the initial kyland
system basically has two parts a
preprocessor and some classifiers and
other kinds of learning algorithms so we
go from Wikipedia we were fine the
scheme adda so we go through Wikipedia
pages and we say oh it looks like
there's a lot of pages that have a u.s.
dash County info box class in them so
that's an interesting class of entities
and now let's look at the attributes and
it turns out that the different county
pages some of them have some attribute
some of them have other attributes some
attributes are only used on one or two
pages and so the first thing we do is
figure out what the relations that are
mentioned frequently enough that we're
going to try to learn them and then we
use that heuristic idea that I just
described to
to build this training data set and then
we're going to learn these classifiers
and the extractor so how many every I'm
giving this talk a couple other places I
assume everybody knows precision to
recall so why don't we skip right
through that and I just gonna be talking
about the area under the precision
recall curve is the measure that where
we're interested in measuring so my
apologies i am going to talk a little
bit more about the classifier in this
document so one thing we're going to do
is we're going to learn a document
classifier and that is for every we want
to be able to see every document that
comes through we want to be able to
predict what kind of info box class it
would be if it has an info box of course
it's easy so we're interested in the
pages that don't have info boxes in the
back because we essentially want to
extract the information that will let us
build it info box secondly for every
sentence we want to be able to predict
which attribute might be might have its
value described in that sentence um and
so if we've got you know an infobox
classes we need to learn and document
classifiers if there's k attributes /
info both class we need to learn k and
sentence classifiers and then for each
document class and attribute we need to
learn an extractor and we're going to
use conditional random fields for that
okay so pass this stuff so that basic
idea and there's a bunch of details I'm
sweeping under the rug because I want to
get on to some of the more new material
that that basic idea worked really quite
well and on popular classes like us
county or university or a number of
other ones we got some great great
precision as high as 98 little over
ninety eight percent and the recall was
pretty reasonable to up to to say
ninety-five percent depending on the
attribute certain types of attributes
were harder than others so we were
incredibly excited when we published the
paper it was only afterwards we came to
serve the obvious conclusion which is
this works great but actually in the
classes where you don't have quite as
many existing infoboxes the whole idea
doesn't work so well and unfortunately
looking a little bit more carefully
there's a long tail behavior so pretty
much all of the classes
don't have very much training data so
forty percent of the infobox classes
have less than ten info boss boxes in
them it's pretty hard to train with with
ten training examples even if especially
if you're generating I'm heuristic Lee
so if we want to really be able to cover
a wide variety of knowledge here we need
to come up with a different scheme so a
natural scheme to use is shrinkage so in
particular if we're interested in the
infobox class performer there aren't
very many instances of performer but if
we knew that performers were subclass of
person and actors were a subclass of
performer then we might be able to use
the training data we have four actors in
the training day we have four people to
what to help us learn how to extract the
different relations of performer
particular they probably still have the
birthing probably all have a birth date
and they probably all have a spouse
relation and so on and the problem is
that Wikipedia wasn't really engineered
by anybody and so each of these schemata
really they don't line up naturally and
so one thing we need to do is we need to
have this taxonomy which Wikipedia
doesn't have for us and we also need to
have be able to match the attributes of
each of the classes together but if we
had that we could do shrinkage and it
would be great so coming back to the
outline what I want to do is talk about
how we automatically generated a
taxonomy like this and then i'll show
how we were able to improve our recall
enormously using shrinkage so here is
the architecture of our cog system which
appeared in dub dub SS spring and again
we use a very similar schema cleaning
approach and then we try to detect izi
relations and the basic idea is to build
is to use markov logic and networks to
do a big you know joint inference
problem and we're going to try to
determine when we've got a sub sumption
relation at the same time that we're
trying to map the various classes into
corresponding wordnet classes and then
after we've done that then the next
phase is to do a schema mapping where we
match up the relevant attributes for the
parent-child classes
and focusing on that for a second
basically we treated as a binary
classification problem and the
interesting part here is in the features
and we use lots and lots and lots of
features going into them in detail is
pretty darn tedious so there's sort of
the obvious string features and
information retrieval methods there's a
mapping to wordnet we also use what I
call Hurst patterns which is basically
looking using web search results to come
up with matches two phrases like you
know X is a city you get information
about classes and subclasses by by doing
that as talk both in Marty hearse
original paper and also the the
know-it-all work done at UW and then
actually one of the most important
features is comes from this revision
history that wordnet has so it turns out
you might have an article on Albert
Einstein and it turned out initially he
was you know when the info boxes first
came around somebody may have created a
scientist info box class and Einstein
might have been a scientist info box
class and then later it got changed and
in the class of of Einstein's info box
got changed to physicist and so on and
when you see that kind of a change in
the in the revision history it gives you
a strong clue that scientists and
physicists are closely related you would
think that things always get specialized
but in fact just as often they move the
other direction but it certainly is a
strong clue that there's a parent-child
relationship even if you don't even if
you don't get a completely perfect
signal on which one's apparent in which
one's the child gets the other thing you
might expect would be that the number of
attributes would sort of be strictly
growing as you go down in the taxonomy
and that also turns out not to be true
nevertheless there's a huge amount of
signal and these features and so setting
up the learning algorithm we actually
looked at a couple different learning
algorithms but I'm not going to go into
details there we're able to learn this
nice nice taxonomy
so sorry here I'm going through and
illustrating that example of using the
revision history and again we used an
SVM we also did this joint inference and
that worked better so skipping past
those results let me talk about the
schema mapping so here we did a little
bit more of a quick and dirty job and
basically we have a bunch of heuristics
so looking at the edit history and also
sing string similarity gives us a huge
amount of clue we've got basically three
rules we apply them in order and that
tells us how to match up to match up the
attributes and the different in the
different scheme otta and overall the
approach works really quite well our
precision was ninety-four percent a
recall was eighty-seven percent and so
the final taxonomy we generate is really
is really pretty good in fact what we'd
really like to do is instead of first
learning the sub sumption relations and
then going on and trying to do the
schema mapping it makes sense to do both
of those at the same time in a joint way
and so that's one of the things that
we're working on right now the problem
being of course getting the whole thing
to scale yeah recall numbers those are
on a leaf / link evaluation or is that
yeah that's a link / link evaluation to
be honest I can't remember whether it's
just looking at this absorption at the
izzo relations i think it's just looking
at the is relations as opposed to the
schema mapping as well but that also
worked quite well as long as you got
there is a relation right are there
other questions
okay so now that we got this ontology
let me come back and show how we can use
it for for shrinkage and the basic idea
there's really only once you've got the
ontology the real question is how do you
actually wait the examples from related
classes and we considered a number of
different approaches including trying to
wait the fact that if you got lots of
data then that's good or trying to look
at what your confidence was in the
parent-child relationship and giving a
higher weight if that work we also tried
looking at the extractor suppose we've
learned some extractors for actor we
could actually try them on the 44
examples to see how well those
extractors are working on the
corresponding on the corresponding
attribute of performer and if they work
really well then we could wait these
examples you know much higher and we
tried a bunch of those different things
and to be honest it didn't really matter
all that much but the net effect was and
I'll show you a slaw an experimental
graph in just a minute the overall
effect was it really worked extremely
well but since i'm giving it over you
talk i want to talk about this next
technique we did as well it's called
retraining and the basic idea is that
when you're learning just on Wikipedia
text oftentimes the Wikipedia pages are
quite stylized and one example would be
an article on Albert Einstein would say
something like Albert Einstein
parentheses and it would give one year
dash another year parentheses you know
is a you know physicist or something
like that and our system given a bunch
of training examples like that quickly
realizes that if you've got the first
sentence and you've got two things that
look like dates inside parentheses with
a dash between them then probably you
can get the the birth date and the death
date that way and since that's the way
birthday then and up here is typically
in Wikipedia pages you tend not to get
any training examples to say you know
Joe Blow was born in nineteen sixty and
so if you later find a page where it
said that we're gonna have trouble
trouble extracting that so with
retraining what we do
is we take the extractions from Kyle in
with ones that were generated by text
runner so this is system by oren etzioni
and his colleagues and i'm guessing that
many people here know about how text
runner works come in to see hands a text
runner how many people don't know what
text runner is okay so text runner is
also interested in a very similar
problem to what we're interested in it's
oren likes to call it open information
extraction so again going out and
extracting information off of the web
without actually trying to provide
without providing any training data for
the specific relations but instead of
using the self supervised learning
approach that we do with Wikipedia to
come up with to automatically build a
training set for each relation what he
does instead is it looks for grammatical
patterns that are independent of any
relation so you might say subject verb
object in a case like that you can sort
of extract a tuple where the verb
denotes the relation and the subject and
the object are the arguments of those of
those tuples he's got a number of other
patterns like that he's able to learn
them off of Wall Street Journal corpus
and then he or rather Michelle Banco
also has built a CRF extractor which
just goes for those high level patterns
of this is probably a relation here and
it extracts the relations one of the
arguments so it's a very different
approach but it's tackling a similar
problem so and the main benefits I'll
come back to this a little bit later
what are the main benefits of the text
or approaches its you know very very
general one of the weaknesses is can
they get their precision as high as an
approach like ours so what we're Justin
doing is taking those tuples so they
might find some sentence out there that
says albert einstein was born you know
on does anybody know when he was born
I'll say 1960 because that's when I was
born so Albert Einstein was born on 1960
and what we do is we basically look at
the extractions that we've found and
that we've taken as our training data
off of the info boxes and we look at the
things that the similar kinds of tuples
that text
runner has learned and when they match
then what we're able to do is get that
sentence that extra one are used for its
extraction and take that as a new
positive example for our training set of
a relation specific extractor if they
don't match we throw it away so by doing
that by sort of looking at the sentences
the texture owner was able to extract
the same kind of thing as we've
extracted we get more positive examples
of sentences and they're labeled and
also we're able to to get rid of some
sentences that we have been assuming
we're negative training examples
previously until we can get rid of those
so retraining basically lets us augment
the training data we're using when
training our relation specific
extractors so if you combine those two
techniques shrinkage and Retraining then
here's what we get so over here here's
the performance of Kyle ins extractor
using the base technique on the Irish
newspaper class and after applying
shrinkage and Retraining you can see
that the area under the precision recall
curve is I don't know what five thousand
times better and down here you can see
that on the writer class kailan did
better but if you apply shrinkage and
Retraining then we still get a big
improvement to the area under the
precision recall curve so what's the
difference well in writer we had two
thousand infobox instances that we could
use for our training set for irish
newspapers we only had 20 and soak
eyelid wasn't able to do very well with
just those info box instances and that's
really where shrinkage helps enormously
so and here what are we doing we're
shrinking from from newspaper general
newspapers you know should there be an
Irish newspaper class I don't know but
the people who built Wikipedia thought
there should be so we get a fourteen
percent improvement in the area under
the precision recall curve in a very
very popular class and we get you know a
much bigger improvement on a sparse
class sort of what you would expect in
any case it's it's a great improvement
now the thing that's
the thing that's especially good is by
having these retrained extractors we can
do a lot a lot more on them so what I
want to do now is show you how we're
able to bootstrap this technology and
apply it to extraction from the broader
web and why do we do that given that I
went on about how great Wikipedia is you
know with what 2.6 million articles in
the English language and so on well it
turns out that forty four percent of the
articles in Wikipedia are stub articles
are labeled with the keyword stub which
basically means somebody created them
but there isn't you know enough text
there yet and so even if we had a
hundred percent perfect extractor if we
run our extractor on that text the
chances are it's not going to have the
data value we want and we're not going
to be able to extract it so what we'd
like to be able to do is go out to the
web especially for those pages but in
general and be able to find the right
attribute values in fact we could then
go back and sort of start writing
autonomously writing the Wikipedia
article so what do we do well natural
approach is to take our favorite search
engine and and use that and we issue a
query that's composed of the title we
actually tried a number of strategies
here again but basic idea is you issue a
search engine query with a title of the
article and some keywords that we found
tend to correspond to the attribute
we're interested in and again we can't
just take the infobox attribute name
because that's usually some sort of get
rich that looks more like computer code
however in our retraining process we've
been able to find out what kinds of
words tend to go with the birthdate
attribute and so we would have
discovered at that point that born in is
a key phrase that we oftentimes see
around around birthday so we would issue
a query that would look like something
like quote Albert Einstein unquote quote
born in unquote and then we get back
some pages so the challenges are how do
we maintain high precision the pages can
be noisy out there and furthermore in
contrast to Wikipedia many of the web
pages out there we might come back with
a web page which is a great source of
information but it's not just talking
about Albert Einstein it's talking about
you know
all Nobel prize-winning physicists and
so if we're not careful we'll extract
somebody else's birthday and this is a
Kiki challenge again it's the fact that
we're training on stylistically very
very simple pages on wikipedia where
each article is typically about one
topic and now we got into the web we
have to worry about that and then
finally we have to figure out how do we
want to extract this when do we know
that we're confident enough about
something we extract from the web that
we believe it's more more likely than
anything we've extracted from the
wikipedia page so the key the key
solutions here are one make a very very
specific query to send off to the search
engine and to be very careful about
where you extract when you get that page
back make sure you find you extract only
from a paragraph that seems to be very
likely about the target without going
into the details there that you can see
if you want in our kdd paper you can see
the myth again is actually sort of a
recap of the previous slide with a new
slightly more exciting color that shows
the benefit of shrinkage and Retraining
and now you can see how able how well
we're able to do when we bootstrapped to
the web so these are using extractors
trained on Wikipedia generalized with
retraining and now apply to the web and
the thing that's really interesting is
where shrinkage and retraining didn't
really help on the writer class before
because of all the examples it turns out
still that there's a lot of articles
about writers on Wikipedia we're about
pages that we know that our classifiers
determine our about writers where once
we got to the web we can actually find
extra information which we could bring
back and create a new info box for
example so we get benefit on weirdo
pages weirdo classes like hourish
newspapers but also really mainstream
classes so and I guess again I'm not
showing you all the experiments we did
if you try to do extraction from the web
without doing shrinkage and retraining
it just doesn't work at all so it's
really important to do first shrinkage
then retraining and then and then and
only then try to go out and bootstrap to
the web okay so we were really excited
and actually we're excited before we got
the most recent results and we figured
right we're doing extraction from
Wikipedia now what we really need to do
is close the loop will go back and
automatically create new info boxes on
on Wikipedia and so we want to do things
right and so we checked and there's of
course a bot policy on wikipedia that
tells you exactly what you need to do to
hook your software agent up to to
Wikipedia to make contributions on its
own and we you know sent in a very
careful application talking about what a
great scientist I am and how dedicated
loyal we are an hour impressive
precision and recall results and we got
a very prompt response back from them
saying absolutely no way and in
particularly the response says who are
you you haven't made any contributions
to Wikipedia how do we know you're not a
jerk which of course I said I could have
worked up my robot without asking you
but anyway the the response was from you
know Wikipedia master grand master
whatever the title is anyway he's a 14
year old finished boy and so this was a
big blow to my self esteem but we
realized that you know we couldn't just
go back and put these things cuz even if
we've got ninety percent precision
that's not quite good enough to put back
into into Wikipedia yeah what me
personally um now every time somebody
tries to add me to wikipedia i revert it
alright maybe he's reverting it i don't
know but yeah you guys please put me in
there um I think Kyle in is but but not
me ok so we realized that ninety percent
precision is just not high enough to go
in there and so we really need to have
you could try to get our machine
learning better and better and better
but as you guys all know that's you know
it's hard to push out those last few
percentage points of error and so it
seems much more natural why not get when
i get people involved and while the this
finished kid wanted me to to do it i
realized that wasn't scalable and so
what we'd really like to do is get other
people involved so how can we improve
precision with people and here's the
vision so what we'd like to do
and let me take a step backwards I think
what's really interesting is that the
Internet has enabled two technologies
for large-scale construction of
structured information ones obviously
information extraction and you guys know
there's lots of success stories there
another one is community content
creation Wikipedia Amazon reviews and
and all sorts of other things are good
examples there but there's been very
little work on how to put these two
together and it really seems as if you
should be able to create a synergistic
feedback cycle so what we like to do is
use information extraction to create
quality pages if we get a quality pages
and the traffic coming to those pages is
going to increase when people come as
long as you can get them to make edits
that improves the content which drives
more traffic but also those edit
improves the training data which you can
then use to make information extraction
algorithms better so what we wanted to
do is to try to to see if we could get
this feedback loop going and this leads
us to the HCI portion of the talk
basically what we're interested in is
trying to foster contribution as a non
primary task so of course lots of people
have been out there trying to build
power tools for Wikipedia editors to
make it so that it's easier for them to
edit things but what we like to do is
make it much easier for you know Joe
average person who's coming to make a
contribution to to Wikipedia so we'd
like to encourage these contributions
without annoying people and we built
five different interfaces and we ran a
rather unique study to to try to see how
well they work so here is an
illustration of one of our interfaces so
if some person who's just coming not
trying to edit or anything but just
coming to check out ray bradbury they
may hover over one of these buttons then
we pop up a little interface that says
we think that the summary over here
should say the Ray Bradbury's birthplace
was a waukegan is this what the article
says and then the user you know can see
that as highlight up there and they can
say yes you know where they can just
click the bloody thing because they're
irritated at us
and we tried a number of different
interfaces then we try we had to vary
the interface as a number of ways
because the first ones that we wrote we
discovered people were misunderstanding
and again we have a whole Chi submission
on this and I can only give you that the
high-level a version of version of this
but we also did a survey afterwards to
find out whether people were
understanding and we took metrics and
showed exactly how how much people were
we're contributing and the interesting
part of our thing is what we want to do
is we want to get casual users who are
not part of any study just to see if
they would actually contributes while
they're engaged in their primary task
which in this case presumably is reading
about Ray Bradbury so we can't just go
and hire College sophomores and ask them
to come into our lab we really need to
get people in the middle of this primary
activity and so what we did and it
really wasn't intended as charity to you
guys is we bought a bunch of Adwords and
we popped up little ads like this any
time somebody did a search on ray
bradbury or any of the other 20,000
sorry two thousand articles about
authors we would pop something like that
if somebody clicked on her ad we gave
you pile of money and they came and saw
our little mirror version of Wikipedia
which just to be clear also told them
that it wasn't really Wikipedia and
explained the details of the study if
they were interested although they
didn't usually do it and we had
JavaScript to pop up these interfaces
and we went through a bunch of different
interfaces those being the primary for
and we did some variations on that and
we tracked the clicks and the loads and
did the survey as i said before on the
net a function i can say sort of one of
the main conclusions out of this talk is
that you guys are making a lot of money
because I know my grand budget went way
way way down on this when somehow
mysteriously the price kept on rising
for Ray Bradbury I really didn't think
that Ray Bradbury is such a popular a
popular popular thing but maybe the mark
was turning away from mortgages towards
towards science fiction authors okay
here's what we found we came up with a
bunch of different interfaces in some
things you can think about them in terms
of this efficient frontier where the
x-axis is how obtrusive they are and
this is
having really put the unis here but it's
a liquor scale that comes back from the
survey results and then on the y-axis we
see how likely they are to contribute
and I'll just point out in fact out of I
don't know maybe showed about 800 people
our baseline or face the ordinary
Wikipedia interface absolutely no one
tried to contribute based on that
although you can also just sort of look
in terms of how many people who go to
Wikipedia actually contribute and there
it's about 1.6 percent but presumably a
lot of people who are going to Wikipedia
directly are actually going because they
want to contribute and that is why
you're getting that one point fix rate
which is higher than our are zero
percent rate but people who are going to
Wikipedia as a result of a search seem
to be doing a lot less contribution
nevertheless we were able to get up to
thirteen percent and actually I think
we're able to get higher than that with
one of the more annoying interfaces but
with the revive the pop-up interface the
revised icon interface we had thirteen
percent and then furthermore we able to
take the training data that we got from
from people who were correcting things
and we went back and use that improved
training data to retrain our extractors
and sure enough the precision and recall
of our extractors went up as well so we
demonstrated I think both sides of that
feedback cycle when we get the
contribution rate up higher and two
we're also able to get our extractors
working better and there's a lot more to
say about the community side if people
are interested in talking about how this
fits into community organizations
Wikipedia etcetera I can talk about that
but let me come back to the final part
of the talk and this is a probabilistic
inference and this is just a quick
couple slides from steph filmmakers talk
at emnlp a week ago so the high level
idea is we want to be able to do
inference we want to take information
that's been extracted from different
pages and use that to answer questions
and we use a kind of knowledge base
model construction so we have a set of
inference rules specified kind of like
data log rules these drive off of a set
of knowledge bases some of those
knowledge bases or extractions we also
use a knowledge base which corresponds
to wordnet and we do the sort of
backward chaining inference to create a
proof tree we prune the crew the proof
tree and we use that to generate a
Markov logic network so an undirected
probabilistic model and then we do
probabilistic inference over that to
estimate the probabilities of various
tuples in particular the tuple that
somebody was asking questions about and
the result of our probabilistic
inference is a set of answers and
depending on time we can go around this
loop a couple times doing lazy inference
to get more and more answers so there's
basically eight logical rules so not
very many rules each one like they tend
to to say things like certain predicates
are transitive and so on so we're taking
very very simple kinds of inference and
we ran this on about a hundred middle in
tuples that that we extracted from text
Runner and so there's two points to make
one is actually we do get a fair
improvement in terms of the number of
questions we can answer correctly so
roughly doubling the area under the
precision recall curve and the
interesting thing is that the speed of
this process appears to be about linear
in the size of the corpus so we ran an
experiment you know basically giving it
increasing sections of the tuples and
you can see with these correlation
coefficients that the overall time taken
by Holmes the inference system is is
basically linear in the size of the
corpus that's pretty cool but that's
sort of surprised us because we expected
that it would be more like N squared
because you know well that that's the
worst case so the question is why why
are we getting this sort of higher order
polynomial blow up and you can look at a
query you know give me all the people
who were married to somebody who lived
in a particular place and the question
really is how big is that join going to
be and in general of course the join
could be N squared and if you've got
more tuples or more inference rules it
can be a higher order polynomial but
when you get that worst case you get the
worst case when you get some person Y
prime who happened to be married to
everyone and lived in every single place
and that typically doesn't doesn't
happen
and so looking at that a little bit more
if we look at the sort of married
relation you know again we get kind of a
long tailed distribution and there are
some problematic cases here but they're
in the minority and most everybody is is
down here so the thing that's nice is we
can characterize the cases where if the
distribution of our tuples in terms of
how many matches they have once you fix
one argument or the other argument if
you look at that distribution and that
distribution of Bayes certain certain
parameters then we're guaranteed that
inference is going to be linear and in
the simplest case you've got a
functional relationship if everybody's
married to one and only one person then
that's very very easy and we call it
sooner functional if there's some bound
which may be as a function of Y that
says how many people a given person is
married to and then we basically say
well what we're really interested in is
approximate approximately sort of
functional relations where most of the
relations are bounded by some small
constant and the number of bad apples or
bad spouses if you will really are very
rare so basically if you've got at most
log log of those marriage relationships
then you're going to be okay and so we
got a theorem saying if this is the case
if that well if your relations are
approximately pseudo functional then we
can do inference in linear time and we
think that that captures this sort of
phenomenon where we saw that we in fact
did get a linear time behavior and sure
enough you go out there and most of the
relationships are are approximately soda
functional okay so there's a large
amount of related work and I'm not going
to try that go into all of it I do want
to point out the text runner system
which I mentioned earlier done by
michelle banco for PhD thesis and and
oren and collaborators that takes this
very different approach of trying to
learn structural techniques there's
another system done a rochester called k
NEX which does a similar a similar kind
of a thing for a more expressive
intermediate form than the kinds of
three tuples that we extract and that
orange group extracts and then there's
other systems which you can sort of view
in the light of unsupervised information
extraction that extract large numbers of
tables and of course preceding this work
there was a lot of work some we did some
with you can think about in terms of
Google sets that extracts large numbers
of lists and these also can be seen as a
kind of unsupervised information
extraction skip over this stuff for now
skip over this stuff for now and let me
talk just very briefly about the
conclusions and then spend a minute on
future work so what I did is start out
by talking about self supervised
extraction from wikipedia where we use
the info boxes to bootstrap and then we
use shrinkage and Retraining to to get
us better precision recall so that we
can go out and do web extraction talk
about how people are essential to
getting the the precision up high and
how we can maybe do that for free talk
about generating these taxonomy xand
probabilistic inference so so I think
one of the most interesting things to do
going forward and one that we're
thinking about a lot is trying to take a
step back and look at these different
approaches to open information
extraction so we advocated this
relational specific approach and if you
think that there's on the order of five
thousand different info box classes in
Wikipedia and each in phobic class has
approximately ten or more attributes our
approach should be capable of learning
about 50,000 relations which is not
unbounded but it's an awful lot of
relations and presumably quite
comprehensive on the other on the other
hand there's other approaches to open
information information extraction which
take a very different approach a
structural approach they don't try to
learn any specific relation but they try
to extract the relation type as well and
what do they do they look at at
different kinds of structure in the
systems so systems like k next and text
runner look at grammatical structure
then there's also approaches that go out
and look for a Dom structure you can
look for table structure or list
structure and these are all different
ways of extracting you know relational
information but in a very very different
way from the relation specific
so I think the really interesting
question is how do we combine these two
techniques and our retraining method is
one first step at that and it gave us a
lot of a lot of power but we're we're
interested in trying to combine these in
different ways going forward so
particular seems like the racial
approach has much greater precision but
this approach has greater just in
generality so how do we combine those
another thing that I didn't have time to
talk about we have a wisdom paper coming
up talking about multilingual extraction
I alluded to this you've got pages in
different languages and exploiting
trying to do the extraction
simultaneously or jointly in different
languages gives you a lot of leverage
this is just the first step there
there's lots more to be done I talked
about how we learn this taxonomy and how
we did it in a pipeline way first
learning the is relations and then
learning the schemata schema mapping and
clearly one should do that jointly
there's lots of other information
sources out there that you could bring
into the mix and of course lots of work
on schema matching and and so on in the
database community that we need to take
into account and again the key the key
challenge here is going to be
scalability and then the final thing
I'll talk about and this is something
that my student Raphael Hoffman is
working on right now is trying to come
up with a sort of better set of features
to do information extraction in the
first place so if we look at what kinds
of structures a CRF generates we've got
training data like this you build a CRF
and is basically using obviously uses
grammatical features and whether you've
got capitalized words and and part of
speech tags and so on but a lot of
what's going on is learning you know
that Swedish tends to indicate a
nationality for example and as a result
we're not really generalizing very well
so what we'd really rather have you know
when we test on data like this you know
structural is very similar but we may
not do so well what we'd really like to
have is you know a feature that says if
is that word on the list of occupations
or is it on the list of nationalities or
something like that much more high level
features so the key question is where do
we get those lists from and
and so what we've done is mine list from
the web and uh approach to the stuff
that we did much earlier with list
extraction and know it all and what
Google sets does and of course similar
to the stuff like a ferrell is done in
terms of extracting tables so we've got
a set of 55 million lists and now the
interesting thing is how do you come up
with the right the right features so
we're given a set of seeds that come
from the ordinary extractor that that we
learn right now and given those seeds
the key features are the features that
have high you know hi weights on them
after we learn an extractor right now
what we can do is you know pop them into
an interface and then use them basically
do a bunch of matrix multiplies by these
lists to come up with the right list
that have the most important information
going forward so this is something that
we're working on right now and hope to
have results on quite shortly so that's
it thank you guys for coming look
forward to to questions and again let me
thank all the all the people who worked
on this and again especially Faye whoo
who built the the basic island system
Raphael Hoffman who's been who did this
sort of bootstrapping to the web and is
doing the new extraction Steph
Schumacher's who did the work on in
France and everybody did something
interesting there so happy to tell you
more thanks for much yeah
talk about teamwork by automatically
automatically lost to a phylogenetic
you smarter
has hansoms okay I'm not familiar with
that what's her view and then more
recently Michael Collins very
successfully so basically they do with
wall between fostering becoming okay
they don't and they do have some some
nice result so but you working
apparently there do you know where the
Collins paper appeared always
and I'll just ask very reason but it's
amazing probably work that
thanks I'll check for that that's a good
tip
great thanks any other questions great
well look forward to talking you offline
thanks again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>