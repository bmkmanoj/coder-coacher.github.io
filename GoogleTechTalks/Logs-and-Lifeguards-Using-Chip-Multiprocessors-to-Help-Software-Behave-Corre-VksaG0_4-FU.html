<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Logs and Lifeguards: Using Chip Multiprocessors to Help Software Behave Corre... | Coder Coacher - Coaching Coders</title><meta content="Logs and Lifeguards: Using Chip Multiprocessors to Help Software Behave Corre... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Logs and Lifeguards: Using Chip Multiprocessors to Help Software Behave Corre...</b></h2><h5 class="post__date">2008-09-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VksaG0_4-FU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello good afternoon my name is Brad
Chen and I'm delighted to introduce Todd
Maori this afternoon was going to be
giving a talk on logs and lifeguards
clever strategy for making dynamic
analysis tools faster and better in
other ways too I wanted to mention that
Todd's going to be talking today about a
project that was one of the research
academic research proposals that google
has recently funded and and he's going
to be actually as it turns out on
sabbatical at Stanford for the next year
so so folks who are interested in this
should consider themselves encouraged to
engage with the speaker he's actually
pretty friendly he just had lunch with
me and everything and and and yeah i
mean the academic research relationship
is something that we want to you know
try and leverage so so with that Todd
thanks Brad so I'm gonna be talking
about this project today that I've been
doing in collaboration with a bunch of
other people so with some PhD students
at CMU and this is also a project I
spent three years at Intel research in
Pittsburgh they have a lab there at
Carnegie Mellon and this is and the
research that's done there is all sort
of open in terms of IP and everything so
this is a joint project that we did with
Intel people and then one of our CMU
faculty has moved to Switzerland but we
still work with him anyway so since this
was something that we've been doing with
Intel one of the things in our minds was
how do we how do we explain this to
people on a hardware company and
hardware people we're you know early on
we spend a lot of time worrying about
performance and then more recently Power
has been another thing that we worry
about a lot and there are trade-offs
that you have to make between power and
performance you can't have your cake and
eat it too if you want more performance
usually pay for more power and so on so
you to make the right trade-offs there
so that's what hardware people think
about so Intel CPU designers this is
their world what what a software people
care about well
I think the number one thing is probably
correctness is my program doing the
right thing because if it's not doing
the right thing nothing else really
matters probably i don't care if it's
fast and power efficient if it's broken
I also care about performance it needs
to be doing operating quickly enough and
I want to stay within my power budget so
and as I just said I think of these
three really correct mrs. is very
important because if that's not working
then nothing else that really matters hi
jack so so what we're we're focusing on
here is why it is that we're trying to
do something about the cases when
software is misbehaving so why does
software misbehave well one potential
reason is that the hardware's
misbehaving so the software is trying to
do the right thing but the hardware's
flaky so in this case so for example
there could be a transient error and or
maybe it's a permanent error and the
main trick here is to exploit some type
of redundancy either within the
processor or within the system and this
is something that people have done a lot
of work out here at Google and this is
actually not the focus of this project
or what I'm talking about so that's one
important thing is the hardware can
flake out but we're not worried about
that instead the thing we're focusing on
is that the software itself is broken so
this is a fact of life it's very hard to
write program without bugs in it and as
we write larger and larger pieces of
software it gets even more difficult to
come in and modify someone else's code
and make sure that you didn't break
anything and it's not just that even if
you have a bug for some rare corner case
that in nature should never really be
exercised people can still potentially
write an attack to exploit this bug for
this thing that's not really supposed to
ever happen so this is a big problem
that we'd like to do something about so
from the perspective of people who care
about large data centers what the
software miss behavior mean well if it's
due to
a hardware failure well the good news is
that those things tend not to be heavily
correlated with each other if something
dies somewhere hopefully it's isolated
we lose a little piece of the system but
not that much but if it's due to a
software error the problem is that all
the nodes have the same vulnerability
and you could potentially lose all of
them at the same time so that's one of
the reasons why we care about this so
now as you know Intel AMD and AMD are
now racing each other to ship more and
more cores on each chip that they sell
you so we're up to four cores now and
their product road maps are sort of
increasing exponentially over time so
how does this change the world well
first there's some bad news which is if
people are going to start trying to take
advantage of more cores and start
writing more parallel software we know
that that's not an easy thing to do so
that suggests that there might be even
more bugs in the future and not just
correctness bugs but the reason why
you're right parallel software usually
is for performance and there are all
kinds of performance bugs that can also
prevent you from getting good
performance and there are tools for
trying to reason about this but they're
not great generally so we like to have
better tools so this looks like bad news
but the idea behind what we're doing in
this project is we think that there's
some good news here also which is we can
actually use these this abundance of
cores to do something to directly try to
address software the software bug bug
problem and this is really what I'll be
talking about and the rest of the talk
today okay so all right how can we now
how do we know that the software is
buggy so this is actually a really
difficult thing it's very difficult say
for the hardware to figure out that the
software is broken because from
hardware's point of view it's just doing
what it was told it didn't realize it
was being told to do something that
didn't make any sense so the way that
you try to find bugs is typically you
have some model of good behavior versus
bad behavior for example if
acquire a lock I need to release a lock
at some point in the future etc so one
way that you can try to look for these
so you have this model of behavior and
you check your code against this model
and one type of tool is static tools
these are like compilers that take all
of your code and look at all of it and
try to reason about it and these are
really useful tools a good thing about
it is they can consider all possible
executions that might ever occur so if
they say something's definitely true
it's it's true always a downside though
is that they that there are many things
that you don't know at compile time you
don't know much about the real dynamic
behavior and if you have to make
conservative assumptions you may have
you may be too pessimistic so you may
have more false positives than you'd
like to have so it's still these are
useful tools and we want to have these
but what we're looking at is in this
project is something that's
complementary to it which is tools that
actually run dynamically as your
programs executing and we call these
lifeguards because the ideas analogies
it's like a lifeguard at a pool who's
watching over people as they're swimming
so this is a piece of software that's
watching over your program to make sure
it's safe and nothing bad has happened
and in particular we're interested in
the tools that that are the
sophisticated tools that typically need
to look at every instruction as you're
executing so you can build dynamic tools
that only look at say system calls or
procedure calls are things that are
infrequent enough that you can run
quickly and do everything you want to do
but we're interested in the tools that
are doing really very sophisticated
things I need to look at all the
instructions so we like to do is first
of all notice that something's gone
wrong and if possible we would like to
even correct the behavior if that's
pasta an so in some cases like data
races for example you may be able to
notice Oh something wrong but wait if I
back up and reorder things I might be
able to fix the data race and keep going
so our ultimate goal would be to fix
problems so the way that these things
are done today is typically through
binary instrumentation so we take an
executable
will we rewrite it and add in more
instrumentation and their tools like
valgrind or pin that do this type of
thing and people have written many
interesting and useful tools on top of
those infrastructures so that's all good
news they only you know and the good
news more good news is that these things
can reason about dynamic behavior so
that potentially very precise with
respect to that particular execution
that you see right now they can't prove
things about all possible executions
that they haven't seen but they know a
lot about this particular execution the
big piece of bad news here however is
that they are slow when you have a
sophisticated tool monitoring every
instruction you typically see slowdowns
on the order of about 25 or 30 and
Intel's thread checker which is another
powerful tool runs about a hundred times
slower than normal execution so this is
just a graph showing you that two tools
written in valgrind they run much more
slowly than normal execution so what's
wrong with that well because of that
overhead people only use these tools
when they're explicitly doing debugging
so if I believe that something is wrong
or if I just happen to be in some
testing cycle I might take the time to
run this thing 30 times slower than
normal but then what you do is you take
that you shut it down you take the tool
out and then you deploy this code
without the lifeguard so you're swimming
at your own risk so we'd like to be able
to do is we like to be able to run the
lifeguards all the time to do this we
need to reduce their runtime overhead to
the point where it's negligible so our
hope is that you would always have these
things running and they would be making
your program more robust okay so how are
we going to do that well the basic idea
is that we're going to use some of the
cores on the chip to monitor other cores
and I'll get into more detail about this
so and you can choose whether you want
to devote any whether you want to have
any lifeguards running or not you may
decide that you don't want any of the
or you may decide that you want quite a
few of them that's up to you and then
our the idea here is that the
applications are not modified so we're
not doing binary instrumentation they're
just running on bare hardware at full
speed and these lifeguards we can
dynamically attach them or detach them
as you want to so they don't have to be
there all the time they can be there
when the machine is idle enough or your
what your power budget is comfortable
enough or whatever okay so a question I
get sometimes especially at Intel is
well wait we can never do that because
if I'm devoting some coors to doing this
monitoring that means I'm hurting
performance because I can't be using
those cores to run the application and
the answer is well if your speed up
curve looks like this if you're getting
linear speed-up then that's true I'm
it's a zero-sum game it's between
performance and correctness and I have
to take away some performance to get
this even if that's true I may still be
willing to do that because if my program
speeding up and getting the wrong answer
I don't really care about that I'd
rather have it speed up half as much and
figure out why it's broken then run at
full speed and get the wrong answer but
many programs actually look more like
this so they speed up to some point and
then they stop speeding up so in this
case there really is a point where it
doesn't make more sense to throw more
and more cores on it and I'm not allowed
to say that it Intel but but so there
may actually be let's say for example
that I have 16 cores and my program is a
parallel program but it's not really
speeding up much beyond four cores so
that means i may have 12 other cores
sitting around and maybe i would want to
run some lifeguards on them to check the
robustness of my program okay so that's
the basic idea so what we're really
doing is adding another vector of
goodness that a system can play with so
hard where people are used to thinking
about performance and power
and now we can use some of these cores
for correctness also we didn't invent
this idea but that's what we're working
on and then you can make choices in
terms of what trade-offs you want to
make about between power performance and
correctness depending on what you care
about you may care almost entirely about
performance so you just want to run as
fast as possible and don't run any Life
Guards or maybe things look very broken
and we want to run a whole lot of Life
Guards or maybe we're very power
constrained so we want to back
everything off and that's just up to the
end user to make those choices and just
to illustrate this same idea a little
bit differently there's this whole
spectrum between whether you care a lot
about performance or correctness where
you can use all the cords for running
application or use many of them for
running lifeguards and so on okay so the
idea is to get the raw horsepower for
running these tools from other cores on
the same chip but there's a lot more to
this than just getting the raw
horsepower so in particular these tools
are very tightly coupled with the
application that they're monitoring they
typically need to think about each and
every instruction that's executing in
the main program and so they have to
somehow observe this dynamic behavior at
a very at this very fine granularity
without disturbing the execution the
main program that's part of the reason
why binary instrumentation is so slow
because getting in there and cooling and
all the information has some significant
overhead the other thing that we would
like to be able to do in some cases
whenever it's possible is to actually
fix a problem so if we notice
something's wrong we don't want to just
crash and say oh now I've got an even
fancier core dump for you to explain why
the system just went down we would like
to actually fix things or isolate the
problem so that it doesn't take down the
whole system for example and then the
idea of the the project is are the thing
that we're exploring is we think that
logging an architectural e supported
logging mechanism is something that
would give us both of these things it
would allow us to
observe things that a fine granularity
and it would allow us to rewind and
intervene when we needed to so to
illustrate this the idea is some
applications running on some processors
and the lifeguard is going to specify
what it wants to see maybe it wants to
see every instruction and it needs to
know the data addresses maybe it only
needs to see memory references whatever
it needs its going to specify this to
the system the system will capture this
and put it into a log which you can then
subscribe to so this information is
getting pushed over somehow to the
lifeguard and you want to do this in a
way so that there's you're not feeling
any of the overhead on this side and
it's not very difficult to do this with
a little bit of hardware help and the
nice thing about the log is once you
capture the log you can multi cast it
out to any number of lifeguards so maybe
they're looking at different things and
you have to take the union of all the
things that they want to see but
hopefully many different life guards
that are looking for completely
different things could be attached at
the same time again without slowing down
the main program so let's that's the
idea so that's how we can use the log to
do fine green observation the other
thing we want to do is to deal with
rewind so as we're monitoring something
sometimes when we observe a problem a
bad thing has already happened so the
system is in this process of crashing so
in some cases like if it's a transient
hardware fault or say buffer overflow
there might be a way to deal with that
so let's say it's buffer overflow what
I'd like to be able to do is somehow
back up the system before the point
where that malicious packet came in kick
it out of the system and then keep going
forward again so if I roll back I might
be able to fix things and keep going so
a log is is also a mechanism that can
help us do this rewind along with
checkpointing and other things so that's
why we think that logging is very
interesting here having said that I'm
not going to talk that much about the
logging itself i'm going to focus more
on the lifeguard and consuming the
because that's a more general problem
and that's where the real challenges are
so in terms of this work where we think
of it in terms of four different phases
and the first phase is what we've been
concentrating on so far and then we're
in the middle of the second two phases
and then the fourth phase is coming up
next so the first thing we were trying
to work out is if we were willing to
throw as much hardware at this as we
needed can we reduce the overhead of
having these fine grain lifeguards on to
the point where we could run them all
the time and the answer is yes we can
get the overhead down to something
that's fairly small as you'll see
shortly the next phase which we're doing
now is okay given that we could do that
with some non-trivial hardware can we
back off the amount of hardware support
and still get the same benefit by being
more clever in software so that's
another thing that we're doing the third
thing that we're doing in parallel with
this is where we're specially interested
in monitoring parallel programs because
we think these are the programs that
probably need the most help and it turns
out that there are some complications in
trying to monitor parallel programs and
make sense of what's going on and we'll
talk about what we're doing for that and
then finally looking ahead one of the
reasons the reason we're doing this is
not just to take existing tools and make
them faster we're hoping that with this
infrastructure where you could do this
in real time the people might dream up
new tools that we don't even see today
that could do even better things to make
software more robust so here's an
outline of what I'll be talking about it
more or less follows what I just
described so first I'm going to provide
a little more background about
lifeguards so that we can understand the
rest of the talk and then I'll talk
about the more Hardware oriented stuff
that we've been doing briefly and then
I'll discuss the ongoing work that was
the subject of this google grant that
Brad mentioned and talk about future
directions so just to set a little
context here I'm this is not the kind of
talk where
coming to give you all of the answers so
this is actually a talk that's happening
relatively early in the stage of the
research we're doing and the reason why
I want to do this is to get feedback
from you to help us guide the research
and make sure we're doing things that
you care about so although we've worked
on the hardware part and I'm going to
describe that to you I think the thing
that's probably more interest here is
what we're work just starting to ramp up
and that's the thing I'm really
interested in talking to you about I
don't have a whole lot of results here
just preliminary results and I'd like
feedback on that great so first I'm
going to talk a little more about
lifeguards and their structure so
lifeguards typically do one or more of
these three things so let me first
describe what often happens when
something goes wrong so I'm executing
along something goes wrong but typically
I don't realize right away that
something has gone wrong until something
goes really wrong so now thing crashes
and I'm in trouble so one thing that we
try to do is detect that the system is
in a bad state and ideally we want to
detect that early we don't want to
detect we'd like to detect it right here
we don't want to detect it at this point
so you can think of this as a search in
checking or looking for invariants and
trying to check them efficiently so the
next step would be okay detection says
things are bad what I'd like to do next
if I could is to figure out what caused
it to go wrong so here you can imagine
looking back through the log and
analyzing the code hopefully with some
automatic tool to say aha actually this
is the root cause of the problem and if
we could go back and do something about
that then none of this other bad stuff
would have happened downstream and then
finally if we understand the problem
well enough and we have a way to do it
we'd like to rewind fix the problem and
then and then go forward again so there
are many different kinds of tools like
this that you could imagine writing and
people have written a lot of these types
of things so there are many of these
things that look for correctness
problems for for example memory problems
security problems
currensy problems you could also write a
lot of performance performance analysis
tools we haven't really made that a
priority of our project because we were
interested in correctness but there are
many things that you can do here also
but just to illustrate what lifeguards
might look like i'm going to talk walk
through three different lifeguards and
show you what's similar about them so
the first one is just a simple memory
checking lifeguard it's trying to make
sure that in a language like c that you
are using memory that's been properly
allocated and there's a version that
also makes sure it's properly
initialized the second one taint check
is doing information flow tracking to
see that when a packet comes in over the
network does it possibly affect things
like system call arguments or jump
targets or things that are likely to be
exploits for an attack and then lock set
is a concurrency checking problem that
tries to see whether your program uses
locks properly so although these three
tools have very different models of
what's good and bad behavior and they're
checking very different things they it
turns out that they do share some things
in common so first they all maintain
this extra state so your program has its
state and these tools create their own
state that shadows your program state
and so for example in the address check
case for every byte there's a bit that
just says was this byte properly
allocated or is this by tainted or in
the lock set case it actually keeps a
set of locks around for every word so we
have this extra state and the lifeguards
actually operate exclusively on this
metadata they don't touch your data they
just work on their own data and it's a
lot like Duke solving some type of data
flow problem in a compiler they're just
doing it at runtime okay the next thing
is for all of these different lifeguards
there are particularly interesting but
thankfully fit relatively
infrequent events that have a very big
impact on the state so for example for
memory the memory checking one anything
related to allocating and freeing memory
is a big deal so when this happens we
have significant changes to this this
metadata and similarly you know lock and
unlock or network packets arriving
jumping ahead another thing that they
all do is that they check this state at
particular points to make sure
everything looks okay so for example if
you're about to do an indirect jump
through a target that's a good time to
make sure that that's not unsafe or
corrupted thing or if you're about to do
a system call you want to check the
arguments there okay and then in some
cases but not in all cases they also
propagate this information as the
program is executing so for example in
the information flow tracking case and
taint check anytime I I use data and
produce new data I'm propagating the
tainted pneus through all that data but
the operations a little bit different if
the original if the application is say
doing an ad what i will do is look up
the tainted no status of the two inputs
or them together and store that in the
result so it's doing typically just
boolean operations on binary values
although it could be doing arbitrarily
more sophisticated things another thing
to know about this is that the way that
these tools are usually written is in an
event handler style so although the
application that I'm monitoring is
compiled and has all this nice structure
the the handlers are these generic to
fairly generic things which might say
okay if I have an operation that uses a
4-byte memory location and otherwise
operates on ret two registers in x86
this is the piece of code that I want to
execute and it's going to go in and get
the metadata or together the taint
status and proceed to the next stub so
you don't need to look at all those
details but the point is these are has
an event-driven programming style okay
so now I'm going to tell you a bit about
the
hardware support that we've looked at so
far and don't get scared I'm not saying
you have to have all the support in
order for this the rest of the talk to
be useful I'm just saying this was what
we did in our phase were willing to
throw a lot of hardware at the problem
okay so this is a kind of a block
diagram of the the things that we the
different pieces of the system here so
every processor would actually have both
the incoming and outgoing pieces here I
drew it as though it's hardwired between
two particular processors but it's all
symmetrical so on one processor where
I'm writing my application I'm going to
be capturing the in log information
compressing it transporting it
decompressing it and then feeding it
into the lifeguard and all this stays on
the chip it doesn't have to go off chip
which is good from a bandwidth point of
view and so I'll just go through a
little more detail about how this works
so the first step is capturing the
information for the log so the idea is
that the the tool will specify what it
wants to see similar to how in say pin
or valgrind you can say here here's the
thing i want to attach to i need to look
at all instructions or just procedure
calls or whatever it is so you specify
that and then with hardware support we
want to pull this out of the out of the
pipeline and stick it somewhere into a
buffer now the good news is that an
interesting thing about these tools is
that nearly all the tools we've looked
at so far they need to know which
operands you're using so if I'm
accessing memory I need to know the
address of the memory or which register
am I using but they don't usually care
about the actual contents of those
values because what they care about is
the metadata so they just need to know
which piece of metadata do I go look up
so they just have to know this
correspondence so typically they just
need to know the program counter the
address and which registers and what
kind of event was it so was it a load or
store add and of these several of these
things are static for instructions so
which registers they are the type that's
all static
it's only the instruction address and
the data addresses that are dynamic and
those happen to be very easy relatively
easy things to pull out because
instruction address we know that a
decode time we don't have to pull that
out of the middle of the execution phase
of the pipeline and the data address is
they also get pushed out to the memory
system so we can pull this out at a time
that's not critical and just stick it
somewhere and then put it into a buffer
later also we have the luxury of time
because the the lifeguards running
behind it doesn't need this right away
so we can take our time gathering this
information okay so this isn't too
difficult to to do the hardware
designers don't really complain about
this the next step another part is that
you need to transport the log somehow
from the application you're monitoring
to the lifeguard now if you had some
this is basically streaming data and if
you had some fancy onship streaming
interconnect you could use it for this
because it is a stream but but in fact
you don't really need that what we found
is that you can just use the on-chip
cash for for pushing this information
across and it's perfectly fine the
bandwidth is fine and there's really no
reason not to do that sometimes this
buffer which is part of the fairly large
cash it may get this place sometimes or
it may there may be a tiny amount of
interference with other data but it's
fairly negligible so using the cash is a
reasonable thing to do and then the next
thing is in order to make that work we
have to do some compression so we use
trace based compressors and what they're
really doing is they're doing value
prediction they're trying to guess the
vet next value of the program counter so
for example it's a fairly good bet then
the next value of the program counter is
just the next sequential address because
it tends to move like that until you hit
a branch so there what you do is you
build a set of different predictors and
you just need one of the predictors to
be right and then all that you need to
put in the log is to tell it which
predictor
it should look at so the idea is that on
the producer side and on the consumer
side we have the same predictors and
they're building the same tables so
although the consumers are lagging
behind by the time it tries to
decompress something it will have
exactly the same state and it can use
that to decompress things ok so the
punchline is using this type of approach
we could reduce the amount of storage
for each blog entry down to a less than
a bite so typically about six bits per
instruction and it would be nice if it
was even less but it's a small enough
amount that this doesn't scare hardware
people this is fairly comparable to the
traffic of the data cache and the
instruction cache misses so this isn't
scary ok now turns out though that this
isn't the hard part the easy part is
capturing things and transporting it the
hard part from a performance point of
view is keeping up on the consumer side
so it's drinking from the firehose
that's that's the challenge so what we
do over here so one thing is on the
consumer side you actually could imagine
doing this with no special hardware
support I could just put a log into
memory and then have a loop where I just
pull things out of memory and decode
them and execute them and that would
work functionally it's just that it
would be very slow because I have all
these instructions to go pull things out
of the loop and since we know we're
going to be executing this way what we
propose is this new instruction that
just says go grab the next thing out of
the log decoded get it ready for me
because I know I'm going to be executing
its handler next so this this
instruction does a lot of work in the
sense that it has to go get something
out of log decoded get put some
arguments into registers and transfer
control but the good news is we have all
the time in the world to do this because
we know that we're simply going to go
from one thing in the log to the next to
the next so we can pipeline this in
arbitrarily far ahead in time so we're
not really worried about the performance
of this instruction
okay so that was basically the baseline
support and although I'm not showing the
performance yet that that support I've
described so far improves the
performance by about an order of
magnitude we go from something like a
30x slow down to something like a 3x
slow down with just this as you'll see
in a bit hip breath if you built all
that into the harbor then it would be
difficult to be flexible about what they
didn't go into long do you actually
think you can come up with a log format
that is complete or maybe you're
considering some amount configurability
or what are your thoughts there so the
way we've done it so far is we've
actually we always put everything into
the log and like for every instruction
we put all the register data address
information we haven't because the the
amount of storage for that seems small
enough we haven't worried about putting
less information in the log so we think
the log is fairly complete now the one
thing we're not putting in the log
though is the actual data values of in
the application because we don't have
any tools yet that we've been using that
care about them if you wanted to add
that that would be the only other thing
that's missing and that would just
increase the bandwidth and storage a bit
but but but we would there's certainly
room for reducing that bandwidth by what
we imagine is that the lifeguards would
register and say here the set of things
that I care about make sure that at
least this amount of information is in
the log and you just merge all that
together for the different life guards
that are active and that's what you'd
pull out so another question if the
lifeguard can't keep up is the
application going to be blocked yes
that's you can in some cases if you
would if you were just doing sampling
you could imagine that if it couldn't
when it couldn't keep up you would just
drop things for a while until it could
catch up again but when we do our
experiments it blocks so there's a
buffer that holds about a hundred
thousand instructions and one
fills up we stall and in practice we're
always stalled because the lifeguard is
is not fast enough yet so it's always
full almost so so we this baseline
support it got us we got an order of
magnitude improvement in performance
from that which we liked but we were
still running 3x slower than real time
and we wanted to close the gap further
so then we started looking at other
features that we might add to reduce the
overhead more in particular we looked at
three things and this appeared in a
paper a disco this year and we actually
believe that these things are fairly
generic generically useful for a lot of
different monitoring tools the first
thing is that we spend a fairly large
amount of time accessing the metadata so
the thing about the metadata is that
it's smaller typically than the real
data and you don't want to initialize it
for your entire address space until you
touch it so you typically want to do
something like hashing or spilled some
clever you know table like that or index
table structure to access it and that
takes several instructions to do that
work so of the say eight or ten
instructions in a handler typically half
of them would be spent just getting to
the data the metadata so the idea is
rather than doing that we propose this
small TLB like structure that would just
do this translation for you now this
turns out to be a lot less onerous than
normal tobs because the normal TLB is
attached to a page table behind it in
this case there is nothing behind it
it's just a little cash that we program
and say here is how we do this mapping
if you don't hit in the TLB then it just
pops you back into software and you can
do it yourself which is what you would
have to do normally so this is a very
small hardware structure it doesn't add
that much complexity but you hand it the
ideas you hand it an address from the
monitored program and it hands back the
address of the metadata that you want
and that alone reduces the overhead
quite a bit as I'll show you in a second
then the other things are one of the
things that occurs a lot and maybe this
is more unique to x86 code but is that
often things will meditate something
will come in from memory in the
application and will get copied through
several registers and then it'll get
pushed back out to memory and so what
ends up happening is the metadata keeps
getting copied around many times and we
came up with a hardware mechanism that
recognizes when all we're doing is
essentially it's basically copy
propagation optimization in hardware
where what we do is rather than keep
going back to the lifeguard and saying
nope copy this state from here to here
we just postpone all of that until we
hit something that's really interesting
like a store to memory and we might say
ok get the metadata from that memory
location and copied into that memory
location and you can now skip the
copying between register steps in
between so that that actually saved this
a fair amount of overhead and finally we
notice that some of the checks that we
do are we can tell ahead of time that
they're redundant so for example if I'm
doing a memory checker if I've touched
an axe a location that's properly
initialized then unless I free something
or do something else with memory if I
touch it again it's guaranteed that
still be properly initialized so what we
do have here is software-controlled
cache where if we check something and
it's good and nothing interesting
happens and interesting is defined by
the lifeguard then it'll just discard
checks ok and the lifeguard has to
decide whether to turn that feature on
or not in some cases that's not safe
depending on what you're doing ok so
this first optimization I mentioned the
metadata access acceleration the idea is
here is a real life guard handler for
taint check this is one of the more
common handlers in the information flow
lifeguard and has we can get rid of
about half of the instructions by having
this load metadata instruction so that's
useful
and here's at the table showing how much
we reduce the average size of these
handlers so it's about half as large on
average so that that instruction is very
useful okay so now I look at some
performance numbers to see how this how
this looks okay so in this experiment
we're just looking at a single threaded
application running on one core and a
lifeguard running on one other core with
Linux a modified version of Linux that
will support feeding the lifeguards with
a little bit of magic in simox to make
the lifeguard show up at the right place
and then we're monitoring all the
hardware support that I just described
modeling it okay and here's some more
detail if you care about this and we're
modeling a chip multiprocessor that has
two levels of on-chip cash okay so i'm
going to show you results for a couple
of lifeguards so as you may recall at
the very beginning of the talk i showed
you the slow down for address check
which was way up here I need al gore's
elevator to go up and show you where
that would be but that was like 30 times
slow down is what we started with and
with the baseline hardware we get down
to something like a three-time slow down
and then if we add in the metadata the
optimizations I just showed you we're
actually running in real time well
within two percent of real time so we
can now fully keep up on this particular
lifeguard now this is a it got faster in
one case so I was going to ask about
that okay why did that happen I think
that that would have to be I don't know
the answer off the top of my head I'm
sure there's an answer
right what we're running it is it's well
the yeah there's we're actually running
it across two cores when we're running
it so it's going to change the cache
behavior a bit when we're running it and
presumably it changed it in a way that
was slightly good but if we assume that
this is noise within plus or minus some
amount of time the point is we're this
is you know close to real time so okay
and we're running Linux and all that
other stuff so there are a lot of
reasons why there's probably some noise
and our measurements okay now address
check is the simplest lifeguard that we
have though it's really doing fairly
simple things a more interesting one a
more sophisticated one is tank check
because it's actually propagating
information through every instruction so
it has much more overhead so in this
case the baseline hardware is about
still about 3 to 5 3.5 times slower and
on average we can get this the slow down
to about 1.3 there's one case bzip2
where it's still fairly high and you can
see they're actually cases where it's
again within this noise margin it's
actually close to real time so it's not
surprising that this is slower because
it's doing more work but the only point
here is that we're getting within within
shot within range of running with no
slow down here we have other tricks up
our sleeve that we haven't played yet
and we think we can improve this even
more okay so there's an even we also
wrote an even fancier version of taint
check which not only tells you that
something is tainted it also creates
this trail so that you could go back and
figure out what was the thing that
actually caused the problem so that's
going to have even more overhead and
that's another now we're running about
1.5 like a 50-percent slowdown in that
case still it's not real time but it's
way better than 50 times floor which is
what it would be otherwise and then
finally this is lock set this is a
concurrency checking lifeguard that's
checking your locking behavior
and that's running about 1.4 times
slower with all this stuff okay so the
next thing i want to show is to break
down the contribution from these
different optimizations we said there
were three different things we did in
hardware there was the metadata load
metadata acceleration and the redundant
checking and the copy optimization now
not every lifeguard uses all of them
they all use the metadata optimization
because they all need metadata and
that's useful in every case and then for
whichever other optimizations they use
either the copy propagation or the
that's here and here or the we're done
an event checking that's also adds
another improvement on top of it so
these things are complementary and it's
good to have all these things okay so
that's yes we're looking at scale across
one of the things that stand out
it's superscalar processors to be
generating events and you know whatever
ILP right on one side but then you're a
lifeguard because it's more than one
instruction per instruction on the other
side doesn't benefit from a lot of that
i LP do you give them equitably what the
numbers would look like an acting so
about what you can do about it perhaps
even more cores for the left or
something like that yeah actually i'll
talk a bit about using more course for a
lifeguard in a minute but you but
roughly speaking the lifeguards do look
different they have their more
instruction and more compute intensive
and far less data intensive so they're
cashing behavior is typically very good
much better than the main application
and that's part of what allows them to
catch up because they still are doing
you know multiple instructions of work
for every instruction in your original
program and but they tend to have very
simple control flow I mean within the
event handler it's typically straight
line code or very simple code so that is
also so I think that for but they also
have data dependencies so I don't know
how much they would speed up on a
superscalar core the main reason we
looked at scalar was just we didn't have
a good simulator that could do out of
order on x86 if you know of one let's we
can use let us know but but I don't know
that that would qualitatively change
things so ok so now on to the part that
that I'm you know really interested in
now which is what do we do this is work
we've done so far what are we doing now
and one of the things that I'd like to
do is get this good performance but
without having to throw these Hardware
gadgets at it to accelerate all these
different sources of redundancy so one
way to do this is to use compiler
optimization techniques but we're going
to apply them to the lifeguards so we're
going to take this so as I said before
the lifeguards are this event driven
code you can think of them as being
these threads so the first order a
really simple oversimplified way to
think about this as you would just for
every instruction in your program
you can imagine cake creating
instruction where you just in line the
handler and take this big thing and
compile it and now that's your life
guard now that that would be a big thing
to compile we didn't necessarily one
that want to do it that way so instead
we'll probably do some you know jit
style optimization what we're actually
doing is we're profiling paths going
offline and compiling things and then
and then measuring their performance for
now that's not a way we'd really want to
deploy it but that allows us to do our
research and so the idea is that we will
create well in line all of the handlers
for a particular hot an important path
and then hand that to our own compiler
now an interesting thing though is the
things that the handlers these are not
things that were written by some
application developer they're written by
the tool developer and we can
potentially give the tool developer our
own language and say here's here's a
template or here's a way that we like
you to write these handlers and if you
do it this way our compiler has a very
good understanding of what your handler
does so we're actually writing our own
intermediate representation and so on
because there's a lot of good high-level
information that we have knowing that
it's as a lifeguard and lifeguards do
certain things that we can take
advantage of and I think it's
interesting because i'm using the
behavior of one program to optimize the
completely different program and so
we've done a very preliminary study
where we're trying to eliminate detect
redundant propagations and checks and
we're using several things from compiler
analysis and RNA preliminary result here
is that on for these different handlers
on different programs the y-axis is the
fraction the percentage of handlers that
could be eliminated where the software
can detect that this is unnecessary and
we can throw it away and in fact what
we're seeing is that there seems to be
more redundancy in these handlers than
there is in normal code
and they're part of the reason for this
is that they operate on these very
simple data types they're typically just
boolean values and they're just oaring
together values so there's a lot of room
for folding instructions together or
proving that things are redundant and so
for example also accessing the metadata
I said before that there's a special
instruction that could accelerate that
but we can also detect that we are just
redundantly accessing metadata again and
again so we only have to pull it out
once we keep it in a register and we can
get a similar benefit all in software
another thing that we see is that
there's really good lupin variants in
the Life Guards because whereas in the
application there has to be something
that's not a loop invariant for the loop
to make sense in the lifeguard it may be
that everything really is loop invariant
once you all these boolean values get
set they won't change inside the loop
and you can actually just eliminate a
whole lot of work that way one of the
challenges though is we have to be a
little careful because you know one of
the nice things in a compiler I guess is
if the programmer gives you a broken
program you're free to optimize it into
any you know equivalent broken program
that you want to so if they haven't
specified things well the optimizer can
can have fun but we want to make sure
that we don't take broken programs and
optimize away things in the lifeguard
that shouldn't be optimized away I don't
want to say well this seems to be dead
code and I'll eliminate it but oops I
forgot to check something we have to be
a little careful about how far we go and
optimizing the structure okay so that's
one thing that we can do is use we're
trying to use compiling a compiler
optimization to eliminate the amount of
work to help us get to real time now if
that doesn't work the next best
alternative perhaps is to use
parallelism so if we've got these cores
maybe I can take the lifeguards and
spread them over multiple cores so I'm
generating a log and i'm going to
distribute it across multiple cores in
some cases this is completely straight
forward in a dress check other than at
memory events everything in between is
come
pletely independent we can actually
trivially paralyzed between those points
but for something like tank check where
it's doing information flow propagation
there are a lot of data dependences and
it's actually very hard to paralyze that
so we had a paper in spa this year where
we talked about we present an algorithm
for doing this information flow tracking
so if this is our log what we want to do
is break it in two different chunks and
process them in parallel but the problem
is that this chunk depends on the
previous chunk and so on so there are a
lot of data dependences here so what we
do is we we try to quickly create some
symbolic representation of what that
chunk does but for all of the exposed
inputs where we don't know the value yet
we just leave them as variables and so
we doing symbolic inherence tracking
then there's another phase where you
have to resolve you could then take the
real inputs figure out what to do with
them and so on and we have a way to do
this that in theory should be asked on
top to have asymptotically linear
speed-up the way we really implement
this is with a sort of master worker
model it turns out that this applying
the different summaries is a very small
amount of work and we just do that on
one on a master thread and we use the
other threads to do this in parallel and
here are some numbers from this paper or
last spring it's missing some of our
bells and whistles to accelerate
performance so we're still we're still
getting slowdowns even with this but we
are speeding up our slow down we're
slowing down less with more cores I
don't know that I want to really throw
eight cores at at this but there you can
and this is another way to make things
faster so it's one of our tricks that we
have in our bag of tricks if we want to
use this right
simulation there's this real let's see
we it was run on let's see well it's a
simulation because the log we we have to
generate that in simox but the timing of
everything above simox was it was
implemented in real software and we
measured the real slow down as it ran on
Linux and everything so it's fake in
that there's this layer above the
hardware which is simox that's
generating the log but everything else
above it is real the pieces of hardware
that I'm trying to get rid of here
aren't the whole baseline infrastructure
but the extra things to accelerate it
more okay but then the third thing that
I'm really that I'm actually very
excited about and we're in the very
early stages of figuring out how we're
going to take advantage of this is you
know one thing about static analysis is
that when you do static analysis it
converges so I'll get an answer and I'll
stop thinking about it I may get a
conservative answer but I get an answer
whereas dynamic analysis never stops
thinking it keeps you know thinking and
thinking and thinking as long as your
program executes but we've noticed is in
fact the metadata stabilizes it tends to
stabilize very quickly or it may go
through phases where it becomes unstable
again but we think there's a real
opportunity to take advantage of this
because this is unusual normal programs
don't behave this way okay so what we're
showing here is of all of the metadata
accesses for taint check it's a
breakdown of which of the accesses in
red actually change the metadata and
it's relatively few of them on average
it's just a couple percent actually ever
change the metadata many of them are
reads so they don't change anything and
many of the rights are silent rights so
they they're writing but they're writing
the same thing that's already there so
really they aren't changing
anything and what we've done is we've
started creating this tool to visualize
what's going on here this is gzip over
time on the x-axis the y-axis is a
representation of the address space
where a pixel is a page I think and the
red red pixels are the cases where we're
actually changing the metadata and this
is the metadata not the program data
although there's a one-to-one
correspondence but we're talking about
changes the metadata and then the blue
these are accesses that aren't changing
anything there either reads or they're
silent stores so if you look at this
carefully when the program starts off
there's a lot of red and there's red
periodically but a lot of these accesses
are blue there's a lot of times where
nothing actually changes and in fact if
you plot a histogram of this and you
wait it by frequency and the size you
see that you know eighty-eight percent
of the of the axis is de metadata
involve these really long runs where
nothing ever changes a million or more
accesses to the same metadata so what
can we do with this well first of all
one thing that suggests to me is Wow
data value prediction is going to work
really well because once i get n also so
a normal data value prediction in a
typical application I'd be trying to
predict say a 32-bit or 64-bit value
which can be difficult here I'm
typically trying to predict whether this
binary value is a one or a zero and once
I get it right I'll be right for a
million times in a row so if I'm trying
to paralyze things like to run it across
break the lifeguards across multiple
processors data value prediction is
going to work extremely well but the
thing I'd like to do beyond that I think
is even better is I'd actually like to
turn the lifeguard off and do no work
when I detect that it's not possible
that anything could be changing and
we're trying to figure out exactly all
of that involves but if you know that
the inputs could not have changed if you
know you've done this before and you're
applying the same inputs and
they haven't changed then you know that
the output won't change and you know
that you don't need to do any of the
work you don't even need to pick up the
data or look at it at all if you know
that it hasn't changed and you've done
this before you can skip all the work so
that's something that we want to take
advantage of the next thing the next
major theme that we're focusing on is
how to deal with parallel applications
so the most conceptually simple way that
we could imagine doing this is if if
only we could take so we're going to be
logging the log generational occur per
core so if I could simply know the order
in which i should interleave all these
accesses if I merge them into one log
and fed that to a lifeguard the nice
thing is I wouldn't have to change
anything about what I do over here I'm
just processing a log but unfortunately
that doesn't work from performance point
of view because I'm already having
difficulty keeping up and now I'm going
to be throwing even more at this one
core so that won't work so instead what
we need is at least we think at least a
one-to-one correspondence between unless
we can put them have them sleep for long
periods of time between lifeguard cores
and application cores but then that
complication is well sometimes their
dependencies between these different
things and how do we capture those and
think about them so there's been a
fairly large amount of work recently in
architecture conferences where people
are trying to track dependencies between
threads for deterministic replay so
flight data recorder bug net and so on
the difference in what we're doing is
they're doing their work to do
post-mortem replay like whoops it
crashed and now i want my debugger to be
able to go back and replay what happened
in our case we don't need to terminus
decree play we just want to drive our
lifeguards but we have to do them in
real time so we're not as interested in
compressing out the number of Arc's but
we have to keep up in real time and do
that so a lot of interesting things that
we're looking at about how do you create
the log and synchronize it tracking
coherence messages one thing that's
especially interesting is sometimes
these races occur
more at a semantic level not necessarily
at an address location level so if I'm
accessing memory and I'm freeing a block
of memory somewhere else the free itself
doesn't actually touch everything within
the block so if I'm just doing coherence
tracking I'll miss that dependence so
that makes things interesting the other
thing is we'd like to support relax
memory consistency models because that's
what real Hardware behaves and none of
the existing work does that they all
assume sequential consistency and then
go you know in the first point of the
paper and then they go on and the paper
gets accepted but nobody can implement
any of this stuff so in addition to the
memory consistency model of the
application the other thing is we're
realizing that we need to think about
what the consistency model is from the
perspective of the lifeguard this is a
different thing and it gets very
confusing as you start to think about it
so for example lifeguards don't
necessarily do the same thing to the
data to the metadata that the original
program did a read in the original
program could turn into a read modify
write in the lifeguard and in that case
it's not the same thing you have to
worry think about the consistency a
little bit differently and then a more
radical approach that I think that we're
taking is I challenged my group to I
said okay people are thinking of these
ways to track in or threat data
dependences and hardware what if we but
that's really hard to do especially when
you think about relax models and
everything what if we just assume that
that can't be done so if there is no way
to track at a fine granularity these
dependences what could we do the idea
was let's assume that the one thing that
we can not assume is that things in the
distant past are observable so at some
point after a processor executes enough
instructions everything has to get
flushed out of the buffers and things
will show up in the memory system so if
we imagine that we know things from the
distant past really have happened but
for things in the recent past or the
near future we don't actually know how
they interleave with other threads how
would we build a lifeguard to deal with
that and what that implies is that
we only have partial ordering of
dependences not full ordering and that
means there are many different possible
interleaving that could explain what
happened and so what you end up with is
possibly this big States place space
explosion you don't want to have to say
well here's a and B and a could go
before be or could go after be and then
let's throw in three you know see
through Z and everything else so there'd
be too many different possibilities to
deal with so the way that we've been
trying to deal with this is I thought
well this sounds a little bit like
something that we do in compiler
analysis in particular there's something
called interval analysis where you try
to summarize the net effect of these
high level structures in the control
flow graph in one shot and the most
interesting one more interesting things
there is dealing with loops so you can
write a function that will say well
here's the net effect of this cycle in
the loop tada there's the answer and you
get an answer that may be conservative
you may lose a little precision when you
do that but it's very efficient so not
going into a huge amount of detail the
idea is that if this is the different
threads we're going to break the threads
into these windows where the window is
large enough that we know that all the
memory buffers will clear out within
this window size so if I'm sitting here
in this window here I know that the
things to windows before me had to have
committed to memory I also know that the
window right above me occurred before me
because we're in the same thread but if
I think about everything in the other
threads I don't know how anything here
here here interleaves with my thread it
could be jumbled together in any order
so what we're doing is we're trying to
build this equivalent of a cleaning
closure operation to describe the
effects of all of these different
concurrent things on the different
threads and we've tried to build this in
a way that's consistent with typical
data flow analysis and we've gotten
several the lifeguards into this model
now so and then there's a I won't go
through all this but the way you do this
is you
Oh first summarize the state of all
these things on the side and you have a
way of passing those summaries to the
block that you're executing now and you
can compute its effect and notice this
problem here and go on so it's right now
it's two linear passes we have ideas
about how to reduce it to one pass but
the reason why I'm excited about this is
that it will free is completely from
needing special hardware to track
dependences now what we're giving up is
some amount of precision we will get
some false positives because we will
consider orderings that might not really
happen but this is something we're
working on I'm almost finished here so
those are the things that we're working
on actively at the moment but looking
ahead even more some other things we'd
like to do is I said earlier that it
would be nice if these tools could not
only find a problem but go back and fix
the problem that's not always possible
but it's possible in some cases and we'd
like to do that in fact we'd like to be
able to rewind past system calls if
possible with the help of virtualization
magic the other thing that I think I
mentioned before is I'm excited about
what we could do if this infrastructure
really worked like what new things could
we do and in a way I view it as it's
like jit style analysis where you're
analyzing your program as you execute
but it could be always on so normally
when I do get analysis I have to sample
things or I have to wait until something
gets hot enough that I can look at it
but I'm just looking at a snippet of it
imagine if you had that analysis from
the beginning of time so from one of the
system booted up or when my program
began I can I know everything that's
ever happened to it that might be really
powerful i also am trying to engaging
people in formal verification to see if
they can typically what the things that
they do are static types of analysis and
i'm curious about what you could do if
you combine some static verification
analysis with some amount of dynamic
verification analysis and finally it
might be interesting to think about
whether you would write your
applications differently
if you knew that this mechanism exists
and that the log was there maybe there
are other things that you might do so to
summarize we think that the results that
we've gotten where we're willing to
throw some hardware at this are
encouraging we can go from say a 30x
slow down to within about a thirty
percent slow down so we're it's the
overhead is small enough that you could
imagine really deploying this and
turning it on all the time I talked
about some more software or any
techniques that we're working on that
look promising and looking you know more
broadly although I spoke about this in
the context of capturing logs and having
this baseline architecture I think any
type of event driven lifeguard that
processes streams of events could
potentially benefit from the kinds of
optimizations we're trying to do to make
lifeguard processing faster another
thing that you can do that we have been
haven't been doing is sampling if you're
willing to do some sampling you can
immediately reduce the overheads as much
as you want to the trade-off is you
don't get a complete trace any more you
get snippets of it but like trishul cho
em biet microsoft has done a lot of
build a lot of interesting tools based
on sampling and if you could increase
the coverage of the sampling from say 0
point 0 0 1 percent of time to say fifty
percent you would get qualitatively
different results so and then the hope
is if with this type of support we can
use chip multiprocessors to make the
world safe for a parallel programming so
that's it thanks
well my first question is will we ever
get the car do I support you told us
about well where I i also have an intel
hat that I where part of the time and
we're working on it so that yeah I don't
have anything that I can say but we're
working on that there's been I mean
there's been a big shift when I started
talking to people really no real
hardware architects about this at first
they were very negative about it but the
overtime they become it's this is move
up to the level where it's something
that's been giving serious consideration
actually getting something into a
prosser's that's a very high bar we
haven't reached that point yet but where
it's now at the level where it's being
seriously looked at so on the software
front have you given any thought to
potentially actually weaving together
the original application and the
lifeguard and then actually looking at
the the problem differently and saying
well I want to make this weaved
application run more efficiently on a
multi-core processor and parallelizing
that you know that merged program and
you know things that just might end up
together is that you might be able to
exploit some locality and control flow
by grouping together the lifeguard and
the original application for a region
and doing another region of the
application somewhere else or something
like that oh yeah so the quick answer is
no we haven't we haven't looked at that
and that's another interesting
alternative to this so that that's
another way to go and that we'd like to
look at that too but haven't done that I
mean the interesting thing is we get a
lot of benefits I think to them for the
most part the application a lifeguard
interfere with each other you do want to
follow control flow and track certain
things but the reason why the overhead
is so bad with binary instrumentation is
when you do
cram them into the same thread and same
register said and when they're using the
same cache and everything it does there
are significant disadvantages to doing
that so giving the lifeguard its own
registers in its own cache that
accelerates it dramatically and there's
good locality you know the spatial
locality you have in the main
application translates into spatial
locality within the metadata so it
accesses the cache very nicely I don't
know we haven't really looked at what
happened when they're on the same core
we just know that with binary
instrumentation today it's it's not very
good so that's why we looked at it this
way but that's another alternative
okay thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>