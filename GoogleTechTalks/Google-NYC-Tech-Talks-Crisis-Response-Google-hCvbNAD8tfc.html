<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Google NYC Tech Talks: Crisis Response @ Google | Coder Coacher - Coaching Coders</title><meta content="Google NYC Tech Talks: Crisis Response @ Google - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Google NYC Tech Talks: Crisis Response @ Google</b></h2><h5 class="post__date">2013-10-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hCvbNAD8tfc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks everybody for coming we're really
really happy to have you here and to
have a chance for some my engineering
colleagues to talk a bit about what
we're doing on Crisis Response here in
New York and in our teams in Mountain
View and elsewhere I said I'm a part I'm
a product manager at Google here and I
focus on civic innovation and a crisis
response work which is basically some
work that we host out of google.org the
philanthropic part of Google but it's
really a lot of it's a cross company
initiative where we see how Google can
really contribute to making the world a
better place ultimately and the crisis
work is something that Alice will at
least talk a bit about the history about
how we started doing that and but just a
couple of things that I I wanted to note
here was that you're all here because
part of this are this talk series that
we're giving and just to remind you
would have seen this the tick-tocks fill
up really quickly so for next time you
made it here so you know how to play the
game but tell your colleagues to sign up
fast and we really want to hear what you
want to talk about so as we said we're
super psyched to have you here to talk a
bit about what our work is and to hear
what you have to say and ask of us um
this is the agenda we're just going to
work move quickly through my part which
is the least important bit and they have
the Q&amp;amp;A afterwards just briefly about
Google New York City um it is a fairly
old office 13 years this building 10
years I believe now right and Craig
Neville Manning who was the first image
lead for Google's crisis team actually
was the first engineer here in the
engineering director here in New York so
there's a there's a ton of history about
what being able to do but it's a very
large office we've got three thousand
people here thousand plus engineers a
whole range of projects from dr to some
maps work there's clearly lots of ads
work and so forth and there's a bit of
our our crisis response and civic
innovation team here as well critically
I have to say we're hiring we love to
find really good talent and we need more
people to come and work with us on a
pile of awesome stuff that we we're
trying to get done um the culture you
know you're in a great space here
there's note about that we've got great
Terriers have to warn anybody who's
thinking about coming there is this
thing called the Google 15 that is the
15 pounds you put on because of the free
food that arrives when you join the
company and we basically have a great
time except for the fact that we're
working pretty damn hard to get our
projects done so the summary on tonight
I did want to just briefly introduced as
i said i'm the product manager the
product lead for this team but Alice and
Phil you can read their buyers here
they'll do their own intro to the sort
of i'd like to say really keying
critical engineers on our project and
they've both been well Alice certainly
has been with the crisis team for a lot
longer than I have and Phil's been a
sort of key component of it and we do a
mix as Alice will talk about between
responding to disasters like seeing how
we can help with something like
hurricane sandy and building tools and
infrastructure that are sort of ongoing
enhancements to help people find
information during a disaster as an
interesting mix about how we involve the
rest of the company and our work
volunteers from around the company and
how we actually do straight up normal
product and hardcore engineering work to
support ongoing responses and build
tools ah that's enough for me I'll hand
over to Alice to get us started i think
but thanks a lot and we really want to
hear your questions and see what you
think of us so i'm going to use this mic
and you hear me well also in the back ok
ok so google crisis response as nigel
mentioned both thank you so as nine
riddle mentioned Google crisis response
our goal is to make critical information
available in times of disaster Google
mission is to make to organize the
world's information making it
universally accessible and in times of
crisis it's even more critical we want
to deliver accurate information that's
very important it's accurate but also as
fast as possible and we want
do that on different types of products
mobile mobile is really key during an
during a disaster you're not always in
front of your computer and doing that
for open standards so we're going to
talk today about some of the tools we
developed some of the lessons learned as
well as some standard that we're using
but before doing that I wanted to give
you a little bit of background about how
that team was created for in at Google
so it's actually all started on January
12 2010 when a magnitude seven
earthquake did strike Haiti near Port au
Prince the main city your herd and I'm
sure you remember the devastation that
happens they were more than 300,000
people died that day any leads to a huge
devastation in the country a few hours
after the earthquake and some volunteers
here at Google get together to see how
we can we can help and we did what we
did in past disasters such as the
Katrina hurricane the first thing we did
was trying to get satellite imagery
satellite imagery is really key just in
the aftermath of a disaster to try to do
damage assessment to try to see what's
going on on the ground at that time we
managed to get satellite imagery
available in Google Earth and Google
Maps in less than 24 hours and to give
you an idea of what that looks like here
is a picture of from the satellite
imagery of the Petionville golf course
pitching villa is a really near product
rains and that was before the earthquake
so you can actually see this big green
area this is the exact same picture one
day after the earthquake and you can see
here that people started to go on the
golf course trying to go away from the
rumble away from places that were unsafe
this is the same pictures ten days later
that became one of the biggest camp in
Haiti where people when and leave their
while waiting to to find another place
to live 40 satellite imagery a lot of
things can be done you can try to
automate and do some damage assessments
you can try to locate what are the
bridges that
it still seems to be up and running some
people located like hospital and things
like this so this is something that in
the past google has done and try to do
as fast as we can lady satellite imagery
but the devastation in haiti was so big
that we thought there's something else
we can do there was devastation in terms
of going there and helping people go out
of robots but also trying to help all
the responders and people on the ground
coordinate so some volunteers at Google
got together to create some maps there
was a lot of data sets out there on the
internet using different formats being
hosted in different website so we worked
on putting them together a lot of this
work at the time was very manual trying
to get some files in shape files for
example putting that into some chemo to
put that in some good viewers and so a
lot of these things were a little bit
chaotic but we put that together to make
it available to people so that a lot of
people could download it could go on
side and watch these data and being able
to share it with other people we at
least up hospitals at least of cams and
things like this another big issue that
happened just after the earthquake was
people trying to find their relatives
find our friends howdy how do you get in
contact with how do you know your
friends are fine and are safe cell
phones didn't work most text messages
didn't work how do you do that so some
people started to put different website
with list of missing people you could
find this list of missing people on CNN
you could find that on the New York
Times and some people it set up some
sites specifically for that Haitian
quake was one so if you were looking for
for someone you had to go to all of
these different websites to find
information so and and some of this I'd
also had issues with load because so
many people were just searching for
people that some website just went down
because they couldn't handle the load so
issues about organizing information in
different places and scale this is
something that at Google we know how to
do so a group of volunteers got together
and said we need we need to be able to
help here and we started a hackathon to
try to build a tool that we call person
finder that will allow to connect all of
these databases in one place and
exchange information so that if you go
to any of this place you would be able
to see that list of missing people from
any of these other sides so we studied
this hackathon with volunteers that were
helping from all of most of our
engineering office from Australia to
London to Israel to the east coast or
the west coast of the US so pretty much
we had 24 hour coding happening and in
72 hours we got a refrigeration of
person finder out there launched in
English French and Haitian Creole and
i'll talk more little after that but the
standard that we use and the how that
worked technically but from this
experience a group of these volunteer
that Hulk that you know there's so much
more we can do with technology in terms
of disaster why don't we just have a
team that work full-time on days so went
to see Google leadership and say why
don't we create a crisis response team
and that's how the team was created so
since the great spring 2010 we have
engineer that work full-time on trying
to solve this problem and the first
thing we did was wondering we're a
company mainly developing internet tools
is the internet even working during a
disaster you can say you know if it's
not working you can just build the best
tools but no one will be able to use
them so we studied that looking at
traffic from places where there was a
disaster to Google but also looking at
feedback that we got from users during
crisis situations and here's what we saw
we said that there's always a little bit
of internet so this graph is the graph
of queries going to Google from Haiti
and so you can see here on January 12
there's a big drop this is what the
earthquake happened and actually Haiti
is a country that was not that well
connected before the earthquake and what
you can see here is that in a check for
months for Haiti to go back to their
connection pre-earthquake but what is
interesting is you can see the
blue line doesn't completely go to the
ground like this there's there was still
some some intimate available just after
the earthquake and we know that from
graph but we also know that from
experience have been traveling to haiti
in past 10 years and so had contacts
there and two days after the earthquake
I got a chat on gmail from a student I
met at conference prior the earthquake
and he was at an internet cafe trying to
reconnect with friends he was at an
internet cafe 48 hours after the
earthquake had a connection and could
chat people he could not call them he
asked me to call his family here so that
I could actually give some news but he
couldn't call but he had internet we saw
this also here that's a similar graph
for an earthquake that happened in chile
in conception on februari 27 2010 the
earthquake happened during the night
when so the shape like this is distance
during the day and this is during the
night and what you can see in one week
here we go back to the pre auth quake
traffic and the day after the earthquake
there are some connections the last the
last graph is about the earthquake that
happened in japan in march 11 2011 and
you can see here there's also a drop at
the time of the earthquake but very
quickly after you can actually see that
there's some connection and this is just
based to TCP IP tcp IP has been designed
to be resilient so we can see from this
graph that there's some internet we can
also hear that from our users this is an
email we received from one of our user
in Japan that told us that on March 11
the desktop email didn't work it didn't
go through but phones and SMS didn't
work either but gmail connected them it
connected them to each other in the
company but also to their friends around
Tokyo and around the world and that was
not only gmail that was also Facebook
that was Twitter that was the internet
and people connected even if SMS or
voice and their phone didn't go through
internet works so if people can access
internet and turn to the internet to
find information what do they search for
this is a graph
queries that goes to google for users in
hawaii and these are only queries that
are about tsunami you can easily see the
two spikes after each earthquake after
the earthquake in chile and after the
earthquake in japan if you zoom in on
the day of the earthquake in japan this
is what it looks like in red this is the
the tsunami warning that was issued at
like seven fifty six pm and just after
you can see the number of spikes the top
of the graph here represent twenty-two
percent twenty-two percent of people in
hawaii at the time that we're searching
on google we're searching about the
tsunami so this is people turn to the
internet to find information you can see
here the second way like the second big
spike was when the wave did hit Hawaii
so people went back and insert for
information so from there we can see
people turn to the internet and what
type of information are they looking for
there when you're in a situation of
disaster you're looking for three types
of information what is happening how bad
is the event if you did feel the ground
shaking a little bit it was it just like
a small earthquake or that is near you
or is it like a super strong earthquake
that was for a way but you just felt
only a little bit of the shaking and
what are the road condition is it safe
to go if you're at work is it safe to go
back home where are your family what are
the resources if you need to go to a
shelter or if there's no more power how
do you find information how do you find
information about the nearest hospital
these are the type of information that
people are looking for and a role is
when you organize this information to
make it available to users as fast and
as reliable as we can so for that what
we learn from the last three years is
that anything we build will have to be
simple standard and open simple as we
need a simple UI a simple flow time of
crisis is not the right time to learn
about a new UI to sign up for a new
account to go through reCAPTCHA all of
the things it's like people are stressed
and people are really in despair so you
want something very simple to use
you want anything that you do would have
any data that you produce would have to
be standard collaboration is key during
a disaster you want people to
collaborate but if everyone has data in
different format how can you just change
this data it's really hard whereas if
everyone just produced data in the same
form and then you can really work
together and finally you want to an open
system open in terms of open source so
other people can reuse your code but
also open API so you can exchange open
data having your data public having your
data available for other people to use
so we're going to talk now about
different products that we think
implement this simple standard and open
and feel is going to talk about the
first product that is crisis map yeah
thanks Alice um hi everyone I'm Phil so
the first tool that I'm going to tell
you about is one that we've used very
effectively to help get information out
in times of crisis Alice started to
motivate the problem but I'm going to
continue before I tell you exactly what
the tool is so when a crisis strikes we
know that there are many important
geographical data sets that can be
vitally useful to preparation and
survival to recovery and to response
efforts just a few random examples the
National Weather Service puts out
forecasts of Hurricane tracks and river
flooding so this is a screenshot of a
map viewer on their website but they
also make the underlying data sets
available for download FEMA publishes
data sets for maps of evacuation routes
among many other things this is an
organization called geo Mac that
publishes maps of current wildfire
boundaries so it takes tremendous human
and organizational resources to put
these data sets together but the maps
are only effective if you can get the
right maps in front of the right people
at the right time all of these maps are
hosted on different websites not
everybody knows where to find them some
of them don't get the audience's that
they deserve every month every website
has its own map viewer every map viewer
looks a little different every map
viewer works
little different some of them work
better than others some don't work on
mobile devices which is really important
in times of disaster and perhaps the
biggest problem because the data is all
separate on separate websites it's hard
to see it all in context so here we've
got the American Red Cross publishing
valuable real-time data about which of
their shelters is active and what
capacity remains they're really good at
this in disaster relief we're always
talking about coordination this is a
great example of coordination between
different Red Cross shelters but even if
other response organizations were to
follow their lead and also publish their
operational data to a map there wouldn't
be a really easy way to combine all of
that information to a common picture so
you knew what everyone was doing recap
relevant high quality maps not always
easy to find people affected by a
disaster responders on the ground
probably can't be expected to chase down
every different website that has
relevant information to map viewers
varying quality some are awkward to use
some not accessible on smartphones or
tablets and three data is isolated when
it's silent at individual websites it's
hard to see in context so these are some
of the problems we're trying to solve
with our teams crisis map tool so here's
a screenshot crisis map is a tool that
lets anyone make a mash up of multiple
maps related to an event so by way of
example check out this is the google
crisis map for superstorm sandy last
October this is a screenshot of the map
we published as it looked just before
the storm hit land to each of the layers
you see on the map and there are many
all on it once they come from one of the
sources on the right hand side and are
being updated in real time on the map so
all of this data is constantly changing
while the National Hurricane Center is
updating its outgoing feed of the
hurricane path and forecasts the Red
Cross is also updating their outgoing
feet of open shelters and then so the
person curating the map in this case the
crisis response team is only deciding
which layers we need to promote or
deprecated and what are the other data
to show in context with it so any viewer
of this map can alter the view by
zooming or changing the viewport by
toggling layers by affecting
transparency on some of the layers like
the cloud
you can search for a location and then
you can reshare it to highlight that
information that's relevant to you so I
said before that good maps are hard to
find this remains true but with a tool
like this the task of finding compiling
and curating map data can be handled by
a small number of individuals or
organizations and you can then promote
the single mashup map to the affected
population and to the responder
community so in the case of superstorm
sandy there was so much additional data
available that we actually created a
second map just for the new york city
area so well before the storm reached
New York we published this mashup geared
at preparation as you can see in context
the mandatory evacuation zones are in
orange the evacuation centers are all
over the borough's centers run by the
Red Cross show up his pins centers run
by the New York City Office of Emergency
Management show up as red dots same map
after the storm passed the needs change
the pins you see here represent
community organized volunteer centers
and through partnerships with external
organizations like Noah we were able to
obtain updated aerial imagery for
coastal areas this proved useful both
for individuals who had evacuated the
area to assess the state of their own
homes and for response organizations to
understand the extent of damage so we
think that the simple open and standard
principles apply here this application
is based around the Google Maps
interface that many Internet users are
already familiar with to navigate one of
these maps should be a fairly familiar
experience which is important because as
Alice said mid crisis is not the time to
learn an entirely new application the
interface adapt seamlessly to different
display sizes from full desktop tablet
mobile and we make it really easy to
embed on another site through an iframe
the application is all built on open API
s and open source libraries tell you
about that in a second and by open
sourcing the code we also allow anyone
to stand up an instance of the app and
begin to create and publish maps not
just us finally the entire back end is
built around open data formats
so we support rendering maps using a
bunch of different formats the most
common one is called KML is anyone here
familiar with that okay a few people I'm
going to talk about that in a second so
the first two on the list here can I
land georss are both xml-based formats
we really like these because they're
both really easy to generate they're in
there within reach of essentially any
developer who can write code that who
knows how to write code that spits out
XML we'll take a closer look like i said
at KML on a second the third option on
this list WMS is a bit more advanced or
side the fourth option is a bit more
advanced it stands for web map service
it's a protocol maintained by the open
geospatial consortium this is actually a
brand new addition to crisis map so if
there are any GIS experts in the room
please take note tile service is also an
advanced option this basically allows
you to prepare map overlays in the form
of tiled images across the globe and
supporting this is what allows us to
slurp up new aerial and satellite
imagery in a real hurry the next options
here Google Fusion Tables and Google
Maps engine are applications that allow
you to author maps of your own so Fusion
Tables is available to the public free
of charge you can check it out Google
Maps engine is an enterprise product but
there is a light version available that
you can check out on the Internet so
recall that crisis map is a mash-up tool
since it doesn't host any geodata it
can't actually support authoring or
editing map content during a response
will often lean on products like Google
Maps engine Google Fusion Tables to
create maps with place marks out of
tabular data this allows us for instance
to maintain a spreadsheet with a list of
volunteer centers maybe each with an
address or lat long point and then we
can have each row plotted on a place
mark on the map layer so here's a real
simple example of KML it was originally
developed for use with google earth
formally ho formerly called the keyhole
earth viewer hence keyhole markup
language so every KML file specifies a
set of features here a placemark is a
type of feature that we use to make a
pin appear on the map when you click on
that pin you'll see a heading with the
text google NYC that you see inside the
name and
the description this text will show up
in an info bubble it will so you can
create KML files that pinpoint locations
think map pins you can specify bounding
polygons you can add image overlays you
can specify time stamps time spans and a
lot more there's a lot of information at
this link down here in case you're
interested in learning how to make your
own KML this is what our system
deployment diagram looks like since this
is a tech talk I thought we should go
into this so top center is our map
viewer client which runs in any browser
on a computer tablet or phone the client
is written in pure JavaScript using
closure tools is anyone familiar with
closure tools to people great so so
closure helps us a lot currently our
JavaScript code base is 1.2 megabytes of
uncompressed JavaScript and that's
clearly too big to send out over the
wire to any random tablet or smartphone
over the cellular network post closure
compiler though that gets down to about
285 kilobytes not bad I'm still too big
for a quick download over the over the
mobile network so with the help of
closure tools to dynamically load
modules as needed we're able to get our
initial download down to about 75
kilobytes still a little bit higher than
we'd like but pretty good compared to
1.2 Meg's of uncompressed so that client
a code on the top means very heavily on
maps api on the right hand side of the
slide in fact we delegate almost all of
our format support to those underlying
Maps API services now on the left side
the data model that describes one of
these mashups including all of their
lairs their data sources their default
properties is a format that we call map
route that we've also open sourced it's
basically a JSON object client knows how
to read and serialize pass along to our
server which is written in Python
handles things like authorization
mappings between map IDs and published
URLs and it acts as an intermediary with
the data store so I should mention that
crisis map runs on App Engine which
we're using primarily because it scales
and it scales really well so as you guys
can imagine crisis map is not exactly an
application with a steady state traffic
pattern when there's something going on
our traffic hockey sticks
just like those graphs that you saw and
so we've had really really good
experiences with App Engine scaling up
quickly for us which leaves us to focus
on the task of map curation rather than
chasing down production bottlenecks so
during a response our small team spread
across New York and Mountain View
remains engaged around the clock over
the course of the sandy response we
curated more than 50 layers between our
two publicized Maps just out of
curiosity did anyone here create or work
on any data sets that were involved in
the sandy crisis response few people
awesome i'd love to hear about your
experiences later on so here i'm just
going to give you a few examples of
layers that we included on our map
thanks to the help of responders like
yourselves organizations like the MTA
and the community at large so after
sandy rolled through here it took a
little time to restore normal subway
service so in the interim the MTA ran
shuttle buses over some of the bridges
thanks to their helpful coordination you
know getting on the phone and email with
us we were able to keep an up-to-date
layer with those shuttle stops also
thanks to the MTA and data published by
WNYC we were able to keep this up to
date layer of running subway lines as
you'll see on this day the AC line the
blue line on the left here stopped at
thirty-fourth Street I think later this
day it started running again a community
member and doctor by the name of when
Dombrowski helped to crowdsource and
curate details on service off services
offered specifically to senior citizens
is wine in the audience tonight maybe
not but using this form that one set up
you could submit information including
the category of service you have to
offer the location contact information
detailed and description and this data
made its way to a Google spreadsheet
from where we were able to import it as
this layer on the sandy crisis map so
next to wrap up crisis map I'm going to
dive into an end-to-end story of one of
our crowd-sourced map layers the gas
stations of new york and new jersey
as many of you might remember not long
after Sandy's landfall gas availability
became a major issue lots of people were
waiting on lines for hours people were
driving really far and using lots of gas
while I drove really far to get more gas
we heard from the NJ OEM which is the
Office of Emergency Management that they
had gas station information this is
great basically they have a data set
that shows us which stations are open
which are closed so this slide shows you
how the OEM was making this of
information available to affected people
on the ground what we have is a PDF
embedded in a PDF viewer on a website
it's great the data is out there
actually really great that this data is
open we'll talk about what we did with
it in a minute so they were getting this
data from the All Hazards consortium who
in turn was aggregating it basically
from point of sale units they were
monitoring point-of-sale units they say
oh if your credit card reader is off
you're probably closed so that's a good
signals you should not go there to get
gas you know I think that's a pretty
crafty solution in the time of crisis on
our task at hand was how to get this
information from this PDF onto a map and
what happened was very helpful volunteer
by the name of atanas and chef who
manually geocoded all of these points on
the PDF in turn made it into a map layer
on Google Fusion Tables which ended up
in this spreadsheet so you'll notice how
each row gets a unique stable ID which
helps us to track changes of status over
time as columns added for latitude and
longitude so we know exactly where to
place pins on the map and here's the
spreadsheet information on the map
success right so we've tried to do all
the right things we took data from the
authoritative source we made it more
easily available but it turns out that
the data we were getting wasn't exactly
up to date the OEM was updating its PDF
only once per day and we were actually
in the end doing a disservice not having
the most up-to-date information I guess
it kind of sucks going to a gas station
that you thought had gas only to find
that it actually I guess yesterday so
here we thought about getting help from
users on the ground users would come to
our map now we're going to ask them does
this station have gas does it not have
gas we took this data from the OEM Hut
that the OEM published once a day we
kept it up to
date with user reports on the ground and
this was circular once we'd get those
reports we'd send them back to the OEM
and here are just a few of the crowd
source responses we got from users the
first one says it's open i just filled
up there all types of gas available
second one says it's open so this just
pointed us at the need to do more crowd
sourcing with user-generated content in
addition to authoritative content um
next we got pointed to map ler this was
a gas station map being maintained by a
team of students led by one sue em out
of New Jersey these students were
working around the clock Manning phones
they were calling gas stations they were
keeping what became the official data
that the Department of Energy was
looking at to figure out where to send
trucks with gas and generators
eventually this data became our default
gas layer but on the bottom right notice
a recent feedback from google crisis map
we were able to put their crowd-sourced
data on our map and also send a get
comments from users and send that back
to them so that they could update their
data again so we were able to create us
a little ecosystem with these students
here is a photo showing this great group
of students helping their community so
the takeaways here for us or that
authoritative data is great but in a
disaster it can become outdated very
quickly for all kinds of reasons and so
you really need to take advantage of
local expertise on the ground to bring
about the best most actionable data in a
crisis if any of you find yourselves in
the position of curating a data set like
this especially in time of crisis please
keep in mind that single source hosted
data is the easiest to keep up to date
as soon as you send out an email with a
file attachment that data begins to get
stale so for this reason we tend to
prefer hosting rapidly changing data and
tools like Google Spreadsheets where all
you have to do is reload the page to see
the latest version if you're interested
in learning anything more about crisis
map we've open sourced the code
everything is available under the Google
crisis map project on google code and if
you're interested in playing with crisis
map there's a screencast linked from
this page that will walk you through
more of the details there's also a link
that will let you sign into our hosted
instance with any Google account that's
all for crisis map I'm going to pass you
back over to Alice who will tell you
more about person by in fair
Thanks so I talked before a little bit
of person finder and how it was
initially created as a hackathon and
after the earthquake in Haiti but the
situation in Haiti after the earthquake
of people searching for the loved ones
and their friends was not specific to
Haiti usually after every earthquake
that is something that that happens and
we had to launch person finder again for
the earthquake in chile in 2010 and then
the earthquake in japan in 2011 more
recently we launched it for the
explosion in boston and right now
there's still an instance running for
the floods that are happening in India I
want to give you a here a little bit of
walkthrough of how this tool is working
and then we'll go into details about the
standard that we're using here to
exchange data so person finder is very
simple as we mentioned anything that we
do has to be simple so if you go to
person finder and you can use the URL
google org slash person finder / test
new key which is a test instance that is
always running so people can play with
it get come here you have just two
button that you can click I'm looking
for someone or I have information about
a person that I want to share with
others so if you click on I'm looking
for someone you get to a list of results
so you're looking for your favorite
friend John Doe who haven't heard of
since the last since the earthquake you
can have several results of because
there's several people that can I have
the same name or entered several times
here you will have the status about that
person in green if you click on one of
this link you go to a details page that
looks like this you have information
about the person their name information
about the location some descriptions so
here you know usually about like the
size brown hair you can see the person
here was very stressed so forgets the H
so and and so you have information about
that person and then you have notes
attached to it so anyone can go here and
if they have information
about this person they can add a note
saying I'm also looking for that person
or I talk to that person on the phone or
I heard you know my brother had a
discussion with that person and so you
can share information with other people
even if you don't know these people and
that's the point of person finder is
sharing any information that you have
about a person with all the people that
you might not know you can also
subscribe to a record if you want to
have updates and received an email each
time someone post a note if you think
that this is not the John Doe that you
were looking for that there's another
one that is not in person finder then
you can go back to the original page and
click on this create a record and if you
click that then you just fill in the
fields and that's done you have the
information about the record and you can
go and regularly go see if other people
put some notes it's also used a lot and
that was the case after the explosion in
boston for people to just tell people
that they are fine so that they will
enter their own name so i will enter you
know name first name Ally's gonna be a
my last name and then I'm just going to
put a note saying I'm I'm Ali's Bonham
yet and I'm fine and so people going
there will be able to see that I'm fine
so this code as most of the tools that
we developed is right now open source so
you can go to code google com google
person finder and you can find more
information about it this is developed
on App Engine and as phil mentioned for
crisis map we have exactly the same
patterns on person finder where most of
the child isn't traffic something happen
and it's fikes and when you say it's
facts for the earthquake in japan in
2011 in a few hours it it's 52 thousands
of requests per second so from zero to
thousands of requests per seconds and we
had no issue because appengine just you
know create several instance as much as
as it's needed its App Engine and it's
Python on App Engine we use the data
store the App Engine data store to store
the records a person what is very
important is that we have a data API and
this data API is key to present
find it because this is with this API
that we can exchange data list of
missing people with other repository
that are out there are missing people so
we implement a standard and I'll talk
about this later 54 person finder
interchange format this is p thief was
actually it's a standard that already
existed before the earthquake in haiti
was created in 2005 for the hurricane
katrina and it's been it's been evolving
since then in person finder we
implemented search api array DPI and the
right api for this api most of the time
you need to have an authentication or
than sorry and authentication key that
you will need to request just because we
don't want to have everyone be able to
download long list of missing people and
so we just want to be able to track who
has access to what if you want to go
again this we have an instance running
the test instance doesn't require any
key so people can try and leave lot some
application using the key using sorry
the API without requiring a key so if we
dipped I have a little bit more into a
payav soap if it stands for person
finder interchange format and the link
here goes to that particular format it's
been based on very basic principle the
roots one convergence so we're going to
have different repository of data that
have different type of data and we need
to be able to bring them together and
the standard has to support that then
these data must be traceable very
important to know where the data come
from because you wanna you don't want to
trust information if you don't know the
source of the information so that's very
key then there's no central authority
there you have different repository and
that's the aggregator of this different
repository that can decide which data is
forced to trust and finally there gonna
be some duplicates in we talked about
this in terms of crisis people are very
stressed and they get certainly going to
enter several time the same person in
different places or you're going to have
two people knowing the same person
entering the same records so the
standard has to be able to support that
and be able to reconciliate that
if you go into more details pitov pretty
much it you have a list of records for
persons and then you can attach to each
record a note as we saw on the UI where
we implemented pivot then you can have
different repository each repository
will have a list of person records and
note records each repository is
identified by a domain name and that's
how we know where the data come from and
this domain name is used to identify
every record and every notes and that's
very important because then when
repository are going to exchange data
they're going to keep this single
identifier so let's imagine that you
have this food com you have a record on
there that you want to copy into another
one you into another repository you're
going to make this copy but keep the
original identifier so now let's take an
example of having four repositories and
we're going to create some entries and
copy them among repository and see how
we can reconciliate that so let's assume
that on january four we create a record
for bob what is important is the source
dates the souls day it corresponds to
the dates that this particular entry was
created then if this particular records
is copied from phu kham to ABC net on
january five what happens is the copy of
that records keeps the original so
states and we have a field called an
entry date the entry date keeps track of
when the entry was created in a
repository and the source date keeps
track of the date when these record was
originally created in the original
repository so now let's assume that you
go to back to food calm and you change
the records in that case everything
changes the so state and the entry dates
because you change the content so no
let's make things a little bit more
complex let's say that you make a copy
of this updated version of the record
into a third rib
tree on january seven so as you said
before we keep the same sauce day that
was generous six of the original record
and then you update the entry date to be
january seven if at the same time a copy
happens or a day later a copy happens
from the old one of the old version of
the records into another repository what
happens here is that the the su state
will still be the January for so the
out-of-date versions and the entry date
will be the new one so know what happens
if you borrow org and XYZ gov wanna
exchange the data what they realize is
that there's a conflict there they don't
have the same version and which one wins
how do we know so for that we heavily
rely on the source dates the source date
is the authoritative information and we
just keep the more recent so state so in
that case XYZ gov will get the latest
version of the records because it's
going to take the one that was created
on 086 we use the entry day mainly to
synchronize between repository and when
people implement pivot they use a lot
some API where you would talk to your
pros a tree and say tell me any give me
any record that was updated since that
particular date and in that case we use
the entry date so we use an API where
that will take a feel for the entry date
and give you anything that changed since
that date so we as I mentioned before in
person finder application that we
developed at Google we implemented pivot
in our API in order to be able to
exchange data and I want to give you a
concrete example of what that means so
when we launched person finder for the
earthquake in Japan on march 11 2011 we
launched in a few hours after the
earthquake very quickly it's fights a
lot of records went in there a lot of
because people couldn't call and
couldn't use SMS so they entered
information and we got contacted by a
lot of people to to that also had list
of missing people to put that into
person finder
and exchange information one thing in
particular was that a lot of people had
to run from home and go into some
shelters and they didn't initially had
time to bring everything with them so
what happens in these shelters is that
people to let people run them know that
they were in this shelter that wrote
their name on a piece of paper and it
looked like this look sorry so that road
names on piece of papers and put this
piece of paper on the wall of a shelter
and the few people that had that did
bring them phone with them took a
picture and applauded add into a picasa
album so there was all of these pictures
of list of missing people in different
shelters in picasa hallberg then
volunteers from all over the world that
did speak japanese would go on this
picasa album look at the picture and
transcribe the names so as a comments to
the photo you had a list of names and
then from there with an API you could
use the person finder Search API to look
if that names was in person finder if
yes then add notes to the records saying
we saw that particular name of that
particular person in this particular
shelter so people could have some lead
so it's never sure if it's exactly the
same person but the people that enter
information in person finder could
certainly recite receive this
information and be in contact with that
particular shelter from that efforts
done completely by volunteers we got
9,000 pictures that were taken and able
to obtain 137,000 records in person
finder and these type of things is
possible because we have an open API
because also the standard pivot is
simple so when something happens
developers can quickly understand the
the format quickly understand the API
and quickly develop some solutions and
we have stories like this in lots of
different disaster where people will
just code something on the fly to
interact with with person finder and
that's possible because of this open API
so that's the story for person finder
I'm going to hand our again to fail to
talk about one of our last project that
is mainly based here in York called
public alerts
hi again so up until this point we've
talked mostly about tools to aid in the
recovery and aftermath of a disaster but
one type of information that's critical
to mitigating the effects of disasters
is official public warnings and alerts
the for TV watchers the US has the
emergency alert system which is a
successor to the emergency broadcast
system that announces important alerts
to targeted areas from official agencies
like the FEMA like FEMA or the National
Weather Service so what's the equivalent
for the Internet age people are able to
find deluge of unofficial information
through social media but it's really
critical that they can easily find
official information as well in the past
almost every official agency that
produces such alerts users its own
format and its own distribution
mechanism some publish alerts to a
website some have a twitter feed some
offer email or SMS subscriptions some
don't have any information online at all
and even if the information was
published on a website common theme
people going about their daily lives
aren't necessarily visiting those sites
nor are the sites necessarily organized
for the right use cases and very often
those sites are not provisioned for
sudden spikes in traffic like in the
case of an emergency there would be much
easier if these agency if agencies and
organizations uses the same standard
ways of disseminating warning
information luckily there is such a
standard it's called the common alerting
protocol cap so it's an xml-based
alerting spec it normalizes the
formatting across many types of alert
messages it can provide a standard and
flexible way to target alerts by
dimensions like language category
geography so this was designed in
consultation with over a hundred
different emergency managers and it's
been adopted by multiple standards
bodies as an international standard in
particular I'm supposed to read it out
Oasis the organization for the
Advancement of structured information
standards and the ITU the International
Telecommunications Union so
organizations like the US Geological
Survey and the National Weather Service
already produce feeds of
warnings and notifications using cap why
is it so important if data providers can
provide atom or RSS feeds of their data
in this format then the alerts can be
easily exchanged across various
platforms so our team has embraced this
standard and we're doing as much as we
can to drive adoption of it across the
globe so i'm going to show you what
we've done to address this need for an
emergency broadcast system there are a
few different ways that we surface this
data but first we call this slide from
the beginning we know that people come
to Google to search when a major event
happens so in this case imagine that you
are the you are a user who's aware of a
wildfire nearby you've heard something
about an evacuation warning but you need
more information you search for
evacuation on Google people do this so
if there is an active warning at the
time of your search related to your
search query and location will show you
a one box like this note the snippets in
this particular one box indicate that
there is a mandatory evacuation in
effect so you would click on the more
info wing you're taken to a details page
hosted on App Engine with extended
information about the alert you just saw
now this details page is actually for a
tsunami warning I picked a different one
just to keep you on your toes because it
has a little bit more information on it
note the map on the right hand side with
the red markers clicking on any of those
markers will show you the estimated
arrival time and the expected severity
of the tsunami at that location and we
have similar map integrations and
structured data tables for all sorts of
different alerts if anyone buddy wants
to play with this if you want to see the
alerts active in our database at any
given time you can go to google.org /
public alerts so in addition to desktop
web search our team has done a lot of
work to make public alerts available
across a broad surface area these are
some screenshots of alerts as they show
up on mobile web search and on google
maps but if you haven't already gotten
word that there's something going on you
probably aren't going to know to search
for it so enter Google now Google now is
available for both Android and iOS
broadly speaking it's designed to get
you the right information at the right
time so it'll tell you today's weather
before you start your day it'll tell you
how much traffic to expect before you
leave work and if there's a public alert
near you Google now will show you a card
very similar to this one to let you know
about it here's a sneak peek at our
system diagram I'm going to step through
it in a little more detail but very
quickly alerts enter our system in the
red box at the upper left they flow
through the alert hub which manages
polling subscriptions it makes alert
data accessible to other subscribers
without adding load to publisher sites
alerts then enter our ingest server
where they're processed and saved off to
a geospatial index for quick serving by
a Maps web search and Google now we'll
focus first on the alert hub so we run
an instance of the open source
pubsubhubbub server it's kind of a funny
name it's anybody heard of it yes
excellent so pub is sense for published
sub subscribe hub is a hub and then bub
it was developed by a couple of Google's
in their twenty percent time and that
it's now in use across Google many
Google and non google products it uses
feeds essentially of HTTPS posts it's
simple and has no complex ap is it
supports https like i said so you can
pass through digitally signed XML which
is important for us it's efficient it
defines an easy way for publishers to
push alerts using HTTP posts all the
complexity is hidden in the hub it's
open we know this is important because
the standard is public and there's a
bunch of open source code for both
publishing and subscribing but finally
it's scalable so if you push alerts to a
hub not only will Google be able to
subscribe to them but so will anyone
else you can have hundred you can have
thousands of subscribers everyone will
get near instant notifications the hub
will handle the load so that you don't
have to and the hub does duplicate
detection says subscribers will see
alerts only once this is one of the
things we're doing to promote cap
publishing because if you're publishing
an alert that people need to see we
don't not everybody can actually has the
resources to run the server to maintain
that sort of load so this alert hub is
responsible for procuring updated XML
feeds of alerts publishers can either
ping alert hub when I have new data or
alert hub can pull them at a
configurable interval once it's
discovered new content and a feed alert
publishes that content to all of the
registered subscribers in blue and we
allow anyone to subscribe to feeds
through our hub as I
said this is just one way that we stay
true to our commitment to open data and
standards and our own ingest server is
also a subscriber so like other tools
like everything we do this runs on App
Engine after receiving a new alert the
ingest service job is to get it saved
off as quickly as possible so that we
can start showing it to users the first
step is permissions checks was this
publisher allowed to publish alerts in
this geographical area does the alert
expire in a reasonable amount of time so
a tornado warning is typically active
for under 30 minutes we don't expect to
see one that lasts 24 hours for example
that would get really confusing to
people and then we have a number of
other checks for things we've seen go
wrong over time we assign a score it's
necessary to make sure that our system
has some notion of the relative severity
of events so a tsunami warning would get
a very high score while an air quality
warning still important and interesting
to some we get a relatively low score
next comes the really cool part our
geospatial index is based on tokens
generated by the s2 geometry library I
would love to tell you more about this
you can approach me afterwards or take a
picture right down this link there's
some more information and then we
finally commit to the database part of
the deployment of this geospatial index
includes an in-memory cache that we
replicate widely that keeps track of all
of the active alerts in our system and
it's capable of answering queries in
under 2 milliseconds google maps google
web search google now i'll communicate
with this index as a part of responding
to a user request so this is you know we
rely on trusted partners to provide this
or authoritative information on
emergency events and this is just a
sampling of the providers that that we
work with and the types of events that
they publish of course we're constantly
working to add support for more
publishers across the globe so if an
alert provider has a well implemented
cat feed the our job of adding their
support for them the public alerts is
pretty straightforward we've noticed
though that even even though the cap is
a standard there are a few things left
unspecified they can make it a little
bit more cumbersome so to help new
implementers of the cap spec we host a
cap validator tool that can help you
recognize systemic issues with your cap
deviations from the specs and sort of
lint style messages based on our
observation this is we've open sourced
this cat validator along with the Java
library to aid in the creation of cap
you can find more information about that
and public alerts at these URLs that's
all the public alerts thank you thanks
so as we work towards wrapping this up
as an engineer working on these tools I
can say it feels like we're doing the
right things um but how do we know that
we're actually having a positive impact
there are various measures of success
but within our team broadly we'd say
that you know if we've contributed to
saving a life to preserving property to
averting some sort of misery then we've
probably been successful at Google we
like data you know this but these things
are hard to measure it's not a new
problem lots of response organizations
have similar challenges so here are some
of the things we look at we start with
user feedback this sort of thing is
really helpful sometimes it comes to us
via submit feedback links on our
products sometimes it comes through
email or social media the feedback for
us that carries the most weight is that
which tells us how we made a difference
how we helped someone make an informed
decision before or after an event so the
substantial feedback we get tends to be
positive this is good in fact this is as
much as as much measurement as most
organizations ever get but we put the
very bar very high and this is still not
quite quantitative analysis so we do
have some hard numbers about the amount
of traffic our products serve we saw
absolutely huge amounts of traffic to
our crisis map during Hurricane sandy we
know things about this traffic also on
October 29th the day of the day of
landfall in new york city we saw that
eighty-two percent of our traffic was in
the non-google referral category so in
other words external sites embedding our
crisis map sites like Huffington Post's
other news sites and other third parties
sharing the map help to draw traffic
towards us so this is a pretty good
signal that other free agents thought
our tools useful so we also watch these
numbers very closely but we're still
left
with this divide how do you make that
leap between page views and the numbers
I'm talking about and live saved we
don't know we're still working on this
so our hypothesis is that there are some
set of actions that are strong
indicators or predictors that are tools
delivered meaningful value this is not
just page views or clicks or bounce rate
this is deciding which interactions on
our tools are actually substantive so in
person finder if you perform a search
you find a record for a person that
contains a meaningful status maybe that
person has checked in or left a note
that they're safe that probably falls in
that misery averted category so check if
you print something or you ask for
directions you're probably going to
change your offline behavior based on
that content well that doesn't
necessarily translate to saving a life
it might be as close as we're able to
measure at this point in time I'm going
to pass it back to Alice for final
conclusions so so its conclusion what
what feel said is something that we
actively working on how do we know we
have an impact and how do we measure
that to also know what should be our the
next thing that we should be building or
how to improve the current role that we
have so there's tons of things that
remains to be done in the field and some
of them are how could we respond to more
disasters so in the last three years we
responded to more than 30 disasters in
10 different languages but there's many
much more we could do there's some
disaster that we could respond to and
one of the key for that is more
automation public alerts is a great
example for that during the the recent
tornadoes in oklahoma we displayed
tornado warnings to millions of users
without having us to do anything
everything was automated it just tornado
warning was issued and then it just
completely went to our pipeline directly
to users but that's not the case for
crisis map or person finder where when a
disaster strike we manually need to
create some the crisis map enable the
crisis map get some data set collect
information so we're always working on
automating more also trying to engage
and empower
citizens to respond themselves if people
can actually use the tools directly or
you with crisis map if people can create
their own data set and make them
available to users we also want to
better engage the community so we saw
with the crisis part during sandy about
the gas station where people will just
go on if you're in the gas station and
obey the status I want to do more of
that I think that that is something that
really has a lot of potential of how do
you engage the community to update all
of these data among that goes all the
social data there's so many tweets and
updates on Facebook and Google+ during a
crisis how do we make use of it right
now it's really hard what about a
there's an earthquake and there's ten
tweets that arrives about a person being
stuck under rumble how do you know it's
one person or 10 / 10 people that are
stuck at a rumble how do you know that
this is something that is still true
five hours from now how do you know that
there's already a search and rescue team
that is on the ground working trying to
save these people how do you coordinate
all of these efforts that is something
that a lot of responders agencies are
struggling with there's this overflow of
information how to make make better
sense of it that's a big challenge and
finally offline what do we do for four
places countries or communities that
don't have internet access or very low
penetration this is something that we're
still thinking of what should be a role
in in this place but I want to conclude
on reaching out to you how you know if
you're interested in getting involved
what can you do there's a lot of things
been done for those communities you can
join crisis mapper is a community that
leverage mobile and web-based
application that also share lots of data
sets of imagery 30 special platform
visualization tool crisis Commons is the
organization that I actually started
crisis cans I'm not sure if some of you
are familiar with crisis camp so when
some big crisis happened instead of some
camp Oliver in different cities and
people go there to her
that could be developers to to help
develop some tools with some api's could
also be translator or the group we saw
in that was in New Jersey about the
different gas station this is a little
crisis camp where people go and take the
phone and update data sets so if you go
to crisis comments that orgs there are a
lot of information about this there's
also lots of other tools Ushahidi and
sahana some example of such tools to
help during crisis management finally if
you just want to spend a day trying to
help and hack some solution go to some
hackathon random acts of kindness is an
example of an organization that runs
different hackathons all over the world
at different times of the year so you
can go on the website and check when
they lose some hackathon and a last call
for I'm happy to see it as several women
engineers or developers here in the room
there's the Grace Hopper conference
which is a conference for a woman in
computing and there's a special day for
open source data that's focusing on
humanitarian project and the project
like Sahana or google crisis map is
going to be at this conference for a day
of hacking that in minneapolis on
october six so if you're interested
please go and i think that's it so thank
you very much for your time and we're
going to take questions if you go to the
mic thank you it would just want the
question of the mic so that this could
be recorded
Bob shelter a observation in question
about the future we have increasing
amounts of broadband connectivity
provided by fiber with local battery at
the subscriber location we have a known
problem which occurred after sandy where
the cell sites had maybe 24 hours of
battery power before they died any
statistical data in the past with
regards to your queries as to how much
of the installed base so to speak of
connectivity is vulnerable to disruption
due to power and any thoughts on how we
fix this underlying problem because
frankly if the mobile sites go down
because the bland wine power is out
armies travel on their stomachs and easy
way to knock out an armored Brigade has
always been to blow the tank so fuel
train and we're all hung by our
umbilicals power yeah I mean the idea of
trying to gain some insight about power
connectivity of a community and how
that's vulnerable to disaster is a very
interesting one we haven't done anything
with that I I know that it's always you
always have to be careful when when you
look too closely at any data is I don't
even know what level of that data would
be available but that's very that would
something very interesting that for us
to think about in the future my
apologies but a follow up your data sets
may not go back far enough to September
11th of 2001 certainly not and I'm
talking about Google's generic activity
but one girder coming off the tower took
out broadband for most of southern
Manhattan the AT&amp;amp;T switching hub next to
the Trade Center got shish kebab and I
had clients on thirty-fourth street
without broadband for months
oh that there's actually some
organization that there was the case in
Haiti that comes in on the ground after
a disaster to actually set up some
networks and so that could be through
satellites that could be a fool or the
mains yeah yeah so you need if you don't
have enough access you need to bring
something yeah acknowledged I am Dave a
mobile engineer and there's been a bunch
of articles about Project loon that
google has launched with similar
question where google has launched Wi-Fi
balloons to help situations can you talk
about that and how it's progressing so
um I saw the video like the rest of you
and that's about all I can say looks
pretty cool though right but you can
imagine having a lot of potential in
terms of prices yeah hi my name is love
I have a question in line with the whole
test situation through hurricane sandy
how reliable is the traffic information
on google maps and how real time is it
actually because one of the ideas I had
sounds simple enough that if it's pretty
real time the more people are lining up
cars are lining up at a particular gas
station you can should be able to see
that on google maps and try to avoid
that so should be easy enough to create
a Google mash of sorts absolutely so our
crisis map actually has a layer for
Google traffic the same layer that you
would see on google maps and we did
receive feedback from users that they
were using that traffic layer in
conjunction with the map ler gas layer
to figure out which gas stations had gas
and the shortest lines right and there
are definitely cases where it did work I
I can't comment to as to the exact
accuracy but I know that there are
definitely cases where that approach was
work okay also one thing I found out I
tried making up a mashup
it seemed kind of tough because you guys
don't have any direct API so all you
really have is just enabling the traffic
layer and just seeing traffic for
particular address or particular
viewport are there any plans in the
making for an API for traffic data and
making that open source um you know I'm
actually not aware of it we we don't
actually work on the traffic team we
just consume the we just get to turn on
the layer not in my that's a yeah that's
a great idea of making everything open
source for crisis situations absolutely
thank you um I had a question regarding
I guess the prevention in part of the
public alerts so I guess what there any
plans to I guess kind of once we
realized there is less a tornado warning
or something like hurricane sandy what
there actually be some I guess were
there any plans to kind of
conglomerating anything like like
shelters you go to or basically any
supplies we could we could get
beforehand is I guess something like
that and I guess maybe it's even farther
like kind of take that you know what I
guess non-personal is there any way we
kind of make our lives a little bit
easier right so that when a crisis does
happen we're not suddenly all frantic
right and everything something spiking
yeah so you're talking about how to get
prepared fairness is it is also a big
sign of what we're doing and actually
the details page when there's a public
alerts turn in a warning and you click
on it there's links to how what what to
do just before a tornado and what to do
during and what to do after so there's
information here on how to get prepared
we also do some some working with some
organization to do campaigning about how
raising awareness of how to get prepared
in particularly in region I know in
Florida there was things about like
hurricane and how to get prepared for
huge tornado is a little bit different
here again you have a few days to get
you know things are coming and
definitely here there's information and
think some sites like FEMA have
information on how to get prepared and
so we tried to link as much as you can
to these places yeah yeah the flow the
state of Florida has actually put
together a has adopted crisis map to
build a prepared in a preparedness map
for their entire state along those lines
and so we definitely we definitely
helped to to drive that forward wherever
we can Milo ask a second question all
right oh I guess this is maybe more of a
side one was um I guess so what do you
guys do with you if your fault if you're
fed like false data or maybe someone
let's say and people find her um I enter
name and I accidentally misspelled a
person's name um and so I end up you
know creating an entry for this person
who i think you know maybe something
happened um is there like a good way to
deal with the case like that well yeah
actually we even had some spam and
person finder where people enter by the
information on purpose like some notes
saying wrong information so there's a
way to on Kristen finer you can delete a
records you can also report some notes
as Pam so they would actually be hidden
and other people will not see them so we
we build we integrate that into the tool
to allow people to so either as you said
like make mistake and you can delete
because in terms of crises you're going
to make mistakes also to just block
people that have bad intention to hurt
other people hi my name is token
bohemian um I think everything you're
doing here is great i love it but i want
to ask in terms of africa to you know
Nigeria Ghana South Africa I mean
there's are places where the internet
connection is not great but they could
use help so I just want to know what
your thoughts are on places like that
quote unquote third world countries I
mean you're completely right where
that's a part of what i mentioned
offline what do we do where people are
not that well connected and i think some
of the thing we can do here is actually
how responder el organization that are
on the ground by providing some of the
two
you could imagine you know taking
running your own instance of person
finder in some places where and then you
have people going on the ground and talk
to people and then in trying it to like
a few places where you have internet
connections but that's so far we mainly
put most of our efforts where there's a
good connection and this is something
we're thinking of and it's actually
working on this team sometimes you're
frustrated because you're I we could
help there's so many other places where
we could help and and so far you know
it's really hard to see what would be
our impact in places like this so if you
have any suggestions we welcome them I
was born there hi my name is ouch name
is Marco Varitek on the
telecommunications engineering from
Israel my question is basically about
the reliance on the Internet of course
everybody keeps asking the same question
once upon a time we had chacho right you
could do a google search through SMS
many such there are many times where the
internet availability is not of is not
anywhere near what SMS is capability is
example with you there might be more
support for older gsm for older gsm
generations and not for broadband ER or
any advanced internet support the
question was whether or not we could be
able to put a support for SMS SMS based
access to a person finder or even or
even finding or even the the the crisis
map this is something that I find that
should be brought back I don't know why
I cha-cha with this but not absolutely
so uh so an SMS gateway for person
finder is actually something that's come
up many times I it happened in japan
right and
actually in India recently somebody who
I'm not going to cheat on yeah utrecht
on in india and some feedback was that a
lot of people there will have access to
SMS but but no smartphones so we have an
open API so we just got an SMS gateway
and there's a number where you can
actually just SMS search and the name of
a person and then you get some results
and that was launched as of like a few
weeks ago so definitely in some
countries SMS is is the way to go for
crisis map I guess that will be a little
bit more of it depends exactly when you
want to support for SMS we will look
right okay location services are
available usually in the situation's I
think it's also very wise to access that
the same way where you can guess where
somebody's Nuala maps you can say no yes
I think that's the part if you have open
API is then other people can just build
some other parts that will just
integrate with you because it's people
on the ground that understand best what
are the needs of the ground and what are
like the resources so if we can empower
them we just say it is the data there if
you made the little bridge between this
apps working on the internet and people
here then you know go and use the API
and build a bridge okay so basically
empowering people to explain the API hi
I'm the name is Louise um you have
talked about um crisis response in cases
where nature it's and I mean we always
see that we have crises that are not
related to nature but political turmoil
or war and I was wondering if you have
in your roadmap and the intention to
cover and turn a dish or options for
people living in countries that are
suffering that kind of a crisis and
mothers looking for the kids or you know
a lot of what you mentioned for nature
crisis resonates with these kinds of
situations
um I'm not talking about supporting a
government or supporting any rebel
forces but you know bringing some tools
for families to get reunited it sounds
like a lot of what you have been
especially you what we talked about do
you have something like that in the
future are you looking into it I mean
that's something that we get asked very
often and so so far we put most of our
efforts trying to response to natural
disaster but one important point of what
we do is our tools are open source so
anyone can go and create their own
instance of crisis map can create their
own instance of person finder for
example and so that's why we open so
that so that if we cannot engage and we
cannot response than other people can do
at the same time you also need to be
careful in the way that you use these
tools you think person finder in places
where there's a lot of political
conflicts and war it means you're going
to have a list of people that you put
out there on the internet and and
everyone can access it so you have to
think about is it going to help the
people or can it hurt them more because
this list will be in the hands of people
that shouldn't have this list so that's
something that is very important to keep
in mind but you can you can start you
can use these tools directly my question
might have been covered can all over
that but I was wondering if you guys
have thought of with Google having
experience of having I guess your
enterprise level drop in like search
servers where the in-house if you set up
pre set up you know instances of this
where they might not have internet but
you might be able to want to generate a
week ago drop-off in instances and kind
of bring everyone's essential place in
order to link them up together easier I
know how Hades something like this where
was done after the earthquake Google
work with some isp to bring them servers
and
then go back up to speed much faster we
also did bring on the ground some laptop
with Google Earth and some data uploaded
directly into that because it was no se
ho we put satellite imagery available
for Haiti and people with like the
little like dial-up connection how can
they just get the satellite imagery so
there's a few things like this but it
was more like someone off and I yeah
it's yeah just thinking like with you
know the size of Google now in the
resource you have behind you the great
for you know especially for you know
google.org to have there was resources
in place and ready to go with you know
partying is red cross in order to bring
those to you know wherever Mike eat it
ok so um my name is Kalpana have two
questions basically one of them might
have been covered but the first one is
like basically how you're handling this
whole data that you have data from
coming from google map support I'm more
curious are you considering the data
coming from facebook twitter if so how
are you integrating it I would like to
know the technology that's used behind
maybe if you can allow like the Big Data
thing or the notes equal thing like you
know more technology wise rather than
you know as a business perspective I
like to know about that and the second
one is probably somebody already raised
not just natural disasters right this
thing that happened two days ago I
suppose it's a full flight number 124
I'm looking for I nobody was on the
fight that I knew but I just did look
for the flight work that my acid slice
it is app said that it's landed so I
looked up on google now which said that
the statuses on uncertain or something
like that basically didn't mean she's
kind of at the sign if my husband is
falling I like to know you know if the
fines landed or if is it something you
know at that point if you can call a
lines you can call our region cruise
lines which has like four thousand all
that but nobody you know you can't reach
in that kind of
our situation like this is a specific
group of data like you know you know the
passengers information is google going
to with hot you know what i mean like
basically it's not random data you have
specific data are you um you know if
there's a way of handling that you know
like you know there's a book veo so I
mean so for the first one I did make
sure I understand the question is I mean
if you're interested in knowing about
the the data standards we use to
interoperate right I like suppose the
data is coming from PDF and there's a
twitter feed away she treated feet which
is coming in json whore you bring it
together absolutely um so I mean I think
there was a slide where I went over all
of the the different sorts of map data
types that we support underlying Maps
API is sort of a very large
infrastructure that supports KML
natively so that XML format that
supports WMS this this web map service
that supports tiles on these sorts of
things how does it all come together
what I'm trying to say is this basically
that's huge amount of data coming at one
Lee like in nir like you know crashed
you might see data in the very first
hour you might not see for the rest of
three days you know that's only when you
have to bring it together you know what
I mean like I'd like to know how to a
doctor that's that's a big field of
research you know if you could just
ingest all the social data at vixen I
mean a lots of researcher working on
that right now and all right I'm trying
to understand if you're putting some
machine learning algorithms together
behind it or either any anything that
perspective eyes if you can throw out
some ideas how we are here we would love
that yeah so like everything like
everyone these are some problems we're
thinking about but we yeah we don't have
anything right now
which was how to best have like
real-time updates about things right
like you have a specific data that you
don't have like you know so many act
number yes dark but you just know how
many people but i think that's that's
part of the after like in san francisco
the after the plane just crashed I think
the fastest information you have come
from social networks right before I like
any official sources you've had in that
type of situation so I think that goes
back completely to your first questions
about how you could have that type of
information via feed of like public
alerts and then you'll have in Google
now a card not just lettuce tomato
warning but here is the information and
right now some of that makes it who like
Google News for example it's it's not in
seconds but now it's in a few minutes
and right now if you go in in your hair
I mean I heard about it very very
quickly just after just from all of
these things and yeah that would be
interesting to almost have we have
authoritative sources as input into like
this public alerts tool but you could
imagine having like a community feed or
based on magically understanding all of
these social data and making a pool and
being able to tell people about it but I
think pretty much is this issue on how
to do it I problems fastest data on just
they follow last question fastest data
on a flight status is probably flight
tracker which copies the FIA fee in the
faa zee air traffic feed digitally
aircraft airline dispatch uses that the
schedule things like the baggage carts
because there's a sterile cockpit below
10,000 feet that one for a while marked
the plane has landed well i believe they
identified it was a problem real fast
the comment i was going to make to
reduce load and emergencies there are
two categories of users of people finder
related tools there is the incidental
user who's
involved on the receiving end of a
disaster there's also what I'll call
professional responders namely first
responders volunteers and like we're
going to be in the system chronically a
suggestion produce a app download not an
HTML based and JavaScript based they
compile it into an app for iOS and
Android so it can be directly downloaded
onto a tablet and will only make the
daily queries up the stream in an
emergency response situation where
you've maybe got a truck with a mast and
a Wi-Fi note on it and a satellite phone
knocking down 7 70 or 100 k 2 3 or 2
will go a long way toward increasing
usability that's I bet if you look at
your IP addresses and your traffic logs
there's a distinct population which is
banging away on it constantly take them
off the off the bandwidth take them off
everything else major change second of
all they related comment and instead of
a server deploy a proxy that have a
proxy box it can be literally the size
of a handheld that provides the
JavaScript files locally that is local
ISP can resolve people finder dot type
people that people find a JavaScript
google com themselves an answer it
directly off the handheld that will also
eliminate a lot of uplink traffic
chattering back and forth I think God
look pretty make sense I think the part
about installing an app the issue is
that people don't install an app in case
there's a disaster
for professional dress code okay others
yes yes yes exactly another board amount
by proxy yeah so then you save the band
yeah I d I'm just repeating because you
don't have the my instruction yeah the
IDS try to save the bandwidth for very
important traffic that's pretty much the
idea there can we like you yes two more
questions alright thank you so for
projects like this it it sounds like
you're you're dealing with a lot of
governments particularly local
governments and local government often
doesn't have a particular sophistication
when it comes to the IT technologies
that that they have do you have have you
compiled list of recommendations before
hands that that local government could
use I mean going beyond your file
formats presumably there are some other
types of data that they could be
gathering in case of emergency maybe
have things ready in particular formats
and also I'm wondering how often you end
up collaborating with with local
charitable organizations that are active
in in dealing with with the responses to
these crises so our I mean we do have we
do have several people on our team who
work sort of non-stop in in partnerships
mode working on preparedness with local
governments and emergency agencies so to
that question absolutely absolutely we
go out and we work with people second
part I'm sorry what was the second part
okay oh no call agent local
organizations yeah charitable
organizations yeah I mean pretty much I
think for each crisis we work with
whoever is responding on the ground so
for ago
like his group of students in New Jersey
that was mapping the gas station you
know we work with them so pretty much is
it there's a lot of local efforts and in
some groups that will get together and
try to do something and in this group
may not exist before the crisis they may
just spontaneously get created at the
time of crisis and then if they have
valuable data and we think that it makes
sense to work together then then yes we
do hi ladies I live um you'd mentioned
earlier that someone was asking you to
make a direct connection to their
relatives because they didn't have
telephone service had you guys
considered implementing something maybe
in people finder where people could do
that on behalf of others in other words
make a direct connection that make sense
though people would volunteer people who
did have phone service someplace else in
other words could volunteer to make
those direct connections on behalf of
people looking to reach their family
members oh so for example that way that
was done a little bit like manually in
some of this crisis cam that I mentioned
before and so in Haiti after the
earthquake in Haiti there were some
groups of volunteer that pretty much
will look over person finder where
someone say I want to let my family know
they're in in New York and hear the
phone number and there was some
volunteer that would go through that and
make these phone calls and and so that
when and then we'll update person finder
back on this so there's no right now
like to support for that it's more like
spontaneous groups that but I think
there is there was some discussion so
you shaheedi is a tool that was created
after the riots following the election
in Kenyan 2008 and some part of that of
the tool the goal is to able to map some
incidents and I know they tried to also
integrate some tools that we little
things a little bit like this but it's
important
years for the earthquake in Haiti they
developed a project called 4636 so
people on the ground could text the
message at Cross 636 that was a pretty
phone number where they would say I'm
stuck here and I'm located in this
particular place usually that was sent
in Haitian Creole so that was sent to a
server that you shaheedi was hosting in
the US and then you had volunteers that
did speak haitian creole will read that
particular message will do quoted on
page because the location was not an
address the location was usually I'm you
know near the supermarket that is near
the church in this neighborhood which if
you from there you know where it is but
if you're not you don't know so people
would go read that put on the map
categorize it is it someone stuck under
rubble someone needing water and then
based on this it would branch to go to
there was US Coast Guard to go and how
people or some we're sending an SMS
thing let my family know I'm safe and
then they will send for the present
finder API so they did something that is
trying to use the crowd and
crowdsourcing that information</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>