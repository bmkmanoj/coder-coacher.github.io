<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Reconsidering Relevance | Coder Coacher - Coaching Coders</title><meta content="Reconsidering Relevance - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Reconsidering Relevance</b></h2><h5 class="post__date">2009-01-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4LZNqV4qZR0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">what I'd like to do today is talk about
relevance in a somewhat critical eye and
since you know we're a cozy enough
audience feel free to interrupt at any
time if we go too far off track I might
push things till after the the end for
questions and answers so just to
introduce myself my name is Daniel
tongue clang I'm the chief scientist and
a co-founder of in deca my background I
did my undergraduate at MIT in math
computer science 1863 for books for the
numbers and my PhD at CMU my background
is actually not in information retrieval
per se I really studied more
visualization of information and kind of
cave sideways into the IR field I
actually think that in some ways perhaps
gives me I know the advantage of an
outsider and questioning some of the
canonical ideas of the IR field
obviously this is not a sales of
recruiting talk for and Dekka but for
those of you not familiar with what we
do an enterprise search and information
access company and a lot of the ideas in
this talk I would say oh it's worth
philosophically the foundation of our
approach so the overview I'm going to
ask us to reconsider relevance then the
least I can do is define what i mean by
relevance so I will do that and then
offer a critique as well as a
exploration alternatives but this it's
not enough to knock something down we
have to offer something constructive but
with the title of this talk
reconsidering relevance is actually it's
an allusion to a paper by thethe ghost
or a civic who's the library scientist
at Rutgers I wrote a paper called
relevant to reconsider he actually wrote
that think about 25 years ago so it
gives you a sense of the amount of time
people have been thinking about this i
highly recommend one of his lectures the
2007 Lazzaro lecture it's available in
video if you liked this talk you will
love his he's a much better speaker
so let's first set the stage now I know
that I will never be allowed in Google
cafeteria after this but I actually want
to offer comparison that I think has
some merit and I actually don't think
the big is particularly derogatory which
is between Google and McDonald's now
these are obviously two of the iconic
industries I'd like to say McDonald's of
the 20th century Google of the 21st
remarkably similar taglines even to the
point that the main competitors of both
have interestingly similar approaches
Burger King's with have it your way and
Yahoo actually doing something
remarkably similar with its boss builder
on search engine as sort of the well if
we can't win on the default let's offer
a non-conformist alternative now the
success of both of these companies is
what people in the sort of the NBA
literature folks like Deming in the
total quality management school which
says scale orchestration right it's it's
a triumph of process but there's a dark
side yeah I'm not claiming that the
Google is doing anything as as harmful
to our bodies as what a certainly an
overdose of McDonald's might do to
someone who eats it non-stop for a month
but there are actually been people
questioning whether precisely the the
simplicity if you will the prefab
experience of Google is making a stupid
this is a Nick Carr article in the
Atlantic that drew a lot of attention
now okay it's important to not just look
at what people in the press or people
who can come up with cute slides might
have to say users are ultimately what
matter and if you look at users
specifically not just users of web
search in general but specifically
Google's these numbers are incredible in
the high 80s mean this is the most
satisfied user community I think you'll
find for a business especially one that
caters to such a global
audience so why you know if it ain't
broke well I try to fix it and there's
an interesting observation you can
easily find this online and on the blog
of the person who wrote it but what uh
what this guy John said was that look
search on the internet is a solved
problem i believe Marissa Mayer said
something similar although she capped it
at ninety percent solved and his
complaint was about search in the
enterprise now not actually care to talk
about the difference between Enterprise
Search and web search can do that
another time I actually think it's the
wrong way to look at what John was after
here the real issue is that there's a
notion of search on the internet as one
kind of problem and search in the end
and the enterprise is a different kind
of problem perhaps that notion isn't
even made explicit so people assume it's
a question of the domains but it's
actually but there are different kinds
of information seeking problems involved
now let's see also what this has to do
with relevance the best way to do this
is with some examples let's look at some
easy and hard information seeking
problems and i'm using examples that I
hope will be clear is not being some
kinds of corner cases that you know well
we could solve 99% of the problems those
ones aren't ones that people care about
I think these are real problems and
incidentally ones that people are able
to solve it's just that they don't solve
them in a relevance eccentric way so
where I want to find where can i buy and
Yuri exile that's pretty easy right it I
would hope that if i type ender in exile
to google i find it within seconds and
it's just a matter of you know who's
done the best search engine optimization
perhaps to get their copy to me no
problem but let's say i'm taking a
vacation so I do every few years and I
want a good novel to read on the beach
it's a real problem it's not clear how I
solve that I mean I know actually the
way I would solve that is by asking
friends
it's but it's certainly not something
where I type good novel treat on the
beach and expect some combination of NLP
and you know knowledgeable my past
behavior to certainly crank out that hey
Orson Scott Card wrote a new book you
should read it taking ones that are a
bit more technical finding a proof that
sorting with only access to a binary
comparator is n log n that's pretty easy
anybody who's trying to cheat on their
algorithms problem sets can probably
find that one again in seconds but let's
look at a problem that's actually
similar enough I have a partially
ordered set and a comparator and I'd
like an efficient algorithm to solve it
I actually this is a legitimate one and
if in fact uh I don't know if you guys
are still interviewing lots of Engineers
but I welcome you to use this one as an
interview question I tried to look up
that answer just like I looked up the
one for the previous one and it took me
several hours to realize that I wasn't
getting anywhere with it so this gives
you and the reason is it turns out is
that it's not a neatly solved question
for which somebody has written a
document to answer it but here you have
two questions one is easy one is hard
you can't tell from looking at the form
very clearly that one is going to be
easier or harder and yet the difference
is there so these are you know there's a
feeling of easier and harder information
seeking problems and the easy ones the
relevant centric paradigm works quite
well for the harder ones it doesn't so
with as a context let's actually try to
get a working definition of relevance or
at least something that we can get a
shared understanding of now alluded
earlier that there's a history to the
relevant as a field and it's about 50 or
60 years old to think of relevance in
information retrieval one of the
earliest folks to work on it was William
Goffe mean it was actually a a believe
is considered a mathematical information
scientist was the title and he made an
observation which is surely a
uncontroversial to most people which is
that there
relevance is a way of relating queries
to documents that's the the first part
in fact that is how its defined by most
folks but the second part is one that I
think a lot of people overlook which is
that that relation isn't enough and it
goes on and there's a literature that
that runs with this and again I
recommend tempest or a civic or stephana
bizarro for giving more of the looking
at relevance as something as a work in
progress as opposed to a function to be
optimized now I decided that it would be
a good idea not to come up with all of
my own definitions for Sandra our
concepts so let's borrow a few from what
I think is the most standard textbook in
the field which is advisor Yates and
Rivera NATO's a modern information
retrieval so I are let's say actually
have a somewhat conservative definition
it's basically retrieval of text
documents but I don't think there's
anything in their definition that is
necessarily restricted to text so that's
fine more important part is that there
is a presumption of user information
needs so let's dive into that and there
they have actually what you might think
it was a peculiar definition which is
that a user information need is a
natural language description of the
informational need of a user now that's
a little bit circular but you can take
that up with with Ricardo the important
part there is that they're stuck because
typically you're using these definitions
operationally to evaluate information
retrieval systems so you can't just say
that I need is something that exists in
the ether in someone's head instead they
modeled these as text descriptions but
can live with that the important pieces
of course that there's also a notion of
a query and a queries an expression of
that information need which is in a
language provided by an information
system the case of Google obviously it's
a it's a collection of words or perhaps
in the advanced search there's a little
bit more going
to structure that query now
interestingly that glasser didn't have
relevance in it which I thought was a
little bit odd but it's implicit because
when you define relevance and
information retrieval what you actually
do is to define an information retrieval
model which is if Gotham put it as a
function that given a query in actually
in some of the more purist I our assigns
a binary value of each document either
is relevant to that query or not and
actually more carefully it's relevant to
the information need or not because if
you have this of course you can do batch
testing you can say here's the
information need here are the documents
I know irrelevant I take a system how do
I evaluate it I look at how many of the
documents it got back that it was
supposed to and what fraction were bad
you get precision and recall and that's
hunky-dory and you get lots of papers in
cigar if you could keep improving on
this now once you have such a system you
can use it and this shouldn't be all
that that's surprising right I have some
information need and let's dispense with
this notion that that we have a natural
language representation or heads that's
kind of not too important but somehow I
translate that into a query then I send
you to the pigeons I hope all of you are
familiar with pigeon rank since i
believe it was developed here on april
first of two thousand four and no the
patient's use their various algorithms
these ones are pretty smart they've
learned about tf-idf and page rank for
any of you who haven't I actually
recommend modern information retrieval
as a text and finally they present a set
of results presumably ranked in either
the probability of relevance or if you
have sort of notion of the amount of
relevance you can actually live in it as
well and then you pick boy and you know
sort of Monty Hall let's make a deal
style you open your result and you hope
that it's not a goat
now great seems like a reasonable model
what's wrong with it well we have some
communication problems and this nice
little flow the first thing is that
information needs even the natural
language representations and their long
I don't know how many of you are
familiar with thee with Trek but I mean
I look at these things I think who even
comes up with these that can be 20 30
words as you know is famously quoted by
Googlers and others the average query
length last I heard was 1.8 words let's
even round it up to 23 but clearly the
you know the amount of information that
a two or three word fury can bear is
nowhere near as rich as what might be in
someone's head natural language people
have tried it apparently you can sell a
company for 100 million dollars if you
can generate enough hype about it but it
hasn't really taken except in some very
limited cases like call centres where
people are speaking it can make some
kind of sense perhaps to convert it
telepathy it's actually not a horrible
idea and for any Samuel ohms here at CMU
they're actually researching telepathy
as an interface but it's not quite ready
for prime time so that's one problem
then the next stage well the pigeons
might be smart but they're not going to
be perfect and these I our models
they're only as good in theory as the
ability to consider relevance to an
information need as a function of the
query which let's remember is an
expression of that information need in
the first place and we also have
cumulative error now so in this sort of
game of telephone things are getting you
know going from bad to worse potentially
and then finally the notion that simply
you get back a set of results and select
from them
at a time it's a bit inefficient because
obviously you're getting a small
selection of possibilities of the ways
that's interpreted you're going to
select from what's put in front of you
and if you don't see what's there it's
not clear what you do next you might try
to read what's there and learn from it
but essentially it's try again there's
not much of a back-up plan and I like to
pull this out as an example and it's
remarkably resilient and works very well
for information retrieval conferences
when I look up I are I'm thinking of
information retrieval I guess a lot of
people aren't there are a lot of IRS out
there but and this is beautiful and
maybe even meets the needs of I suppose
twenty thirty fifty percent of the
population it's hard for me to know I
don't have the stats you guys stupid I
doubt you'll share them but I doesn't
actually tell me what to do now of
course I have a brain and I realize oh
maybe I should type it all out but this
gives you some sense that when things go
wrong the interface hasn't helped me and
relevance obviously subjective I mean
you might might be that over some
averaging most people who type in IR
would be satisfied with one of these
it's a little bit of satisfying and
let's take you know you could say well
that was a trick one because I took it
you know deliberately ambiguous
abbreviation fine search engines now
this is something that at least by my
understanding Google is really good at
stuff involving search engines on this
list of results and frankly even the top
ads I mean maybe people have learned
that buying AdWords for search engines
is a strange thing but it's not very
informative it's not bad actually in the
sense that probably that Wikipedia entry
for web search engines is a good place
to look I'm surprised it's not the top
result but this is not giving me a sense
of the
information that is available about
search engines I'm you know I wanted to
get it here that the communication
channel of a ranked list is a death you
know okay and the saving grace ear is
incidentally that wikipedia has done a
great job of organizing the information
here soon as you can find that you're
probably in the clear but good luck if
you're looking for something like this
that isn't so sort of summing up what's
wrong with the with the approach it
assumes that you know what you want it
assumes you know how to say what you
want it assumes that what you say will
then be interpreted by an IR model that
you can trust that the answer is in a
single document of what you're looking
for and so that I mean that's implicit
in this notion of returning a list of
documents ranked as your results and
finally that you're optimizing for doing
this in one shot because if you have to
do it again it's almost like starting
from scratch so can we do better and as
I'm looking forward to screaming hordes
saying in a few weeks yes we can so well
I have to actually proposed the change
we can believe it so what are the
alternatives well a few years ago Gary
martini is a professor of library and
information science and I believe also
have computer science at University of
North Carolina gave some lectures called
towards human computer information
retrieval and he looked at the work that
had been done and searched and more
generally what's called information
seeking the library scientists have
slightly different terms for what most
of us end up calling search and said
look the problem is really simple we
shouldn't be having the machine try to
read people's minds I mean we don't do
that as people and we're not even
necessarily self-aware enough to know
what we want so let's not be in the
business of having the machines primary
responsibility be to guess the users
intent or maybe let's do as well as we
can
in there but then let's optimize for the
communication between the human and the
computer and the second part is
important to which is that we should not
just ask from computers and from search
engines what they can do for us but ask
users what they can do to help
themselves find what they're looking for
let's make this a two-way street and so
we want to ensure that if users are able
to put more effort in they're rewarded
for that effort which to put it a
different way for anybody who's seen the
Charlton Heston movie Soylent Green
where at the end he discovers that
within the food supply its people and I
want to take a very concrete use case
it's for a few months ago that
illustrates what an HCI our experience
is like so a colleague of mine had run
into this guy Steve polit and said hey
this looks like stuff you should be
looking at that's I said sure I'll check
it out and yeah I like Google my until
recently actually is my primary search
engine and the Google's it found him on
linkedin turns out that's a different
seat polit the the OCLC one is is right
but it didn't really get me very far
wasn't that helpful as it well okay I
the guy told me some kind of research
thing so I'm using the wrong interface
let's try google scholar now that does
better I'm finding a couple of papers
bye bye Steve paulet yeah document
retrieval this sounds more up my alley
the glycoprotein yet sure etic peptides
sorry I don't speak organic chemistry
different polit but that already raised
some of the problem when we saw before
right we have a nigga's name so i guess
i could try you know refining my queries
against the
pilot computer science quote in a s
polit polit I decided to actually let me
try something else at the University of
Massachusetts they have something called
rexha that's essentially but for those
not familiar with it the Center for
intelligent information retrieval at
UMass Amherst is an amazing source of
lots of IR goodness such as the
involvement in the Lemur and injury
toolkits I believe you guys have hired a
bunch of their their graduates actually
well what's nice here is that even
though it may be a ranked list I at
least was able to get ah that's that's
the pollen I'm looking for and hone in
on his documents this is improvement in
a significant one because yeah he hasn't
lived that long and we actually died
last year but I could go through his
writings that's better although honestly
I was hoping for something a little bit
better than reading everything he'd
written just to find out of what he was
what he was up to so this is an
interface that I designed with some
folks at in deca in collaboration with
the Association for Computing Machinery
we've been working with them with their
digital library how many folks are
familiar with the ACM digital library
okay so most of you so it's all computer
science documents similar in that sense
to rexha but what's nice about the ACM
documents to the extent possible they
demanded their authors actually tagged
documents with what they're about the
instructions are what would you want
somebody to find in an index if you
imagine you know back when people use
books indexes I guess people still do
that sometimes to to look things up what
how would you tag it now Lennon when we
first started working with him we said
this is can be great because everybody's
tag your documents and I think we all
know that it's a lot of effort to get
people to tag documents so this is this
gives us a leg up and making this kind
of more exploratory interface possible
but we were wrong because it turns out
that authors as my
they know about their documents don't
tag them in the way that other people
would want to find them to have a tunnel
vision and they're tagging was very
sparse however there's wisdom in crowds
could borrow a quote from Peter Norvig
more data versus better algorithms well
we needed the algorithms but the data
certainly helped so we were able to
basically bootstrap on the author's
vocabulary collectively to generate the
vocabulary that we needed and then we're
actually able to tag other documents by
text mining for that vocabulary in them
you can actually read this as a paper we
presented at the second workshop on
human-computer information retrieval
this past year now a 2008 it's online
but immediately we see what Paula is
about when we look them up and I looked
up just politic of the we don't have any
support for a gaming gah variation here
and we see search information retrieval
other things and I said great let me
actually refine to just those the
documents tag that way and so now we see
a pilot and information retrieval and I
see refinement is one of the tax and I
said okay that's really up my alley
because if there's one thing that now
I'm concerned with certainly at a deca
its interfaces that support refinement
and the information retrieval process so
I keep going and in fact at this point
Paul it's great but I'm interested more
generally and who else has worked on
information retrieval and refinement I
mean I might not have realized that
there was this wealth of things out
there when I got in but if you're going
to show me stuff that I want I'm going
to dive into this and you know it is
kind of a rapid walkthrough of it but
what I want to make clear here is that
my information need changed the system
helped me with something more than
ranked lists it's supported exploration
by giving me directions to looking and
worked with me now I know that there's a
lot of skepticism about more complicated
interfaces than search but I hope it's
clear here that there's value
that I got from this to a real
information need that I could not have
gotten if all I could do was repeatedly
search and get back rank lists now with
this as sort of a a concrete example
let's see what this end up looking like
in a more technical perspective
attending the earliest work that you
could think of as HCI aarush is the
scattered gather work done at Xerox PARC
I think it's in the early 90s by I'm
actually blanking on the names and I'd
like to give credit where credit is due
and that would be to Doug cutting
believe is now at Yahoo David kharghar
yon Peterson and John to key and they
started from a premise proposed by then
read this risby Orson sorry is one of
our founding fathers of IR and I said
that basically documents that are a
correspond to the same information need
should be grouped together so so well
let's instead of just trying to say well
let's hope that when people put in a
query all the documents come back
together let's cluster them now fifteen
years after the fact this may not seem
all that novel but this was a big deal
this was really i think the first work
on document clustering and here you see
a query for star and the documents are
first clustered based on inter document
similarity and then they take those
document clusters and they try to find
the most frequent or information bearing
terms within them that's not bad right
you get four star you get stuff related
to the star-spangled banner or you get
Hollywood stars astronomic stars kind of
the flora and fauna category this is
is a nice step now the problem with
clustering though is that it doesn't
actually communicate as well as you'd
like the documents similarities are
somewhat opaque I mean you might be able
to explain them my saying look we took
this bag of words this dagger boards
they're two vectors we can do to the
cosine and tada you know the aristocrats
but the users don't like this and it
actually it's a real problem it's the
reason I think that clustering oriented
interfaces have not taken off it's not
so much that they're not good enough is
that they don't fail very gently faceted
search on the other hand has done
somewhat better just quick show of hands
folks familiar with faceted search
smaller set so I'll explain it and
forgive me those who are bored back I
think that's quite a hundred years ago
there's an Indian librarian Ranga Nathan
who would look at the way that libraries
are being organized using taxonomy right
this is familiar you know tree oriented
categories and said this is really bad
because there may be multiple ways to
organize things and you want to think
not in terms of just one hierarchy but
multiple ways of categorizing things he
took this a little bit further than I
think was appropriate by actually
thinking there was a canonical set of
these facets for all information in the
world so that you could basically assign
any fact or any piece of information
kind of a point in this five dimensional
space perhaps because of that rigidity
he was not able to get the system to
plant the Dewey Decimal one although he
still revered I think as one of the
greatest librarians historically and
particularly from from India but the
idea of faceted classification as
opposed to taxonomic classification did
start to take off more generally in fact
Steve polit whom I was talking about
before proposed something called view
based searching which is essentially a
form of this facet of view but I think
more than anyone else Marty hers that UC
Berkeley with a flamenco
project put faceted search on the map
what you see here are this is this is a
prototype application they developed for
architecture you do a search which is a
full-text search for this case Gary and
it allows you to further refine by the
facet of people periods location
structure types and the idea is pretty
simple you're able to progressively
refine your query using the different
ways that makes sense to narrow and one
of the nice things about about Marty's
work is that she does lots of user
studies and a lot of people believed
very strongly that people love search
the hate navigation search is more
efficient navigation never works as
you're actually using I did a bunch of
use cases and found consistently that
people did better on this style of an
interface even though it was less
familiar to them and it's not like they
were being you know bribed to prefer it
as well so this is this is a big deal
now you might wonder given her success
why aren't we seeing faceted search
everywhere once you are seeing you in a
bunch more places we're seeing it I
think at this point almost everyone
frankly except for Google Product Search
is using fascinated search in e-commerce
I'm thrilled a lot of them are a deck as
customers but a lot of them aren't at
this point it's the norm you see it on
amazon you see it on ebay now the
challenges though end up being that the
original faceted search model assumed
that you simply showed all of the facets
and all of the values that were
available with a little bit of smarts
about it you would show a value that led
to no results but that was about the
extent of it now that breaks down it
turns out there are too many facets
there are too many facets and those
first two problems are hard enough but
then the third problem is actually that
you know the folks saying that people
don't like complex interfaces there's
some point to that sometimes it is easy
to figure out what the user wants
in fact sometimes it may not be easy to
figure it out but faceted refinement is
not the right way to figure it out as
you guys are surely familiar with from
things like did you mean so just
interact with users doesn't necessarily
mean that you want to use fasted
refinement probably the last one is the
one that people get most hung up on
which is that this is great but how do I
tag everything now I hope the example I
showed with the with the ACM data shows
that you can at least semi automate this
process and I'll show an example later
which does even more of that now which
ones to show this is home depot and the
deck of sight and if you're looking for
microwaves you see things like the
microwave capacity the wattage the color
of the finish you're looking for ceiling
fans you see color finish still applies
but you say things like whether it's a
remote lighting &amp;amp; fans style in the home
depot's case I believe that's done
editorially or through some that the
they've decided whether through explicit
or implicit selection when facets come
up there are other ways to do this right
you can use information measures to
decide oh ha at this point to fasten a
sufficient coverage or sufficient
entropy that it's worth using and this
turns out to be critical because the
number of facets even in a relatively
simple ecommerce site can be far more
than you would want to show to users and
let alone if you're looking at something
like for example in manufacturing
application where almost every kind of
part has its own fast at some
overlapping so it is important to use
some kind of measures to favor the
facets as I said coverage information
measures these do quite well the thing
to keep in mind of course is that that
happens behind the scene there's still
transparency as to what these things
mean when they're available now the the
other thing I want to focus on I'm not
going to talk about which values to show
but rather when to use faceted search
and there's a there's a bit of a debate
in the in the facet search community as
to whether fast
refinement versus clarification using
facets mean the same thing no and I say
there's a debate I mean I I'm perhaps
blowing it out of proportion I mean
folks like me sudam a and Microsoft
Marty her set at Berkeley but let me
explain what what I'm after here the way
that faceted search is typically done
and for those of you who have seen it
and perhaps now that you know what it's
called recognize it you do a search it's
full text you get back a bunch of
results you can slice and dice it but
some queries aren't as good as others
and you may get have a query that comes
back with a bunch of photo technically
crap that is there's really nothing
coherent coherently holding the set
together and faceted search inherently
is a set retrieval approach not a rank
retrieval approach you get back anything
that matches your set and that is your
your expression that you used in the
case of a query it's like looking at the
about 1 million documents that came back
not just the top 10 otherwise it
wouldn't be much point and refining it
but if that sets all over the map
suggesting the people are they can
refine by brand or a price or what have
you is kind of silly they're not there
yet they've made a query that's
ambiguous you first want to clarify
their intention you want to go from
something that is ambiguous to something
that may be broad but is not ambiguous
so here I do admittedly a toy query but
hopefully when I could that communicates
the point of it I do a query for steamer
well it turns out that the system is in
no position to know whether I want to be
steaming mussels or standing muscle
shirts but this is this is a classic
case up there with java jaguar die
forget the other classical list but this
one happens to be one on home depot so
what do we do we recognize that just
from the query text itself when we say
aha let's instead of doing full text
search do what used to be called
category search we consider it dimension
searched mortgage we're looking in the
space of fast but you get the point this
is the way we push this clarification
process vixen well this is kind of easy
oh it can be a little bit harder so
here's a query for story this is a
prototype application on a its retail
data but it's all over the map lots of
books lots of electronics it's a large
let's call it fortune five retailers
data and if you do a query for storage
it's not so much that it could say aha I
have a value of storage here it can say
look I can tell that this set doesn't
hold well together it's actually what
it's doing here is it's actually
piggybacking on the on the relevance
ranking a plus it on and saying when are
your relevance ranking the top results
don't look like they are representative
of the set of things that came back
uh-oh that means that either I have at
least two working hypotheses the set of
results is a good one or the relevance
ranking knows better than the set of
results and is actually pushing things
at the right direction and so when in
doubt do what uh what fine movie do you
just ask and we say hi well let's figure
out what's special about that beginning
set and we look for the most information
bearing term in this case now we are
using the tagging that's there with the
heights this category toys chest and
storage while if we say what makes the
set most different from that well it
turns out it's computers and just to be
super careful that we're not relying on
the metadata being so great we show
exemplar documents and instead of just
showing all the ways you could slice and
dice the set we promote these saying
we're pretty sure that we don't know
exactly what you want so let's ask this
may seem like this is what this is what
people do right if you don't understand
someone you ask you reflect back to them
what they said and when you can you
propose meaningful directions and by the
way when it's obvious what they said you
don't do that because then they get
annoyed with you that it was a you know
a stupid answer so this is this is end
of HCI our spirit having the Machine try
to be a bit more
we're of the likely effectiveness of the
communication now the last point I want
to use here is that the you know people
say well but how are you going to get
the sorts of tagging I'm not I'm not a
sports buff but I had the opportunity to
work with a leading sports programming
network based in New York that I'm not
allowed to name on video and I did hear
about Roger Clemens and I thought let me
look him up well what you see on the
left are the top let's call them tags
values would have you associated with
Clemens over all but the past year what
you see on the right are the tags
associated with the more recent year now
for those of you don't know Roger
Clemens baseball player was with the
Yankees probably took steroids although
that's still under investigation and was
featured in the Mitchell Report and
fingered by his coach Brian McNamee well
that's nice this isn't necessarily
telling me something I didn't know but
the point I'm after here is more where
this data comes from turns out ok the
names of players and of teams everybody
has that data that's easy but it turns
out that the authors of sports related
articles are not necessarily tagging
things the way that computer science you
know graduate students hoping to get
tenure Sunday do so where we're going to
get tagged like Mitchell Report and
Brian McNamee from users this is
something that I think folks here are
probably quite familiar with user logs
are a pretty valuable source so we
crowd-sourced the vocabulary from the
user logs now you might say that's a
little bit dangerous what if people
start looking for porn all the time but
then we text mind against the data
itself so you could get tagged unless
you're actually about something but we
use the users to figure out what they
were looking for because what they were
looking for is likely to be what their
things are about or at the very least
what they want and again put this in the
stir
verse of ways to communicate what
document sets are about going a bit
beyond again just returning rank lists
and I'm sure you can find this online if
you go through the leading sports
programming network sites now what we're
after here is trying to trick the
classic trade off of information
retrieval which is that you have
precision which is how many of the
documents returned are relevant you have
recall which is all the documents you
could have returned to a user that are
relevant which ones you get now I know
this is a bit of a set retrieval
oriented way of thinking of it for those
who want ranked retrieval there's you
can think of mean average precision
somewhere in this framework but the
point is this you know it's like it's
like Soylent Green tells us people can
help people can cheat this because you
don't need to guess what's in people's
head you can actually ask them and
perhaps for those who are deep in the
bowels of IR who have been taught really
since the 80s that set retrieval isn't
the way to go you should be doing rank
retrieval i'm here at a company that
probably to almost everybody outside of
this audience is thought of as ranking
things or people just take it for
granted that ranking is the way to go i
hope you'll consider the possibility
that ranking isn't always the way to go
because it's not actually communicative
enough that if you think in terms of
retrieving sets there's something you
can do that ranking doesn't give you at
least you can do ranking as well but
with sets there's a notion of
summarizing of them and giving people a
more holistic a more efficient and
aggregated view of what all is out there
and then importantly where you can go
from there now this isn't necessarily
the only way to implement an HCI our
approach but frankly I found that it's
the most effective one because you're
basically taking that screen that
limited real estate just imagine on an
iphone instead of on the screen of this
size and trying to pack the most
communication bandwidth for your buck
and
that interaction with the user and you
get to just three principles read if I
can have the takeaways from this as to
what I would say you can do with this
right HCI are is a nice idea but how do
you make it real well you make it real
by emphasizing set summaries over ranked
lists you make it real by instead of
making the interface be dominated by a
minimal user input followed by lots of
crunching on the machine side to try as
hard as possible to get it exactly right
don't optimize for getting it right in
one query optimized for getting it right
period that might not be good for casual
users I'll concede that there are some
people who will try one query they don't
find it they'll leave but there are a
lot of use cases and I mean I thought
well you know my mom she teaches Latin
American literature she won't relate to
this at all but it turns out she does
when she's looking for things often
she's doing a dozen queries now perhaps
part of the great success google has
achieved is that she blames herself and
not google for not finding something
that's impressive and in fairness it
takes to I sometimes look at her queries
and I'm inclined to blame her too but
the thing is it it's the you know a
user's query is not the last query
there's a lot more you can do if you
take a more dialogue oriented approach
and then it's not just about finding
what the users said he or she wanted
right now exploration and discovery are
not nice to have for many problems their
must-haves because people don't know
exactly what they want and if they're
not satisfied meeting their information
needs you can't go to the Mac doors and
say hey well it's your own fault for not
having figured out how to express that
in a query I mean we're in the business
of helping people find what they want
it's important to actually support a
broader set of information needs than
the easy ones the ones that in many
cases actually have Wikipedia pages that
answer the questions I mean if Jimmy
Wales can do that on you know 6.3
million dollars and a b-25
underpaid employees we're not doing our
jobs and I think this this is a bit
redundant but that's good because I'm
running out of time support this
interaction it's true that lots of
people like eating at McDonald's but
isn't it a good thing that people are
some cases are more demanding than that
and I realize I'm provoking and only
allowed to do that because I'm control
the microphone but you know google has
really solved the easy use cases really
really well I'd be the first to admit
that but I think that there is a bit of
reluctance to look at some of the harder
ones I'm not sure why and perhaps when
I'm you know pummeled with questions
after this I'll get more of a sense from
you guys and one more thing the googles
mission is to organize the world's
information and to make it universally
accessible and useful but I went to hit
it go on this organizing thing the
Alexandria library was perhaps one of
the first efforts to organize the
world's information right to classify to
put it in different places so that
people could look for things a lot of
what we see now specifically in web
search focuses on an adversarial model
where the most important thing turns out
to be to take the competing people who
are trying to get ranked at the top of
list for queries and somehow adjudicate
that that's in fact why in the earliest
days Google was able to basically
trounce every game in town it wasn't
just because of having a streamlined
interface and it wasn't just because of
speed it was because of relevance and
what relevance meant in that age was
doing better than the spammers who were
exploiting algorithms that folks like
lycos and alta vista were using that
were many cases built on TF IDF and its
variants that could be easily gained by
spammers the notion of using link
for authority brilliant because it mean
first it's a good idea but secondly it
was really hard to game now that was
then and this is now and I know that
there are lots of folks in Amundson
Gauls group who probably spend every
waking hour discovering the ways that
people fake link farms I you know I
dabble and seeing when I can it a sort
of Google Google bond things myself but
let's even imagine that that is that
crew is doing it all right but that's
refereeing it is not organizing and I
think that everyone but especially folks
here should be embracing this vision
that is actually in the statement of
being more than just adjudicating
between the competitors for ranking with
that would like to thank you and open
for questions
that's a fair point and it's actually
it's funny right because I'm sure bunch
of folks are familiar with the the saga
of go to overture yahoo would eventually
the settlement before the IPO but I
think that it's it's people who look
they're forget you're right that Google
actually figured out how to take this
notion of bidding for ads and make it
something that people would actually use
and Knobloch the questions
so they just repeat the question because
I'm not especially for the online
audience is how can you recognize when a
query requires clarification and the
what we did in the in that data driven
one I think is instructive which is that
we look at the results the user would
see based on a ranking algorithm and
that ranking algorithm we knew had
actually even had human input because it
was reflective of someone familiar with
the data and we compared that to what
would have happened if we had
essentially ignored that relevance
ranking algorithm returned the whole set
the reason we actually did that
comparison wasn't just so see what we
could do with our hands behind your
backs but because we knew that we would
be driving all the sorts of refinement
behavior from that set which is a common
approach when there is a divergence
between that and that we can quantify we
used a variation of relative entropy or
call back live there divergence to make
that information gain I think it's the
final synonym there to say hey those
look different now you could say what do
we mean we those look different well if
we were to compress those sets in that
case we had the benefit of some metadata
but we could have generated that
metadata as well or even in an extreme
case he was kind of a bag of words
representation and we said look you
would not be able you say these would
not look like the same query therefore
we can see that there's going to be
ambiguity now there's a worst-case or
you can say well what if we didn't have
relevance ranking at all then that was
actually where we started on this we
decided to be more conservative there in
that case it's almost like the game of
Jeopardy you could say I'm going to show
you a set of results now you have to
come up with a query
now i'm not going to show you a whole
set of results because that's a lot
maybe i'll just show you a random subset
maybe i'll show you top 10 by some
sorting the point I'm after is that the
compressibility of asset is actually
reflective of how efficiently it's
communicating to you the work there is
actually inspired by some stuff done at
all so at the University of
Massachusetts on query clarity and they
were looking at it in a slightly
different way that was more queries
centric but we realized you could
actually look at sets the result set
themselves and you could think of it as
yeah could I succeed in the game of
jeopardy if you saw my results could you
reproduce what I'd gotten as a human
being so okay so the question is
basically it's a computational question
which is that if you have to do this
kind of disambiguation it at runtime
then how are you going to do this like
in the query for storage we first have
to get those those results then we have
to somehow look at them recognize it as
ambiguous and so forth so the first
answer is yes you have to perform an
analysis on those results and in
particular you know volunteer that it's
not the sort of analysis that would be
easy to do if your only access to the
data was by means of an inverted index
where you can look up storage you have
to actually be able to look at the
documents and gather information from
them in order to then decide that you
even have ambiguity in the first place
so it's a good idea to store the
information that you're going to need to
do that in a way that you could rapidly
go through it on the documents
the good news however is that you don't
necessarily have to look at every single
document to recognize that your result
set is bad I mean human beings aren't
going to do that either so you very
quickly develop a statistically
significant intuition if you will that
something is wrong so on one hand you do
need to have the data close by so that
you can see it but the amount of data
that you have to look at to recognize
the problem is not so bad once you've
recognized that this is going on
actually coming up with the alternatives
isn't so bad because then you're in a
position where turns out your inverted
access is more helpful now perhaps one
thing that is important here the I
didn't sign the thing that lets me find
out how all of googles indexes are
stored but I have these suspicions that
there is a favoring for as the only
ranking algorithm that is allowed on the
web I'm not allowed to rear ank my
results by clicking a different sort now
that is a bit of a problem because in
order to if when we do this kind of
disambiguation we don't necessarily want
to be top heavy on all of the top-ranked
results it's important to have some
ability to do random access there so
that there that's not so much Sara Lee a
computational overhead it's a data
structure choice you have to make
particular direction is think about sort
of other dimensions of search no this
was sort of all about searching text
documents and fundamentally the kinds of
things that I sort of would love to be
able to search our you know I have a
picture of somebody the hell is it have
a picture of a building Thank piece of
art or an object now the typical thing
is I find a piece of plastic right on
the floor in my house now either it's a
piece of junk that my kid left around
that i can safely throw away or it is in
fact a crucial piece of something and
then in six months of them i will
discover that I wish to hell I haven't
thrown away I'd love to be able to take
the thing in front of my camera and go
click and say what is this mm-hmm so
where is that in all so so the question
is where are things like image search by
example like I take a picture of
something and search for it where does
that fit into this HCR framework and its
orthogonal to a point which is to say
the problem of figuring out from that
picture for example let's let that
becomes our query let's assume that you
know you input it as a picture and then
your relevance algorithm is some notion
some kind of image similarity and I'm
not an image an image I our guy but you
know I know a company called modesta for
example and their various others I know
google has been doing some work on that
as well that can give you a set of
plausible alternatives now if the set of
plausible alter their a couple of things
if the ovens are not very good you can
run into some some problems if the
algorithms are great and your query is
unambiguous this becomes an easy query
and it's just that you know you just
have to make better algorithms the place
where this comes into play is when the
set of things that could be relevant to
your query is actually sizable so to
take an example that may be a little bit
of field look at the ESP
right or which now use for google image
labels that finding an image from a
description it is actually somewhat
challenging honestly when i use google
image search which in fact helped a lot
for finding a variety of the of the
pictures you see here which I want to
say are credited in the presentation
itself it actually helped a lot to use
the most recently released feature that
I could at least see what was clipart or
what was photo and so forth that was
very helpful that was a step in that
direction it would have been nicer to
see other kinds of organization because
even if I'm not great at describing a
picture being given a little bit of help
to elaborate the query would help now
you could say well but who needs that if
I just have the picture there and can
search for it well if it's it's like
chess if once the machine gets good
enough but just doing the matching if
that's if the simple is that all done I
point at this the startup modesta and
yes the free product placement for them
because they're looking at how to find
shoes and it turns out that just because
you know that you like one shoe doesn't
mean that that's exactly this you're
going to buy another things is probably
too expensive and so you want to explore
the related space I think that's closer
to a use case that you're describing but
they don't the input is not necessarily
that you you know scan in a picture
they're presuming that you're looking
around and probably clicking on one but
arguably that's just an implementation
detail
mm-hmm so the question is is let's look
at the other side of the disambiguation
problem which is not so much overly
vague queries as overly specific ones
well you could actually look at that d
pullet example as a case of it where
it's a bit after the fact when I
discovered that I was more generally
interested in what people are doing in
refinement and information retrieval
once I was there so another way of
thinking about that is how they're there
are several things one is if I find a
document what context should I look at
it in and I think a big the biggest part
of that problem is how should I annotate
that document in a way that plays well
with the rest of the corpus most of the
work I've seen on annotation is very
document-centric rather than seeing give
me a vocabulary that will be helpful to
the corpus as a whole that was actually
something that we paid a lot of
attention to in the work we did with the
ACM because oxygen their authors had
that problem that's one version of it
the other version can be your query
itself is overly specific and as a
result you're getting too bad documents
well a hopeful case is we can recognize
that the result set is bad because it's
for its size anyway there it's really
small you know maybe we've decide some
all sets or bad but some are small set
unless they match the query exactly you
can pick your heuristics for this but
more with the set if the sets are
actually a little bit larger their lack
of coherence is usually a good signal
that something is wrong well the thing
is we didn't just you know when we got
back the set of results presumably we
did what a conjunctive serves the
intersection of every query term but
maybe expanding for stemming the sore I
there is nothing that said we had to do
it that way we could have dropped a few
terms we could have searched different
indices there are lots of things that we
could have done to in
interpret the query so then you can step
back and say well maybe we should have
and you can see if a different query
interpretation would have gotten you to
a better result set now I believe one of
the earlier questions was about what the
computational intensive pneus of this
and it becomes a search problem in the
AI sense of search that is if you have
lots of ways you can process your query
which way should you do it you can
certainly get into trouble there
computationally but you can work as hard
as you feel you need to hopefully most
people aren't asking hard questions or
dumb questions like my immediate family
so that you can process their pores
easily and you know divert the rest of
your service to the harder ones there
are ways to do that so I mean those are
your two as far as i can tell two
approaches one is see if you can process
the query differently and the other is
the query is fine see if you can provide
more context to the results that come
back so the book on the beach certainly
right I mean part of it is going to be
so an example where we worked with one
of our clients we started from a further
simple you know collaborative filtering
style interface where it would show you
all the books that people with your book
purchasing history liked but then we
said well but we're going to offer a
faceted search on top of that so what's
nice here is that let's say you liked
Michael Lewis's Moneyball now some
people might like it it's a Michael
Lewis the liar's poker guy fun author i
think it just came out with a new book
as well Moneyball is about the stunt the
the statistical science of baseball now
what's fun about that book from my
perspective is that you're either
interested in it because you're
interested statistics or your student
baseball maybe maybe receiving a rare
set of people who care about both but
the point is I don't want to be
sent a whole bunch of baseball books in
my own case just because I like money
ball and I'm sure the baseball
officinalis feel the opposite way so
offering the ability to explore that
space in a faceted search interface on
top of collaborative filtering is one
way the other way is what i like to call
social navigation which is again instead
of looking at recommendations to the
sort of black box of saying well this is
what people who like my stuff liked
actually think of navigating the space
of demographics if you will I want to
know I you know if I'm buying a book i'm
buying it for say my mother-in-law and
what do I know about her she is in her
mid 60s I want to get in trouble for
this and is female lives in California
etc what do those people like so I can
actually think of the lens itself as a
means of navigating and we have
something like that actually if you look
at the Fandango for looking at movie
reviews exploring that space by by the
view it's it's only a small step in that
direction but it's in the city in the
spirit of that the the partial ordering
one I'll give you guys a hint it's not
going to be very efficient
thank you Mark for hosting thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>