<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep Learning of Representations | Coder Coacher - Coaching Coders</title><meta content="Deep Learning of Representations - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Deep Learning of Representations</b></h2><h5 class="post__date">2012-12-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4xsVFLnHC_0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so talk about earning I'll talk about
the presentations and learning
presentations and they were deep here
so my goal is to constitute to building
intelligent machines how do we get
machine to be smart to take good
decisions
well it means knowledge they are
searchers from the 30 days fifties
sixties seventies trying to give the
knowledge to the Machine the knowledge
we have explicitly and it didn't work
quite as well as well suppose one reason
is that one of our knowledge is not
something that can communicate verbally
that we can write down a photograph so
that knowledge has to be taken in
somewhere else and basically when we
have found this you can get that
knowledge through observing the world
around us that means learning okay so we
need learning or yeah what is learning
what is machine learning it's not about
learning things by heart that justifies
what it is about it is generalizing from
the examples you've seen to new examples
and what I like to tell my students is
it's taking probability mass that is on
the training examples and somehow
guessing where it should go
which new configurations of the things
we see make sense are possible this is
what
it's guesswork first we can measure a
welcome guest and I'll mention something
about dimensionality and geometry that
comes up when you think about this
problem and one of the messages will be
that we can maybe find this mentality
problem by allowing the machine to
discover underlying causes acrylamide
factors that explain the data and this
is a little bit what that is about
so let's start from learning and in
these important learning let's say we
observe XY pairs where X is the number
one is the number N and as far as here
representing the examples we've seen
that XY configurations so we want to
generalize we do have the durations in
other words for example like this
problem didn't even want to predict the
Y given a given X and there is an
underlying relationship between might
actually be expected value of y given x
which is given with this purple curve
but we don't know it that's the problem
with machinery for trying to discover
something we don't know already and we
can guess some function this is the
predicted or learned function so how can
we go about this one of the most basic
principles by which machine learning
algorithms are able to do this is assume
something very simple about the world
around us about the the data yet or the
function we're trying to discover it's
just assuming that the function we're
trying to discover is smooth meaning if
I know the value of the function as
and I want to know the value at some
nearby point fixed right then it's
reasonable to assume that the value and
expire of the function 1 to learn is
close to the value of x that's it I mean
you can formalize that mathematically
many different ways and exploited in
many ways and what Annie's here is if I
ask you what Y should be at this point
what I can do is look up the value of y
that I observed at nearby points and
combining these make that a reasonable
guess like like this and if I try to do
that problems like this is actually
going to work quite well and a lot of
I'd say a large fraction that's the
applications of missionary use this
principle and even even off huffman just
this principle if you only rely on this
principle personalization we're going to
be in trouble that's part of them one of
the messages I want to explain so why am
I going to be in trouble well basically
we're doing some kind of interpolation
so if I see enough examples the the
green stars here who cover the ups and
down of the function I'm trying to
fireman I'm going to be fine but what if
the function of whether Lerman has many
more ups and downs than I can possibly
observe data there's even Google has
finite number of examples even if you
have millions or billions examples the
functions we want to learn for AI are
not like this one they have the number
of configurations of articles of
interest that may be exponential
exponential e la razón something any
bigger than the number
so there's no way we have enough
examples to cover all the configuration
for example think of the number of
different English sentences which is
something that Google is interested
readers and this problem is illustrated
by a so called curse of the nationality
where you consider what happens when you
have not just one variable but many
variables and all of their
configurations how many configurations
of n variables do you have well you have
an exponential number of configurations
so if I want to learn to go in a single
variable and just you know define it
seems the real available variable and I
divide its value into intervals and then
count how many of those bins I've seen
in my data I can estimate the
probability of different intervals
coming up so that's easy because I only
want to know about a small number of
different configurations but if I'm
looking at the nerves then the number of
configurations may be the square bigger
and have three maybe even more but
typically I'm going to have hundreds you
think about images this is it's
thousands tens of thousands hundreds of
thousands so it's it's crazy on the
configuration there are alright so how
do we possibly generalize to new
configurations we cannot just break up
the space into small cells and count how
many things happen to each to up because
the new examples that we want to care
the human iterations that may be asked
about might be in some region where they
haven't seen no exam so that's the
problem of generalizing nominally
so there's one thing that can help us
but it's
insufficient it's something that happens
with the eye problems very often light
machine vision that's the language
processing that are spending many other
problems where the set of configurations
of verticals that are plausible that can
happen in the real world occupy a very
small volume of all this set of possible
configurations so let me give an example
in images if I choose the pixels in an
image randomly there was if I sample
only an image from completing a uniform
distribution I'm gonna get things like
this just like doors and I repeat this
for eons and eons and I'm never going to
assemble something that looks like a
face so that means that faces image of
their faces are very rare the space of
images they occupy a very small volume
much less than that this picture that
suggests and so this is a very important
hint it means that actually the task is
to find out find out where where this
this distribution concentrates I have
another example here for dignity magenta
before at this one and you do some
geometric transformations to it like
we're dating and scaling it you get
slightly different images and if you at
each point you allow yourself to make
many of these transformations you can
create a solar panel also the surface of
possible images each
image in the number of different changes
you makes is basically the
dimensionality of this so in this case
even though the data lives in that
attentional space the actual issues we
care about are pretty much gone and
knowing that we can maybe do better in
terms of land one thing about the first
dimensionality is I don't like the name
in astronomy because it's not really
dimensionality you can have many
dimensions that have a very simple
function what really matters is how many
variations
does the function have how many ups and
downs we actually have some theoretical
results ago the number of examples you
would need if you were only relying on
this smoothness assumption essentially
is linear in the number of ups and downs
of the culture anywhere okay so that's
compact through this idea of learning
where to do math so in machinery what we
have is data each data each example is a
configuration of variables and we know
that this configuration procured in the
real world so we can say the probability
for this configuration so this is this
is the space of configuration and
showing
Toodee so we know that this
configuration is plausible is it's like
somebody just put a big of a movie here
and we can put a link at every example
the question is how do we think this
probably masked and sort of give a
little bit of that to other places
particularly reflectively mass in
between if there really wasn't manifold
that has some structure you could
discover that structure it would be
great so sort of the classical machine
learning way of doing things is say that
the distribution function that function
that that we're trying to learn in for
this case is smooth so if if it's very
probable year it must be also probable
in the neighborhood so we can just do
some mathematical operation at low shift
summarized from here to there
neighborhoods and we get distribution
like this is our model and that works
reasonably well but it's not the right
thing to do in splitting mass in many
directions we don't care about instead
we're going to do is to discover that
there is something about this data there
is some structure there's some
abstraction that allows us to be very
specific about where we get improve
probability mass and and we might
discover something like this which in 2d
doesn't look like it may be difficult
but in high dimension the number of
directions are allowed to move here is
very small compared to the network
and the volume goes exponentially with
dimension so you can have a huge gain by
guessing properly which directions
things smooth are allowed to keep pipe
ability
alright so now to a more of the
presentation which is about
representation marinade I have talked
about learning in general and some of
the issues some of the challenges with
applying learnings we have now when you
look at how machine learning is applied
in industry when people do for nine
percent time with what they do with the
effort of engineers is not really
include machine learning that used
existing machine learning but to make
the machine learning algorithms where
well they do so both feature engineer so
that means taking the raw data and
transforming it extracting some features
deciding what matters for in a way the
things we think don't matter and that's
essentially using humans and our
intelligence and our understanding of
the problem to figure out the factors
that matter figure out the dependencies
that matter and so on so what
representation learning is about is
trying to do with machines what humans
do right now which is extracting these
features it's covering what a what is a
good representation for data and what we
could think about it is the machine is
trying to guess adjust those features so
those computations that are useful for
us explain variations but really what
are the underlying factors that explain
that it what are getting causes and the
guesses about what these are for a
particular example is
you'd like to have as our reservation of
course this is this is hard to define
because we don't know what the right
factors are whether the right cost of it
is this is the objective we have this is
rightly so there is a very family of
algorithms that she will mention from
your old nets that have you belong since
since at least those distant days and
they have multiple layers of
computations and one of the things I've
been trying to do is to find properties
that they have that other algorithms may
have that may be useful in try to
understand why these properties are
useful in particular there's this limit
death so the idea of deep learning is
that not only going to have the
visitations of the data that they
learned that we're going to have
multiple levels of excitation and why
would it matter to have multiple levels
of representation because we could have
have low level and high level
observations where our high limited
editions are going to be more abstract
more non-linear capturing capture
structure is less obvious in the data
so what equal learning is when the
learning on one can discover these
observations and even decide hugging
them especially around so so I mentioned
Nunez
learning what these algorithms do is
they learn some computation some
function that takes an input vector and
map it to some output which would be a
vector through different levels of
representation where each level is
imposed that units which do a
computation that's inspired by heart to
the brain so they have a property which
we don't find in any learning authors
called distributed reservation so let's
first see how these other learning
others work how they jump laughs
remember this is heavier than what I
talked about this Kunis's they didn't
rely on the scoring system even earning
also revised this because that
introduces additional prior additional
knowledge so when you only rely on this
motion assumption that we learn is to
essentially take your English space the
same space and break it up into we
engineer for example this is it happens
with clustering your neighbors and any
possible dumb parametric algorithms
decision trees so what what happens is
after seeing the data you break up the
people space into regions and you
generalize locally so what if you if you
have a function that helps something
here it is you've seen an example
we can you can generalize and say well
in the neighborhood the output is going
to be similar and maybe some kind of
manipulation with the neighboring
regions but the crucial point from that
from a mathematical point of view is
that this accounting argument here which
is how many planet Earth 20 degrees of
freedom we have to define this partition
well this eking you need at least one
parameter per region the number of
parameters is going to grow in the
network regions if I'm going to
distinguish two regions I need to say
where the person is or how to separate
between these two and example I could
imagine specifying the center of each
region so the number of things I have to
specify from the data is is essentially
equal to the number of regions out of
the sandwich okay so you can think well
there is no other way you do with that
right I mean how could you possibly
create a new region for which you have
not seen any data and distinguish it
means well you can't
this is what happens with this truth
participation which have a look inside
factor models pcard M illness persuading
regretting what you're gonna do is
you're gonna still great the beat with
space into regions and able to
generalize locally in a sense that
things that are nearby are going to have
similar outcomes but the way in the
north as computer so for example if you
could imagine a name is an input space
and then into it is in that I'm going to
break it down in different ways that are
not mutually exclusive so here what I'm
thinking about when I'm building this
publish there are clean factors that
explain that to the inputs X n so this
is between dementia input space and I'm
breaking this space into different
regions so for example the black line
here tells me that you're even on that
side of it of the other side of that
size t1 equals one that's ID c10 so this
is a bit that doesn't mean whether I in
this secular this side and I have this
other bit that tells me where there are
on this step in this head or that scent
and now you can see that the number of
regions I can define in this way can be
much larger than the number of
parameters because the number of
rameters wasn't even the number of
factors the number of these amenities so
by being smart about claiming to define
those regions by allowing to compete at
targets would help us you can get to
finish the exponential game expressive
power of course from the machinery point
of view this comes with an assumption
the assumption is that when I learned
about being on that side and that's why
this is meaningful independently in some
sense quite disabled sense of what
happens with the other configurations
the other so that makes sense if you're
thinking okay this is images and this
one is going me is this a man or female
does he wear glasses or not so so if you
think about these factors as cementing
the meaningful things usually you can
vary them an event like the causes that
explain the global practice and that's
why you're able to generalized your
assuming something about the world that
gives you a kind of exponential power of
dissipation
now of course given in the real world
the features we care about the fact that
there are not going to be simple linear
separators so that's what we want deeper
that's one reason why many people
dissipations
otherwise just a single level would be
enough all right let me move on it's
displaying so sis going from my
brother's sunny we gave a talk here
where they use this idea of
representations in a very interesting
way where you have theta two different
realities you have images books you have
images and you have text queries what
sequence of words and the alert
representation for images to be mapped
the image to some hundred dimensional
vector and they learn a function that
represents queries and a map query
through this also enter dimensional
point in the same space and they learned
them in such a way that when someone
types dolphin and that is shown an image
of dolphin and it clicks on it the
representation for the image and there
was a vision from the very end up close
to each other and this weekend when she
learned that you can even of course the
pool has to be first
like answering queries and find images
that match fairies that somehow you
haven't seen before
all right one question that people call
sign machine learning ask being
considered missionary are doing is this
is crazy and humans can learn from 33
examples and you guys need thousands or
millions of examples you're doing
something wrong and they're right so how
many humans managed to toss me learned
something very complicated from doesn't
presents make other students learn
something
well their number of essence one is
brains don't start from scratch
and asking price and particular I'm
interested in generic priors that allow
us to generalize the things that we was
not even trained are the species to do
still we do very well so we have some
very general purpose fighters of their
environment and I'd love to figure out
which they are because we can exploit
them as well also and this is very very
important if you ask it you watch or
something we're girly but the first
adult has learned a lot of things before
you give it given a few examples and so
he's transferring knowledge from
previous time this is crucial and the
way of doing that is building his volume
representations of the objects of the
types of the without Easton in which
you're giving the examples and and these
representations capture the
relationships between factor
three factors that explain what is going
on in your particular setup of the new
task and he's able to do that from
unlabeled data from from examples that
were unrelated to the task of trying to
solve so with which human is one of the
things that humans are able to do this
to who is both supervised learning
they're able to used examples that are
not specifically for the past few care
but to generalize there are people who
use information about this structure of
sentence equal structure the things
around this to quickly answer new
questions so here is say so I'm going to
give give me just two examples we want
to discriminate between degree in the
blue and a classical algorithm who just
would do something like put it straight
line it in between but what if you knew
that there are all these other points if
there were nothing even related to your
task but but these are the
configurations that are plausible in the
input distribution so those backgrounds
you don't know what they are green or
blue but by the structure here we guess
that these ones are all blue at least
one in between so you would put your
decision like this so we're trying to
take advantage of data from other tasks
that are unable to find something Derek
about the wolf add variations usually
happen in this direction and use that
quickly generalize operation example
Susan okay so of the motivations for
learning about death there are vertical
situations that come from the discovery
of families of functions mathematical
functions that can be represented very
efficiently if you love
they have multiple levels but might
require exponentially more numbers
bigger representations if you only about
one or two levels even though wonderful
levels are enough to represent any
function it might be very efficient and
of course there are biological
motivations like the brain seems to have
the picture is especially powerful but
there's no cortex which of the five
names confessed and that the cortex
seems to have a generic putting on
events which same principle secretly
inward in terms of learning everywhere
projects finally their cognitive
motivations like we learn simpler things
first and then these simple things to
build tiny level abstractions this has
been this has to be exploited for
example Stanford like like me and others
to show how the patient's can learn so
many things like it just combined them
parts faces another sort of simple
motivation is tell me program computers
do we program computers die I need a
main program that has a bunch of lines
of code or we program computers by
adding functions or 17 that Col 17
facilities this is the we program if
we're forcing program that way wouldn't
work very well but mostly machine
learning is this be trying to solve the
I using this and a lot of the programs
they use that structure of the function
that I want
as there are also personal deviations
from looking at what we achieved by
explaining death so I'm stealing this
slide from another Google product that
by Geoff Hinton last summer which shows
how deep Nets
compared to this standard which has been
the state of the art in speech
recognition for three years and be
substantially improved by exploiting
these public levels of participation
even and this is a something new that
impressed me a lot even when the amount
of data available is huge this one comes
from a something that happened in 2006
when first Geoff Kimpton
followed by Montreal found that the
National Train very deep neural network
while using a few simple tricks and the
simple tricks essentially that we're
gonna train later by later dismissed
revised learning although recent work
now allows us to Train deep networks
without this trick and using other other
tricks this has been wise to lots of
industrial interests as I mentioned
condition
I'm going to talk about some
competitions we've won using deep
learning so last year we transfer
learning competition we're trying to
take what was envisioned learn from some
data and apply that on other data
relates through similar but different
tasks and so there was one competition
where the results were announced that a
CFL because the 11 level in that doesn't
say this is that less than a year ago
and what we see in those pictures is how
the accuracy improves with layers but
more precisely each of these graphs has
the x-axis the log of the number of
labeled examples used for training the
machine and you see that as you and the
y axis isn't accuracy essentially so you
want this to be high and for this task
as you have more levels of
representation what happens is you
especially get better in the case where
you have dirty legal discipline so he
was talking about that humans can do so
well
July's conversion examples because
they've learned presentation builder on
one of the learning algorithms that came
out of my lab has been used for this is
for the using 1/4 and what it does the
principle is pretty simple and you will
learn elucidation you take an inch input
example and you corrupted by say setting
some of the hoods to zero randomly and
then you learn another submission so
that you can reconstruct the input but
you want to reconstruct the unprompted
with the cleaning that's why it's gonna
be noisy and then you would be trying to
make this as close as possible
this is possible to walk and profit
English and we can show this essentially
models the density of you for this
illusion and you can you can marry these
these are as a nation and stack them on
top of each other
Oh
6:19 i have until
okay so I want me to hear a connection
between those the New Zealand reporters
and and the manifold learning idea that
was mentioning earlier so how do these
algorithms discover the manacles the
regions where the configuration of the
variables are plausible where the
distribution concentrate so we're not on
the same picture as before so these are
our examples and what we're trying to do
is to learn or was invasion so mapping
from the input space will appear led to
a new space such that we can essentially
recover the input and we've been losing
information but at the same time because
of the denoising part actually you can
show that with this is trying to do is
throw away all the information so it's
it's it seems seems crazy but we wanna
key will be information with you wanna
throw a lot information but there's
attachment here you want to undo people
to ribbons from these examples not
necessarily any configurationally so
you're trying to find a function which
will preserve the information about
these guys well for these guys always is
able to reconstruct them but yeah in the
d function report when it's applied on
these drums we can apply it in other
places it's allowed to do anything it
wants and so in order to and explosively
running smoothly so in order to do that
let's see what happens let's consider a
particular point dear particular example
it needs to distinguish this one from
its neighbor in the representation you
can learn solely the workers need to
learn so that guy has to be different
enough from that guy that we can
actually recover and distinguish this
one
so we can learn an inverse mapping
across many where's happening from the
representation okay so that means you
have to have a representation which is
sensitive to changes in that direction
so when I move slightly from here to
here the representation has to change
slightly as well on the other hand if I
move in this direction then the
representation doesn't need to capture
them
it could be constant as I move in that
direction in fact he wants to be
constant in all directions but what's
going to happen is that they be constant
in all directions except directions that
it actually means to reconstruct the
data and in this way to recover the
directions that are iterative this
presentation function and you recover
the directions of the manifold
attractions where if I move in this
direction I still I stay in the region
that's what the metaphor
okay so we can get rid of this direction
and recently we end up with an algorithm
that you can use to several frontal
models so be it if you have an
understanding of the manifold is
something that tells you at each point
these are the directions you're allowed
to move so as to stay in the region so
these are the directions that keeps you
engine to the manifold thing basically
Agadir goes well we are at the point we
move in the directions that are other
than discovered to be good direction of
change possible directions have changed
and that might correspond to something
like taking an image and translating it
or omitting it or doing something like
removing a part of the image and and
then projecting back towards that
manifold it turns out that
reconstruction function does that and
then it's raining that doesn't walk to
get a simple model and apply this lovely
faces and digits now let's come back to
this question of what is it go to
recitation it's weird it's used a lot
when you handcraft features so I
remember at the beginning I said the way
most of our supplies they karate and
crafts teachers based on their knowledge
of what matters in life doesn't matter
for example if your input is images we
would like to design features they're
going to be insensitive to translations
of your in because typically the county
of every year trying to detect
should not be better the small
translation so this is the idea of
comparative features but if we want to
do other supervised learning where no
one tells us edifying what matters
remove doesn't matter what the task is
going to be then what do we know which
which condemning this matters for
example it's a regular speech
recognition
well if you're doing speech recognition
then you want to be invariant to who the
speaker is and you want to be invariant
to what kind of
and what's the volume of the sound but
if you're doing speaker identification
then you want to be invariant to what
the person says and you want to be very
sensitive to the identity of the person
but if someone gives you speech and you
don't know if it's going to be used for
recognition but phonemes order for
recognizing people what should you do
what we should be doing is learning to
disentangle factors basically
discovering that in speech the things
that matter are 42 person microphone and
so on these these are the factors that
you'd like to discover automatically and
if you're able to do that then my claim
is you can essentially get around the
curse of dimensionality you can solve
very hard problems all right there is
something funny that happens with the be
very obvious I proposed earlier which is
that if you train these presentations
for universe provides learning you
discover that the features resonation
that you find have some form of prison
time I think that some of the units in
the neural net are very sensitive to
some of the underlying factors and there
are persons new manufacturer very
insensitive to other factors so this is
what the same thing
develop a no one told these algorithms
what those factors would be the first
place so something good is happening
that we don't really understand why and
the letters deadlines one of the one of
the things that you've seen any of these
algorithms is so called sparse
representations so what is that well up
to now funding of our stations just a
bunch of members associate to remember
but one thing we could do is learn to
visit patients that have the property
that many of those members happen to be
zero for some customers it turns out
when you do that that it helps a lot at
least for some problems present a
picture that it helps us disentangle the
applying factors in the problems where
basically for any example there are only
a few concepts and in factors that
matter so in in the scene that I see
right now that comes to my my eyes of
all the concept that my brain knows
about only a few are relative to the
same and it's true of almost any input
that comes to mind like my sister my
sensors so it makes sense that
elucidation of how this party is without
them even though we have large number of
possible features most of them ours you
know not applicable to the current
situation not applicable in the state 0
so just by forcing many of these
features is output not applicable
somehow they're getting better it was
efficient
this has been used a number of papers
and he used it using a so-called rectify
our networks in which the unit or to
function like this on top of today's
world linear transformation that before
the result is that this function is
Maxim's your head so when X here is some
weighted sum from the previous layer
what happens is even at the output is a
positive real number or the output is
zero so it's in Incan was a random
centered around zero that happened to
time those features without the zero and
you can just learn to shift this little
bit to the left then you know so it's
very easy to get sparsity with these
kind of monetary in terms of the these
million areas are sufficient to
complicated things and that was used in
particular in a really outstanding
system built by alex process key here in
english discover geoff hinton toronto
recently where they obtained amazing
results on one of the benchmarks that
good revision people really care about
for image net with a thousand facets so
this contains millions of images taken
from google image search and a thousand
classes if you're trying to classify
so these are images like this and there
are a thousand different categories you
want to detect and this shows some of
the outputs of this this model there's
and they managed to bringing the
state-of-the-art from making small
incremental changes
17% that's pretty amazing and one of the
tricks that used this before this whole
dropouts and this year something else
and it's a very nice trick
this Indian is added some kind of
randomness use you think that randomness
hurts right so if we learn a function I
say think about the brain beginning
something if you add noise in the vision
of the brain you think it hurts but
actually when you do it drink training
it helps and they kept for reasons that
are yet to be completely understood but
the theory is it prevents the features
they learn to depend too much on the
other the presence of the other so half
of the features will be turned off by
this trick so again ideas you take the
output of you know and you multiply it
by 1 or 0 we probably need 1/2 so you
turn off half of the features layers and
whatever at this time you you don't do
this kind of thing just multiply by now
so we have interesting same thing but
how does is drink training that you
should learn to be more robust and or
independent of each other and
collaborate less pride right away this
is actually similar to the
earlier he introduced corruption boys
appear to do it at every layer somehow
this very simple trick helps a lot in
many contexts so they tested different
benchmarks these are a free image they
SS speech and okay submissive
improvements let's get back to the
participation learning algorithms they
are based on many of them are based on
learning one layer of one of the
algorithms that has the third property
between eyes restricted Boltzmann
machine of RPM and as a probabilistic
model it's polite
this way basically we're trying to model
this region of the vector and introduce
a vector of this age and we consider the
joint distribution of these vectors that
pulls some formative we're trying to
find the parameters in the format that
you see in the W so that so that P of X
is as long as possible
turns out that in this model it's very
popular for deep learning
you need to simple from the model in
other words the model is representing a
distribution and you'd like to generate
examples according to what the model
things is possible in principle there
are ways to do that even through things
like deep settling whether what we and
others have found is that these sending
others
that have some work you don't do exactly
what we like particular they we say they
don't mix well so what does that mean
for example if you start chain of
symbols so you're going to check you're
going to create sequence of samples by
kneading small changes that's what they
call the Markov chain methods do well in
terms of it you get change like this
where it in space around the same kind
of with examples he doesn't move to it
like a new category for you'd like your
simpler time phenomenal to be able to
visit all the plausible configurations
of giving a jump from one consideration
of one region and concentration space to
another one where he's had a chance to
visit all the places that matter but
there's there's a reason why this is
happening
I'm gonna try to explain into the
picture so first of all as I knew as I
mentioned MCMC methods moving
configuration space by making small
steps you start from a variation of the
examples they say saying wherever
sitting is configuration of XY
coordinates and I'm going to make small
steps such that if I am in a
configuration of high probability I'm
going to move to another high podium to
direction and if I'm in a long row
configuration I'm gonna tend to move to
a memory hyperbole deterioration and
this way you stay there
region regions and you in principle can
visit the whole distribution but you can
see there's a problem this if if this
one thing here is ine crumble then the
black thing there is any probable the
gray stuff in the middle is very
impossible how can I possibly make small
moves to go from here to here so this is
illustrated in the picture like this in
in this case this is a density okay so
the input is representing different
configurations of the variables of
interest and this is what the model
thinks that the distribution should be
so it gives like we'll need some places
so these are those and the MCMC is is
making a small steps now so long it is
unity can go through this they have
enough Lizzy it can move around here and
then quickly go through these and do a
lot of steps here and go back this way
simple all the configurations with
talking about the ability the problem is
remember I said in the beginning that is
this geometry where the eye problems
have this property the things we care
about the images occupy a very small
volume in the space of configurations of
pixels so the right distribution that
we're trying to learn is one that has
very big piece where there's a lot of
probability and most of the places
probably over a tiny tiny tiny Spanish
nice walk so we're trying to make moves
between these lows and now these modes
are separated by vast empty spaces
deserts of probability where it's
impossible to cross unless you're in
huge jumps okay so that's that's a
really big problem because when we
consider algorithms like the RPM the we
was going on is for learning we need to
sample from the model initially when a
model starts learning it says I don't
know anything I'm assigning a kind of
uniform ability for everything so the
model thinks everything is uniform the
quality is the same thing so we seem to
move everywhere and as it keeps learning
it starts evolving these Peaks these
levels but still there's a way to go
from multiple go through reasonably
probable concentration as learning
becomes more advanced you see these
Peaks emerge and now it's because
impossible to cross unfortunately we
need the sampling to learn these objects
and so there's a chicken-and-egg problem
we need the assembly but if something
doesn't work well the learning doesn't
work well so we can't make progress so
this one thing I wants to talk about is
a direction of solutions and it involves
explaining guess what D presentations
the idea is instead of instead of doing
these steps in the original space of the
inputs where we observe things if we did
the MCMC in abstract ID illumination
maybe things would be easier so let's
consider for example something
images of digits it's me had a machine
that had discovered that the factors
that matter for these images is
something like okay what's the
background is the background black in
the for program white or vice versa this
is one bit this is clip black and white
and using category 0 1 2 3 so that's
just 10 makes the possible category is
and with the position of the digit in it
but so these are high level factors you
can imagine that learner to discover if
it would discover these things and if
you resent the image in that space the
intention would be much easier
particularly you could go from say one
of these guys through these guys
directly simply because these are the
zeros and these are the threes and
there's one bit that allows you to flip
or two bits that want you to flip from 0
to 3 so in the space where my
representation has a bit fuzzy will be
through three it just need to flip two
bits that's easy it's a small move in
that space in in in the space of
abstract representations it's easy to
generate data whereas the original space
is difficult one way to see this
visually is to interpolate between
examples at different levels of
representation
this is what we've done so if you if you
look in the pixel space interpolate
between this - picture in line this
picture of three linear interpolation
what you see is between the image of an
eye and the image of a three you have to
go in between through images that don't
look like anything and they don't look
like a digital the things that we want
to see
so the MCM see if it lost all that's
possible thing that's all know this this
is not across well this is worse I'm
coming back so it's never going to go
the other side of this so this is a
one-dimensional sensation explain now
what you see with the other two lines is
the same thing but different levels of
representation as being very using this
device try and what you see is that it
has the learned representation which has
kind of skewed the space so that somehow
I can make lots of small change small
changes and stay in a three and then
suddenly just you pixels flip and it
becomes a nine that's me and so I don't
have to stay very long in a moment the
matrix assignments or all of these moves
are rather possible so you can smoothly
move from low to mode
so it's not like it actually discovered
these actual bits but if it's discovered
something that makes the job of
something easier and we've done
experiments to validate that so the
general idea is instead of settling in
the original space we can learn new
sensations and then do our assembly in
which you can each
the presentations in that tight little
space then once we found something in
that has tracking station we can because
we have inverse mappings because that
back to the input space and guess say
the digital Tara Tyler Facebook ergo so
we even find this for example with face
images and what we find is that for
example the red sphere uses a D
participation the blue here uses a
single layer RVM and we find is that it
can for the same number of steps it can
visit more mobile phone classes by
having a pair of a salvation alright so
I'm almost done what I would like you to
keep in mind is that machine learning
involves really interesting fundamental
challenges that have a JavaScript nature
and Ziggler and in order to face those
challenges something we found very
useful is to a lot of machines to look
for extraction move for high-level
limitations of the data and I think that
we've only scratched the surface of this
idea that the algorithms we have now
discover still a rather low level
abstractions there's a lot more that
could be done we're able to discover
even higher level abstractions ideally
but valuable abstractions do is
disentangle separate out the different
underlying factors and explain the data
the fact that we don't know what the
thing mission to discover see we know
the factors we can somehow change and
give not to the machine by telling it
these are here are some random variables
of yoga and here
valuing these variables in sections are
saying but we're not going to be able to
do that for everything we need machine
that can make sense of the world by
themselves something to some extent so
we we've shown that more extraction
stations give rise to successful
transfers so being able to generalize
domains
before any questions that Lincoln's
thank members of my team so the path of
course important for my work will be the
questions yes there's a microphone
right so the problem you mentioned that
they keep sampling isn't that you can be
slowed by this family constructs which
right that I mean I'm at least are going
to be maybe you to know so the problem
in the happens innocent if you recommend
restart was to be given happiness your
blog is going to bring you to one of the
computer demote was always the same the
same ones right and so you're not going
to visit everything no I wouldn't bring
you all this to the same model because
you have to think of it's like a
dynamical system right so most somehow
most routes go to wrong for some ways
well in your case it matters the most
moose go to a few big cities that's what
happens we obviously try this business
there was a question right here
so I was just wondering about sampling
from the model I thought that was
interesting we have different levels of
abstraction David yes you could sort of
breaks that space so because you're able
to visit more classes but does that make
this sort of distinction between those
classes more like it seems to be
bringing those classes closer together
so in terms of my event minutes now
that's lesson we have trouble
understanding what was going on there
because you think if your genital
pictures Lucas
thinking of somehow we made the reason
the nice closer at least it's you know
the harder from discriminating whereas
before we have this big empty region or
even clearer separator so I don't have a
complete answer to this it's because
we're reviewing these high dimensional
spaces that things are not this is our
intuition to suggest what happens really
is that the the manifold so that the
region is where say the threes are is a
really complicated curvy surface
publishing space and so is the line and
in the original space these dirty spaces
or you know intertwining comforted ways
which means machinery has a hard time
separating three knives because even
though there's lots of space meter times
and trees it's not like you have nice
young trees here it's a complicated
thing and what we do what we move to be
fine
Matan those those services and then we
can interpolate between points you kind
of stay high configuration so this
flattening so the funding also means
that it's easier to separate even though
they may be closer it's easier to sit a
simpler surfaces
this is a rejection of making I haven't
actually seen in this item it's
basically this is what makes me
but let me guess right so for example I
showed early on results modeling faces
from Sanford where another people that
have the same values that picture that
these inner is the learning algorithms
discover patients that at least for the
first few levels seem to be similar to
what we see in the visual cortex so if
you can be one one major first area of
visual cortex were single wide you
actually see you on the detector
sensitivity
of things and say clear v2 is a layer of
area well you'll see these things but
you see combinations of ages and are the
same group has actually compared the
brain seemed to buy a pointer scientists
with very models discovering they found
some similarities the other things you
can do that I mentioned quickly was some
cases we know what the factors are more
with what humans we look for so you can
just try to correlate the features that
have you learned with the factors we
know humans still are important so we've
done that we're here attending the world
the right guys yes this one okay so we
for example we've trained models on from
the sentiment analysis were given the
sentence you trying to predict the
person likes what we found is that we
use purely as provides learning so it
doesn't know that their job is super
sentiment analysis some of the features
specialize on the sentiment is this was
a positive or negative statement and
some features features specialize on the
domain because we train this across 25m
terrain so some easily detect or are
highly correlated with is this about
boasts of cooling system videos music so
these are underlying factors we know
we're presently there and we find that
the speed features tend to specialize
where's these things much more than the
original
because insurance as soon as it whatever
it's for example with the 3d data
espouses a focused yes to you
somebody's only when n is complete an
example
contrarily honey the dream was ill
covered it so the sudden I saw three in
Sabadell oh boo
I can see last 7000 per aircraft for
example these on board is conveniently
long served in a sturdy assess said I
have the beta constitutes that country
completely absolutely for them excuse me
sister network but I Explorium to
Kanazawa resin so summarizing English so
when standing in the question is can we
just not use our knowledge our prior
knowledge to build in the proper
structure
and my answer to this is of course this
is what we do this is what who we need
to machine learning does and this
especially for computer vision where we
use a lot of prior knowledge of
understanding of vision works but my
belief is that it's also interesting to
see whether machines could discover
these things by themselves because if me
how long does that can do that then we
can still use our knowledge that we can
discover all the things that we didn't
know or that we we're not able to online
and with these algorithms actually is
not too difficult to predict our
knowledge you can have a surrender guard
list of correspond to the things you
know matter and you can put extra terms
maybe looking model that correspond to
prior knowledge you can do all these
things and some people do that if you
actually want to think if I work with
the industrial printer because I want to
have something that works the next six
months but if I I want to solve AI I
think it's worth it Explorer or
general-purpose methods it could be
combined with the kernel but to make the
task of discovering these
general-purpose methods easier focus on
this aspect is I find that
correction the voice was a very specific
human knowledge although different
researchers the NFL exams even
traditional over never went back to the
west and he recognizes America at is
under antimatter share another loop
especially a parent I hope to Devo
concentrate on the example who we are
but they face apology especially as a
family model overly tight as possible
you can do all kinds of things this
what's a freedom to combine prior
knowledge and many different ways it's a
subject of mini free papers combined
prior knowledge with learning so you can
do that sometimes though when you put
too much prior knowledge
it makes it in first
so how much this is a network couple it
matters and well isn't that kind of the
biological prior to retouching about the
very early side right and isn't that did
you kind of feature engineering of
learning when figuring out the right
foot so the answer is no to order this
equation okay I could for example the
size of those layers doesn't matter much
matters in the sense that have to be big
enough for capturing data regarding
violent evil so then because I was
thinking right through our brain comes
why are you certain ways right now
specialist
we've got so um there might be things we
will learn that we'll be able to exploit
at the generic enough
I'm interested in the results that I
mean random variation we need to look so
that reminds me about simulate and
delete when you try to avoid double
minimum but then there were girls along
with the bottom and eating scheduling
and such so is that really important you
know you can see this trick is so simple
I can whiten in one line so there's
nothing complicated in this particular
term you just add noise and dumb way and
somehow well with them in an evening
it's different
Oh Dennis servicing birds you're also
not realize I might become a true feel
awkward sorry
I mean when you're going to eat your
filled room in my ears like yeah you're
trying to minimize the error but under
the perturbations created by this point
so there's a very rough connection
ethically but then poor thing in an
evening is plentiful food rental fee
they also have like temperature to be
being independent that
so we're yeah here is not happening in
this space of parameters are there and
welcome back so consumed leaning and
trying to do some optimization here the
noise is uses a regular eyes meaning
it's it's injecting sort of a prior to
that your your known that should be and
robust to half of it becoming bed that's
beginning you were lucky above
absolutely okay so I can speak for hours
yeah so the general help you so need to
iron the entire that we consider is is
is he condemned utilized a different
answer to your question so the case of
the sparsity prior was mentioning it
comes about simply by adding a term in
love like I had an acquirer explicitly
or do it in the case of the prior that
he mentioned the prior that input
distribution tells us something about
the task and we get it by combining
providing expertise and it gives up the
prior that there are different
abstractions that matter that levels of
abstraction in the wool promise we get
the prior by having a structure and
model that has these different matters
over there are other priors that I
didn't mention for example one of the
priors it's been interesting in studying
the constancy fire or the slowness that
the factors that matter that explaining
world about this
some of them change slowly over time
like the set of people in this room is
not changing very quickly right now it's
a constant over time eventually it will
change but there are there there are
properties the world around us remain
the same for remaining time steps and
this is a prior which you can also go
right in your model by changing the
training criteria to say something like
some of the features they should stay
the same from TVG this one this is very
easy to put in so each prior you can
think of a way to incorporate
investigates changing the structure
changing the training is so I have some
guests as I mentioned if you already our
research is basically about finding out
what these priors are that our
engineering will work for all for many
times to forget thank you very much even
ask one last question as software
practitioner for a lot of machine
learning experts if you think it's
worthwhile for us
for everyday probably one month you
should take jet engines Coursera
a Coursera course yeah you can probably
do it there was also another one by
entering yes there are libraries that
people share it helps to have a cluster
for train how once you have the right
model you can even whatever and the
reason you need a cluster is that these
algorithms have support ever grounders
many dollars set and you want to explore
many configurations of these nodes but
actually training one model can be long
and regular to it's just that you want
to try many tough situations
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>