<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>LH*RSP2P : A Scalable Distributed Data Structure for P2P... | Coder Coacher - Coaching Coders</title><meta content="LH*RSP2P : A Scalable Distributed Data Structure for P2P... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>LH*RSP2P : A Scalable Distributed Data Structure for P2P...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bcTkFig6kyk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello welcome my name is Eunice Carson I
work here at Google and this is very
happy to introduce professor Whittle
litvin here he's going to talk about the
scalable distributed data structure here
this is a very interesting area of
research it was also was working with
whittled on some of the data structures
for my thesis and this is still an area
which needs to be applied but I think
we're going to see more and more of it
being applied it's currently getting
attention in p2p networks a lot
professor Litwin is currently at
computer science in University pollito
fin and he is part of his director of
Syria and his research concerns is multi
database systems data structures and
some of you may know the linear hashing
data structure which was invented by
whittled in the 80s and is currently
being used by almost all database
systems for internal representation you
have Netscape Browser servers Microsoft
postgres sybase SQL server etc are using
this data structure so little has been
them has a past here in California he
was to teach here at at Berkeley and
also been at Stanford for two years and
it's been recorded many times so he's
enjoys being recorded here again and
this is going to be a public talk so any
Google specific questions please take it
off to a talk and we will be available
here also thank you ok so I hope you can
hear me with this Magnus folks a thanks
I would like to thank you much for the
invitation and I'm pretty happy to be at
the your place I passed a number of
times in front with a car but never went
into this particular building so I hope
you will enjoy the talk enough to
justify the invitation now I I don't
know how much time we have 45 I for
ministers question okay so what I will
present you as you could see read in the
meantime I will be the new data
structure which is called the scalable
distributed data structure will tell you
in a moment what is it for those who are
not familiar and it belongs to the
family called LH star which means
essentially distributed leaner hashing
now i will take 'le little bit more
about it in in in a few minutes
according to this plans first of all I
will overview for you the concept of
scalable distributed data structures as
you might not be familiar with that also
it was invented here in nearby in 92 at
HP Labs when i was there by by that time
we'll talk a little bit about a p2p for
those who are not familiar with this
concept and then we will enter
specifically this algorithm has that has
nice properties of being in some sense
optimal the fastest one from the
addressing point of view and will also
overview so called chart management
which is essentially the high
availability which is absolutely very
important in the p2p environment as
peers can be quite often unavailable or
live the configuration and then we will
conclude them now I don't know the local
custom you want rather focused
interrupting this question are you hire
them up or up to you we could be the
space in any questions if the most
questions before the video recording ok
so the afraid to ask a question on the
way if you feel that something is not
easy to understand now a the objective
of this particular data structure is to
adapt the sdds methodology which was not
specifically designed for p2p the
concept of p2p did not exist by the time
to the p2p environment using some
properties of the peers which will
appear in in few minutes and there's a
result this data structure as far as I
know is the fastest one from the other
thing point of view because in case you
miss the address which is essentially
that one peer incorrectly addresses the
query to the pier which the original few
things should be that received but the
theory is wrong then the forwarding of
this message towards the correct correct
address takes only one message to
compare with our vlog messages for
instance by cord so this property makes
it the fastest structure at this time in
p2p environment and probably the fastest
structure ever because there is no less
to do not a way to do better than one
forwarding message in case of addressing
error and also we have added a high
availability to it to it because in the
p2p rd roman if you don't have high
availability then forget about the
scheme because of the churn so let's
call it all and the target is obviously
very large scalable files because this
is intended to work on hundreds or
thousands or millions of interconnected
computer soon and intended for data that
may scale all the time and you will not
see that data got somehow reorganized so
a little overview of what sdbs was a I
do not intend to read all the slides
where you can read them but essentially
a scalable distributed data
structure is intended as a new who was
intended as a new class of data
structures well first of all every data
item was identified by an object
identifier or the key the characteristic
property was that there was no
centralized the addressing because it's
become it becomes a hotspot that is
totally against the ideas in distributed
computing now originally in such a
structure you could have servers that
contain the data and you have clients
that had faced it to the application and
the servers usually were assumed to hold
data in so called buckets or pages if
you like and the structure and any a
sdds evolve it through splits of
overloaded servers so if you had too
many records on a server the server is
supposed to split and the new server is
allocated to the address space and a sum
of the overflow records and some other
move to the new server now the very
important axiom is that the servers do
not notify the clients about the split
at least synchronously because this
would again be very tough if you have
millions of clients a split would be a
very concept cumbersome process besides
the clients may be on enough it may be
your laptop and then you if you close
the laptop there is no way to notify the
something change it out the servers so
how do the other thing proceeds in this
case if you don't have centralized the
addressing clients are not notified well
it proceeds through what is called
client images it depends them on the
particular as the d/s that you use how
the Clyde image is structured but
essentially the client image give some
image of the file state and the client
uses this image and sometimes okay
sometimes not okay because some server
split and the client was not aware or
merged give the data collection
decreased and as a result the
can make the addressing errors in which
there is a they have to there has to be
some system and build in the service
which are able then to forward the
message among them until it reaches the
correct server now how the client knows
that something was wrong with addressing
process it gets back so called the image
adjustment messages that are sent by the
correct server that finally got the
query which in the message says
something like hey your image was
incorrect now we corrected at least
partially what I know as on my data as
the correction for you maybe not the
global correction but at least in this
correction the data are sufficient to
avoid that the client makes the same
same error twice let's see aksam in
every sdds so those principles were
fixed in some somewhere around 93 we
have published a sigma paper by that
time with maria named Anton Donovan
Schneider who are both in the Bay Area
married on a mat in the meantime became
vice president of Oracle recently so
pretty skilled lady and after all there
is about 50 50 or more well maybe around
a hundred of sdd schemes around if you
if you want to see some of them the best
thing you go to to some indexes scheme
all available on the web which is called
google and you ask for scalable
distributed data structure or scalable
distributed structures in plural you get
something like 30,000 estimated
reference comes and then you can look it
in into and you have dozens and dozens
of papers presenting this or that sdds
so it's pretty pretty widely known oh
now if you want to make some guidelines
to that people reconstructed somehow the
classical data structures in the scope
of this terrible distributed data
structure axiom so we have hashed based
scalable distributed the structure and
we have three waste scalable distributed
data structures a
part of the thing i have here does not
seem to be visible it's probably my
fault the it seems that some some color
of the font does not go through so it's
unfortunate that I cannot correct it now
there are more than that but anyway and
something happened no it's all my
computer is the same thing so there is
some bigger trouble I will correct it on
the copy I believe this so anyway the
three based structures are like let's
say be three but scalable distributed
called alpha star and the other schemes
called battle by jagadeesh and some
others or they can be multi-dimensional
including a scar bill distributed
version of our trace if you have
familiar with which we have just
presented at ICD in Istanbul and and
many other on the hush site the basic
structure is LH star which was published
in 93 linear hashing distributed in are
hashing and there are many structures
which are based on so called distributed
in hash table the affected DHT I don't
know where this DD h it should be the
DHT which was originally proposed by a
Bob divine in 94 and somehow took later
bye hellerstein with professor at
Berkeley and and the stoica to a pretty
well-known p2p structure which is in
second sdds called corner where they
used excessively distributed hash tables
and there are other schemes it would not
look upon and finally there are some
schemes which introduced into the skin
with the notion of a AK availability
which is essentially high availability
and we'll talk a little bit about
because the algorithm show you today is
based on that one which was published in
Todd's ACM tats two years ago and there
is an also concept of some kind of
security through the related concept
called algebraic signatures I will not
talk about it today but there was
I CD 204 paper on that and there is a
vldb 07 paper on that coming so if you
are interested by this aspect of
security we can talk after the talk but
I don't enter it today no time so that's
about the picture half a typology of SD
DSS by today here are the missing stuff
Phil is the missing stuff now there is
another universe called structure at p2p
schemes which are in fact as the DSM
bizarre other as if ESS which are pretty
well known today again I will not talk
about now when little bit faster how
does n s dds behaves essentially you
have the clients or peers like here and
then when the clients insert the records
into into the sdb SNSD s start to grow
through splits and grows and grows and
grows and so on now how the image of the
client is managed it in this example we
have three clients this plant has an
image that the file contains only one
side or one bucket this one thinks that
the file contains two buckets this one
thing that the file contains four
buckets but in fact the file contains
four buckets or four nodes if you prefer
in not forum 37 is six no seven but none
of this plan knows it because maybe the
clients didn't use this file when the
last pleats occurred the clients are not
notified so in this example disclaim
sends a query it happens that the data
the client was looking for our in this
particular bucket so the result comes
back to the client and the client is not
notified that something was wrong with
the image okay in this example this
client sends the query and it sends to
the bucket number one this one is 0 this
one is one because it this client knows
only about two buckets so it cannot send
it anywhere else now it happens in the
example that this 8 query was not for
this bucket in the meantime this data
migrated through split somewhere so
those who as the d/s servers have a
built-in mechanism to forward and
finally the correct packet happens to be
this one
so this bucket will send the image
adjustment message to the to this this
client and as a result the crowds would
adjust the image the image and we'll end
up with the image of six buckets you may
notice that the seven bucket is not in
the image still and the next time this
client will send the query to this data
item it will send it directly to back at
six and as long as this item did not
migrate because this bucket did not
split it will get the data item directly
okay so that's the principle of image
map adjustment messages in in the intern
at shell now there is a prototype of all
those features which is available at our
site that you can download turns out
that this prototype implements something
that Johan yun estimated his thesis and
publish an EDD paper which is a pretty
good achievement called LH star r l h
which is a fancy way of combining the
use of hash function for a internode
addressing with the hash function used
for intra not addressing which is not a
bigger problem for at all and the his
thesis and the design of david was was
efficient enough to be still use the
despite that it was done when 90s
beginning of the night is something so
this is very efficient work and we were
very happy and ed BTW reviewers as well
because it went though now this
prototype works in distributed ramens
you know this is probably the future if
you looked on the let's say Jim grace
last talks before before his
disappearance he is claiming that the
disk is became the archival storage
everything will be in distributed from
so this system is quite advanced and so
on so well just a little comment when we
talk about three hundred times faster
what does it mean in practice it
practice it means that if you have a
database the respond
time to York where it goes down from
let's say five hours 21 minutes that's
what means three hundred times faster
search time and so it's very important I
think now the other area what we had had
was the so-called p2p it appeared in 95
as many of you knows through kind of
idea of stealing essentially dvds and
movies Embley and songs but pretty
quickly people came into the concept
that this is a useful concept but they
have to use something which is called
structured p2p to avoid the flooding
where flooding is the query that you
don't know where to sensors you send it
everywhere so we when once i started to
to do p2p structured p2p in fact it
became as the dss yes sir yeah that well
yes and no they use small or small world
graphs at one point but if you didn't
know where it was a on smaller scale
would say the small world guys graphs
were quite similar to flooding at least
if you didn't know exactly that was my
opinion after reading paper but but
still you you write the small one glass
after a while avoid flooding somehow and
which is pretty nice idea okay it's so
apparent you are aware of of this part
of computer science methodology so with
respect to structured p2p probably the
best word non-work is court and also p
trees which are by karl aberer somehow
little bit less known but predated cold
as the typical performance with respect
to forwarding is 0 of log of n where n
is a number of nodes so if you have if
you have let's say million of nodes then
you your query may need something on
average like 20 forwarding ylh started
in mention it has the upper bound of to
a message is only for various reasons
now and I started I will not go very
deeply into it if you wanna recall the
features of the target there was a paper
in touch in ACM 1396 which describes is
very much in detail see there you go to
ACM digital library I can somehow send
you after the talk so now we go
specifically to the new algorithm now
this algorithm is based itself on a
variant of H star which was designed for
high availability and publishing cuts to
the years ago called LH star RS RS
standing for its Salomon with was the
kind of the codes which we useful to
manage the high availability parity and
it has p2p because we wanted to murder
the idea of SD DSS and p2p what was this
idea what was the merging point in in
SDS methodology we had plant and servers
and the client were not let's say
reliable at all in the sense that the
client could be your laptop and the
client could be on enough completely in
the p2p terminology as we understand it
the clients are not such plans are not
very well seen especially it because to
be on and off and just use the file as
soon as did yes they are called
freeriders for instance in infinite a
rather everybody who wants to be a peer
make some commitment that it will share
also the burden of having the data so
here we assume that every LH style RS
node has the client component and the
server component this is the kind of the
commitment now this change is completed
nature because if this node becomes a
also a server component it means that it
offers some kind of minimal reliability
because it's not fair to have server
component and hold some data and
frequently to be off of line so under
this assumption
a the internal design of the algorithm
how changes in the sense that we can
organize some kind of passages
information between client component and
several component that I will show
shortly and this passage of information
between the client and server makes the
system more efficient than otherwise if
you have let's see traditional clients
and servers now in this architecture we
have some peers all each of them has a
client components has a several
components of wholesome data but we also
have or may have in the system some
nodes which would like to be it appears
but simply there is no way for them yet
to have data because there is not enough
data we call them the candidate peer so
somehow last you say I want to join the
community give me some data whenever you
have this data and then you will not
forbid you to from working because there
is no data for you so then you act as a
spare you are just waiting for data till
you get this data in the meantime you
are allowed to use your client part is a
free rider because you are not afraid
either because you committed to have
data whenever the file needs you and
those guys are called candidate pins and
they are managed somehow as I will show
you in a moment by some peers and from
that point the a view of those peers
they are called pupils and every at
least some of those pills may have
people's not necessarily they have but
they may have and for those people's
those guys are called tutors the
vocabulary you will see and essentially
the tutors make aware of people a pupils
which are only a few about the file
evolution as you will see very shortly
now people have to be somehow identified
to the tutors and the identification we
use is simply the IP of the people which
becomes kind of the hash key that's the
same thing as in corn for instance now
this is essentially on what you see on
the left side to it makes reference to
the internal instruction of alleged our
file is essentially the server part we
have a param
call Jay mr. g gr Jay Jay Jay which is
so-called level of the hash function
used for the server and here we have the
client image which is essentially
defined by two parameters called I Prime
and prime it will become clear in a
moment what is it so now I have to
recall a little bit of Unknowing stuff
about the LH star there is a little bit
of formal is but not too much it will
not to over only to talk so essentially
if you take an alleged our files also
alleged RRS file and many other versions
of LH star the each each record has some
global position in the file which is
defined by the linear hash functions
which are somehow listed here and
essentially if you want to know where
the record should be then you take the
key of the record you apply the current
hash function defined by what is called
file state with the value I it gives you
the bucket number which is 0 1 2 3 4 5
and so on and the record is supposed to
be there in this bucket except that what
happens at the nobody knows this is
those functions essentially nobody knows
the value of i and this parameter n
because because the clients because
there is no centralized addressing
component in fact there is one which is
called coordinator but this component is
only useful street and no client has
access to them so the crane has to have
to compute the address otherwise and
they have the image which is essentially
using the same functions except that
they use every client uses the value
which is called I prime and n prime in
this image which are simply the best
approximation decline no of the actual
values of i and and which are unknown to
the client ok to any client so each
client has some value of item and Empire
it defines the address
schema each client uses the same
function which are essentially hashing
by division is very nice properties and
but each client knows only its local
values ok so then knowing that you have
to know a little bit how the file
expands so the file expansive splits and
the splits in the linear hashing of the
particularity which made this algorithm
will known that is when you can have the
overflows on some buckets but if you
have an overflow on the bucket it
doesn't mean that this bucket will split
immediately because of the overflow the
overflows the splits are managed using
something which is called the split
point error and that's the value of this
variable m and this value of the
variable n points to the next bucket to
split and the coordinator somehow
manages I will show in in a second how
what is the next bucket to split when
some buckets has an overflow so the
scheme is roughly as follows if I bucket
overflows it allows the coordinator
saying well I have an overflow dan the
coordinator will say to some other
buckets to split ok and this bucket will
split and then switch surprises always
and then things simply seem not ok or
because this this bucket the original
back is still states with over four but
in fact the movement of the pointer is
organized so that sometimes later of
this bucket that had an over four way
split in turn so this please have some
haoles a little bit later now some
theoretical properties behind it
motivated this scheme obviously ok so
you have some details of this scheme and
this is organized so all together that
the other things king addressing space
expands gradually and every time they
split occurs there is a new address edit
which is the same always the number of
equal to the number of addresses in the
file plus one that's a property of the
division function that's why it is used
there are two hash function with those
nice properties one is called division
which is well known and other whisk is
called
multiplication function which is a
little bit less known i don't know why
because it's really beautiful function
and described in details includes the
art of computer programming book but
somehow people do not read the art of
computer programming maybe it's too old
and somehow nobody uses the
multiplication function because the same
property anyway so we see the the bottom
line is that when they file you start
your allege that file you started this
bucket called 0 and then the first print
will add the bucket one and then the
pointer comes back to zero and the next
please use is the next dinner hash
function which will with I greater by
one so the next bit will be from 0 to 2
and the bucket one will split into the
new bucket three and then the pointer
comebacks 20 and then in the next
nowhere flow 0 will split 24 and one
would split 25 and two will split 26 and
30 SB 27 and now the pile has as you can
see the load of the file is distributed
over four buckets and then the pointer
comes back to zero and we start to use
the higher level of I so now we we the
first bucket will split 28 using I
equals three and the bucket one will
split 29 and two is split to 10 and so
on so until we get to the 15 and then it
comes back to zero and we start to use
the next little hash function and so on
and so on see and every time the new
speed as the bucket which is just behind
the existing ones so it's it's a smooth
extension and the every speed
distributes blows evenly on the back of
the splits and the new bucket so the
file is never really overloaded there
are a bunch of papers in the on the web
accessible through google that analyze
the load factor of LH star if you like
mathematics people made very nice things
to guess what could be the overall load
in the bucket sun devils the schema but
bottom line is it's about seventy
percent of load factor now in
what what is the real new things in LH
started RS p 2 p with respect to alleged
r RS one of the new things is that since
now we have the piers and the server
communicates with a client of the same
site or has some Purple's then in the
moment when the server splits it can
adjust the a client's image what didn't
exist in any previous version so it does
it immediately using this outlet which
is pretty trivial as every says okay I
just split so I know perfectly perfectly
what is the level of of me my bucket
what is the hash function use and I know
perfectly what is the next bucket to
split which will be the next one behind
me so a plus one is this is the bucket
to split so I pass you on this
information now at this particular
moment us my favorite client because you
are on mine own you're just my client
component you perfectly know what is the
email the state of the fire water is a
value of i and a high end n so those
value are equal so you set up your I
prim and Prime and n Prime to I and and
and that I know and at this particular
moment you are very lucky because you
will know the exact state of the whole
file and that's exactly what happens
it's a big benefit okay so then the
things go obviously in different way
which is the declarant progressively
become decentralized with the file state
the actual file said because other
buckets will split and they don't mean
from this particular client but at the
next round when the pointer comes back
to the same bucket the client will be
synchronized again and it happens for
every client component on every PFC and
it also happens to the people so how
does it work for instance in this
example this this buckets place these
are just values that you can study after
the talker sends a message to the
coordinator saying well I have an
overflow the coordinator happens to be
point pointing the bucket 1v1 for
splitting
so it sends a message to p1 saying now
you split pea one does the split and the
p0 will if you end up splitting adjust
the parameters of this client component
through this message and the file expand
by one bucket and the p1 send some some
data to the new bucket which is b3 and
after the speed coordinator are just I'm
sorry mostly too fast adjust its image
to a equal to and then 20 of course you
you have to believe this those Valley
but you can study the talk after after
the slides after the token convinced
that those are true now they the other
part is now imagine that the server gets
an incorrectly addressed worry because
the client didn't know that there's some
split occurs how the server know where
to forward it turns out that the server
will know where to forward and simply
executing this small Albert here which
is essentially saying okay I got this
request so I check with my level of the
hash function I I know whether this key
falls down to me after they executing
this linear hash function HD if it falls
out to me then the client didn't make an
error if it doesn't fall down to me so
it means that in the meantime ibly split
but the client is not they were so I
will need to forward where I have
created a bucket and tells how that can
be only one bucket for that so the
second line of the circuit will tell
where exactly this particular peer is
supposed to forward the key which was
incorrectly addressed to him and there
can be only one such address and when it
comes to the on the other side to that
pier which is there is a child of the
the one displayed the on the other side
it will be for sure the good address as
we will show in the middle ok so the
first time is checking that it is the
correct address on the pier on the
server part of the fear and the second
line is forwarding it in case the
address was incorrect and that's the
whole allowed that is you can see it's
not really
complicated how do we adjust the images
again the pretty simple of it which they
somehow hey I got to incorrect address
so i am bucket a i exist so now taken
out of this and the taken out of this
formally translates to this algorithm
again you can somehow look afterwards
simply the grant will adjust the images
ago if this guy exists oh my i prim I
Prime and N tine have to be at least so
and so and the so-and-so is defined in
by those two lights okay safety three
lines let's say three instructions okay
so you will see it after the talk so in
an example just in the nutshell imagine
that this guy this Pierre wants to
insert the key or search for the key
where the value of the key is 9 so it
turns on really calculates the address
of 9 with respect to its components it
will be how 9 modulo 2 power 2 which
gives the reminder of one so it says
this to the bucket one thinking that
this is the good address the bucket one
checked with respect to j equal 4 but
Jake will for is the division by 2 power
4 which is 16 and obviously when you
divide reminder of 9/16 is not one but
is 9 so it will say hey it's not for me
so where it could be but it would be to
the bucket it has to be forwarded to the
bucket that I have created when I moved
from g equals J equals 3 2 j equal four
but this can be only the bucket which is
number nine so it can the others that i
just got this is a second line of the
forwarding queda so if you resend it to
the nine and nine obviously have to
accept this because there is no other
possibility here you can check it
manually so this will be the end of the
process okay and finally nine the packet
nine ways and i am too p for and pie
before will adjust it its parameters to
those who saying hey I know now that
nine exist so it's how it works okay
so this is a little example of fire
expansion so again you have to go a
little bit faster so we in this case
there is an overflow and then with a
show here in this this guy will split so
it sends the message about the new
parameters it will have two day to day
to its people that's basically basically
purpose of this slide and the people
will essentially know what is the fire
state I will remain in sync sync with
the with the pier that that let's split
okay so that's what will happen to to
the people and by the same against the
same algorithm will know less about what
is the file state less or more but
enough to to to be in sync so what what
is it all about what is the result the
finder is out the final result state can
be stated in the free theorems which is
essentially that under this scheme the
maximum number of forwarding messages
for the key search is exactly one this
is the worst case the other operation I
didn't spoke about which is called scan
when you send a query to values to it's
a non-key query so we send it to
possibly to all the all the notes
because you don't know to where to send
it it's the kind of flooding and that
this assumption the it turns out that
the maximum number of rounds for a
forest can search is to which is again
the best results and finally the self
further and will say that under the
axioms are structured P to P and s dds
most likely there will never be any
algorithmic will be faster in the sense
than the one that you have just seen so
it has an optimality flow now there is
the proof of this in the talk but I will
skip it because we don't have enough
time essentially there are few slides
you see that show what could happen to
the file
from the perspective of the bucket that
tries to address something so for
instance here we have the bucket a and
this part is the image that this bucket
a has have defined in fact the image has
evolved because the pointer and moved to
the right there were some splits and
those please have evolved with the image
here and have a valid the image here
okay so now what can happen when this
bucket this this this Pierre wants to
address something so if the address
think of this pier falls down into the
pier which was not yet split you see
there cannot be forwarding this is this
part of the file if it falls down
because of the key value into the part
that already evolved they were split
they can be a forwarding not necessarily
there is one but they can be and then I
think is the following conserve this
part of the file that was unknown to the
client to this kind component and
finally there can be another situation
everything was split in the painter went
back to zero and came back and then
again as long as we are not in this part
they can be a forward okay it's a sample
of that for the property to about the
number of Francis Slay bit more
complicated for non family for the folks
unfamiliar with zalgar it so I just
displayed and if you really want to know
the details then i left the extended
abstract of the paper on the three with
visual as you can look into the paper
okay so let's keep it okay let's come
back through to the last part in a few
minutes which is called the charm
management what is the chan chan it
means that you have those nodes those
peers who are on and off the order and
then they disappear for various reasons
how is it managed in this scheme it is
based on the thoughts paper scheme that
I told you called LH star RS in LH star
RS in fact all the peers are divided or
grouped into something which is called
reliability groups and you see here so
some of those pills are reliability
and each reliability group is provided
with some parity buckets in this example
you have to party but the bucket
reliability group and the size of the
reliability group is fixed for instance
in this example we have every four
buckets are constitute the reliability
group so the back is for instance 0 1 2
3 will be will have two two buckets edit
it which is which are called parity
buckets and the bucket 4 5 6 7 would
have also parity buckets and so on and
so on now within the parity buckets we
have the parity we have the erect
so-called record groups provided with
some ranks and this record groups each a
record group data record group that you
can see here have the parity records
which are calculated here the same in
the case of the LH star RS p2p the same
is applied to the tutoring records which
are essentially the records we say I
have that you'd pupils or I don't have
the pupils and those tutoring records
also have to be protective against
against Chen so they have their own
parity records here the big deal of
course is how do we calculate the parity
in this case and the tots paper give a
scheme which to my opinion is the most
efficient up to now using a particular
class we have invented with the course
or of that paper who's professor here at
Santa Clara which is a particular class
of reed-solomon codes so let me stop
here the nice property of this stuff is
that the rich Solomon codes as we
defined them allows you to have as many
parity records as you like here and if
you in this example if you have to
parity records here it means that we you
can lose any two of the data records
here and they the calculor parity
calculus will reconstruct this bucket
okay you can lose up to any two if you
want to have a better protection then
you can set up with three parity records
/ reliability group then you will be
protect
it against three why it is so important
because when the structural scales up
then the probability that you have
multiple failures grow up as well so we
have the concept of so-called scalable
availability in this system where the
number of parity back is protecting your
reliability growth group growth growth
goes automatically up with the file size
it's an automatic process process we
have to see the table now we have two
things that we can have in a theater
peer system with a live here living with
notice or without notice if it lives
with the notice then it it somehow it
notifies the first one of the parity
bucket which is called the reliable
reliability group manager and the the
reliability group manager will find a
candidate PA that's why they have all
those peoples and will somehow move the
data from the living here to the tier
that leaves the the configuration to the
candidates pier and the things I've done
the more complicated case when the pure
lives without notice and suddenly you
don't have those data okay then the reed
solomon the parity calculus goes to the
full swing and well basically you have
to loop the paper to see how it works
it's little bit like red accepted it
uses not only X or calculus but also
multiplication in Galois fields and and
finally it lets it can recover what was
on what what was lost with the skill why
we use the reed solomon because in a
reliability group it may of course
happen thats ever appears lived without
notice at the same time and you have to
be protected so in this case there will
be simply the reconstruction basically
somebody will address this period that
is gone so if it does not reply then the
query will be sent to another than any
other member of the reliability group
the next year this one will alert the
the parity coordinate
lady bt group coordinator and this one
will simply recover the pier on some
candidate pills and that's it okay so
and the answer I'm sorry the answer will
come back from the new pier and show it
from the new pier and the client will
notice that this is the there is a new
physical address of the bucket that
responded okay now the most difficult
case is the following one when there is
a communication failure and the puree
somehow failed and then recover but in
the meantime those disappear was
recovered somewhere else so you wake up
you don't know that you were cut off of
the rest of the community and it may
happen than some other peer also doesn't
know that you are cut off i will send
you a query despite the fact that in
fact you will recognize the constructed
summer so this is an online in case but
not that I'm lying in fact it whenever
your query is an update then the the
parity pill will know that something
happened to the to the pier which which
is gone because the the update request
have to necessarily be forwarded to the
parity back is just for update of the
parity the problem with any problem is
use the search because the search is
normally not forwarded to the parity
bucket and simply it can go to the
appear which doesn't know that it was
reconstructed somewhere and you can get
the fakery response to your search okay
without knowing that this effect with
fake response so we have a new concept
which we call the shore search okay
which is that even if you really want to
be sure that the result is always
correct when you set up the LA star RSC
p2p fire so that they even the search is
forwarded to the reliability group
coordinator and this guy will always
know that the the bucket the pier that
was gone for because of the transit
filer failure is really gone so it will
say hi you are not the right guy so it
will forward it to the right guy with
some
and this guy will respond to the pier
which ask you the query the search and
this pier will notice that it was wrong
adjust the image and that's it ok so
this Shore search concept is the new one
I've never seen anywhere it looks sample
but it's still no so i guess in the
nutshell i told you everything about
this algorithm so what is it it's a hash
based schema which has a nice property
of require only one forwarding in the
worst case and this is the unique
property of this elder it up to now it
also protects efficiently against churn
which is somehow big problem is few
answers and up to now as far as i know
you have if you want to to see on the
state of the art you look into papers by
could be 80 which works a lot on that
ball play and they might have might get
different ideas from what I presented
you at least appreciate the idea that I
presented you could be out of which the
skins are very good by the way a famous
paper about managing the cows that I
strongly encourage you to read if you're
interested by the domain and finally it
is intended to manage very large
scalable files obviously going because
of the simplicity of the addressing and
because of the lack of any table it is
intended for people to be communique
configuration going up going up to
millions of notes if you need them and I
guess should have numerous applications
because of its nice properties whether
there's an application at Google is an
open subject but about you work on
something called big table I understood
so naturally it seems to be a big table
now current and future work this alga
tease pretty pretty recent so we are
trying to implement at this point based
on work of some other guys and do
performance analysis and do nice
variants some of them being addressed
already in the paper a little on ok so
that's it for me I think I didn't run
over time thank you for your audition
and open for other questions
yeah so if it has any questions
or is there a lot of state I'm not sure
I understood completely the question you
talked about storage space requirements
the load factor of this target is the
same one as for about the same one as
for a LH star and any dynamic hash
algorithm which is about seventy percent
of precisely lock to of e for for the
data application data here we also have
a little overhead for the parity data
obviously now the overload overhead for
the parity data depends essentially on
the size of your reliability group so if
you take the size of reliability group
let's say sixteen which seems to be
quite practical then and you decide to
have one parity bucket when the file is
small for those 16 data buckets and you
will add on it one sixteenth to the load
factor if you decide to have two packets
parity buckets will have to 16 and so on
I'm not sure whether I responded
completely but that's that's about it so
it's pretty spaced efficient as
efficient for data sp3 dynamic hush
scheme of any kind extendable hashing
and so on linear hashing or LH star has
one of the properties that it be the
data which a client needs to keep about
the structure is quite minimal and is
only these two free variables which
keeps the state associated those are two
state parameters plus IP addresses of
course leaders if you send declare you
have to know where the pier is that's
obvious and more questions
all right thank you very much for coming
okay okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>