<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GeoDec: Enabling Geospatial Decision Making | Coder Coacher - Coaching Coders</title><meta content="GeoDec: Enabling Geospatial Decision Making - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GeoDec: Enabling Geospatial Decision Making</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1q7J00jD0Qk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I
today we are very pleased to have Cyrus
shahab II from the University of
Southern California here to give us a
talk on his work in geo deck enabling of
geospatial decision making professor Xie
javi is a currently an associate
professor and director of the
information laboratory at the computer
science department he's also a research
area director of NSF's integrated media
systems at the University of Southern
California he has his BS and ms and PhDs
from University of Southern California
and for the two hundred articles and
chapters and south papers in area
databases GIS multimedia current
research interests include geospatial
and multi-dimensional data analysis
please join me in welcoming professor
shabi to google all right thank you very
much if for the introduction and thank
you all for coming I know that it's very
tough to leave the delicious food in the
cafeteria and come over listen to the
top why I appreciate it so this is as
chip briefly mentioned there is this
there's this center at USC called
integrated media system center or IMS
see over there and it's basically a
center supported by national science
foundation for 11 years it started 96 so
we're kind of towards the end of it one
of the goals of the center is to do
multidisciplinary multi investigator
research so as an example what I'm going
to talk today about this geo deck which
stands for geospatial decision-making
and this is this this project that has
basically more than one faculty involved
so as you see a group of five faculty
involved and for some reason my laser
pointer doesn't work but basically the
Boehner idea is to have faculty from
different areas so you have people
databases AI computer graphic computer
vision systems collaborating for for
this project and if you guys are
familiar with the accademia is usually
tough to get a bunch of faculty to work
together so this by itself is an
accomplishment just this first slide to
have all these people agreeing to
contribute the same project and have
their names collectively there of course
as usual the real work is done by the
staff and the students so you're just a
PR to tell you what they did in one
slide if I want to summarize the vision
of geo deckhand and challenges and so on
so the vision is basically we want to be
able build these two spatial spaces
build these two special spaces like a
city both quickly and accurately so
those are very operative key words here
rapidly because we basically want to
distinguish ourselves from for example
Hollywood where they spend a lot of time
and a lot of human resources building
these 3d virtual environments and
accurately because we want to be able to
build things that are basically unlike
Google Earth for example that the main
users are a public that they just want
to find houses and stuff you want to
build things that can be used for
geospatial decision-making for querying
therefore they need to be more accurate
and I will show some examples of cases
where we need to do things more
accurately than for example Google Earth
and then finally the the idea is not
just the space but of course time and
then be able to do querying an axis so
if I want to just summarize the
challenges we have is to get a raise is
for realistic rendering and accurate
information fusion and so on there are
many applications out there i was
talking actually to the googler chief
technologist and telling him that
I see more and more applications that
serious users using google earth versus
a lot of s 3g is type products to to do
real GOG is integration and an analysis
so examples include city planner
emergency response military intelligence
simulation and training and so on that
the last item was kind of interesting
because when we started project we were
just showing this to a bunch of folks
and then we see a lot of people from NBC
and ABC coming in and showing interest
in the project and we asked them what
why you're interested I said we for
example there's a location like a
Vatican and we want to do a quick news
on in that area so it would be very nice
if we can quickly build a 3d model we
can fly through and do all sorts of
information integrated into that
environment so we added that as another
application which can benefit from a
system like that this is just to give
you the type of data sources that we are
looking into integrating for geo deck
very much I guess similar to the type of
things that Google Earth integrates
right now satellite images aerial
photographs 3d models and so on and the
idea is that they all coming from
different sources they are having
different formats and different
standards and so on so right now most of
our demos are basically in this blue
area the the the experience part however
we are trying to move into this area
where you can start asking questions and
I'm doing analysis on it on the data and
by the way if you have any questions
feel free to interrupt me and ask the
question on any of these this is another
block diagram of the system this is
basically showing all the data sources
here and you see some of these data
sources directly being used for
integration some of them in addition to
being used directly they also go through
some computer vision computer graphic
processing and as a result we generate
new sources of data that again they
would get integrated and the final
result be displayed to the
user for interaction I'm gonna a little
bit of a change here minimize my
shortest path alright so now let's get
to some of the underlying technologies
that we have and how they come together
for building a system such as the one
that I just mentioned the three specific
pieces i'm going to talk about are the
building the 3d models then putting
textures on them and then fusing all
sorts of geospatial data so as I said
this is multidisciplinary project so the
first one for example is done by my
colleagues in computer vision so I have
a little bit of information about the
details so I tried my best to answer
your questions there but if you have
more detailed question I might refer you
to them and second one is by my computer
graphic colleagues and then the last one
is the piece that's mainly by myself and
my AI colleague so the first step
basically is that you want to build 3d
models i think google recently acquired
sketchup as a software to build these 3d
models so this is an alternative
approach to do that this project
actually started by receiving funding
from NGA and went through different
phases so right now it's actually the
last phase and it's kind of ready to
become a product and the idea is to get
two images say stereo images aerial
images of an area and then by
interacting with the image but but
minimal interaction with the image you
basically built a 3d model I think the
best the way to show it is through a
demo so I I think I can leave this slide
somewhere for you guys if you need to
see more information but the system is
called imds and let me just quickly show
you how it works by quick video so this
is you start with the 2d image we work
on one image only you start the box is
automatically generated around the
building then this is the user clicking
on specific points and basically
making the court identify in the corner
points the height information is
calculated automatically by the software
by using the other image as I said there
are two images but the user is only
interacting with one image so with some
little clicks here and there you end up
with an accurate model I show you some
examples of the model so here are
basically here are the example of the
aerial images that you have and these
are the models that are built using the
software as a result as you see can get
into very much details on the model if
the application requires such a detail
so for the basic I'm going to show you a
couple of areas that we actually built
them also for an area in DC that i'm
going to show you night next actually it
took half a day but this is the PhD
student who actually built the system so
you're assuming an expert user here but
one of the reason NJ kind of like this
is because at each point in time you
only work with one to the image as a
user so you don't really need to have
any 3d view of anything just work with
the 2d images the depths of heights is
calculated automatically by the software
by using the parameters of the stereo
images because you have the image from
two angle you can calculate the heights
so here is the DC area that I'm talking
to these are the Google models and I
assume that the focus there was not
accuracy as I said because of the
application so there are some areas that
I want you to look at for example that
area of up there and these buildings
here that are kind of boxes and these
are the models we built in half a day
with the software so you see all the
details of of the buildings are
incorporated into the into the models
and this is another area we did this is
USC campus i'm going to show you
actually a demo of this one so so that
was the first step you end up with these
models but as you see the models are
very dull these are these white blocks
that you can't get a sense of what
exactly is the building so the next step
is to apply textures to these models so
this is the part where my computer
graphic colleague work and his basically
team looking at different techniques to
use both aerial image as well as
pictures taken from buildings and then
use that to text your map the models
that some of the challenges he's
involved with is for example here you
see that you can't see the whole
building because of the three so how you
can basically either extrapolate the
texture behind the trees or if you have
another picture from a different angle
try to fuse the two and generate these
missing textures so it's basically the
the one that extrapolate or interpolate
is called synthesis the illusion
occlusion detection and removal is the
part where you use multiple pictures to
basically see what's happening
underneath and remove the obstacles so
here is an example of again the USC
campus that i'm going to show you where
we wanted to text your map this building
and we didn't have the full building in
one picture so we have multiple pictures
that are fused the other piece that my
colleague work is not only mapped the
static textures from images but also
mapping textures from video so that
would give this a sense of dynamic
moving objects to the model so i have
another quick video and if you think i'm
moving the mouse very slowly make sure
that i understand that i'm doing this
backwards
okay so this is the 3d models with the
video textures the buildings are real 3d
model so as we move the navigate through
things they stay correct but the video
like the like the objects like the map
poles or or the trees these are 2d
objects so so as we move around you see
that they actually get bent because
there are just 2 D frames on top of the
3d model some areas like here for
example you see the cars disappear
because we didn't have any video camera
for that area so we just use the texture
map from the aerial image and so on
okay so the the last piece that I'm
going to talk about and this is the
longer one so don't expect this one to
finish soon because this is my own work
is the part that you want to now
integrate all the other geospatial data
you have about that area so what you get
from the first two pieces are basically
the models and the textures but this is
only good for navigation you don't have
any real information about the area if
you want to do real geospatial
decision-making you need to have
information so there are a bunch of data
sets out there and I'm sure you guys are
aware of so imagery a very good
resolution a lot of them there are many
maps of different areas there around we
can get road network data power line
data railroad tracks so a lot of vector
data available out there in our case for
example we have database from NGA of the
entire world vector data for the entire
board elevation data digital elevation
data available also again from NGA a lot
of coin data available for around the
world again ng is one source it was
called Nemo before but that's basically
lat and long of points with the
description of the point and all sorts
of information about it there are a lot
of websites that are not geospatial by
nature but they have to especially
related information one example is
property tax site where basically it has
parcel information there are online
telephone books and Railroad schedules
where you can find for example temporal
information as well as spatial
information like the station is the
coordinates the addresses and so on a
lot of other sources for temporal data
right traffic data GPS data and video
out there so and then of course weather
information so the question is that how
we can integrate all these different
sources and and the idea here is not to
do it her application from scratch but
to have certain tools and certain
framework so
you can do the integration quickly for
any given application so if I want to
for example focus on an emergency
response application identify these
certain sources that the focus of our
work is to build tools so you can
quickly wrap these different sources and
then generate this integrated model so
that you can integrate all these data
and then start asking questions so I'll
give you a couple of actually three
examples of different fusion challenges
that we worked it specifically and start
with the first one where is the
challenge that I think it's very
relevant also to google map on google
earth which is integrating road network
data with aerial photographs or aerial
images so if you look at the data here
for example you see an aerial image and
you see the vector data the red lines
and as you see the red lines don't match
the actual roads on the image and this
is a very typical problem not because
either of these sources are wrong it's
just because because the earth is not
flat so people use different projections
or they use different elevation models
to fix their data sets and then as a
result when you put the two on top of
each other they don't align so this is
not just our problem here are examples
from other places actually Google is
doing pretty well I don't know where you
guys get your vector data but even I
mean still doesn't match correctly but
it's pretty close but it depends on the
area that you look at I know Microsoft
using the same vector data that we're
using which is navteq and there and they
don't do any correction so you see it's
actually worse in this case that the
road network and the image they don't
match so what we want to do is basically
start from this which is exactly the
road network data from navteq and then
automatically generate the blue one
without any human in the loop so you can
fix the error so you might say why this
is important so as I said depend
the application if the application is
just for a human to view things it might
be okay if there are some mismatches for
example like here however for example if
you're building a simulation training
system and you want for example tanks to
move on the road and you have the
satellite image or 3d model as the
background the tanks can actually move
on vector data are not on the image
because that's the information they have
in a simulation and you don't want the
tanks to move over the houses sometimes
you do but hopefully you don't so so you
depending on the application you may
want to have a very accurate fusion
another example actually is if you want
to do geocoding so I assume that you
have the correct alignment of the vector
data to the to the images then it would
be much easier to identify houses and
parcel information and then this is
where I told you that you can integrate
this with all the geospatially related
sources like yellow pages and property
tax sites and so on to get all sorts of
information about public information
about the area so this is actually what
is called geocoding you start from a
mailing address and you it's reverse
yokan you start from a coordinates and
get back the mailing ends so what's our
approach to do this so this is just the
the technical term for this work is
called conflation which is the work that
you get two sources of data in this case
a used at you especially in this case a
salad image in it and rode vector and
then you go through the process of
aligning them correctly the traditional
way of doing this is basically a human
basically sit down and say okay between
these two sources I find the set of
compares that points that I call control
point pairs that they match with each
other so I know for example this
intersection here is the same
intersection as this one okay so a human
basically identify these points and then
you go through the process of forget
about the filtering where you go to the
process of rubber sheeting which
basically align
the two sources the the reason that is
called Robert is because as if
one of the sources is on rubber and you
move it around okay so it matches the
other source and the reason that this is
rubber and not still is because you
dealing with bunch of local
transformation so the problem is not as
if you just take one of the sources and
move it to the left or the right and fix
it you need to do a lot of local
transformation so it's not one global
transformation its bunch of local
transformations and that's why you need
all these control point pairs to
identify where you apply the local
transformations so but but so that they
just use it so that's actually not our
contribution Detroit the transformation
we just use a known technique okay the
contribution is actually on this part
where we replace the first part that I
told you that the human season identify
the control pointers when you place that
part with our algorithm okay and the way
we do it is this so let me first tell
you how this would have been done
traditionally traditionally if you want
to do this automatically for example for
the alignment of roads the images the
best feature that you can find for the
control pointers or intersections ok
because intersections are easy to
identify on vector data and they're also
relatively easy to identify on images ok
so if you want to use a pure image
recognition technique you basically use
a some sort of machine learning
technique like Bayes classifier and then
you classify you first chain it you
basically go a human goes and say ok
these pixels are on road for example and
these pixels are off road ok and then
the system learns this and classify
things automatically so after a while
basically any pixel you give to the
machine learning techniques would tell
you whether it's on road or off-road ok
in this case the white pixels are on
road the black pixels are off road ok
now here is the problem the problem is
that if you do it only that you see that
area the black area in the road even
though those are on road pixels they are
colored black and the reason that their
color black or because of the tree here
right that's why they are
if I'd wrong the other example is this
area here over there you see that there
are a bunch of white pixels that are
classified on road while they're not
really on road but the reason is this
rooftop this has pretty much the same
color and texture as the road and that's
why you miss classified so you know no
matter what you do your end up with this
noise and if this happens and you just
doing a blind intersection detection
there is no way that you can identify
this as an intersection from that image
right that's just noisy all right now
what we do which makes this a little bit
easier is we use the vector information
to narrow down our search space okay so
from the vector data we know that there
is already an intersection we know that
it's not exactly turbo we notice in an
area around it okay so first of all we
only need to search that area for
intersection at the entire image more
than that we know that there are two
lines for example on this intersection
and also from the date metadata about
the victor we can sometimes know even
what how wide these roads are so we
basically generate a template of what
you're looking for and then now that we
have exactly the template that you are
looking for and the area that we are
looking for there is a better chance
that we actually match it and identified
as an intersection so that's the that's
the piece that basically the domain
contribution from our site is to do the
control point pair detection however no
matter how well you do things you will
end up with wrong noisy points so here
is an example where you first find the
intersection of vector data the red
points which is kind of easy and then
using the technique that I told you we
find the corresponding intersection
points on the images so as you see you
still end up with wrong intersection so
for example this one here right this is
the wrong one detective so no matter how
well you do you still end up with those
but another advantage of our technique
is that you don't really need to find
all intersection points you need to only
find a certain number of good points so
we use again and a known filtering
technique which basically look
at the displacement of the intersections
and then find a median vector and throw
out all the outliers so as a result of
that we end up throwing out this one for
example because it's an outlier as
compared to the other vectors okay so
okay now now after we find those
intersection pointers that we know for
sure they are correct then this is the
part again that we are just using a
known for shooting algorithm okay which
is the Delaunay triangulation so
basically from those points generate the
triangles and then from and then use the
used the vertices of the triangle to
generate the transformation matrix and
then multiply the transformation minute
to the entire triangle okay so that's
the way that the transformation works so
here are some results within a project
for USGS to align their data set
automatically we first started with
tiger data which is pretty bad so the
red one tiger data is really bad quality
vector data and we did some fixing their
here is navteq data oh I don't have any
more but if you're interested in the
actual quantification of our technique
and we did a lots of experiments to
compare how well we did on different
areas so I can send you papers if you're
interested the other the second problem
now that I'm going to talk about is not
only overlaying vector data with images
but also maps with images okay now here
we have two problems one problem is that
maps or raster data they are not vector
data so your need you need to basically
align raster with raster the other
problem is that we do not assume that we
have the corner points the coordinates
of the corner points of maps okay so
this is the reason for this is that a
lot of maps that you get from the web
for example you don't know the
ordinance you know generally the area
but you don't know exactly what are the
coordinates of the map so here is the
they are approached basically to do for
this problem we basically start with the
map we we have a lot of I will talk
about that map processing techniques
where we identify the intersection on
the roads on the map and the same
technique that I just described for the
vector data we use that same technique
to find the intersection points on the
image okay so now what you have is bunch
of points here a bunch of points here
you know that this map relates to this
image but you don't know exactly where
okay here is where we basically observe
that there is a pattern to the
intersections of a map that you can kind
of use it as a fingerprint of that map
and by doing the pattern matching
between this set of points and this set
of points we can identify the area of
the map that matches with the image so
let me show you a little bit of a if
you're interested again I can tell you
more about the techniques we use for the
map intersection direction but that's
why itself is a challenge because a lot
of maps are very complicated they have
all sorts of background layers they have
text in the middle of the roads and so
on so we need to identify all these
layers and then isolate a vector layer
and then find the intersections on the
vector layer so that's why itself is
challenging but once we do that then we
use our point pattern-matching algorithm
that basically identify the match of
this set of points on that set of points
and here we are not doing a blind kind
of point pattern matching we do you
exploit the jew space because we know
for example the directions and certain
orientation and we use that information
to reduce our search space to find the
pattern and then again we do the same
approach as before for the rubber
sheeting algorithm we do the
triangulation and the transformation now
the last space which is a little
different from vector data is that we in
order to overlay this one on this one we
basically do what
graphics folks to has texture mapping so
we take that triangle and texture map it
on its corresponding triangle and we can
do it either way so you can either put
use the map as the source or you can use
the image as the source so the result
would be something like this where you
have the map on top of damage you might
say that why this is important I give
you a couple of example applications one
is that for a lot of areas in the world
we have aerial images and we have maps
what we have we don't have anything else
so by doing something like this
basically you label the roads on the
image with the information on the map
the other use for this is for example
for an area here from the map you know
that there is a park so if you just have
the map you know that that's a part if
you want to for example landed
helicopter you want to make sure that
there's an open space in this part and
that's not given to you by the map but
once you put it on top of the image now
you know that here is a part and there
is an open space in this area for
example so the combination of the map
and image give you much more information
at each one of them separately another
example that we did actually for the DC
area was to align metro and bus maps of
DC on the satellite image so here for
example you get the very detailed
information of the relationship between
metro lines and houses for example for
emergency response type application this
is very important and you see that the
metro lines in our case there are all
these jagged lines and curvy lines and
the reason that they usually metro lines
are straight lines so because of our
alignment process we ended up to deform
them in order to match to the roads
ok I think I have time to go through my
last example as well so this last one is
just an example of an integration of
sources with both spatial and temporal
information so here we have three
sources we have a source the public
website that has the information about
different railroad stations and their
their addresses from which we can do
geocoding and get the coordinates we
have railroads information the actual
vector data of the railroads and we have
the schedule of the trains leaving for
example one station arriving the other
station so from these three sources
separately you can ask certain questions
but you can never for example as that I
want to know all the trains that would
be in this area for a given time frame
ok none of these sources by themselves
alone can answer this question but once
once you integrate the three sources
together and that's what we did
basically then you can start asking
questions that basically range queries
on space and time so you identify time
interval and space and you ask for all
the trains for example that would be in
that area and the system now can answer
this sort of question so this is yet
another example of the power of
integrating multiple sources together so
for this demo that i'm going to show you
now we use all these techniques that i
described so we built the models we put
the textures we also conflated both
vector data as well as raster maps the
maps are from the tram routes on the USC
campus that we didn't have them in
vector information and they also did
some temporal data we did tracking of
the trams as that they get up to updated
every 10 seconds and we put that
information on top of the models as well
so just a quick summary of what we did
throughout the past year so the project
started officially kind of like in april
two thousand five our first demo
in 2000 june two thousand five for NSF
was that Washington DC them what I
showed you with the video of the cars
moving and so on and we had a separate
demo that shows the integration of
sources that was the metro map by 2005
for the USA on or 25th anniversary we
did a modeling of the whole USC campus
and integrated everything together as
well as adding the live video live gps
and also a gesture-based user interface
for coolness factor and then i'm going
to show you this one as well where by
May we did another demo for an area in
downtown LA with our own 3d models and
live streams of area of the video in
addition to that we also generated a
Google Earth version of it and i'm going
to show you that one as well this one
only has the art 3d models but there are
very accurate models so this the project
was that they want to this area here
they want to get rid of these two
buildings and get bids for putting a new
park in that area and at the beginning
and this is a portable LA Times so at
the beginning they just want to put some
pictures of the area on the web and what
we suggested is instead of the pictures
put this something like this so that
people can navigate and also in addition
to that put google earth with all
sources of data i'm going to show you
later that the added to it so that
people get much better view of the area
that they want to change so the yellow
area is the area and these are the two
buildings so we did a little bit of
special effect for them to that they can
as you see this one was pretty equity it
even had the train information and some
very tough buildings now let me show you
the USC campus demo
so this is the we started from the
aerial image we did correct the vector
data the techniques that I described for
you so here is the vector data now
exactly on the 3d model at the right
place after correction as you go close
zoom into the model we know that this is
the tram Road so as you see that
original map was actually not correct
spatially but then once we put it on the
roads you see all the curvy lines that
as a result of the conflation as you
zoom in to the buildings here is a query
system that actually goes to our
mediator to information into mediator
and get information about the building
so the name of the buildings are not
hard coded they are actually query to
another source that gets the name of the
buildings and they pop up this is the
video texturing if you come to my lab
and see this demoed my life is actually
in that building and these are live
video camera so if you go from the
window you can see the same view as what
we show during the demo
right now we have much better textures
on this building so the new demo looks
much more real but the models are the
same models just that we redid the
better texturing
here is the gesture base user interface
so you can actually control the system
with gloves kind of a Tom Cruise fashion
and those are the blue things are the
trams so their location get updated so
right now we are working on a query that
hopefully should be ready in two weeks
for the NSF site visit where you ask
questions about the trams and their
trajectories and then see the
corresponding video live on the edge
here and show Google demo to Google
folks and this should be fast here right
that's the server should be
aren't you either one maybe just one
it's not in the building no this is me I
just want to have my own display as well
as much easier to control things okay so
let me first show you the USC campus
so this is now that the same tram map we
align it with the Google Earth so that's
actually what at some point mark
suggested us to do so you basically get
the same tram route information overlaid
on Google Earth as well and now have a
couple of queries that we're working on
right now so one is first show you the
station so these are the four stations
okay the first unit Rams and see if it
works or not okay here we go so this is
now the live location of USC trams so we
have an XML file that get updated and we
read it and generate these KML files
that gets refreshed so you see that the
tram for example moving over there so
this is the real location of us see it
ran right now and one type of query
we're thinking is that if you have the
station's then let's say that you have
station one these are translations in
station to let me turn on a couple of
stations here three and four okay now if
I go and want to know for example here
what is the closest ramp so we kind of
enhanced google so you can ask queries
the result of the query and I don't know
why this actually moved a totally
different area but probably did
something wrong but it's trying to oh I
know we go back to USD campus here we go
so that was the result of the query the
green thingy so I did a little bit of a
too much movement but the green thing is
the closest Tramp to station for at the
time of the query of course things
changed i guess this is not the closest
now this guy is closest so if i do let
me turn it off and do the query a game
oh ma know what just do it here all I
need it to station one so the closest
trying to station one is this guy okay
another query is if we want to just
track all the trams closest to station
four so basically this is now tracking
so it's not only at the time of query
but it basically keep tracking the tram
closes to station for so if at some
point the tram changes it would change
that we can do the same thing for like
station 2 so this is now the closest
ramp to station two all right turn all
these off and go to the downtown area
so now we're going to downtown LA area
for the same area that they were
planning to do the bidding I don't have
time but you guys can go on the KML
files on my website you can go and play
with it but basically we we show the
building's the 3d model of buildings
both our buildings and you google earth
buildings so you can do a comparison if
you turn the Google Earth buildings off
you see our building so for example this
area here you see the details of our
building versus Google Earth building
actually there are certain areas that
Google I think build a model from the
older imagery so there are certain areas
that Google doesn't have a blooming and
we do have a building and here is the
parcel so the red lines that you see are
the align parcel data that we aligned it
with the imagery and if you have you can
click on it and ask questions so for
example here you can say I want to
compute so this gives you all the
information about the parcel if you
click on compute basically send a query
to our mediator that hopefully will come
back at some point and okay here we go
so it comes back with an XML file that
has the actual calculation of the area
of that parcel and there are all sorts
of other queries that you can do on a
parcel so for example you can ask for
the nearby parks so if I say find it
basically tries to find the nearby parks
and there isn't taking time is because
every one of these clicks actually goes
back to our mediator submit a query and
get the result back okay so you know if
you're interested I have lots of KML
files that we did a lot of querying but
just let me finish with this slide
basically shows where we are so right
now this this is the the current state
of the system up there we have three
different user interfaces I already
shown you a V that was the video that I
show you that has video texture map in
google earth I also showed you and the
guy is one that you're working right now
and can do a lot of query and
interaction with the models so all of
these are basically built on top top top
of this thin made a middleware called
Julia where it basically has different
blades for each user interface that we
add but then sends a unified querying to
our spatial databases to our mediator to
our video server and then basically this
would generate new queries to all these
other sources and the result goes back I
think one of the main problem that we
are dealing with which I think it's
similar to the type of things that
Google Earth might be looking at is that
if you look at the problem very
abstractly we have a real world data or
terabytes of it down here and we have
limited pixel space up there and the
question is that how to go from here to
there efficiently and right now there
are two different approaches that for
example most of the web based systems
use this approach where they build their
these each frame at the storage and send
it out or the GIS products which is the
other way around they actually send
everything to the client and the client
decides what to show and what not to
show and we think that there is a
middleware there's a hybrid between
these two so we are this is our main
kind of free
search problem that we are looking into
and this is my flashlight on the
different funding we get from different
places for this project and some of the
prospects okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>