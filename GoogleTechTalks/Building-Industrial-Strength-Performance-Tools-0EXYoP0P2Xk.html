<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Industrial Strength Performance Tools | Coder Coacher - Coaching Coders</title><meta content="Building Industrial Strength Performance Tools - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building Industrial Strength Performance Tools</b></h2><h5 class="post__date">2007-11-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0EXYoP0P2Xk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon welcome to this
afternoon's Tech Talk I'm Brad chana
work on performance tools in the system
infrastructure group and I'm delighted
to introduce Marty Jewett's whose
project lead at Sun on their performance
analyzer before that he was one of the
main contributors or perhaps the
architect even of SGI speech up and
hopefully you'll mention some of the
other fascinating projects you've worked
on anyways Marty welcome to google hi
thank you my name is marty is chris's
brad said i am the project lead for the
sun studio performance tools and I've
been doing performance tools for quite a
long time what I'm going to talk about
today are the issues involved in
building what I call industrial-strength
performance tools that is tools that can
be used by a random user on whatever
application they're working on and they
just kind of need to work well and
easily to use so with that there's an
outline I'll first talk about the
requirements that is how I see the
issues that are important in developing
such tools and then I'll talk about move
on to what the user model would be first
talking about how you compile the code
in order to do measurements how you do
data collection and what the trade-offs
with various technologies are how you
present the data and how you deal with
I'll deal with two particular tricky
bits of presentation one is to show Java
and the other is to show OpenMP and
lastly I'll talk about how you validate
it how do you know that your tool is
giving the right answers so what are the
requirements the main requirement is
that the tool has to be easy to use it
needs a very simple user model we think
it's important to have one tool that is
one interface to learn for all kinds of
performance data whatever you might want
to collect and then it's very important
to present the data in the context of
the users programming model there's the
user wrote something and even if that's
not how things run the user wants to see
the performance in from
nation in terms of what they wrote the
objective of the tools is to make things
as simple as possible but no simpler to
paraphrase Einstein the important thing
is to make sure that the user has to do
minimal effort minimal number of mouse
clicks to get to the aha point the point
at which they slap their head and say
what idiot wrote that it's important
also to have trustworthy data and that
means the data has to be tested and
validated and the tools have to be
bulletproof unlike special-purpose tools
where you can get away with having it
work most of the time if you're selling
a tool or even if you're giving it away
into an audience you need to make sure
that the tools do get it all right so
and you also need to make them
absolutely robust so what's the user
model the user model we're fond of is
pretty simple three steps compile the
target and that means if you're trying
to measure some production code you need
to compile the target just the way you
would for production use optimization
paralyzation it's often uninteresting to
tune a nun optimized program because the
hot spots in the unoptimized code have
no relationship to the hot spots that
will show up in a production system
second step is to collect the data and
that means a command that says I want to
collect this kind of data on this target
run with these arguments and we do that
and we have options to say what kind you
know what are the different kinds of
data you collect and which ones do you
want and we do a lot of consistency
checking to make sure the user doesn't
ask for things that really cannot be
done together the options all specify
what kind of data you want on this
target and the last step is to examine
the data and you can do that either by a
GUI or by a command line there are
certainly some class of people who think
that real programmers don't use gooeys
so we have to have command line but
command line is also useful for other
things and I'll talk about that a bit
later so first of all compiling the
target
it's important we think I think it's
important to avoid having to do some
special compilation for doing
measurement and there are a couple of
reasons for that one reason is that any
special compilation is a barrier to use
some industrial codes like one big
database code whose name I won't mention
takes almost 24 hours to compile so if
you have to decide you want to do a
particular kind of measurement you don't
want to have to spend 24 hours
recompiling it for that measurement and
it's also true that if you do some kind
of special compilation to do
instrumentation you're running a
different program sometimes that program
is close enough that it will point you
at the right places but sometimes it's
not at all sometimes it's really you've
got a different program and again just
like turning off optimization you're
doing a measurement on a different code
from the one you're trying that you care
about there are times however what you
really do have to do something there are
times when some kinds of information
cannot be obtained except by recompiling
and then you do it and we can do that
for some kinds of data we do it in
particular to prepare to get count data
how many times a function was called or
an instruction was executed and we also
can do some not quite performance work
but what I would call correctness work
we can do data race detection and for
that we again need to do a special
compilation it's important to allow all
the optimizations in parallelization
even though they may greatly complicate
your life in terms of interpreting the
data for example if you do common
sub-expression elimination which very
common compiler optimization you can get
some pretty funny behaviors in
particular you can see profile hits on a
line of code that you know is never
actually executed because it had an
expression common with what you are
executing and the compiler simply tagged
that code to generate that with the
wrong line number well wrong in this
context
line number you also need to understand
what happens when the compiler optimizes
loops doing loop fusion loop fishing
when functions are in line when bits of
code are taken and outlined when
functions are cloned as for example you
would do if you have a function that
takes a parameter you might want to have
one piece of code for any parameter and
another piece of code if the parameter
is fixed by the call parallelization
makes life even more complicated in the
Sun compilers and in almost every
compiler I know of parallelization is
done by abstracting the body of the loop
being parallelized into a separate
function son's terms that's called an M
function other compilers called an
outline function and how those functions
appear and how they interact how they
show is being called in the rest of the
program is an important issue and I'll
talk about that in some detail later
another interesting thing that's in the
Sun tools and was also in some compilers
and in the SGI compilers our compiler
commentary the compilers often know an
awful lot about the code they have
compiled and they can tell you that
information it's sometimes really
interesting to know for example this
particular loop was not paralyzed
because the compiler detected a data
dependency on one particular element or
you couldn't parallel eyes this loop
because it couldn't tell whether it was
safe if you have a of B of I equals
something if B of I is what's called a
permutation vector that is you know for
sure that no be of eyes know to be of
eyes have the same value it's safe to
run in parallel if you don't know that
you can't make that assumption so it's
often helpful to have the compiler tell
you stuff sometimes you know enough
about the code to be able to insert an
assertion or a pragma that says it's
okay to do this because I know this is
true I know it I know this construct is
a permutation vector i know this
function is safe to call and
multi-threaded code
so that's all I want to say about how
you go about compiling the code I'll now
talk about the data collection issues
there are two really important issues in
data collection which I refer to as
dilation and distortion dilation is the
Delta in run time between the uninst
romentic unmeasured code and the
instrumented measured code in general
dilation is tolerable it often doesn't
matter if you're doing a performance
measurement and the code slows down for
five percent or ten percent sometimes
even fifty percent it doesn't really
matter as long as you get the
information that tells you what would
happen in the uninst romentic code now
if you're trying to do measurements on
production codes in production fifty
percent dilation can be fatal but in
general dilation is much more tolerable
than distortion again dilation if it
passes the users tolerance for delays is
not acceptable distortion is far more
serious what distortion does is it
changes the relative weight of different
sections of code and therefore may
really produce misleading results if for
example you instrument for function
counts and small functions may have very
high overhead large functions will have
negligible overhead and that difference
may cause you to look in the wrong place
to tune something another important
issue for data collection is scalability
how big a program can you deal with and
canonical programs we talked about might
be 300 megabytes of an executable which
is built with a hundred thousand source
files I would not I would say this is a
very large program but by no means the
largest that one would want to deal with
you also need to deal with high counts
of threads for some applications
especially petascale applications that
are being developed by government labs
thousands of threads may be running and
you need to deal with that many threads
sometimes you have to deal with a
variance in running time for some
programs it's fine to take measurements
on a few tens of seconds for others you
really need to run four hours to
understand
so you have to have some way of scaling
your data collection so it can cope with
either of those ranges another issue in
data collection is aggregation it's you
often do not care about each individual
event that you're recording for
profiling or each function entry but you
need to aggregate it to get a picture
and statistical sampling for example of
what's the aggregate profile over all
those events and you have a choice about
where to do that aggregation you can do
it at collection time which has the
advantage that you have much lower data
volume but you do have higher overhead
and you've lost detail once you've done
the aggregation you cannot disaggregate
in retrospect when you're examining the
data on the other hand in post
collection aggregation you have the
problem that you're generating a large
volume of data perhaps more than you can
handle very easily but you do get the
more detailed information and lower
overhead in collection overhead in
examining the data we think is less
important than overhead and doing the
data collection so another issue with
data collection is who's doing the
collection there are two easy ways to
think about it one is you can do it in
the target itself and that's the method
that I've used primarily to deal with
user codes you can do it either by
compilation options that insert the
instrumentation or better to use some LD
preload mechanism that actually inserts
your data collector into the targets
address space you then collecting it
from the target itself there's lower
overhead and it's it's easier to control
but there are many little tricky end
cases you have to deal with especially
if you want to either manage multiple
processes for example one of the
features in our tools is the ability to
understand when your process is creating
descendants when it Forks when it execs
something else and you may want to trace
all of those like database servers often
fork many processes for their clients
another problem with collecting it from
the target itself is
an interposition for tracing can be
pretty tricky to do it's often very hard
to get exactly the right into position
in just the right place in fact we're
dealing right now with a problem where
several layers of interposition manage
to trigger needing to extend the file
which triggers a call back into our own
code and leads to funny behavior but
we're fixing that other way of
collecting the data is through some
external monitoring process it has
higher overhead but much more control so
you have much you can have better
controls over what you're doing to the
target on the other hand it usually
needs pretty heavy duty colonel support
often very much what you need for a
debugger and the real downside of it is
that the overhead can be really
substantial because you may have to you
may have to intrude on the process in
order for example to unwind its stack
and that may cause a pretty significant
delay there are lots of different
technologies that you can use to get at
the data that you want each has
strengths and weaknesses and it's always
a trade-off as with everything in
programming one technology for
understanding how a program is running
is to use a cycle accurate simulator
this is great you get really good data
another way is to collect with either
compiled in or patched in
instrumentation gives you a way to get
precise execution counts for example you
can also trace interesting events for
example lock contention events or malloc
and free events or and this is my
favorite you can do statistical sampling
statistics really work well and if what
you're interested in is the aggregate
behavior over a significant amount of
time statistics are the best way to get
to it and I'll talk in some detail about
the trade-offs for each of these when
you're doing statistical sampling you
can either sample pcs or as we prefer
you can sample call stacks that gives
you the context information of how you
got to the
spot in your code so running in a
simulator running in a simulator is
really great because it can give you
absolutely accurate behavior on what's
going on in the machine especially for
CPU usage it's less helpful for i 0 or
paging i oh but again depends on what
you're looking for which technology you
want to use the con of using cycle
accurate simulators is that they can
have enormous dilation it can take you
an hour to run five seconds snippet in
the execution of a process it also
depends on how accurate the cycle
accurate simulator really is and most of
them despite what they say aren't quite
as accurate as they might be there are
some corner cases they don't quite
handle right often that's not a problem
but sometimes it is but using a
simulator is terrible in terms of
scalability you need you may need to run
for weeks in order to understand the
performance of a tiny application on us
on a simulator another technology is to
compile in or patch in instrumentation
there's an obvious dependency on the
compiler and that may produce a really
different program depending on what kind
of instrumentation you asked for for
example if you ask for accurate function
counts you no longer can do in lining of
functions because you can't quite get
the account for that right there are
lots if you're looking for line accurate
line information like tea cuff does then
you've got the problem that now you're
inhibiting optimizations that may move
instructions around lines but the
advantage of it is that you can get
whatever you want and a couple one tool
i'm familiar with his pixi which was
done it originally at mips in SGI which
takes an executable and transforms it to
collect counts for instruction execution
and you can also take compiled object it
has compiled object code transformation
and there are many other related
programs that do the same sort of thing
the disadvantage is that actually
patching transformations to do
instrumentation can be extremely
challenging depending on the instruction
set architecture you're dealing with in
the peculiarities of various
instructions it may be very very
difficult to do just the right kind of
transformation and dilation may also be
a problem for pixie in particular
typical dilation was three to seven X
which is pretty high another technology
you can use is tracing which means you
can trace interesting events now in
solaris 10 and later there's a very
powerful mechanism called DTrace that
was developed by the kernel group that
pretty much allows you to trace anything
you want makes it a bit harder to get
kind of generic information because you
have to pick what you want to trace you
can also do interposition for example
the MPI libraries have hooks in them
called PMP I which allow you to pick up
allow you to interpose on any one of the
interesting calls and call your code
first then call the real code then call
your code afterwards and that means you
can do whatever measurements you want in
that interval any kind of tracing can
get you very interesting data you get
memory allocation and de-allocation data
you can get message passing information
from MPI you can get synchronization
excuse me synchronization operations
like malik's and freeze the con is that
it requires a lot of care you have to do
pretty careful interposition to make
sure you're getting it just right and it
also doesn't scale well tracing
technologies in general generate data
for every event that passes through and
the longer you run the more events you
care about the larger the data volume
and the harder it is to process the last
technology I'll talk about is
statistical sampling and this I very
much like the
advantage you can you can sample either
program counters or call stacks the lat
the if you sample only program counters
all you can get is what we refer to as
exclusive metrics metrics spent time
spent in this function or this source
line or this instruction whereas if you
sample call stacks you can get inclusive
time as well the time spent in this
function and everything that it called
you can understand how the time spent in
matrix multiply propagates up to all the
various callers of it so that's actually
my favorite kind of technology one big
advantage of it is that is very scalable
it's inherently scalable because if
you're doing statistical sample and you
can throttle how frequently you do the
sampling depending on how long the run
is how fine a granularity of the data
you want and statistics really do work
many users somehow don't trust something
that's not 100% reproducible and I
cannot tell you how many times in the
last 10 or 15 years I've gotten a
complaint from a user who said I ran
this little program that only runs for
tenth of a second and I did you a
profile on it and I did it twice and I
got two completely different profiles
they don't quite understand that
statistical profiling only makes sense
if you have a large enough space to do
your statistics in fact in the current
release of our tools we will post a
warning if there are too few events so
that we think that the statistics are
not meaningful the disadvantage of
statistics is that you got to be a
little bit careful on how you trigger it
because the worst thing you can do with
statistical sampling is have your
statistical sample trigger in a way
that's correlated with the actual
behavior of the program you do that and
you will get unbelievably distorted and
confusing results there are several
techniques for coping with that you can
either induce jitter in your over in
your profiling interval or you can make
sure that the profiling interval
especially for Hardware counters is such
that it's very unlikely you can
construct a program that has
the same cycle as you're profiling cycle
not impossible but if you profile let's
say on one interval that's close to 10
to the 8th but is a nice prime number
another one that's close to half of 10
to the 8th and is also a nice prime
number it's virtually impossible to
write a program that will get
correlations with both of those another
disadvantage of using statistical
sampling for data collection is that you
may miss rare events but I would argue
that for performance monitoring rare
events don't matter if they don't show
up in the statistics they're not
important enough to be affecting your
overall performance another issue about
data collection is understanding symbols
most of the technologies that are used
for doing instrumentation and
measurement get raw data in the form of
addresses either program counter
addresses or data addresses and you need
to understand how that address relates
back to what the user wrote which
function is this really and that means
that you need to record at the same time
you're recording your profiles you need
to record a time-dependent address space
map one of the corner cases that the
tools have to deal with is running this
application a deal opens a shared object
and it closes it and a deal will open
something else you now have the case
where you may have the same address has
a very different meaning at two
different points in the execution of the
program there are ways to deal with this
we do it by interposing on dl open and
dl clothes we interpose on em map to
understand how data addresses change
there are also other techniques based on
what the kernel can do Suns runtime
linker has something called LD audit
which allows which allows you to put in
an agent that will get reported to
whenever the address space changes
another issue is that to get symbols
right for some kinds of programs you
really need to be informed when the
program does something so for Java we
use the jvm TI interface which will tell
us whenever hotspot generated dynamic
code for us for user methods we also
have an API that anyone can use if they
write code in their data space and then
jump to it they can tell us where this
code is and what we should call it so it
will show up normally in the profile so
after all I really want to say about
data collection and now I'll talk about
the presentation issues as I said before
we think is very important to present
data in the context of the users model
of their code and for some compilers or
languages that can be pretty tricky for
C++ for example different C++ compilers
do different name mangling and you
really need to understand how to how to
do that so you can present the method
the user wrote and not the symbol as it
appears in the symbol table Java has a
problem that first of all it has its own
name man gleam but it also has hotspot
compiled code where sometimes a
particular method is being interpreted
sometimes it's it was compiled by the
java runtime and now it's being executed
in machine mode is that the same
function or is it not we think most
users think it is the same function and
they want to know how much time was
spent in that function another issue is
OpenMP when you do paralyzation as i
said before you can generate these
outline functions or em functions and
they get called in a particularly funny
way often through various layers of the
runtime often in different ways in a
master thread and slaver and I'll talk
about that more detail later I also
think it's very important to have a
graphical user interface and as I said
there are people who think that real
users don't use gooeys but I'm not one
of those people I think that a picture
is worth many many thousands of words
megabytes of words and so getting the
right image can really help you
understand your program
you need to navigate through the data
that is you need to bring up a screen
that shows you kind of the right thing
to look at first and from there you need
to be able to navigate it in the tools
I've developed the first thing that
comes up is usually a function list with
metrics of performance for each function
so I'll see user CPU time or cache
misses against various functions click
on a function I can go to its source I
can look at its disassembly I can see
who called it and who it calls and so
forth another really important thing is
a timeline that is showing the profile
events in a line that goes across time
and doing that allows the user to
exploit the single most powerful pattern
recognition device on earth which is
namely the human mind you can look at a
picture and see instantly a correlation
that it would be extraordinarily
difficult to write an algorithm to pick
up from the raw data however graphical
user interfaces are not everything there
are times that you really want
command-line interfaces you want to be
able to script analysis sometimes and
especially for testing is very good to
have command line access to the data so
you can do automatic test and
verification talk about that later as
well so I'm going to talk in detail
about two particular issues for
presentation java and OpenMP problem
with java is that you might naively
think that the JVM is just some plain
C++ program well yes it is but on the
other hand it's really a pretty peculiar
C++ program for one thing the first
thing that JVM does when it starts is it
actually constructs the Java interpreter
in its own data space one humongous
function through thirty five kilobytes
another thing is that in Java especially
mixed Java you may be bridging between
Java and J&amp;amp;I and native code and back
again and we need to understand how to
undo that operation to reflect the data
back and the users
model and for hotspot compiler a hot
spot comes in it takes a method makes
machine code for it and now suddenly
you're executing different addresses
different places but it's still the same
method in fact I believe in some of the
job the JVM it may dynamically recompile
a particular function many times if it
decides that the compiled version is
still not fast enough it may recompile
it with higher optimization more on
inlining so we need to deal with all of
that another peculiarity of Java is that
there are many threads there are all the
user threads which are visible to the
user but there's a number of what we
call jbm system threads we used to call
them overhead threads but the JVM folks
made us change that so now they're
called JVM system another interesting
property of Java is that there are
really two call stacks that makes sense
if you're doing call stack sampling one
is the Java call stack that is the call
stack that shows which method in which
bytecode index called which other method
and so forth but there's also a native
stack and sometimes you need one
sometimes you need the other in
particular if you're dealing with Java
mixed Java C++ the only way to get a
good user picture of what's going on is
to take the Java call stack and the
native call stack and stitch them
together to put the native the j and i
called stuff in the middle of where it
really belongs in the user's java so
i'll go through a little example a toy
application that i wrote called jason
probe sin probably a synthetic program
and the j is because this is the java
version there's also a see version that
we did earlier in user mode what you
really want to see are the methods
functions that the user wrote sorted by
some metric we have a side panel you
collect select any function in a side
panel will show you everything that we
measured for that method so that you
don't have the problem of having 20
different measurements in 20 different
columns of data that you can't quite
pick out you can
select which ones you want to see in
which ones you don't and which ones you
want to sort by you also want to see
callers and call ease with attribution
of the metrics up and down the stack and
you want to see annotated source you
want to see how much time was spent on
each line of the program you're dealing
with for mixed model java and and C or
C++ you really want to see a seamless
transition although I don't have slides
that show this you run such a code and
you look at Java code that's calling in
to Jay and I you'll step into a method
from the Java into a method from C++ you
can step down the C++ methods then you
can step down when it goes back into
Java and the transition is seamless you
see what's really going on for Java
there are people and frankly I think
these are mostly the JVM developers who
care about this assembly and they want
to see in Java mode they want to see
what the JV java bytecode is that's
being interpreted and how much time was
spent on each of those bike codes for a
machine code you really want to see the
actual machine code and the instructions
and how much time was spent on each of
those so here's a picture of a
screenshot of our current tool in Java
mode so this is user mode it's showing
what we expect the users to see so we
have two metrics user cpu for both of
them but the one that has a little
dangles on it is the inclusive user time
the one that doesn't is the exclusive
user time and you can see different
numbers for them and in this case the
bold face shows that's what I'm sorting
by so you see not surprisingly the top
of the call stack is Jason probe main
because that's where the work is done
and although you can't see it here some
of these methods are Java methods but
some like see funk is in fact a C++ a C
or C++ function another thing i want to
show you here is this line that says no
java call stacks reported and you can
see it's about maybe two percent
or thereabouts-- of well one a half
percent of the total run time in order
to get java call stacks we actually has
have to ask the JVM to do it for us and
in order for the JVM to safely unwind
the call stack it has to be at what it
calls a safe point that is it has to
know that for example it's not in the
midst of moving garbage collection doing
garbage collection or moving memory
around so we made the value decision
that rather than introducing a safe
point at every profile interrupt we we
specify the call that the JVM used that
we use in the JVM to be hey if it's not
safe to unwind it don't just return us
an error and this no job of calls that
recorded represents all the cases where
that happen and as you can see it's only
a couple percent probably not
significant in the total execution of
the run so here's a picture of the
timeline I said we have this timeline
that allows you to pick out patterns and
what you that out of the way and once
you see here our change in pattern in
the behavior of the code now I said this
is a synthetic program and that means
each one of these little different
patterns represent some little bit of
code that we thought was typical of a
typical Java code these really tall
stacks here are recursion one of them is
recursion deeper than we support data
collection for the other one is deep
recursion but not and this one is
something we call bouncing where a calls
because a calls because a equals B and
again this code is written primarily as
a test case but it's also a pretty good
demo for what the code does you see
there's a line here and although you
can't tell this bar is whiter not gray
that the crosshatch between those two
represents the specific event collected
and when you select an event
this is what you see for it it shows you
what the event was in this case it was
clock profile tick tells you the
timestamp the timestamp how much time it
represents what state it was in the
Solaris keeps track of what it calls
microstate accounting so we distinguish
user CPU from system CPU from page for
weight or CPU weight but we get profile
ticks with all of those counts and
lastly it shows the call stack at what
happened at that particular place in
this case the Java users Java is single
threaded even though the JVM when it's
running has I think a dozen threads we
also support another way of looking at
it called machine mode in which we
actually record the data at we actually
display the data as shown this is really
the implementation model and that means
when the JVM is interpreting code we see
the JVM interpreter we're basically only
looking at native stacks for all threads
and we show all threads so even the Java
threads show the show a native stack and
that means that for the most part you
see interpreter timeline shows all
threads as well both the user's threads
the garbage collector thread war threads
depending on which garbage collector
you're using the hot spot compiler
thread and a bunch of other threads some
of which I don't even understand what
they do I think there's the one that's
their only to deal with when the process
terminates so here's the same timeline
on the same experiment but now we're
looking in machine mode so the first
thing to notice is this top thread which
is the users thread now looks
qualitatively quite different and that's
because what you're seeing here are
methods in the interpreter a couple of
other threads to point out one is thread
2 which is the garbage collector thread
and another is this thread 7 which is
the hot spot compiler thread this shows
you what really is happening but it
doesn't relate very well to what the
user expects
to see so we implemented a third mode
what we call expert user mode it's a
compromise between user mode that only
shows the stuff the user wrote and the
Machine mode which shows far more detail
than the users want to do what we do is
for Java threads that is users Java
threads we actually show the Java call
stack for non Java threads like the
garbage collector and so forth we show
the native stack where it makes sense
and the timeline shows all those stacks
and that can be very useful I'll show
you why in a moment we also separate the
consequence of doing this is that we now
instead of seeing JVM system for all of
the overhead of the garbage collector
and the compiler and so forth will now
see those individual methods so the user
can understand what's happening and the
timeline shows all threads and that
turns out to be very useful so here's an
example of that same experiment except
now it's in expert mode so you'll notice
that if the notice this line now looks
pretty much like it did before it's
different at the very beginning which is
actually the initialization of the JVM
which is before it gets into user mode
because you're now seeing native stacks
here before the thread becomes a java
thread but after that what you're seeing
is just like you saw in the Java thing
but now you're also seeing these other
but these other threads will show you
what's happening in the code in
particular let me look at Java to thread
2 which is where I've collected which is
what I've selected an event in
particular it's right in here where it's
not this an interesting pattern here but
there's actually something going on and
you can see from the call stack
something is really going on and you can
guess from the names that this is the
garbage collector what it means is that
now whenever you see a spike of activity
in the garbage collector thread you can
move up to the native thread and see
what's happening I don't have slides for
that because this talk isn't more on how
to do tools and how to use our tool but
what you could do for example here
select this region where there's a lot
of garbage collecting going on move up
to the next level on what you'll
actually see in this case is the the
user Java code is allocating a huge
array inside a loop and it just keeps
overriding the same array which means
it's creating garbage as fast as its
little legs can carry on so I think
that's all I want to say about Java oh
I'll talk about how we do the
reconciliation in order to get this
right we need support from the JVM we
use JVM TI we can also support older
jvms that have an older interface called
jvm p I and that allows us to get thread
identification hotspot compilation and
deletion of methods and we have a method
that we that we persuaded the JVM folks
to implement to give us a a sync signal
safe unwind of the Java call stack I
could not persuade them to put it in the
JVM TI spec however so it's just there
and some people outside have stumbled
upon that and realize what it must do
and have exploited it excuse me hey I
think it's the right thing to do i'm
glad other people are using it we can we
also ask the JVM and they agreed to do
for us to use our api to describe where
the interpreter is in in space where
they put various bits of bridge code
between java and native and so forth so
we can get a pretty good idea as to
what's really in the address space the
presentation changes we did are based on
whether or not we have thread IDs that
tells us whether this is a java thread
or not and whether we have java call
stacks or not so i think that is the end
of java so i've talked about the issues
in compiling the code the issues in
collecting the data and presentation for
java now i'll talk about presentation
for another programming model that has a
amount of complexity in it specifically
OpenMP OpenMP is a scheme for doing
paralyzation and co's you can say
parallel do parallel regions and various
other constructs within it and the
specification for OpenMP is written only
in terms of the user model problem is
that nobody implements OpenMP that way
openmp is always implemented with a
whole lot of runtime magic and the
actual execution behavior isn't much
like what the user thinks is going on
I'll get into that a bit later so
presentation you certainly want to show
the user what they expect in the user
model but sometimes you really want to
see what is actually going on in the
runtime but the actual data is recorded
it in the implementation model not in
the user's model but the tools are smart
enough to undo that so I'll go through
an example of two level nested
parallelism in an open MP so what are
the characteristics of the user model
first of all when when you have a serial
code that goes into a parallel region
it's what's called a fork join model it
Forks and now you have n threads
executing in parallel when the parallel
region is finished it joins up and unit
back to only one thread and in the user
model all the threads are the same once
you're in a parallel region the master
and the slave threads have no
distinction between them however the
data is X is X is collected in the
implementation model and then the
implementation model the execution
really takes place in this outline
functional m function that represents
the body of a parallel loop for example
and the M function as called in the
native model is different between the
master and the slave most
implementations do have outlined
functions they have different naming
conventions depending on who wrote the
compiler we're perfectly good at
compiling code generated by our compiler
about understanding code convince jen
erated from Arkham
less good about other compilers and
there's another interesting issue which
is CPU time is not necessarily the same
as work time in particular and OpenMP
you can say I can either do a sleep
weight or a busy way you want to do a
busy wait if you really don't have any
use for the CPUs otherwise and you want
to be very responsive in waking up when
suddenly you go parallel again and you
have use for these other threads so you
can park them in a busy weight which is
spinning as fast as it can to look to
see if any new workers come in there's
also a sleep wave which doesn't use cpu
time it goes to sleep and then when work
comes in it relies on something to wake
up that thread but to the user both of
those are weights and the user doesn't
really care whether you're doing user
CPU time or sleep time but it means that
users CPU time will be much higher if
you're doing busy way so to fix that we
created another metric called OMP work
and that does never happens when you're
waiting for something whether you're
busy waiting or not waiting you're in
OMP wait state if you are executing in
the user code or in single-threaded you
are in OMP work state so here's the
timeline as a user would expect to see
it you'll notice the first thread the
initial thread comes along it then goes
parallel and you'll notice that all of
these threads look the same they all
have the same call stack you can tell
from the shape the leaf pc will be
different of course because they're
executing asynchronously but in general
they'll be in the same function and this
is in fact what nested parallel parallel
ilysm looks in that model so you see
start which is where all programs start
calls main which calls work which then
calls this outline function this is its
real name but this is how we translate
it that then calls a function called
compute some but compute son has
embedded within it another
level of parallel so what you're seeing
is the outer nest and the Internet's of
paralyzation in this code in the
implementation model of threads look
really different because the master
master shows that it was started from
start calling main and so forth the
first level slave which I call the slave
master shows a descent from lwp start
which is where threads are created and
it will show both levels of nesting
however the the slave slave that is the
second level slave caused by the nested
parallelism only shows descent from lwp
start directly into the EM function so
this is what the stacks look like here's
the master thread here are the it goes
parallel with four threads and here are
the four threads you'll notice the three
slaves look one way the master looks
different and here and there are more
below this are the slave slave threads
the ones that are created for the
internets in particular here are the
call stacks for the three different
kinds the master master shows you
starting from stick from main and now
this differs from the user model in that
you suddenly see things like empty
master function run job invoke em funk
empty master etc this represents the
openmp runtime which we abstract away
from the user model because they don't
really care in the slave master that is
a slave thread that is a master of a
region itself you see it's starting up
it's immediately in the M function calls
another function and now it looks like
the master slave thread starts at lwp
start and finds itself immediately in
the inner nest so these are the
different call stacks that we really
measure but we transform it so that it
matches the user model and we do various
bits of magic for it first thing that we
do is that when a thread Forks that is
when I don't mean fork and UNIX fork but
when it goes into a parallel region
create slave threads
we track the Masters call stack at fork
events we actually won't do that anymore
and I'll explain why in a second we
assign each parallel region a unique ID
and we unwind the stack and save the top
part of the call set the park is above
the openmp runtime and we associate that
with each region ID when we get a
profile of hand we capture the car and
call stacked but we also ask the openmp
library what's your current region ID
and the unwind we do is this is at the
bottom of the call stack below the
openmp runtime and when we do the
analysis we actually stitch these two
together we do the reconciliation so
that we take the top part of the master
stack stitch it with the bottom of the
slave stack and that represents the call
stack in the user model and we do
exactly the same reconciliation for a
fork event that's inside a parallel
region we take its master call stack we
stitch it together with the master call
stack of where it was created to get the
user model call stack and we also add
pseudo functions when we're not in the
openmp work state that is when the code
is waiting at a barrier or waiting for
something else we show it as being in a
different function now I said before I
said we capture the call stack at fork
event and that was true when I first
wrote this talk however we discovered a
significant performance problem in doing
that many many people and many libraries
use OpenMP parallelism four very tiny
parallel regions I think in general this
is a mistake because you have all the
parallel overhead of going parallel and
you're not gaining much because it's
such a tiny region but the serious
performance consequence for us came here
we wound up spending an enormous amount
of time unwinding master stacks at a
fork event and recording the data but it
turned out when we do the post analysis
we never saw a profile event there
to it who cares why are we doing this
and the trick to fix this which we've
just recently put into our code is to
reckon recognize that after the fork
event finishes at the joint event when
you're about to return back to the
original single threaded code you're in
the same place in the user code as you
were when you do the fork so what we do
now is we still generate a region ID we
do not unwind the stack but we
associated with these two region ID a
bit that says do we care about this or
not and every time we get a profile
event that asks for a region ID we mark
that bit saying we care no such advanced
camera came in the region ID is shown as
not interesting at the join point we
simply return and ignore it if we do get
the bit that says you use this then we
record the event and this reduced our
data volume for many cases by more than
an order of magnitude and changed our
dilation for these pathological cases
where the code has tiny parallel regions
from a factor of 5 or 6 to the normal
five or six percent so I've talked about
what kinds of measurements to do how you
prepare your code what the data
collection technologies are what the
presentation technologies are now i'll
talk about validation it's really
important if you're doing performance
tools to make sure you get the right
answer one wild goose chase that you've
sent the user off on means they'll never
look at your tool again they say I
wasted three days because it told me the
wrong thing I won't believe anything it
says so you got to get that right and
the first law of programming is if it
ain't tested it don't work and so you've
got to be really careful to make sure
you do testing what I believe is the
right thing to do is to do automated
nightly testing at Sun and at SGI we
built our tools every night we run a
very large test suite test has several
hundred tests and we run it on it at the
point some 35 different machines we have
different machines for different chip
types because hardware counter profiling
is different on different machines we
have all too many different flavors of
Linux because every Linux system is
different and as it turned out every
time we were asked to support a
different flavor of linux we had to make
source code changes so far and I'm
crossing my fingers we have one source
space that works for all the different
Linux's but it's been quite an adventure
I'm not a real fan of Linux for that
reason anyway we do different tests for
different os's different compilers
different chips optimizations and
different compilers are different and we
have to deal with all of those and we
want to make sure that they work so for
some kinds of data you know the
measurements are exact if you're doing
function counts you can be pretty count
confidence you've got the exact count
then you run it again you'll get the
same count turns out that that's not
quite true for example and again this is
this is a war story I did profiling I
couldn't understand why instruction
counts for F printf we're different well
it turns out it executes differently
depending on the string it's trying to
convert and in particular for dates and
time stamps we got different counts
different execution counts and there are
other exceptions like this for exact
measurements it's pretty easy to
validate by comparing to a gold file you
know what the answer is supposed to be
make sure you get the same answer but
you have to have done the step of
validating the gold file the first time
making sure that what it does make sense
another kind of validation that's
important is regenerating the output
from pre recorded data we record
experiments and we run the same script
to look at all the reports from it every
night and make sure by God we got
exactly the same answer from today's run
as we did from yesterday's run there's a
bit of work to get that right because
when you fix about when you change a
behavior because you've decided to
present something in a bit more graceful
fashion you have to change the gold file
but that's pretty straightforward more
difficult problem is validation for
statistical sampling we're now your
results are not precisely reproducible
and we do that by gender by using
tailored targets we build programs like
the sin program Jason forgot about that
do their own measurements at runtime and
generate what we refer to as an
accounting file so they say how much
total time how much CPU time was spent
in each method they write this out in
the file and mind you this takes quite a
lot of engineering effort to develop so
we then run the experiment we look at
the accounting file we look at the
output from the tool and we write a Perl
script that compares the data and make
sure that it's close enough you have to
have error margins because these are
statistical and we say for some kinds of
data five percent and 500 milliseconds
is an anomaly that's so large it's a
problem for other kinds of data we
restrict it only two percent and 200
milliseconds another issue about
validation is bulletproofing and that
means you got it to construct test cases
for all the corner cases you can think
of all the strange things that you've
ever seen a program to do you need to
get a test case for and also whenever
you find a bug and fix it you in general
want to incorporate a test case for that
bug in the test suite and that's mostly
what we do so I've talked about what I
think are the issues in developing
industrial-strength performance tools
what how you measure them how you
present the data how you validate the
answers you get and I'll be happy to
take any questions I think we have time
for a couple questions if anybody has
anything in particular yeah what do you
do we have a lot more when you have a
lot more data than will fit on the
screen like say you have a thousand
pixels horizontally but you have a
million or ten million statistical
points you can't show all those in your
timeline or what if you have a thousand
threads ain't really show maybe 10 don't
do a good job with the threads but for
events we do you'll notice there's a
little black line here that shows that's
present only when you have multiple
events that map to the same pixel that's
actually I think a pair of pixels cuz
you can't see a pattern on one pixel and
in order to see what's going on you can
zoom in let's see this these set of
buttons zoom in zoom out we'll go back
to full screen and you can also use this
to navigate among events you can move
left or right or up and down so you can
move from here you can either move to
this bar or from here you can move this
bar or that bar or you can move left and
right and if you zoom in eventually you
can zoom in far enough that every
individual profile event will show up
and the black line at the bottom will
disappear and what happens vertically if
your call stack is 100 frames looks like
you can only display it's a usercontrol
how many you want to see but however
many are visible you were always if you
select an event will always show you the
full call stack so you can control how
many frames you want to see you can also
control whether you want to see it from
the leaf or the route that is you want
to start from Maine and go up where you
want to start from leaf and go down
however many frames you want to see I
think there I think there are other
techniques we can use to do better job
on this but we just haven't had the
resources to do that
and for threading that's of course a
more serious problem we can selectively
decide which threads the show and which
says not to show and if you have
multiple kinds of data for each kind of
data you'll see a different bar here so
if we had a hardware counter for process
one thread one we'd have clock profiling
we'd also have one for Hardware counters
and you can select which one's of those
you want to do what we're expecting to
do next is to allow the user more
freedom to rearrange which ones we want
to see ya so this measures only the
program that's running if you had a
bunch of other programs running on the
computer and you get swapped out or
something you don't see that effect in
this well I can't tell what uh when we
by default we only show user inclusive
and exclusive CPU time however we do
record as i said on solaris we have
microstate accounting so we will record
a packet that says waiting for cpu and
you can their buttons in the interface
here this one is the data presentation
button and all these guys have tool tips
so you can figure out what they are you
can turn on how much weight CPU and see
which functions were spent waiting for
CPU we we've found that there's actually
surprisingly little distortion when
you're recording data and other
processes are running most of our tests
pass anyway while it has passed simply
because the target and the measurement
of seeing the same kind of disruptions
due to other processes so we still get a
match and that's what matters I'm
wondering how you deal with multiple say
multiple runs like in working with tools
like this it's great to do before and
after benchmark or before and after an
optimization is does your tool have
anything built into deal ha ha last know
it's one of our oldest or FES with how
to do comparison comparison of the same
and multiple runs is relatively
straightforward but when you talk about
before and after it gets a whole lot
trickier what does it mean how many
times is this function called if in one
run it's been in line and the other one
it hasn't how much time we spend in a
function when you've suddenly move when
you transform the code so you've moved a
lot of the work that it's doing into
some other function how do you do that
comparison and it's a very hard problem
we are hoping to address it but we don't
quite have the resources it will take to
do it in the near term yeah do you do
you have any methods in this to
correlate unseemly unrelated events such
as cache misses at the same time as you
have an eye load I load byte code Oh
what kind of course I don't java a
bytecode that's like load an integer
onto the stack um just for things like
this the correlation between cache miss
that you just don't see until you put it
on a timeline in one direction oh well
we see cache misses because we can
profile based on Hardware counters right
and if you profile based on let's say
the D any cache miss counter you will
see all the hits will show up on those
functions and instructions that are
having the cache misses fact the spark
chips have a really great counter called
cash recirculate counters a set of them
and this represents not the number of
times you missed in cash but the number
of cycles you stalled waiting for your
cash request to come in and that's
really nice because it reports directly
in time and one of the things that we
can do although in the current tools it
takes two experiments is you can profile
based on cycles d any cache misses eat d
cache misses I cache misses and CPU time
you profile on all of those you can
actually see a function and see
difference between user CPU time in
cycle time turns out to be time when the
colonel thinks you're in user mode and
the chip thing student system mode which
turns out to be mostly tlv misses
so you see that right away you can see
that cycle time is time executing but
the DD cast all time is time spent
waiting for the DNA cash subtract that
out you know what's going on you see the
D cash subtracted from dnd cash and you
see the e cash time so you really can
see each component in the memory
subsystem and what its cost to her one
last question perhaps so I'm sure lots
of sons customers are building clusters
these days a very popular way to go most
of what you talked about though has been
more oriented towards debugging a single
node application do you have any
favorite tools or techniques from what
you've seen for for distributed system
performance analysis well most of the
clusters we see in our experience in
high performance computing at least do
MPI communications our tool supports MPI
profiling by tracing the MPI API calls
although it does i would say a moderate
to mediocre job and dealing with it
we're putting more effort into getting
that right there are a lot of issues
involved clusters especially if you want
to see correlations between what's
happening on two different nodes got to
make sure you're synchronizing the
clocks or know what the difference
between the two clocks are in order to
see that so you can at least preserve
the before after issue you know we
started this project was actually
started in 15 years ago and in the last
it's only been the last several years
that clusters have been become important
to our business so sign is now putting
the effort into understanding that
better but I understand that where you
folks clusters are perhaps a bit more
important you don't have a million CPUs
single address based system do yeah well
uh thanks very much thank you thanks for
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>