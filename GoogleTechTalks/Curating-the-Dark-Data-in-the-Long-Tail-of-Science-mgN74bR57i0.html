<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Curating the Dark Data in the Long Tail of Science | Coder Coacher - Coaching Coders</title><meta content="Curating the Dark Data in the Long Tail of Science - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Curating the Dark Data in the Long Tail of Science</b></h2><h5 class="post__date">2008-09-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mgN74bR57i0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Rebecca Shapley and I am a
user experience researcher here at
Google so mostly I work with the ads ads
quality group on that sort of stuff but
in my spare time my twenty percent time
I've had a hobby of working in
biodiversity informatics which is how I
know Brian and I'm delighted to
introduce Brian heidorn today so Brian
is from from the NSF right now he's a
program manager there and in charge of
the cyber-enabled discovery and
innovation program and the advances in
biological informatics program where he
does a lot of stuff with receipts a lot
of proposals for genetic information
databases and other sorts of things and
he's also part of the assembling the
tree of life initiative and working on
new programs such as so he's part of the
data working group in the virtual
organizations working group before the
NSF stole him he is a was at the
University of Illinois and there he
helped to develop the data curation
concentration for the master's in
library and information science and the
new masters in biological informatics at
the University of Illinois and his
research covers machine learning for
metadata and NLP and I'm delighted to
have him here to present about the
curating the dark data in the long tail
of science thank you for that
introduction so as my job at National
Science Foundation one of the things I
do is working on the data working group
and inside of that group we sort of face
in and face out we face out in creating
new programs to generate research
associated with data and there are some
data programs such as data net which
I'll mention again during the talk to
get the research community to figure out
solutions for data management themselves
but also we do though looking in issue
about data so what should our data
policies be so when we when we do set up
new solicitations what should the
Armaan's be for science about what they
should do with their data and part of
the talk that i'll be presenting today
is the presentations i did with in the
biology directorate to try to push that
agenda along in particular directions
that I'm interested in so this talk sort
of evolved out of that the policy issues
within NSF and how we're going to deal
with data long-term and that's certainly
not solved and the things that I present
here are my opinions and not necessarily
those of the US federal government or
the National Science Foundation but
nonetheless they are some of my thinking
within the context of National Science
Foundation yes how interdisciplinary is
this program well the data working group
is across NSF so it's every division
every different every science discipline
covered by a National Science Foundation
and there's certainly not agreement
about what to do about long-term data
policy across NSF different Sciences
have different interests interest which
are valid you know it's very difficult
to come up with one policy about what to
do and also it's just tradition as well
within the science within the sub
sciences in NSF so it's a political
event to try to get a shift in policy
within NSF is there a given on the
proper on the problem yes there is I
think general agreement on the problem
size very supportive yes yes
yeah so first I thought I should define
data curation a little bit data curation
is not data preservation a lot of people
hear the word curation and think
preservation I mean the acquisition
management appraisal you know whether
you're going to keep it or not maybe
throw it away and then serving of the
data to maximize its usefulness so so
what we would like to do is make data
management longer term than it is right
now and useful longer term we don't just
want to save it we want to make it
useful over that term and other people
have thought about that before there's a
data curation movement or digital
curation movement in Europe associated
with the east science initiatives in
Europe and in some ways on digital
curation there ahead of us in the
sciences at least they have centers and
things that have been running for a
while to deal with the problem what I'm
going to talk about is a particular
subset of that problem which is dark
data and some of you prolly thought
about the dark web as an analogy but
generally it's data that we know that
should be out there or at least it used
to be out there but we can't see it and
the example I have here is a Hubble
telescope looking at a ring of dark
matter so we can tell it's that the dark
matter should be there by the darkness
by what we don't see we know what's
missing and I would argue that there's a
great deal of science data that we're
losing or missing that don't show up
inside of publications and so how do we
deal with that so that raises a set of
questions so how much dark data is there
what's the origin of dark data because
that'll help get us an answer on on
where it is right now what impact is
dark data have on the universe
how do we collect it how do we organize
and service so one thing I've
hypothesized and there's some evidence
for it and it's much of the premise of
this talk is that a disproportionate
amount of the dark data is in the long
tail of science and I'll explain a
little bit what that is I think this
group being somewhat business oriented
probably knows about the long tail long
tail economics so I'm borrowing some
ideas from longtail economics Chris
Anderson and wired while ago and it
wrote an article about the law economics
so the long tail and for the those of
you that don't know about it the good
example is netflix versus blockbuster
and i'll get back to science data this
is it's a direct analogy to science data
it's just like netflix blockbuster sold
or rented blockbusters so by definition
what they were interested in renting
before the internet was those items
which many many people wanted to see and
then they even made their name that
blockbuster because the assumption was
that that those the money was to be had
in the blockbusters and they had
economic constraints on the delivery of
the the titles that they had they had
have floor space inside of a town and
they couldn't afford to have things that
were not blockbuster using up floor
space in their store so it was a wise
economic solution to only stock
blockbusters Netflix came along to do
rentals over the Internet and the first
reaction a blockbuster was well no one's
going to wait for for the US mail
service to deliver their dvds when they
can just go into the store and get it so
we're not going to worry about Netflix
well it turned out and Netflix started
making a lot of money and they were
making money not just on the head on the
blockbusters the very popular items but
on the tail which is the many thousands
of titles that only a few hundred people
in every city one
but you couldn't afford to have floor
space but you don't care about floor
space anymore because you have you can
keep your inventories differently so
that led to a change in the economics
and profits and blockbuster eventually
went online when it occurred to them
what they had missed so what I would
argue is that science initiatives fall
into those categories as well and that
largely what we've been handling is the
head of the curve so the very large
science projects have data management
plans the satellites instruments that
cost many millions of dollars or
billions in case of colliders so create
we worry about what's going to happen to
the data a lot of people are sharing it
and we have a good plan and we we deal
with it pretty effectively but what we
don't deal with an example in biology
would be GenBank we know that we have
that kind of data it's be generated all
over the world we organize it take care
of it somewhat well through GenBank but
we don't deal with is Joe's lab that's
generating data and while there might
not be as many people interested it's
not a blockbuster the data coming from
Joe's lab there are many Joe's labs in
the world and if there's just ten
scientists that want the data from Joe's
lab that's a lot of traffic so what
we're talking about sort of is I'd like
to think about big science versus new
science and the new science the
interesting science is happening in the
tail the really innovative stuff is out
in the tail of science we have to define
what the tail is so this is a power-law
distribution so the kind of curve that
I'm talking about here is that one the
question is the science follow that
curve so what we really want to do is
measure the amount of data being
generated by different science projects
and then plot them on a curve that's
really hard to do and I couldn't figure
out how to do it quickly we know that we
have science projects like GenBank and
pdb protein data bank and others that
appear on the left hand side you'll see
most of my examples
biology because some in the biology
directory but and do biodiversity
informatics so we know we can count
those without too much trouble but the
tail we don't know about we're really
not tracking we don't know how big it is
so I don't know how much data is out
there but we can make another assumption
we can say that one dollar will generate
us XX bites we don't know how many bytes
but it doesn't matter the shape of the
curve will be the same based on that and
i can analyze the dollars so if we look
at the whoops that was a slow slide for
the last full year of funding i plotted
this is most of the grants that were
handed out from NSF and sorted them by
their size of the grant so you have a
couple of grants around seven million
dollars and then you see a quick drop
off in the size of the grants and then a
lot very long tail as you might expect
with many many grants out there and the
question is not how high is the curve
but what's the area under the different
areas the area under different sections
of the curve so if we do the 8020 rule
and just like how big is the first
twenty percent in the head versus the
eighty percent after the head these are
the kind of numbers that we get and the
bottom eighty percent is about equal in
size to the head for the amount of
dollars that are being spent on that
lower part of the curve so that means
that we're spending money on the lower
part of the curve the US government
spending money on the lower part of the
curve on grants of three hundred fifty
thousand dollars or less which are
modest grants but for the most part we
don't have a unified data management
plan for what to do about it and not
only that even if we did have a plan we
don't have a technical mechanism to deal
with it so we don't want to actually
know what to do with all those tens of
thousands of projects that are running
all the time generating data in
relatively small chunks but they're very
important chunks
I wasn't sure of my number so i went
back and replicated for 2005 so this is
2005 and I'm preparing a paper on that
so I'm doing analysis over three years
so I stuck this in on the train on the
way here so you see it's very similar
curve in 2005 now there are some related
ideas we have here that that I overlap
with or that i'm not talking about and i
wanted to point them out john porter
talks about deep vs wide databases and I
think that is correlated with it the
long tail phenomena deep databases tend
to appear on the head they tend to be
uniform generated by instruments and a
bunch of other properties I'll talk
about later why databases tend to be
much more heterogeneous a lot of
different topics being covered in them
and the properties and we'll see that
we're talking about different properties
when we talk about the head in the tail
and therefore we need technical and
social solutions for what to do with
those datasets Swanson talks about
undiscovered public knowledge he's
really talk about inference in text
although I am I am also talking about
undiscovered public knowledge in my case
that science is generating a lot of data
a lot of it is public data but we don't
have access to it but my definition is a
little different than his we also have a
definition of dark data that refers to
failed science so if your science
project doesn't come out quite the way
you expected your hypothesis proves to
be wrong it's very difficult to get it
published in a journal so you know
hardly ever get the negative results
published in journals and I am talking
about that but that's just a small
subset I'm also talking about the data
behind successful science or the raw
data where it's an observation and it's
neither neither failed North successful
it's just data it's just information
about a phenomenon in the world yes
Yeah right there there have been some
efforts so I mean that's important work
to try to get at the problem of failed
science data being lost the dark data
that's failed science because a negative
finding if you don't record the negative
findings then you're doomed to repeat
your failures so it is important to
record them well it's not dealing with
all of the data certainly and not all
the failed science data it's still very
difficult for scientists to get
published the experiments that didn't
work out I have a question yes I my
training is mostly as a computer
scientist but I recently wrote a paper
and economics and looked for an
economics journal submitted to and was
surprised to see that they require you
to submit your data along with your
publication which is essentially unheard
of in computer science there's a little
bit of a problem for me because I had
about 20 terabytes of data the problem
put it on a CD and mail it but I'm
curious across the different disciplines
how common that is that when you submit
something for publication you also
submit your data because that kind of
took me by surprise and that appears to
be you know it doesn't address this
failed science issue because if it
doesn't get published the data wouldn't
get published as well but those data
typically get published along with the
paper itself in different fields yes
that is happening more often than it
used to and in the sciences as well so
some of the science publishers are
beginning to agree to publish data
behind the publications and that and
that's good and it's a moving in the
right direction but one of the problems
is the very problem you pointed out is
the date well two sides of it they only
want to publish the data that's directly
related to the information that's
presented in the journal article so if
you have a chart in the journal article
they want the data that's behind the
chart and that's good but usually that's
very abstracted and since
data at the point of your presentation
in the journal article so it's not raw
data and the other thing is as soon as
they opened up that door they realized
the mistake they had made or the problem
that they were opening up for themselves
is the size issue that when they were
thinking that it was just going to be
the chart you know the data behind a
particular chart everything was fine but
it turns out that if you're doing
climate modeling you know there are many
terabytes of data behind it and the
software that was behind generating the
model and all of the other elements
which is way beyond the scope of the
mission of the publication so while it
does help some it doesn't address all of
the problem and we should keep moving in
that direction because I have too many
times taking charts and tried to
recreate the numbers to put in to see if
I could replicate the research and yeah
that's obviously a waste that answer the
question the science Commons is one of
the potential answers and they do bring
up the differences between big science
and small science and I'll actually
argue somewhat that's small science is a
misnomer just because it's a small
research project doesn't mean it's a
small research question a lot of
ecological research projects are
relatively small a couple of scientists
walking around in the field measuring
how big trees are but the questions are
large you know what's the impact of
climate change on the production of
lumber in the northwest or something and
for that what you want to do is take the
many small data sets and aggregate them
and merge them and answer big science
questions but using small science data
and then they're certainly as an analogy
to the dark web but again dark data is
is different from the dark web the dark
web generally it's information that is
in electronic form but there's some
barrier to getting it out and getting it
indexed on the World Wide Web but it is
in a database or some some other place
not all of the data that we're talking
about is or was in digital format
or not on the internet not in
instruments that would have done it new
data is generally digital at some point
in its life but it may have never been
put online and there's data that was
never digital even now there's data that
was never digital so again it's sort of
a superset subset relationship so why is
the tail important you know I'm going to
argue that we don't do paid as much
attention to the head heads important
but we should pay attention to the tail
there's valuable science going on in the
tail I certainly hope so because half of
the National Science Foundation's money
is going into the tail and I would guess
I would like to see NIH and other other
agencies as well and probably businesses
probably have similar distributions so
we had to do some research see if that's
true but given that we have that
expenditure it better be worth something
I hope that many scientists could use
the data in the tail I don't have a
whole lot of evidence except that we
know that scientists spend a lot of
their time trying to find data organized
data synthesize data so for example in
molecular biology when a scientist wants
to study a particular gene they have to
go to the databases even the databases
that are online and have information and
search many many databases many separate
sources and to have a very long research
phase trying to get information that is
in theory online and not of the not all
the information is online yes is that is
that potentially a problem that the data
in the tail is patulous formalized yes
and therefore harder to really reuse by
other people that's right yeah I think
it's it tends to be less formalize and
I'll have a list of characteristics
coming up and that's one or there are
many formalization so there may be many
subsets of science which are all
following somewhat the same format but
there are many small projects and
they've never been unified so even
though the practice could be unified
they
so both phenomenon so part of the
argument for it's a value this is an
example of historical data that just
came out in a publication last year
looking at there are followers of
Thoreau that walked around New England
and doing like Thoreau watching nature
including the bloom taught first bloom
time for plants so they walk around see
when something blooms recorded in their
diary so they have a nature diary also
at the same period of time photography
was coming around and developing and
becoming available so there are pictures
of flowers being made in natural
settings and they're being taken since
they're in natural settings when the
flower bloomed so those became become
data points for flower blooming but the
research question we have now is what's
the impact of global warming on the
bloom time of plants and is that and we
want to look at some plants before
carbon was at the level that what it is
now and other information so where do we
get it well it turns out it's embedded
in these photographs and in these
Diaries of people scattered around so
those are examples of what would be what
start data not available but these
scientists went out gathered it up they
also had a meteorological station
collecting temperatures for the time
period from 1888 to well a little later
9 19 23 or something like that and then
did the regression lines to show the
relationship between temperature
variation across years and bloom time
and indeed there is as you might guess
there is a relationship and that is an
impact on things like pollinators so
when do the bees actually be around or
one of the hummingbirds around and
when's to flower blooming and if they
don't match that's bad because you won't
get fruiting so this is just one example
of those that kind of thing I would
argue that science innovation happens in
the tail
that in big science we generally know
what we're looking for you know we were
looking for the cork or whatever the
next particle is we have a good theory
about what it is and we spend a lot of
money to generate a Collider to look for
that particular fact it's very important
fact but we know exactly what we're
looking for but in the tail what you
have is many thousands of scientists all
innovating and coming up with new ideas
the projects are small because no one's
confident yet enough to spend a billion
dollars on following up on the concept
but that means it's a very rich ground
for new ideas and at the moment we're
losing some of those new ideas or at
least the data behind them unpublished
negative results is a subset we actually
know very little about the tail I'm
making arguments that it's important but
we really don't know much about it I
measured how big it is by the amount of
dollars we spent in 2007 but we don't
know a lot I have transformative science
here because National Science Foundation
has been told by the National Science
Board to be transformative we're
supposed to spend spend money on things
that that make very important
transformations in the way we view
science and reality and not incremental
science so I'm picking up those words
and saying okay we should pay attention
to the tail then not the big stuff and
computational thinking is needed to free
up the tail so computational thinking is
also a big push in NSF right now
computer science moving into every
science which I'm sure you're aware of
and that term computational thinking has
come up and I think we need to be
rethinking how we do all of the science
the basic procedures of the science even
for those small projects we need a
sociological change in a procedural
change and apply apply new methods and
we have to make the solutions easy and
there are current we're spending money
in the tail so globally everybody in the
world is spending money in the tail
nobody's really doing a good job about
it not
thessa and there are international
agreements that we should be sharing
science data so this is another argument
for doing it the agreements for the oecd
principles and guidelines for access to
research data at least in the tail we're
really not doing it all right so here's
a list that i have and we've already
touched some of these topics the head
tends to be homogeneous the tail tends
to be heterogeneous in its content the
head has a few concentrated datasets
where the tail has many distributed data
objects like the web the head tends to
be mechanized data collection so there's
instrumentation generating the data set
a lot of the tail data is hand generated
by humans like web pages there's a
uniform procedures generally in the head
but unique procedures in the tail there
are many many different ways of the data
to be generated the head tends to be
centrally curated so we don't have to
worry about it as much the part there's
not as big of a problem we have to keep
curating it but at least it is centrally
curated the tail though is individually
curated in very few cases are those
small science projects have a central
place to put their data that's not
always true but it tends to be true that
also means because of that curation
issue is that the data in the head can
be maintained over time because there
are enough people the projects tend to
be longer term the interest is there the
plans are there to maintain the data
long term but in the tail what you have
is is you know a three hundred thousand
dollar project or a hundred thousand
dollar project termed for three years
the scientist is interested generates
the data and then doesn't get redoes
integra nude they really don't can't
afford to pay a graduate student to keep
maintaining the website or do whatever
is necessary so it's just not maintained
over time it's not their fault it's just
there's no time or money to do it the
head tends to be open access at least
for public science data you know we
think about it ahead of time and say its
share
in the tail we don't really enforce
sharing of data so it tends to be
obscured intentionally or protected or
just unintentionally obscure and there's
an important social issue is the the
data in the head is big and important
enough that the people that manage the
data have to have a career in data
management and can get rich and famous
and get promoted and all that for doing
a good job but the data and the tail
nobody loses their job for losing the
data and nobody gets there makes a
career out of managing the tail data so
what's our mission here it's to organize
oh I left the word down there to
organize sciences data and make it
universally accessible and useful when
it is scattered over the web in many
formats you might notice that mission a
little bit i just changed a few words
but an added problem is that it's not
all on the web right now because it's
too difficult to put it there so we
actually have an acquisition problem and
it's currently organized maybe that
should be in quote in billions of
different structured formats there are
billions of records out there are
trillions so how do we get all of those
scattered objects into an organized
format so there are some technical
solutions and i'll be talking about
projects within NSF that are either in
my portfolio or or related that are
potential solutions so just ideas to
start but it's certainly none of these
things are solved so one is data data
and software standards helped to unify
data within projects and within the tail
so if we have if we invent the standards
and get and enforce them then it'll be
easier to organize the data later
metadata is important we need some way
to add information to the data itself to
help organize it and then protocols if
the data is sitting on databases or in
other silos somewhere we need mechanisms
to be it'll pull it out of the silo HTTP
is not always the best answer
some other solutions or controlled
vocabularies and I'm just listing some
in biology and that are in the
biological portfolio for approaches that
we're taking ontology such as the gene
ontology which is a collection of
anthologies actually yes otay pair is uh
what's it replacing its replacing a
digger so it's it's an access protocol
for museum holding data so each of the
museum's tend to have in other
organizations have databases containing
all the specimens that have the species
the date the collector of the ops or an
observation data as well you know I saw
a hummingbird of this species at this
date at this location that information
it's distributed over thousands of
museums many millions of records tape
here's the protocol for harvesting the
data out of those collections the data
format the carrier format for that is a
Darwin core this is sort of the
lightweight version of for data and this
is a slightly heavier weight access to
biological collections data heavier
weight data transport package
representation but this is the
communications protocol for that abaya
logical example
so we have controlled vocabularies
ontology 'he's actually gene ontology is
bordering on being a controlled
vocabulary but it's close to an ontology
and other applications here so people
can bring them up we have the Semantic
Web it's a potential solution for some
of this but certainly well we don't even
know what it is yet what but as long as
we don't define it it can keep being the
solution and data net is an instance of
a platform for experimenting for
solutions so data net is a program
within NSF a grant program within NSF
that's asking for consortium of people
to get together and make proposals to
run experiments on curating science data
over large swathes of science it's a
pretty major initiative it'll probably
the first set of awards will probably
around 20 million dollar range will be
awarded probably in December although I
can't promise anything like that but
assuming all the approvals go through
there'll be a couple then and then a new
round following as well people here know
what ontology czar so I don't for the
biologists often I have to go in and
explain what they are so this happens to
be a biological ontology okay they're
their institutional solutions so who's
going to who's going to deal with this
data for us who are the institutions
will do it well we know that a whole
bunch of well-paid librarians have been
managing our publications for long
periods of time and they love to work
for free they just really like to do
that all the time for very little money
and they're very happy to take on the
rest of the data problem for free as
well museums have been managing our
science data for biology and some other
sciences as well and we know that
they're there well-heeled and have lots
of money as well you can tell by how
well they kept this specimen
of a plant within the collection they
get moldy in the bugs eat them and
whatever they generally underfunded so
maybe museums can't won't automatically
be doing it we should be doing something
proactively professional societies will
probably do it because they have lots of
money they don't know they don't have
any money all the generous publishers
that are going to put the data up for
free they will if pushed put a certain
amount of data up because it will
increase circulation and the usefulness
of the data of the publication's
themselves but there aren't a whole lot
of them that are willing to address this
problem head on and then you know who
are the people are going to actually do
it I have business listed here these are
the annotators that are all lined up
ready to get all these science data sets
and markup our metadata for us so we
don't really have a solution well who's
going to do it but i would say that all
those agencies actually are part of the
solution the trouble is that that the
structure and running of all of those
things isn't designed yet to solve the
problem so we're gonna have to put some
effort and cause some change and make
money flow differently than it does
right now to make it happen we have set
up and provided funding to try to solve
this with in particulars silos of
science so these are lists of
investments that National Science
Foundation other agencies have been
putting money into to try to solve the
problem so lter is an example long-term
ecological research sites around the
United States and around the world down
to are trying to do science project in
the long term and five or six years ago
it turned to them were doing occurred to
them that they were doing long-term
research but not long-term data
management so they started five or more
years ago too
to work out global data management plans
for long-term research because its
long-term research but we can look at
them for examples of what to do and in
that case the data is a little different
it's not a three-year project that goes
away it's actually a project that we
intend to go for a hundred years
collecting information we have synthesis
centers National Center for ecological
analysis and synthesis the National
evolutionary synthesis Center but all of
the that's not right that's yeah that's
right that's right that's the trouble
with those organizations is that right
now they're funded by NSF on grants that
are long by NSF standards five years
sometimes renewable for another five but
then they end that's not long-term data
management that's ten years so what are
we going to do these these agencies are
our organizations are trying to figure
out what to do long-term because we're
telling them we're not going to fund
them forever but they don't have an
answer about what it's going to actually
do do those kinds of organizations have
databases that they've been developing
on the grant money that they have yeah
that will potentially be orphaned at the
end or are they figuring out knowing
that they're going to be run out of
money are they figuring out much longer
term solutions there are certainly
groups working on figuring out much
longer term solutions but if someone
went to one of these organizations and
said here's a solution would you like to
use it they would probably be receptive
yes okay yes because it's not completely
solved you know there are people working
on it have ideas they are looking at
trying to figure out new funding stream
mechanisms you know charge charging for
deposit and and other or we're charging
to get data out or some other mechanism
but it but it's not agreed upon and
there's a lot of pushback from
scientists either to pay to put data
enter paid
get data out so I have a series of
questions about the long tail I had
questions about dark data but we also
have questions about the long tail how
long is the tail was the area under the
tail how steep is the back of science so
where's the cutoff point between big
science small science that's sort of a
there is no real head and tail just sort
of a dignity well that's pretty steep
how valuable could the tail be we don't
know how often a lot of the data that's
in the tail we should probably throw
away so because the procedures if the
procedure is wrong when the data is
collected you know critical pieces of
metadata weren't collected we look at it
now it's really worthless now so it
should be thrown away and some may never
be used again some data can never be
some data can be recollected so if
you're doing a measurement of a
repeating event or you could set up a
procedure you put the right chemicals in
and the rights you're going to assume
that the chemistry will be the same the
next time data you generated from that
chemistry you can do anytime you want we
generate the data but if you're looking
at a natural phenomenon that never
repeats you can never get the data back
so there's different values for the data
itself generated from an experiment
based on the nature and cost of
replicability of it is there a
difference between tail science and head
science so the activities that are going
on there certainly is some difference
but what are that are the differences is
there a differential distribution across
the sciences so I did a graph of the
financial expenditures across an SF
across all science that NSF covers we
don't do medicine for example but it is
there a difference and I think that
there probably is and I have some graphs
and data that I'm still working on to
try to figure that out and then but I
don't know why why the curves are
different across the sciences
so we have a list of barriers here that
I'm not going to go into for time sake
but some of these have already come up
in our discussions I want to point out
that many of these are sociological some
are technical but some are sociological
or psychological and a lot of it has to
do with how much time and effort it
takes for people to do stuff and where
the money is going to come from to
support the time and effort and we can
reduce the time and effort by
introducing technologies institutions
and methods that help streamline matters
but that'll take an investment as well
well I have a are but you're right
privacy yeah
this is just a list of projects and
solutions text mining data mining
publishing models social science
individual projects that are that are
working on this kind of problem data
integration automatic classification
machine learning we're pretty far along
so I'm going to skip some of this
automatic metadata generation from
images is what just went by so in order
for curating dark data in the long tail
of science we have to do we have to
solve a series of problems or at least
recognize what the issues are we have to
probably first do a valuation analysis
so how much is the data worth how much
investment should we be putting in we
don't have a final solution but we need
to justify our actions along the way we
have a data creation issue in many cases
we have an acquisition issue and we
probably want to tie those two together
when the data is created it's best if we
have it acquired into our online systems
immediately to help reduce the cost
organization distribution issues and
then most importantly probably has
changed in business as usual so we have
to deal with all of those social that's
aside the sociology of science so that's
it for the formal slides and
presentations so anyone have any
additional questions comments the
solution in your back pocket so I could
don't have to get the talk again I have
the opposite of a solution in my back
pocket listen given the magic wand and
you can wave it and a solution pops out
what does that solution look like
well certainly from my last slide it has
to be a very multifaceted solution
because we'll have to do things like
educate a whole group of people probably
we need specialists to deal with the
data we need the individual scientists
have to be educated and changed the
nature of the instrumentation we
probably change instrument the companies
that produce instrumentation we want to
influence them to have them generate
metadata automatically with instruments
that's appropriate for long-term
preservation and reuse of the data we
need storage mechanisms that can be
preserved long term and and data pulled
back out of them again but also
migration strategy so just doing a bit
bucket doesn't solve the problem
completely you know you put the data in
in whatever format it's in the formats
going to age and eventually it's going
to have to migrate into something else
so the solution is I see as being
probably many many solutions that we're
going to have to layer together as a
complex amalgam and probably we won't do
a solution but a series of solutions so
chip away at the problem do the cream
first they're low hanging fruit and then
move ahead from there can you mandate
many of those things in the grant
structure I mean all the NSF
participants have to propose to an AO
and those AOS can say you must provide
some form of data preservation mechanism
yeah part of your grant if you don't
you're not going to get the grant so
yeah force curation not happening I'm
pushing that it happens already inside
of individual programs so like LTE are
the long-term ecological research if
you're participating at one of the sites
if you're generating data at one of the
long-term sites you're required to
deposit your data and you're required to
fill out metadata about your data or
you're not allowed to do the research at
the site it is that only valid for lter
because there's a place to put your data
automatically oh yes I'm not in the
others because of
lack of that well because there was an
interest in mandate to have long-term
preservation of the data the mechanisms
were developed and put into place they
are there even they are wanting so you
must put it in but it turns out that
generating the metadata is really hard
and therefore people don't always do it
well it's hard to people put in metadata
whether it's useful metadata or not as a
whole other story so we can buy policy
though make that data be collected but
it's not currently done throughout you
National Science Foundation and
certainly not throughout the government
funding of agencies it's not imposed but
we're trying to make that happen so that
one of the objectives of the data
working group is to make a more uniform
across NSF policy for that everyone must
have a data management plan they don't
necessarily have to make their data
public we're not going to tell them what
metadata should be associated with it
but it's a recruit review criteria and
since only ten percent or so of grants
get funded I would see it as a strong
motivating force eventually because if
within a field everyone's really poor
doing the data management it doesn't
hurt you much not to have a data
management plan but if you're the first
one to have a good data structure and
then there's a decision point should i
fund this one or that one and this one's
data management better it'll get funded
well that pushes everybody to make
better data management throughout so we
don't have to impose what the management
is just that there must be a data
management plan appropriate for that
science and we're pushing for that but
that's a long term political issue
within NSF but we're moving that way
so when you can't ask people to keep
doing something after you're not funding
them anymore that's right and NIH has a
data sharing plan yep you just have to
propose a sure yeah there's no one to
enforce it right and across NSF right
now the enforcement of data is very
spotty and one of the concerns that came
up about having having a broader data
policy is the time and effort cost that
will be on staff at NSF so we're not
getting more staff to enforce any such
policy so what meaningful impact will it
have if we have a policy no one enforces
it right that's exactly although maybe
they should i because if we're doing an
investment of making the data in the
first place then it's probably wise for
for NSF or any other funding agency to
set aside a certain percentage of money
for managing the data longer-term so it
should be in National Science
Foundation's interest to provide money
to do it it should be the trick is from
an agency's point of view I have the
most valuable days that you've ever seen
you should pay me to curate it for the
rest of right exactly happens at my
program all the time so for an agency's
budget that means well I agree with you
but that means that that amount of money
has gone from my budget for the rest of
eternity so now my budget now I Congress
the same be transformational and I've
just reduced my budget effectively
that's right it seems to be the problem
right exactly yeah and people recognize
that within one of the main programs
that I'm a manager for advances in
biological informatics it used to be
called last year biological databases
and informatics and one of the first my
first Jobs was to help rewrite the
solicitation to make it advances in
biological informatics
partly because we wanted to we've
spawned the creation of hundreds of
databases and what we don't want it and
we're spending money to maintain those
databases and we don't want to make more
databases there is important data to be
generated but we have to spend some
effort on economies of scale and
integration and other things so we
changed the solicitation around and
change the focus away from that new data
generation focus to a more integration
focus scaling focus in hopes that we'll
bill to reduce the cost and come up with
mechanisms that are more maintainable
than spinning off new databases every
year and then having to maintain them
forever but you're right it's also i
think it's not as you indicate with nih
it's not just an NSF problem it's it's a
problem for everyone in the world really
both governments and business side
foundations data is being generated
everywhere and we're not we don't have
good solutions for what to do about it
and some of them are social solutions
like where do we spend our money and we
have to reconsider yeah Brian um
definitely a great problem you got here
okay yeah it's a really good problem i
also in the computer science side of
engineering and spent years at art both
and NSF and is also eight years computer
history museum so so one the
preservation group is not one is not a
bad one to kind of get aligned with in
some way because i think there's there's
all kinds of data out there that just
totally been lost and that turns out to
be a large incentive and there's so much
informations being lost it's a little
different from your community than the
computer science community i think
because we do have certain kinds of
databases but the digital libraries
project which spent millions and
millions of dollars for example and book
my hat on from CEO of the Computer
History Museum to delete in the past I
would say really hasn't delivered in 15
years it's a very difficult problem even
there and and so you've got to do this
kind of stepwise secondly the issue that
I find
challenging is you know how do you
measure the quality it may not be known
only the long tail versus the head as
much as the quality of the information
and how it gets sifted in different
scientific communities which I'm sure
that's probably the political problem
you got inside of of NSF yeah but within
within corporate America I also believe
that there's probably some alignment I
haven't give that much thought I've only
been Google a few months myself to how
that could actually be done because they
have a similar problem of reuse of the
information that they want to be able to
have a heads on advantage and I think
that would be a very attractive
proposition for some of them provided
you know that they could see a way in
which they could get through the maze
and the fog because the more fog we
create in this thing the harder it's
going to be for you to come up with
solution right yeah I think it's to
avoid that fog it's good to focus on a
particular very finite problems definite
problems that look like they're solvable
right and tomorrow I'm here in San
Francisco or I am in san francisco to
talk to the Society of American
archivists yeah and and there i'll be
talking some what about this tale
problem but but mostly about education
programs and data curation and digital
curation but but the arc archivists are
clearly an important group to bring in
and the librarians and others that have
been thinking about long-term
preservation for a long time but yeah
the digital library initiative in some
ways with success almost all libraries
are digital but it keeps costing money
which shouldn't be surprising but it
does
one thing before everybody leaves we are
going to go to lunch after this and
you're welcome to join us for lunch so
swearing approached you haven't
mentioned given scarce resources if you
do a self-serve system which is anybody
you just provide the disk space and a
mechanism and anyone can put stuff there
and you create rules of some kind you
get you get site it has to be it has to
metadata or it can't be encrypted or it
has to be publicly available or to be
publicly available at five years or
something if you make the rules
agreeable enough people get the benefit
that all their stuff is backed up so
people may just use a self-serve system
and then you can get around to analyzing
it and actually doing stuff with it five
years ten years 50 years whenever you
have the manpower of the technology yeah
out of anything like that yeah actually
the the article i mentioned that i'm
writing about this is for library trends
on a special issue of for institutional
repositories and institutional
repositories are sort of of that ilk in
that there's some metadata associated
with it but it's not highly structured
it tends to be just digital blobs
although they're moving in a direction
to deal with structured data better I
hope and certainly with with my paper I
hope I push them that way further but it
is the idea it's minimal requirements
for what metadata you put on but at
least it's an easy place to put it it's
going to be backed up and preserved for
a pretty long period of time at least in
bit format will be preserved whether you
know it won't be migrated in all cases
but even that they're working as play as
time goes on they're working on
migration plans for particular formats
at least so if you put it in in XML so
with an ASCII under liars or episodic or
whatever you're using at that period of
time it'll migrate from eps adicto a
reasonable format as long as you have
the metadata march for for what the
format is at that time so it'll happen
but but it's not as rich as we would
really like thanks
so you describe the problem and then you
said that one approach was to kind of
move that long tail towards the head and
so in order to create more repositories
to create the ability for the different
data sets to enter talk to talk to each
other to increase the standardization if
you put a vocabulary there people would
start to use it but you also mentioned
that one of the advantages of the long
tail may be that its innovative that
it's different that it's new that's
measuring new things so what is sort of
a solution with another approach that
isn't necessarily about increasing the
standardization or making head datasets
possible where maybe they hadn't been
that allows you to take to sort of
maintain the advantage of the fact that
you know this is a new format of data
and new whatever it is that keeps the
innovation and the uniqueness and the
smallness of things yeah well I think
some subsets of data that are in the
tail are mirja below the natural history
data is a good example the collections
data and observations data pretty much
you can come up with a natural structure
for representation of a darwin court or
something and pull it all together into
big big sets like like it jeebus but
that's not saying that when the the
scientist is in the field and collecting
some unique piece of information about
you know there might be behavioral
information about the species or
whatever that's not saying that they're
not collecting that data as well it just
means you're not going to be able to
pull it as easily because it's not going
to fall under that framework it's
slightly different in those cases of
which there should always be a lot going
on if we're doing innovation you're
right we should be happy that there's a
lot of heterogeneity out there we do
still need to educate the scientist or
provide them with trained staff members
that know about digital data management
and it's not necessarily the case the
scientist has to do all the work we
don't want the scientist a biologist
spending all their time doing computer
science we want the biologists to do
biology but we can give them a support
structure that when they have data it'll
go somewhere and get saved in some
format
dependent of what it is you know because
there's a group of people that know how
to do it and there's machines and the
infrastructure available to do it so we
can provide an infrastructure at that
level for those but I don't know what
infrastructure thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>