<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The OpenOnload User-level Network Stack | Coder Coacher - Coaching Coders</title><meta content="The OpenOnload User-level Network Stack - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The OpenOnload User-level Network Stack</b></h2><h5 class="post__date">2008-02-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1Y8hoznuuuM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Arthur you know everybody I'm Steve Pope
from Solar Flare communications I'm here
to tell you about the open onload
project I thought I'd start with a
couple of interesting dates from the
history of local area networking and a
bit of a history revisionism lesson so
Ethernet this year is 35 years old then
the memo in 1973 Bob Metcalfe wrote
started off that whole whole revolution
as it were and then 2006 Ethernet is now
running four orders of magnitude faster
at ng base T and Solar Flare sampled its
first 10g base T Phi in August 2006 just
a couple of months after the standard
was ratified so that allows me to get my
one and only product placement out of
the way I can breathe easy now and then
for a bit of his bit of revisionism so I
don't have you noticed but the the
picture at the top is not Ethernet it's
actually the Cambridge ring which is
another local area network which is
invented at about the same time as as
Ethernet I guess the point is that you
know everybody thinks about Ethernet
over the years it's obviously being the
big dominance local area network but
there have been lots and lots and lots
of others there's some like arc net for
example in use today so really in one
sense local area networking has been a
bit of a parallel set of parallel
universes over the years we've all
watched the Golden Compass and
everyone's going between the different
universes then so I've drawn this chart
here showing the step changes of
Ethernet speed this is the base T steps
of Ethernet over time that's the blue
line with a step function you know going
from 10 megabits up to 10 gigabits and
then the dotted line is showing the
performance of what item to be the R&amp;amp;D
or the high performance compute network
of the time so over the left you've got
FDDI and an ATM ATM 622 megabits and
then so Mnet's technology is said to be
the atomic project for example it was
into movie net never come and then later
on you got infinity buns over here and
so of course did and there are lots more
and you know I'm not even going to
attempt to to name that all of the order
of H high performance compute research
efforts over over the years but one of
the one of the common features of that
whole R&amp;amp;D high performance compute
effort well first of all it's it's been
in common use I mean it's in production
for an awful lot of places these sort of
networks but secondly over time they've
very much challenged the i/o bandwidth
of the machines which they've been
plugged into and that's that that's
meant that the the guys involved in
those networks have been pushing network
architecture and they've been at the
bleeding edge of network architecture
right the way through the last 20 or so
years certainly for the last ten years
picture on the slide here kind of
constant gives gives what constitutes
what I call the bleeding edge of network
architectures and and and that very much
is that the years starting with the
network adapter at the bottom network
adapter provides a protected virtualized
interface which allows the application
context to talk communicate safely with
the network without using the operating
system a big feature of this
architecture is it is it reduces the
overhead required to invoke the network
stack so you can reduce reduce overhead
significantly by not going to the
operators and under some circumstances
you can you can actually run with much
much reduced rate of interrupts as well
now it actually turns out and Dave's
going to go into it and a lot more lot
more detail later on I'm really just
introducing this but it does turn out
that some models of computation just
lend themselves much more naturally to
this kind of architecture and the
classic the you know multiple processes
with message passing that's used in
hyper forms commute compute community
with age with MPI and all the all the
high performance compute codes that over
NPR
do Fitness architecture very well
whereas the kind of classic will be more
commonly used threads with shared memory
type model of computation has a lot of
difficulty with this sort of
architecture and we're going to go into
this and a lot more detail through the
talk so if you yeah it's it's not just
the computational model of the
application for you know if you're
trying to apply that kind of
architecture out of the box of
applications which are able to use
middleware provided for them if you're
trying to run with a what I say bleeding
edge network architecture but a standard
or a couple or a classical and everyday
application you also run into the issue
that these applications have written
with with particular api's and they've
written to support semantics in
particular api's for example POSIX and
again David's going to go through a lot
of these examples in in a lot more
detail but just just very simply if you
do exec and you're a you've got a load
of network protocol State tied up in
your application context you've just
lost all of that state so how does that
and how do you cope with that and and
lots of other issues but generally again
a common denominator of you know what's
the nub of the of the difficulty in
supporting an everyday application with
a high performance network is the state
which is required for to perform network
protocol processing and having that
state living in the applications address
space the applications context that's
the problem we're dealing with that
state is one of the reasons or it's one
of the implicitly one of the reasons why
a lot of high performance networks push
that down into the network so you see
effectively transport processing being
being done on the network or very strict
model of network communication enforced
on the application
so that's until now with the open on
Lodz project they open on layered
architecture we're trying to address how
to crossover between those two parallel
universes how to achieve how to apply a
high performance compute network
architecture to in everyday application
that happens to be using the network I'm
going to go through some of the basic
principles of the architecture and then
we're going to go down a level and talk
in a lot more detail about how how they
implemented so the first principle is
that is is the is actually the OS bypass
property that I described earlier that
the the network needs to provide a fully
protected virtualized interface that
means that you could have a whole load
of applications each of those can have a
hardware mat network virtualized network
adapter in to its address space it's
safe for an application to transmit and
receive packets that means that an
application can't DMA data into anybody
else's address space there needs to be a
model of virtual memory which the
adapter is part of it also means that
the separation of those interfaces needs
to be enforced did by Hardware so you
you can't hog all the resources of the
machine because these applications that
spaces are not necessarily trusted the
second property is the protocol
processing can take place both or
protocol processing for given flow can
take place both in the application
context but also in the operating system
context and that's important where your
web we're processing needs to take place
on behalf of an application for example
an application disappears yet proceeds
processing needs to need to occur and
then the third property is that
both if both the colonel and the
userspace are involved in protocol
processing on behalf of an application
then they need to communicate through
sheds state so it needs to be an
efficient because the whole point of
this is to avoid overheads and it's be
an efficient means of communicating that
state back and forth between the user
context and the kernel
and because the kernels involve that
that communications channel needs to be
protected from malicious or buggy
applications so that's the open onload
architecture quickly and David here is
going to go through a whole load of
implementation details and how we how we
solve those specific problems to dealing
with with States and so forth in the in
the current application context ready
thanks very much Steve
so the first thing when you're creating
a high-performance networking is what do
you concentrate on how do you improve
performance and the answer is overhead
CPU overhead that's almost always the
answer it's first of all if you're
spending time on the CPU doing stuff in
the network you're not spending that
time in your real application and that's
where you want to be spending your time
right some applications however can't
make progress because they're waiting
for something to happen so in in that
case the latency of the network can have
an effect you're waiting for a message
the time that that message takes to get
to you is determined by the latency what
is latency composed from well it's
composed of how long it takes the
hardware to send your message out on to
the wire how long it takes to cross the
network and how much time it spends
being processed by the network protocol
bandwidth is the the number that most
people think of when they think of
performance but actually all bandwidth
is is a thing that constrains latency
when you've got large messages if you've
got high bandwidth then your large
messages will get there quickly and what
determines your bandwidth your link
speed obviously secondly algorithms if
you've got any loss on TCP your
bandwidth will suffer horrendously and
the thing that will determine what
bandits you get will be what are the
algorithms that you have for recovering
from loss and retransmitting and all
that sort of stuff the other thing is
buffering if you haven't got enough
buffering to satisfy your bandwidth
delay product then you won't yet uh
saturate the link and finally it's
overhead if you haven't got enough CPU
grunt to actually push the packets onto
the wire quickly enough will receiving
quickly enough that will limit your
bandwidth so overhead is is that is the
thing that we can improve because the
link speed is fixed the algorithms for
TCP basically have been specified in the
RFC s there's a little bit of cleverness
you can do it basically you have to
follow the rules and the buffering
that's up to the administrator really
did to make sure there's enough of it
and finally this the scalability so
you've got your server with lots and
lots of cause
everyone knows that most applications
don't scale linearly with the number of
course you get crosstalk between the
cause and all sorts of stuff and that's
an area where the architecture of your
network stack and all the software that
goes with it can have a big effect on
your performance but really all of those
crosstalk effects have the if what
they're doing is in increasing the
software overhead so overhead is the key
and that's pretty much what I'm going to
be talking about for most of the talk so
here is um what traditional standard
kernel networking looks like what I've
done is I've got a gray box on the
right-hand side and you'll see this in
most of my diagrams and that these stuff
that's going on in the OS kernel on the
left-hand side you've got stuff that's
happening in user space and the box is
an application where actually it's
really a process this bit of the box
here that overlaps the kernel is the bit
of kernel state that's associated with
that process and here you've got a file
descriptor table so user level you
reference files and sockets with an
integer and this table just converts
that integer into some bit of data
structure that implements your father
your socket
you've got your application logic and
applications talk to the kernel through
the system see library which is just a
dynamically loaded shared library which
then invokes system calls an interface
that we're most concerned with these the
bsd sockets interface because that's
what everybody uses possibly windsock if
you're on Windows and aspects of the
system call interface that that have an
effect on user processes that will come
to shortly so the other parts of the
system of course are the stack which
runs entirely in the kernel a socket and
the network interface controller or
network adapter all of this is generic
and your network adapter will come with
a driver and that driver is the thing
basically that pushes packets onto an
off the network so here is a sort of
classic simple approach to doing user
level networking
essentially you stick your stack at user
level as Steve described you want to
bypass the OS and you've got to get
network packets in and out all of your
user process so you've got to have a
direct connection to your adapter and
this is your virtualized interface
Phoenix stands for virtual NIC you've
got to intercept the sockets BSD sockets
API that the application is invoking and
you've got to get there before Lib C
does and fortunately on UNIX operating
systems as there's a system called LD
preload that basically it says please
load my shared library ahead of anybody
else's shared library and please
consider my symbols that I export to be
the ones that are are dominant so all
we've got here is a is a library which
exports up this edge read write send
receive listen accept all the standard
system calls and implements them for
these level stack of course we have to
cope with kernel files and sockets as
well so we have a file descriptor table
here and and it we've got one user level
socket in this case and anything that
isn't present in this table we assume
that it's a kernel implemented file or
socket in which case the system call
just gets passed through into Lipsy and
then up to the kernel and in this case
i've just illustrated a file as well as
our socket so how do we do the the safe
user level access to the network so this
is a smart NIC it's a standard NIC won't
be able to do this without modification
here's the bit called the v-neck and
essentially it's a receive interface a
transmit interface and a way of
notifying the application of what's
going on so secondly of course not all
packets are going to want to go to user
level so we have to have some means of
deep multiplexing receive packets and
sending them to the right place and
that's what I've got over on this side
which we call the filter table in our
implementation
so packets come in we extract fields
from the header compare them with
entries in this and that identifies a
particular v-neck the NIC will support
lots of Enochs so that you can support
lots of applications and of course
you'll have some special v-necks which
are just used by the kernel for standard
networking so packets come in filter
table sends them to a v-neck and then we
look in a descriptor ring so this ring
contains the addresses of buffers that
receive packets should be placed in
because we have to protect the system
from badly behaved applications the
addresses in here won't be physical
addresses they'll be virtualized
addresses and those addresses are looked
up in this table which is essentially a
special sort of page table and that
resolves the address to a real buffer in
the applications address space when the
packet arrives we then send a
notification into a notification queue
so this is how the application
determines what's happened transmits
just the reverse these are the addresses
of packets in memory that we that we
want to send and the final thing is we
have to have some doorbells and these
are just means of telling the hardware
when we've put new descriptors into the
RX and TX rings okay Steve mentioned
earlier that there are some difficulties
with user level networking and they all
arise from the fact that the OS has no
idea what we're playing at and we're
trying to hide it from them first one as
Steve mentioned is exec what happens
when you do exec bang everything
disappears and in the kernel that's just
fine because you've still got your
kernel portion of the process state
referencing the socket and the file and
when a new application gets loaded it's
able to use those assuming it knows what
file descriptors to use it can just go
ahead and use them and everything's fine
back to I use the level stack bang
everything's gone application comes back
we'll assume for the moment that it
loads the same user level library will
assume that it can set up its connection
to the vena qug n but it's forgotten all
about the socket it's rotten all its
protocol state and there's no way to
save you from that
another interesting one is fork so Forks
dry it basically makes two copies of
your process everything at user level is
duplicated everything in the kernel is
duplicated but you don't have two
sockets and two files you just have two
references to those sockets and files so
in other words these guys if they want
to they can share those sockets and
files so they can both use them at the
same time they might not want to in
which case one of them might close them
but the point is we have to support that
behavior because some applications do
here's what happens when you fought with
our our trivial user level
implementation the kernel files okay but
now we've got two copies of the socket
two copies of the protocol state it's
not really obvious what happens to this
direct connection to the v-neck probably
will point to the same place if you've
got a shared mapping and now you've got
two people trying to use the same bit of
hardware and they're not coordinating
with each other so clearly that's not
going to work and there's a whole bunch
of other problems you can pass any file
descriptor including sockets through a
UNIX domain socket to another process
what happens essentially is is the the
OS understands that this is happening
and it just creates a new file
descriptor a new reference to your
socket or file in the new process
invents a new file descriptor 2 for that
and tells the process what it is and
clearly in the user level model that's
not going to work very well because the
new process might not even have loaded
the the relevant library and it
certainly won't have the protocol state
associated with your socket and what's
more the kernel just won't do it it'll
say I don't know anything about this
file descriptor I've never heard of it
fail process exits that's not just the
end of your networking if you've sent
data on a TCP socket and then exit your
process it's a bit like doing shutdown
that data will still get to the receiver
actually there are some exceptions but
basically you've got to keep doing
protocol until the sockets close
gracefully
and finally processes aren't always
continually invoking the the network but
even when they don't you've still got to
make progress now one solution to this
that some people have looked at is is
using things like signals or extra
threads to make progress when the real
application isn't doing anything
networking related but the problem with
that is that you're really changing the
environment that the processes is
running in and it's very difficult to
hide that from the process especially if
you use things like signals because
you've now taken a resource that the
process thought it owned so here are the
requirements for our user level
architecture first of all the state is
going to have to live outside the
process because it has to survive when
the process exits and and also to
survive except when the user level state
disappears we have to represent sockets
in the FD table that's because we've got
to be out of pass them through the UNIX
domain sockets and also because the
kernel really has to know what file
descriptors the process is using because
otherwise it will reuse them and you'll
get clashes and finally the states got
to be shareable between processes in
order to support stuff like fork and
passing sockets through domain sockets
so here's our solution here's the
architecture essentially our so-called
user level stack is created and
basically lives in the kernel it's
divided into two bits we've got a
trusted bit which is only visible to the
kernel and we've got the shared bit and
this is the bit that user level
processes will have direct access to
here's our application his are our
preload library and what we've got here
is a mapping onto this bit of shared
state as you can see a socket here is
represented in the kernels FD table
there's a bit of kernel only state here
and this is essentially a standard
kernel data structure that implements
the the file interface but it's got a
reference to our our user level socket
and the user level FD table of course
also knows about our user level socket
and this is the place where we do the
intercept so that we can do stuff at
user level if we want to
so how does this architecture solve the
problems that we talked about before so
here's what happens if we do a fork
essentially it just works what we've
done is we've mapped this shared state
in as map shared and what that means is
when you fork instead of getting a copy
on write copy of the state you actually
get a shared copy of the state and the
other bit the kernel bit of course just
works because that's what the kernel
already did and exec as expect a little
bit more interesting so first of all
everything gets blown away when you're
doing SEC
fortunately the stacks still there in
the kernel and we've still got our
reference to the socket in the processes
FD table process comes back we're going
to assume in this example that um that
it's still using the user level library
the preload library and now it's going
to come in and try and use this socket
and at the moment it doesn't really have
a clue how that socket is implemented or
even whether it's a socket at all so the
first thing it's going to do is invoke
stat and that the operating system will
tell us a little bit a bit about the
file descriptor that is implementing
this particular file descriptor and the
stat will come back and say it's not a
socket it's not a file it's a an onload
thing and what will then do is we'll say
okay we need to map in this the the
state of the stack into our use level
address space we can do that because
we've got a file descriptor and we just
invoke M map on that file descriptor
with the appropriate arguments and then
finally set up the the pointer in the ft
table question is of course how do we
deal with these sockets with these
sockets and files that aren't
implemented at user level and the answer
is same thing will happen you'll do stat
and we'll determine that actually this
is just something else that isn't a user
level socket and we put a special entry
in the table that says don't ever bother
setting this one again just pass it
straight through to the OS so that's
exec and fork passing file descriptors
through domain sockets and and similar
and surviving process exit hopefully is
obvious
Prosis goes away this bit dozen until
all the sockets have reached the closed
state so what's in that shared state the
answer is absolutely everything that you
could possibly want in order to do
anything performance critical at user
level so that includes sockets the
buffers that are going to contain the
packets for transmission and the receive
packets it's going to include the state
that we need to manage the interface to
the hardware it's going to include the
d-max table so this is when we receive a
packet we have to take the headers and
resolve that to the actual socket state
timers so TCP protocol requires three
transmit time is zero endo probes all
sorts of timers keeping track of free
resources some statistics and some
configuration so what we've got now is
some state which is moderately complex
it contains a whole bunch of data
structures and it's going to be living
in the kernel and multiple processes
potentially and there's absolutely
nothing to make you believe that their
address spaces are in any way common and
there's absolutely no way that we can
arrange that this shared state is mapped
into the same address range in all these
places therefore it cannot contain any
pointers at all everything in the shared
state is in directed in some cases it's
in directed at a fairly high level so we
have indexes that say this socket index
number three and you find it just by a
simple indirection
from the base of a memory map in other
cases for more complex things like
linked lists and stuff that would become
really painful and so in those cases we
just encode pointers offsets relative to
some well-known pointer and of course
each of the address spaces that that has
the shared state will have a variety of
pointers into the bits of the shared
state that a map separately now the
problem of course is that we've got this
state and it's it's touched by the
kernel in fairly complicated ways and
it's touched by the user level process
in complicated ways and
not all processes are trusted in fact
generally speaking none of them are and
you don't want anything that an
application can do to compromise system
integrity in any way at all so this has
two implications first of all it does
change the trust model of the system
slightly first of all applications that
share shared states if they're both
mapping that shared state they have to
trust each other the reason for that is
that the cost of validating everything
is a it's going to increase overhead and
secondly there's really no point because
if you don't trust this guy and he can
smash up your stack whichever way you
look at it he's going to be out of
damage behavior secondly we have a
workaround for it which we'll come to
but the thing that really matters is the
kernel and I'll come to how we protect
the kernel in just a moment first of all
how do we deal with the fact that we
might not want to trust every process
that we pass a socket to so here's a
classic example you've got a server
let's say a web server it receives
connections from clients and then it
wants those connections to be
implemented in some untrusted process so
perhaps your web server runs as root but
you want to run a CGI script as some
unprivileged user first thing is you'll
accept a new connection and then you'll
fork so this is exactly the same thing
that we saw earlier and then you'll exec
so that guy will disappear and then this
time when we load him we don't allow him
to map the state back in so this would
be a configuration option for the stack
because we don't trust this guy we're
not going to share the state with him
his only route into this socket is
through the syscall api now you might
worry that this is going to cost you
some performance but generally speaking
handlers for connections aren't all that
long-lived so they're not likely to be
doing a huge amount through this
interface you're going to get some of
the performance benefit of onload anyway
just by virtue of the fact that using
the onload stack and the third thing is
that mapping that state into user level
has a cost and it's not trivial memory
mapping isn't incredibly cheap so if
you've got a short-lived process it's
actually better that it doesn't matter
use the state down to user level
anyway and the reason this works is
because hanging off here is enough hooks
that we can just implement the Cisco API
at the kernel level so we have no
problem of getting at this state and
just doing exactly the same things in
the kernel that we would have otherwise
done at user level now the key one is
protecting the kernel how do we make
sure that when the kernel goes through
all this potentially arbitrarily
corrupted state that it doesn't panic
itself so we've got the trusted state
that we saw here and this contains all
the pointers that we need into basically
base regions of the shared State it also
contains any data structures that are
sort of part of the kernel and it also
contains pointers to any resources that
we need so there's no way that user
level can overwrite something that will
cause us to leak resources all those
resources are always hung off trusted
State the next thing we got to do is
make sure that we don't cause the kernel
default and basically what that comes
down to is anytime that you compute a
pointer using state from the shared
state you have to validate that and make
sure that it is not going to generate an
address that's outside of the shared
state and essentially there are two ways
of doing it one of them is by explicit
checking the other one is essentially by
applying masks to offsets to give you a
pointer that's guaranteed to be within a
certain distance of the beginning of
well you've got to guarantee that if
your pointer is here and it's a data
structure of size X pointer plus X
doesn't take you outside of the shared
State and that can be done very cheaply
finally we're following things like
linked lists and it's obviously
trivially easy to set up a linked list
that's a circle and so if your algorithm
just follows the linked list until it
gets to the end then it's very easy to
see how you could put the kernel into a
tight loop and the way that we solved
that is simply by any loop that the
kernel can execute is bounded with a
counter and the maximum bound of that
loop comes from some trusted state so
you can probably make the kernel work a
bit harder than otherwise would have
done but you can't actually lock it up
completely
another thing that's in the shared state
but is a little bit special is the
control plane so this is the ARP table
the ARP table is the thing that says
given an IP address what's the MAC
address and the route table which is the
thing that says given an IP address
which interface am I going out of and
the interface table which just gives you
a bit of information about the the
network ports there's just one copy of
that and it's shared by all of the user
level stacks of which there are
potentially many and it's share and it's
mapped read-only so no untrusted code
can corrupt it so it's always guaranteed
to have the right stuff in it entries
inside that table are protected with
generation counters and so there's a bit
of protocol that says how you have to
read it to make sure that whatever you
read is fully valid and to make it
really cheap anything that is always
talking to the same person so any sort
of connected socket whether it's TCP or
UDP keeps a cache of their layer 2
information keeps a cache of the headers
and all they've and also the position of
the entry in the table that they're
using and all I've got to do to check
that their information is still valid is
just check that the generation counter
hasn't changed so that's really really
cheap and it doesn't even involve any
bus locks or atomic operations which is
very important if we do make changes to
the table such as well let's say we make
changes to the route table that could
actually have a knock-on effect on any
number of TCP sockets but we don't want
the TCP sockets to have to check the
route table every time they send a
packet so what we do is if we change the
route table any possible entry in the
art table that might have been affected
by that route change they all get their
generation counters bumped and that
forces the the sender to go onto a slow
path makes him have a look at the whole
of the routing process from scratch and
get new information if necessary and of
course this information has to be kept
up-to-date because things can change on
the fly so we synchronize it with the
Linux control plane using netlink
sockets
so um we've got a parallel stack there
are two stacks going on here there's
still a kernel stack of course and
potentially there are other adapters
that aren't unload enabled there's more
than one route out of this box and of
course you've got the loopback interface
you've got to be able to talk to
applications on the same box when you
open a socket you've got no idea where
it's going to go until they connect or
bind in fact if it's a UDP socket they
may never connect it in which case at
any point it can receive packets from
anywhere and send packets to anywhere
when you do come to connect something
well initially you're going to have to
have made it as a user level socket and
but you might discover that actually
it's going to go out on the loopback
interface or perhaps it's going to go
out on another interface in which case
you've got to have a kernel socket
listening sockets are a bit like UDP
sockets in that they've got to be able
to accept stuff from anywhere from local
loopback from other interfaces the whole
shebang and UDP I've already mentioned
the only way that we can support that is
with a kernel socket so this is the
diagram you seen before all we do is we
had a kernel socket there that means
that when we come to accept new
connections from a user level listening
socket we have to check the user level
state and say are there any new
connections ready to be accepted at user
level and we also have to have a look at
the kernel socket and say is there
anything there but of course we only
have to look at the kernel socket if
there's nothing available in the user
level socket so in other words we only
take the hit of going into the kernel if
nothing's happening at user level in the
case that you've got an active open
connection say a TCP connection and you
discover after you've done all this work
and created this that actually the
connections going to go out over the
kernel socket we can cut use a level
stack completely out of the way just by
doing that we basically take the kernel
socket put it straight into the FD table
where the user's level socket was market
is a pass-through socket we might have
to keep the user level state around
because it's possible that that socket
is known about in another process as
well and that process won't have seen
this
to the FDA table but if we know that
that stock it's only referenced in this
application then we can get rid of the
user level state as well and it all gets
cleaned up okay so moving on how do we
actually process Network events sure
Oh common right yeah so the question is
essentially do the two stacks have a
single address space for network
endpoints and in particular let's say
port space for TCP sockets and the
answer is yes so because for every
active open connection that we create
for every unique port that we use we
have a kernel socket and anytime that we
assign a port we actually bind the
kernel socket and only when we bound the
kernel socket do we know that we've got
that port now it sounds a little bit
like we're burning a lot of resource
here it's not quite as bad as it sounds
we only need to create this socket for
active open connections and listening
connections every connection that is
created by accepting a new connection at
user level from a user level listening
socket it's going to have the same port
number as the listening socket and
therefore we've already reserved that
port there's no conflict so handling
network events the way this works in
traditional kernel stack is packets
arrive but it delivered to memory by DMA
notifications go into the notification
queue that I showed you earlier and an
interrupt gets raised the vendor of the
hardware handles that interrupts talks
to the hardware create some packets data
structures that the kernel understands
passes them into the kernel stack
eventually perhaps if you've got some
payload in those packets it'll get
queued in the socket at some
indeterminate time possibly later the
application will invoke the the OS do a
receive and say give me that data or
possibly the application will ready did
that and maybe it's blocking waiting for
the data in which case you have to wake
it up and then it can can pull the data
we do things a little bit differently
first of all by default no interrupts
reason is we're trying to get absolutely
minimum overhead and interrupts cost you
quite a lot so what's going to happen is
those notifications are going to go into
your notification queue just as they do
with the kernel stack except no one's
going to look at them
until the application says I need that
data so when the application invokes
receive goes into our user level receive
code we have a look in the received
queue of the socket if there's any data
there we just return it if there isn't
any data there then we'll invoke
Stackpole and Stackpole will look at the
notification queue see which packets
have been received pass them through the
protocol potentially they'll end up in
the socket if they do data gets returned
to the application similarly and send
the only reason that we did invoke stack
poll is because we think it might
possibly cause some progress to be made
if we can make progress without calling
Stackpole we will and also poll and
select so these are the things that
allow you to find out which of a large
number of file descriptors are ready for
i/o the problem is with these that they
look at lots and lots of sockets
potentially and you don't want to be
calling Stackpole each time you look at
a socket so we arranged that with these
guys we call stat poll once when you
first do it on a socket in a particular
stack but any other sockets in the stack
won't get Stackpole called if a thread
blocks on a socket then somebody has got
to wake that thread up when something
happens and in that case we do enable
interrupts it doesn't hurt performance
particularly though because there's a
good chance of a threads blocking then
maybe the system isn't as busy as it
otherwise would be and besides you've
got no choice the interrupt basically
behaves exactly like it does in
traditional architecture which is to say
it causes us to look at the notification
queue and handle a protocol the only
difference is because it's an interrupt
we're doing it in the kernel instead of
it user level so what exactly does
Stackpole do will come to this one in a
sec looks at the notification queue
which is this the point of which it
handles all received packets and
determines which packets have been sent
refills the RX ring so this means give
the hardware more buffers to deliver in
to refill the TX ring in other words
make sure the hardware stays busy so
that you saturate the link
and process timers so this is the bit
where we say did our transmit timeout in
which case do we need to retransmit it
so the behavior that I just described
where we only process the stack when we
need to have some consequences first of
all it can mean that you get more
batching there's a reasonable chance
that if you haven't called Stackpole for
a while when you do call it there'll be
lots to do and generally speaking if you
have lots to do you can do it very
efficiently
compared with if you do things piecemeal
so that allows us to reduce overheads a
bit on the other hand this one kind of
these two don't both happen you get one
or the other really the more responsive
what this is is that if you go if you
call receive and the data is ready
you'll get it straight away that isn't
always true with the kernel stack it's
very common to have your interrupt rate
moderated so what you do there is you
say I don't want to get interrupted an
arbitrary rate because they can
overwhelm the system so we'll say we
only want and interrupt every 60 micro
seconds at most and that means if you
call receive at the beginning of that
period you might have to wait anything
up to sixty microseconds before you
realize that your data is there that
model this model doesn't suffer from
that that pause although it will suffer
from that pause if it has blocked and
going to interrupt mode but the big one
the really key thing is this improved
temporal and spatial cache locality when
the kernel processes Network events it
does so because the data has just
arrived in memory it doesn't do so
because the application is about to use
that data and there might be a big gap
both in terms of time and space so first
of all in terms of time because you can
process the receive data and it can sit
there in a socket for arbitrary amounts
of time before the application actually
wants it and in space because the
interrupt might happen on one CPU but
your application will be potentially on
a different CPU in which case you've
bought all of the protocol and socket
state into one cache and then when the
application receives the data some of
that states at least will have to move
into a different cache
another problem we talked about earlier
was what happens if the application
isn't processing the network and in
particular with our incredibly lazy
model where we don't bother they even
look at the network until the
application asks us to if an
applications away doing something else
then potentially things are going to go
wrong in particular if packets keep
coming in and we're not refilling that
descriptor ring eventually they're going
to get drops on the floor the TX ring is
going to go empty and deal your link is
going to go idle and if you're not
processing your timers then you're not
going to comply with the TCP protocol
and finally if you don't respond to
receive packets promptly so for example
if you receive data on the TCP you've
got a racket within a certain time frame
if you don't the other guy might read
transmit it and if he does that all hell
breaks loose performance will dive and
our solution to that is that we've got a
kind of like a watchdog timer on the lip
on the on the network adapter and this
thing is running all the time and when
we call Stackpole occasionally we'll
reset it and say start again at 500
microseconds or whatever if that thing
ever hits zero
it raises an interrupt and that force
assists to call Stackpole and that's how
we make progress okay so a big deal for
us is concurrency control concurrency
control is an area where you can really
easily throw away all of your
performance and it's also a place that's
really tricky for a user level stack
particularly if you're sharing state
between user level and the kernel in
particular you can't just use simple
locks let's say I grab a look at user
level and then we want to have a go at
accessing that stayed in the kernel
perhaps from an ISR you can't ask the
kernel to block on a lock which is held
by user level user level might never
drop that lock user level might be
stopped might not even might not be
scheduled for a very long time so first
of all there are correctness problems
that we have to deal with and secondly
concurrency control is always expensive
locks are expensive contention at user
level is particularly expensive because
when you get contention with with
traditional locks you
yourself to sleep if I'm waking up costs
you something the colonel doesn't have
such a bad problem because state in the
colonel stack is generally speaking
protected either by clever tricks like
RCU or alternatively using spin locks
and the colonel can be absolutely
certain that whoever's got a spin lock
will let go of it pretty soon and so if
you spend some time spinning it's not
going to cost you that big a deal and
it's almost certainly going to be
cheaper than blocking but we can't use
spin locks because as I say first of all
the person holding the lock might not
release it soon he might be DJ jeweled
and secondly the colonel and the user
level can't contend with each other so
we've got a classic trade-off we want to
minimize our locking overhead and that
means we don't want to be taking and
releasing lots of locks that means we
only want one lock really because that's
that's clearly the minimum problem is
one locks not going to work very well
because everyone's going to contend with
each other especially when you've got
lots of threads on lots of course
therefore we've got to have a
fine-grained locking right well it's not
quite that simple because there are some
clever tricks that that the academics
have come up with they've got all these
lock free algorithms that essentially
use atomic operations atomic memory
operations that allow you to do stuff to
memory atomically compare and swap
atomic increments variety of other
things and these allow you to construct
data structures that that you can
manipulate without holding any sorts of
lock and it's guaranteed that everyone
will make progress the problem is all of
these algorithms depend on bus locked
atomic operations and these are
expensive literally hundreds of cycles
each time you invoke one even if there's
no clash between you and somebody else
accessing that memory location and
that's the sort of place where overhead
will just eat you up so how do we deal
with this conundrum well first of all
anything that we do a lot of it better
not involve atomic operations and it
better not involve taking a lock each
time we do it
so we do have one big lock which covers
everything that you could possibly want
to do in Stackpole
Stackpole will almost never have to do
any more locking or any atomic
operations things that applications do
we try to avoid them touching that lock
so Stackpole is all about processing
receive packets and putting them in
places and sending packets in response
applications are typically about
receiving data can change in
configuration and sending data and the
main overlap there is the sending data
one because clearly sending is going to
have to coordinate with what's going in
on instead Pole and we'll talk about how
we solve that in a moment the other
thing we do is we try and minimize
locking so we try and make sure that for
anything that we want to do we don't
have to take more than one lock in the
stack and finally we want to avoid those
bus locked operations if we can although
they do come in handy occasionally so we
will use them if we have no choice
because we want to avoid contention but
in the cases that we can do stuff
uncontained 'add then we'll try and
avoid the bus locked operations so in
other words we want the best of both
worlds here's the model first of all big
lock covers a whole variety of stuff it
covers lots of the protocol stuff it can
it covers parts of what's in your
sockets particularly stuff on the
transmit path and some of the receive
path it covers completely the v-neck
interface where you talk to the hardware
then you've got the socket lock one per
socket and that protects mostly the
receive path and little bits of the
transmit path and finally we've got a
lock over here which is protecting this
FD table actually this isn't as simple
as a lock again if we just had a lock
there it would cause horrific contention
problems with multiple threads accessing
this table so it's actually a lot
cleverer than that but I don't have time
to go into the details instead we'll
talk about the receive path so first of
all every socket has a receive queue
which is where data goes once it's been
identified by Stackpole sent to a socket
gets plunked on a receive queue and
that's just a linked list so I've got a
head pointer
tail pointer if you receive another
packet it'll just get appended to the
end of that list and the tail pointer
moves on as I said anything that happens
in Stackpole is protected by the stack
lock so that just happens what happens
when you want to take some data out of
the receive queue so the application
comes along first of all we've got a
serial serialized multiple threads that
are trying to receive data from the same
socket it doesn't happen much generally
speaking only one thread will be using a
socket at a time but it could happen
therefore we've got to protect so that's
where the socket lock comes in and we've
got an extract pointer and it can be
pointing that null or it can be pointing
into the queue and essentially it's
pointing at the first packet that
contains data that hasn't been consumed
yet and as we consume data we just move
that along but we don't free up the
buffers and there are two reasons we
don't free up the buffers is one is we
can't free them up without touching this
pointer here and we can't touch that
pointer because we don't own it when
we're holding the socket lock and
secondly if we did free them up we'd
have to free them up into a free pool
and that free pool would have to have
some sort of synchronization mechanism
and that means either a lock or atomic
operations and we we don't want to do
that cuz that's expensive so we just
leave them there we've consumed all the
data now and they stay there essentially
until either we need some more memory
and we'll go and look for it or
alternatively next time some data
arrives on this socket so new packet
arrives gets appended to the list and
we'll have a look at the head pointer
and we'll say while that pointer isn't
the same as the extract pointer through
those packets up unfortunately we can't
free the last one because it would
violate the protocol there's there's no
way that you can get rid of the last
buffer without holding the stack and the
socket lock but we consider that to be a
small price to pay and if you really
need that memory you can you can just
normally if you're trying to free up
memory you'll be holding the stack lock
you'll come along try and grab the
socket lot using a tri lock operation if
you can get it then you can free that
buffer up if you can't and you just say
hey tough luck we'll live without it
transmitted paths a little bit more
complex first of all again we've got a
serialized multiple threads talking to
the same socket so we do grab the socket
log we grab some buffers here we don't
want to be contending the stack lock but
we do have to get some buffers from
somewhere so we have a shared pool of
buffers on an atomic stack based pool
essentially so here we are using a bus
lock top operation in order to grab some
buffers we grab them we fill them with
data we don't bother to initialize the
headers particularly we might initialize
some of the headers the bits that don't
depend particularly on knowing what
state the sockets in but essentially we
leave those alone so we just put the
data in the buffers then if we can get
the stack log we'll grab it and we'll
put the packets in the send queue
assuming we've got some congestion
window and some receive window available
we'll send a packet and in this example
we haven't got enough window available
to send both of them so that one stays
in the send key this one goes in the
retransmit queue so that's the queue
where data lives until it's acknowledged
by the receiver in other words we have
to keep it until the receiver says
thanks I got it now that's the easy case
that's the case that we were able to get
the stack lock without blocking if we
weren't so now we've filled a couple
more buffers instead we check it on this
thing called the pre queue this guy
atomic bus locked operation so we can
put stuff on here without holding the
stack lock and we can take stuff off
concurrently so adding buffers to that
queue and taking buffers off that queue
can be done concurrently now the problem
is we can't just leave them there we
have to get them into the send queue and
potentially send them out on the wire
but on the other hand we don't want to
wait for the stack lock in order to do
that because that's going to kill us
we're gonna have to block and clearly
that's going to hurt performance so
instead what we do is that we Marcus a
bit
in the stack lock that says when you
finished what you're doing please go and
have a look at the sockets that are on
the deferred queue the socket is put
into a link list which is also on an
atomic data structure of sockets that
contain work to do and when the guy
who's holding the static lock tries to
release it he gets forced onto a slow
path and before he releases the lock he
will take these buffers remove them from
the pre queue move them on to the same
queue and have a look see if he can send
some more data so the first buffer to
Scotland knowledged
and maybe maybe now this time we can
send two more buffers job done no
contention the only time that we all hit
contention on the send path is if we run
out of buffers in the the pool of
buffers that can be taken without
holding the stack lock and what we do is
we try and make sure that that pool has
about the right number of buffers in it
and the way that we do that is whenever
we free up buffers that have been
acknowledged we don't return them to the
sort of central pool we return them to
what we call the async pool which is the
pool where you can grab them without
holding the stack lock the really nice
thing about this model is when we filled
those buffers with data that's a mem
copy that's the expensive bit that takes
all of the time on the transmit path the
rest of it is just pointers and a bit of
protocol which is really really cheap by
comparison
so the expensive bit isn't done while
holding the stacked log only the cheap
bits are and therefore the amount of
time spent holding that lock is small
which again helps us avoid contention
okay so it's always nice to be able to
see what's going on inside your network
stack and that's a little bit trickier
in the kernel than it is at user level
obviously one thing you can do is just
attach a debugger follow some pointers
and see what's going on we've got
another solution though which is called
stack dump and it's this it's just a
process unprivileged although you do
have to run it as route essentially it
opens a special kind of file descriptor
on our unload device and says I would
like to attach to
this particular stack and then does M
map and gets a copy of that stack down
at user level and now this guy can do
anything he likes he can inspect the
stack he can grab the lock in order to
stop anything from happening so that he
can fiddle around and see see what state
were in he can grab sake locks he can
mutate the state and basically do
anything this is really useful for
debugging really useful for profiling
your applications you can look at all
your sockets work out how much data is
go through them and see what bandwidth
they're getting and see if it's fairly
shared between the sockets all sorts of
stuff so here's an example output here's
some information about a particular
stack information about the the lock
it's not locked at the moment and
there's no deferred work waiting well in
fact there couldn't be because if it's
unlocked
they can't be deferred with interrupts
are not enabled here's some information
about how many socket buffers it's are
in use twelve sockets at the moment
here's some information about the packet
buffers have been allocated here's
basically this is information about the
hardware interface the transmit
descriptor ring the received descriptor
ring looks like we've got 13 transmits
in progress at the moment and here's
some information about time this track
the stack clearly has to keep track of
time so that it can keep track of timers
this is saying that the last time the
stack was I shall not sure that is my
apologies this one says last time we
were polled was it this time this one
says what the time is now and that just
says that's how long ago it was that the
stack was last polled and here's an
example output for a socket so your your
fourth tuple of IP addresses and port
numbers is in the established state not
currently locked this is state that that
relates to how we support blocking and
waking up processes here's some socket
flags it's got them the non-blocking
flag it's got tcp no delay enabled it's
got a filter so that means it's got an
entry in the hardware filter table
so a listening socket or an active open
socket would have a filter a passive
open socket wouldn't have a filter
because it would just share the filter
that the listening socket is using
here's just some generic socket
information here's all the send state
sequence numbers windows congestion
management no congestion at the moment
we're open here's the receive state
again sequence numbers fast path that is
enabled fast path basically allows you
to avoid some of the checks that you
would otherwise have to do whenever you
have any sort of out of order data or
anything or any congestion you get
pushed off the fast path here's the
state of the receive q 11 packets are in
the receive queue at the moment this
means there are that many bytes of data
in the received queue no out of order
data and information about the Windows
bit more information about MSS is and
user IDs and all sorts of stuff and here
at the time is associated with this
socket we've got some data in flight so
that's data that we've sent but that
hasn't been acknowledged yet and so we
have to have an RTO timer running so
this is a timer that will fire if we
need to retransmit data and we've also
exhausted the the window that was
advertised by the other end so we're
running as edwin timer and the point of
that is we'll have another go at sending
a small amount of data we'll just send a
probe if this timer expires and that
protects you from acts getting lost okay
so finally what is all this add up to
does it help performance I probably
wouldn't be here if it didn't help at
all so our test platform is is just a
really standard server possibly even a
slightly underpowered one by today's
standards it's got a single quad-core
xeon processor not horribly fast
reasonable amount of ram standard intel
chipset it's got one of our solar storm
NICs in it CX 4 so that means it's not
actually running over the 10 gig base t
it's just got a very simple fire in this
example back to back so we've got no
switch and we're running standard Red
Hat
enterprise so first of all latency
simple ting ping-pong so that just means
two applications on different nodes this
guy sends the other guy a message he
sends it back how long did that take
divide that by two that's your half
round-trip latency now first of all
using that v-neck interface that you saw
I can actually construct a very very
simple user level process that doesn't
do any protocol at all so it doesn't use
onload all it does is construct a packet
in advance in a buffer and then say send
that packet to you of a guy and the
other guy will send it back again no
protocol at all all it is is pinging the
packet backwards and forwards so in
other words there's no overhead from
intercept there's no protocol there's no
locking there's no nothing so this is
measuring the hardware performance both
the PCIe bus and also our NIC and the
fie and the time it takes across the
link and that takes 4.2 microseconds
sure
yeah so the question was it was at a
single packet or thousands of packets
and averaging and the answer is is a
million round trips and we're taking the
average here's what the kernel does in
this particular configuration eleven
point two microseconds and here's what
unloads getting now it looks like we've
halved the latency and we have the
important question is how much of the
overhead have we taken out so this
number is simply that number minus the
hardware component and so this is
telling you that a single send plus a
single receive is costing you seven
microseconds of CPU time with the kernel
stack and it's costing you 1.1
microseconds of CPU time with onload as
I said bandwidth basically comes down to
your overhead assuming that you're not
limited well depending on with your link
limited or not here's what happens when
we compare onload with the kernel first
of all both of them can saturate the
link and for TCP when you take into
account protocol headers that's nine
point four Giga bits the interesting
space is this space so what we're seeing
here is because we've got reduced per
packet overheads and reduced per message
overheads we can saturate the link a lot
earlier and for your small message sizes
we can pretty much double your
throughput but onload is running on just
one CPU here not because we've said only
run on one CPU but because we're only
doing the polling of the stack in
response to the application requests
because it's a single threaded
application it will naturally only
consume one CPU core while it's doing
this benchmark the kernel however was
configured for this tester on two CPUs
the application is bound to one core and
the interrupt that the the hardware
generates is bound to another core the
two cores are sharing a level two cache
and that turns out to be the best
configuration for performance and what
it means is that this guy to get this
performance is using rather more si
you than this guy and here's what
happens if you bind the interrupts and
the application to the same core so this
is really I think this is the fair
comparison here's the performance that
you get when you've only got one core
available to you and finally I've added
another one into the mix what you've got
here is the unload stack but without a
user level intercept so if you remember
we had the case of an untrusted child
where we we forked off we didn't exact
but we didn't allow them to map the
stack into user level and that means
they still have to access the stack
through the the kernel interface in that
case you lose some of the per message
benefits it becomes a bit more expensive
for each message that you send
but the per packet overheads and all the
other overheads that we've cut out you
still get that benefit which means when
you get to the larger message sizes you
basically get the same performance so
here's a bit of UDP performance sorry to
stick with that one
so one send a single socket kernels
getting best part of half a million
messages per second which is pretty good
that's only a couple of microseconds per
message onload doing four times the rate
again what I've done here with the
kernel is I've said run on as many CPUs
as you like which means in practice it's
going to be using one and a bit cause
for this particular test if you have two
senders so this is two completely
independent processes you're going to
get a bit more from the kernel but at
some point you're going to get a
bottleneck and that's because you're not
able to spread the load nearly as well
over the available cause unload however
in this example you've not quite got
double but you're not far off it you're
not actually going to go any higher than
that unfortunately because you've now
hit Hardware limit on Nick won't do more
than four million messages per second
but the point is unload is scaling
nicely when you add more work yes
this is a - 618 - eight standard Red Hat
Enterprise five colonel so it's not
quite the bleeding edge but what we've
noticed and and I think other people
have noticed too is that they did a lot
of good work in terms of performance
between the early to six Colonels and
around the to six sixteen time and since
then not much has changed so performance
hasn't changed that much I think you'd
see similar sort of performance with the
latest stuff yes yes right so this is
this is the colonel doing the best that
it can
but using more than one core yeah this
is what happens if you only allow the
colonel to have one core and so now I'm
saying like for like colonel on one core
on load in the colonel with one core and
unload at user level with one call right
yes so if if you took the sorts of
techniques that we've taken and applied
them to the kernel stack to reduce
overhead you could hit this line
absolutely absolutely so the the user
level part of the of the equation it
buys you this gap here so in particular
if your application sends massive
messages you probably don't need the
user level aspect of it
but if you want the absolute best
latency and you want decent performance
with small and medium sized messages
then user-level buys you something but
user level is only part of what we've
done as you can see okay so that's UDP
transmitted his UDP received performance
I'm showed three things here this is the
kernel forced to use only one core this
is the kernel when it's allowed to use
to pause and this is onload again it's
naturally just using one core in this
example by the only reason that this
doesn't go down to zero is because I've
got ward but what's happening here is on
the x-axis I'm increasing the load so
we've got a transmitter that's sending
anywhere between five hundred and three
million packets per second so five
hundred thousand and three million
packets per second and we're measuring
how many the application receives there
comes a point when the system gets
overwhelmed and you start throwing stuff
away the kernel reaches that point a
little under six hundred thousand and
what happens then is as you're offered
load increases you spend more and more
time in your interrupts handler
processing receive packets less and less
time in your application so therefore
you receive less and less packets and
eventually performance bottoms out and
that's a problem that's called receive
LifeLock that was covered in the
literature years ago that was a really
really big problem until we came across
multi-core machines and then of course
now actually this is the behavior that
you get which is that interrupts will
happen on one CPU probably saturate that
CPU complete
but at least your application will be
able to still run on another core and
make some decent progress so really this
is two calls with the kernel this is
onload using just one call and as you
can see it's getting rather more packets
per second through another question so
the question is in the round-trip test
where we polling or using interrupts and
the answer is for this example we're
using polling now that is appropriate in
some domains and it's not appropriate in
other domains so what we're doing is
we're saying when the application
invokes receive and the data isn't yet
available we will sit there in a tight
loop calling Stackpole over and over
again for all we usually use 100
microseconds and if the data arrives
within that hundred microseconds great
you haven't blocked if it doesn't arrive
at that point you go into the kernel
enable interrupts and you block and what
that means is that one of the reasons
that you're saving time in this
round-trip test is not just because
you're reducing overhead it's also
because you're cutting up cutting out
the time that it takes to wake up and
that's caught the time that it costs to
process the interrupts that isn't
appropriate for all applications but it
is certainly for some and particularly
the HPC ones which we'll come to in just
a say
um well actually so what would what
would do bit effect be yeah so the
question is really about the trade-off
between when do you want to poll and
when do not want to poll well the if if
you enable interrupts and you don't have
any interrupt moderation then your your
round trip increases in the onload case
I'm not I'm not absolutely sure but it's
probably a couple of extra microseconds
on your on your one-way latency order of
that in terms of when do we do polling
we have it available as an option which
you can specify with an environment
variable and you can choose how long you
want to poll for and by default we don't
poll at all
and the reason for that is that we want
good performance on multi-threaded apps
and if you've got multi-threaded app
polling is a disaster assuming that
you've got work to do because you have a
thread sitting there polling waiting for
data in its socket it's consuming CPU
time that might well be usable by
another thread that that can make
progress so by default we don't do it
because the absolutely care about most
are sort of enterprise apps should we
say I did use it here because I was
trying to illustrate the round-trip time
the people who care about round-trip
time are the people who are doing HPC
type stuff the enterprise apps don't
really care about round-trip time they
care about throughput of messages and
transactions and that sort of stuff so
different application domains
you can do but it doesn't make any
difference okay so I did I did for this
test yes that's right so
I'm not sure I'm somehow practicing
context which is saying that
I think we have to be because of the
gonna be event-driven right right so in
actually in this this number both of
these numbers come when you do a
blocking receive this number in fact
both of these numbers barely change if
you do non blocking receive so if you're
going in and out of the receive you add
a tiny bit because there's a chance that
the packet arrives at a point when
you're not inside the received code so
whatever it costs to get from your
application code into the Stackpole code
that the code that actually looks at
hardware events that's the very thoughts
the amount that you potentially add to
your latency if you're doing non
blocking and polling it at the
application level however the reason
this guy won't generate much benefit or
any benefit from that is that he still
got to wait for the cost of an interrupt
to happen and all the stack processing
and that will be asynchronous with
respect to what the applications doing
so the application is doing its poll in
and out in and out all it's doing is
looking at the receive queue of the
socket in the kernel example in the
onload example he would be going in and
out in and out but whenever he looked at
the receive queue he would also call
Stackpole and have a look at the
hardware queue so for all the reasons
that we've talked to that for the
reasons that we reduce overhead that
we've talked about earlier he gets the
benefit of those and this guy doesn't in
this case the data path is still got to
go through the interrupt service routine
even if you're polling now there's no
reason why the kernel stack couldn't be
completely rewritten and totally
adjusted and use a user driven mechanism
and if they want to do that that's fine
by me
okay so the final numbers I've got are
just an example from the HPC world so
MPI is a massive message passing
interface and it's a standard API that
lots and lots of HPC codes use to get
their parallelism and it's an abstract
interface to communicate between
processes and their model is typically
one process per CPU call some on the
same box some spread over other boxes
and the particular characteristics of
these applications is that they're
generally speaking not multi-threaded
they generally speaking expect to own
the call that is you don't have to do
anything else on the same call except
perhaps process hardware interrupts
therefore this is the classic example
where polling is just fine and if you
want to do that you can and in fact some
of the MPI implementations will do the
polling for you which is to say we don't
even need to do the polling ourselves in
onload that the application itself will
go in and out using non-blocking sockets
but the first thing I've got here is is
to show you the trade-off that you've
got in the kernel stack between using
interrupt moderation and not so
interrupt moderation says I want a
minimum minimum interval between my
interrupts let's say 60 microseconds so
that means if an interrupt fires another
one can't happen for 60 microseconds
that means that you're never going to
get a great round-trip time if you want
the great latency the lowest possible
latency you're gonna have to turn
interrupt moderation off and that does
give you the best latency so that's this
line down here however when you get to
decent message size is your bandwidth
suffers so interrupt moderation off best
latency interrupt moderation on best
bandwidth the next slide just compared
the kernel stack with onload and the
kernel stack has been given interrupt
moderation on in this case so it gets
the best bandwidth that it can so really
it's doing the best that it can at this
end of the curve but unload of course
wins across the board it's getting the
benefit of the lowest latency and and
it's getting the benefit of the best
bandwidth and if onload happens to get
itself into a mode where it's using into
drop switch which it can do depending on
the application behavior
it'll also still benefit from interrupt
moderation because it algorithms applied
as well in case you're wondering about
the latency the kernel stack when it's
got me interrupt moderation off is
getting about twelve microseconds
latency here sorry
versus the sixty or so that it's getting
here whereas onload is getting
application to application MPI latency
of just over six microseconds this is
this is just like the the round-trip
test that I talked about earlier the
only difference is that the message size
is increasing and this one is very
similar but they're sending data in both
ways at the same time so it's a
bi-directional bandwidth test one thing
I haven't really had time to talk about
much in this talk is scalability and I'm
I'm not going to be able to because
there's an awful lot to talk about and
it really is a whole nother talk but
there's one really obvious way in which
this scales which is if you've got
separate applications and they're using
separate stacks then there's absolutely
no crosstalk between them at all and
they all scale beautifully across your
course but the architecture does scale
very nicely across the course even if
you do have a single stack shared by
multiple threads and the reason for that
is that we can use the RSS algorithm
that kernel stacks use to spread the
load across the course and the way that
the kernel does that is it spreads
packets into multiple receive rings at
the hardware level and then each receive
ring is handled on a separate core but
you've still got one stack really and so
you still get a bit of crosstalk between
the processing of those receive packets
what we do is we have a stack per core
essentially the whole of the stuff that
I've talked about you replicate once per
call and therefore there's pretty much
no crosstalk at all and it scales very
nicely and fade that's all a lot of time
to say about scalability so in
conclusion what we've done is improved
performance but remained completely
compatible we think with everything out
there now of course we have bugs which
means that you're bound to find
application
behaviors that trip us up but whenever
we find them we fix them but we're
Ethernet
everyone loves Ethernet for all of those
reasons we use the standard socket
spazio api's and we don't change the
environment that the application is
running in to the to the extent that we
possibly can and finally with tcp/ip we
don't ask for any exotic protocols on
the wire therefore you can stick on load
on one end get single ended acceleration
you don't have to deploy it through your
whole network and we're releasing this
as open source so if you're at all
interested in seeing how the code works
let us know and we'll point you at the
download thank you very much are there
any more questions
hello your test platform that you
present it was a single socket system
that's right and all cores are sharing a
cache level would you expect what do you
expect for differences in performance if
you have a multi socket system either to
socket or four socket system so the
question was we've got we showed as our
benchmark system a single socketed
server it's a single quad core it
actually has two level two caches it's
it's the Intel tape tooled to drill
cores and stick them together solution
so we have actually got two caches in
that example there's no tile free in
that architecture no but the question
really was what changes when you go to a
multi socketed system and how would that
affect performance the first thing is
you can get frontside bus limited in a
single socket configuration and that
means you can't quite saturate both
directions at once with a single socket
and what happens when you do that is no
matter how much CPU grunt you throw at
it you spend all of your time trying to
suck data through a narrow pipe and your
CPU utilization goes up and your
performance doesn't so going to dual
socket is a big advantage if your
frontside bus limited in other respects
if you have an application that tends to
share state
between threads that happen to be
running on different packages then it
will tend to be less efficient than if
your application is clever enough to put
things that happen together on the same
cache and that is true there's nothing
that we can do about that
shall we say that that's a problem that
applies to any stack
if I counted we trance
is there any kind of coordination
between the
if you are
so the question really is do our user
level sockets get reflected in the
standard system tools so when you do
netstat will you see our sockets at user
level as well as the sockets that are
coming from the kernel stack the answer
is yes and it's disgusting I'm I'm
utterly ashamed of the way that we
implement that we actually have to go
into the proc file system where this
stuff appears and we insert a new proc
file in place of the one that was
already there we sort of replace its
entry in the directory if you like so we
get it we then query the original file
and say what are all the kernel sockets
and then add our own information about
user level sockets to the end of that so
the user level tool sees all of them so
sorry
confident that you are by the isolation
that you know news from ablenet was
something to distract the answer is that
is in progress and not 100% complete yet
and if you download the code you will
you will be able to find examples where
it's not complete and I certainly
wouldn't recommend that you take this
and run it on our untrusted applications
today I believe it's doable I believe it
is possible to completely solve that
problem and the real key to that I think
is having a very formal layer software
layer through which you access the
shared state low level primitives and
don't allow code to go around that and
make sure that those primitives at the
point at which you're secure when we
compile for user level those primitives
will effectively be no ops all they'll
do is add to the indirect offset onto
the base pointer and give you the
pointer back we don't particularly mind
if user level destroys its state and
then causes itself to seg fault the
kernel however those little thin
wrappers will turn into the techniques
that I described earlier where we apply
a mask or a bounds check before we
construct the pointer what we don't do
is we don't we don't say if bounds check
good do stuff else follow arrow path
what we do instead is that we just
guarantee that we all will always return
a pointer that points into the shared
State yeah now it may point at complete
rubbish
if the shared State has been corrupted
but it won't actually cause the kernel
to fault
every every every time we generate a
pointer we generate a check so for
example if we're following a linked list
of sockets every time we get a number
that says socket 5 we apply a mask to
that number that will generate a number
that's in a range a power of 2 range we
then look that up in a table in the
trusted State every entry in that table
is guaranteed to point somewhere in the
shared State now we've got a pointer
which is in the shared State
it's guaranteed to be far enough away
from the end of the shared State that
when you indirect into that data
structure from that pointer you'll
always get a non faulting address so
most of the accesses don't have checks
creating pointers have checks
TCP stack protocol features new
options for TCP
things like knowing
first of all people that you've been
comfortable with so that there are
really two questions there the first
question is how do we deal with
evolution of the the system call API and
exciting new features and the second one
is is a question really about how we're
going to keep this thing going and what
are we planning to do with it I guess so
the first question sometimes we don't
have to do anything to cope with
evolutions so for example we don't have
to do anything at all to support select
poll any poll except implement one of
the file operations in the standard
Linux struct file operations table and
there's a single call they're called
poll which queries the state of a file
descriptor in the kernel and that's how
select poll and the poll are all
implemented using that one mechanism and
also async i/o as well I can partly uses
that at implementation however you don't
get the best possible performance that
way so we do also intercept select and
poll although not Yeti poll we haven't
done that yet at user level and
accelerated at user level to get a
further benefit so the good news is some
new features just happen which is to say
if we can compile on that kernel version
then the feature will be there by virtue
of the fact that we look like a kernel
device as well as looking like a user
level device another example of that is
send file send file is done it purely in
the kernel just by implementing the send
page file operation so we don't we don't
have to do anything clever to do that
and in fact if you don't implement the
send page operation
the colonel will use your right
operation to do it itself slightly less
efficiently so that's good too
the second question is how is this going
to evolve and how do we keep it useful
we are actively working on it
and as I say we we are releasing a
design source now we're happy for you to
download it and play with it what we've
not quite yet got is the resource to
turn it into a sort of full community
project but that is our goal that's
where we're heading later this year and
at that point what would be really
helpful is to understand what
applications people are really
interested in because that's where we
can concentrate our effort in terms of
validation and making sure that it's
really robust for people because one of
the problems that we found when we
started doing this stuff is that the
variety of behaviors that applications
display are just mind-numbing ly huge
and there are all sorts of sock
adoptions that you've never heard of
until you find that your application
doesn't work and you dig in to try and
find out why
it's popular
work my bonus like it
some stranger's
lost that role it investigates
there's no build down start playing with
it
yeah but the other thing is that when we
very first started doing this we wanted
it to be zero effort install our drivers
it'll just work and that meant
effectively accelerate by default and
that means we're exposed to the whole
gamut of every application that might
appear on a box we think probably
although that was a very good exercise
in terms of finding out what
applications did you can't possibly hope
to see every application in in our labs
so instead now our approach is
accelerate explicitly you have to say I
want to accelerate that application and
by default applications will just run
over the kernel stack
actually this business place for
definite which
so we're trying to come on firm
foundations then qualify
stop
occupation matrix
outside of that matrix
any more questions thank you very much
indeed sorry one more types of
api's which might get up here and copy
hacks
yes so the question is have we looked at
any more other api's that would get rid
of mem copy overheads the answer is that
we're not interested in implementing
anything non-standard because we just
want to plug in underneath existing
applications and just accelerate them
obviously we thought about them shall we
say and there are other people who've
been thinking about those things all
Rickett refer the maintainer of the see
library I know he Linux in Auto learning
symposium year before last he was
talking about exactly that I'm not aware
that they've implemented anything along
those lines yet but essentially the idea
would be on the receive side instead of
the application saying here's my buffer
put data in it the stack the application
will say give me some data and this the
stack would say your data is here
consume it from this buffer and on the
transmit side
okay the applications got to say here's
my data but that's actually not a
disaster one thing you can do on
transmit is you can do zero copy if you
arrange things right particularly if you
do a sync i/o but a sync i/o is not
something that's well supported in Linux
sockets sockets and therefore no one
uses it if that became better supported
by a decent API then we would definitely
do a sync i/o and we'd see probably a
big benefit from it so another question
yeah crack in that presentation he
didn't know what he was talking about in
the meantime he's gone back and done a
lot more research and he's wrote a very
good paper that showed up on lwn net and
I would refer people with that instead
of his talk at all uh jazz strong a
Google employee ripped into at that
conference during the talk there if you
see the rest of the notes
yeah I was there pare down my point my
point was simply that people are
thinking about it it's been proposed but
we we probably wouldn't build it into
one load until it became something that
the kernel supported on the other hand
if people wanted to play with it and try
it in onload they could and that'd be
great fun okay thank you very much
indeed</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>