<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Analysis of Big Multidimensional Data | Coder Coacher - Coaching Coders</title><meta content="Analysis of Big Multidimensional Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Analysis of Big Multidimensional Data</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DAXIWd_LDgI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you guys for coming let's give a
warm welcome to Shmuel friedlin from the
university of illinois in chicago he's
gonna be talking about the analysis of
big multidimensional data thank you very
much it's my pleasure to be here just
the sort of I volunteer myself to give a
talk and since I don't know what is the
audience and I was told that even Google
one should be relaxed so I didn't
prepare that much what I sort of
collected is the things that I gave
lectures in the last two years on the
subject and and maybe it's interests of
for you guys or not so please feel free
to interrupt me and you know and to ask
questions okay so so what I'm trying to
do let's see if there is a pointer here
so I'm trying to sort of to tell you a
little bit about what sort of what I did
in this area of analysis of some data
now usually one is interested at least
in two-dimensional data like a picture
or will say I have more like web pages
and so so the question is how to okay
let's see so maybe it's maybe it's more
calm music I was 10 here so so the
question is number one is that usually a
big data which is multi damage
dimensional it comes there is a lot of
reductive information like a picture you
know there is a lot of repetition and so
what one wants to do want want to
storage the data is somehow to have a
mathematical tool to sort of to reduce
to reduce the data in such a way that
you have most of the information to
store it the second thing is the noising
data so sometimes you see that something
doesn't fit so how would you if you
the data and in certain problems I'm not
sure if this is related the problems
which which you are doing is there is
sometimes a missing data so you look
let's say Nana photo and you see
something which are blank spots or maybe
you seem corrupted spot so is there any
way to correct it mathematically okay so
so essentially for two dimensional data
as a photo and some other stuff we are
think was there always many ways to do
that
but one very useful tool that is a
singular value decomposition and it was
was applied to many different problems
and usually it works very good now so
this is sort of I will talk this will I
will show you the strengths and what
kind of problems one can do if you do
this two dimensional things so for
example how one can do a face luring
low-rank approximation and so so this
would be in order to reduce the storage
then if you suppose you have sort of
let's say if you go to DNA which has
also a lot of data and he we are talking
about microarrays for example so you
want to compare you want to compare like
two things you want to compare like two
certain photos but it's not exactly
photos and you want to compare let's say
photo of elephant or a photo of human
and see what is common well and so so
that is there is also a way to do it
using SVD now and that's I will discuss
in detail now more much more interesting
or let's say much more challenging are a
data which has which is
multi-dimensional
three dimensional picture or you have
voice so those are in mathematics are
called tensors and now the Siri singular
value decomposition breaks down now it
is very important for example in an area
of face recognition because you want you
know to see a person and to recognize
it's on the photo for many various
reasons I don't know if Google is
already in this area but maybe not but
it is very important and so for that
usually what one does is take two it's a
photo of the same person in several
profiles and so it has a tensor and how
to identify that so this mean this is
sort of mathematically is very wide open
okay so now so this was sort of what I
am trying to do and if you really are
interested in more details and you can
find some of that on my website so most
of the things that I have is lectures
that I gave I didn't prepare any special
thing for Google so now let's let's talk
about let's see sort of the first okay
let's see if I can do it here you actual
so that's I mean let's see if it pages
okay so she so this talk I gave about
well a year ago so so here sort of we
can see the problem is with the colors
okay here at least I don't know maybe
from the distance you you look you can
see it but anyway so the problem is as
follows so here is suppose you have a
picture so what is a picture is
essentially let's say it is if you have
a standard picture it would be say maybe
5 by 12 by 5 by 12 matrix and each and
each point you have a pixels okay maybe
it has a color and a density but here in
this case let's assume that it has only
one variable so because we are talking
what may
if this each each let's say point here
at also more information like what kind
of color etc this would be already a
tensor or you so sometimes you have the
DNA microarray which would be like sixty
thousand the number of genes and the
number of experiment what you do now
here is maybe what Google is more
interested is suppose you look at the
web pages so and you so then a IJ this
entry would be the number of times web
page a was accessed from web I okay so
so that would be well very big matrix of
course so now so so in this so how about
condense the data and storages to store
it store it effectively so so for that
so let's so just before I start to do
mathematics because s I'm not sure if
your interest so let's look at it in
this example so what you have here okay
so I better look what's there because I
cannot read it so so on this side you
have the original picture now this
picture comes from I think MATLAB
toolbox now and this so this is a
compression here how big is the
compression okay so this would be the
compression maybe it would be this is
let's say 80 maybe certain maybe 33% so
this is actually maybe the compression
would be close to 512 maybe okay
maybe one over eight okay so let's say
you just have certain perception of
information here now can you tell the
difference well I mean I cannot tell
maybe if you look little bit a little
bit more precise this is that here is
maybe is slightly worse but anyway so
this is so this is example number one
and then you'll explain what we do here
let's take example number two so this is
the same exists
so this is a smaller picture the pixel
is 256 by 256 and so here I think you
can see that this is slightly less clear
than this one so okay so here you can
see maybe the difference so okay so what
so what so what what are the tools here
now so again I mean I don't want to mean
I'm not sure about my audience I don't
know if I should go to all these
formulas but basically so I'll just say
in words the formulas are not that
important unless one needs to use this
so so what is SVD is singular value
decomposition so if somebody remembers
about symmetric matrices well it would
be spectral decomposition but this
usually a matrix this sort of this
matrix it doesn't have it doesn't have
to be a square matrix you don't have you
know look at even the screen here it's
not it's not square so so the matrix is
rectangular so what you want to do you
want to find two orthogonal matrices
such that au times Sigma V is going to
be your matrix now Y orthogonal because
orthogonal sort of they distort as less
as possible in other words orthogonal
is that you ever talking on rotation of
your coordinates and Sigma this is a
diagonal matrix oh so the information
just sits on the diagonal okay so so now
when when yes well it is very close but
not precise suppose if it was a matrix
was symmetric its then symmetric yes it
would be the same but if metric is not
symmetric we have to symmetrize that so
essentially what we take we take a times
a transpose and you get a symmetric
matrix so this gives you sort of the
auto or eigen vectors of a transpose
which called a single left or right
eigenvectors not important right now and
then you have to use that to obtain the
other part so it's very close what you
say but not exactly so for symmetric
matrices answer yes for non symmetric
matrices you have to do this eight run a
transpose day
okay so in other words so in other words
you're asking me what happens if you
sort of multiply matrix bike or by
orthogonal matrix from left and from
right if that's what you mean okay so
that's let me see if I if I still okay
so I mean if we have if suppose we take
any orthonormal basis that's what you
mean there's anything to do this
eigenvector so you can X you can take
your vector and expand it and let's say
Fourier series you can expand your
vector in another not exactly Fourier
series but but it's the coefficient
would be different but you will in other
words if you can expand it will have the
same vectors that was your question
something oh yes yes that is sort of yes
because but that has anything to do with
the Maivia symmetric so you see unless
you know it's it's it's unless see if
you if you have here sort of this matrix
orthogonal matrix means you sort of you
change a basis you change the basis so
think about it
heirs and now you just rotate it in
several so that's a change by so the
same vector when you'll that's what I
tell to my students if you stand here is
them in New York it won't be the same
but it it would different presentation
so okay let's try to continue so unless
but on the other hand it's good that you
are asking the question so so now so now
we're going back to a classical result
which is probably 100 years old and was
discovered by Schmidt so now so what so
what does it mean that your data has a
lot of let's say
lot of redundancy it means that the rank
of a matrix is much smaller than
symmetric so for example let's say so
how can I sort of convince you or give
you a feeling yes si so I'm looking if I
look of rank of a matrix and can look
let's and each row so let's say here
let's say five twelve row so you can
each row so you look in this row and
you're looking row by row it's almost
look the same so it's linearly dependent
of course there suddenly you come to
something so to say to say that their
data is reductive in terms of patterns
it means that you have many dependent
rows and so therefore so the idea
mathematically is that you have many
singular values which are close to zero
so again so so if you sort of feel sort
of more comfortable with thinking that
about eigenvalues eigenvectors so single
T's you have a symmetric something you
have you have a picture that has sort of
so it has a symmetricity in reality you
won't have it but so then it means that
you have many eigenvalues close to zero
so so you say okay so they are close to
zero then let's just discard them so
there so therefore if I want to do this
is one way there are many ways but we
are talking about this way if you want
to do that reduction is say okay I want
to approximate my given matrix by a
matrix of much lower rank and so it is
standard thing in approximation theory
in reality I want to let's say I want to
take some differential equation with
many frequencies and approximate by only
finite frequencies so what I would do
what I say okay we would like to
approximate in our finite parameter
space as close as possible that is
standard idea in engineering and
automatic so so essentially okay I mean
I'm not sure how much technical I shall
go but just so what you do is okay I'm
looking all matrices of rank less equals
M K so it's called our MN this is a
dimension of a matrix thing 512 by 512
and in so in these pictures that I had
before okay so let's so let's go to this
picture so this was 512 512 so so then
the rank of the maximum rank of the
matrix can be only 512 but there are
many redundant says okay let's try to
approximate here 80 rank so instead of
512 maximum rank each rank so so this is
let's say roughly 6 times okay
and so you see so when you do that if
you do it right you lose very little
information so the question is okay how
how to do we try to know how to take
this matrix B of rank 8 it most it would
be as close as as possible now so here
so when we say to data close as possible
what do we mean so the simplest thing is
to think is okay so how do I measure
distance between two point it has
recorded so I take some of the squares
of the difference coordinates right so
here so this is so this one this one you
can think about it sorry as okay sorry
as a sum of the squares of all the entry
and square root because the distance in
mathematics is called we call it the
Frobenius norm just in honor of some
german mathematician Frobenius all these
guys lived in the beginning of 20th
century so but you can also use other
Norse but right now it's we don't want
to be technical so so we want to find
approximate our matrix a or data with
lower rank and how to do it right and so
in what is the right track
so for so that's where the SVD
decompositions come into picture it says
that essentially if you do the SVD the
composition of a and then you will take
this if you think about it let's say in
terms of symmetric matrix you take only
the I let's say like in music instead of
instead of take all frequencies I'm
going to take the first 50 huh with no
top frequencies and corresponding modes
this is it so this is exactly the same
idea but now huh but now how to do it
well it's it's mathematics okay so and
the question is okay so what is
essentially what would be how many modes
I should take first 100 or first 50 well
it depends you see you look at that
let's say at your picture and see well
actually I have only 50 big modes soon
they'll take the first 50 so how do we
know that well it's sort of it's a it's
more now becomes engineering problem in
art now now so what you so when you had
this picture and you look at this
picture of what is this thing that
Google Street what's this 3 2 so you see
also sometimes it's blared right well
what do you have there well you have no
it's there now you can say well because
we took many cameras put together but
you have noise so what would be the
right thing is maybe the noisy so how so
one of things the noise it it means you
should filter out the lower frequencies
which are this one if you do that you
may improve it's not always true but so
you can do that ok so so this is so this
noise reduction applies an image
processing DNA microarray and data
compression now so now how about data
compression why it's important in data
compression ok so so and that is
essentially a problem in a very big data
that ok so
so well what what you do you have you
have many so you you take vectors which
sort of rows and columns so in other
words you have several since several
rows and columns of the same pictures
not exactly and then you just several
take several of them instead of let's
say a thousand you take only 100 or if
you're lucky maybe 10 so suppose it was
only 10 so you have to take a search
only of 2 times 20 vectors which is much
less so that's how you do a storage okay
so now yes questions okay so now the
next thing is sort of so the next thing
is what one can do is well and that is a
problem actually even somebody contacted
me he had a big DNA data and and maybe
it didn't have a supercomputer so how do
you if you have a very big data how do
you process it you usually you cannot
process it let's say online because it's
millions by millions so what so what is
the solution so so that is essentially
what is this part of the stock I'm not
going to all technicalities is
essentially is how would you how would
you try to reconstruct your data from
taking but it's you assume that your
data is at least two-dimensional by
taking some by taking some number of
rows and columns and try to reconstruct
it together now the in in mathematics
and also well in applications you know
you don't know which let's say which
rows that you pick up and which columns
that you pick up would really contain
most information so therefore the best
solution is take it at random in other
words you have some something suppose
you have a million okay so it's sort of
like going to Las Vegas
say what were all lottery right so you
just said well I mean it's I just call
my computer to pick up let's say 100
numbers at random
okay so that is random process so in
this way you mean this way you means
that you don't have any information and
the information is spread is sort of at
random and when if you pick up enough a
trend oh but not too many you will have
most of the information so this is in
many algorithms that's how it works oh
that's so here sort of in this I in this
let's say topics as I suggest how to do
that but I don't think that there is any
point to go to the exact algorithm
unless you will ask me to to give the
details now what what one can look here
also so sort of okay so here is so so so
what we do usually we do something like
simulations now again this is terrible I
cannot see anything I hope you can see
something well okay so so what we see so
we see this lifting body image now so
the it's it's always in mathematics and
engineering is the question is of price
you want to do something better it costs
you more money and in computation means
you it cost you more time so that is
then you say can you produce something
efficiently fast enough and cheap so if
in this case cheap means less
computation so so this is so if you want
given your data and you say
two-dimensional I want exact SVD and
then then it cost you much more and if
you take the let's say an approach that
you pick up at random certain number and
do some computation so this is this is
sort of this graph shows you that after
maybe after a few so this would be on
the bottom would be sort of the best
that you can achieve
using rigorous computation here you see
that after maybe four or five you
already are at the best but essentially
if you look at the scale I mean this is
10 to the power minus 4 so it is good
enough so this is simulation number two
so this is for this for cameraman okay
let's see okay okay well that we can do
it well okay so let me so let me take
back and just increase their yeah so
let's try to increase it okay first of
all let's take one picture and let's try
to increase can we do that okay so now
can we see now right okay okay so okay
so right so let's see so here so right
the error here I think it's in the
Frobenius norm so so so I want to
approximate rank 80 let's say for
example right so say so this is if I
doll if I this would be the best I think
and here if you do this maybe what if I
remember that let's see it's say okay
let's see
oh so first of all the error I would
think it's it's a relative error so you
take so you take a okay let's let's go
back and let's so let's try to to
explain so what is the error here okay
right so
so let's sue now so let me just take it
slightly smaller if I can do that just
to see okay
so so here so let's look here so here is
I think that if you get up to zero or
something that will give you that you
restore the full matrix otherwise you
take a - a K so the difference in the
Frobenius norm divided by so it's always
a relative error now here we see you say
well if I want drink a tea my original
matrix wandering so this would be the
best I can do
now here it tells you that that here is
that what you do with the roundup with
you you don't have the full sort of SVD
decomposition so that's so you you get a
certain limit as far as I remember okay
now so what else we can do here let's
see okay so maybe slightly go back to a
smaller scale and then okay that's I
think that's let's see so
so again so here this this this the
graph just tells us that how you mean
it's that you have sort of the price
that you pay pay maybe six percent you
you have error in six percent if and but
you have a speed-up of south of three
times okay so so so okay so what is the
next thing so the next thing is so now
so let's go to the sort of bigger bigger
picture so the next topic we can do is
let me just try to make it smaller is
how to how to reconstruct to give to
give you the how to reconstruct let's
say some missing data so again so you
suppose you let's say you did you you
send something to Mars or further away
it send you back the pictures and there
are some spots you cannot do it again or
it's too expensive or you do some
microarray experiment and because it's a
very big you always have some spots so
so first of all you have to decide that
those are spots are missing and you want
to reconstruct just so you want to
reconstruct the spot so how are you
going to do it well again there are
several several ways to do it
okay so let's see I'm not sure it's
okay so that's so let me just try okay
so then so here so what is the idea here
there so this we are discussing now this
missing entry problem so you have so you
have can you read here okay so you have
some data and you're missing so how we
reconstruct it so what is that the idea
is as follows just explained so I assume
that in my data let's say it was 512 by
512 say but the rank was only 20 or only
5 rank 5 so that's all so that's a sort
of I know from other pictures or from
other experiments that the effectively
the right rank is 5 so the idea is here
that I complete the data in such a way
that I am close as possible to rank 5
and if you do that you can come up with
the algorithm which is which works now
so this is so this is the other
algorithm so I'm not going to discuss
you know any more
you mean it's I don't singlets so what
you would like to know that all this the
test but but let's just again so so when
you do like that do simulation so you
compare several methods and you can say
well you know in every papers that you
see as I said well my matters is the
bessel so it's always you can always
cook up a data well we didn't try that
your method is the best on this case is
that our method is sort of compares
favorably the other methods okay so so
what else now the other okay so what
else can you do with this singular value
decomposition so the other topics that
let me just show you what you can do is
to compare so as I said you you have
suppose I mean let me just see I
probably should do something faster than
this one okay well anyway I don't know
how to do it
okay so so the idea is suppose you want
to compare the microarray of human to a
microwave East so you want to see sort
of what is the common features and what
are different features so how would you
do that so for that if you want to
compare two sort of pictures and say
well or two micro sec how would I do
comparisons I see that some functions
are similar and some functions are
different so for that we okay this is
terrible again but we have this tool
beast called generalized SVD again this
is tall
it's not found by me and actually was
successfully applied by only altars that
she she got her ass impeached in
experimental physics in Stanford and
then she switched to DNA and let's say
bioinformatics so so it is sort of the
same tool by SVD but now you do SVD with
two matrices now just to explain briefly
because I don't think that now you
instead of taking matrices which are
sort of orthogonal you cook up a new
metric you sort of change it change the
distance when you measure and then you
have a common decomposition of these two
matrices and from that you can see what
is common and what is not now how what
can you do with that more I really don't
know but that's but that is this this
matter so okay so what else let's see so
okay so now the final thing is about
tensors so let me just let me just give
it briefly so okay so I should take it
smaller so what is so what is the
problem with tensors so tensor so in
this particular talk or we are talking
about three tensors so suppose I measure
a three dimensional picture now you see
when in mathematics we have real numbers
and complex now are in numbers in I'm
not sure if anybody if you use ever
complex numbers but they are sort of
sometimes they're good to have in
physics or in LA electro magnetics now
the theory of let's say SVD or
two-dimensional picture it's all real
you start with rails and you approximate
with real every single the same the
problem with three-dimensional tensors
if you start with real but you want to
again you want to condense as much
information you it's better to do it
when you go to complex numbers and this
is so this other guy right away tells
you what is the the problem now so so so
essentially there is no SVD for three
tensors you can do it if you want to do
it really condense it then you should do
it you should go over complexes so the
standard thing now what the it's done
let's say and for this tensor is called
flattening if you have if you have
three-dimensional pictures you say okay
you know what I'm going to sort off to
sort of okay how should I say so
so if three-dimensional pictures you can
take a pages okay so you said so
therefore each so I'm looking only a
page number of pages so it's
page number 20 now I look at the text
here then I say okay this text is going
to be just one line so so what you did
you took a tensor you took
three-dimensional picture and flattened
it into two dimensions and say aha now I
know how to do SVD but of course by that
you lost the three dimensional picture
so then it's useful what to do is sort
of to go let's say I have three
variables three dimension to flatten in
each different direction to do this SVD
and try to combine them together so it's
not optimal but essentially that's what
it's done in practice and in this
conference last year and Stanford Yahoo
Stanford the they were discussing this
this approach in let's say in face
recognition because if you want to
recognize a face that's that's the
technique that's available right now
well I think that I sort of presented ok
so I sort of presented most sort of most
of the problems in this talk and I think
this probably the right time to to stop
and let you ask questions yes
okay so let's talk about SVD okay well
you have to assume that it's fine that
you have she won't be precise take
double precision that's it I mean that's
all the computations are done usually in
double precision so it's not so you have
to come from I suggest a mathematical
thing yes other questions yes right oh
sure sure it's no problem it's not just
you okay but then you have sure you just
say okay so I have four dimensions a two
dimension and squeeze it sort of two one
so so let's see if I don't sure so so
tensor is let's say for dimension thesis
ijkl so I and J runs from 1 to 10 so
let's make it okay let's say IJ is just
index and it would be I J 1 to 10 would
be you have 100 so ya 1 again so yeah so
you it's it's this technique is
universal well yes but anyway so anyway
you put you know tensor is you know
suppose you have 10 by 10 by 10 so it's
4 times it's 10 to the power 4 so say
the number of data this is it's that's
that's to do with the tensor the
question is how to approximate is is how
to approximate is good with trying to
reduce the the dimension of at end of a
tensor as a rank but so you can do it
you can try to do it but if you have big
debt as there is no way it's that's a
big data what you are trying to do is
sort of to decompose in such a way that
you have to store rich much less and
preserve most of it so you can yeah you
can if you feel fourth dimension you
can have you have more freedom but is
the same idea other questions thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>