<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Googlewhacks for Fun and Profit | Coder Coacher - Coaching Coders</title><meta content="Googlewhacks for Fun and Profit - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Googlewhacks for Fun and Profit</b></h2><h5 class="post__date">2008-09-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/R0Z-PybQ8Gw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you all for coming I'm really
happy to be here presenting at Google
and to the engineers who helped make my
research possible I'm going to start by
introducing Google act by a show of
hands i saw that most of you have heard
our google acquires and maybe even found
yourself basically it's a search for a
pair boards that returns exactly one hit
in Google as shown when you do the
search it says results 1 of 1 and that's
how you know it's a google why some of
them are kind of funny posted on Google
a calm my favorite is protozoa spliff
but you'd imagine that to really common
words would not be a Google act because
you'd have more than one hit we're at
the other side to really really rare
words would have no hits at all and so
it wouldn't be a Google act so really
the numbers that are important are the
number of Google hits for the first word
and the number of Google hits for the
second one and I define them here with a
and B also I is another important number
which is a total number of pages on the
Google index I this summer used to be on
the homepage are I'm just defining to be
the variable for the number of results
for both words together which for google
lack is equal to one now if all yeah
actually someone pointed out that
marshmellows here is spelled wrong but
in fact if it was spelled right it
probably would not have been a google
act now if you can assume that all the
pages on the index are approximately the
same the same size so the probability of
a word appearing on a page is the same
for any page in the index then that
probability will be equal to a the
number of hits for that word / I which
is the total number of places where that
word could be to find the probability of
both words appearing on a given page you
just multiply those two probabilities
together and then to get the number of
expected results you add all of the
probabilities for each of the pages and
since there are I pages you multiply it
by I which gives you r is equal to a
times B
over I now for Google wack reset r is
equal to one and to demonstrate it I'm
going to use a special log log plot
where the x value of all the points will
be equal to a divided by p that's the
ratio of the number if it's for the
first word versus the number of its for
the second word and there will be two
points for each pair with the x value
determined by that and the y value will
just be the height the the number of
results a and B I took a few hundred
Google wax from the website from what
they call the wax stack and it looks
pretty good a friend of mine a Tom
Poquette wrote a program that allowed me
to automatically find all of the search
results for the words individually and
we see that they do pretty much follow
the two expected lines from the formula
on the last slide where I i just plugged
in the number taken from the google
homepage now it looks script but
something is up all the points seem to
be lower than the theoretical lines with
less results than you'd expect them to
have but if we go back to our assumption
that all pages are equal that's not true
because really there are few pages that
are really really large and a lot of
pages that are really really small now
words are more likely to be found on the
larger pages and since both words a and
B are both like they'd be found on the
same page that means that you need rarer
results rarer words for Google ack and
so all the points are actually lower
than you'd expect them to be now to to
refine the model well we're going to
need to know the distribution of page
sizes on the internet now lucky for us
is it so I demek and Q Berman have
studied this and they've shown through a
number of studies that the size of the
nth largest page or the size of the
pages follows as if LA where the enth
largest page will be proportional to 1
over N to the alpha where alpha for the
internet is equal to one so if we rank
the pages on Google from one all the way
to the capital I and apply there's a
flaw we see that the number of pages the
number of words per page on the add page
will be proportional to one over aya to
the Alpha but we need to know the number
of unique
because it doesn't matter how many total
words are on the page but the number of
different ones if we want to know the
probability of a word appearing on the
page and so we use heap flaw keeps law
which is the linguistic law which says
that the number of unique words in a
text of size n will be proportional to n
to the beta where beta is another
constant normally between point four
point six or the English language
merging those two together we can find
that the number of unique words on the
ice largest page will be proportional to
one over I to the alpha times beta R
alpha is the zip coefficient and beta is
they keep saw proficient now that is
only if there's let's say one hit on the
index then if there's one hit over all
the pages then the probability will be
proportional to the number of unique
words or equal to K over I to the alpha
beta for some K now if we sum all the
probabilities the same way we did for
the basic model then it should equal one
in this case and that allows us if we
approximate that sum from 1 to eye with
an integral then we can actually come up
with a formula that will be equal to K
which will give us this formula for the
exact probability of if there's one hit
of a hit landing on the I page where
alpha is again this zip block
coefficient and beta is the heaps law
coefficient but we need to know the
probability if there's more than one hit
on the page turns out that if you add
more hits you just simply it expect goes
proportionally the probability so the
probability if there are eight hits on
the item the probability of a hit being
on the ice page if they're eight hits is
approximately equal to x times the
probability if there was just one hit
plenty of X now to demonstrate that
though I made a computational model
where for an Internet of 100 pages and I
randomly did a randomly did it a few
times and found the exact probabilities
of hits landing on the I page where
again the the page sizes of the
probability distribution is given by
that coefficient alpha times beta is
point 5 2 which later refined is
actually the case for the internet
and so we see that as long as the number
of results is much smaller than the
total number of pages on the index then
these linear approximation lines work
well now actually the lines are starting
with the blue line which is the
probability for a word landing on the
largest page which goes up quickly as
the number of results increases and the
purple line is for the smallest page
which goes up slowly only at the end
when you've almost filled up your entire
index does it approached a probability
of 1 but again the important thing is
that as long as a is much smaller than I
these lines approximate the curves
accurately and for the case of the
Google X all the results use are much
much smaller than over a hundred times
smaller than the total size of the index
and so we're okay using this
approximation which we now use to find
the probability of a given word being a
given of two words being bound on a on a
given page I and it's equal to a times B
times that formula there with the index
I being the I page which we then if we
want to find the total number of results
from both words together the same way we
sum over all the index from the first
page to the last page and we approximate
that some with an integral which gives
us an exact formula or the number of
results in terms of a B capital I and
alpha beta again now if we look at this
though it's since everything in that to
the right of a times B is a constant
it's really very similar to the basic
model the only difference being that
constant and so we can lump it all
together and define it I call it the
effective index size or I eff to to
being because it's the effective in
exercise for a pair of wards and make
that all just call it that and it looks
the same which allows us to graph it Oh
before a gap it any times B for a given
number of results in this case 1 should
be equal but since it's really these are
random things we're dealing with the
probabilities we should expect some kind
of distribution for the results and
since it's a times B which is
multiplying two random numbers will
expect a lognormal distribution which in
the enhanced
this is a histogram of all the different
numbers of a times B I'm except that
there's a sharp peak there which we can
find by taking the geometric mean so
with the geometric mean of a times B we
can plug it into our formula for the
number of results and of course sinning
r is equal to 1 because all those points
had one result I are going to take off
the Google homepage as around eight
building and the zip flaw we're going to
take from Adamic in Cooperman and set
that equal to one so the one that's left
is beta which we can solve
computationally for and come up with
beta is equal to 0 point 5 2 which fits
nicely between 0 point 4.6 as other
results have shown now what's really
neat about this is that previous studies
used only about 20,000 pages where this
one actually used up the entire Google
index of around a billion at the time
when in practically a fraction of the
time used for the normal normal methods
but of course that's not counting the
hundreds of people that spent hours
trying to find the Google X in the first
place but it's still it's still faster
at least for me we go back to this graph
we can simply since the model is really
the same with only that constant being
different we can plug in the effective
index size 4i which now shows that the
theoretical lines lie right in the
middle of all the points so this is a
good result but we know we're not
actually limited to Google wax because
our in the formula works for any number
of results and so I search for random
word pairs in this case the the x the y
axis is the number of results for both
words together and the x axis is the
product of the individual reward results
that's a times B which for a given
number of results should be constant and
it should follow that the black dash
line and in fact we look at the cloud of
points it pretty much does and it's even
clearer when you look at the geometric
mean that's the red line taking a
geometric mean for all for any given
result are we see that that red line
follows a theoretical line rather
closely
but the green points are points that I
their word pairs that I chose by hand
because I thought that they were related
in some way and so you'd expect them to
have more results than it would just
based on a random process and in fact
they do you see that all the green
points are above the black dashed line
showing that they have more results than
expected just from the model to quantify
at nine i define the strength of
associativity to be equal to the log
base 10 of the actual number of results
/ the expected number and for this
example Smashing Pumpkins has a value of
2 meaning that that rock band has so
many fans that their number of Google
hits for those two words is over a
hundred times what you would expect it
to be whereas surge protector has a
value of one meaning it's about 10 times
more more likely to show up then he
would expected to grand slam is an
example where I thought that there was
something you know a good relation
between them but really just the random
the random process was much more than
what I had in my mind and so it only has
a value of point five now there's a
value for effective index size in my
formulas but of course that grows as the
internet goes so on the next plots I
show an experiment where I'm just going
to keep the geometric mean line so these
are the these lines of the geometric
means for a search of the same points
actually one in august 2005 when the
index was around eight billion and the
other result was the other graph is from
april 2006 with the same words but then
the Google index was known to be about
25 billion and we see that the
theoretical dashed lines match well
we're the only difference was plugging
in that value the known value for the
index sites now at some point though
Yahoo's claiming that they had a larger
Nixon google and i'm not sure what
happened but in the end google took that
number you guys took it off your
homepage but it's still in my model and
so with the help of another friend
Virgil Griffith I got identical data for
Yahoo compare them this is two
experiments at the same time with the
same words with the search engine yahoo
and google plotting the number of
results versus the proud
individual results and that dashed line
I chose so that I chose picking and
effective index sides so that I match
the data which you can then extrapolate
back to find the actual index size of
being around 28 billion or both yahoo
and google so in the end they pretty
much have the same in excess now we're
not limited to word pairs either by the
model if a Greek an extended quite
easily to word triplets by simply adding
a third word multiplying that
probability in that is the third word
has a seat the number of results but
third word is C and you sum over the
index again and come up with a times B
times C over i squared will be equal to
the number of results that's for a three
word search but this is the basic model
to go to the refined model you might
have guessed already that you simply
plug in an effective index size 3 into
the basic model and to get the result
but the math behind it is made easier
because we already defined all our
parameters before we already have the
numbers in so it's just calculating an
actual integral we come up with this
value from the heat effective in
exercise so and it actually fits the
data nicely that the all the points lie
around the theoretical line which is
good because even though we recalibrated
it for the basic model with this there's
no extra calibration it just this result
came straight out from the formulas you
may be wondering though why the points
are let's say they kind of seem spread
around the line they're not exactly on
it so we can ask the same question also
for the word pairs maybe the
mathematical model has some description
which could explain why this spread is
so large and also why it seems to be
shifted over to the left with less
points to the right to the right of
theoretical not to do that we need to
find the exact probabilities so we're
going to go back to the basic model one
more time and say that the probability
I'm defining is Pete of are the results
come at a come to be common I eyes again
the total and exercise and the basic
model again is that all pages are equal
so the probability of a word appearing
on a given page neither
doesn't matter which page you choose so
the probability of a hit being on a page
will be equal to the total number of
ways to combinatorially arrange our hits
over I pages divided by the total number
of ways to arrange both hits a and B
scatter them across the IP ages and so
you can solve for that and you get this
big formula here but now to go to the
more refined model we again we simply
plug in the effective index size for
word pairs into this exact formula for
the probability with basically but to
verify that i use the computational
model again this time with an index size
a big internet of 500 pages and alpha
beta is again that's the distribution of
page sizes will be point by two which
then gives us an effective index size or
277 simply by plugging it into the
formula and this graph is an example
where the number of hits for both words
were both 40 out of 500 pages so the
blue line is a is a discrete probability
density function that is it shows the
probability of getting exactly that
number of hits in this case with a times
B equaling boarding so for example the
probability of getting five hits is
approximately point2 well the
probability of getting two hits is about
point 05 now the dashed line is the
probability taking just plugging in the
effective index size into the exact
formula for the probability from the
previous slide and it actually it
matches quite well and as you lower a
and B it eventually matches almost
exactly but what's actually important is
the way that it differs is not in the
width of this not in the width of the
the peak which is what we're interested
in we want to explain a spread so if
that peak was really wide then it would
explain the spread but in fact it's not
if we if we plot now the exact
probabilities overlaid on the same plot
for the word pairs here those lines
represent lines of equal probability
where you have a high probability over
here and it gets less and less as you
for them and actually it does it doesn't
look bad because the general shape of
that probability consort closely matches
the general shape of all the points
spread out but actually it's not for it
doesn't explain it entirely because the
area that's white has a probability less
than 10 to the minus 15 and in fact that
means that there's a really high
probability of it lying within that
spread whereas the points as you can see
they're not all in spread their kind of
still spread out to to emphasize that
point I picked an example here that has
a ninety-nine point nine nine percent
probability that the results will be
between 600 and 800 well if you put
those words into a Google search then
you'll get only 12 and that's actually
not uncommon that you'll have a really
high probability of results being
somewhere and there somewhere completely
different so to explain this we actually
have to go back to our assumption that
the number of Google hits is actually
the number of pages that those words
appear on now a search for George Bush
so I searched for miserable failure it
became bet became famous when basically
it returned the web page for George
Bush's biography on the whitehouse gov
website obviously the words miserable or
failure where it found nowhere on that
page but Google still counted it as a
result and counts it as a number so
there may be other other reasons which
perhaps you guys can maybe come up tell
me some reasons for why the number of
results may be different also I'm
assuming it's mainly because you know
it's more important to have relevant
hits then actually display the exact
number of words that appear on the page
but i can still even though i can't
explain why i can still come up with a
value for the magnitude of these effects
which will then allow me to explain this
spread in the model so one example is
the order of the search terms which
actually changes it a lot even though
clearly the actual pages that be both
the words appear on doesn't change
depending on the order that you put the
words into but in this case the number
of results is off by 10 times so in
doing searches like this I found that on
average he's just switching the word
pair
order will get results off by one and a
half times which isn't enough yet to
explain the spread but this is a case
where that's definitely the reason for
it also another way to come up with a
measure here a bracket a is defined as
the number of pages with word egg
brackets a minus B is a number of pages
with word a but without word B which you
can find simply by plugging in the minus
character before beat and the Google
search will give you that number and
bracket a space space the number of
pages with both words a and B you can
come up with this arithmetic formula
where the number of pages with a but
without B plus the number of pages of be
both at a plus the number of pages with
both a and B will be equal to the number
of pages with a plus the number of pages
with B plus the number of pages with
minus the number pages with both a and B
which you can rearrange to this formula
here which are then tested for a few
results checking both sides of this
equal sign and found that they were off
by on average 10 to the over over a
hundred times and sometimes as much as
10 to the five point six so changes of
this magnitude are definitely definitely
enough to explain all the spread we see
in all the pots to conclude I developed
a model to predict the number of results
it should be returned by a pair of words
entertain any internet search engine
based on the number of results when you
search for the words individually I use
the data from Google X to calculate the
heaps law parameter beta for over a
billion pages when previous studies use
spotless and in a very short amount of
time I demonstrated the robustness of
the model with word triplets and with
different sizes of the Google index and
I also use it to compare yahoo and
google to find that they both have an
indexed size that is about the same
regardless of whatever claims I
quantified a potential cause for the
widespread of results when the model
didn't explain it alone I also described
a best method for quantifying the value
strength of associativity between any
two words and a pair of future work
might look into whether visit and keep
flopper ammeter 'he's will change
depending what time as the internet
grows also someone might be able to use
Google's advanced search options which
allows you to specify a particular
domain to maybe investigate other
languages finally heaps law coefficient
for other languages or for particular
countries now also someone can make a
more refined model with better
understanding of where the differences
from this model come which would give
you more accurate values for all the
parameters as well as strength of
associativity now I'm going to end by
mentioning some potential uses for this
value which I think might be interesting
first of all it's very quick to actually
come up with your value only takes three
google searches which is fractions of a
second and it can be automated or a
large list of words in a very short
amount of time so one thing you could do
is maybe with Adwords or search results
you could you could link and add to a
page not by words actually appear on
that page but maybe words that are
strongly have a strong association
between two words even if they're not
both on the same page also anomalies it
is values of really really high strength
of associativity or really really low
might be able to find let's say a bug in
the number of search results returned or
if it's not a bug then I'll call it an
interesting discovery so thanks again
for listening and if you have any
questions
yeah probably if you could let's say use
20,000 pages oh do you mean use 20,000
pages oh well if you could any in any
way find definite exact results rather
than you know there's some I think
discrepancies between the number of
pages and the number of results
displayed so if you could come up with
your own search algorithm that would
actually without it would take away all
the fancy tricks that allow for better
searches and simply return the number of
pages only then yeah you would probably
get more accurate results Luke hopes up
this spread for Yahoo was comparable to
that of Google so it's a good question
there were they pretty much the reason I
use the geometric mean is because if you
look at them both over lado just see a
mess of points and they're pretty much
equivalent
so that's a good question um so what yes
that's a very good point so I have to
assume here that most of the pages are
in fact English and the only way that
may not be true especially today and the
only way that I have to support the
model is basically if that were the case
then the word pair the word triplet
results would be off and they keep slot
parameter would it also be off and as
the index group of course lip might have
other things so the fact that the mall
worked for those cases shows that it's
not just the day English pages or the
English pea size
yeah so I suppose if it's way off like
if the search engine is claiming that
they have let's say under a double or
ten times more than the model shows then
perhaps I mean they could be lying or
they could just have a lot of pages that
are blanks you know like at it maybe
they may be actual pages which they're
counting as pages but they don't have
any words on them so I don't want to
accuse anyone of lying especially when
certain cases some of the results
they're not a hundred percent of their
really they count as pages the real
pages with both the words on them so I'm
going to hesitate to accuse anyone of
lying but I suppose if you're into that
you could
I don't think so because it's just based
on the number of hits so and some of
them are actually quite large because as
the other one of the graph showed this
one as you get to here that's actually
so it's not always very common
that's a good point and the fact that
the geometric mean line if you remember
it squiggles around the black
theoretical line that can definitely be
a good explanation for it maybe as it
gets more as you have higher results
because it's let's say including pages
from other languages
I think as long as the words are
scattered randomly unload spam pages
then it's okay so the results should
should say the same as long as that's
the case
yeah so basically what it was is a
hundred pages and there was a given
probability that a word that a hit would
appear on a page based on the heaps law
coefficient that was discovered and then
what i would do is basically just
scatter a hits randomly across the pages
scatter be hits and then see or well
actually in those graphs so it's just
scattered eight hits and then find out
the probability do that like a million
times and then find the probability of
one of those hits appearing on The
Biggest page all the way down to the
smallest page did that answer the
question so I you I used that same
distribution even though it's we're a
smaller size yep
Oh
yeah all i care about is that the
probability the probability of a word
appearing on the page is given by that
distribution
yep
um I can give them to you if you'd like
oh it will be on YouTube sure you'd like
them
yeah i can give you my developing or
it's up there
I guess thanks for listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>