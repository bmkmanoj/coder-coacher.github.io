<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Interactive Ray Tracing: A Better Way to Program 3D | Coder Coacher - Coaching Coders</title><meta content="Interactive Ray Tracing: A Better Way to Program 3D - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Interactive Ray Tracing: A Better Way to Program 3D</b></h2><h5 class="post__date">2009-07-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rfgz90Y93c0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in charge of our our sales and marketing
and we have our vp of hardware and one
of our top chip architects here as well
to join us although i think i'll prolly
end up doing a fair amount of the
talking here today so the things i'd
like to go through don't have a
tremendous amount of time but we'll
we'll go into this I'm sure there'll be
some interesting questions proposed at
the end I'm going to go in a little bit
into what are into our company like why
did we what is our vision like what are
we doing talk a little bit about the
platform that we've created the open
platform that we are proposing and what
you can do with it and will to give a
new middle sort of a demonstration of it
functioning with with this hardware
which I'll get into and will also then
go into a little more detail on some of
them what the problems associated with
this and how some of those problems may
have more general applicability even
beyond producing producing images we
feel that could be more general
applications of what we've invented here
and then do some Q&amp;amp;A so you know what is
the vision of caustic you know it's
essentially today to do very high
quality image reproduction this is done
using technique called ray tracing and
it's traditionally a extremely slow and
expensive offline process now in recent
years there has been some development in
you know increasingly trying to make ray
tracing faster and gradually towards
interactivity but those efforts if you
compare them visually to what you see
when you go to the movies there's still
a phenomenal gap and you know what we're
going to get into is a little about why
this is and why we believe we have the
solution to that so you know we view our
technology as essentially you know a
there's going to be a convergence
between the techniques of doing offline
production rendering and interactive
graphics and we believe that that's
extremely soon and there's a few
breakthroughs are needed to make it
happen but we'll go
into that so our company you know we
enter we enable interactive graphics
cinema quality graphics at high speed
we're finding no 6 on where headquarters
up in the city so we're located up there
about 32 people most of its engineering
pretty much even split software hardware
design and we have about 16 patents
filed so far and we're continuing down
that path we're opening up a pretty new
space here and we're privately funded
right now so you know the future of 3d
graphics like where's that where is it
where is it going um you know there's a
lot of people who debate about ways
rasterization a better way to produce an
image or is racing a better way to
produce an image you know I we're not
going to steak put a stake in the grind
of it which is a better wage produce an
image because it today it's not
practical to use ray tracing for
interactive graphics so it's not even a
debate however if ray tracing we're
extremely fast and the programming
interfaces to we're familiar and
unstandardized it's our belief that
people would be using ray tracing a heck
of a lot more to synthesize images than
they do today you know ray tracing has a
number of benefits it can have better
image quality and it's it's but it's
really it's the easier to easier content
creation model that's the thing that
really got us excited because you can
make beautiful images with rasterization
if you put enough artists work and
enough tricks and hacks and engineering
work into it you can make it produce
great images as we as you know many have
seen from looking at today's games but
behind the scenes there's a colossal
amount of work going into pre baking
information and faking a lot of that
stuff and it's our belief that you know
it's the combination of the promise of
higher image quality from ray tracing
but the easier content creation model
that is the very exciting part of it and
it's our belief that once the technology
is there and available to the majority
of people that most interactive 3d
content will be rate rest so you know a
lot of you may already know
basically what ray tracing is but I'll
go into a little brief description of
that so you know rasterization works by
essentially taking taking your geometry
or 3d description of the scene taking
those triangles and kind of projecting
them onto 2d space on your display
figuring out what pixels are covered by
that triangle and then either going down
in what are called scan lines or tiles
and essentially each of those pixels or
fragments as they're called are mapped
basically to a very wide parallel
processor and shaders shader programs
are run for each one of those pixels to
figure out what color that fragment
needs to be and this process continues
you go through all the geometry and you
end up panting onto the screen in in 2d
screen space you essentially attract
painting triangles onto the display ray
tracing works works rather differently
what ray tracing does pardon is so they
build and go through here is essentially
what you do is you go through you
iterate through all the pixels on your
display and you create sort of a like a
model of a light ray so that light ray
has you know an origin like comes from
somewhere has a direction vector where
it's traveling to and you can set other
things like how far you're going to
allow it to go to and also you can have
other data carried along with that ray
as well and what you do is you let the
raycast out and your question you're
asking that the algorithm is what is the
nearest object to that Ray's origin you
figure that out which is a caustic
process once you've determined that then
you execute the shader to evaluate what
the material on that surface is the neat
thing about ray tracing is that those
materials can in turn generate more
Ray's and continue to cast them so the
beauty of that is that things like
reflections refractions shadowing a lot
of different types of effects that we're
very used to in the real world are much
easier to recreate because you can
actually determine visibility between
objects inherently with rasterization
there's no such physical model existing
you're kind of taking the 3d
squishing it into two dimensional space
and panting onto the screen so there's a
lot of tricks to try to make you know
video games look good today with ray
tracing it is a little more natural to
develop for and create artwork and
shaders for now I you know ray tracing
is a is a fairly straightforward problem
to accelerate if you consider only the
initial rays coming from the camera
these ones right here pretty easy to map
to a GPU today however if you look at
what happens you know ray tracing is
inherent value is in its ability to cast
what are called secondary raised on lots
of them in very random directions so
what you end up with is a phenomenal
number of these Ray's traveling in very
different directions inside of the scene
each one of those needs to walk in
enormous database to figure out what the
closest triangle to the Rays origin is
then once that has been determined you
need to then execute a shader at that
Ray's intersection point and this is a
very tough problem to parallel eyes
because of the of the incoherence of it
because of the the essentially random
order that falls out when you start
producing really high-quality images so
i don't like triangle diagrams but I'll
be very brief on this one you know what
is caustics you know how are we trying
are we going to make money well the
answer is well we're selling these we're
selling these these hardware devices to
protector markets you know our first
market that we're going after is
professional 3d so people like
architects industrial designers and also
people who work in the film industry who
need to pre visualize the animation or
the SATs that they're going to do before
they actually send them out to a render
farm to to produce the images for so
those guys already I'm sound ray tracing
and why it's valuable so you don't need
to sell them on why they should switch
to ray tracing because they already use
it so it's an easier you know we have
one for your problem to tackle and that
in that space so that's what we're
initially looking at we believe though
that with the appropriate advances in in
our ability to fabricate a more compact
chip we're going to move this towards
game
there are a lot of other interesting
markets that this can be applied to as I
want to get into in this presentation
because what we have is a very general
solution to a database problem that we
think actually might have applications
outside of rendering so you know not all
rate racers are created equal so um
today's current approaches for
accelerating rate racing tend to
basically end up with highly reduced
image quality and not that the reason
for that is it can't generally
accelerate anything past the primary
race or some extremely simple shadowing
so if you the big thing that we have
going is that we're able to actually
take it full advantage of general
computer hardware and and our hardware
and we're able to saturate it even in
cases with arrays or traveling very
randomly so effects such as global
illumination where you know ray hits
surface and you want to integrate the
lighting at a particular point you need
to shoot you know sometimes hundreds of
race in random directions to be able to
integrate the the incident light at that
point that's true global illumination
it's very hard to accelerate that but we
excel at it soft shadows where you know
very subtle shadowing going on here that
is another case where you have end up
needing to shoot a lot of very scattered
rays glossy reflections depth of field
is another big one where you actually
might want to model a real a real camera
lens and again you do that by taking
random samples on a lens model so again
involves random numbers and shooting a
lot of random samples so you end up with
a lot of very incoherent race to produce
those kind of images and it's that kiss
that we're that we're good at and the
inclusion is another big one it
basically is kind of like shadow rays
except they tend to be a little shorter
and it's used to try to figure out you
know if you have an object with sort of
like a sculpture and has folds in it you
know things get darker and those folds
that that's something that ambient
occlusion is used for and actually if
you look at games today they tend to do
ray traced ambient occlusion not
on the clients device but they do it
offline in the game production process
and they bake those ambient occlusion
maps into large large textures which are
then looked up into by the shaders
running in the rasterizer so we actually
think our technology even has
application before it ever ends up in a
game console we think it can really help
the production processes of those games
or other things potentially here like
Google Earth we actually think there's
obligations potentially for pre baking
lighting data using ray tracing which
could be pretty pretty interesting and
today we have a very compelling solution
for caustics these these these dense
focus points of light or another area
where you need to shoot a lot of very
scattered raise to recreate so what if
we what if we we solved I've went into
this to some extent you know if you look
at today's GPUs they're very fast
devices indeed however if you Vicky if
you try to program one of these things
you realize that you basically have to
be running the same program over and
over on a large block of data elements
all need the same basic set of
operations to be performed on them you
know if you if you give it a stream of
data and ask it to run a completely
different computational colonel on every
different and every other piece of data
GPUs are a lot slower than CPUs they
only work well whenever you can whenever
you can gang together large amounts of
data that all basically needs the same
instructions to be applied to it um and
we can maybe talk in more detail
afterwards but the detail I want to keep
going in the presentation but it's that
issue of being able to saturate compute
hardware that we've been able to solve
for scattered raise the other issue is
that you know cash hierarchies where you
know you've got a very large scene
you've got a lot of parallel computer
elements trying to trace raise you know
if there isn't locality of reference you
may as well not have a cache because
you're going to be basically fetching
from your systems dram or worst case
we're still disk all of the different
textures shader
code and geometry data that you need to
do the ray tracing so the algorithms
that we've invented are able to extract
locality of reference from the ray
tracing problem and that algorithm is
implemented in the fall in a small
coprocessor which basically attaches to
today's big computer race so we actually
do despite the fact we've built a
coprocessor we actually are leverage and
commodity compute hardware contrary to
what some people think that we're
building a new type of GPU that's really
not true it's our hardware is nothing
like a GPU it has no no similarity to it
at all so we do leverage existing
computer hardware to do the shading so
the code that you run once the rays
intersect objects is running on today's
graphics hardware um the other big thing
is if you look at if you look at ray
tracing and compare it to rasterization
ray tracing is a highly a mature
technology and Internet in a weird way
because you know people used to all
right their own rasterizer by hand you
know the underlying hardware would be
different people would write their
rasterizer zin machine code by hand but
it got to a point where people realize
that was a bit silly and they wanted to
build custom hardware to do it so
industry standard interfaces OpenGL
being one directx isn't really an
industry standard but a lot of people
use it these two platforms got created
to a lloyd game and interactive
application programmers to stay
abstracted from that hardware and write
their applications at a slightly higher
level rate Racing's not like that today
if you look at mental ray or Brazil or
other types of big offline production
ray tracing packages you know they the
shaders are all written in C++ and
they're just LinkedIn they're just big
monolithic blocks of code interspersed
with machine code to do raid traversal
on whatever architecture they're running
on which today is pretty much at the CPU
ray-tracing hasn't undergone that sort
of step where an abstraction layer has
been created to allow rendering packages
to to sort of be abstracted from the
underlying hardware we believe that this
is extremely important for ray tracing
to be more broadly adopted so what we've
done is we've figured out
take the OpenGL API is an ad a fairly
condensed set of extensions to it to
allow you to be able to cast rays inside
of an OpenGL application with a very you
know mellow learning curve and the other
area is the ability to handle dynamic
scenes so let's say you could trace
raise you know phenomenally fast artists
are going to want to start picking
objects up and manipulating them and
actually changing them not just moving
the camera so that database that's
required for a ray tracing system
whatever kind of a database it is had
better be quick to be able to update or
it's kind of not going to be useful for
interactive applications so we we about
three months ago delivered our platform
and our coprocessor in a box to a number
of developers here and I building it
into their pipelines and building some
custom applications on top of it so our
platform to basically go through its
layering essentially you've got the
caustic food processor which basically
used in tandem with your existing
multi-core CPU or GPU produces the
images and you program this through the
caustic GL interface it looks just like
OpenGL except you can cast rays now um
and then basically rendering clients are
things like those big off slow offline
ray tracing packages similarly
interactive applications so let's say
you wanted to build an interactive ray
tracer an interactive ray tracing system
our vision would be that that would be
exactly the same as an offline raytrace
or used in a render farm the only
difference would be they might turn the
sampling rate Stein further integration
and should few arrays but other than
that why should there be any other
difference between an interactive
application and an offline render our
vision is that they'd be the same and
today they're not today you know film
studios only want to do
pre-visualization they sometimes use
game engines to do that and that's all
rasterized but then they get really
angry when the graphics don't look as
good as what they get when they actually
submit it to the the offline rangers
so what is the caustic one card
basically I've already sort of described
this but is it say it's a pci express
card plugs into your computer it's
powered by the bus fits in a single slot
and it's fpga-based today we essentially
took our ASIC design and scaled it down
by about a quarter and reduce the clock
speed and that's what we've put on our
fpga cards for now to get this in the
hands of developers and start getting
applications written on top of it and
even with the you know with these FPGAs
there's still a very large speed game to
be had using this versus just doing
everything on the cpu so it's a
developer product essentially for 2009
not an end user product we're not
selling this to artists today because
the applications have to be written on
top first for it to be of any use our
hardware basically deals that the
massive acceleration of incoherent raise
it also deals with things like photon
mapping as well which I'll get into what
kind of database operation that is but
you could think of our hardware as kind
of like a massively parallel scheduler
which basically takes off loads the ray
tracing problem offloads all the data
base side of it and then essentially
schedules work for whatever CPU or GPU
you have in your system let's give a
demonstration of this so basically what
you're going to see is just an
application it's running on Mac OS 10
right now this app is it's got a Coco
user interface and it's written directly
on top of the caustic GL the cool thing
about this is that you know anyone who
knows OpenGL sit down and write
something like this in a few days it's
really very very easy to program and
that's the cool thing about this you
don't need to become a you don't need to
start writing low-level code to get this
kind of performance with the system
so
let me go through a little bit about the
scene configuration here and show you
what we've got here so basically this
scene that you're seeing has 5 million
triangles in it to these two cars right
1.6 million each and then the rest of
the the same geometry makes up the rest
of the five million triangles how the
scene is set up basically there's a in
our in our OpenGL we have two
programmable parts of the pipe well in
addition to vertex shaders we have ray
shaders which are it's a glsl shaders
that runs once raised intersect objects
and that's how you define the material
that's how you program what the material
does which and it can in turn emit more
rays or access the frame buffer and then
we have a thing called a frame shader
which runs at the beginning once for
every single pixel and it's in there
that you can start shooting the initial
primary rays into the scene so what's
happening right now is every frame here
the the frame shader is running once for
every pixel and setting up the Rays
they're going out into the scene they're
getting ray traced on our hardware and
the Rays are basically constantly being
reordered during their traversal to
decrease the memory bandwidth access to
the scene and then they're being
scheduled to this multi-core nyalam CPU
to be shaded so our caustic GL
essentially compiles those glsl shaders
into SSE code chunks which are then
mapped out to the to the 16 threads on
this eight core system so as I move
around you'll notice in the same things
that you can see the reflections there
between the two cars if you can see it
in the projector you know that's that
would be a nun effect which in a
rasterizer you would have to be quite
difficult to produce self interactions
between two objects which are right next
to each other you can kind of fake in
reflections of a very large environment
by putting it into a texture map and
looking up but here we just create a ray
off the car pant and reflect over and
I'll hit the other car and so forth
you'll notice the taillights on the cars
I go over and take a look at those those
are not a texture
essentially there that's just a bunch of
geometry defining the tail light and we
have a shader which refracts rays and
one which reflects rays and we put some
bulbs in there and that the sort of the
image falls out of it now you'll notice
the the the shadows here what's
happening is you know the ground itself
the shaders doing it's all try linear
filter test or so we're doing MIT
mapping and I'll go to a little more
detail on with some pretty cool ways of
doing that what's happening is you can
see the shadow there no need for shadow
maps in this you just create a ray
inside the shader and shoot it towards
the Sun and your scene and it'll either
you'll get self shadowing for free it
just it happens you know it's very easy
to do that but you'll notice it's still
just a hard edge shadow and it wouldn't
really look that way so the cool thing
is if we go in here and I select this
well this is now doing is I've
quadrupled the number of rays that were
cast so we're not up to about average of
about 10 raise inside of every pixel so
what's happening is now instead of
shooting one shadow ray towards the Sun
we're doing two samples of ambient
occlusion at every intersection point so
to randomly cast Rays to figure out the
visibility of that point to the ambient
lighting in the scene and we're
additionally shooting two directed but
random shadow raised towards the Sun so
you get sort of a soft shadow being
created now what happens is in when you
do ray tracing the way you do this
integration is you use you use a table
of random numbers generally to to figure
out how to direct those rays within a
certain range and you know it's done
like you would expect just the random
numbers put inside a texture map and we
sample in the shader and set up race to
go out into the scene now you'll notice
here this is the sampling rate so if I
were to go to their put in increase that
now what's happening is you see the
noise went down so essentially all that
happened now we're shooting it it
randomly directed shadow raised to work
the Sun and then he it randomly directed
ambient occlusion rays and that stuff
just falls out and of course if I were
to start moving the objects around the
shadows all just sell
update there's no tricks required it
just works so if I go over here and you
can you can really see the ambient
occlusion effect over here on the
fountain if I go back to just you know
naive lighting that's just a shadow ray
being cast towards the Sun so you're
either in direct sunlight or you're not
but overhear you switch on the ambient
occlusion suddenly now a lot of the
surface details starts to come out you
can actually see if darker in the folds
where it will be last exposed to
sunlight and again you turn up the
sampling rate and the noise goes done
you get and you get a nice solution in
the image now a few other things to look
at in this system I went inside the car
is a the shading system so they go to
here and let's uh let's take a look at
this car here the car paint itself so if
we look closer at the car pan it's
actually a fairly complex shader there's
we're using Perlin noise to create a
paint flaked so there's you know but
obviously reflections are coming off but
the shader itself is fairly complicated
so if I were to go over to here and this
is a list of all of the objects in the
scene and I were to look for the body of
the car in this drop-down all this is is
basically a list of text files which
contain the reishi ators which define
the material on that surface so if i go
to hear obviously there's a whole bunch
of parameters which i can adjust so if i
wanted to say no adjust the or diffuse
color which is the color of the car pan
you know obviously i can make those
those changes you know on the fly there
that's just changing settings to the
shader but more extent more in more
detail if i go and edit here this is the
act this is just straight-up glsl code
so people who have programmed OpenGL
before in written shaders will be
incredibly familiar with this there's
the uniforms defining all of the
settings of the car pan
and you know here it's it's got we
actually are doing water droplets on the
pant as well so there's a whole bunch of
stuff in this shader it's quite it's
fairly complex but let's say that i
wanted to i want to make the pant sort
of have some holes in it so i want to
make it that there's a pattern on the
pan and sometimes the Rays go through it
and sometimes they so it's kind of like
someone has taken a buzzsaw to the car
pant I've cut some wavy holes in it in a
rasterizer this would be quite difficult
to recreate that effect in this it's the
kite it's extremely easy so it means
them back out from the car little let's
start typing so I'm going to generate a
signal here so as I'm typing what's
happening is our our OpenGL its
implementation what we're doing is we're
constantly calling upload shader string
every time you press a character and
then hour runtime compiler is generating
sse code as we type and submitting it to
the to the multi-core cpu for the
shaders so if i keep going i haven't
actually affected the look of the image
yet accumulate so with that what that is
doing right now is that that's just
showing Perlin noise being directly
visualized so Perlin noise for those who
aren't familiar with it it's basically
just a waveform that's random but
smoothly varying and it's a very useful
building block for generating what a
procedural shaders so like you can use
that random noise signal to position say
leaves on like you know in a forest and
you'll get a random distribution of
those leaves or you could use it to
create would grand or you can use it do
anything you want it's like a random
signal that's used a lot for procedural
shaders now what I'm going to do is I'm
going to use that to figure out where
the holes are going to be so I'm going
to take that jet float and that end
variable going to take that and I'm
going to cut it you know what that's
doing is it saying that anything below
point2 goes to zero and anything above
to goes all the way up to one so you're
either all on or off and that is going
to be a function which I'm going to then
branch on in the shader and if I'm in in
n equals 1 I want the car paint to do
exactly what it was doing otherwise I
want it just to create a ray that goes
straight through so let's comment that
out because that was just to see what
the signal looked like now i want to say
if and it's greater than 0 then do then
do something in return we go so now I've
got the cases where and is not greater
than 0 doing the car pant like it was
before and otherwise the shade is just
returning but I want to go in here and I
want to create a ray so create array
emit ray so the default behavior is for
the Rays just to inherit the incoming
ray so this shader was triggered by
array hitting the surface so by default
any further raise you create they or
their origin begins at the intersection
point with the material but they can
they inherit the direction so you can
see right now looking at the car you
notice how the Rays are kind of going
through it and I can see the interior of
the car itself now let's say I wanted to
reflect the Rays off that area so let's
change the outgoing ray so and go to
hear and say GL array dot direction I'm
going to change the direction of the
outgoing ray I want to reflect it out I
don't want it going through I want it to
reflect back so glsl has always had its
reflect function it just takes a vector
and uh normal and figures like what the
reflection would be so I'm going to
reflect the incoming ray direction with
normalize the surface normal so now I've
got reflections coming off now I want to
make the reflections a little darker so
I want the I cooing ray color to be the
incoming red color dined by quarter so
now I've got like shiny blobs on the car
pen right but actually I kind of liked
it better
just see through but anyway so you get
the idea so now I of course if I were to
go and pick up they pick up the car body
itself lift it up let's see that right
is that that stuff just falls out
natural and you can see the shadows all
updating all on the fly so it's it's
very very easy to program this and you
can sort of imagine creating an awful
lot of things I'll heck of a lot more
quickly than you might be used to doing
doing doing graphics with OpenGL thanks
so a few other things to look at okay
let's imagine we wanted to do things
like reflections that aren't just like
perfect mirrors everything you've seen
here is perfect mirror so I'm going to
go here and say there's another object
in the scene which is hidden right now
and all it is is just a few triangles on
the floor to create a big round plane
and there's a shader assigned to that
right now that called specular glossy
now I spam ear shaders actually really
really easy i just sort of did one you
just take the incoming ray and reflect
it but let's say i wanted a not so
perfect reflection i want to kind of
look like brushed metal so the shader
itself there's a table of numbers like
kind of like I've been doing for all
those other glossy effects like shadows
and stuff there's a table of numbers
called the halton sequence which is a
popular qmc sequence which is used for
generating random numbers for for
sampling and it's a good sequence
because you can have a very any number
of samples than your it's always going
to tend to give you a pretty good
unbiased sample sampling but if I go in
here and set the number of samples and I
see I want to take start with two
samples and gloss focus so what's
happening now the gloss focus if I set
that to say point nine seven what's
happening nice it's shooting two
reflection raised off but there's like a
cone of discrepancy so they're starting
to they're becoming more glossy if i
were to put that down to like point nine
it becomes a more glossy surface and
up the sampling rate say 106 now you're
starting to create sort of imperfect
reflections there's sort of it's kind of
like a hazy reflection now so off the
floor now anytime rays have been hitting
the floor that one incoming ray has
resulted in six outgoing rays that are
very slightly offset from each other
yeah there you go glossy reflections um
no problem very very very easy to do now
you can imagine when you start having
fun with this it's not doesn't take long
before you start spending raised like
crazy and it will obviously become not
so real time but the big thing is that
our system is able to saturate a lot of
very white simdi hardware even in cases
where the Rays are extremely scattered
so we're able to keep the acceleration
up whereas if you look at the other
real-time ray tracing demos out there
they're not showing you these kind of
effects because their hardware falls on
the floor when they try to do it now
that's a few other things like that
lenses lenses are pretty fun to play
with so let's say I want you know you'll
notice everything's in focus right now
and in a rasterizer it's that way too
because it's basically modeling a
pinhole camera where all of the Rays are
kind of originating at one point what I
want to you know I can do things I would
say just made my field of view and so
forth but you know if I were to take a
photograph of that with a real camera
depending on the size of the aperture
setting I was out at this point in time
that nearby object might be out of focus
and I might be focusing on different
levels of depth so let's say I wanted to
mimic that well that this is doing is
it's switching to a different glsl
program different frame shader and that
glsl program is now casting random
selecting rounding points on like a on
like a thin lens and then figuring out
how the Rays would refract through the
lens at that point and setting the
origin and direction of the Ray based on
that so now it's kind of out of focus
and I'm only taking two samples so it's
kind of
any right now but I'm going to adjust
the focal distance so I'm focusing on
the red car right now and you'll notice
the foreground is kind of out of focus
focal blur this is like changing seema
depth of field is narrowing and getting
longer you know if I go to a very it's a
spherical lens model so there's a proper
you know really wide angle lens no I
won't rid of the noise well that's
pretty easy to do just take more samples
so if i go to get like that if i were to
punch in sex they're you notice it you
know the random noise goes away and you
end up getting pretty nice depth of
field it's easy to do so the other thing
is yeah I sort of showed I was moving
some objects around but the system can
handle dynamic geometry dynamic geometry
is expensive there's no doubt about it
there is a database to be updated in
real time when everything's start moving
but we're still able to handle a lot of
moving objects because of the nature the
database that our hardware uses so if I
were to say go and select the sea
I believe this will be the body of the
the red car so i would say i can i can
move things or basically our system is
such that moving some object moving an
object doesn't have any impact any cost
impact on any other objects that are not
moving so that's the bits the basic step
the other thing is that doing rigid body
transforms so essentially translation
rotation and scale is almost free in the
system except for the time obviously to
update our hardware's deram whenever the
objects are moving but even that's
pretty fast the other thing though the
next step beyond that is fully dynamic
geometry where every vertex is moving
fully independent like so if I were to
go to here and select twist shader here
car paints it's picked up the weird car
pant change i made let me delete it
there we go the carb body is nice solid
and then i go to here and start twisting
it play so now that what this is doing
is it it's basically running a vertex
shader if we look there but the vertex
shader is doing is its flying sounded
like a simple twist function to every
single vertex on the car body and
updating that on the fly so the system
can handle dynamic geometry it's not
that slow but you know it's not ready to
go in a game console just yet we're
still a little ways from that but it'll
happen yeah that's another thing that
what I think the water is also watching
there's basically all that happened when
I hit play there's a uniform this is
being advanced and I think a number of
the shaders are probably latching on to
it updating their their effect the water
itself is is an interesting little
shader basically what it does
so what the water is basically doing is
again it's using Perlin noise to produce
that waveform for the waviness and it's
taken when it went array hits the water
it depends what kind of Ray it is so
basically when an i Ray or any normal
ray hits the water what it does is it
evaluates that wave function and then
refract the Ray through the water and
dine it goes but it also reflects array
off the surface based on the frenetic
term so if I go over and look at that as
I get more oblique against the surface
you can see the you can start to see the
reflections being created there and
you'll also notice that there's there's
actually lighting being created
underneath and the way we're doing that
here I mean this is really just up to
you to write your shaders whatever way
you want I did that go in there and take
a look well it's basically happening
there is when the shadow raised go up
through the water they're being
attenuated or their their contribution
to the sample is being modulated based
on the same wave function of the surface
so it creates the caustic effect but
those who done caustics before in a
production renderer I'll realize they
generally use photon mapping for them
and not not done this way but it's still
if we for like a game quality effect to
do to do sort of fake caustics this
would still be a heck of a lot less work
than what you would go through in a
raster eyes or today to create the
effect cool thing here is you don't need
a texture to fake the caustics on every
object that the light could interact
with it just works it's doing both it's
da it's doing both actually it's
actually amplifying you can do that too
you can create a lot of pretty weird
effects with it now the final thing to
show is you know people often think roll
rate Racing's like photorealistic
rendering technique and all a lot of the
time that's what people use it for but
you don't have to use it for that at all
in fact it can allow you to create
non-photorealistic effects a lot more
easily too so let's say I wanted to
to do some of that I'm going to go and
hide that weird object
okay and I switch the lighting model out
so what this is going to do I'm going to
change a couple of things I'm going to
get rid of that HDR sky background that
all the Rays were hitting and switch
this to tune now what this is doing is a
pretty different effects turn all that
on um this is doing is basically you can
see it sort of looks like a cartoon
shaded effect let me increase the
resolution a little bit I'm just going
to bring it up to 720p will be a little
more clear what this is doing so this is
running a 720p right now you know it's
it's a decently quick basically what's
happening here is the shaders I've
basically swapped out the glsl shaders
like one shader that has a function that
evaluates the diffuse lighting and what
it's doing now is if the Ray if the
cosine of the ray with the surface is
kind of a bleak then it'll it will it
will sort of killed array resulting in
that black that black lining that you
can see on on sharp edges on the
geometry but then if it decides that
know this I want to shade the surface
then what it's doing is any secondary
rays that are cast off their direction
vector is then quantized into like
several like bands so it creates sort of
a a cartoony and a look which is kind of
neat so like the interior of the car in
particular this is actually this is a
technique actually in car design there's
a lot of people who really an industrial
design that really want this type of
view because it gives them a better
sense of how the objects are actually
shaped that is sometimes better than a
photo-realistic shading of the surfaces
it actually ends up being kind of
important so yeah easy to do that in the
system it's because it's fully
programmable so that's the i think
that's that's all i'm going to go into
now on the demo i want to go on now
before maybe some questions I'd like to
go into a little more about what we're
doing here
and how what applications it might have
beyond maybe the obvious ones so you
know what is it that that's been
limiting ratings performance why is it
slow and I want to get into that a
little um ray tracing you know if marb
in our opinion used to be a compute
problem primarily you know if this was
button if this was a you know 1994-1995
the way you'd make great racing faster
would be to build a processor that had
more floating-point units on it that
would be a pretty good way to speed up
ray tracing however now you know if you
look at a GPU it has ridiculous amount
of compute on it that it really isn't
the problem you can pack a lot of
computer onto a chip and you know art
let's just say that what we've built
isn't isn't we aren't competing against
you know intel on like mips or anything
like that we're not trying to pack more
compute onto a chip ray tracing is
essentially a database problem and now
that you can put you can throw
phenomenal amount to compute at this the
problem becomes how do you get the same
data to those compute units and saturate
them efficiently to be able to solve
this problem you know the database in
this in this world of ray tracing the
things that are stored in that database
include you know you're seeing geometry
the shape of things and any vertex
attributes that you might have the
shader programs themselves so the code
that runs once the Rays hit things and
then of course the texture maps which is
obviously still need those huge artist
generated images or a random number of
tables or whatever else you're putting
into texture maps those all comprise the
database and with ray tracing unlike
rasterization you've got tremendous
numbers of very high frequency and very
small scattered accesses into a very
large database and that is why it's very
hard to saturate today's stream
processors at doing this type of ray
tracing now the types of queries that
we've you know that we've honed in on
you looked at the ray tracing problem at
tryouts kind of as a database problem
and what are
types of queries that it does and it's
actually this that our hardware is
exceptionally good at um there's the Ray
query which is the one that we were
basically everything you saw there was
all done with huge numbers of Ray
queries the Ray query there's two
different we break that into two
different types there's does this ray
you know any hit or sorry the closest
hit is one which most common which is
here's a raised origin and direction and
maybe a maximum travel distance limit
potentially and we say okay what is the
nearest pieces of seen geometry to that
Ray's origin that's a database query and
we're very efficient doing that on it
with our hardware design there's also
the interval test where you have array
beginning point an endpoint and you
don't want to know what the closest
object was you just want to know was
there something between here and here so
for ambient occlusion and shadowing
that's a very very common query
operation is there anything between here
and here the other type of query which
has we believe very broad applicability
to is a three-dimensional k-nearest
neighbor query so I have a point in
three-dimensional its say I have a
database not of triangles but of just
records in three-dimensional space and
let's say I want to do a query where I
have a point in three-dimensional space
and I want to get the K like I say I
want to get the 32 nearest point and
point nearest to that point within some
kind of a maximum search radius this is
used in production rendering where you
have a database of what are called
photons and they're basically little pre
cached fragments of light which you then
query you do these k-nearest neighbor
queries in and figure out all the
photons within that range and then you
integrate them into the lighting so you
can essentially store instant radiance
incident incident readings in the scene
and then look up and reduce the number
of rays you cast so our ASIC actually
has the kit has built into its design
the ability to be very efficient at
these three-dimensional k-nearest
neighbor searches
it's our intention in the future we've
been looking into the idea of making the
queries themselves actually programmable
so our solution like what we're actually
doing here I'm not going to go into the
hardware design itself is simply no time
to do that here but basically our whole
systems built around the ability to
regather coherence among those those
many queries so you've got a whole bunch
of queries coming into the database and
those queries are essentially random
they're all going to be packing at
different parts of the database our
hardware and our out our algorithms
implemented in that hardware are able to
sort of gather queries that tend to be
moving in similar directions inside that
database so it'll actually push it has a
whole bunch of queries in flight at any
one point in time like many many
thousands of them and essentially is
able to to schedule in and out any one
of those queries and group them together
when a set of queries are all going to
access or need to test against the same
similar things and our hardware
basically is able to you can do that for
locality of reference when you're
accessing memory but you also need to do
that whenever you're trying to fill a
simdi register to do a very wide vector
test you can apply the similar concept
of rescheduling work to be able to
gather together operations all needing
to be done on a common object that's
another thing that our hardware does and
then of course it since we're not doing
the shading on our hardware it applies
and after its figured out what
everything's hit and what the query
results are the results themselves are
reordered based on the code they'll need
to run on the GPU so that there's
locality of reference when the results
get to the device that emitted the
queries so if you think about it not as
a ray tracing machine but it's a more
general a more general engine
essentially you could use our system and
our hardware design for problems with
these types of characteristics where the
database side is too large
fit in any practical process or cash the
other important characteristic is that
the in any in flight queries count
themselves edit the database so you need
the database to remain static for the
der ich while those queries are in
flight you can't you can't modify the
database where any one of those queries
could end up needing to go there that
would be a disaster to try to do that
however you can do things like edit
other parts of the memory you could
double buffer you could have animation
going on and you could be updating a
different half hardware's deram as long
as you could guarantee that no none of
the in-flight queries would need to
touch that that part you're editing your
problem had best have millions of
independent queries a second the queries
in our system are completely not coupled
to each other so we've actually are yeah
you need to have a heck of a lot of
queries in order for this algorithm to
work because it needs joyce's and the
other thing you better not care about is
the order of the results so you know
think about admitting queries to a
database and not caring about when
you're going to get here any particular
result back the order can be extremely
random but the benefit is that there's
ality of reference in the result in the
order of the results come back in and
the memory bandwidth of those database
queries this is orders of magnitude
lower than it would be if you did it in
a naive way um finally the ASIC itself
so that was f PG a based hardware
running there the the caustic two is
what we're calling it this is the one
that we have fully intend to sell to as
many people who will find use for it
it's an ASIC base that will be running
at about three and a half times the
clock speed and has four times the logic
of the FPGA so we're looking at about a
14x throughput increased with the ASIC
that we're creating so what you saw here
today was doing it shading on a 8 court
in a column intel processor and then
compared with our
with our hardware you won't want to be
doing your shading on the cpu we're
writing an open CL back end right now
from our glsl compiler which maps the
shading to a GPU and that is required to
pair with the the caustic to hardware
just because it's throughput of queries
so fast that you need something you need
a pretty serious piece of compute to be
able to do the shading so we're pairing
that with a GPU it will work with cpu
but i would imagine in some cases you'd
be CPU bound it's a 16 LAN pci express
card still single slots it's still bus
powered and if you've developed apps for
caustic GL now it's a transparent
upgrade there's no like changes required
to your software so that's the end of
the presentation i guess any questions
that's an interesting question the
question was when will this technology
be going into laptops it's entirely
practical for it to go into laptops I
think the bigger question is when are
there enough applications running on top
of it that people who own those laptops
say I want ray tracing acceleration in
my laptop you know yeah yeah well you
know people need to use it and then
it'll go into laptops quite quickly
there's no real technical reason why
that you know people pack enormous GPUs
on two laptops today so really this
should be part of a GPU I think
yeah well we're you know we think that
the biggest the question as I understood
it is in my presentation I said that
we're initially targeting 3d
professional the question was when are
we going to be looking at game at gaming
applications for the technology is that
okay um you know I would love it if this
was in a console and we could build we
scale up our architecture and actually
go all the way with this but there's
practical constraints there that limit
that however there's a step I think
before this actually is in is in the
actual hardware the gamers are using and
there's two interesting things one is
like I was saying the ability to pre to
just accelerate the ability to bake
those assets which are used for the
games we are actively looking at that
right now if anyone has applications
where they they use ray tracing to
produce assets that will be ultimately
rasterized you could use it now and we'd
be more than happy to talk the other
interesting aspect is there let's move
towards certain type of games I think it
can be playable online where the
rendering is done server side and the
content is then delivered to the end
user with a thin client you know there's
there's a some evidence that that may
become increasingly more practical and I
think our technology right now instead
of having to figure out how to get it to
consumers in their devices you know it'd
be a lot easier to put it in a server
farm and basically what this would do is
it would very much increase the density
of the number of users that you could
internet interactively raicha session in
a server farm so it would be like a
power consumption and area conservation
and I think that that's so a way that it
could potentially end up in gaming far
far sooner again we need to talk to
people who see applications there we're
really excited about it
clear though we see the obstacles gaming
is not being necessarily technical we've
got sort of chicken and egg problem you
have enough these cars in the field for
gamers feel it's worth writing games and
you have enough gamer so willing to buy
these cards are available yeah the
polycount I'll give you a rough overview
of it the polygon count was 5 million so
there were five million triangles in the
scene when I was moving any of the
geometry the actual actively move number
of dynamically manipulated triangles was
on the order of about fifty to a hundred
thousand that were actively being
dynamically manipulated at any point in
time the number of raise per pixel which
is a really important one like how many
actual rays were being involved on
average there the Ray per pixel was sort
of between five and ten when I turned
the quality up to where all the noise
went away it was sort of around 16 frame
rate frame rate yes the frame rate well
I guess I should do it again I don't
know yeah certainly at 720p we generally
do the interactive demonstrations at
about vga resolution so yeah it's about
it looks like it's about seven but seven
frames per second
yeah yeah yeah absolutely yeah yeah
we've thought about it our answer to
that is as follows um you know right now
our fast path is triangles however we
are like I was saying about the queries
potentially becoming programmable in the
future we are looking at the idea that
the actual testers themselves could be
programmed but right now you actually
could intersect surfaces that weren't
simply triangles by creating a shell out
of triangles and once the reishi ater
runs inside of there you can do ray
marching to you know to do like you know
voxel rendering you can do that right
now or you could you could you could
represent a nerves an herb surface and
actually intersect it within the reishi
ater so you can procedurally intersect
things there but you would need to sort
of bound the area of space that could
contain those complex objects with with
like a box if that makes sense and you
would still gain the locality of
reference like the the underlying
algorithm would still take those rays
and gang them together and schedule them
for shade at the same time so you still
get the benefits but the answer is our
system really works in triangles okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>