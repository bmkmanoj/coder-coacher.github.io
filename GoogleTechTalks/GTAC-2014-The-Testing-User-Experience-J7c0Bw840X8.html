<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2014: The Testing User Experience | Coder Coacher - Coaching Coders</title><meta content="GTAC 2014: The Testing User Experience - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2014: The Testing User Experience</b></h2><h5 class="post__date">2014-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/J7c0Bw840X8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">&amp;gt;&amp;gt;Sonal Shah: Alex is very passionate about
developer testing tools, and he's going to
talk about what testing means for Google Cloud
platform today.
Alex.
&amp;gt;&amp;gt;Alex Eagle: Hi.
Thank you very much, Sonal.
My name is Alex.
I've been at Google for six years.
Before that I worked at some startups, some
interesting places like Geico.
Google is a lot more fun.
For the last five years, been working on our
developer tools.
I was the tech lead for a system that captures
all of the build and test results from almost
everything we run at Google, which you can
imagine we do interesting things with that.
Then I spent a couple of years as the tech
lead for our continuous build system, which
again covers almost all of Google.
And most recently I've been working on some
new stuff which I'm going to talk about today.
I'm really excited to get sort of out of the
closed wall of being inside of Google and
talk to all of you about this stuff.
This is a really cool opportunity.
And I don't even know where my clicker is.
Okay.
Now I'm ready but I'm not clicking to slide
yet.
What I'm going to talk about is something
different than everything else we've heard.
We've heard great stuff about how to write
tests, how to write tests that are owe bus
and catch bugs and execute those tests in
a lot of environments.
I want to talk about what happens next.
In particular, what do we do with all those
test results and what do we do about failures.
It so happens since Google has -- I don't
know how many thousands of people, so there's
enough people that a few of us that a few
of us can work specifically on what do you
do with test results.
So I hope this is interesting for you.
And I'm sorry that the slide title is not
the same thing that's on your badge, so if
you're here because of the thing that's on
your badge, you know, feel free to do something
else.
I still think it's a good talk.
Yeah, that was that slide.
So my team works on basically continuous delivery.
That's kind of the theme of what we're working
on.
We want to frequently release working software.
We feel like this is an achievable goal.
Ankit talked about pushing every hour or at
least cutting a release every hour, pushing
daily and not breaking things.
So we feel like we're an important part of
that.
I know that all of you believe in that.
And I think that part of having a good testing
culture and getting your engineers really
bought into doing testing is to sell them
on this idea that, like, this is part of their
passion; right?
And that it benefits the engineers as well
as the project that they're working on.
So this isn't something like a manager has
to insist you do.
I think most managers have this kind of impulse
to get stuff out the door.
But it's a hard problem.
There's a few reasons it's a hard problem.
They're all valid.
For example, you need a lot of resources to
run your tests a lot.
I think there was a good question about, at
Facebook, how do you get the resources to
run all these tests all the time.
At Google we use every day 60 million -- sorry,
we use 60 CPU years of execution, CPUs every
day, to run about a hundred million tests.
And I don't know if that's a lot because I've
lost track of scale after working at Google
this long.
You can let me know -- it sounds like a lot.
It's got a million in it.
That's not all I'm going to talk about, like
sure, get enough machines.
You can use Google Cloud Platform or Amazon.
Get more machines.
It's also hard to write tests that are reliable
and that catch real defects and that run quickly.
A lot of talks about that interesting hard
problem.
I think in my view, some of the best engineers
on your team should be working on that stuff.
Like I think the test automation is really
where a lot of the challenge is in making
a product that can reliably ship.
Seen people get excited about that.
Also not going to talk about that, other than
I just did.
So what I'm going to talk about is the people
problem.
How do we motivate engineers to really buy
into doing this.
Imagine that you've got 15 minutes left.
You have to go home for the day.
I've got a three-month old baby so I've got
a pretty hard cutoff.
So you have, like, an interesting feature
you've been working on all day.
You really just want to send emails to your
team and be like, wow, look, I set up a demo
server locally on my laptop -- or on my desktop
machine since I'm about to pack up my laptop
for the day, and I really want you to come
see this cool thing I finished.
It's so close.
And then there's a build breakage and I'm
like do I want to go fix the build right now?
And the build breakage is a bummer.
That's not something I'm excited about working
on.
So how do you take that feeling about you
just don't feel like test something the way
you want to spend your time, how do you -- especially
in a typical company today or at least a company
like Google, like a lot of startups, engineers
have a lot of leeway to make their own decisions.
This is not -- you know, we're not working
at NASA Ames and we have to like verify that
the space shuttle is going to launch and it's
going to work and it's really hard to ship
live updates to the space shuttle and we're
not going to watch the logs and be like, wait,
wait, wait, we have to roll that back.
We totally think we can just roll whatever
it is back.
And we think we have the leeway to decide
whether to write tests or not.
The engineer is the person you need to convince.
It's not the manager who's going to make the
decision to write more tests.
And I've certainly been frustrated when I
have trouble convincing engineers about the
value of testing.
And I'm sure a lot of you at your companies,
maybe you are, you know, the equivalent of
Google's SET.
I guess it's an SDAT at Microsoft.
And you have to spend a lot of your time just
being the advocate for testing.
And it feels a little bit like you're telling
people to eat more healthy.
It's kind of they want to do testing, they
know it's good for them, but they really don't
want you to tell them and block your code
review and be, like, you didn't put tests
in there.
So how do we convince them; right?
How do we line up the incentives for an engineer?
So they want to write the next feature.
Here are two quotes from real engineers that
I made up.
[ Laughter ]
I don't want to be distracted by bug reports
in the last release.
I'm going to read the slide to you, I know
you shouldn't do that.
I wish those problems had been found by the
testers.
This is somebody, like it's not my job, the
last release, I don't even remember that,
I don't remember how to find that branch.
That says I don't want to waste time on broken
tests.
If the build has been broken for a couple
days, I just ignore it.
So how do we balance these two things.
These are contradicting?
So we're pretty sure that if you want to -- if
you don't want to have to be distracted by
bug reports in the last release, you need
to know that the last release worked.
How can you be sure the last release worked
if you didn't write tests for it?
And I think there's been a little bit of discussion
of well, if you leave testers -- if you leave
developers on their own, you don't give them
a safety net, they don't have somebody writing
the test for them, then they're going to naturally
realize they have bugs in their release and
feel like they have to do some kind of testing.
And ultimately, they want to have -- we need
to make this argument to them that sustainable
velocity comes from having healthy codebase
and having tests that tell you that things
are working every time you commit.
But a lot of engineers don't do it.
They think it's, like, a waste of time.
And a lot of startups don't do it because
they're in a hurry and it's a waste of time.
So is it a waste of time?
Like, how would you -- like, how do you answer
that question?
Does it really increase or decrease the long-term
velocity of this engineering team?
In some cases, it actually -- like, it is
a waste of time.
If you're writing some prototype code and
you're pretty sure that only one feature in
ten is going to launch to production, it is
a waste of time to write tests for code you're
not going to keep.
There are some other cases where it's a waste
of time to write automated tests for a feature
that's going to get turned down in two releases
because it's just for converting people from
the old thing to the new thing.
You might as well just manually test it for
two releases.
But I want to talk about the kind of testing
that we're used to.
When we put tests in the codebase, we expect
them to stay there, we expect them to catch
defects.
And all of these things contribute to adding
more test maintenance and giving people the
sense that they're wasting time with testing.
I think -- I think we should take these costs
very seriously.
So starting at the top, Ankit said in his
keynote that we have to weigh off the costs
versus the benefits of having tests.
So, in fact, like, their team tries to have
fewer integration tests because they cost
more.
So if we can reduce the cost, then we can
afford more integration tests, given the same
amount of effort.
So let's say you write a new testing tool.
A lot of you have.
And the user has to learn how to use that
tool.
They have to learn what the failures mean.
Your error strings don't make sense to them
until they've had that error a few times.
And the worst thing for them is that there's
a bug in your tool and the test failed and
it wasn't their fault and they have to go
track it down in your tool.
End-to-end tests we've talked a lot about
at the conference.
They're really great at catching real defects
that you wouldn't catch with unit tests.
But when one of those things fails, it's basically
an exercise of, like, debugging your production
system.
You have all the same parts running.
You have to go look through the log files.
The failure message is just, like, &quot;I got
a 500 and I don't know why.&quot;
When the fixture is complicated, in other
words, getting the system under test up and
running, it can get -- they're very brittle.
They can be broken in a lot of ways.
So, like, we have security comes and says,
&quot;Oh, you can't actually make a connection
from that place where your test is running
to this other place where your server is running.
That's not cool.&quot;
Or you push some new configuration -- you
build some new configuration in your code
and then you discovery, oh, when I bring it
up in the tests, I need to have some different
configuration to make it come up in the tests.
You move servers around, things like that.
The text logs I will talk more about later.
We've all had to scroll to the bottom of a
huge text file.
And flaky tests, flakiness.
So I'm also going to talk more about that
later.
And certainly, like, we've talked about that,
too.
Like, what do we do?
Do we delete those tests?
Do we try to fix them?
So these are kind of a lot of costs for doing
testing.
You don't have to do these things if you don't
do tests.
And the last one up there was the growing
codebase.
So this is a picture from Jenkins.
So if you hook up Jenkins to find the XML
files after each run, then it will count the
number of test cases.
You can get this pretty graph.
It's nice for motivating people.
You see this graph, you're, like, I'm so proud
of my 1,000 tests.
Like, oh, now I have 2,000 tests.
I love my tests.
They're so cute.
Look at them.
But then this is sort of monotonically increasing.
And that's fine.
This is a healthy project.
But the burden of keeping all these tests
passing means once you're up to 2,000 tests,
you have maybe not twice the chance, but much
more chance of something failing than you
did when you had 1,000 tests.
And parts of this codebase you're not even
actively maintaining anymore.
These are tests that you just have in place
because you're hoping that the legacy thing
keeps working.
Obviously, there is a tendency sometimes to
delete some tests, and then now you're scared
to change that production code because you're
not sure whether all your defects there are
covered.
Okay.
So I'm going to digress for a minute to try
and, like, figure out, like, what do we need
to do here for this problem of the tests being
too expensive?
And, in particular, testing is needed to keep
engineers productive, but it makes them less
productive.
So how do we solve that problem?
I think static analysis is a good example
here.
And I actually had the great chance -- the
great opportunity to work with Bill Pugh,
who wrote FindBugs.
He was a professor at the University of Maryland.
And I just happened to work on a team where
he was reporting to the -- or he was sort
of being brought in by the guy I worked for.
So I got to sit down with Bill and sort of
learn about what FindBugs does.
And we were running it at Google, and I became
involved in this project.
And we even got to the point of integrating
with the build system so you could just say,
&quot;Analyze my code.&quot;
And then we built it into the code review
tool, so it would say, &quot;Hey, I found these
problems.&quot;
Are they real problems?
Well, we did this Fix-It that Bill and I think
one of his students cowrote called the Google
FindBugs Fix-It.
A Fix-It at Google is where we go back and
spend some time to clean up some problem that
somebody has identified.
So in this case, it was can we go fix all
the real issues in our code that FindBugs
is finding.
And we discovered that, yeah, these are real
bugs.
77% of the issues that it found were marked
manually by an engineer as either should fix
or must fix, which are the classifications
of, yeah, that's definitely a bug.
It turns out that when the Fix-It was done,
we still introduce these things in the codebase.
Why does static analysis not work?
People who work in static analysis have the
same frustration.
I found real bugs and I showed them to you,
why was that not enough?
Why do you not take this seriously?
Why do you not eat healthy?
You didn't do enough.
You showed them all kinds of real bugs.
It didn't work.
How do you make it work?
Well, what we did is we owned more of the
problem, I would say.
We said, okay, our job here is not to find
bugs, although that is important.
That's -- you can't do this static analysis
without the static analysis tool.
But, like, what do we need to do to drive
to you fix them.
We built static analysis into the tool chain,
that means the Java compiler and Clang.
And then we made those things produce fixes
so we can say, hey, like, I found this problem.
It should probably have been this.
&quot;Fixes&quot; sounds cute, like, yeah, people want
to click &quot;yes&quot; to their compiler a bunch of
times.
But actually what we did with the fixes is,
we built enough infrastructure so that we
could go across Google's whole codebase and
apply the fixes, which meant that we were
now free of this bug pattern and we could
enforce it in the compiler, it could never
happen again.
So it's ratcheting it up.
Every time we go through this iteration, we
have one more little fine-grained bug check
that you can never do that again in the codebase.
That's the case for errors at least.
For warnings, we can't stop them from being
introduced, so instead we were very careful
in code reviews to only show things that you're
likely to need to look at right now.
And we have a feedback loop where you can
click either &quot;please fix&quot; or &quot;not useful,&quot;
and we use those clicks to determine which
checks are valuable or not, file bugs against
people who write the checks, and take a lot
of ownership of this problem; right?
So this turned out to work really well.
So in August, our numbers were 255 Java bugs
a day, and 71% -- 71 in the code reviews,
and we found that 74% of the bugs that we
marked in compilation and 95% of the bugs
we marked in code reviews got positive feedback
from people.
So we did a round trip to see if these were
important.
Can't give you any absolute numbers on people's
codebase or the number of bugs.
I should also add that Error Prone is my 20%
project.
So go use that, quick plug, since I'm up here
and you all have to listen.
So, you know, I think this problem is largely
solved.
Like, we actually have way fewer of these
bugs getting in now.
And for the ones that we have made errors,
we have zero of them getting in.
What do I think is the lesson from static
analysis?
It's that we should think less like just it's
our job to show people test failures and more
like if this was a product and the thing we
wanted users to do is we could treat it like
conversions like for a marketing Web site,
we want people to go from, &quot;We reported that
there was an issue,&quot; to &quot;They fixed the issue
and the build went green again quickly.&quot;
It's not good if there were other outcomes,
like &quot;They decided to suppress this test,&quot;
or they never fixed the build and it became
a broken window,&quot; or, like, &quot;more and more
build breaks piled up.&quot;
So I think this is really our goal, keep the
build green with less maintenance work.
And this is how we should measure success,
to the extent that, you know, success measurement
is a big deal at Google, we try to gather
stats about whether what we're doing is working.
If we're going to gather a stat, it should
not be we broke the build a lot.
Didn't we do a good job building, we found
lots of defects.
It doesn't count if they don't get fixed.
What can you do about this?
Like, I'm not suggesting that you should all
build an infrastructure to capture these stats.
But there are certainly some simple things
you can do.
You can make sure that when you write a test
and you have an assertion and the assertion
fails, it prints out a message that's clear
about what's going on.
If you're writing a testing tool, you can
make sure you're prom propagating the things
the user needs, like a stack trace that has
the right things on the stack, that explains
where they can go to find more information.
A Wiki page that says if this fails, here's
what you need to do about it.
It's essentially treating it like every time
something fails and it's hard for somebody
to fix, it's kind of like a postmortem, what
could we do to make it simpler to fix next
time.
But we've been doing more than that.
And I'm excited to talk about it.
So there's this word breakage up here.
That's, like, a super big deal.
I'm really excited about this slide.
Sorry, I didn't act excited enough.
Yay!
It's a big deal, because sometimes to solve
a problem, you just need to model things correctly.
And then by modeling it correctly, a lot of
the right solution just drops out.
So a breakage is what the user cares about,
the user in this case being somebody who's,
like, got a test failure, basically.
Because you have users running tests and if
they pass, they don't care.
They didn't even know the test was being run
because you've hooked it up nicely.
A breakage is not a single failure of a test.
So a test might fail eight times in a row.
That's one breakage.
It might be more than one test.
It might be that five different tests broke
but they all have the same root cause.
Not that it's easy to determine that they
all have the root cause but that should be
one breakage logically.
A breakage might be you have several different
versions of the same build and they all use
the same library and that's one breakage.
And it might be you're just running locally
and you've made some code changes, and now
the tests are failing.
They weren't before.
That's still a breakage.
The interesting thing about a breakage is
that it has a culprit.
So really your job, as somebody with a broken
test, is to be, like, what happened?
Can I roll somebody's change back?
Another common culprit is that the environment
was broken.
Like the server wasn't running that you actually
connect to from your tests, and so like the
test is just getting, you know, connection
-- connection dropped.
And there's a life cycle to a breakage.
And what we've done is try to manage that
life cycle really carefully, to drive somebody
through to number four.
So first of all do we need a human to be involved
in this breakage?
Let's try not to interrupt anybody at all.
Taking an interrupt for a broken test is annoying.
If we want our users to be happy, then, like,
we should only bother them when we really
need them to fix something.
And then if we're going to bother somebody,
let's try to find the right person.
And let's also try and, like, let everybody
else know that we've picked this person.
So, like, whatever communication, somebody
is like, hey, that broke me, too, or I synced
to my client, like I synced locally and the
code doesn't build anymore.
Whose fault is it?
Try and get all of that thread in one place.
Okay.
So now we've got this person and we've brought
them somewhere to see a giant log file, maybe.
Let's do more to try to explain what the problem
is to them and some likely causes.
So we could list like, well, you know there's
some changes recently.
Did you know this test uses this server and
this server hasn't been up lately?
And then finally, gather the success metric.
So did they resolve the breakage quickly?
Oops.
The other green button; right?
Everybody else did that, too.
It's easy to go back twice.
So flakiness.
I'm so excited to have my chance to stand
up here and talk about flakiness.
[ Laughter ]
Flaky tests are not that bad.
They're not that bad.
[ Laughter ]
There's a scale; right?
So, like, I've definitely been super annoyed
at flaky tests and gone and fixed them.
I've also had tests that a little bit flaky
and I deal with it.
I think flaky tests do catch real defects
that none of your other tests catch because
the large integration tests that make sure
everything works end-to-end are the flakiest.
But the problem is a failure of a flaky test
is not necessarily a breakage.
It's a really very noisy signal.
And so if we want to get a useful signal from
flaky tests, we need to do a little bit of
very simple signal processing on it.
We need to look over our little sliding window
of time at the pass/fail ratio of this test
and it can have one of three states.
It's totally passing, it's flaky, it's totally
broken.
So if the transition from zero to nonzero
occurs, that's a breakage.
The test went from not flaky to flaky, we
probably want to do something about that.
And the other transition from a nonzero ratio
to one is also a breakage.
That means this test is now failing.
It's telling you something.
Here is the signal you wanted.
While this test is flaky and annoying, we
didn't bother you about it until we were pretty
sure you need to go do something about it.
It's pointing out a real defect.
There are a lot of things we do here so this
is just sort of a mockup of something that
could exist.
Hint, hint.
So it matches a known flake.
Maybe we can look at the stack trace and be
like, oh, yeah, we've seen largetest.go at
line four.
That fails all the time.
So we can give you some hints here to try
to explain that, like, what's the action you
should take?
Nothing.
You didn't break it.
Don't look at it.
We don't alert the user for this one.
This is assuming that the success ratio hasn't
changed.
Let's say that there is really something broken.
We need to get your attention.
So, yeah, we need to alert you somehow.
There's like we need to have a call to action
this gets the user to come look at the problem.
And this is something that we're all familiar
with getting an email from the continuous
build system that says it's broken but I want
to point out that you should only get one
email for this breakage.
It shouldn't be that every time the build
runs it keeps emailing you over and over again.
So what can we do to help you?
So at this point we've picked an assignee,
and we say, okay, we haven't identified the
culprit yet but there are five possible culprits.
Possibly it's five different commits that
came in but, like I said, maybe a server being
down is one of the possible culprits.
We say it's probably not a result of flakiness,
if we can figure that out.
And importantly, we're coordinating here.
So like this is a page that everybody can
see if they're affected by this breakage.
They should be able to come here and say,
cool, I see Vince is on it.
That's good.
We could even do clever things like, you know,
we look at the pull request that came in with
one of the changes and somebody made an after-the-fact
note on there, hey, it looks like this broke
something.
So we should be able to do a really good job
of helping you find what the culprits could
be.
And then at the end we have the successful
conclusion of the breakage.
We see that the thing has gone green again
and now you can get back to work.
And we want to get you through this flow really
fast.
Okay.
So this is a UI that you're probably all accustomed
to.
So this is a Jenkins build.
Even if you don't use Jenkins, you probably
have this many lines.
This is not the whole log.
This starts at line 2,050, but the last line
is the last line of the log.
So step one, scroll to the bottom.
And then step two is, oh, well, it says here
that there was a failure, blah, blah, blah,
go look at this file.
But I happen to know, oh, well, this is Jenkins
running Maven, so now I need to scroll up
some.
I scroll up a little while until I can find
more errors on the left-hand side.
And like that might suck, but hey, this is
actually the best case scenario because maybe
the failure is actually happening on the server
my test talks to and it's on the server log.
So if it doesn't show up here then I have
to email the sys admin and say, hey, can you
let me log into the server because I think
the error that is causing my test to fail
is in the log file somewhere.
So I would argue even though it's possible
to find a failure in a test log, that was
not a good user experience and that would
make people less inclined to fix this breakage.
So internally at Google, we've managed to
make a number of sort of micro formats that
can express different things, like oh, this
piece of infrastructure can sometimes fail.
We have a way of dumping information about
that.
We have, you know, individual logs from different
programs.
So, for example, each time we run the compiler,
we grab its individual standard output and
we have that somewhere so if the compilation
breaks, we don't show you the giant log file.
We show you the four lines from javac that
says the compilation failed, the problem's
on line 15.
So landing on that log file is not the desired
course that users take with their application.
Sorry, I reordered some slides at the last
minute so now I have to try to use the back
button again.
Come on, back button!
I don't have the capability to reload.
Oh, it's going back.
Now let's see if I can find the right place
to go back to.
Yes!
Okay.
So now we've drilled down into sort of there
was an investigate button on one of those
pages that was supposed to help you, like,
okay, you're the person we picked.
This is a real failure, you need to deal with
it.
What are we going to show you?
So it's pretty cool if we can show you something
to do with the root cause of the problem so
we can try to do delta analysis.
In this case, like within this one log we
were able to pick out an assertion that failed
and it had an expected it was.
Maybe we can put this next to each other.
We might be able to do the same thing with
this failure and our previous paths of the
same test.
So a lot of the times like 2000 of those lines
will be the same when it passes and when it
fails.
Can we just show you the delta of when it
failed.
And it would be really cool if we can show
you things like screenshots, or videos.
So yesterday we heard from Jay at Appurify,
and we saw some of the awesome data that they're
able to gather out of tests.
We saw that again today with Spoon.
It has a pretty cool model of stuff that you
can pick out of a test that helps you figure
out which devices your test is failing on.
And if we can combine that data with the context
of the fact that this is a continuous build,
or maybe you did a local build but you're
synced to a change at which the continuous
build ran, we should be able to do some sort
of delta between the state before and the
state after, and that should really help us
find the root cause of the breakage.
So I'm hinting there at the data model.
And like a big text file.
Is that cool?
Obviously I think the answer is no.
There it is again.
So a test result really like logically, what's
in there?
It's the representation of everything that
happened while the test was running that could
help a user figure out what's wrong.
Screenshots, constraints.
So maybe this test is now consuming more memory
than it did before so it no longer runs on
this mobile device.
Maybe you have in your builds a timeout, like
can we tell that this test timed out instead
of it just failed and then you go dig in somewhere
and you see, oh, the build tool said that
it timed out but I got back from it was it
fail.
Can we figure out that there are points in
the infrastructure that are flaky, so like
ADB is flaky.
Can we report that as ADB flaked, not your
test failed?
And then can we do other things while the
test is running?
So can we gather the network analyses?
We saw that from Appurify.
Can we get other metrics that you might choose
to publish?
And can we get logs from all the machines
involved?
And can we correlate the time stamps on those
logs?
So can we say that at about the time the client
had this problem, even though you were hitting
a prod server, heaven forbid and there were
lot of other requests coming into that server,
your request was one in this small range and
you can correlate that with the failure on
the client.
So that would all be really cool.
I don't know how it do that with the tools
today, as far as I know.
XML -- the xunit XML that was -- that you
get mostly out of the tools doesn't represent
any of those kinds of results.
It can give you stack traces.
It can give you a tree of test cases.
I think there may be some way to get, like,
the execution time in there.
But there's no way to attach screenshots.
The screenshots just get written to some directory
on the disk where the test ran and now you
have to go find that place where the test
ran.
If you're running that test on cloud, then
that machine could have been deprovisioned
already.
Like, now you have to do all this extra work
to make sure you gather up all the screen
shots that you wrote, and only you know the
path where you wrote them to because it's
your job when you run WebDriver to tell it
where to write the tests -- where to write
the screenshots.
So it's really hard to gather all this stuff
and bring it back.
So, like, the giant text file is kind of the
best thing we have today.
I think we need a better way to gather all
that information.
I should point out that there are a couple
of things.
Test anything protocol and subunit which didn't
really look like something that we would,
as an industry, want to build on top of.
So, yeah, are we going to do something about
it?
Yes.
Yes, I hope so.
So one thing we've done, I've got a URL there
which you're all probably starting to type.
This is a work in progress.
This is not an announcement that you should
all go code against this thing.
But what I would like to get is some sort
of format that we can mostly agree on that
a lot of consumers and producers would write.
What are consumers and producers?
Well, you have a lot of tools like maybe you
have your continuous build and then you have
your build tool and then that calls your test
runner which then calls your test which then
uses some libraries.
Those all are things that could potentially
produce outputs that would be relevant to
figuring out why it fails.
And the consumer's is, you know, the UI you
look at, but it could be something in your
build tool that wants to decide whether it
should retry or not, so it would be nice if
it could tell if this was an ADB that failed
to initialize the remote device as opposed
to your test really failed.
So is it transient or a permanent failure?
Can it be retried?
I think there's a lot of stuff that can be
built once there is a format that at least
a lot of producers and consumers are writing.
How many are doing it now?
Zero.
I think we read the Xunit XML.
So this is where I hope we will make some
progress in defining a format and then eventually,
maybe with some of your help, getting tools
to start writing that format.
So like I said, this is still at the beginning.
I met with somebody who works on protractor
today, and we're like, oh, that's cool.
They're actually getting ready to start writing
the XML files.
Maybe we can start grabbing screenshots that
you grab from WebDriver as part of protractor.
It's all in the future.
The basic design is a file system, because
that's convenient.
Like even if your test runner is a bash script,
you can probably manage to emit some lines
to a file.
Next to those files we need some sort of metadata
that annotates it and says the mime type and
the description and, like, maybe what UI you
want to open it with.
Not a lot of data.
And then ideally, we want to be able to have
multiple processes and multiple machines writing
this stuff at the same time.
So it has to be a format that allows you to
have uncoordinated processes all writing to
the same place.
So you might need to have a distributed file
system.
Maybe Google Cloud storage, maybe Amazon S3,
maybe like a network file share within your
company where the tests are running that you
can write all this stuff.
And it needs to be extensible because none
of us knows what tools need to write there,
and there will be another tool tomorrow that
we would like to be able to use the format,
and it should be streaming.
Like, we don't want somebody to have to wait
until the entire process is done and we've
torn down all of the remote machines where
things are running before they can even see,
like, oh, yeah, at minute one out of 20, there
was a failure from the, like, build tool that
I could have already been working on that.
And obviously it needs to be cross platform.
Like, we're not going to build this in in
such a way that you can only use it with Java
or you can only use it with Android tests.
It needs to work for iOS and so on.
And the other thing we're working on is a
service to do the sort of breakage life cycle
management that I talked about earlier.
So this again, like, we're not ready to announce
it today.
I hope we'll be ready to soon.
So this is going to be something where like
if you've written one of these file systems
with stuff in it, you could hand it to us
through an API and we can figure out and help
you, like, identify these breakages and manage
the process.
I hope I'll be able to announce that soon.
So if you'd like to be an early user of that
once we're ready to sign people up, you can
talk to me.
Also my coworker Mo is back there in the blue
hoodie.
We'd love to talk to you about that.
I think the best way to get engineers excited
about testing is to make it not suck for them.
Which means that you need to not just show
them a bunch of red boxes, but getting a green
box on the end of that sequence.
And I think breakages is a nice concept for
modeling that in your head and making sure
that you're thinking about the end of a breakage.
And I think that we need more fidelity in
the test result data.
And it's lame that the best thing we have
now is you dump some stuff out to a machine
that you may not be able to find later when
you're investigating the failure.
And of course, somebody might file a bug for
a failure.
It could be weeks later that somebody needs
this particular screenshot.
And I also hope that since I don't have anything
ready to announce to you today, that you can
-- you can start thinking about what can you
do when in your test framework to make it
easier to track root causes and failures.
And what can you do to have a postmortem culture
of when something breaks and it's really hard
to figure out why, how can you make that better
for somebody next time.
What was it about that breakage that made
it so hard to fix?
Yeah.
And I hope we'll be able to work together.
So that's all I have.
Thank you.
[ Applause ]
&amp;gt;&amp;gt;Sonal Shah: Thank you very much, Alex.
Some excellent thoughts over there.
So any questions for Alex?
Anything on the moderator link?
&amp;gt;&amp;gt;&amp;gt; Question.
Where do you think you stop pleasing the engineer?
I had instance when developer will tell me,
&quot;Well, until you have a reproer in the debugger
break in that point, I'm not looking at it.&quot;
Where do you think -- How far do you go?
&amp;gt;&amp;gt;Alex Eagle: The question was, at what point
do you bother the engineer?
&amp;gt;&amp;gt;&amp;gt; When do you stop doing everything you
can to give up get the developer to look at
the problem.
&amp;gt;&amp;gt;Alex Eagle: Okay.
When do you give up?
If the build is broken and nobody on -- and
nobody's taking ownership of fixing it, I've
suggested that you pick an owner and tell
them to fix it.
Maybe they reassign it around.
It goes for days, it doesn't get fixed.
That's definitely a problem with the testing
culture in the team.
I think the typical thing to do is to, like,
suppress that test so you can get the build
green again and move on.
Some tests will probably die along the way.
Obviously, the more important thing is that
you continue to have your other tests running
in green so that you don't have the broken
window syndrome and somebody breaks another
test and says, oh, well, the tests are broken
anyway.
I don't know if that answers your question.
Feel free to, you know, clarify it.
&amp;gt;&amp;gt;Alan Myrvold: We have a couple questions
from the moderator.
Nonhermetic tests will appear to have been
broken on a particular change that has no
relationship to the root cause.
Is the only solution to scour all tests for
hermeticity?
[ Laughter ]
&amp;gt;&amp;gt;Alex Eagle: Scouring tests for hermeticity.
I'm trying to imagine how you would do that.
There's a little bit of a yes to that.
So we've been thinking about -- we run all
the tests in a remote execution environment
where we can trace all the network calls they
make, maybe we could give you a report that
says, hey, when your test runs, these are
all of the external resources it talks to.
Not the only way you can have hermeticity
problems.
Like we saw earlier, you can have a race condition
between the UI thread and the testing thread
and all of the external resources it talks
to.
[ Speaking too quickly ]
Oh, speaking too quickly, it says.
I wish I would have seen that earlier.
Maybe I would have used all my time.
Thank you.
So could you repeat the first half of the
question again?
&amp;gt;&amp;gt;Alan Myrvold: Nonhermetic tests will appear
to have been broken on a particular change
that has no relationship to the root cause.
So -- what can we do about that?
&amp;gt;&amp;gt;Alex Eagle: So I think, ideally, the logical
way to model that is that it's the same -- if
the test has been flaky and it continues to
be flaky, that's not a breakage and we should
do things to make that test remain useful.
So there's a spectrum here, a judgment call
to make.
If it's incredibly flaky and you can never
release because you have ten of these things,
you'd have to have a really fantastic chance
they all pass at the same time.
That's, obviously -- those are not good flakes
to do something about it.
If it's really flaky but it gives a little
bit of value.
If the test fails, I didn't learn anything.
If it passes, the whole thing works.
There's no way that could pass unless the
whole thing was wired just right, I think
that's the valuable kind of test to have.
I think flaky tests are kind of okay.
But without the right way to, like, alert
the user to was it really a breakage, like
a transition like I said from passing to flaky
or from flaky to failed, then it's very annoying
to have them show up in front of developers
all the time.
So I hope that's a problem we can fix.
&amp;gt;&amp;gt;Alan Myrvold: Different question.
xUnit was originally designed for unit testing,
presumably.
Most rich test data results come from larger
tests.
Is it time to call into question the entire
unit test model for large tests?
&amp;gt;&amp;gt;Alex Eagle: So the unit test model.
So, yeah, I mean these large tests are not
unit tests.
Obviously, naming things is one of the two
hard problems in computer science, along with
concurrency and off by one errors.
[ Laughter ]
I think -- I think, like, the model of unit
testing is you set up a fixture, which is
your system under test, then you exercise
the fixture and then you make assertions about
the side effects it had.
That's all the right model.
And I think, you know, these typical xUnit
tools, yeah, they gather the right information
for a typical unit test and they don't do
a great job of gathering all the information
you need for larger tests, so we should gather
more information.
I don't know that that really changes the
model.
&amp;gt;&amp;gt;&amp;gt; So you mentioned that the breakage tool
will tell you or the user or the developer
that -- whether a test is flaky.
How does the tool work to detect flaky tests,
and if -- will it work with a new test?
&amp;gt;&amp;gt;Alex Eagle: No, it wouldn't work with a
new test.
So if you've never seen a test before and
it fails, there's no sliding window over which
you can tell that it's a mix of green and
red.
We've tried a few different algorithms and
we've had a lot of different teams at Google
trying to deal with flakiness.
A simple one is just you look at the sequence
of red and greens and, like, if it's, like,
one red sprinkled in there every now and then,
it's probably flaky, so we can mark it, like,
we can look at the sequence of runs for a
test and if we see that, we'll tell the user,
that one's probably flaky.
At least if you do see a red build and you
come to our UI, then you can quickly decide
it's not your fault and leave.
I know some people have a way to manually
mark a test as flaky.
One cool thing we have is that if you do manually
mark it as flaky, it gets retried by the build
tool, which you can probably build into some
of your testing frameworks.
But you would have to have a way to know this
test had been marked flaky.
Yeah.
&amp;gt;&amp;gt;Bryan Robbins: So when it comes to making
a new test format, right, the xUnit format
is sort of just pass/fail, and it works for
everything, because everything has to pass
or fail.
But it seems like a generic format -- a richer
generic format would have to be compatible
with a whole lot of different types of artifacts;
right?
And aren't the artifacts going to be really
specific to the type of testing I'm doing
or something?
Do you have any thoughts on, like, how flexible
or generic a rich format needs to be?
&amp;gt;&amp;gt;Alex Eagle: Yes.
So, yeah, in our experience, and, actually,
if you took the office tour, you might see
above the toilet there's an article right
now internally about how do you gather all
the different types of outputs that can come
out.
It does need to be extensible.
So we don't know right now what all the different
tools will emit.
I think there are a lot of things that are
very common or at least I hope we can model
a great deal of similarity between tools,
so things like screenshots and videos, you
just associate the mime type.
Ideally, you associate them with the test
case that passed or failed.
But that will work in both, you know, Android,
iOS environment, or browser, WebDriver environment.
I think there's certainly, you know -- I'm
taking kind of a risk to propose this format
without having figured all these questions
out.
So there's obviously a risk this doesn't succeed
and we continue to have large text files.
I hope that we can find a model that, you
know, let's say the 80% rule that covers 80%
of the stuff with only 20% of the work.
&amp;gt;&amp;gt;Bryan Robbins: Cool.
&amp;gt;&amp;gt;Alex Eagle: All right.
Thank you.
&amp;gt;&amp;gt;Sonal Shah: Thank you, Alex.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>