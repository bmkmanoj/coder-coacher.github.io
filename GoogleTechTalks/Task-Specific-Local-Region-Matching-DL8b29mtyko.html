<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Task Specific Local Region Matching | Coder Coacher - Coaching Coders</title><meta content="Task Specific Local Region Matching - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Task Specific Local Region Matching</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DL8b29mtyko" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so let's get started um today we
have with this horse oh I think oh no
sir um he's a graduate student at UCSD
in the summer using turning and hondas
research lab and third round he deals
he's going to give this preview of some
work is going to be presented at ictv
doctor thanks for the instruction should
just use the lot better you know to flip
so this talk is called task specific
local region matching and this is work I
did at UCSD my advisors search blonde G
and Peter dollars the other co-author
he's a he's finishing his PhD at UCSD
and he's going to be at Caltech next
year and please feel free to interrupt
me at any time to ask questions if
anything's unclear so the general
problem that I'm going to be talking
about is the so what's known as the
correspondence problem in computer
vision most people are probably familiar
but just in case it's when you have two
or more images of the same object or the
same scene and you want to find pairs of
corresponding points or points that
correspond to the same physical point
and there's many many applications for
this this kind of like a low-level
vision task and we're staying agnostic
to any possible application but there's
many applications like three
reconstruction and recognition and so on
the challenge is that in the two images
corresponding local image Regents might
look substantially different because it
might be various geometric and
photometric transformations that take
place so this is kind of a thought
experiment that I like to go through
what's the brute force approach to
solving this problem so this is a not at
all practical in any way but what's you
know what's the first thing that comes
to mind how would you solve this problem
so you could do is go through every
possible patch image patch in the first
image and crop it out and then generate
a set of different versions of that same
image patch by applying a bunch of
different transformations
so this row is a bunch of scale changes
bunch of rotations contrast changes and
so on so obviously this list would be
infinitely long because possible
transformations is theirs they're
infinitely many and once you have this
list for every for every version of the
patch in the list you go through the
second image and you you know again
slide a window around until you find
something that looks exactly like but
what you're searching for so in this
case the image is rotated so one of the
rotated image patches gets matched so
there's there's two problems with this
right that make this impossible
practically the first problem is you
don't want to search through every
single patch in the image in both
actually images and the second problem
like i mentioned is you don't want to
generate this list of all possible
transformations because there's
infinitely many so how do people solve
this now so just as a show of hands how
many people know the sift algorithm okay
and how many people actually used it in
that application ok cool so I'll be kind
of picking out sift but by no means do I
mean sift in particular just happens to
be the most popular algorithm right now
so there's there's three steps usually
like most of these algorithms that you
know again every year a new one comes
out so but there's usually pretty much
the same three steps which are decsion
description and matching and I'll go
through what they are first step is
detection and this basically tries to
get at the first issue that I mentioned
which is you don't want to slide a
window over every possible patch in the
image so you basically want to want
something more sparse so people usually
do is they run some sort of saline see
detector that it finds the interesting
patches in the image or the patches in
the image that contain a lot of
information and there's a lot of
different examples of algorithms
probably the best-known one is Harrison
first nur well which are slightly
different but we're very similar and the
easiest thing you can do for this is
just do random sampling right to pick
random random patches in the image and
people have actually done this and
gotten things to work
and the detection step this this work in
particular is not going to try to do
anything interesting with detection
we're just going to take some off the
shelf detection algorithm the second
step is description which is where you
go through every image patch that's
centered on all the points that you
found in the first step and you compute
some set of statistics right and you get
this high dimensional feature vector
that gets assigned to every point and
the goal is that the this feature vector
is invariant to all kinds of different
transformations meaning if you take this
image patch and you compute the feature
vector and then you transform this patch
maybe you rotated by 90 degrees and you
compute the feature vector again ideally
you get the same exact thing so again
many many different examples of shape
contexts so the easiest thing to do here
is this should take the raw pixel values
but obviously the problem with that is
you know if you rotate the patch by 90
degrees the description is completely
different and the last step is matching
which is basically taking advantage of
the fact that you know we assume that
these descriptors are going to be
similar for similar image patches so
people do either nearest neighbor search
or they threshold the distance in
feature space so you go for every image
patch in the first image you look at all
the descriptors in the second image and
you find that the closest one in feature
space or you threshold the distance and
there's a bunch of different heuristics
that people use on top of that so what
are the challenges in designing these
descriptor algorithms there's basically
two goals the first goal is that similar
patches are going to get descriptors
that are close to each other in future
space and image patches that are they're
not similar are going to get descriptors
that are far away from each other so
that when you do nearest neighbor search
these two are going to be a match which
is correct and there's there's two
things going on here there's with two
two properties
the descriptors there's invariance and
there's descriptiveness so invariance is
what i mean by invariance is when you
have a patch that gets transformed
somehow you still get a pretty close
descriptor and descriptiveness is when
you have two different patches you get
different descriptors so the way I think
of this is there's some sort of there's
some sort of spectrum between
descriptive and invariant so for example
if you wanted the description is
completely invariant to all kinds of
transformations you could just output
zero always right and then no matter
what you do to your to your image patch
no matter how you transform it rotated
scale illumination so on you'll always
get the same descriptor so it's
completely invariant but it's obviously
completely useless because then you'll
get a bunch of false positives and on
the other side you could just take like
I said the raw pixel values which would
be very descriptive but it would be not
robust whatsoever penis future right
exactly so that's a another version of a
descriptor right so that fall somewhere
in between right on this line it's so
it's not completely descriptive but it's
not completely invariant right so maybe
that's not invariant to illumination
changes or color changes or things like
that and in some cases maybe don't want
that because maybe you don't want
rotational invariance so and that's the
question I'm really getting at is you
want something that's kind of in between
on this line and the question is where
exactly do you want to be here the other
issue is when I say invariant will
invariant to what kinds of
transformations rotation illumination
change scale shear all this kind of
stuff so where exactly do you want to be
on that line the answer is it depends on
the task right so here's just a little
example let me explain what these these
images are these are the same persons
face right and it's the same same patch
except there's lighting change and this
is the same exact patch but one is
rotated
and if you were working on some
application that dealt with faces and
you wanted to find correspondences for
faces then you'd probably want these two
to have similar descriptors and not
these because usually faces are you know
the right way up there's no there's no
rotation and if you are if you're
descriptor actually does have complete
rotational invariance it might actually
hurt you because the more invariance you
have you're taking away from the
descriptiveness of the of the
descriptors so here's another
illustration like I said I'm going to be
picking on sift but if you plug in any
other algorithm that you basically get
the same thing so this is a little toy
experiment that we did the by no means
is this a practical application but what
we did was we have letter pairs where we
switched black and white right and then
we ran the Swift algorithm to find
matches so each each letter pair is
separate right that's why you don't see
any lines between between different
letters but you can basically see that
it totally doesn't work right of course
you could argue that you could go into
the swift code and add a few lines and
basically make it work on this
particular example because it's a pretty
pretty simple one right but the point is
that when sift was designed when the
algorithm was designed they had a
certain certain transformations in mind
that they wanted to be invariant to like
scale rotation some illumination stuff
and this particular transformation was
not in that list right so usually when I
when I talk about sifting then all the
related algorithms I call those
general-purpose descriptors they're
designed for to fit most applications
but when the application becomes a
little bit more exotic or different than
they fail so how do people usually deal
with this they have to either go in and
you know design a completely new
descriptor from scratch or they have to
go to the algorithm and like tune the
parameters and maybe add some sort of
heuristics to make it work
and the idea behind our solution is to
pose this as a machine-learning frame
problem and basically to learn a
matching function right and learn from
examples so the difference is here let's
look at the difference in terms of what
work you have to do to make this to make
this work right either you have to go in
and change the algorithm or in this case
you have to go in and provide some
training examples and in some cases it's
actually easier to provide training
examples it's more intuitive because
changing the algorithm it's not always
clear you know how to change the
algorithm to make it work for your
particular application and we're using a
boosting the viola Jones type framework
so the algorithms called boom for
boosted region matching okay so this is
a very simple illustration we want to
build a classifier such that when the
input is too similar patches and outputs
true and if the input is two different
looking patches that outputs false so
it's a binary binary classifier so this
is that same example again and what we
did here was we trained with the first
13 letters of the alphabet and then ran
the algorithm on the whole awful alpha
bit so you can see here the training
data is basically free just because of
the way that the data was generated but
again this is just a toy example to
illustrate the point so let's let's look
at how what the training data looks like
the training data is basically a bunch
of patch pairs right and they're
positive if they're similar and there-
if they're if they're not if they don't
correspond to the same point and during
testing like I said we basically taken
off the shelf detection algorithm we run
it on both images so each image is now
represented as just a bag of patches and
then basically run the classifier on
every possible pair from from both
images so the first thing that should go
off in your mind was like
is slow right because let's say both
images have about n patches this is
basically N squared the classifier gets
called N squared times and at first you
know that's that sounds pretty bad but
this is very similar to the object
detection problem where people slide a
window across the image right and if the
image is roughly n by n pixels the
classifier again gets called N squared
times and one way that people have come
up to it to deal with this is the the
Olin Jones framework with cascade so
again just as a show of hands but I
guess most people have heard of this ok
cool yeah so this I literally stole this
figure from their paper and I just
crossed out some windows and I put patch
pairs so this this kind of thing is
helpful when a lot of the test samples
are negative right in our case they are
and same with the detection detection
case right if you're looking for faces
in the image than most of the sub
windows that you look at are going to be
negative so this is a and i'm just going
to fly through this each node is a to
boost classifier and this is a decision
list where in order to get classified
true you have to pass all all the
classifiers and if at any point you get
classified as false the trainings the
testing example gets thrown out rejected
so the basic idea is that most of the
times that you called classifier it's
only going to execute you know the first
couple of stages and you save a lot of
computation that way it's another
question is what are the features that
we use to feed into this boosting
framework right so I'll go over like the
what the viola and Jones features are
again probably people are familiar and
we just had to make a minor modification
because in the object detection problem
you want to compute features for a
single sub window and in this case you
want to compute features for a pair of
image patches
so basically this is some slight
notation I basically want some sort of
scalar feature given to image patches so
what we do is we compute to patch
features for each patch we get these v1
and v2 values and then the pair feature
is just some function G that combines
the two outputs so I'll go through a
visual of that this is for a standard
object detection like standard viola
jones thing they use hard type features
so let's say you want to detect eyes say
and the features look like a bunch of
boxes and the value of the feature is
you sum up all the pixels in the green
boxes and you subtract all the pixels in
the red boxes right and you get some
scalar value and then on top of this
they use a threshold a really simple
stump classifier and that gets fed into
a Debus so in our case we actually have
two patches so we have two hard type
features one for one for each patch and
we apply them and we get two values v1
and v2 and that gets into that gets fed
into some other function so I'll say a
little bit more about these features
that like all this kind of stuff fully
we didn't invent obviously it gets used
quite a bit but it's hard to say where
it came from there's a bunch of papers
on all these boosting frameworks but one
cool idea is instead of just using the
gray scale version of the image patch we
compute a bunch of different image
channels like the gradient and the
gradient orientation and so on and hugh
and so on the other cool thing is
instead of just taking sums inside of
these rectangles you can actually
compute the histogram and there's this
trick that you can do it's kind of like
a dynamic programming trick called
integral integral images makes computing
these features really fast and it works
for both histograms and if you just want
to do the standard some stuff so in some
cases the
one in v2 are actually histograms in
which case the this final feature is the
l2 distance between the histograms so
the interesting thing to notice here is
that this these features kind of have
all the ingredients that you know all
these different algorithms usually
contain like so for example if the image
channel is the gradient orientation and
you do a histogram and then you do I'll
to l2 distance that starts to sound like
sift right the nice thing is that you
can very easily add on into this
framework different different kinds of
features and different kinds of image
channels you have a bunch of pairwise
matches then the first time you can do
is for the pixel transformations that
these matches employment trying to do
history of mobilization between the
industry and so forth prior to doing any
feature extraction that's true that's
true so it's a it's a different take
it's kind of like generative models
versus discriminative models right so
you can model the actual transformation
that happens but here we're just
interested in a like a binary decision
are these the same or not right so you
know in a way this is actually easier
you don't really need to know what the
complex transformation is because it
could be very nonlinear and complicated
right that's a justice is generalization
with no shelling by future replacement
to take
it was in the first place them their
counterparts and future replacement
search for this I quiz that by I'm not
sure from the latest anymore I'll have
to check that out I me after there's
there's there's some work on image
analogies and like transforming one into
the style of the other and that kind of
thing yeah so one question is should the
to hatch features be symmetric so like
we saw in this example they they happen
to be symmetric right there exactly the
same but it turns out that if you lift
that constraint and you just let them be
whatever then actually you get these
kind of intuitive structures so we did a
little experiment where we trained it on
90 degree rotations and what you get is
features that look like this and not
showing the like all the information
that's that's inside the features but if
you just look at the the rectangles of
the HAR features they really make sense
because right because the right halves
are always the 90 degree rotate rotated
versions of the left halves so what's
happening is it's taking all the sum in
this rectangle in some this rectangle
well those boxes are right on top of the
same exact pixels so you'll get the same
value for the book for both of them and
when you do when you subtract them where
you do the l2 distance you'll get zero
and that's for a positive for a true
match and then if there's two different
patches here you'll get some nonzero
value so it's easy to to train or to
learn
so here's another philosophical question
we've done all this work to design these
features so doesn't this kind of go
against what I said before how we don't
want to spend all the time you know
designing the algorithm and so on well
the difference is these features get
designed once and then for each in your
application you do basically just
provide the training examples and it
learns whatever it needs to learn it
learns the types of invariance is that
it needs the parameters are learned from
the data I'm actually going to skip this
stuff okay so I just want to get to the
results first so we wanted to do a bunch
of experiments that kind of uh were all
over the place basically right because
one of the points is this isn't for a
specific application this is kind of a
general method so we did fingerprints
faces and graffiti image they you ever
seen any of these papers that they use
they always use the same image and we're
comparing to sift like I said there's
papers since sift the claim to
outperform sift but it's usually pretty
marginal improvement and it seems like
people can never agree on what the best
algorithm is so I'm just taking it
because it is probably the most widely
used and for the detector we're just
using the difference of gaussians
detector that sift uses just to be
completely consistent right and isolate
the descriptor proud the descriptor in
the matching part and for all the
experiments were using pretty much the
same parameters maybe like a few
parameters were changed but all the
features and stuff and all the image
channels are all the same this is the
first thing these are fingerprints so
these are there's a cool little tool
called I'm not sure how to pronounce its
finger or spin J which synthesizes
fingerprints and these look pretty real
it's a printing an application you can
get a demo version for free but I think
if you want the actual thing you have to
pay for it and what they use this tool
for is to actually test fingerprint
tching algorithms so this tool has tons
of different parameters that lets you
control you know the rotation and all
this kind of stuff in one of the
parameters is how hard the person is
pressing on the sensor right so we
generated a set of fingerprints where we
took each fingerprint we generated two
versions one where it's light pressing
and one where it's hard hard pressing so
I kind of zoomed in here on two local
patches and you can see they look pretty
different right and this is kind of
interesting an interesting application
and this is an application that you know
has been in the industry and people have
developed very very specific algorithms
for for solving this problem so we tried
running sift oh sorry I if we have to
mention the other algorithm we're
comparing to they're called pixel is
just taking the raw pixel values right
it's just a baseline so we see that SIF
doesn't do well at all again because
this isn't really the sifter wasn't
really designed for this right but with
some with some training examples with
train boom and we get pretty reasonable
performance and this is this isn't this
isn't an ROC curve for fingerprints it's
an ROC curve for the point matches so
some people get confused to that so yeah
so when I say we test about 33 image
pairs people say like oh that's not
enough old is actually like thousands of
points in each image so you get a pretty
good idea
was this person it's the classifier
threshold so the classifier returns a
probability right says probability
eighty percent these are a match so
basically threshold that probability
sweep through that the second experiment
was with the Yale face database there's
this data set to where they have this
rig or people sit and within two seconds
it takes like hundreds of pictures of
the same person from different angles
and different eliminations so the nice
thing is that the photographs are
registered right pixel register there's
no there's no shift or anything like
that so we took pairs of images with
different illumination and we we did
some we split the data in half right we
use some people for training some people
for testing this was the result the
interesting thing is that just taking
the raw pixel values actually does
better than using sift again because you
know faces are pretty pretty specific
they don't have the usual kinds of
transformations and sift is probably
better designed for for geometric
transformations like rotation and stuff
like that okay and this is a another
cool example this is a sift and then
sift with some post processing just just
showing the matches you can see there's
a lot of incorrect matches right so so
if the line is vertical that's basically
a correct most likely correct match just
to visualize so we get a lot more
vertical lines and this is the picture I
was talking about that you see in a lot
of these papers basically this is the
reference picture and then each one of
these material these are actually not
synthesized in any way and somebody
actually went through and clicked on
clicked on points and they computed the
true kamagra fees between these and
that's available on the web
okay so that the previews the previous
images those were actual real images and
somebody clicked on the points to get
the filmographies and we use that for
testing so we use the real data for
testing and for training we took another
graffiti image and this time we
synthesized a bunch of different a bunch
of different out of plane rotations and
so on and we use this for training so
we're training on synthetic data and
testing on real data and we get pretty
good results the blue curve this yeah I
guess you can't see it all but the blue
curve I shoulda saw this before the blue
curve is is our algorithm the red dashed
line is sift and the black dotted line
is just the pixel values so obviously
the pixel values don't work very well
because right if you rotate the image
patch you get a totally different value
and this is a this is an interesting
figure 2 so this is these are all the
different image channels that we used
like I mentioned we use just the
greyscale pixels Hugh histogram
saturation gradient magnitude histograms
of grading orientations and then cosine
and sine of the gradient angle and this
just shows which features it's picking
for for the different data sets and some
interesting things to note here so the
fingerprints and face is that that date
is black and white so it's not picking
hue or saturation which makes sense
because those channels don't really make
sense for that data and what else is
interesting so for fingerprints you see
that it's picking mainly the channels
that have to do with the gradient
orientation and and for the graffiti
experiment the auto plane rotations it's
not picking the image channels that have
to do with gradient orientation because
as you as you rotate it the granule
mutations to serve them change
so some open problems in future work so
the biggest the biggest problem with
this is providing the training data
right so you might have noticed all the
experiments that we did we got around
that by using either synthetic data or
write data that was collected in a very
controlled manner and in a lot of
applications you can get away with that
you can you can train with synthetic
data and then actually test on on real
data but if there was some some easier
way to train to maybe some people call
this weekly supervised do people do
training for object recognition and
detection in like a weekly supervised
manner where they give a picture and
they say okay somewhere in this car or
somewhere in this picture there's a car
right and the algorithm automatically
learns something like that would be
interesting right so let's go to see
here yeah and I think that's about it
two people have questions sure you have
to getting to that
you imply that this about the woodblock
always better than sift if you're not
together can we make some some kind of
generalization a lot or it's just a I
wouldn't go that far so here's an
interesting thing we haven't actually
tried this but we're hoping to try this
so all the experiments that I've shown
are kind of specific tasks right and
that was kind of the theme with the
papers or if you have some very specific
task and you want to learn a matching
function that works really well on this
this example and that seems to work but
what if you had a huge training set for
all kinds of things right just like
arbitrary 3d matching right could it
actually work in that case that's a hard
problem right because you know it has to
learn that's really fine like the sweet
spot on that on that spectrum and I'm
not sure I can't confidently say that it
would work better than so it is quite
possible that in a completely general
setting something that's hand-designed
likes if it might actually work better
this will ensure your complete
complement of transformation that your
complete the transformation pretty much
yeah I know what else each other night
in which is any provisions within the
algorithm for running these in the
transformation in the independent way
or getting pixel values in histograms
that are counting the pixels in the
bottom so we did do that we did
histograms in those boxes so right all
those things like yeah they were both so
we throw in so the nice thing about this
boosting framework is you can throw in
millions of features we randomly
generate these features right and then
the boosting algorithm picks the best
ones according to the training data true
true with the you know with certain
things in mind but tuning the actual
parameters happens automatically right
given the training data what do you
think is the biggest difference between
tuning the parameters for something like
sift and selecting the features to learn
on for your approach I'm sorry if you
compare the pros and cons of each sure
so I mean I really think that once you
enemy I saw this one I was doing all
these experiments as you start adding
more and more features it actually
starts helping less and less so I really
think like once you have these this kind
of set right there's not going to be
much that you can add that's going to
really make a huge difference so I
really think this is more of a one time
designing thing right you you design
this this pool of features and let the
algorithm pick the parameters and stuff
that seems contrary to do right because
if you're seeing more features it your
performance doesn't improve on that much
that suggests one can get away with the
smallest set of futures we should be
that challenging the parameters well no
no no so okay I ask you are you saying
I'm saying that like past a certain
point it's not going to make obviously
you have to have some some reasonable
features to work with right otherwise
nothing is going to work but if you take
the set that I showed and you try adding
stuff like unless you have some very
very specific application that has some
sort of strange transformations
maybe it'll help but otherwise you did
frame this as being tasks right a lot of
different tasks to know there's a fairly
face everybody will call camels and
bottles and over the actual title but
they found that the standard complement
sort of texture features weren't very
helpful with the problems and they had
to do the contour of interest I put it
in yourself gradients great okay so so
sure no though I guess that's a
reasonable question so the nice thing is
that you can add add things like let's
say you want to do contours you're like
edge type features right all you have to
do all you have to do in this in this
framework is add an extra image channel
that contains like the canny edge
detection we actually think we actually
try that at one point but it was too
slow and it was making a huge
improvement but and I think that's a lot
easier than going into sift and sift
head again not just sift like this
family of descriptors that are similar
to sit going in and trying to tune some
of the parameters and some of the
parameters are very non-intuitive like
force if for example sips as a histogram
right so you have to pick number of bins
that you're going to have how do you
pick that number did you try your
algorithm to learn because there's
another paper at CD pro7 that we didn't
know about this paper while we were
doing this obviously it's by a brown and
what's the other guy's name we can we do
yeah that paper yeah cvpr seven if you
look that paper up so did it a similar
thing except where they take kind of
like a generalized sift descriptor and
they they learn all the parameters but
but in that case you're still kind of I
guess yeah I guess it's analogous but
you're constrained to the to the sift
structure right and i think this
framework is a little bit easier to
extend like let's say you want to do
something with the edges right you just
throw in that image channel and not
change much else but
so only men you mentioned is that the
biggest bottleneck is the training data
where you get to paint mirror from right
if you or something except it doesn't
need a greater so is there any thought
in for combining these different
approaches there something which doesn't
actually need training get our provides
of the training data right so that's
definitely a provocative idea actually
one of the reviewers that I ccv
mentioned like you know why don't you
use sift to get you some training data
that sound like a reasonable idea the
only thing is in some cases like like
Anna fingerprints I go back to that
slide SIF does so bad that probably no
no learning algorithm with people to
learn from this fantastic thing to bring
smashes out there we should do much
better than the EPO ninety percent right
right that's from Susan and then the
really interesting can reveal a
you do something they human we can
improve it yeah so using something like
boom and doing some some hand designing
on top of that yet that's that's an
interesting line of work yes I guess
there's two two directions you could try
learning something that's completely
general using a huge huge data set or go
in the opposite direction and look at a
very specific problem and try to
actually improve the state of the art
for that specific task using some
learning on top yeah but in in this in
this work we try to keep all the
features the same for all the
experiments and try to change as little
as possible basically other than the
training data any patch crews did you
wind up using your training for training
dependent on the experiments but I think
on the order of a few thousand i think
and i get in the most cases you know we
picked the data such that the training
date is free basically so so in that
case like the only bottleneck is the
memory and how much how much you want to
wait for it to train
kept for you an experiment surface look
at the scene if you should remove
relaxing the huge channel will happen to
the than some other features which make
the performance comparable to what we
deserve um it'll be worse so the reason
I say that confidently is because you
can see here Hugh is the one that it
picks the most so if you take that out
and you don't add any sort of like color
information it'll definitely do a little
bit worse I it I'm not sure how much
worse but there's a lot of information
in the collar train and color like again
Hugh histogram that's invariant to
rotation more or less so that's why it
works well right he just wouldn't pick
yeah we're he saying like what if you
take out something that it actually does
pick all right so I obviously then the
performance will go down a little bit
yeah so like I said you have to have
some reasonable set of features to start
with so something something that has to
do with color maybe if it's not Hugh
maybe it's like ran RGB histogram or
something you try the injecting some
noise to your training set see hopefully
does the one was degrade you might be
picking on the run formation to learn
you know so we actually yeah we actually
had to introduce a little bit of noise
in terms of the location because the
detector doesn't always detect in
exactly the same point in the two images
so that's just a minor detail but
everything is labeled airship Oh totally
mislabeled um no we didn't try anything
like that
but I mean that kind of goes along with
this whole idea of like how can you make
training easier I think it's a bigger
question machine learning in general how
do you make training easier you know how
do you get away with noisily labeled
data alright so if we tried something
like using sift first to get the
training data there would be a lot of
noise so we'd have to probably change
something to make it very robust to
noise yeah see you get a probability and
that's how we draw those RC curves so
that's a pretty useful thing so i should
mention this there's another cool thing
about this is by changing the depth of
that cascade you can actually control
how fast this thing is right so there's
you know there's a trade-off obviously
between how fast it is speed and the
accuracy but for some applications maybe
you have some real time application
where you don't really care about the
accuracy as much you can easily change
like one parameter right the depth of
the Cascade and to make it run faster
whereas with sift it's not really clear
how you would do that so a question
design is it just be a binary classifier
fact is not just given
that's rude mr. you actually get a
probability
was there aren't any more questions mr.
speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>