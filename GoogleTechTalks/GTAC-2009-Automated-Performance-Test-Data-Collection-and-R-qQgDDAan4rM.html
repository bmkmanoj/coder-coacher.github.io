<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2009 - Automated Performance Test Data Collection and R | Coder Coacher - Coaching Coders</title><meta content="GTAC 2009 - Automated Performance Test Data Collection and R - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2009 - Automated Performance Test Data Collection and R</b></h2><h5 class="post__date">2009-11-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qQgDDAan4rM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">they won't come back so now we're gonna
have the last presentation with living
bones and David Anderson and they're
going to present something that I really
really like because it's one of my
favorite interview question how to
report and collect data test data from
performance tests and here we go did you
know that when it comes to load times
Yahoo sort of nearly a ten-percent drop
in traffic when they introduced at four
hundred millisecond delay into their
page load times similarly ambulance or
nearly a one percent drop in sales we
need to use that one hundred
milliseconds worth of delay page load
times cost real money and real traffic
so we need to keep them under control
thank you all for your attention this
afternoon at Google for letting us be
here today my name is David Henderson
and I'm a front-end developer I can
normally find working in c-sharp and
JavaScript and today I'm going to give
you a bit of background on to how
performance client-side performance
affects our users some of the simple and
easy things that we can do to actually
fix that as developers and testers and
then a little bit later on are we
talking about the reporting aspects of
the project that we put together
I'm working on the web accessible parts
of our application today I'm going to be
talking about how we went about
automating the process of collecting
client-side performance data and how
this fits into our test and development
cycle as well as fitting it back into
the business we work for smartFocus
digital a we develop a suite of
web-based software as a service
marketing tools our software allows our
customers to generate large complex
digital marketing marketing campaigns
and see the results of this so the
organizations that use our software tend
to be fairly large and have a fairly
large user base and this user base uses
our software to target their customer
base they they do this by generating
their creatives either using our
web-based editor or on their PC and then
uploading it into our system it means
that our users tend to use our
application for hours on end and
unfortunately they they tend to be on
the end of highly contended network
connections so we needed to have a look
at how we could optimize our site and
specifically for these users and then go
from there so we needed to find tools so
that we could do this see our issues and
start setting a baseline so that we
could go from here to here and and
because we are test strategy relies
quite heavily on test automation we
needed to find a way we could automate
this process and do the reporting of it
so we needed to just work through the
process and start recording all of this
data so I'm going to pass you on to Dave
who's going to explain the consequences
of having us
those sites so if you've got a slow site
there's some really quite easy things
that you can do about it and and these
slow sites they tend to cause less user
interaction and these tend to lead to
higher bounce rate so if users
dissatisfied with a load time on the
site they'll often use it for a little
bit get fed up and then move away and
find somewhere better and this leads to
high bounce rates so as Dave mentioned
earlier we've got a pretty complex
system with a large number of users and
they've often got slow share connections
so we really need to optimize the
product to make sure that these users
are taken into account and it's a give
them a really good experience any
Content redundancies or page bloat
within the site do slow down the user
experience so it takes longer to load
longer to get into a state where it's
usable and if you're using a product for
a long period every day as it's kind of
like your core business tool that you're
using you're going to get annoyed after
a while and you're going to get fed up
and we really don't want that so we
really need to look and see what kind of
things we could do to get around this
fortunately there's some really easy
things that we can do to make some quick
wins to get rid of some of these
contents some of these performance
issues we can do some compression
combining and minifying static content
such as JavaScript CSS files and for
each bit of content that we don't have
to send back down to user they can get
it it comes down quicker and they don't
have to it doesn't have to go over our
infrastructure or bandwidth caching is a
really important thing to get right if
you don't get it right users got to pull
down the same content over and over
again from the server takes time takes
bandwidth again going with this HTTP
request can be a bit of a killer on
particularly bad connections with high
latency each extra HTTP request occurs
the latency penalty and also some
browsers limit the number of concurrent
connections so this can be a real
bottleneck
so if we can reduce the number of HTTP
requests all the better redundant
scripts CSS files things are getting
cluded twice on your site this is pretty
easy to happen if you're running a
really complex site where things kind of
mesh together you've got master pages
and systems like that so it's pretty
easy to get duplicates cheaper to get
scripts cropping up so we can pull them
out again it's all better for their
energy to experience and images we've
all had images where it's been using the
wrong image type for a particular the
particular content that is showing so if
you if you can look for that get it
right again it's less content in the
user needs to download nerve at all we
can use spriting putting together
similar images it's one gnarly large
image that gets downloaded the beginning
of the page load and then you can box
out using CSS to get the right content
to share in that particular area again
reduces of connections reduces the
downloads and can again be cash so
before we started this project we didn't
really have any metrics available to
show the end of use of performance of
our sites it's a large app it's lots of
data so there's often some pain points
and slow bits that we kept on going user
comments saying this bit slow and it
really used to be faster which was often
kind of a court came from where we'd
made sequential additions to add new
features to an area and we didn't really
think about the consequences when it
came to use a performance so as a
developer when we used to tackle this we
did the did some development and can't
really see the results of what we're
doing we could kind of rely on stop
watches and Firebug to when actually see
how the request took Firebug and fiddler
to actually look at the complexity of
the page and see what requests were
taking place what content we're
downloading but we couldn't really get
any consistency in that particular
process so we went out looking for tools
to help us kind of optimize this
automator and make it a whole lot more
consistent so when we started looking at
this issue which is about a year ago now
there were two main tools available to
us there was a Wells pay chest which you
may have heard about and there's also
why flow from Yahoo we gave these tools
a bit of an evaluation we found it
wisely kind of medal needs a little bit
better is bit easier to configure and it
provided all the data that we wanted we
also provide some really nice
recommendations to improve the
performance of your site and you can see
up there on the right it can provide
some really nice reports a bit of
history about why so it's a tool
developed by Yahoo's exceptional
performance team they initially to help
optimize their own such sweet at sites
and is then released is a free tool to
the community and basics a a basic
firefox plugin it hooks into Firebug and
it analyzes the page as you load it in
in the browser so it looks each of the
components within the page and analyzes
there the size the type and also the
caching things that go along with that
and then produces in really nice reports
so here are the actual lint rules that
why slow uses to make these
recommendations so as you can see it's a
pretty big list there and it's pretty
comprehensive if you look a bit closer
you can see that there's some of the
really easy wins that kind of provide
maximum benefit for not very much work
that we were talking about earlier so
why does cover these things that we're
looking into and can help us to monitor
these things so then so now that we know
kind of what we're looking for we need
to find out where these things are
occurring within our sights go forward
and fix them and then keep on monitoring
the situation so we make sure that we
don't get any regressions so at this
point in our story we were kind of using
wisely as a kind of ad hoc manual
testing tool so say if I was doing some
development work I would take a baseline
test with wisely save the report down to
a PDF or something do it do the
development work to make them
improvements to that particular area and
then run the wife slow tests again and
this would be a pretty manual pro
yes and it would involve eyeballing the
reports and trying to work out the
differences now there's problems with
that we didn't have any historical data
yes we had the before and after but say
we wanted to look and see how it was two
months ago we just wouldn't have that
data available to us so we didn't really
get any consistency and feedback of
those results so this all took a lot of
developer time to actually do that
manual process and is in many software
houses developer time and tested time is
a really precious resource and there's
not a lot of it to go around and in this
environment there's often a lot of
pressure on on the management teams and
product owners to keep on shipping new
features and this can often detriment
the testing of existing features and
kind of the overall performance of the
site so we found it without the data to
show performance issues within the site
where they are and any optimizations in
it that we make the effects of these we
found it really hard to get development
time kind of allocated to do this work
so we need to look into a better
solution so that given Davies our
automation chief is now going to talk
you through how we implemented the
system automated and got it going so I'm
going to start with on your average a
dev and test cycle you got your
developers creating some production code
with some units and integration tests
checking these in continuous integration
service coming along building it testing
it the products being shipped to the
testers who start exploring through the
system and and start feeding back any
bugs back to the developers
unfortunately at no one point are we
actively looking for any performance
gains that we could be doing or seeing
if there are any issues at all so since
we kind of started using why slow as a
manual process we needed to see if we
could automate this and if we could
automate this could we log the results
and then from there start reporting on
them
so we started so we needed to set up
some requirements and from there we
could start doing everything and these
are the requirements that we came up
with so we needed a system that was
consistent it needed to be run quite
regularly and against sequential bills
and produce valid comparable data and
when we needed a system that was
automated the less you have to babysit
it or even just think about it the more
likely it's going to get run this frees
up any developer of resource and in
testa resource fizzes being done the
system needed to be low-maintenance once
configured again it needed to be forgot
about this again freeze up any tester
and developer resource and finally we
needed something that was fairly easy to
analyze this is probably the most
important requirement that we came up
with it needed to be viewed by technical
and non-technical users so that they
could get a clear state of how the
system is at that time and we also
needed to be able to show long-term
trends such as page creep with small
incremental additions to a page suddenly
make the page load a lot slower so how
do we go about doing this well while
we're doing a bit of research we found
that why slow has a small hidden feature
that it allows the reports to be sent
out as a beacon request this is a get
request where the report is put onto a
query string and sent out to the beacon
page beacon page then just passes this
get request and you can store the data
in the database and so it's time to show
all that the component sizes and and
everything that's in there unfortunately
while we were doing this we also notice
that why slow wasn't producing as much
information through its beacon request
as it was I'm through the visible
port so and since why slows to stay
firefox plugin we downloaded the Zippy
file opened it up and found the
javascript that we needed to change
slightly and so like we wanted to see
the primed and unprimed cache size of
components that were on the page and and
then we just zip to back up created a
brand new firefox profile and stored it
and installed it on the firefox profile
and started going through everything and
we then needed a way to start walking
through the system so since you can set
y slow to autorun that was our first
step that was our first step to just
automating everything and we also needed
a way that would be that we could just
load pages go to the page that we want
or like go through a number of pages or
something and and record these
measurements unfortunately for our
system you can't just fire up a page get
the results and let it die down and
because we have longer log on pages so
doing that you just get the logo on page
and everything else you'll start getting
access denied so we needed another
mechanism to start walking through the
system and since we rely quite heavily
on selenium for our testing it was quite
a natural choice and since we write
everything in c-sharp we just started
running it through end unit so we need
fire up our tests and they start running
and we start collecting and all the
information from wasco so and but
unfortunately selenium also has one
small feature that it's quite good if
you're just wanting to test your average
code but for this is quite bad selenium
blocks quite a few head caching headers
from going through it's good if you have
if you just want to test your code quite
regularly but if you want to start
seeing any of those caching information
it you won't ever see it so when we
started running
quite regularly we were started getting
false negatives because you couldn't get
all the caching information and then we
started running this every 30 minutes
against our test on the the COS would
then push everything into our test phone
when we start running it we ran this
quite regularly so that we could try
remove any potential anomalous data
coming into the system and we did find
quite a bit with every so often there'd
be one run in you get something that you
weren't expecting but if you do it ran
straight afterwards everything would
even out again so here's a data flow of
kind of how everything works and we
start by getting a request ID other the
database we pass in the page ID the
serve server name the build number and
the date that things were owned and then
we start running our tests loading up
Firefox start walking through the pages
d the while we're walking through this
why slow starts firing off its
information collecting all the stuff and
then we have our beacon page that's
starting to collect all this information
pausing it and then pushing it down into
a database and and so that we can now
reference our why slow data against our
query quick all the wife a request ID
and and because everything's being
collected we can just always see builds
against runs the next like thing that we
started started thinking about was
because this is we're collecting all
this information we're starting to see
that and pages are getting a lot smaller
and our our paid our timings getting
quicker to load things so obviously the
natural choice is how long does a page
actually take to load but we also wanted
other measurements such as how long did
I alongs take to load up and since our
systems quite complex we have some
Explorer views so how long does a tree
node take to expand and things like that
um and since we've really got the
request ID we just mattered against that
so now we've signed to collect timing
data of things loading against our why
slow data and and we assigned to collect
lots and lots and lots of information
quite quite consistently AB through
quite a lot of number of runs so I'm
going to now pass you over to Dave he's
going to explain how we started
reporting on this so we wanted our
reporting to be pretty much visible to
anyone within within the company and as
we're a web based company really we
develop web solutions is the obvious
choice we make fairly heavy use of
jQuery within our products so he went
out looking for something that could
help us to do the reports we found flot
which is a really nice reporting plug-in
that kind of those leveraged its
functionality to create these these
reports it's a canvas based plugin so it
works in most three newer browsers
there's also a wrapper for IE that kind
of helps it along the way a little bit
we have a c-sharp back end and a sequel
database this and pulls the data from
the database transforms it into a JSON
data set which the browser then pulls
down from a JSON web service as each
page is requested so then we pull it
together in the browser in JavaScript
and we create the reports as you can see
at the top right there and using flot so
if we take a look at the bottom of the
two reports there we've got the page
school report now the set up for this is
we have the builds running along the x
axis the vertical lines that you can see
each vertical line is a new numbered
build so those can happen fairly often
every every couple of days or so
depending where whereabouts within the
iteration that we are we internally run
monthly iterations so they're fairly
rapid and after the after we've done
some of this performance were
we managed to get a regular kind of four
days or so a performance performance
workbook to each month so that's kind of
what would the figures and we're working
with when it comes to look at the
performance of a game from this page
score plot shows each of the why slow
metrics as a separate series and also
the run count which is the aggregate
number of times that the why slow data
has been gathered for this particular
page this plot is just for one page and
it is showing it against against the
bill numbers so if we actually now go
and have a look at some of the data if
you see there we're looking at the
caching and expires headers why so
metric and around the 1.7 mark just
highlighted their big red bar to the
beginning of iteration we did quite a
lot work to unify the way that are
cashing works and this also included
some work to combine the javascript
assets and you can see that the we've
got a huge leap between that between the
bill before it and that build where that
work came into effect and it's been
measured we can see those changes over
time so if we now look at the page size
for the same data you see it's pretty
much the same set up the vertical bars
representing the builds the builds are
still only along the x axis along the
y-axis we've got the size of the
particular measurements we do have the
total size of the page that's that
that's the unprimed side so that's the
size that you will download for the
whole page if you're visiting it with an
M Prime cash we also have the prime
total size which is the third series
down in blue there so that's the prime
size where you get full cash for
visiting the page with second time or
subsequent times and that will be the
amount that you download in those
situations we also break down the
individual components of the page that
kind of make up these sizes both in
primed and unprimed cases so you can
actually see exactly what's going on in
the particular page so if we look at the
same section again you can see that the
prime page size prime total page sizes
drops rather
we as a result of the work now this kind
of ties in with a caching score
increased in the Y slowed pots we made a
conscious choice to target the primed
page size now if you think back to when
we're talking about our users they're
not really the type of users that use a
site for once or twice just visit the
site and then leave it and never come
back they're working with it all they're
working day so it's an obvious choice to
get the caching right so that they
download stuff once and it's done with
and this actually shows here where we've
created these JavaScript sets yes all of
the JavaScript that's included in the
set may not be used in that particular
page but they've downloaded once and
it's a kind of one-time hit after that
they will download a lot less data as
they move through the sites and as
assets will be reused on different pages
as Dave mentioned earlier we also take a
fair amount of timing data in the system
this is again the same setup pretty much
apart from its time along the y-axis the
builds are the same each data point is a
build and that obviously comprises of a
number of runs Dave mention that we're
taking them every 30 minutes or so so
it's actually quite a large amount of
data we do share the maximum average and
minimum load times and we also do the
extra user actions that I use a wood
particular usually complete whilst
they're on the sites such as opening a
dialogue expanding a treeview something
like that and as you can see there is
actually rather a large spike just to
the right of 1.8 there this is kind of
the reason why we have the maximum
minimums it just let us kind of look at
the data and kind of analyze it and see
okay yeah there's been a large spike
here this is something we need to
investigate and see whether it's a
continuous issue this is a recurrent
issue or as in most cases it's probably
something down to do with a server in
the environment for instance we're next
shop so when you update to a new build
it often takes time to compile and that
often shows you in these timing ports if
it's the first thing being run
and so yeah that's that's a timing data
as well now from the ease of analysis
requirements it's all well and good to
have the individual plate page plots you
can kind of see what's going on on a
page at the page level but we needed to
see the whole site at a higher level so
we could just take one look and go this
is what's going on with the site it's
particularly useful for managers so they
can kind of look down and go yeah this
is what's going on so we created the
Delta plot now this shows the change in
total page size for all for all
monitored page size for all monitored
pages on the same plot so each series is
a separate page and there's
approximately 30 or so pages on this
particular plot now as you can see you
can spot large changes in it in a single
version for either a single page or
their whole site as a whole so if we
look at it here you can see that we've
done some work I think I was removing
some excess JavaScript across some
include files or something like that
where the page size across the site just
drops and that's great you can actually
see what exactly what's going on and if
for some reason we included JavaScript's
or excess bloat on across the whole site
we'd see that as a as a whole you can
also pick out large changes in
individual pages so you can see here we
found initially it shows an issue where
by a single page has excess data and
it's it gives you a clue that you need
to go in and look a little bit deeper
and see what's going on and in this case
we did go forwards and you can see in
the next build remedy the problem and
this kind of helps with the turnaround
time spotting these problems if you can
see after each build what exactly
happened on the site you can correct it
as soon as possible flot the plugin also
provide some really nice hooks that we
can use we created it so that you can
highlight a single series by clicking on
either the legend title or actually on
the particular series itself
hovering over any particular data point
shows the name of the series there the
build number and the date so you can
actually see exactly what the particular
data point is for and we also do zooming
so you can see see what exactly is going
on in a particular area in deep delve
right down into exactly what you're
looking for so that's a reporting
section of the site Dave's now going to
talk a bit about the how this kind of
fits back into the development lifecycle
that we had before and how it kind of
augments it so if if we go back to our
initial dev tests cycle we now have an
addition to that we're we're collecting
performance and data and this is being
fed back into the development team but
it's also being fed back into management
team so the operations teams can see
what's happening operations managers and
an dieser to the development managers
because now performance is not really
being seen as an afterthought has
actually been seen as a continuous
feature of our application so they
signed to allocate time to actually fix
these issues because they want our
customers to have a very good user
experience while working through
everything but that's all really nice
but we have some thought and how could
we start feeding this back into what our
testers are doing and and so we went
about creating a testers heads-up
display it's an overlay that we put on
to our website that um pulls in a number
of data sources so that well a like
testers exploring through the system
they get to see what's actually and some
of the extra information and so our
third is a jetpack extension for Firefox
so it just pulls in everything and
starts showing everything like we hear
like they've shown earlier we have Jason
feeds for a lot a number of things and
is this is pulling in the JSON feed from
all our why slow information so while
you're exploring through the system and
you have that information right to hand
and you can see everything that's there
and we also started pulling in and feeds
from our source repository and from a
bug tracking software so that it
hopefully removes the hit and miss that
can potentially happen with any manual
testing because now term testers while
they're doing their job can see that
this part of the page has changed I'm
going to concentrate just on that and
start working through that and that will
hopefully potentially allow them to
track bugs back to the commits that made
them so here we have our final test and
development cycle there's three feedback
paths that are happening we've got
initial one in the blue that's doing
what what most people tend to do plus
we've got our performance information
this is being fed to the management and
through to and the developers as well as
now to the testers through their tests
as heads-up display the testers are
collecting quite a bit of information
about the structure of the page what
changes are being made to and any
previous bugs or features that have been
added to it and this is being fed back
to them so that they can start and
finding bugs and feeding that back to
the development team so what's in it for
the product owner now well the product
owner has am is starting to see the
benefits of lots of information flowing
around to different people they can see
when if changes or see the changes that
are happening both positively and
negatively and and so now they can start
allocating and time to work on specific
things not just trying to give out
features to our customers that they want
but we can also start allocating time to
bugs that could potentially just be a
quick fix or bugs that are and that of
happen through regressions and they'll
be done a lot quicker and because
there's performance data is now coming
back to the product zone and they can
see what's happening it's a lot easier
to justify any chain like time
allocations that they start making so
the question is now where are we we can
now got a system where we can view
historical data a glance and it makes it
really easy to spot trends as a
happening both with between individual
beaten builds so the immediate changes
and also the long-term trends as they as
they grow and we have seen up to an
eighty-five percent decrease in prime
page size as a result of this work and
this has gone forward into the customers
and actually makes their page load times
quicker so as you can see there the
prank the prime page size has has
dropped and and these prime these
content improvements do move forwards
into the infrastructure team because the
infrastructure team now have to spend
less less money on servers to actually
support the same amount of convents
bandwidth to actually serve it up and
that makes everyone happy and in the end
if we get happy infrastructure team
happy developers a happy support team
because of getting less queries because
obviously we're as software as a service
company so there any issues that our
customers have they come directly back
to us and I know obviously cost money I
mean and more important than me we have
happy customers happy customers I mean
they give better recommendations to
potential new customers so it's an all
round win so in conclusion we've we've
kind of taken you today from where we
were before running ad hoc manual tests
with no instrumentation or metrics
available to us through to a fully
automated system that uses selenium RC
and you to end unit c sharp to actually
automate the collection process why slow
to gather the data and run the metrics
and produce the recommen
asians we've introduced a reporting
portal based on C sharp Jason jquery and
floats that gives a free flow of the
information that we need to help us fix
the bugs find them analyze them and
actually get the resource allocations to
do so from a tester's point of view we
now have the third which really makes it
easy for the test is to examine the area
that they're exploring at that point in
time and get all the information that's
relevant for them at that point of time
without having to go off do some context
which is to actually go and find that
data thereafter it's right there for
them so it makes their lives a lot
easier we've seen the site improvements
for our customers with the side with the
page size decreases and up to
eighty-five percent in some cases and
has a bit of a bonus all around and yeah
we just had generally good feedback from
our customers there is a demo of the
performance of the reporting portal with
some test data available some
performance smartFocus digital comm edge
to go have a look have a click around
and hopefully there's been something to
take away from this today and you can
help implement that your own companies
thank you and there are any questions
the Trey Trey Trey different yeah do
mentor to return to the graph with your
timings yep oh yes there is much I feel
you know maybe I'm wrong but you guys
seems like he didn't wrong totally wrong
dude I ocean no we start it up with a
timing go two seconds yep it's got it
now we bout five sec yeah that is
actually an artifact of what we've done
and
the timings these absolute timings have
increased part of this issue is we don't
actually model currently the customer
connection as it as it actually is we
have the monitoring server SAT right
next to the the test farm so we don't
see these latency delays or in fact
they're kind of throttle bandwidth that
would actually see from a client site
and this is part the future where they
will want to do we want to introduce
some kind of simulation of this this
kind of throttling and some of the
changes that we made like creating these
JavaScript and CSS sets they do mean
increased downloads the first on you to
visit the site and obviously that will
be reflected in these timings so we do
need to do some work on this to actually
make this a bit better more
representative but on the other hand it
does show relative changes which is why
we still use it so if we do see
something that has increased markedly
between us in between two builds we can
then go and investigate it so yeah we
know the shortcomings but as long as we
know them we're okay with them we're
going to do that with a yes but you know
is the you should probably stop fighting
for both eyes and start fighting with a
that oh yeah we've um we've just now
completed all of the work that we've got
to do with trying to get the right page
size or I caching and they're all that
for our customers we haven't actively
done anything for timings yes bad as our
our next goal is to now start moving on
to making things actually load faster
but there's a slight difference between
actual speeds and perceived speeds
because you can make things look like
they've loaded faster and but they
actually haven't they still loading in
the background and where these are more
taking into accounts everything being
loaded rather than the actual perceived
speed that uh that's things happening so
it's trying to work out the right
balance between the two of that so and
exact that point of view leads us
to my other question which is do you
have any criminal requirements
especially bill or problems and we do
now in the last like six months but
before then we didn't it was it was like
I said you know there are for the
initial development and test cycle is
that you know we develop we test through
the CI we get some manual testers
exploring through the system finding and
bugs causing that back to the developers
so we're continually adding to our
feature set but not actually looking at
how the user experiences for being like
you know again going back to the
perceived speed the page sizes like so
is a downloading a Meg when it should
only be downloading say 100 kilobytes
and trying to work out that yes but you
know what we're 180 to see yes it's some
sort of catastrophic yes and this is
this is why i said we we run this quite
regularly in 30 minutes because we get
these anomalous bits of data that
suddenly just appear for no reason and
this could have just been a pea the
application pool resetting itself when
the test started running so so it's got
to recompile everything yes and right
here we can start with one simple one
simple rule no bridge can be loaded more
than 10 seconds yes yeah yeah and we
they follow that man which leads us to
do my first request is there any bugs in
your Park System dedicated to only the
problem see in their eyes there are
performance testing is still a testing
and it's still stuck to the pool testing
process find a bug finally something
that not fit you know your rules you
baseline so it's back go guys go fix it
we have had those am and I this where we
did have people saying right this is
slow fix it so we don't need just
concentrate on that one aspect so say
like the Explorer view it would take
because it's all JavaScript and you try
to do this in internet explorer things
can take quite a while to actually start
rendering and doing the reflow in the
browser and so we start just
concentrating on that but that could
have a potential knock-on effect
somewhere else and because we're not
setting a baseline anywhere when we were
doing our previous performance bugs
because we we have been looking at
things and just you know fixing them as
customers have complained and but it was
always as an afterthought not actually
setting an actual feet feature request
is that we were just doing this hit and
miss type effects of testing and just
going or developments just going yeah I
think it's faster my stopwatch says it's
a little bit faster but my thumb could
have been a lot quicker the Sun than the
last time I then eq pout shrunk or not
so strong requirement see for example no
the any page can be loaded more than 10
seconds so it's very easy to create a
bug if it's something yeah yeah but
that's what if we go back to that where
we that we do have a 10 second
requirement and every time something's
gone over 10 seconds it's gotten
straight away the next build as you can
see in the graph yeah so we do have that
now but this is in our process of trying
to go from you know having all these
issues that we and not being able to
have a baseline to start with because
you need some way to show you where the
issues are and then build from there and
previously monitoring that would have
been very expensive because we'd have
obviously had to manually go to each
page
timing and given the number of builds
and the quick turnaround we need with
iterations it just takes far too much
time to do that so we needed some way of
passing gathering that data
automatically yeses it looks like
regression performance did yeah yeah
Venus so I spur my other questions and
to talk about hello hello soccer
apologies if this was already talked
about it in your pocket do you have
plans to open source your code and if
you do what's your like plan for doing
so the selenium project would love to
have your the changes that you made and
I'm sure than the webdriver to you also
love this yeah we do we do have plans
we're being the selenium team we're all
one big team with all the love your
patches yeah we do we do have plans to
open source it we just need to make sure
that it's we haven't got any potential
company things inside it so we just need
to do that and then we're going to be
open sourcing it a thing to be honest
the the changes to the selenium code was
a two line comment out where it was
blocking their tethers through imagine
the more the awesome rabbits are all
your graphs oh yeah those are those are
actually on the demo site it's it's all
in jaso's right it's also Snicket right
now yeah and change your copyright
notice and then yeah you can get
off the google code so the graphing
utility great thanks so as far as i
could tell this only applies to firefox
oh the browser's not really doesn't
really matter for us and in this case
because we were just trying to set a
baseline because other than timings cuz
reflow and things like there tend to
work a lot better in firefox and but
page size is going to be the same if
it's coming down in IE or if is coming
down safari or firefox so the browser is
not really relevant yeah we'd rather
have a consistent platform where we just
get the same data every time rather than
picking up multiple sources where you're
not exactly sure what's going on densha
Lee says hopefully about consistency and
also the fact that why slow is at five a
firefox plugin and is the easiest
cheapest method to actually achieving
the end result and the beaconing i'm
just wondering does that all happen once
the page is loaded yeah basically once
the page loads fireboat why slow creates
create a JavaScript image objects and it
sets the source to the beacon page with
the results appended to that ok so just
inserted right at the end so then you
know and then that gets logged by the
page that's the target of that that make
sense yeah I was just wondering if it
was running concurrently it seemed like
it could have a performance in peppers
life it measures everything I collates
the results and then and then things it
off it's a pretty quick process and is
that the the your your beaconing
receiver is that likely to be open
sourced as well it's your honesty it's
just a standard C sharp page and that
pull pull stuff out of the am after the
query string and shove it into the
database so there's nothing the men so
clever yeah we'll say welcome date well
munity it seems like it would be useful
for anyone who's yeah I slow at all to
hell yeah the record are actually a
couple of other projects that sprung up
since we started doing the work and
there's a few things to look at this
show slow it's a similar concept but
obviously it uses on unmodified versions
of white light so it doesn't pull down
quite as much data it basically just
pulls down the total wife so score the
score for each of the whites liberals
and the total page size so we obviously
added in the extra extra metrics in got
a share in the report and there's a
extra so that I think it's also cesium
yeah it's a firefox so it's a mini
project that they doing for mozilla labs
and it's when it's a bit of python that
just fires off a
headless Firefox instance collects all
information and then dies so but if you
have any form of states in your
application e you can't really use it
unless you somehow got the cooking it
beforehand and since we do a lot of
marketing data our data is very very
sensitive so like anything that appears
to be like you kept trying to get into
the system we just closed that that log
on and you have to read log onto the
system so the cooking becomes invalid
and things like at so that we can
protect our you our users and who are
using their data and so we can meet some
European data protection laws well thank
you very much with it very nice looking
stuff and these graphs or the
performance test is actually run on Deb
right ah that really gets the test yeah
it's not production data we were honest
that's one of the other things that
we're looking to DC and the company I
work for we had the same problem because
our performance was going down in
production and we didn't know how to
performance test it but our dev wasn't
the same as the production so we didn't
have the same server in the caching yeah
but to give us a pin point we started
same thing the same graphs and that
would give us if a picture was too big
or some JavaScript he was done badly
suffer death but our problem was still
that our production was never in Devon
so we like if you did that performance
hosting it will still be better in
production and so I think we're kind of
saying is that we we don't do any load
testing against the test test farm at
the minute against a system this is
running against so yeah it would be good
to get it also a view of what the
production servers are doing in the same
way because you get these tests
especially the timing test yeah yeah
because in actually it there like two
different systems right yeah because you
don't have the same amount of service
going on dev that you have in production
well yeah from our situation we try and
keep them as similar as possible kind of
as many resources as possible to kind of
keep them kind of the same thank you
very much I've been doing something very
similar just type of recommendation yep
instead of doing some simulation for the
actual loading times I can recommend you
look at jiffy by whitepages com alright
yeah they actually measure the loading
time by real users right and I've
plotted it also against this timeline
it's really nice for instant feedback
after release oh yeah sounds for a
production system in
vision know which one that last one that
was jiffy I think Jay I'd double FY
think of this yeah all right thank you
thank you very much so you're gonna</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>