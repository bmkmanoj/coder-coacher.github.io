<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2015: The Uber Challenge of Cross-Application/Cross-Device Testing | Coder Coacher - Coaching Coders</title><meta content="GTAC 2015: The Uber Challenge of Cross-Application/Cross-Device Testing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2015: The Uber Challenge of Cross-Application/Cross-Device Testing</b></h2><h5 class="post__date">2015-11-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/p6gsssppeT0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Next up, we have two engineers from Uber who
have come to talk to us about Octopus, taking
on the Uber challenge.
I'd like to welcome Apple Chow and Bian Jiang,
from Uber, to the stage.
[ Applause ]
&amp;gt;&amp;gt;Apple Chow: Hi, everyone.
My name is Apple, and I manage the mobile
testing Infrastructure team at Uber.
I'm very excited to be here today.
So first, let me give you some information
about myself.
I am an ex-Googler.
I was at Google for nine years.
And I led the testing team on a wide array
of projects, including YouTube, Hangouts,
and Google Maps.
At first I thought I've seen it all in the
world of automated testing.
When I joined Uber in March, however, I found
that the level of challenges we face at Uber
is at a uniquely different level.
I'm very excited to share with you some of
the technologies we built from scratch to
solve those challenges at Uber.
And we are also planning to open source some
of the solutions very soon.
So I hope that you find something useful in
our talk today which you can apply to your
projects.
So what is it that makes testing at Uber unique?
Before I reveal the answer to my question,
first, let's take a quick survey.
How many of you have used Uber?
Wow.
That's a lot of you.
That's great.
And my next question.
How many of you know that there's a separate
driver app?
I see.
Not that many of you.
Okay.
Now to my third question.
How many of you have ever tried out to be
an Uber driver?
Well, there's lots of perks about being an
Uber driver, but that's not the point of today's
talk.
So after my previous hint, now can you guess
the answer to my question?
What makes testing Uber's mobile apps significantly
different from, let's say, testing Google
Maps?
Anyone want to tell me the answer?
Any guess?
&amp;gt;&amp;gt;&amp;gt; (Speaker off microphone.)
&amp;gt;&amp;gt;Apple Chow: Two apps.
Good, that's great.
So what's our Uber challenge?
The driver app and the rider app are tied
together.
So this is one of the first challenges we
faced when we were investigating UI testing
tools for automating the sanity test scenarios
for the mobile org at Uber.
We found that many of the scenarios required
the rider app and the driver app interacting
closely with one another in order to complete
the scenario.
So, how do we solve this?
And now let's first take a moment and think
about our current and previous projects.
Have you ever encountered a case where multiple
users have to collaborate together in order
to finish a scenario?
And have you ever faced a scenario where you
have users -- your app has to talk with another
app in order to finish a scenario?
I can immediately think of a few cases where
this applies.
For example, at Hangouts, we have one-to-one
or group conversations or video calls where
multiple users have to collaborate together
to finish those scenarios.
Also, my friends at Games.
There are so many games that require multiple
users participating together to finish a game.
So, basically, anything that involves multiple
users talking with one another in order to
finish a scenario will face a similar challenge.
So what's unique to Uber is that our core
flow is consisted of many distinct steps that
are happening not just across different devices
but across apps, the driver app and the rider
app.
So that makes it even more challenging.
I think the more common case is that you have
multiple devices interacting with one another
but still with the same app.
So how do we solve it?
Our solution is called Octopus.
So even though our challenge is unique, our
solution is a generic test runner that can
be applied to run any scenarios that are involving
cross-device or cross-app communication.
So ready to see Octopus in action?
First let's watch a quick demo showing the
interactions between the driver and the rider
for our core chip flow scenario where the
Uber driver is giving a rider a ride.
Then we will look beneath the surface and
see what octopus is doing.
So in this demo, you see Octopus launching
to Android emulators side by side, one running
the driver app and one running the driver
app.
So let's play the video.
So here you can see the rider and driver both
sign in.
The left side is the rider.
The right side is the driver.
So, first, the rider sets the pickup location.
And then the driver goes online and is ready
to get requests.
And now the rider on the left clicks on &quot;request
UberX&quot; to request for UberX.
Driver gets a dispatch, accepts it, and is
en route to pick up rider.
Driver then picks up rider, begins the trip.
The rider on the left sees that he's now on
trip.
And driver arrives at destination, drops off
rider, rates the rider, and then rider will
rate the driver.
So this is the sequence of our most basic
trip-flow scenario where the driver gives
the ride to the rider.
Now, think of a more complicated scenario
like the Uber Pool.
I'm not sure how many of you have used Uber
Pool.
In an Uber Pool scenario, you have one driver
and usually at least, like, two to three different
parties of riders.
So over there, you have three to four devices
collaborating together in a scenario.
So the synchronization gets even more complicated.
Now, let's look at the scenario step by step.
Remember in the previous demo I showed that
Octopus launched both the driver app and the
rider app at the same time, and each of the
driver tests and rider tests goes in and executes
their own sequence of steps.
Now let's imagine what could go wrong there.
Remember that those two tests are running
in two separate processes, even though it
looks like it's all together in one scenario.
So underneath the cover it's running in two
different processes.
What if the rider logs in before the driver
and requests for the ride before the driver
is ready to accept the ride?
Then the request could time out and then you
wouldn't be testing the scenario that you
want to test, right?
So how does the rider know that driver's online
and now I can send the request?
So this is where Octopus comes in.
So Octopus, we introduced this concept of
signaling.
So using signaling, Octopus can -- the driver
app can tell the rider that, hey, I'm online
now.
Now we can request.
Then the rider test can go ahead and request.
So using signaling, Octopus allows you to
set up different checkpoints along your test
scenario to make sure the steps are executed
in the right order so that you can still to
communicate with one another.
So would Octopus help?
So here's the correct sequence.
So after they log in, first the driver goes
online by clicking on &quot;go online.&quot;
And then the request goes to the back-end,
and the back-end acknowledges it.
Now, Octopus sends a signal from the driver
test to the rider test and say, Hey, I'm online
now.
Now we can request.
The rider test now clicks on &quot;Request UberX.&quot;
And the request goes to the back-end, and
then the dispatch gets sent to the driver.
So in a similar fashion, you can see that
along the way you can put in different checkpoints.
For example, after that maybe the driver can
tell the rider, Hey, I accepted the ride.
Then the rider test can check the status bar
on the top and say, Okay, I can see the driver
is en route to pick me up, things like that.
Let's talk more about Octopus.
So what were all the reasons behind why we
built Octopus?
First, we wanted a unified test runner for
both our Android and iOS apps.
Secondly, we wanted extensibility so that
our runner can be integrated with different
UI testing frameworks.
Thirdly, we wanted to support parallelized
runs so that we can speed up our tests.
Lastly, as I showed in the previous demo,
we want to support signaling so our tests
can communicate with one another even though
they are running across device and across
apps.
And most of our talk is going to focus on
a signaling functionality since that's what
enabled us to do the cross-app and cross-device
communication.
And what does Octopus do?
Well, Octopus streamlines the following functionalities
in the comment line.
So whether it's run by an end user on a laptop
or it's running by our Jenkins CI, it runs
the same exact command.
So this makes integration with Jenkins super
easy.
So what does it do?
First, it prepares the test targets.
So test targets, I mean either, it can be
a device or it can be an Android emulator
or iOS simulator.
So it installs apps on them, puts all the
test configuration test data on the device
and launches the app, for example.
And then it runs the test and then handles
the underlying communication across different
tests using signaling.
And then at the end of the test, it creates
a test report and also pulls all the necessary
test artifacts like bug reports, screen shots
from a device or emulators and performs any
necessary cleanups.
Now, let's see another demo of Octopus in
action.
In this demo, you see Octopus launching multiple
iOS simulators at once and runs a test on
them, waits for all of them to finish before
it finally exits.
Let's play.
So here you can see Octopus launches five
iOS simulators.
Each represents a different iOS version.
So it runs the log-in and log-out for all
of the simulators.
And one by one they log out and they exit.
And Octopus will wait for all of them to finish
before it finally exits.
So using parallelized test runs, we can use
it to shard our test across multiple test
targets to speed up our tests.
Another usage is similar to the Web testing
world, you can run the same tests across different
iOS versions to make sure the test runs for
all of them.
So now I'm going to hand it over to Bian who
is going to deep dive into the design of Octopus.
&amp;gt;&amp;gt;Bian Jiang: Thank you, Apple.
My name is Bian.
I'm for the mobile testing team at Uber.
Before joining Uber, I was working for Facebook
on the performance test team framework for
mobile applications.
Today let's talk about some technical details.
Before that, let me share with you some of
the design philosophies.
So, first of all, we want to make sure Octopus
is easy to use.
So all the demos you see today in the slides
is triggered by a single Octopus command line,
which is unified and simplified across different
platforms.
And we want to make sure Octopus works the
same way with different platforms like Android,
iOS, simulator, emulator, and real devices.
And we chose to integrate with the existing
testing frameworks such as UiAnimator, Espresso,
or iOS reautomation so that the developers
can use the platform-specific functionalities
directly without introducing another layer.
Let's talk about signaling.
Before I get into the technical details, let
me share with you some of the technologies.
The test host is where Octopus runs.
It could be a Mac Mini or a laptop which you
work on every day.
And the actual test code is running on test
targets which could be simulators, emulators,
and real devices.
And between test targets, we have communication
channels.
And in the communication channels, we pass
around signals.
We want to make sure that Octopus is simple
to understand and easy to implement.
So we just use very simple streams for signals.
And it is the test targets and test code's
responsibility to interpret the meaning of
the signals, for example, driver online, as
Apple mentioned earlier.
So how does it work?
Using the same example in the end-to-end trip
Apple just mentioned, we have the driver test
target and the rider test target.
So first the driver sends a driver online
to the rider test target.
And the rider test target requests for a trip
and sends back another signal telling driver
that I have already requested a trip and you
can go -- you can proceed for the verification
UI or test steps.
On top of the high-level workflow, we introduced
two very simple APIs: The read signal which
will block until signal is retrieved or is
timed out.
It is a read signal from a channel in the
stream or timeout.
Correspondingly, we have write signal which
writes another stream to a channel, which
is a non-blocking call.
The API works as below.
Using the same example, we have driver and
the rider.
So, first, the rider calls read signal on
the channel called rider inbox.
By looking at the name, you know the rider
inbox is a channel which contains all the
incoming messages for the rider.
And then the driver will call the write signal
on the same channel, rider inbox, with a signal
called driver online.
And this function call will trigger the return
of the read signal on the rider side.
After that, the driver waits for another signal
by calling a resignal on another channel,
which is driver inbox, which is all the incoming
messages for the rider -- for the driver.
And this read signal function call will be
triggered by another function called write
signal from the rider side.
So this is how the API works at a very high
level.
So how do we implement it?
At a very high level, we have the two-way
communication channel between test targets.
But, internally, we have two one-way communication
channels, like the rider inbox and the driver
inbox.
So it's called P2P communication between two
test targets or multiple test targets.
It is really easy to think of some P2P technologies
like Bluetooth, direct Internet connection,
NFC, or Airdrop.
So we have plenty of choices, but we do have
a problem which is the consistency because
one technology might work perfectly on one
platform but not on another.
So if we choose different implementations
or different technologies on different platforms,
we will end up with multiple implementations
for different platforms, which will increase
the complexity of our system.
So we switch to a more consistent way which
is the relay test host because in the test
scenario, the communication channel between
test host and test target is always consistent
and reliable.
So in the beginning, we chose a client server
architecture, which is very easy to think
about.
So the test target and test host communicates
with some network protocol.
This is easy, and this is typical.
But there's a problem in this architecture,
too, which is the reachability.
Because your test host, your Mac mini or laptop
is usually running in your protected network,
like your data center or your corp net.
But your test target might be running in the
cellular network or public WiFi or even no
network if you want to test it in offline
mode.
So it's no guarantee that a test host is always
reachable by the test target.
So that means we need to find something more
reliable and always available because we're
running a test scenario.
So we chose a more reliable connection, which
is a USB.
So imagine you are doing your device testing.
You always connect your device using a USB
cable to your test host.
It works the same way with the simulator and
the emulator because the testing framework
provides the consistent communication channel
between test host and test target.
So by using this, we implement a virtual two-way
communication between test targets.
So now we have the communication protocol
or we have the communication channel.
What do we pass around in the communication
channels?
We chose the most fundamental and most reliable
storage unit in the operating system, which
is a file.
Here's how it works.
So, first, the driver generates the file containing
a string called driver online on the test
target.
And then this file is passed from the test
target to the test host.
And the test host relays the simulator -- the
signal stream to a test target.
So, logically, we have the same implementation
of write signal.
Now the question becomes: How do we pass around
files?
So specifically we have two questions to answer.
The first one is: How do we send a file over
the stream from test target to test host?
Which is essentially the implementation of
the write signal.
And correspondingly, we have another question?
How do we watch while monitoring a file on
the test host from the test target because
your test code is running actually on the
test target, which is essentially the implementation
of the read signal.
This is the way Octopus diverges on different
platforms so let's talk about them one by
one.
On iOS , the iOS reautomation framework provides
a very convenient API called UIAhost.performtaskwithpathargumentstimeout
which is the equivalent of exec.
So by using exec, you can essentially run
any share commands on the test host, which
is really convenient.
So with that, the write signal and read signal
become very simple to implement.
So for the write signal, we just write something
to some file on the test host.
And for read signal, we just use cat to check
the content of the file.
So the actual implementation of read signal
on iOS is a little bit more complicated.
We have a shell loop which is constantly checking
the file content for the signal and then triggered
the return of read signal once the file content
has changed.
On the other side, Android -- unfortunately,
there's no direct exec.
Everything should be initiated from test host.
So we have to use ADB shell as a relayer.
So here is how it works.
For the write signal, the test host starts
a daemon process which monitors a signal file
on the test target -- on one of the test targets
using ADB shell.
And then the driver generates the file or
changes the file content on the test target
which will be detected by the test host, by
the daemon running on test host.
And then the test host grabs the file from
the driver test target and relays it to the
other test target for the rider.
So, logically, we implemented write signal
from the driver to rider.
Because we are pushing a file directly to
the rider test target, the read signal becomes
really simple.
We just use the file observer on Android to
monitor the content of the file.
Once it's changed, we trigger the return of
the read signal.
So as you can see, the design of the signaling
Octopus is really flexible and scalable so
that it is very easy to implement one-to-one
signaling, one-to-many signaling, as well
as the many-to-many signaling.
And the majority of the implementation of
signaling is platform agnostic.
The only thing that diverges is the actual
file observation on different platforms.
So that it's easy to implement the cross-platform
signaling.
Imagine you have an iOS rider talking to an
Android driver.
So given that, I would like to show you another
demo which shows the most complicated test
scenario in an Uber application, which is
Uber Pool.
As Apple mentioned, if you have multiple riders
coming to the same location, they can share
the same car with the same driver which requires
one driver and multiple riders.
So as you can see, we have three simulators,
a driver and two riders.
All of them sign in.
The driver goes online.
After that, it sends the driver online signal
to the first rider.
Then the first rider pick up a location and
selected a destination request for the shared
ride, which is Uber Pool, which will be captured
by the driver.
And the driver will pick up the first rider
and then send another signal to a second rider
saying, I have already picked up the first
rider.
The second rider can start to request the
Uber trip now.
The second rider got the signal, picked up
the locations, and request for a shared ride.
And as you can see on the driver side, the
second pickup has different UI so we will
have different verifications of the test steps.
And after the two riders are picked up, the
driver will start the trip.
And then drop off the first rider.
And then the second rider.
And after that, the driver will reach the
riders and the whole test scenario.
So as I mentioned, the host scenario is triggered
by only one Octopus command line.
It handles all the device bootstrapping, initialization,
and the coordination, simulation between test
targets.
And the synchronization is handled by Octopus
signaling.
So that's what I have.
Questions.
&amp;gt;&amp;gt;Yvette Nameth: So the first two questions
are pretty much duplicates, which is, why
is there a need to test both the apps at the
same time, why not simulate requests and responses
and have fake back-ends?
&amp;gt;&amp;gt;Bian Jiang: So we do have different test
modes.
So this one is the full end-to-end test mode.
The reason why we need this is sometimes when
you test the different configurations on the
server side.
For example, we have a test city with certain
functionalities enabled, and we have other
city with different functionalities.
So although we have some dynamics on the server
side, we want to make sure we have a test
scenario to cover different scenarios, both
on the client side and the server side.
So that's why we need end-to-end, which simulates
both the rider and the driver side.
&amp;gt;&amp;gt;Apple Chow: I mean, this is not, like, replacement
for, like, unit tests or your medium-level
component tests, where you can mock out server
responses.
This is meant for a small fraction of your
-- you know, like, your core happy path tests
that give you the final confidence that your
system -- you know, the high-level components
in your system is connected correctly.
So it's only a small fraction of it that give
you final confidence.
So we are using all different modes in our
tests.
&amp;gt;&amp;gt;Yvette Nameth: It seems like it would be
hard to identify root causes of test failure
when there are two separate units under test
and there are associated tests in different
process spaces.
Has this proven to be a challenge in practice?
&amp;gt;&amp;gt;Apple Chow: Actually, for -- you know, using
this actually makes our tests easier to debug,
because each test is only waiting for one
thing at a time.
So when something fails, you -- there's no
doubt by, like, oh, did the rider -- for example,
did the rider -- did the driver go online
before I request?
Like, if something fails, you know that, okay,
driver already sent the signal.
That's online.
That means it's something on the rider side.
So we're -- if you don't have this, you would
be, like, polling for different conditions.
And sometimes the conditions are not even
visible on the UI and it's really hard.
So I think having this definitely makes the
test actually easier to write and easier to
debug.
&amp;gt;&amp;gt;Yvette Nameth: Do you experience flakiness
when sending signals between emulators?
&amp;gt;&amp;gt;Bian Jiang: So because -- because the test
codes are running on different simulators,
so (indiscernible) is challenging.
So that's why we chose the -- as reliable
an approach as possible, like the file-based
signaling, the USB connection, to reduce the
flakiness.
So, so far, we didn't see many flakiness.
It's pretty stable, because we used the most
reliable technology.
&amp;gt;&amp;gt;Apple Chow: So far, this is actually more
reliable than, like, Appium that we look into,
for example, with client/server architecture,
you know.
So far, the flakiness we found is not from
the signaling.
It's more from, like, the real-time server
responses.
Occasionally, it times out or something happens
there.
But it's not about signaling.
&amp;gt;&amp;gt;Yvette Nameth: Okay.
Could you clarify what the system under test
actually is.
It sounds like you -- my question left.
[ Laughter ]
I'm sorry.
Do you have it, Diego, that I can read?
&amp;gt;&amp;gt;&amp;gt; Which question?
&amp;gt;&amp;gt;Yvette Nameth: The --
Is it back there?
Oh, yeah.
There it is.
Thanks, guys, for uploading.
It sounds like you compromise performing end-to-end
testing with your test infrastructure in between
the apps, which is not the case in production.
Could you elaborate on that.
So what are you actually trying to test with
this type of test, is basically the question.
And what -- you know, what were the compromises
made?
Or what other things did you have to put into
place to mitigate any --
&amp;gt;&amp;gt;Apple Chow: I think, on top of having unit
tests and then the tests that, like, if you
mock out the driver and look at the rider
specifically, this allows you to test, like,
the core flow, the more complicated interactions
between the two to make sure when you go through,
like, the core chip load, which is, like,
one of our core cases, right, to make sure,
like, each app can go through the expected
sequence of state changes and then is able
to complete the trip, and, you know, complete
the happy path.
And then at the end, they can reach each other.
&amp;gt;&amp;gt;Bian Jiang: So Octopus is not making any
modifications on the application itself.
It's not -- it's not modifying the back-end
as well.
So we're testing whatever we have.
Either it's production or it's test stage
environment or it's the beta application.
&amp;gt;&amp;gt;Yvette Nameth: Well, thank you, Apple and
Bian.
[ Applause ]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>