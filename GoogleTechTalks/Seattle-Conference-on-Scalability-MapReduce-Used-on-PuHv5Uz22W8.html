<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Seattle Conference on Scalability: MapReduce Used on... | Coder Coacher - Coaching Coders</title><meta content="Seattle Conference on Scalability: MapReduce Used on... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Seattle Conference on Scalability: MapReduce Used on...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PuHv5Uz22W8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right it's my pleasure to introduce
Barry brummett a googler from our
Kirkland engineering office and he's
going to be talking about using
MapReduce on large geographic data sets
play along spot on put one hand up in
the air if you're from seattle put your
hand down if you're from washington
state put your hand down if you're from
the Pacific Northwest put your hand down
from west of the Rockies east of the
Pacific put your hand down if you're
from the US put your hand down anybody
left where you from thanks for coming um
I was just curious and another question
who here writes code in C++ ever Python
Perl okay pretty CSV crown me and one
more quiz I've worry is this so what is
this no it's this is MapReduce and and
what this it's in parallel anyway
it just kind of happened in the last
session so bear with me anyway I'm one
of the people who has been using
MapReduce for all sorts of things that
they had no expectation of using
MapReduce for and I'm going to talk
about that today because it's been a
tool that's made possible for me to do a
lot of really cool things working on the
novice project at Google well I've got
this map up on the screen I thought I'd
show this off which is where the new
office is in Seattle which is on the
other side of that bridge that makes me
very happy one thing we do one thing we
do a lot of at Google is processing
really large data sets and by large you
know i'm talking like hundreds of
gigabytes terabytes of data or more and
if you're trying to actually serve this
data up to people and they sort of
reasonable way you can't be going
through the raw unindexed just kind of
bear form and trying to figure out the
answer their queries it's impossibly
slow so a really common task across many
many Google properties is taking that
data and transforming it into index
files so I recognize that thing in the
middle that's helped card catalog that's
like records and once we have index
files then we load those up on the
servers usually in RAM so that we can
actually answer queries from users it's
really not that useful to have things
that are even you slightly frequently on
disk because you're limited by the
bandwidth to the disk and the number of
seats you can do so you can do maybe a
hundred a thousand queries a second on a
disk but if your google and you're
answering the kind of queries who are
that just that just doesn't do it so a
lot of work actually has to go in to
making these index files be something
sensible and that produce is a tool that
makes that work really well Jeff talked
a lot about MapReduce earlier some ways
I'm sort of bummed about that in other
ways I think of it like teaching math
and that you have to repeat the same
things a couple of times before people
get it so just think of this as like a
refresher or perhaps more drill down
into some more specifics so you choose
your cast as you like it I'm going to
think about map data map date is big
it's not big like all the web pages in
the world big but it's still pretty big
the North America has on the order of a
hundred million roads in it so you can
just about get that member if you really
have to but you don't want to not
certainly with all the geometry and all
the other data that goes along with it
so when working on maps data we've got
this big huge massive set of files that
basically describes everything kind of
roads parks counties streets cities
country boundaries intersections you
know and so on and they aren't really
organized like a database because you
need a really large database it's kind
of a pain to have those formal tables
and you know we just don't really
operate that way it's sort of two
different ways of looking at how you
organize data you can either make it
super super structured and build really
big tools that work on this structured
set like the sequel server or you can
operate like an eunuch land where you
have flat files with text in it and
you're right little tools that are you
know don't processor very complex file
type but can do something useful with it
so if you think about this map data you
can imagine if you wanted to build a
graph you know nodes and arcs that
represent the road network you've got to
somehow go through this large set of map
data and gather the right bits of data
together they're not sorted
geographically necessarily and they're
not like attached to each other because
otherwise every intersection or every
road to have to be attached to every
intersection that it is involved in that
would be a huge blow-up of data so
you've got just this road and you got
justice intersection and somehow you'd
like to take this intersection object
and attached to it all the roads so you
can then have this graph so you can do
things like take computer angles or you
know figure out how much traffic is
there or do whatever you need to do in
this network so I'm going to propose an
algorithm to do that and of course yeah
it's going to be MapReduce but i'm going
to go at it sort of backwards so you
could go through this feature list and
look at each feature and every time you
see a road you omit the road you as a
key value pair where the key is the ID
of all the intersections of the
intersections that it's a part of and
the value is a copy of all the road
geometry and whenever you hit
intersection you can emit a copy of the
intersection key by its ID then if you
were to sort that list you'd have
grouped together by the key
all the things at each intersection all
the roads and the intersection itself
and then say if you got all that data
into one function you could emit an
object that corresponded to a node in
the graph and your problem will be
solved you have gone from this list
thing to this more compact just graph
representation still a list but
nevertheless it actually sort encodes
the graph more directly so this is
surprise MapReduce so if you look at it
in this sense the the map function is
the thing runs down that list and emits
key value pairs and so if you look at
the the road numbered five across the
middle for instance its emitted twice
both intersection three and intersection
six so that two copies of it come out
and each of the intersections is emitted
once and all their the roads that are
emitted once as well and then this fast
shuffle phase happen which sorts these
key value pairs and so you get all the
intersections label three in one place
all the intersections labeled six
another and then reduce is called on
each of those sets of things for the
same key and that emits the object the
ends to end up with a sorted list by ID
of all the nodes in the graph it's
pretty eight it's a little more
interesting than counting to be or not
to be I think um sorry I just think I
just slammed my boss um but the point is
that this is this is a fairly listy kind
of operation it kind of looks like
something you could do with a database
pretty simply there's probably a select
statement you can write that would kind
of do the right joins and stuff so it's
in some ways it's not that excited you
can make the very fair argument though
why don't we just build a big database
and kind of go at it that way let me
there's all the fault tolerance and the
scaling and stuff but you could imagine
doing that the thing that's really neat
about MapReduce is it is being used for
so many things that the folks who would
originally wrote it didn't expect it to
be just give you an idea of how
straightforward it is to use MapReduce
here's a little code example of
implementing the thing that I just
showed you basically your map or class
has it takes an input it's a feature it
turns it into the right type it looks at
what it is does a couple of minutes and
it's done it's like eight lines that go
the reducer gets a list of all those
objects of
the first one's going to be the
intersection and then it loops through
all the other ones attaches them all
onto the onto the object and then emits
it it really couldn't be much simpler
and this is why all those interns can
check in all this code that runs on huge
sets of data on thousands of machines
because that's honestly all the code it
takes me there's some type definitions
he's a little chunk of thing to say how
many machines you want what data center
you want to run in it's a little bit of
over heading stuff but honestly 30 40
lines of code you're running on
thousands of machines at once that's
cool and you can do more than just as
like list processing kind of stuff i
mean this processing great no but you
can actually do distributed rendering
using MapReduce you've all use google
maps and we use google maps to get here
today okay um the way these map tiles
can be rendered is if you know how
you're going to break up the world ahead
of time you can know sort of which
bucket every feature needs to fall into
by say its lat long coordinates or
whatever you might need to do some
overlap to make things render nice or
whatever but first you know for
simplicity sake you can imagine that you
can figure out what bucket for any
feature where it needs to land and so
you can look in the map phase at each
feature look at its geometry look at the
set of buckets you're going to you're
trying to render and emit a copy of the
feature keyed by the ID of every bucket
that you want to render in so for
instance I five get submitted to both a
North and South bucket imagine we're
just rendering two buckets for the
Seattle area 520 gets rendered into the
only the North bucket and i-90 to the
South bucket and I said rendered I said
should have said admitted there and then
the shuffle happens hope the rescue
group together ourselves get grouped
together and then in the reduced face
all the rendering can happen we just
distributed rendering across thousands
of machines we're processing list we
actually are generating now this huge
short and set of graphics without
actually having to worry about any of
the communication any of the fault
tolerance any of the scaling it just all
kind of happens for free under the
covers and that's that's the thing about
MapReduce is it seems to be applicable
to a large large large set of problems
so actually I stole these from Jeff so
just keep college you can actually
process different data types with
MapReduce as well and we've talked most
about lists of data but and it obviously
is going to be some listing asst to it
because you know it reads things
sequentially through a set of stuff but
you can actually operate say on a graph
as well so let's imagine that you had
the problem that you wanted to mark all
the nodes in the US with the nearest go
with a gas station that's within five
miles okay so you could start by knowing
where all the gas stations are like the
red and the yellow one if it's up there
right now and then in the map phase you
could do a Dykstra search out from the
each gas station sort of one Dykstra
search for each gas station and it could
go out to you know say a 5-mile radius
and emit for every node it touches see
if I get this right the idea of the gas
station and the distance to the gas
station keyed by the node ID and so you
can repeat this for all the gas stations
in the world and then it will be sorted
in each node would collect information
about all the gas stations that were in
five miles and then the reduce function
simply looks through the list of gas
stations that it's found finds the
closest one it takes the minute of all
those distances and emits that to
produce the output and so you've just
marked a graph with all the closest gas
stations in the world all the closest
gas stations I think that's pretty neat
too I mean there has to be some
cleverness here because the graphs have
to live on disk somewhere and you have
to be able to read it so you can imagine
a big table like object that lets a node
that's process map reduce its processing
one node read in all the nodes that are
nearby in some sort of efficient cash
way and you can imagine that if you were
clever about it you process lots of
local nodes on the same reducer on the
same mavar so that it wouldn't have to
go disc so often so there's lots of
tricks you can do to make MapReduce more
efficient but when you're trying to
write really complex map reduces that
becomes the game is how do you handle
the i/o if you're trying to not work
with just a single object you're trying
to work with a whole bunch of different
stuff
one thing I didn't say earlier which is
worth repeating now is the basic reason
that fault tolerance works in MapReduce
is because every single map operation is
completely independent from every other
one it just simply doesn't matter what
order they're done in if one fails it
can be just done again and the master
much like steady at home can say oh look
the map 12 hasn't been finished yet so
I'll do another one of those and then it
comes back and it all works out and the
shuffle phase is dependent like it has
to wait for all the mappers to finish
before it can truly close the shuffle
and then the reduced phases are likewise
completely independent if any one fails
you can just start over and do it again
and you know that's what gives it this
this killer fault tolerance which is be
kind of smart about it you want to make
sure you break your tasks up carefully
so you don't end up with say 32 machines
running your 32 reduced tasks then each
one takes an hour say because then if
one of your reduced tasks fails at st.
minute 59 you have to wait another hour
for that to get finished so you have to
do some smart engineering to think about
how to break your problem up and into
the right sized chunks so that you don't
end up with this problem of trying to do
it accu-chek of processing at once and
sometimes that's easy and sometimes it's
hard Jeff covered a lot of these are a
couple things he didn't catch though I
think you mentioned data compression so
when you're trying to transmit data
between mappers and reducers sometimes
it's faster to compress it and compress
it before transmitting across the wire
no big surprise but it's a good idea and
i haven't mentioned gfs at all because
really when you use MapReduce you almost
don't have to worry about gfs it just
kind of takes care of all these problems
for you so it's it's reasonably robust
and requires not too much work on the
part of the programmer to get good at it
another event another cool feature of
MapReduce is that you can run it locally
as like a single thread so if you've got
some complex reducers say that map tile
renderer that's crashing when it renders
something you don't have to go connect
to this cluster that's you know across
the Atlantic trying to figure what the
heck went wrong you can just run a small
chunk of data on your local machine that
looks just like the running process in
the cluster and you can actually easily
get in to debug it and and figure stuff
out and that's pretty cool too there's
also so ability to get some insight into
what's going on let's say you're trying
to count the number of different kinds
of errors I'll take questions at the end
best um seems trying to count different
kinds of errors or different kind of
conditions you actually have distribute
counters so like each reducer can say
whenever it has a problem rendering
something or it can't follow a link or
whatever can increment a counter at the
end you can get an aggregate sum of
those things because it's really hard to
kind of know what's going on across a
thousand machines sometimes and so by
putting in the right kind of counters
you can usually get some sense of what's
up and these slides are totally stolen
from Owen O'Malley who works for Yahoo
invitation as suppose is the sincerest
form of flattery and thank you if you're
thinking gosh MapReduce is really neat
can I try this at home well unless you
have a hundred machines at home it
probably isn't so exciting but amazon
has a big cluster you can pay to use and
you know maybe you do have hundred
machines at home and there is Hadoop
which is an open-source implementation
of gfs and MapReduce and I believe
they're also working on a big table
implementation as well so a lot of the
tools you heard Jeff talk about are
actually things you can start to play
with now if you just search for this
you'll find it there seeing the same
kind of insights if you're trying to
look at a really large data set and
you're trying to process l1 note it
takes too long and if you're trying to
process it on a thousand nodes you've
got a reliability problem and so
MapReduce all of those things um again
they're basically duplicating bits of
the Google system to do the same kind of
work um so this about how much it would
cost if you want to play around with it
on Amazon it actually seems to be a
pretty good system I've talked to the
I've talked to Doug cutting who's one of
the guys who work too I think he's in
charge of it but he seems to be pretty
important the development of it and it's
actually scaled up to you know a
thousand nodes and they've done sorts of
five terabytes of data in a few hours so
it's real reasonably mature so if this
is something you want to play around
with and try running big distributor
processes on it's actually available for
you to do and that's good too so you
know to summarize not produce Oh like
why this is such a great thing it
basically simplifies machine allocation
and coordination the programmer just
really doesn't have to worry about it
there are few permission things you set
up and it just goes it handles all the
cross machine communication you're not
setting up our pcs you're not trying to
figure out sock it's nothing you just
you know you
key value pair is largely it handles a
fault-tolerant something goes down it
comes back dandy you go from one
gigabyte to 100 gigabytes to a terabyte
as long as you can get permission from
the guys running the cluster to use that
many machines hey you're good to go and
the productivity thing is the the intern
thing hf mentioned like it just makes it
really straightforward to process large
data sets um I'm going to take like a
really hard left turn and then maybe
you're right and we had up here um I've
been at Google about 18 months and
Google is a really interesting place to
work and a lot of the philosophy that
goes into why we have my produce sort of
comes out in the way we do things most
devs work out of the same source tree
there really is like one source tree at
Google and I had this disclaimer at the
bottom the Jeff saw this slide and he
said no maybe I'm right so we'll get rid
of that and actually these are maybe
valid observations but all the devs work
out of a single source tree it strike
anyone is surprising I don't know it
strikes me a surprising and mostly it
works like mostly the build isn't broken
mostly I can get the stuff i want you i
say mostly because nobody's perfect but
it's not fundamentally inhibiting how i
get things done in principle a dev can
fix bugs anywhere in the sorcery so if
there's something in gmail that's really
bugging me in there is um you can
actually just get gmail configure it and
make it it's like three commands I'm not
kidding it really builds that easily the
guys who make this stuff work are really
impressive and it makes it possible for
google to have I think 50 engineering
offices around the world because all the
code leads to one repository it's all
written using a consistent style guide
line there's like this 40 50 page style
guide like they have to read to write in
C++ and get someone to agree that we
understand it before we can start
checking things in okay so in some ways
that sucks because you don't want to
have to write in somebody else's style
you know I have the best coding style I
know this but I'm willing to put that
down because when you get a thousand
engineers all writing with consistent
style it means that I can go into that
code base and be like oh I understand
that i can track through that he's not
using exceptions in a funny way
alright macros I don't understand it's
like you just it's like speaking the
same language and that's huge it really
really increases productivity code
reviews are also mandatory for all
check-ins like well on Friday I we had a
something we need to fix and my office
mate wanted me to okay this thing
because we want to get it in for this
push that had to happen i'm like i can't
i haven't read it and he's like well
it's not even my change listed his and
we're like okay we can check it if we
just don't do it because it's not worth
the cost of breaking the build for other
people like you really try to work very
hard to keep everything working all the
time well so really serious about unit
tests so unit tests that run every night
if they fail we get email which is why
it's good I have good food there because
sometimes you're stuck fixing things
when you break the build but it works in
the sense that there are thousands tens
of thousands I can't even guess at the
number of unit tests running every night
to make sure that the software that
people depend on across the company
around the world actually goes i guess
the definition of night becomes kind of
funny I think we're now more people
outside of mounting the amount of you I
don't know what the whole world stories
but there's a lot of people and these
tools are shared company-wide like
there's a small set of tools MapReduce
BigTable gfs and so on and everybody
uses them and you're encouraged to use
them if you don't use them it's like why
aren't you using this what's wrong with
it if you don't like it fix it if you
need something new build it and
convinces why it's better and this works
really well so we get these tools like
mapreduce that are kept running by a
team that really cares about making it
happen also interesting is that google
project cycles are really short like 6-9
months is sort of a normal project cycle
I mean you're shipping more often than
that because you're just pushing things
out um but like a major rev is maybe six
or nine months not say four or five
years like some places um
what oh and does change projects often
like you're not expected to live on one
project for your whole career Google
like you know if you're on a project for
more than two years something's kind of
gone funny like you should be doing
something else now you should be getting
different experience your kind of
encouraged to move around so I think
Google hires generalists rather than
specialists which makes sense because
they've got to understand this entire
source tree and be able to deal with it
and dad's really own the stuff they ship
in the sense that when we ship the
product I was working on related to maps
whatever we had to carry pagers until
the folks who run the clusters or like
okay we trust you your codes good enough
then the pages went down and they took
it over so your feets kind of help the
fire to make sure that it actually all
works um you heard about the peer driven
review process and flat management so I
don't need to go into those therapist
there's a whole culture in this that's
really interesting that plays into how
these big tools work and how Google gets
work done and I think it's a part of how
Google scales or will scale or is
scaling i'm not sure what the right verb
is but we're growing a lot and we still
seem to be successful and we still seem
to be building software even though
we're adding engineers at the point
where i'm 18 years i'm 18 months in and
i think over half the people are older
there are newer than me so questions at
this point
stages
so do you mean if you have to run one
rap produced in another MapReduce um
typically yes yeah almost always you
what you wouldn't I don't think it would
have to you could you could use big
table as well sorry I should repeat the
question I know better what do you use
what does Google use for storage if
you're running multiple map reduces the
answer is mostly gfs you could be using
big table I think it's possible to write
to bigtable string a MapReduce and read
from them but they're on gfs anyways if
you're running it on your own machine
like if you if you're not out in the
cluster then it just reads and writes
from the Linux file system just fine
yeah pretty much it exclusive what's the
word you said mm-hmm gfs can handle it
no um it's either can be a lot of data
there and I'm sure people have looked at
what could you do about chaining map
produces together to be more efficient
with the intermediate storage and to
reduce the bandwidth between them but
it's not something that I know is
actually being done yet but yeah there's
some work there to do a lot of the the
work in MapReduce is the moving data
around and it's interesting to wonder
whether most route produces your CPU
bound under bandwidth bound and I
suspect the bandwidth bound and if
you're running lots and lots of that
produces well you probably to be a lot
smarter about using that bandwidth and
right now I don't think anything's being
done to do that but that's a good
project idea yeah
as far as I know we haven't contributed
to it directly other than presumably
through sort of intellectual background
I've not looked at the internals of the
code so i can't speak to that but i have
looked at examples of hadoop mapreduce
ha's and sort of the way they structure
things and they look pretty much the
same i mean if you squint even their
output screens look the same they're not
exactly the same they do a slightly
different way of defining what a
collection of files is we use a like
file name at 20 they use a directory
name and it's all the files in the
directory six one-half dozen the other
so they're close but not exactly the
same
okay the question was what is the
hardest problem that we use MapReduce to
solve then he has to think to himself
can I answer that question probably I
can't answer that question but if you it
has to do with dealing with large graphs
and doing processing and large graphs
that's that's a really tricky problem
because it involves a tricky data
structure involves potentially very
large computations that use lots and
large percentage of the data as opposed
to something which you know just looks
at a single item or just a couple items
so I think that's some of the more
complex stuff I'm not sure but I think
some of the stuff by workers probably
some of the more complex map producers
that are done anywhere because we're
working on such sort of crazy data
structures it'd be interesting to try to
go around the company and figure out you
know who's the badass MapReduce
programmer I don't know yeah
okay if you're running multiple map
produces on a single machine now is this
like it okay so the question is if
you're running multiple maps on a
machine at the same time how does
resource allocation happen the short
answer should be I don't exactly know
though the longer answer would be I know
there is a system which runs and jobs
have priorities and it swaps things in
and out sometimes your maps will get
preempted by someone that has higher
priority than you if a map is waiting
for data to get there it will block
until all the data arrives and other
maps can run during that time so this
sort of a some friendly multitasking
that goes on yeah absolutely I don't and
that's going to become more interesting
as we have more course
the question was is there memory sharing
policy and yet you have to ask for how
much memory you want to use and if you
blow that memory budget then you get
killed and you go yeah I think priests
remember budgin do it again maybe isn't
the most efficient thing to do but you
know it keeps you honest in the back
okay calm the question was what's what's
the story with google maps where to come
from what's the culture around it when
it was released what's the sort of
design process that went on um I wasn't
there let's sort of the short answer I
know how features happen now is we built
the system that makes certain things
possible because certain things get
faster I know this vague because I can't
talk about things but you get the idea
you realize engineers realize we can do
something that we couldn't do before so
we holo UI person in and say hey we can
do this thing and we'd like to maybe
have it look like this and they're like
that's terrible and and then we go back
and forth until we agree on what a
really good design is and and then it
gets added in some sort of sensible way
the bigger the changes the more layers
of bureaucracy you kind of have to deal
with which is kind of good because there
are millions of users and you'd hate to
irritate them all at once um I don't
know if I answered your question
cessfully but I'm thought I tried um
would you actually you were first I
think in there
by that is if you give a new rope right
everything you would be eliminating how
you about you I understand what you're
saying the question is is there a way of
doing MapReduce in incremental kind of
fashion like if you get one more road
can you rerun things and not have to do
the whole computation over as I know
right now there isn't a way to do that
there are problems inside Google for
which that's a really serious issue and
I know people are worrying about that
sort of thing so the way MapReduce is
set up right now it's not really
designed to do that you can imagine kind
of doing it because you think okay this
is independent it affects these things
you keep like a Providence chain or
something that would figure it out but I
don't think it's implemented oh sorry
yeah I was over there first all right
you keep the serving as simple as you
can basically the serving is like here
take it you don't have to do any process
you don't have to look at the disk have
to search around memory you want to just
barf out the cache data as quickly as
you can otherwise you can't get the kind
of the turnaround that you want it or
disk seek is like 10 milliseconds you'd
like query response feel like 30
milliseconds or something you haven't
got much time left so I'm over there and
then okay I gotta cut so thanks very
much this one</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>