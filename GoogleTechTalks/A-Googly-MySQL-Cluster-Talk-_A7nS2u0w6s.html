<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Googly MySQL Cluster Talk | Coder Coacher - Coaching Coders</title><meta content="A Googly MySQL Cluster Talk - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Googly MySQL Cluster Talk</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_A7nS2u0w6s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I
so this is stuart smith he's a my cycle
cluster hacker from my cycle a bee he's
an Australian as well which is kind of
cool and he's been involved with linux
australia for quite a while now this
talk is going to go to Google Video
externally as well so if you have
confidential questions just wait until
we've turned the camera off please cool
so let's do it Smith thanks Michael this
is episode 1 of a trilogy of three that
will no doubt be better than the first
the new Star Wars movies and the reason
it's in three parts is because
openoffice won't let me paste slides
into presentations so I'll start off
with a bit of an intro to mysql cluster
um do we have any current people who
have played with cluster no do you know
it exists cool okay so you probably
already know a bit about mysql and in 41
we actually added a storage engine
called NDB cluster which is a
high-availability clustered storage
engine and in 500 we made a whole bunch
of improvements to get better speed out
of it and 41 have mainly did best at
like primary key lookups and index
lookups and in 51 without it even more
new features so you know a bit about how
we have storage engines inside mysql
it's really easy to just have different
tables and different types and there's a
bunch of well-known existing ones and
mysql cluster is implemented as a
storage engine so you can have some
tables in cluster some tables in my eyes
on some tables and an o.d be all in the
same server instance which is pretty
cool so mysql cluster in 400 500 is a
in-memory clustered database so in for
one and five point 0 all your data and
indexes have to be kept in main memory
but this main memory can be split across
several modes so if you've got like you
know 10 gig worth of data and you want
two copies of it you can have that 10
gig split over two machines than five
gig each and then another copy of that
and in 51 what we've done is added disk
data to that so you can have a non index
field stored on disk be our index is
still in main memory in a future release
will also have indexes on disk oui
checkpoint the data to disk so if your
whole power goes out on the cluster all
your data isn't lost
you restore to the last checkpoint which
is a nice way of getting your main
memory performance when you're
committing things as well as being able
recover from system failure you can also
operate the cluster in a diskless mode
which does no disk i/o and so if your
power goes out your date is gone unless
you don't so there's a bit of
flexibility there the time between
checkpoints is configurable so you can
work out how much data happens there and
you actually have a formula you can you
can apply to work out your disk i/o and
megabytes per second so you can actually
travel you're pumping parameters there
and then work out I need this disk i/o
throw node so the rough memory sizes
this funny little equation of size of
database x number of replicas and how
many copies you want times about 1.1 due
to overhead divided by the number of
data nodes we designed it for high
availability so we have sub-second
failover for a storage node failure a
really quick detection of node failure
and and fall over we have a hot backup
for the cluster tables see that type
start back at backup from a management
console and we get just taking the
background dump everything out and then
like have triggers on things and
generate logs which does it pretty
quickly and pretty efficiently and again
the rate that happens is configurable so
you don't have your back up like
overwhelming your cluster and of course
the amount of redundancy you have in the
system is configurable as well by an
easy number of replicas parameter we
have a shared nothing architecture so
you don't have an expensive storage area
network at the back end you just have a
bunch of like commodities machines
communicate via ethernet we also have
the transporter layer to the way that
nodes communicate with each other is
pluggable so along with using like a TCP
transporter layer you can also get like
SC I cards so high performance
interconnect that has really like you
know it takes about 1.4 nanoseconds to
do a full bite right between nodes on SC
I and stuff like that to get quite low
latency there which has a big
performance improvement for a bunch of
apps so the way the components look in a
cluster is at the end we have normal
high school applications like using the
client library that talk to normal MySQL
servers so these are spread out across
machines it just so happens that instead
of all these MySQL servers
talking to their local disk they send
messages across a wire to a bunch of
storage nodes so I've got a bunch of
storage nodes that actually store the
data or the SQL servers sit at the front
and then process the SQL and translate
to where exactly it has to look up in
which data node that kind of deal and
these data nodes here perform stuff
internally like talk to each other to do
the distributed transactions and
replicas and stuff you need so
everything here it is synchronous
replication so that any of these servers
when you do a transaction here it's
synchronously available here there's no
like lag between applying a log or
anything like that everything here is
done synchronously we also have a
management server down the bottom which
is it's a central job to construe it
distribute the configuration file to say
a high new storage node look at these IP
addresses for the other nodes and you
just a little management client list
like start backups monitor what nodes
come in and out and there's an API you
can do that too so if you want to have
like link into your own monitoring
systems you can do that so the type of
nodes we have inside the cluster is we
have a storage node which is grouped
into node groups and each no group holds
a section of the database so we have
internally you can partition things out
as well so you can have you know sort of
half your databases in one no group half
of its in another the number of nodes in
the no group is how many replicas you
have so your cluster is available as
long as you have one node in each node
group as in a complete copy of the
database we have as i said before
configurable number of nodes in an oh
group as in how many replicas you have
we have up to 48 in a cluster which some
people say what only 48 but where do we
get like people with transaction loads
that actually count up to that that
don't want to maybe also use replication
stuff so the traditional market of
cluster has been like telcos doing like
telco to let applications which have
like no small fixed size rose and maybe
a lot of them but really want the H a
thing in there and you know the
transaction load doesn't get beyond you
a few hundred thousand transactions per
second which we can quite easily handle
so we can expand it as well so if you
suddenly have requirement we actually
need a
you know 200 node cluster that can be
expanded with a relatively minimal
effort now part and up to like huge
numbers that will get there as well but
this code that has to be written for
that we have a management node said
distributes configuration does another a
couple of interesting things but we'll
probably get more interesting in the
future as we add some features and we
also have a mysql server node and you
can also program the cluster directly in
a C++ API so he didn't want to have to
go through the MySQL server and have
sequel doing everything there you can
actually program direct to a C++ API to
say you know do a table scan on this
table with these conditions and other
such things and get a performance boost
out of that because knowing exactly what
your applications doing and you access
these just like you would any my skew
other of MySQL server because it's just
a storage engine from MySQL right so
it's just really just typing sequel
queries as if it was anything else with
you know obviously a few different
differences underneath so physical
requirements for setting up a cluster is
the node as a process not a computer so
you need at least three machines so you
can have you don't have the problem of a
split brain and decide who's a cluster
we won't ever let you have two clusters
that aren't communicating with each
other and doing updates on both because
that would be like bad so we do a whole
bunch of on a quorum code within to
prevent that management server doesn't
need a powerful machine data nodes
typically have a lot of memory
especially in 41 and 50 since we had
everything had to be in main memory and
also of course caching stuff on disk is
off disk is a good thing it's on 51 when
we've got data on disk it's good to have
a whole bunch of memory in there as well
so you can cash more as well as your
index is still have to fit in main
memory the disco requirements especially
for 150 because we have all these lovely
configuration parameters in there you
can set limits and there's a couple of
cool equations to get that megabyte per
second of how much disk i/o you guys are
going to use they're generally not
cpu-bound you have a lot more AP I know
it's a lot more mysql server nodes have
to be running queries against the data
nodes to get them up to like a hundred
percent CPU usage so you generally need
a little maybe less powerful CPUs there
or less
number of nodes and due processes access
accessing the cluster and end medd we
say a single-threaded it has its own
internal task switching mechanism which
ends up being a bit more efficient than
doing like context switches in the OS
we've a lot less state transferring so
we actually had a good bit of
performance out of our internal
architecture there and if you have more
you have several suffuse in a box run
multiple nodes on the one box that's
easy where to do it my skew all nodes
need the most of you as you doing like
query processing parsing and doing a
whole bunch of extra stuff in there
dealing with results and sending them
out so you can need like maybe bigger
boxes or more nodes more nodes is easy
to add and you can add currently like
MySQL server nodes online so you don't
have to bring down the cluster to add
new mysql server nodes accessing the
cluster you can't add data nodes online
yet we're working on that if you I can
show you a demo patch if you want don't
expect you yeah possible to add data
nodes online later yes it will be
possible in the future so where we've
done some work on that now and we're now
focusing like development work on making
sure that the 50 and 51 releases
especially 51 the new features I'll talk
about later or as bug free as we can
ever make them and then I'll get back to
writing the online add node and adenoid
group stuff so it's in two parts passes
that online add data node one is
actually get the nodes into the cluster
and the other one is repartition data
inside the existing data nodes on to
those new nodes the first phase is
relatively simple the second phase is
hugely complex when you start to getting
into node failures during rebuilding
tables and system failures and stuff and
we want to get that right I wouldn't run
my experimental patch for adding data
nodes yet cool so if we look at an
example configuration of it so we have a
config file for the cluster and you can
say as a configuration is often options
here like number of replicas we say we
want two copies of our data in the
cluster and you can set like how many
concurrent
operations you will allow the maximum
number of concurrent operations so you
don't have this risk of suddenly trying
to allocate more memory for like storing
operation records and running out and
things exploding you set a maximum and
the storage new processes like don't
grow they stay that constant size so you
don't have the problem of like running
out of memory and stuff on machines now
predictability is a good thing when you
want AJ how much data memory and index
memory again predictable sizes we don't
run out that kind of thing that also I
believe is one of the online upgradeable
parameters so you can take down a box
add more RAM sticks in it bring it back
up sequentially through your cluster and
get more memory in that way which
directory you actually want to store
data in you know how many maximum number
of indexes and stuff you want the
cluster and here we have two storage
nodes in this little 11 management
server the management server you can
have more than one in there but you
don't need it for continued operation of
the cluster have it in there so it
doesn't really matter if it goes down
don't get out of bed just fix it in the
morning and we have like a mysqld server
configured here but you can add more so
we have these two storage nodes that
have an interconnect between them if
you're worried about like latency and
want to get bit extra performance like
SC I guys can do some cool hardware that
isn't too expensive and stuff manhunt
server the storage nodes pull
configuration out of the management
server find out where each other is and
a few other things and it's also used in
arbitration in the case of network split
so if you lose this connection here the
management server will decide who
becomes the cluster you can also run
mysql beyond that or another box and you
naturally have sort of the outside world
so your end applications coming through
the mysql servers and talking to the
storage notes so you want to like a
private data network down here so you
don't sort of have other apps like
chewing up for your network bandwidth
and you know it's dropping off so
failure scenarios a mysql server node
can fail so you can easily just restart
these and they come back into the
cluster or you notice there at the
bottom there we don't have like IP
addresses allocated to mysql service you
just plug in another box and say connect
to cluster bang for the old one out
because too tricky to reconfigure them
right and applications can easily
reconnect to another my school
ever known so they connected to a mask
you ever knowed you know they get the
connection terminated and just go to a
list of your other ones and connect it
through a lot of people use software or
hardware load balancers between them so
you can do it that way storage nodes
when a failure is detected all other
ones are informed about the failure and
since data is replicated right we've got
number of replicas equals two so we'd
have two copies of the database so we
can have three or four there is another
storage node to service new transactions
and the current implementation is that
when a storage node fails any client
applications that are doing transactions
involving that storage node their
transactions are aborted and you have to
retry those transactions but all that
should be written that way anyway in
case you get dead locks and stuff in
transactional engines so in reality that
shouldn't really be a problem in the
future we questioned all storage units
everywhere informed of the failure in
case there's any other protocols running
between between each of the nodes so did
I repeat the question then probably not
so the question was I just said nodes in
the no group informed of the failure or
as everyone and the answer is that all
data nodes are informed there can be
other protocols like backup like since
you've got replicas there you can do
other cool things like backup and other
queries and stuff around so everyone's
informed and can take action depending
on what's going on at the current time
and the management server node continued
operation of the cluster isn't reliant
on the management server being up and
you can also have multiple nodes and
when you have multiple management server
nodes the UI for sharing configuration
is make sure your config files are the
same on each machine but in the future
we'll probably fix that up so you just
have to be a bit more careful with more
than one but you know there's clever
guys here so you actually want to use
the cluster out of to this bit of
architects our introduction it's a
storage engine we don't support foreign
keys yet that's planned for the future
as well and we support the 50 features
you can do views on cluster tables
stored procedures that access cloth of
tables triggers that act on cluster
tables use your standard permissions
interface and I say these have to be set
up on each SQL server and
so inside the mysql database where we
store all this stuff that's my eyes on
table still so you can actually have
different mysql servers connect to the
cluster with different sets of
permissions which is either a bug or a
feature depending on who you ask but
people I think of like experimented with
replication and stuff to do this or
using replication to get it the same or
you can just write you a perl script
that runs the same query against each
server optimizations so we added a
couple of bunch of optimizations at 50
one of them is a engine condition push
down option which when you do a select
on an unindexed field so when you're
clearing an unindexed field we can then
evaluate that condition inside the data
nodes instead of the MySQL server node
so instead of doing a full table scan
over the wire or push it down to the
data nodes and if the data is fragmented
across you end up having these
conditions Excel uated in parallel so
you can actually have these unindexed
table scans happen in parallel on
separate machines and then just ship
over the wire your result set and
combine them which gets like a big
performance improvement there and we
seems common to see about five to ten
speed improvement over far for one like
without this feature and you get the
details of this sorry i press one button
in the explain output so we can say what
conditions it's putting pushing down to
the data nodes and this can be like
arbitrary nestings of like and if not in
all these kind of things because we
actually have inside the data nodes a
whole bunch of expression evaluation
language and compile it into that and so
the most girls over and send it off in
50 we also integrate with the query
cache so you can how to use the query
cache to cache queries that are querying
cluster tables so that works well we
also do batch lookups as well so when
you do like select star from t1 where
primary key is in a whole range of lists
instead of asking the data nodes what's
primary key one two or three we'll just
grab those in one chunk and send them
down in one query and you get you know
bashed look up which gets about 23
performance gain over what we had in 41
sorry if I'm going through quickly I've
got like a whole bout half hour of extra
stuff after I've finished these next two
slides
so scheming considerations internally
all tables have a primary key and if you
don't accept set one explicitly what we
do is create a hidden one and you can do
some real cool tricks partitioning in 51
to get like table partitioning across
nodes based on things other than the
primary key we have three types of
indexes we have primary hash indexes we
have unique hash indexes we have ordered
tree indexes and naturally we've
implemented these in a way in 41 and 50
that optimize this for in main memory
implementation so that you don't you
know store your entire key and the index
as well we just said like have a pointer
which is a lot more efficient for main
memory because it's you know costly on
desk but main memory it's cheap to look
up a pointer and you can spit create
like a unique index only using a hash
index so if you're not doing like
ordered stuff and just doing like you
know key lookups you don't need to waste
that extra memory and time whenever you
do updates and the primary key is a
primary hash index and an ordered index
unless using hash is specified so give
flexibility there I wrote this script
called NDB sized up PL which goes
through an existing database and works
at how much space it's going to use in
various releases of cluster and says you
know you will need this much memory kind
of deal to do that as well as it you
know crew is the actual number of rows
you have so it's a little utility to say
will my data set you know reasonably fit
in cluster and which version do I have
to look at do you do space-saving
optimizations we've made in like five
point one right in the middle of the
talk I shall give where you can go to
find out more our online documentation
is ever-improving for cluster we have
like a web-based forum for those people
who can't use an email client and we
have an email based thing and like us
developers do hang out there and ask
quit and answer questions and stuff and
there's getting more people out in the
community as well recently released was
a book on my skill cluster which we've
got some good feedback on to the guys
worth that basically someone came at us
and said you as a bunch of stuff had to
find out myself this book has a lot of
those things and you can also send me an
email
on to part two
it is
yep
cool so what happens during node failure
or node failure in node recovery last
bit of technical stuff here so we have a
very simple cluster here with one node
group and two nodes in it so number of
replicas equals to two notes no one
melts the any idea how it cool it is to
find a picture of a computer that melts
and we go well we detect the node
failure and so we grabbed fragment one
so the fragment of the table from node
to this has two fragments here no two
will take over being the primary copy of
that fragment which is just like some
internal stuff of who takes charges on
the protocols so we do a node recovery
there and start processing transactions
really quite quickly just depends how
long you can configure like distance
between heartbeats and stuff to how
quickly no detection can happen so
assume you now replace node 1 for
example grab a new machine off the shelf
and plug it in and start the storage
node process and node 1 will then just
copy fragment one fragment to so all the
all the data it should have off node 2
and the rate at which it does that is
configurable so you can then work out
how long my nodes are going to take to
recover and how much network bandwidth
are they going to use up while doing
that so you can limit the impact that
node recovery has on the cluster which
is you know good yeah question how do we
determine when a node fails
serving a lot
some kind of comparative and that
without alone on your phone
okay so the question is pretty much how
do we detect that a notice failed like
do we do checks for like if the nodes
serving up bad data we have like a bunch
of check summing stuff internally and we
have a lot of asserts scattered
throughout the code that like checks for
sanity of various things and when we
discovered that a nodes doing something
bad we shut it down like we have a lot
of stuff in there would much rather have
the node go down or you know worst-case
the cluster and start serving up bad
data we're really into that which is you
know interesting when you go to like go
use code for something that wasn't
really isn't originally designed to you
have to realize well that condition has
changed now we have like lots of strict
checks in there you can also add extra
checks for doing extra check sums on the
stuff going across the wire and that's
along with the heartbeat protocol we
have to actually check that the process
on the machine is responding and you can
also then it's view also happens of the
other heartbeats on the other nodes so
if it just gets unplugged from the
network it will say well I can't see
anyone I can't see enough notice form a
cluster and it shuts down so we have a
lot of internal checks and stuff like
that so we pull the pipe the Frank f1
and f2 so the two fragments over the
wire at a limit as which you've
configured and we then no two will no
longer be the primary replica for that 1
and node 1 takes over and starts
processing queries again when that's
done doing node recovery part 3 better
than Star Wars yet no one's as bitter as
I am I
cool episode 3 I have a transition on
that slide did you know that I didn't so
new in 51 and for one cluster was the
new thing on there we started the
integration to have my squirrel talk to
the cluster nodes and the most optimized
way in 50 we added engine condition push
down which I've mentioned before of
pushing like where clauses on our index
fields down into the data nodes and so
you when you have your tables fragmented
across multiple node groups can like do
stuff in parallel and get like cool
optimizations that way we have batch 3d
interface so we send down like the we're
in one two three four down in the
storage nodes in one go reducing the
number of network hops because latency
is a real killer in these apps which is
why you know s CI is there something to
look at like faster transporter layers
and stuff like 10 gige buys you
basically no latency benefit over Giggy
you get big bandwidth but latency is
where your queries stay longer we have
more metadata objects we've bought
support more number of tables in the
cluster that kind of thing we use less
index memory in 51 we made a bunch of
optimizations in that sorry in 50 enable
query cache so we use a query cache in
cluster they work together nicely and
today I'm talking about what's new in 51
this is very similar to the talk I gave
it the users conference like really
really similar we do variable size rose
so in 41 and 50 no matter how much you
store it in like avatar column for
instance we're still allocated the same
amount of space which provides some nice
deterministic things of how much data is
going to have and all that things but a
lot of people whinge about this giant
chunk of wasted space so we took that
thing and just chopped it off so when
our support variable sized rose or
variable size attributes which is the
internal term on Rose and 51 so the
space saving is pretty obvious
especially when you consider rose and
feels the distort in main memory so
instead I'm going all this wasted space
you suddenly pack all these rows
together and you get a lot more inside
your RAM budget and where you naturally
just use as much space as needed around
the place so it's using loss a lot less
memory my NDB size PL script there knows
a bit about that
you really will see the space-saving of
taking your your data to 51 so a
variable size rose we pack more rows in
per gigabyte of memory which means we
can still larger data sets which
everybody loves so we also added in 51
51 I should mention is currently in beta
and will later be and stable when it's
ready and you're complaining and weird
formulas and yet to win that will be the
case so online add drop index so as
people probably know that when you want
to do like even an alter table or an add
an index to a table in mysql we create a
temporary copy of the table that looks
exactly how we want our table to be and
then with a lock held we like copy every
single row and do inserts inserts
inserts until it's phil we then delete
the old table and rename the temporary
table now this one it works it's
universal no special storage engine
implementation but you know it uses
twice as much memory you hold a lock for
a while and you're copying a lot of
stuff around with cluster this is really
expensive because you're no longer
reading stuff off disk and writing it to
disk you're pulling it across the wire
and then sending it across the wire
which who thinks that it's going to
perform well nobody so what we do in 51
is we add an index on top of the
existing table and then just fill it up
in the background while you can still
run or unstuff against it and you have
an index on your table without coughing
all your rows across without using twice
as much memory just constructing it on
the fly and don't worry about is this
all new code and stuff like this build
index code has been tested for a long
time it's like the existing stuff and to
drop it we do the same thing we just
take that into X offline and get rid of
it so how much fast is this really going
to be I now present the world's worst
benchmark which is running an entire
cluster on my laptop with other
processes like with evolution open so
there wasn't much RAM available it's two
gig in their evolution developers so in
50 let's say create an index on
one be going to take so much time ignore
the actual time here for a number of
rows but on the same machine same
configuration not changing anything else
I was running suddenly run that in 51
and it supports online index generation
so that really time there is just you'll
have time to make sure the metadata
stuff has synced across machines that
kind of thing across the nodes and
building the tables pretty quick and
you'll notice that here we say how many
rows have been affected so every row and
the table has been affected by building
this index because it's a every end of
every row has this but in 51 we don't
actually affect any rows we just affect
creating the index so you can like have
online add drop index which does it
faster in 51 actually online has a whole
lot less memory requirements as well as
being more adaptive cluster because
going to add and drop indexes as you
need them we also have user-defined
partitioning so as I mentioned before
you can have your data set split up
across multiple nodes and have copies of
this so currently what we do since the
dawn of time is you have a table inside
cluster with like a primary key and have
a bunch of data nodes you want you're
going to store data on split up into
your new groups or each holds like half
the database we apply a hash function
and we then work out from that where
we're going to store them so you get a
roughly even distribution of rows going
to each day each node group so you have
this perception of one great big table
so you select and get it all in one and
the reality is behind the fact that
spread across machines and it just sort
of magically assembles them back when
you do select in the right order so here
we can say we have two partitions and
the default in the 5.1 syntax we're
doing partitioning looks a bit like this
we have partitioned by keegs with
partitioning by key we're using the
default way of doing keys the first
partition is no group 0 and the second
partition is in no group 1 so you can
actually now like create tables that are
only partitioned on a subset of your
data nodes and do like a whole bunch of
really stuff that makes you have to
think about your data set but in 51
we're going to use a foot if I
partitioning so instead of having this
automatic thing that happens in the
background you as a user via a sequel
can type in where you want things
partition so you can partition by key
which means you can either put in your
own key specifies in there and your own
no groups depending on where you want
different tables to go by range we say
future for cluster we may not end up
supporting this as a supported feature
in 51 these decisions are the ones that
are currently being made so if you
really really want this pipe up now so
you can partition by range which you can
do this on any table type in 50 50 51
but we're just sort of working out was
going to be something we're going to
actually support with cluster but you
can petition by range to say partition 0
has values less than 10,000 and all
those are stored in no group 0 and
another range just like less than a
hundred thousand is in no group one so
if you have your own idea about better
ways to split data across node groups
you can do it that way or to try and you
know balance some load or do something
weird like that so or by list which
again is possibly in the future for
cluster so you can then give a list of
values for example here we have like
country numbers or sorry city numbers
you can say anything with those goes no
group 0 no group 1 that kind of thing so
you just define partitioning gives the
user control over the physical
distribution of data so you no longer
like except our hash function which
works really well with 2 to the N nodes
due to the linear hash stuff we're doing
but if you're doing like non like that
like six nodes you can get a little bit
like six knows two replicas it can get a
little bit non evenly spread so you can
help do better for that that's the
situation you want but we also support
this cool thing called MySQL cluster
replication we're not talking about the
interim internal mirroring of data
between nodes here so not that yeah
nodes in an oh group number of replicas
thing we're talking about taking data
and one cluster and replicating it to
another cluster over a normal mysql
replication channel so why would you
want to do this it's actually the user
usual reasons that you would use normal
mysql replication you want geographical
redundancy in case the data center goes
away for example
and you get blue lights in your eyes but
that's a feature you can get selects
blue lights you can also split the
processing node across the clusters so
when you want to do like just say
monthly weekly reports and stuff like
that you can do that all in the slave
cluster and not interfering with you
know what you really want this hey check
cluster there that has to process those
transactions by priority so monthly
reports is a good example of that so a
quick overview of my CL replication
which probably most people are familiar
with already you have a server you run
queries against it insert stuff there if
queries go to a binary log on disk you
have a slave that sucks these queries
over the wire into a real a log and it
goes through this log runs the queries
and execute it on itself as well as it
can write out to another binary log so
you can chain your replication should
slave and as you do new queries on the
master these are asynchronously traveled
across and apply to the slave so it's a
synchronous replication so you can have
a structure something like this we have
a master that writes a binary log and
you have a bunch of slaves that read
from this master that also do binary log
and they have slaves sitting off those
slaves that also apply all the database
transaction then you have some slow
unreliable network in the middle with a
bunch of slaves on the other side of it
and if this slow and unreliable network
goes away for a little while for exact
example your ISP decides to remove the
internet connection from a data center
and it comes back up it will then easily
these slaves will catch up now they'll
just keep applying things reconnect and
reapply and you could also you still run
some select queries against them so back
a cluster we've got a bunch of data
nodes here in the cluster and a bunch of
MySQL DS accessing you can also have as
I've mentioned a C++ API we call the NDB
API doing doing stuff to the cluster as
well so you have SQL coming in from the
MySQL nodes like update statements
insert update delete anything that
changes data and it kind of like
function calls inside the NDP API notes
so this isn't sequel the C++ API it is
you know perform a table scan on this
table
with these conditions all that kind of
stuff and the MySQL server is in fact
talking to the cluster through the same
API so if you think in a traditional
replication rule these MySQL servers
would be writing a binary log the
queries coming in I insert update delete
but how are we going to get the
modifications made by these ND bapi
nodes into these binary logs and more so
how we're going to ever apply these logs
on a slave that is we have no real
communication between the MySQL servers
to say for example this query was run
first and then this one so what order
would reply these in the serialization
of the queries has done inside the data
nodes so the MySQL servers don't know
about that it's all done inside the data
nodes so what we really need to have
coming out of here is a single canonical
binary log that gets all the updates
being performed anywhere and gets these
from the data nodes from where the
serialization happens so when you get
this binary log and apply it on a slave
you actually get the same data just like
this thing that people like we're here
so we have this new threat inside the
MySQL server which we call the NDB
injector thread and what this does is it
subscribes to events inside the cluster
so NDB is the short word for NDB cluster
which is the storage engine mysql
cluster you know that unix thing short
and everything 24 letters or next less
NDB cluster so we subscribe to events
and an event can be row was committed it
sends across the role of this row was
committed to this table with the road
outta there so we're using row based
replication here we insert the inject
that row into the binary log to can grab
what rows are committed in the cluster
and injected into the MySQL binary log
so we get the single binary log that is
a canonical representing so it's not
just one mysql server there you get the
binary log for the entire cluster
including any ND bapi applications that
could be updating so it looks something
like this yeah that's a master box here
which grabs all the updates happening
here and rice them into a binary log so
if we have a closer look at how this
works we
Master cluster here so our cluster here
but we've got a MySQL server that's
going to act as our replication master
and applications could also query this
but more commonly you'll have them
querying other nodes so you have as much
load spared for this machine as possible
you can also have an MDB API programs
coming in here or the MySQL servers and
the NDB cluster handler inside that is
where the injector thread lives which
will get the events out of the cluster
and write them to the binary log you can
then have a slave which can also be
connected to a cluster so you can have a
slave cluster or if you're feeling
adventurous and think that one of the
other table handlers can keep up you can
quite welcome Lee have like another one
there but we're going to go with the
cluster because that's what you know
real customers are using and what we
envision most people will be using this
for so there's a replication channel
between these two which is the same
normal replication channel used between
MySQL servers so it's using all that
existing code and it's just using row
based events like inside the binary log
which you can also use for other table
types you have an IO thread in the slave
that pulls things into the relay binlog
and then an apply thread that
asynchronous so you a synchronously pull
this across and you have the reply
thread apply these row events inside
your slave cluster and this cluster
could also write a binary log as well
you could also have machines writing
binary logs out of that and chained them
so if your muscle cluster goes away you
can still run applications against the
slave and do some manual failover like
you do now with you say your master went
away in your application setup you can
promote one of the slaves so you knew it
the same way except now there's just
clusters attached underneath instead of
a single box so who spotted a single
point of failure in the setup yeah where
the bin logs sword yeah so we only have
one master here in one slave here in one
replication channel you can also have
another machine attached to the cluster
also producing a binary log and you can
have another slave standing by to apply
things if this slave goes down you can
also have a redundant replication
channel like for example a different
providers internet link so if the master
goes away all the replication channel
goes away
you can do failover so you can hear
typing the commands do fail over and
have this backup replication channel
down here take over replicating from the
master cluster to the slave cluster so I
think this is really cool for now you
have like lots and lots of redundancy
and it's not just cool i call it shiny
so how do you make this fellow ever
happen anyone else get over 500 fan
shiny right ok everyone should watch
firefly so how do i make fell over
happen so first i'll explain a bit of a
concept inside cluster so we have this
idea of an epoch which is a point of
synchronization in the cluster this is
the checkpoints i was talking about
before everybody agrees all the storage
and agrees about what transactions a
disk persistent so in the case of total
system failure what would we all restore
to so in case of a system crash that's
where we recover to that's what we call
about epoch really simple explanation of
it so back to failover it's currently
manual process like the same you do with
normal replication promoting us loading
a slave to a master but it's only four
simple steps for cluster these are also
documented in the manual so don't like
furiously write these down so the step
one is to find out where the slave
cluster is up to so in the binary log
produced by the injector thread each
global checkpoints are each epoch
happening in the master cluster is a
single transaction so we end up having
the same dis persistent recovery thing
in the binary log as well and where we
are up to and applying this is recorded
in the slave so we've added a new system
database called cluster and we have a
table in there called apply status which
is two columns server ID and epoch it is
also engine equals NBB so it's available
anywhere so you can check from any my
school server connected to the cluster
what a pocket it's up to apply so on our
backup slave so I back up the cluster
I'm back up slave we get the max at the
maximum epochs the latest input epoch
that we applied we then go to step 2 we
want to find the binary log position for
this epoch since you have multiple
servers being able to produce a binary
log from the
Master side you can end up having like
purging bin loves different times or
even like bringing them up to produce a
bin low at a different time so the
position in the binary log like this a
binary log file number and position can
be different for epochs on different
machines so we have a bin log index
table which maps pin lock position to
the global checkpoint which is the epoch
and it also has a bit of statistics
there of how many inserts updates
deletes and schemer operations were /
epoch it's at my eyes on table which is
/ master so each master has the
different mapping as I mentioned so we
grabbed the latest one epoch we've got
from the slave and then we want to on
the backup master select where is this
epoch in our binary log step 3 we want
to synchronize the backup replication
channel so we want to run the change
master query it's on a backup slave use
the information we've gotten from the
previous queries and go change master to
LA and hit enter and the fourth step is
the simplest step of all you say start
slave and then it starts replicating
from where it left off and you know
we'll hopefully catch up in time you
have blow up animals were through here
often I work from home sometimes this
weird stuff just doesn't happen in fact
probably more weird stuff happens but
i'll be sure to get the company to hire
someone to walk through with lower
animals so couple limitations and the
current implementation the failover of
the replication channels is manual thing
so you have to type in those things so
we haven't automated promoting slaves to
masters that's naturally as you can
appreciate a tricky problem if you have
a solution yeah we'll listen but you can
scripts like this changing over on it so
you just have to have someone type in to
run the script and since all the updates
are going through only one injector
thread on one node there is actually a
limit to how much how much transactions
you can have xq on your cluster and go
through this replication channel as it
is now so we say this is generally less
than what you can achieve with a
decent-sized cluster and hammering it
which is a shame but you know not
everyone is running 100,000 update
queries a second or something
that so we're working on overcoming this
as well to make sure that you can have
you know the maximum sort of like
throughput of a normal cluster and
replicate that but you can now have this
very high uptime cluster so we say five
nines up time that has really good
performance for a bunch of things we're
getting like more and more performance
improvements as we go like one of the
new optimizations want to do in the
future is to help optimize joins and
have arbitral genes evaluated on the
data nodes instead of up in my SQL
Server so like currently now depending
on the type of joins you doing you may
have to rewrite those queries to have
good performance on cluster there's a
bunch of tricks for that because
generally of course when you start doing
like one table than another than another
and sequentially that's network hops it
gets a lot slower than just you know
seeking a different part on disk so
we're working to overcome that but a lot
of optimization can be done in the query
and sometimes like you know d
normalizing tables and a bunch of other
tricks so I think we have a cool name
NDB is cool we have replication between
it and another cluster so you have load
balancing and geographical redundancy
and we have redundant replication
channels between these clusters so you
can have like internet internet links go
down and select replication channels
happening and otherwise known as
redundancy up the wazoo so a little bit
of a quick talk on disk data so we had
two options we're going to add the
ability to store data on disk in cluster
we could either put an existing
disk-based engine into the NDB kernel or
careful range re-engineer NDB to support
on disk attributes so we decided on
number two for a start this fits in a
lot better without like deterministic
approach to a bunch of operations inside
the kernel and how we implement that and
it also allows us to like do cool stuff
like have some attributes on disk and
some in memory and do like same
transactions between them so in five dot
one we have data on disk and we'll have
index ons on disk in the future
naturally you do your indexes
differently if you're implementing index
is in memory or disk so what we're going
to have to do now is go having a good
solid disk based index implementation as
well as well as supporting indexes in
memory so you get a lot of options here
so a few concepts you're probably
familiar with like in ODB tablespaces we
have things a little bit different in
cluster
now and 51 a tablespace contains several
data files and you can add these as
online operations you can also have
another table space and have different
data files in it like you can split
things up across spindles on different
rate devices that kind of things and you
also have a long log file group which is
used for recovery you can have undue
files in this and naturally add them as
well current limitation in 51 is when
you have one log file group in the
future we'll have it so you can have
multiple load file groups and so you can
have tables in that table space on that
spindle with that log file group on a
different spindle and another table
space and other log file group on
different spindles into you know way too
much stuff for dbas to tinker with and
files a per node because we have
replicas of things and you know we split
things up petition them you have these
files on each node so let's look at the
SQL creating a log file group pretty
simple SQL give it a name add an undue
file the initial size as well as engine
equals nbb you can also add another one
pretty simply alter log file group we
also don't currently auto extend files
but you can always add more files to it
and we do plan to have auto extend in
there in a future release tablespace the
same thing very similar create
tablespace give it a name data file how
big you want it engine equals NDB you
can also add stuff to it as well and we
also don't ought to extend these so how
do you create a table that stores things
on disk so we create table here if a
primary key and two unindexed columns we
also say which tablespace you want to
store it on we say storage disk so it's
going on to disk B and C columns here
will be stored on disk and the primary
key one so picky one will be in memory
as it's an index column if you're also
table to add an index those fields are
going to have to move to me memory as
well so to check free space in there we
implemented a new information schema
table so instead of doing like you know
show status or doing a bunch of other
things to find out you know how much
space is being used by what in your data
file you can now go select from
information schema dup files so
information schema in 50 of extended and
512 data query file
so there's a bit of an ongoing debate
where they're going to keep this inside
the information schema database or if
we're going to move it to a performance
schemer so where you put more
performance metrics inside the server
have like a separate scheme out to query
that say there's no separation between
like ddl changing stuff and stuff that
actually happens to queries it's a
lovely design thing we get to debate out
the actual schemer of the table is
pretty decided and supports like you
know every possible way you could put
data in files so for any soy dejan that
comes along you can actually reasonably
put rose into the information schema
table it currently only has information
about MDB tables but you can pressure
your favorite storage engine developer
to add the code required to their
handler and you do need of course to
understand the output on this table a
bit of knowledge about the internals I'm
getting the time things i'm going to
talk really really quickly soon how do
we allocate disk space for cluster we
have a data file and we split this into
extents an extent is simply a unit of
allocation of disk space for cluster so
when you create a table and store some
rows in it we allocate an extent from
the data file to that table and start
storing Rose insider and each table is
allocated extents so when you create
another table it gets a separate extent
which gives it a room to have like
contiguous rose on desk so you fill up
the table and get the extent to be full
for that thing you need another one so
table gets bigger we allocate another
extent on disk so in this situation we
have one full extent one partially used
and if you delete rows from this one up
here we don't actually like reclaim that
yet we'll probably will in a future
release but you can get that and it'll
still be use for that table so if you
like delete rows in the middle we will
we will if we use that space and here we
have one free extent so if we create t3
we can allocate one extent to it or t2
can grow by one extent or t1 that one
too so information schema files for data
files we have a data file what table
space belongs to what the size of the
extent size is the number extents in the
file and the number of free extents so
the free extents modify x the extent
size equals the free bytes that can be
allocated to tables so this doesn't
we're not reporting here the
the free space inside the extents
allocated to the table we're saying how
much extents can be added to any tables
you can use like useful views like to
get like a DF like output so we have
nothing we create a tablespace you can
see now that we have data files in there
with a hundred percent free space and we
create a table in there and insert some
rows you can see that the space goes
down so you can just monitor this to
work out when you're going to have to
add more add more data files to the
costume for a log space we also have
some stuff in there and if you're
running out to off and you're getting
too low log pressure you may want to add
some more so the future of perhaps we
will we will report an approximation or
even exact count approximations a lot
easier for us to do an approximation of
free space inside the extents and of
course other storage engines so internal
stuff of course we now need a buffer
manager since stuff is on disk we have
to catch that in memory you have to work
out that caching strategy so a good
buffer manager equals good performance
our one is okay in this 51 release and
naturally we're going to optimize that
as releases go on we use a no steal
algorithm which is the same we've used
for a normal cluster that is and you say
the maximum transaction size the maximum
operations can be affected so you have
this like deterministic quality of how
many things can be going on at once and
how much memory you can allocate which
is the same as always and configurable
and also save some disk rights which is
nice we've also added optimized node
recovery so traditional notre recovery
that i talked about before copies
everything over the wire which isn't too
bad from main memory database because a
couple of gigs over a wire isn't too bad
the advantage of this was of course easy
to implement and get correctly not too
bad for your gigs of data of course the
bad size is it's very very bad for this
data when you suddenly have you a lot
more data having to be copied over the
wire so there's a paper out there for
the actual algorithm and stuff and stuff
we considered and basically what we do
now is we can recover from checkpoints
to copy over a delta across the wire
instead of the entire data set which
gets to be quite neat and saving time
and this also works for main memory
tables but naturally if the disc died in
the node you can have to copy everything
across the disk data gets lets you get
more data inside cluster
keeping main memory performance when
needed so you can create some tables in
memory like with some fields in memory
and some one disk so depending on the
kind of workload and transaction load
you have on them and any questions we
have a multi versioning okay we do not
do repeatable read but we do not like
sort of where transactional engine so we
have transactions but not repeatable
read repeatable read gets really tricky
one in when you want to do it
deterministically so it's actually a
tricky problem to solve inside how we've
got things there but you know if someone
really really wants repeatable read
we'll work out a way to do it any other
questions okay I've one little thing to
go over in full 1 and 5 are we've done
table auto-discovery so when you create
table like schemer ops on one thing we
discover that in all the other mysql
server nodes so you don't have to you
know write create table and every mysql
server but in if you did create database
you have to do that everywhere because
we didn't autodiscover databases but if
you're using 51 create database and
every replication on they get detected
everywhere which is just a bit of a UI
950 so we have table auto-discovery now
and 51 database and table 1 as well as
all these lovely features which I've
talked about and the increasing efforts
do better error reporting more internal
QA on a 5-1 release than like we've ever
had so there's been lots and lots of
bugs found internally which means we
have to fix them improvements in cluster
management can you tell her in a bug
fixing cycle at the moment improvements
in cluster management like restarting
multiple nodes at once and other such
things improving documentation so the
documentation Xin a lot better state
than it was like a year year and a half
ago as well as improving the easy ease
of use of configure ating the cluster
and I was taught by support people i
have to add in that we have total kick
ass support so a pretty intense level
support for this kind of stuff so thank
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>