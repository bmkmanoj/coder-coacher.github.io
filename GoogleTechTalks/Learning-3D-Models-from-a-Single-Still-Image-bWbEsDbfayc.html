<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning 3D Models from a Single Still Image | Coder Coacher - Coaching Coders</title><meta content="Learning 3D Models from a Single Still Image - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning 3D Models from a Single Still Image</b></h2><h5 class="post__date">2008-02-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bWbEsDbfayc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm very happy to introduce Ashutosh
Saxena who is a PhD student from
Stanford University visiting us today
Ashutosh has been working with professor
and drawing at Stanford on very exciting
work on learning 3d structure from
single images and also on robot
manipulation and navigation right so
without further ado I give the mic to
Ashutosh hi I am a station I will talk
about how you can get the information
present in a single image to do amazing
things for example if you have a single
image like this the goal of this talk
will be roughly around how can you
obtain a death map from this image a
death map is a map in which the color at
each point gives you the depth value at
each point in the image for example
yellow points are closer and red points
are in the middle and white and blue
points are farther away now let's first
look at how do humans perceive depth
there are a couple of different
algorithms that you men use for
perceiving depth first of the mlst
vision in which you take two images and
you compute the differences between
disparity between two images so
basically the humans of two eyes so from
the two eyes you look at something from
two different viewpoints so you can get
a sense of depth the second one is
motion parallax which means that I if I
look at this scene from this point and
then I move around and look at the scene
from a different point then you will get
some parallax the positions of the
objects will change so you can estimate
that third one is focused defocus which
means if I focus on a certain point in
the image the other parts of the image
will become blurry so that also gives a
sense of depth one of the last and the
most important of the Q's is monocular
cues which means even if I close my eye
or if I look at a single image I can
understand the image completely I can
tell how far each point is in the image
and so on so in the last few decades
very good computational algorithms have
been developed for many of these
properties like exterior vision
structure for motion
emotion is the one in which you take
multiple images from a camera and you
can stitch things together
Chile John Akasaka was working on this
sitting there and focus the focus there
were some algorithms in 1995 however all
of these algorithms take at least two
images to compute a death map they need
required two images to get a sense of
the 3d structure what happens if you
only have a single image right now there
are no algorithms that can take a single
image and computer death map out of it
computing depth map from a single image
is hard because local properties are not
enough for depth estimation let me ask
you how what is the depth of these
patches it's really hard to tell because
you cannot see anything but if I tell
you the whole image then you can
understand the relation of those patches
with other things and tell how far those
patches is so the one important point to
keep in mind is that to understand the
image you have to look it as a whole you
have to understand its relation of the
with other parts of the image so
population of the sky with respect to
the trees with respect to the ground and
so on so we use a lot of monocular cues
used for that perception so if we look
at a single image what do we look at if
we see a consistent texture in the image
for example in a grass field or in a
farm like this and if there are two
patches one here and one here we can
look at the local properties of these
patches look at how the lines converge
in the image to tell if the patches that
patch is closer or farther away this is
similar to the property that you see
usually in railway tracks that railway
tracks will converge towards infinity
now let us look at another image let us
say you have an image of a panorama like
this clean of mountains and how do we
estimate death from this image if we
look at two patches we will notice at
the patch which is farther away is hazy
or blueish as compared to the patch
which is closer so by looking at the
color or blurriness or easiness in the
image we can tell how far that patch was
in the image
there are many other cues that we also
use which are based on shading and focus
and D focus some other cues are known
object size if you look at this image of
a beach then you will notice there are
known objects that you can see in the
image for example these buildings these
people and if you look at the each of
these people then you can infer how far
they are because you know people are
roughly of the same size so if a person
is farther away that he or she person he
or she will be smaller in the image as
compared to a person which is bigger in
the image and therefore is closer so all
of these cues give all of these cues
give like weak information about the
depth so another algorithm we will put
all of these cues together to try to get
information from each of them and also
capture the information between
different parts of the image so to start
what we do is what most of the computer
vision algorithm does we will compute
some features that try to capture some
of these cues we will start by computing
convolving the image with certain
features these are basically edges in
different directions and texture of the
images and then leave to compute get
some more global information we will
append the features of the patch with
the features nearby so this basic many
of these monocular cues will be captured
in these image image features which will
make the bottom most layer of our
algorithm called Markov random field the
basic idea here would be that given
these image features we can we will
model the relations between image
features and depth at the points in the
image and relation between different
parts of the image using a machine
learning techniques known as Markov
random field so machine learning is
about giving a data and some labels you
can infer the relationship between them
in this case what we are trying to do is
to infer the relation between image
features and the depth at each point in
the image so those are our labels
and we are using supervised learning for
this purpose and we will be using some
laser data so basically we have these
image features at each point in the
image and our goal is to infer the depth
at each point in the image and there
will be modeling manipulations such as
the relation between image features and
depth the relation between nearby points
in the image for example if I look at
let's say this image then you can see
that on this wall that two nearby points
are more likely to have similar depth as
compared to two other points which are
farther away and we will do that at
multiple scales so because even if you
reduce the image to a smaller size you
can still get a sense of the 3d
structure so this is the probabilistic
model the machine learning algorithm
here what we are doing is we are
modeling the depth at a point in the
image as a function of the image
features and we are also modeling the
depth relation between depths at
different points in the image so what we
are saying is that the depth set image
is dependent somewhat on the image
features and the depths of nearby points
also close related this is modeled by
the second term I will skip over this
now let's look at this image if you
notice the patches a and B they are at
different points in the image so their
depth are likely to be dissimilar as
compared to patches B and C which are
true which are which lie on the same
building and their depths are likely to
be similar so what is happening here is
that if there is no edge between two
points in the image and if the texture
of the two points is similar then those
points are likely more likely to be at
the same depth so if it also model model
these properties
that is shown here so because there are
two points in the image for example two
points on this ball so both of the
points on the wall are whitish so they
are more likely to have similar depths
as compared to a point let's say over
there the boundary between floor and
wall which has different color and as
the edge between them so those points
are likely to have different depth so I
have a boundary between them so we put
all of this in our machine learning
algorithm and what we do now is we have
a robot that can drive around and
collect these images so these are the
images and these are the ground truth
death map collected using the laser
scanner so we have a laser scanner a
laser scanner gives you the actual depth
at a point in the image for example
Google has the trucks has trucks driving
around with lasers and cameras mounted
on it which they probably are going to
use for street maps once we have these
images we will use the algorithm that we
described to correlate yeah go ahead yes
so images are usually of more than
thousand by thousand resolution and the
death map is usually lesser than 200 by
200 dead points
say that again
so the problem you're addressing is
really hard to get my new details right
now in the images so we are looking at
to get how to get a codes 3d model that
looks good and it's correct so I didn't
get your question actually correct
completely
yeah sure we actually don't need that
full resolution and Ethne for that
purpose because you can use image to
build a high resolution death map if you
need to so after we train our learning
algorithm on this data the image and the
ground to death map let's see how it
performed on some test images so this is
the original image that was taken from a
usual digital camera and this is a
ground truth death map for the collected
from the laser scanner and this is the
output of a learning algorithm so here
yellow points are close or white is
farther away you can see that the
algorithm makes some obvious mistakes
like it thinks that these trees which
are shown correctly in the laser scanner
are actually not there this is another
example of the tree down to death map
and the estimated death map from the
from the machine learning algorithm
so the ground the laser scanner the best
laser scanner that you can get Raziel
reasonably gives depths only up to 80
meters so there is noise in the training
set so laser scanner completely misses
this building because it's just about 80
something meters okay so it turns out
that it actually extrapolated in this
particular case so so it's correct but
correct in the sense is definitely
estimator yeah so we have a model it
which transform the coordinate system of
the laser to the image fin actually in
this one is two particular cases they're
exactly taken from the same perspective
this is after transformation so the
differences you might see only because
the resolutions are quite different in
these two cases
so in this but these particular images
which I am going to show this slide in
the next slide they were taken in
Stanford University but in different
physical locations and they were not in
the training set definitely some more
examples this is the tree structure and
drowned to death map the bush near by
the tree in the background and the
estimated death map it makes some
mistakes obviously thinks that this bush
is actually sort of connected while it
there's a sharp boundary there it misses
the trunk of the trees there's another
example a building with some trees and
this is estimated death map this is the
ground to death map over here
I'm not sure it seemed that you use two
kinds of features one is evidence of
patches right the texture and another
one is from the edges where the prayer
difference in color and presence of
edges yeah
is that all the features to choose and
also it seems that in the questionnaire
all the features the combined is a
linear combination so what does that
mean when you have power so yeah so
there are basically two types of
features let's go back so the features
are basically computed from by applying
these image filters to the image these
image filters are edges and texture
properties which we applied to the each
each patch in the image but that doesn't
give enough information so these are
overlapping patches for example what
what contains the most information is
how the features change as you move in
the patch so if I am looking at a grass
field the patch on the top of a
particular page will change in a
particular way because the parallel
lines are converging so there will be a
shift so appending the
a patch with its top neighbor would give
some sense of depth and doing that at
multiple scales also gives some sense of
that for example if there is a blue
patch and it's a small one and it's less
likely to be sky as compared to if the
same blue thing shows up in multiple
scales and sure so on similarly there is
a that's a particular feature for
example a tree feature over there so
that could show up in different scales
depending on the size of the tree
there's the first feature you/yous
another question was like how do how can
we believe that these features are
linearly related to the depth the answer
is well as long as you have lots and
lots of features there is some relation
that the machine learning has worked and
figures out there's some intuition on
why it consists white does it relates to
that it's because we are computing
texture energies and gradients so there
is some relation but it doesn't matter
actually what features use
maybe if you just throw in the draw
pixel values and you have enough data
you will still get some sense of depth
the second type of features are
features that look at how the texture
changes between two neighboring patches
and they help you get some boundaries
like this one so this blue here is a
sharper boundary as you go to the white
for the building all right this is image
of the corridor so cannot really see
anything in the image but there is a
corridor and there are some walls and
this is the ground to death map you can
see the corridor going in this selection
blue is farther away and this is the
estimated death map from that image yes
I will go to that right now yeah so this
is another example which we have
downloaded images from the internet so
this is the image downloaded from the
internet and the one below is is the
estimated depth map so there is a rocky
structure here and there is a house in
the background so you can see ground
that rocky structure comes up and the
house in the background
similarly the Street View and the death
map night scene in the death map this
mountain scene in the death map now note
that these images are quite different
from the ones trained on we didn't have
any of these images even similar images
in the training set in more quantitative
evaluation we have the laser data so we
can have a holdout test set and we can
compute how accurately is the algorithm
predicting the leg depth so this is the
baseline the arrow sign in log scale the
reason is if you are looking at a depth
of 80 meters then error of 10 meters at
80 meters is same as an error of 1 meter
at 8 meters there are some
multiplicative in nature so what we want
to emphasize here are the log errors in
depth so here we show the baseline is
point 0.295 and our best model gives an
error of 0.1 3 2 which corresponds to a
error of roughly 27 percent in the death
map which means if something is at 100
meters the roughly the error will be
mostly between 75 230 meters most of the
time
in contemporary work some of you might
know the work off home at all in which
they were producing photo pop-ups by
folding the image at ground vertical
boundaries they do not produce correct
depth estimates they're mostly produced
but fly throughs in their 2005 paper now
let's go a step further let's say we
have this one image and this is the
ground to death map for that that we
collected using laser scanner what we
get from the monocular algorithm is a
very coarse step map like this one and
I'd pick certain mistakes like we cannot
expect the monocular algorithm to work
100% of the time for example it misses
pillars in this case now let us say we
have a studio camera that many of the
robots have or any other scanner maybe a
laser scanner then you get some other
depth values but they make also mistakes
for example the stereo vision completely
fails in these black areas because of
this reflection and no texture so what
we do is the extent of algorithm to also
consider the monocular cues as well as
the studio cues and we combine them
together to produce a better death map
this turns out to be more accurate than
either one of these here are some
numbers for this so the baseline error
was 0.3 one on a standard data set and
the monocular algorithm gave point
naught 0 9 and the studio plus monocular
algorithm gave better results than
either one of those
oh yeah for this is point zero nine zero
is the monocular case that's a monocular
only only a lot if the data is different
so in this data that variation it was
more indoor images and close ranges
images so the number changes so we take
the evidence from the monocular features
as well as I studio put that all into
the Markov random field and yeah we use
both of evidence from both of the cases
so in this particular algorithm the
inference runs in about four to five
seconds
that was an optimized code in MATLAB
research level only based on
triangulation it looks at a pixel and
some small neighborhood around it
and it looks at the same feature in
another image and tries to compute this
parity but let's look at this area so
there was supposed to be a corresponding
image which I am not showing here but
this image is a reflection so if you
move the reflection also moves with you
and in those cases a stereo completely
fails further in any cases where there
is no texture then the studio would also
fail on the other hand we are not
dependent on this triangle triangulation
properties we are rather dependent on
some global structure of the image or
some connected properties for example if
there was no strong feature in this part
of the image then I would get evidence
from other parts of the image and that
information will propagate in to help it
out continuity constraints again
yeah stupid our global infants actually
I have a small link here in which we
also show that we also beat the studio
with global influence by some number I
will scoop over there let's go to more
interesting parts so right now so far
this was this was our old work fever we
are only able to get very cool step maps
you see the death maps are blurry
they're not that I create and what if we
want to really get accurate 3d models
starting from this getting the I get
taking some ideas from this death map in
this approach we notice the fact that
most of the scenes around us are
composed of small planes so if we over
segment the image image segmentation is
a algorithm in which pixels of similar
color and texture are grouped together
so each one of these portions you see
are supposed to be pixels of similar
color and texture now because they are
they are small planes most of the time
our assumption works out that these
small patches in the image lie on a same
plane for example this pairs lies on the
ground this pairs lies on a trunk of a
tree and so on
so in our next model our representation
would be these small segments in the
image and our goal would be to infer the
depth and orientation of these small
patches which means if I look at a small
pairs let's say small pairs on this
speaker then I want to infer how far
that speaker is and what's the
orientation of that small patch on the
speaker so that would be over variables
learning 3d structure from a image is a
degenerate problem in that if I give you
an image an image might represent in
finite number of 3d models because it's
the projection of the 3d world into the
image plane so we have to make certain
assumptions to be able to get 3d models
out of it so one of the assumptions we
talked about was vanishing points in
which we said hey there are some grass
fields which are converging towards
infinity or if you look at a root then
there are pal the roads the edges of the
roads are parallel or the railway tracks
are parallel but if as they go far away
they try they start converging
towards the horizon so that is one of
the assumptions that was first made in
by visually at all in 1996 the other
assumptions that were made for example
shape from shading in texture work so if
you take a image you can look at the
shadows or the texture of the of the
image and assume that us and assume that
an image is only made up of one type of
texture so if you look at a let us say a
grass field then if you assume that the
whole image is made up of grass and
there's the texture of the grass is
consistent as you move around then you
can get some sense of depth because the
texture would become smaller as you go
farther away so this assumption was made
in 1999 by saying at all there are some
other assumptions you can make you can
say hey everything in the world is made
up of a ground and a vertical plane so
that works well in corridors which was
worked by another leg biology at all and
by home at all
which is a photo pop-up work in which
most of the images they claim are made
up of a horizontal ground and some
vertical walls but making such
assumptions fails to generalize like if
you are relying on one of these
assumptions then it doesn't work on all
the images for example there are no
parallel lines in this image so it works
on any real two-track image but how
would it work on this image gone
vertical assumption doesn't work on this
image you have a mountain which has
slopes a tree which has different
branches that doesn't work so in our
next model we will extend the approach
which we just discussed and say we will
not make any explicit assumptions about
the scene rather we would rely on data
and some intrinsic properties of these
of the scenes to estimate the 3d
structure so in our new algorithm we
again start similarly we take a image
like this one and we divide the image
into super into small segments so this
time our representations is not each
point in the image but rather they are
super pixels in the image and we will
compute super pixels or small small
patches features for small patches in
the image now the grid might be
because we don't know how the segments
are turning out to be so we take we take
an image we get it super we over
segmented to get something called super
pixels and for each of these small
patches we will compute similar features
texture beaut features and append them
with the features of a small patch above
it with feature of a small small patches
below it and so on we also use some
super pixel shape features which means
what is the shape of these super pixels
if this is small patch or super pixel is
larger and it's in the top part of the
image then it's more likely to be sky as
compared to these small patches which I
usually turn out to be the case in a
tree because trees are mode to Celeste
uniform and so on
so they are only completed within the
super pixel and their shape features for
the super pixel but the important thing
is they are appended with the features
of the super pixel above it and super
pixel below it right and left and at
different scales regions and their
normalized depending on the size of the
region so this is not really accurate
representations or rectangles they
actually irregular regions like shown in
this particular image on the right all
right now what do we want to infer we
want to invert the plane parameters the
plane parameters tell you the depth and
orientation of the image
remember to represent it plane in 3d you
need three values to estimate its
complete position and orientation and
those values are called alpha so if this
is the camera center and we are
interested in knowing the the depth and
orientation of this plane then this is
we call it we represent this plane by
plane parameter alpha and the distance
of the plane from the camera center of
of the plane from the camera center is
given by 1 by norm of alpha you don't
need to worry about mathematical details
but this is just a representation the
idea is given these super pixels we want
to infer depth and orientation in which
each of these planes and now if we have
location and orientation of each of the
plane then we can use just simple
computer graphics technique to pick the
texture of the image onto these small
planes and get some cool looking models
so now that you have this location of a
small plane stand we will have a 3d mesh
which could be used to estimate
so what are the properties that we want
to capture in this image remember last
time we captured two properties the
relation between image features and
depth and the second property was
relation between nearby points but the
images and the scenes there are many
other properties that you might want to
capture so let's look at this image and
look at this particular area of the
image we will notice that if there are
two small patches in the image if they
are if they look similar then they are
more likely to be coplanar as compared
to two patches B and C which look
different and have a edge between them
and they are less likely to be coplanar
remember when I say there is a edge
between them and they look similar and
not hard-coding any of this information
I am just using relying on data to learn
how what is the meaning of looking
different so that's why it becomes worse
in many situations now where to make
this coplanar or where not to make this
coplanar that is inferred by using again
machine learning techniques is basically
take an image compute some features and
learn this sort of edge images or
clusion boundary image from the from an
image so the black areas represent where
the places where the plains are more
likely to be folded I mean not coplanar
or they are like likely to be ah cuted
which means like there is edge between
my hand and me so those are those
boundaries which would represent those
points so now we have a feature
representation from which we just
inferred some boundaries between the
edges and we also put in some other
constraints in the images for example if
you see a long line in the image for
example is sidewalk a line in the
building or a railway track then if that
line is long and straight in two
dimensions then it is more likely to be
long and and it's more likely to be a
straight line in three dimensions so
it's less likely that is this image
would turn out this is the cameras
then it's less likely that this line
would turn out to be curved it's more
likely that it would be a straight line
in the 3d world we also put that
constraint in the MRF Markov random
field so now we are modeling the
following relation relations the
relation between image features and
edges the relation between image
features and the depth and orientation
of the small patches and that this DS
relations represent if the plains that
the nearby Plains are coplanar not
coplanar so and the strength of these
edges like how I strongly do you want to
make two things coplanar is told by this
these edges so in places where there is
a edge you don't want to make them
coplanar and if there's no edge and they
look completely different then these
edges would be stronger and they would
look coplanar some of you might have
worked with energy minimization
frameworks in which you are trying to
minimize competing energies so this is
sort of that so they are the three
different energies when energy says I
want to have the depth related to image
features and I want to minimize that
something says I want to minimize the
relation between features and edges and
this one says hey I want to keep the
plains coplanar and these all compete
with each other and how to set those
parameters they all learn using from the
data using machine learning algorithms
this is a summary we have image features
that gives you boundaries and these two
things combined give you the depth for
the plane parameters let's say you also
have some object recognition algorithm
they are very good computer vision
algorithms that are developed by vision
community in which given an image you
can find some objects like these people
so you can put these constraints also in
the MRF in more like a grammar sort of
framework so if you find that something
is a person and something is a ground
then you can say things like the person
is attached to the ground so the plane
on which the person lives should be
attached to the ground here so those two
planes should be attached to each other
or if you know that something is the
person and something is a ground and
there is no way that the person could be
below the ground
you can put that constraints and
constraint in the MRF also again the
output of the classifier is not correct
always so you are not hard-coding these
constraints you are putting them as a
soft constraint in a Markov kind of
field as another energy in the Markov
random field so all of these things go
together in this problem spec model so
these products basically represent
different energies which we just
discussed I don't look at the math but
just look at the fact that there are
products of different energies so the
first one says the relation between the
depth and the features the second one is
the product of different things like
coplanar t constraints that the
information from objects and so on
it turns out that the way we model this
the inference in this model could be
done by using a fast linear program
which is really efficient and it works
in a couple of seconds so yeah so the
trick here is that we are using super
pixels or small segments in the
main-yard pixels so it's not in thousand
sis I think the total number of
variables is roughly hundred thousand I
heard one hundred thousand by number of
an Italian body that said three thousand
so five hundred features and other
properties which are some four thousand
safer detector you said to use some
constraints but the other hand is not
Molly the uncertainty is the applies to
the network say that I mean we're not
more than in certainty respect to that
play due to the placement of that that's
right
so could be if you have to buy you
survive the price by having say a
certain time
person detector which usually one
particular spoke but not me not
contemplating alternative
yeah that is very true so most of these
classifiers are not reliable at all so
if you look at the images uploaded by
people on the internet and we learned
that is the best of the classifiers that
we have then they mostly make some
errors so actually we have we have
different different classifiers they can
have different kinds of information for
things like cars and people you have the
size of the patches as well you can put
that in and you have the semantic
information that something is car should
be attached to something or should be
behind or in front of something and they
again go as soft constraints so yeah you
don't know the location of the object
precisely for the size of that rectangle
that you get from a classifier is also
erroneous so and could be biased but the
idea is you have you are not relying on
one specific thing but rather a plethora
of things so overall we hope that it
would turn out ok but in summary these
are the before we move on to results the
yeah go ahead
not very single
appearance
increased likelihood
so that we are not modeling right now
yeah it's a very important point like if
I look at this image then this part is
basically alluding the wall behind it so
the properties of the wall here in the
wall here I'm matching but they are
basically broken by this occlusion as
you say we are not modeling that right
now yeah but that could be built in the
framer we have not done that so we are
modeling these properties in some ways
the relation between image features and
depth Copelan arity connectivity and
collinearity which means straight lines
should be straight are more likely to be
straight in 3d and the information from
known objects other than these we have
not made any explicit assumptions about
the scene so hopefully our method will
be able to generalize well so so in
couple of examples we gave our algorithm
this image that we ourselves took on the
campus and this image that was
downloaded somewhere from the internet
and let's see how it what results it
gives
so remember we gave a image that button
nose
oops that was not seen before in the
training set and once we have the
orientation and location of the planes
we post pasted the texture from the
image onto this 3d model and you can
probably probably note I cannot use my
laser pointer when I and it's Cortana is
running it bigger so but you can see the
trees on the right so you can see the
trees on the right top and the curvature
of the trees on this side let's look at
this image which we last saw
hmm not from here but if you go to the
with my website which is unfortunately
down because they have a couple of
100,000 hits you can see the texture
yeah you can see without texture Maivia
you can add text and good what do you
mean by that well the grass important
I'm sure as long as all if I think that
it's all wrong because I've seen into it
objection
yeah that would be an interesting
direction to go for in particular cases
sometimes you cannot see occluded things
so in these examples what we are seeing
is that so for example if there was a
tree in the foreground somewhere and
that would elude some of the parts of
the image then I might use your method
to actually fill information in because
I know it's a parking lot it looks like
a road or a ground so I can color the
texture or that properties inside
pause yeah possibly so anyway so so
imagine that you are looking at the
scene standing in front of this rock
ledge and you crouch down so you get a
completely different perspective of the
model and you look it from the right
turn left and come back up
so this is the image of a mountain which
was all red from the internet and you
can just move in the image look at the
skyline of the mountains on the left in
the trees so it gives you a decent
flight through experience
not not really so not really no because
the skies I mean most of the images the
sky is farther away what's the only
indoor images that you see symmetricity
on top and bottom I guess the important
thing to notice is we have we have one
more information that's coming in which
is a location of the pixels in the image
or something is more likely to be sky
only if it's in the top part of the
image as compared to the so there are
some special constraints for example the
sky could only be in top of certain
things
is there anything
mostly a flat plane this image is
actually very surprising to different
people wired as in it predict the
mountains or the trees here the reason
is that a super pixels have have
sometimes very long so can I use my
laser pointer but there is this league
going on till this point right and these
super pixels help transmit long term
information that you were talking about
so if there was a small patch here that
would have some evidence transfer here
and that would get back to in this part
of the image so it turns out that these
long patches in the image help to
correct some many of these mistakes for
example that one I know now let's talk
about this is only one image now let's
say you have these three images this one
this one and this one
notice that if you take these three
images some parts of these images don't
have anything in common so don't know
what's wrong with my laser so this image
doesn't have anything in common this
some parts of this image don't have
anything in common with the other two
images the right side of this image
there's nothing common over here and
then a lot of repeated structures in the
images as well so you might have known
some algorithms like Microsoft
Photosynth that take multiple images
from the web and combine them to make a
3d point cloud and you they can you can
walk through in that area so what many
of those algorithms such as photo sent
or the technical name for the structure
for motion algorithms fail in cases
where there are repeated structures in
the images for example there are stereo
vision type of algorithms they basically
rely on matching between different
images so if you see something in the
image like a pillar and in another image
you also see a pillar then they are
basically think that they are the same
point and you can use triangulation to
do something but in many cases those
algorithms fail if they are repeated
structures in the image so this pillar
looks very similar to this pillar and
those algorithms feel completely in
different cases so what we do is we take
our monocular algorithm and we throw it
in
these multiple images to get actually a
larger viewpoint so get larger models
like this so this is a composite model
made from three images
you don't get a very good reconstruction
another arts because there was little
information and there were some printing
problems
any questions so far
faces so yeah so what I hear is that
many applications you can have faces you
can consider images from like aerial
images taken from the satellites and
different cases so inherently the
algorithm does not have any specific
problem with learning that is that we
train the algorithm on data taken with
ground images looking forward so it
directly doesn't work on those images
and we haven't tested it on cases like
faces or me aerial images so we don't
know
so basically so we have used two kinds
of training data that's not in our one
of the papers but this have a sort of
future work that we have been doing so
to learn the edges we have various
information we have images we have death
map and we have something called
scribbles which is from the website data
that I have so if the mod the algorithm
makes a mistake then the user can go
into the website and fix certain things
by scribbling over the images so you
have that data as well so putting all of
that in you have raw data which tells
you which parts look do you think they
should be similar or coplanar and which
part should not be coplanar again we
have like 500 to a thousand of features
and we use our training data to learn
edges one thing that helps quite a lot
is it it is easy to learn those edges
when you have some 3d information as
well how much time do I have
yes we use a Markov random field type of
algorithm a Markov random field type of
algorithm so it propagates depending on
how similar the uses the image
information to propagate the step is
potentials in this man
in what kind of objective so there are I
think two parameters which are tuned by
hand the meta parameters all the
parameters are learned so the meta
parameters tell how to weight the
information from edges versus objects
versus features that possibly it could
be learned by cross-validation or by
estimating the partition function but we
said it by hand all the other thousands
of parameters are learned recently we
have been trying to iterate and using
some variation and approaches to learn
it but that's not in the results so far
turns out actually we the thing that is
figuring out the most is gradients and
depth so remember we have feature
especial nearby future features so
they're actually sort of encoding
changes in depth because it doesn't
matter if you scale the whole model up
and down so in some sense it is
relatively maybe modeling it more
explicitly in the MRF could improve
performance that's true let me finish up
by telling some of the results that we
did in 2006 so we asked the users to
upload images on a small website so this
is the original image that users
uploaded the first row and the third row
and we used of algorithm to estimate the
depth map and build some texture models
and took some snapshots so if this is
the original image this is a screenshot
that you see you can see that the
staircase is occluding this balcony over
here this is a night scene which I
showed you earlier and you can see that
the algorithm was never trained on night
singing it I still predict some sort of
3d structure this is the house taken
from a viewpoint on the right of the
house it fails in certain cases like
these ones where it although the general
trend of the 3d structure is sort of
okay
but the texture is a stretched down so
it was considered as an incorrect model
more formally if we did the following
experiment we remember we have laser
death map also for some test images so
we compared various algorithms our
algorithms algorithm by how am i tall
and our descent algorithm this
experiment was done in the AMA 2006 so
that the bottom beats all our little
gotham's in terms of quantitative error
in depth in terms of internet images
which is a more interesting test case we
did the experiment of that users can
rate the models as being correct or
incorrect
based on some guidelines so if the
models visually looks correct the planes
are in proper places with respect to
each other and then that model would be
correct otherwise incorrect so our model
was able to predict quote-unquote
correct 3d models for about 65% of the
images of the 588 images on which the
experiment was run run on as compared to
earlier prior which roughly works with
33% of the models and doesn't give you
detailed structure and doesn't give you
depth either all right I will brief and
recently once I have put this algorithm
on the website we have ran this on
around 3000 images and so far the
performance as far as the users are
rating them is around 50% of the images
now we can have take these algorithms
and apply them in robotics I will
quickly show you two videos one is the
application to robotic navigation so we
can implement a real-time version of the
algorithm for this small robotic car
which only has one really cheap small
web camera and the goal of this thing is
to just drive around as fast as possible
estimate depths in the scene and avoid
obstacles so it potentially has some
applications in autonomous driving
so remember the hard part here is that
it's only seeing a very blurry image
from achieved web camera and estimating
depths in a real-time to drive around
so in this experiment I'm running around
with the handycam and a backpack which
contains a laptop and wireless interface
with our card
we have some recordings so this is the
problem unsolved problem in robotics if
you don't see anything you don't know
that you have not you're not seeing
anything like a negative obstacle
problem so all right you can apply this
to robotic manipulation as well so it
turns out it's a really hard problem in
robotics to pick up an object which was
never seen before but we have seen that
info images contain enough information
so we can use an information to find out
the 3d structure of the scene and figure
out how to grasp a object that was not
seen before so we tested it on these two
robots on objects that were never seen
before and applied to the problem of
grasping objects such as the staplers of
plates lying around in kitchen or
unloading items from a dishwasher so
this is a video of it shows oops
so this record is again taking a camera
finding out how does the image looks and
figuring out where to grasp it
so he's first trained on object data so
similar algorithm was trained on image
features versus what point to grab at so
the labels per what's the best point to
grab an object
and none of these were seen in the
training set by the way and then we
applied it to unload items from a
dishwasher I will skip over this so it
can unload items from a dishwasher and I
will show in steel
something more relevant for this talk is
once you have a 3d model of its scene
then you can automatically plan a path
which is the problem in robotics also
the car has to plan a part to go
somewhere
robotic has to plan a part to grab
something and in this case you have a 3d
model you have to plan a path for the
camera automatically to give you the
best viewpoint imagine that someone
uploads is photon Flickr or Google
Picasa and you want make a youtube video
out of it automatically in which the
camera automatically flies and gives you
different views of the scene so let me
see if I can get it running so take the
3d model compute a path based on some
properties
right so this was what work was done
with different people most importantly
main stone on which the 3d
reconstruction work is based for
reconstruction from single images thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>