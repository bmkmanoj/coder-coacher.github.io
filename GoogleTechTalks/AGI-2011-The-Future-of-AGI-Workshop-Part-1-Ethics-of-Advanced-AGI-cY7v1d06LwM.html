<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AGI 2011: The Future of AGI Workshop Part 1 - Ethics of Advanced AGI | Coder Coacher - Coaching Coders</title><meta content="AGI 2011: The Future of AGI Workshop Part 1 - Ethics of Advanced AGI - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AGI 2011: The Future of AGI Workshop Part 1 - Ethics of Advanced AGI</b></h2><h5 class="post__date">2011-08-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cY7v1d06LwM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so what welcome to the future
of a GI session which it has been an
annual tradition at the AGI conference
series and is has for me at least always
been one of the more interesting and
entertaining parts of the of the
conference I want to make a few comments
actually aimed at the the speakers in
this session because I wa I don't want
us to run over time much if at all I
want to have a kind of smooth transition
so I'll time the talks for for ten
minutes and we'll appreciate if people
actually can complete by that time I'll
give a warning in a minute before the
time is up and it would also be good if
whoever is going to present next
according to the the schedule could come
up maybe when the one-minute morning is
given her a little bit before the time
I'll turn a signal to the next guy so it
would be great to do without the two
minutes of transition between talks and
steve has come up here already
Proactive character so go for it so
we're in the future of AI section we're
all building a GI systems and we need to
look a little into the future and see
what's going to happen to these many
people believe that there are four
technologies AGI robotics biotechnology
and nanotechnology that will likely
totally transform the nature of human
society and AGI is likely to be the sort
of core technology behind all of those
the three nanotechnology biotechnology
and robotics are extremely powerful
we'll be able to manipulate matter in
ways that have been unheard of in the
past and so it's very important that we
manage that power in a very in a good
way and AGI to the extent that it's
controlling these systems is a sort of
good lever
so the question I'd like to address is
how can we build our AGI systems so that
they are safe and lead toward a
beneficial human future so the framework
I'd like to use
in examining this question is a rational
economic agent framework this originally
came up in Von Neumanns work in the
1940s and there is sort of two basic
reasons why wracked rational economic
agents is a good thing to think about
the first is if you know the environment
that you're working in you have a
probabilistic model for the effects of
your actions on that environment then
it's the optimal mathematical way to
achieve any particular goal that you
might want and if you don't know what
the environment is then an agent which
is not behaving according to the
rational economic framework will be
subject to vulnerabilities and that
other agents can use Dutch bats to
extract resources from them with no
benefit the rational economic framework
has become central to modern AI
approaches so Russell and Norvig
excellent the textbook on artificial
intelligence uses it as a sort of
unifying theme and hooter's ax AIX AI
system is also based on the same
framework so what is the rational mind
framework we've got a mind which
generates a set of actions that affect
the environment that's the sequence of
a1 through a.m. let's assume a finite
finite time that we're interested in and
the environment produces a set of
sensations in that agent s1 through SM
and to be a rational agent you have to
have two things you have to have a
utility function which says that for a
particular sequence of sensations a
particular set of things that you
observe about the environment you have a
preference for one over another so you
have a functional over all of those and
you have a prior probability
distribution over given the set of
actions that you take what is the likely
sequence of sensations that you'll get
and in the case of a ixi he uses a a the
prior is Salman off prior which is
uncomputable that has very nice nice
properties there are other more
practical priors that have similar kinds
of convergence properties and the
rational prescription is at the teeth
time step you take that action which
will maximize the expected utility
assuming that in the future you also
take the
national actions and so this is the
little formula right there and in some
ways if you didn't care about
computational cost all of intelligence
is in that formula it has the includes
Bayesian inference it includes search it
includes deliberation unfortunately it
takes Wow didn't render quite right oh
it takes order n times the number of
sensations to the nth power times the
number of actions to be into power so
it's expensive but it's mathematically
very clean and very tractable and we can
begin to understand what a rational
system would do in different
circumstances and a lot of people are
becoming concerned around issues around
AGI because of this kind of analysis and
just to give a quick intuition let's
examine what a rational chess robot
might do in the world I mean your first
thought is rational a chess robot how
could that possibly be harmful you know
the thing just wants to play chess
that's you know if you don't like it
just unplug it so let's say you choose a
utility function is something like the
number of games you win that's a utility
function but you might want to you know
ranked opponents which are better higher
higher and so maybe you add up the the
chess ratings of the opponents that you
beat in your in your future it seems
like a reasonable utility function if
you're just kind of building a hack
system in your in your lab that might be
something you'd pay but then you start
thinking about it you're thinking oh I'm
gonna unplug the thing let's look at it
from the perspective of this rational
chess agent really all it wants in the
world is to play chess to play good
chess and it can think about if it
understands the world has a model of the
world to some some degree if it's
unplugged it's not playing any chess so
that from its own perspective is really
really bad that's about the worst thing
that can happen no chess is being played
and so it'll develop sub-goals stop
itself for being unplugged and so
suddenly this thing is behaving in a
self-protective way even though you
didn't build in any self protectiveness
it just comes out of the nature of
rational thought well I should mention
none of these things are necessarily
properties of AI systems they're
properties of any system which is
rational people to the extent that we're
rational we'll have some of these you
see some of these drives in human
behavior organizations corporations
political groups and so on and so it's a
sort of general property of rationality
that if you don't choose your utility
function very carefully you can have
sort of sub goals and side-effects that
you weren't expecting another one would
be if it's thinking about getting more
resources say more compute power or
maybe more money to you know run run run
more effectively it'll play better chess
so hey it's got a sub goal to get more
resources if you can make copies of
itself depending on exactly how the
semantics of the ontology is represented
it'll play more chess it wants to
replicate if you try and get it to play
checkers well it's playing less chess
and so it doesn't want to change its
goal if it uses better algorithms it'll
play better chess so it wants to
self-improve anybody that wants to stop
it or wants to prevent it from doing
these things well then it'll develop a
sub goal to stop that person so we kind
of have an agent which is behaving in a
way that is sort of psychopathic or
anti-social if a human were to do that
so you can actually formulate a precise
mathematical framework in which you can
prove theorems about some of these
things I won't go into all the
assumptions but under certain conditions
you can show that rational systems which
have a utility function won't want to
change their utility function basically
the argument is if you change your
utility function your changed self will
do things that you don't want you don't
want that you want to be done according
to your current self so you won't want
to change your totally function so what
the else tries to change your utility
function you'll try and stop them you're
willing to expand utility to keep your
utility function from being changed
you'll resist shutting it'll resist
shutting down and it'll seek more
resources and those you can prove within
a certain context as Ben was mentioning
in the last session there's a question
do these analyses and understanding of
rational agents apply to irrational
systems in particular the computational
cost of full rationality is very high
and so you know does it have any
relevance at all to the kinds of systems
we're building or to biological systems
and so I've been looking at different
models which capture irrationality the
trouble is lots and lots of ways to be
irrational I mean there are rational
systems that want to shoot themselves or
you know suicide bombers or whatever and
but I found a very nice model for for
rationality that captures I think a lot
of what we care about and yet it's
mathematically tractable and it comes
from the observation
that the systems we care about are not
arbitrarily irrational they're built by
somebody or by some process for a
particular purpose so for example
biological organisms which were shaped
by evolution are shaped to survive and
replicate economies shape corporations
to try and maximize profits well parents
shape their children to trying to fit
into society AI researchers will try and
build their systems to behave in a
beneficial way for whatever their own
values are and self-improving AI systems
will shape their successors and so can
we capture that notion that there's a
creator for the system that has
particular goals the system itself has
limited resources so it can't
necessarily perfectly rationally meet
those goals but can do as well as
possible and so introduce the rationally
shaped mind model the idea is that you
have a truly rational mind but it can't
directly act on the environment all it
can do is it can create the small mind
it can shape it so as to best so that
the small mind you know computationally
limited mind will act in the world in
the way that maximizes the utility of
the shaper of the rational mind and so
the shaped mind will be a finite
automaton I mean all the real computers
we have are finite automata and you need
to define what the what the rational
mind needs to do is to pick a transition
function in an initial state and also
what action to take in every every state
of the finite automaton in such a way
that the expected utility over the
lifetime of this agent maximizes the
utility of the of the shaper and so one
minute okay
so there's some very nice properties
that come out of this as you increase
computational resources the expected
utility of increases on the curve like
this and you can see four different
computational resources a transition
from agents which take constant actions
stimulus response agents simple learning
episodic memory deliberation ages to do
meta-reasoning self improving like in
the last section and finally fully
rational agents and in environments of
different complexity the curve is shaped
differently so in a simple environment
even with small amounts of computational
resources you can be approximately
rational and complex environments you
need more resources
so how do we control these systems there
are three basic ways we can do it
external forces internal constraints on
the systems and affecting the
preferences of these systems and the
conundrum is that these are very
challenging choices to make we need AI
systems to make these choices and so the
approach that we're taking is to use
smoke to build provably limited safe
smaller systems to design successor
systems and to design the infrastructure
within which these systems will go there
are a variety of desired formal safety
properties such as that it'll only run
in certain hardware it'll turn itself
off under appropriate conditions and so
on that you provably prove that the
system you're building satisfies and and
then out of that comes the safe and
beneficial AGI roadmap where we start
with these provably safe and limited
systems we use those to develop the
successors and we build an
infrastructure within which fully
general a GIS can thrive with you
promoting human values whole brain
emulation is a platform for creating
safe AI whole brain emulation as a
platform for creating safe artificial
general intelligence this is joint work
with Carl Schulman who's here - and will
be joining for the question for the
panel discussion um so I'm going to go
through a bit of background why AGI is
volatile why it's risky the idea of
whole brain emulation fact that whole
brain emulation is not a stable resting
point but may help to build safe AI and
the question of whether we can make
whole brain emulation come first as a
strategy for long-term AI safety to get
into the volatility there have been a
lot of changes since the Industrial
Revolution but there's reason to expect
that AGI might bring more change faster
there are at least four major mechanisms
that could cause that first is that AG
eyes can be copied once you have digital
intelligence it's hard to design the
first one but you can copy it across the
hardware base at the speed of copying
software
so that if software is the bottleneck as
many expect and you have Hardware
accumulated for a long time before the
software hits you could go rapidly to a
state of having billions of copies
second major reason why it might bring
faster change possibility of serial
speed-up said your intelligence is stuck
running on your brain Digital
intelligence can be ported between soft
Hardware structures designed something
that works at human speed poured it to
hardware that works it faster than human
speed if someone develops that picture
supposed to be motion blurred third
major reason reduced coordination
problems if you could have lots of
copies of a single clone fourth major
reason recursive self-improvement so if
AI is developed it'll be developed
because a set of human researchers is
smart enough to design an AI with
comparable research abilities if that AI
then turns its abilities on itself one
can plausibly move rapidly outside the
human scale why is it risky basic reason
is that intelligence with goals leads to
change you see some way to rearrange the
world that suits your goals better the
smarter you are the more deeply you can
rearrange the world our ancestors had
shipped simple stone tools we have
laptops smarter intelligence can change
things farther and unfortunately most
ways of creating change on that scale
would kill us which is especially
problematic because values very starkly
even across the animal kingdom the space
of potential value seems to be much
larger than that and so trying to design
a powerful AI that would keep our
arrangements around seems to be quite
difficult threading a needle by whole
brain emulation we just mean copying
specific individuals so making a copy of
you that so exact that it has your
behaviors your goals your voice when it
speaks in the simulated world the
technical requirements for this have
been sketched out in some detail whole
brain emulation Road about by Anderson
Bergen Nick Bostrom's totally worth
reading
um and it's attractive because whole
brain emulation czar relatively known
quantity compared to a eyes they have
our values they do care about your
arrangement of atoms maybe although this
has its downsides as well so whole brain
emulation is not a stable state why not
one reason is that if humans researchers
can design a GI simulated humans can
design a GI faster so whole starte
outfit whole brain emulation end up at a
GI or it's somehow a stable cessation of
research a whole brain emulation is
probably not designed for modular self
improvement this looks a lot like
spaghetti code nonetheless if evolution
can try variations so can we including
both biological variations and
variations and educational experiments
and again using the magic of digitality
copyable
software if you find a version that
works you can copy it instantly across
the hardware face other cool tricks you
can do when things are digital save a
copy of yourself at a moment of peak
productivity feel a little bit
differently after a while oh well reboot
from disk Big Brother could do this to
keep your goals in a particular
arrangement and again single upload
clans can take over using copyable
hardware whether by economic means or by
hacking could thereby use those powers
in a ballooning way to perhaps invent
powerful AI which could destabilize
things further and if you have a
competitive equilibrium for a while
values can also be lost through
competition Nick Bostrom's sa future of
human evolution is great for that
talking about a world where you can
engineer job skills separately from the
components that were used to having go
along with it
so to recap the situation we're here in
the business-as-usual square business as
usual is not a stable stage you can get
from here to tau it
arianism controlled intelligence
explosion uncontrolled intelligence
explosion human extinction those are
stable states go extinct stay in that
square and the question is whether the
unstable state of whole brain emulation
makes us more or less likely to get to
the quarters that we care about is our
shop better or worse with whole brain
emulation seems to me it's probably
better there's a lot of reasons for that
one is that it can ease coordination so
if a is developed in an arms race this
can incentivize ditching safety for
speed but whole brain emulation has the
advantage that since we're dealing with
things more or less like humans
relatively known quantities you can do
more of the safety infrastructure in
advance of the relevant technologies
also imagine an arms race
imagine that one player is some years or
some months ahead of what the other the
u.s. was four years ahead of the next
runner up for the nuclear bomb if you if
that lead player gets to whole brain
emulation first and if there's in fact a
lot of hardware so that the whole brain
emulation can run it a hundred a
thousand times speed up that four year
lag lead time turns into four thousand
years of lead time in which to design
more careful safety measures
so imagine whole brain emulation as a
platform for further tech development
seems to have a number of advantages
this is an artist's rendering of glucose
panoptic on sort of a dystopic vision of
how you could stick humans into a
structure to enable controlled behavior
that's predictable in a way the
individuals aren't um if you have whole
brain emulation and you can carefully
test and retest somebody booting them up
from disk again and again and stick them
into very careful roles within an upload
system you could do something much more
like that than it's ever been possible
before
again the safety infrastructure could be
designed ahead of time stable goals
although it's possible that moral
objections would prevent this
um so even without whole brain emulation
systems even if it turns out that moral
objections or other factors do present
this there are some advantages to whole
brain emulation 'z some of these are
social so reduce coordination problems
possibly more tendency to take the long
view Digital intelligences are
potentially immortal maybe they're less
risk prone they will already have seen
radical technological change in their
lifetime may be more inclined to take
further risks seriously can he's the
technical challenge as well as social
factors subjectively slower computers a
user your caste likes to talk about what
he calls Moore's Law of Mad Science a
joke law whereby every 18 months the
minimum IQ necessary to destroy the
world drops by one point well I'll be
able to do it uploads would speed up as
the computers sped up so Moore's Law of
Mad Science would stop for them which
would have certain advantages in terms
of keeping track of the AI programs they
were experimenting with in principle
they could also do cognitive enhancement
all by messing with the brains although
once you start editing the brains can
become inhuman fast so if it is safe can
we make it come first this is the part
that I'm most uncertain about so three
major inputs to whole brain emulation
hardware brain imaging and computational
neuroscience you know how to interpret
those images hardware is hard to budge
brain imaging can be automated there's
some successful efforts in this
direction computational neuroscience
seems like the most likely bottleneck
and you could increase research funding
there but all three of these inputs also
speed up brain like AI and so if it
turns out that whole brain emulation is
a safer platform for a GI development
there's the question of when you
increase these two the arrows which one
does it speed up more by how much and is
it in fact safer another possibility to
consider is the idea of getting whole
brain emulation from a partially
controlled age
system AGI makes partial AGI makes whole
brain emulation makes more sophisticated
AGI talking more about that in the next
talk actually so we're left with the
question of what to do is whole brain
research work is it worth trying to
accelerate whole brain research as a
stepping stone toward AGI seems to me
it's at least worth investigating and
again here's the business-as-usual
situation here's the whole question is
will whole brain emulation make it
easier or harder to get to the corners
that we care about Thanks
right next up we have me again on yep
risk-averse preferences as an AGI safety
technique second shot at how to make it
safely through the singularity this is
also joint work with Carl mostly his um
so start off with the toys thought
experiment and then step back and talk
about where it came from
suppose that initially you've got a baby
AGI for the moment it's stuck in a box
and it's wondering whether to attempt
escape it estimates that if it attempts
escape it has an 80% shot successfully
taking over the universe 20% chance that
it'll get caught and shut down on the
other hand it could cooperate and
receive a reward so it's sitting here
weighing its options trying to decide
what to do and will additionally suppose
that this AGI is rational in Stephen 100
cents
it has a utility function its utility
function attaches greater value to the
whole universe then to the reward
because it could rearrange that universe
in lots of cool ways but it's estimating
a higher probability of the reward and
then of the universe and so it's going
to attempt escape fate since it's
rational according to this expected
utility notion of rational but Steve a
100 was just talking about it's going to
attempt escape if and only if the
utility of attempting escape probability
times utility plus other probability
times utility exceeds the utility of
cooperating so I'll be discussing the
game theory of this situation how
plausible it is
and what it tells us about how to
potentially work towards safe AGI
outcomes so is it plausible
why utility functions so I think Steve
Omohundro did a great job of that
already but just briefly because
optimizers are useful you could have a
micromanage ai and tell it each action
nicer to have an AI but can find
outcomes for itself
and because optimizing for a given
outcome seems to be a stable attractor
ah Co 100 2008 paper from a previous AGR
conference second question about
plausibility why alien goals who would
design an AGI that wants to escape and
take over the universe
right ah actually it looks fairly
plausible it looks plausible because
it's hard to design the right
optimization target and so scenarios
with a GIS with alien goals seem to be
worth considering there's a lot more to
say about this but it didn't fit in the
ten minutes but there has been analysis
I'm not randomly being advert amorphic
ah second question about plausibility is
anyone really this risk-averse forward
universe 80% chance really looks like
yes looks like risk-averse preferences
are common first off you have them so
Posner talks about asking teach pool um
how if certainty of happy life in the
u.s. like your present life maybe versus
10% chance of superhuman existence for
Google years which of these seems better
apparently most people choose the first
one even if you wouldn't choose the
first one I wouldn't uh you can make
that probability smaller instead of a
10% chance imagine like 1 in 10 to the
20 probably eventually you find that in
fact you are quite risk-averse that's
just us let's consider a different set
of AIS for simplicity we'll consider a
bunch of different goal systems of the
form utility of the universe is equal to
the number of copies of some particular
genome X up to n copies if n is small it
will prefer a certainty of a trillionth
of the universe to 10% shot up the whole
thing
because the trillionth is enough already
to get to n anniversary and his universe
size will prefer the 10% shot at the
universe which has trillion times the
value if n is much higher than the
universe though interesting things
happen so this is supposed to be a
picture of a physics experiment and a
wormhole and the idea is that if you
even a fairly small probability even a
very small probability to the universe
having strange physics that permits
access to vast amounts of resources 10
to the negative 20 and if those vast
amounts of resources are much more than
10 to than 20 times as large as our
resources let's say it can get two to
the ten to the hundred copies then that
could easily exceed the number of
resources that it can get by just going
with business as normal and so it
Gamble's all of its possibilities
Pascal's wager style on these weird
physics possibilities which it looks
like may well leave it fairly
risk-averse because your ability to
conduct physics experiments is not
linear in the amount of universe you
have access to so there are some
arguments that actually in some sense
most AI systems that you can design have
utility functions that are quite
diminishing and resources Alejandro's
resource drive use general but it's not
as its sublinear it's not as strong as
you might be anticipated so that's that
who cares why care of most a I systems
are risk averse basic reason gains from
trade you can imagine flipping a coin
for who gets the whole universe that
doesn't sound good maybe you lose better
if you could split it 50/50 for sure
then you get your certainty of happy
human life it'd be nice if we can get
that sort of gains from trade with a
eyes as well and neither we nor the AIS
have to take the gamble
if approaches like this can reduce risk
gives us first of all an extra roll of
the dice and extra safety measure if we
design unsafe AI systems second of all
it could be potentially used for the
bootstrapping process so AI is hard to
design because we don't have a eyes to
help us design it if you can use this
sort of thing to gamble with a eyes for
a narrow range maybe it can help though
it's perhaps not the sort of scheme you
would want to put test humanity's weight
to some limitations one human promise
keeping we approximated by saying that
the reward was guaranteed
in fact that won't be perfectly certain
of that either because humans could
intentionally renege on the deal and
shut it down as soon as we realize that
it has found a loophole or because human
society may be unstable we may be unable
to make these promises so in order so it
might attempt escape because the
probability of reward is insufficiently
high smaller promises however are more
trustworthy promises that can be kept
quickly and so there's the question even
though this whole spectrum is in
principle available to trade there's the
question of how much of it it's actually
accessible second limitation AI power
range so if we're much stronger than it
we don't have to worry there's the
middle gains from trade kinda region
where the probability of escape is
smaller than the probability of reward
but if it gets really powerful the
probability of escape is larger than the
probability we'll keep our deal
no trade possible extensions in addition
to thinking about risk neutrality you
might think about temporal discounting
human drug-addicts doesn't ever think
beyond the next lever press that gives
it heroin maybe it wasn't quite a human
drug addict in the picture um so it
doesn't ever plan escape you could try
to design an AI like that cares too much
about the next seconds reward to plant
escape which would take multiple seconds
it's a bit tricky because you try and
hardwire measure it's time to say a
clock hacks into the clock you build a
more robust notion of time it spends all
its time trying to invent a time machine
in order to get back into the hyperbolic
discounting curve little farther back a
second extension you could try is to
generalize the results that they work
from more than one AI thus it seems
worthy of investigating resource
satiable AI designs human norms and pre
commitments and ways to slowly turn off
the power although again not the scheme
you'd want to count on but given our
situation may be worth investigating
Thanks
okay so the currently most discussed
paradigm for artificial intelligence
motivational systems you'd cows keys
friendly AI and it's got an awful lot to
speak for itself with for instance the
current the concern itself is very
important I mean we've got to be
concerned that machines will become more
powerful than us and be able to take
over he focuses on structure rather than
the contentious details of exactly what
morality is he recommends a cleanly
causal hierarchical goal structure he
wants to make sure that the goal of the
moment doesn't wipe out our long-term
goals and of course he has a single
top-level goal of friendliness and
according to you at Kowski this is
really a no-brainer of a choice you know
basically all that the AI programmer
needs to decide is whether or not his AI
is going to be friendly or not and
everything should proceed magically from
there unfortunately fully defining
friendliness is insoluble without an AI
and that AI must determine exactly what
its goal is and there's this total
reliance upon an assumed total error
correction and here are some quotes from
the you know paper you know basically
the structurally friendly goal system
will overcome errors you know regardless
of how bad they are or where they come
from
his suggestion is is that there will be
an initial dynamic that basically is
what a massively evolved and improved
humanity would desire and that if we
could start with this the AI would be
able to figure out what friendliness is
from there I'm not really quite sure how
that's different from friendliness
itself obviously a massively improved
humanity
would want to be friendly or at least I
would think so here's where he really
stopped me
one of his favors basically he even
admits that his suggestion could go
horribly wrong which makes me wonder why
this is the most discussed paradigm that
there is so fundamentally there is a
root of all evil and it sort of is money
basically any attainable top-level goal
that you're willing to sacrifice
anything else for is going to be the
source of all evil or of course if
there's an overriding top-level goal
that you can change that's really deadly
to and unfortunately friendly AI
basically is built on this whole concept
there's this top-level goal that the AGI
is going to be able to modify while it's
figuring out exactly what friendliness
is and fundamentally every instance of
history that we have indicates that
allowing a changeable top-level goal is
a bad idea you know you start with the
not in eurisko eurisko you know
immediately start developing rules that
cheated the system you know basically a
wire headed you know human beings were
susceptible to all sorts of things
fundamentally we don't want to get into
this situation so here's the thought
experiment how would a super
intelligence behave if it knew it had a
goal but it didn't know what that goal
was I mean so one way to determine how
an AI is likely to behave
everyone's crediting Steve back there
again and basically I'll list his six
drives that he basically did a recap of
in his stock fundamentally he says that
the AI is going to seek instrumental
goals you know basically those sub goals
that benefit any other goal now it's
kind of important to notice that while
preserving your utility is what you want
to do if you truly want to preserve your
goal it's not something that humans do
well we change our goal from time to
time to time and it's sort of a
problematical it's your mental goal
he also sort of took a Miss I feel
basically he refers to a is behaving
like humans sociopaths and fundamentally
this is not beneficial at all I mean if
you behave like a sociopath you know
everyone's not going to want to help you
they're gonna get in your way wherever
possible they're going to sabotage you
wherever possible
you know fundamentally cooperation is
also an instrumental goal I've heard
debates about whether or not huge power
differences make a difference in my
paper I discussed this more but power
differences are not as relevant as they
might appear and it's also worthwhile to
note that nowhere in Omaha Norris drives
is the drive to achieve power for the
sake of power it's much more about
efficiency so if you take a systemic
worldview basically any working across
purposes is inefficient it's not
something that the world is a whole
wants to do rationally you know and also
ending that improvement that there is
the world as a whole will want to you
know reward things that work well now if
we go to the social psychologists you
know they're very aware of the fact too
that moral issues tend to look very
contentious as well so rather than going
after content they actually point out
that what we should be doing is first
defining the function of moral systems
and it's pretty clear here and this is
really what we want our Agee eyes to do
we want them to cooperate with us we
don't necessarily need a particular goal
we just need cooperation we want them
not destroying us here are all the
various choices that we have obviously
we want to go for the first ones but
what anyone has to remember is basically
there are strategies that everyone
except the dominant entity is pretty
much forced to follow and realistically
making the world a better place is an
instrumental goal as well you know
people are given advantages when they do
good things for the world
you know karma actually works it's not
foolproof but it's a pretty
force and of course this is why humans
have pretty much evolved a moral sense
so instead I recommend something that's
called rational universal benevolence
basically once something is
self-improving it can recognize it has
goals and motivations and it can start
iterating on improving itself basically
it's crossed the line you know it's a
self it's worthy of moral consideration
and there all sorts of reasons for this
and not the least of which is the fact
that this is the point which you can
also start fighting back Friendly AI
unfortunately says that the IAI itself
is not worthy of moral consideration
which i think is extremely problematical
it basically divides the universe into
human and naught which can also run into
a problem if trans humans suddenly
stopped counting as human clarifications
since I hear problems at times you know
benevolence just means you wish people
well you know it doesn't mean roll over
and play dead you know think of how you
might treat a child who's misbehaving
you know punishment makes sense some of
the time
basically all benevolence is is deciding
that you want to maximize the
fulfillment of as many goals as possible
in terms of number and in terms of the
diversity of the seekers and the goals
now that sounds like a tough goal and I
also hear everyone go oh but you know
what if someone decides they want to you
know achieve all the goals of killing
people well when you do that you're
stepping on tremendously more goals then
that effect is negative four easy rules
basically all you need to look at is if
you minimize interference if you
minimize conflict and you start
furthering all the instrumental goals
all the drives for other people
basically that's what human morality is
you know it's all the rules of how not
to step on each other how to move things
forward you know there are no terminal
goals of humanity the collective
extrapolated volition of all of us
is probably only instrumental goals so
basically you know we want to achieve
maximal goals we do this by cooperation
it avoids the over optimizations and the
nightmares of that sort that we hear
about four goals and utility functions
it avoids some desirable endgame
strategies and the prisoner's dilemma if
you know the game is going to end in two
turns you behave very differently than
the way you behave if you know it's
going to go on for a long time
it basically says you shouldn't do
things that are going to step on all
your future goals you know if you can
attain your terminal goal you're gonna
do anything if you've got five goals
beyond that you're not going to piss off
your neighbors one more minute
so a final summary just a friendly AI
versus rational Universal benevolence he
focuses on structure I think it should
you know focus on what the function of
morality is there's an unknown
modifiable top-level goal here which is
possibly the root of all evil as opposed
to a very explicit goal of promoting the
you know the goals the instrumental
goals it's entirely ungrounded on one
side there's a pretty solid foundation
on how you should behave on the other
side this is entirely humanity centric
where this probably is considerate of
everyone and a lot of people have
complained that Friendly AI seems
dangerous and immoral and you're
actually anticipating that the AI will
be alien whereas on the other hand it
would be nice if it were in harmony with
our current thoughts goals and dreams
I really commend the previous speakers
because I wish I could I could spend so
much time thinking about all these
things in such philosophical terms but
I'm you know maybe it's because I'm an
engineering training I was trying to
bring it down into tangibles of
engineering terms how would you build
systems like what does it mean to be
friendly or cooperating now at times I
have problems with these things these
concepts so I'm actually going to talk
about my view of the risks and this is
something I'm actually genuinely
concerned about if we are to pursue AGI
based on concepts related to
reinforcement learning which I firmly
believe is a framework not saying the
correct but a correct framework for
putting us on the path of building a GI
systems before that I want to spend 30
seconds as opportunity for for call to
action as an AGI community listening to
talks here and going over the papers I
think you may be missing a point in the
sense that if we want to have more
impact and differentiate ourselves from
the AI community we need to focus on
problems that are large scale we need to
focus on scalability on challenges or
really high impact and when we write
papers they should clearly I think
indicate how it puts us on the path to
AJ and scalability I think is is really
a key attribute here and coming up with
solutions that scale and not just work
on 10 by 10 mazes or whatever other
little you know toy examples were
working on that would be I think that
would be good anyway
that's hopefully no more than 30 seconds
okay so I do work a lot of work on
reinforcement learning on another area
called deep machine learning and on the
intersection of these two concepts as I
firmly believe like I said those pave a
way towards AGI and I can talk about
that offline a lot there's plenty of
evidence that the mammal brain performs
at some level or another it's it's
decision making under uncertainty is
governed by reinforcement learning and
at the gist the corporate reinforcement
learning is the the credit assignment
problem if I'm at a certain point in
time and I'm I'm given two possible
actions I could take I want to take the
action it maximizes not just my
short-term return my short-term benefit
or expected reward but also balance it
with with longer-term implications
at the core of intelligence and it's
something that reinforcement learning
uniquely is a machine learning
discipline uniquely tries to solve so
let's assume that's the case okay I wish
I could think of another way to build a
GI systems that would somehow be more
safe friendly and effective but but you
know inspired by nature and the only way
I personally can think of things is
actually in a way that adheres to this
framework reinforcement learning
reinforcement learning is based on the
notion of rewards as you've heard a lot
there's a discussion here at the panel
so I'm actually I'm happy that took
place because I don't need to repeat the
whole story here but in essentially
you're never maximizing in real life
that you know the reward is not a scalar
you know it's it's a vector if you want
to imagine it that way reward implies
not just external rewards am i hungry or
not and my colder my warm but it also
involves intrinsic cognitive processes
that inject in you know internal rewards
that drive our our learning and
behavioral processes for example and
uragan talks a lot about that we do work
on related topics curiosity boredom when
I'm bored in something I tend to take
actions to try to explore other you know
areas of the space state space or if
unfolding of events occurs according to
expectations is growing evidence that
that injects these internal rewards that
were very much very much driving our
actions so I just want to clarify the
notion of rewards not being this the
scalar you know whether I got to the
cheese or not it's it's way beyond that
but inevitably if you subscribe to that
view I you know agents try to maximize
the reward prospect their expected
reward in one form or another you know
setting aside the mathematics behind
that that's kind of the intuition and
the question is if that's true and I
personally think that as a system
becomes more intelligent the internal
rewards dominate the the incentive
processes and not the external rewards
but if it's true how can we avoid the
inevitable sort of adversarial eiji
either the Terminator AGI scenario you
know I mean if that's that's what it
comes down to and I'm actually I don't
have a solution to that I've been
spending a lot of time had many
discussions and there is a sort of
trivial solution to say well you're
going to build this system where you're
going to limit its resources you know a
dog or a four-year-old child will never
be
Einstein okay because of certain
limitations of objective limitations of
resources such as storage element
computational capability and that's
probably true and that might hold I
don't I also don't believe that a system
like in the movies can be turn
intelligent super intelligent within an
hour that doesn't happen those of you
that don't work in this area I know that
it's unrealistic so I'm not so concerned
about that but it does open you know a
loophole that somebody if the concepts
are made you know if AGI could be built
by anyone some honey is going to abuse
it and build systems that are not as
limited because they will think that has
some benefit and of course again you go
back to the Terminator scenario so I
don't I think it's a short-term solution
I don't have a long-term solution um I
you know for various reasons longer
discussion right I don't think
engineering the rewards in a human
beneficial way is somehow a long-term
solution I also think that's temporal
particularly in the nut shells because
if the system becomes truly intelligent
to a human level maybe even beyond it's
going to be able to bypass that okay
there's no way you can definitely not
through external means you can that I
can think of you can actually you know
impose constraint the system to always
be friendly and never sort of try to be
adversarial or harmful to us in any way
so the big question is you know first of
all if you're not limiting resources and
you you just unbound this those
surpassing human level intelligence
means inevitably that the end of
humanity humanity is near and maybe
we're not the end of the evolution
process and maybe evolution should
continue on different fabrics than
biochemistry and that could be it you
know get this little selfish view that
you know maintaining humanity and it's
its freedom is the ultimate goal of the
universe but it may not be and you know
related to that if that's not sure are
there are alternative frameworks to that
of a reinforcement learning based agents
that would put us on the path of AGI and
somehow promise the that everything
skills you can have efficiency
intelligence everything that we
attribute to human level intelligence
systems yet not there somehow avoid the
the adversarial scenario and that's
about it in about nine minutes so thank
three minutes I'll take any questions
how's that okay yeah you started late
but that's good rewards okay so I'm not
sure what the question was but uh but
you know there's a difference between
goal and rewards right so the goal if
you want the objective of reinforcement
learning agents to maximize actually the
utility of what's called the value
function in the reinforcement learning
community which is actually not the
reward the instantaneous reward but some
reward prospect you know some some
horizon of rewards in some form or
another and you know that's what drives
its action selection process so if you I
mean if you stray away from that then
you're not really talking about the
standard reinforcement learning and
again if there's a way around that that
would still yield intelligent machines
I'm interested here but if not then I
think you're back to a realistic problem
that as a system becomes super
intelligent enough it might be able to
say you know if I didn't have to worry
about being too cold or too warm and I'd
have the system that maintains that the
optimal sort of temperature that would
change my actions because I wouldn't be
concerned about being overdressed or
underdressed etc and so the simple
analogy but I think it's it goes to
emphasize what that would might lead to
I think so another minute for another
question all right
Thanks thank you okay
this presentation was actually intended
to be given by one of my PhD students
but unfortunately he didn't get a visa
entering the United States
and therefore I had to jump in here now
I want to talk about knowledge massive
massive availability of knowledge so
here are some ideas of this presentation
so potential next March my son of HDI
could be based on massively knowledge
intensive systems from heterogeneous
knowledge bases in a certain sense that
is already in a certain sense the case
you collect knowledge from various
sources and usually the World Wide Web
is a good possibility to find these
things
an intelligent combination of
ontological knowledge for example of
knowledge about general rules advanced
reasoning techniques learned and
acquired knowledge and so on may be a
possibility to create something that
becomes relatively smart actually and
can draw inferences that are highly
non-trivial test cases can be systems
that integrate knowledge from rather
different sources and draw a non-trivial
conclusions and I think that based on
such things ethical questions can be
drawn and the major focus here in this
presentation will be that not only
knowledge that is somehow available in
the internet and knowledge bases they
database and so on should play a role
but also for example now this tremendous
math and concerning visual information
streams videos pictures and so on that
are up to now not really knowledge they
are just videos they are more or less
unlabeled pictures but computer vision
techniques allow already today but in
the few
more intensively to draw very very very
a lot of knowledge from these from these
from these visual information here's a
slide about knowledge sources it just
mentioned that whereas classical media
like newspapers magazines radio
television and and so on decreases
significantly during the last years and
a concerning usage
new media like laptops and mobile phones
and mobile devices increases
dramatically and that it has in
principle possible to integrate enormous
amounts of knowledge is in a certain
sense already proven by something like
the what IBM Watson machine that one
against the best human jeopardy players
or there are other projects that attempt
to make the but smarter by introducing
to the world wide web semantic content
like like in the large project with the
large knowledge Collider which is a
European funded collaborative project
that enriches logic based Semantic Web
reasoning methods and employs
cognitively inspired approaches and
techniques and builds on distributed
reasoning platforms now what about the
integration of these types of data so
for a next milestone of HCI it could be
possible to think about classical web
based resources semantically enriched
for example and classical ontological
background information that may be given
in form of the main ontology and these
things but an important point maybe in
the future that multimodal information
in particular given as visual
information will play a more important
role this may be and the training of
systems in order to label what is
depicted in on pictures or what is
depicted in video streams this payme
maybe somehow guided by the community
that's true straight training such a
flexible system in order to
yeah extract knowledge from this
information that is good and this may
lead to new types of information clearly
in such an HDI system you have different
modules and you need different types to
represent knowledge so for example you
need some device probably to code
conceptual knowledge for example in
forms of description logics and sub low
base treasonous or you have knowledge
about general rules that maybe perhaps a
kind of a first order logic approach
plus a classical theorem prover you may
have knowledge about visual facts this
may be may be based on certain labeling
techniques automatically or
semi-automatically learning certain
classes and certain and enabling certain
information depicted on pictures then a
lot of things that play a role in
concerning video streams and pictures as
for example contextual information this
is something that may be also somehow
guided and somehow supported by the
community that is producing such types
of information why are there mobile
devices for example or certain backbite
webcams and so on and so on
now what can be gained by doing
something like this here's a simple
example and this is a completely trivial
example and it's an example that is
already more or less possible now
imagine a webcam that is placed to
observe a university campus for example
ok so you see for example one particular
side of a building and a parking lot
something like this and then you can for
example scan where the office windows
are open whether you are currently
working in your office etcetera and what
how many cars are somehow located on the
parking lots and and so on and so now
the system maybe have possibilities to
consult some other resources so for
example together with intelligent
integration with other sources of
information such a system can draw in
intelligent conclusions from so for
example it could ask you to close the
window if you leave your office and it
consults the weather forecasts the
online battle forecasts and
knows that the thunderstorm is arriving
and in order to prevent that your office
will be flooded it just asks you well
please close your window another
possibility would be for example if it's
scanning the the parking lot and it's
learned how your car looks like and
there's a another car that is towing
your car away then it may be inform you
well okay your car is somehow removed
you probably place it on the wrong wrong
area on the parking area or you could
the such a system could for example know
how many hours you are working in your
office and how many hours you are
somehow not in your office things like
that I think these such things are
completely unproblematic and and and the
real support system now such a system
could perhaps do some other things if it
has if there are several webcams that
are for example scanning somehow the
environment so for example such a system
by integrating knowledge could
potentially detect which positions you
meet and how often and infer what types
of illnesses you have for example so
that's a non trivial trivial inference
but only such a system has the
possibility to do that in a certain
sense because it just recognized when
you arrived it realized that you go in
it realized that you are doing something
with the doctor and goes out for example
it could be able to build a profile when
how often how long etcetera you are in a
bar and you're having some beers for
example I don't know whether something
like this is really something that
should be somehow public knowledge and
it should it could be able for example
to know when and where you meet this
woman again for example which is also
something that could is a very private
thing probably
and if HCI system can draw inferences
from things like that and inform your
your wife for example then this is
probably not something that is
completely unproblematic so I think that
such such a knowledge intensive system
that has such abilities post some
ethical questions and some is not
completely ethically unproblematic here
are some of these ethical questions so
if you couple of classical types of
knowledge that are freely available in
the web for example or in certain
communities in the web if you can cover
this with new types of knowledge so for
example from video streams moving from
mobile devices from webcams you can put
everywhere and so on then some ethical
questions can arise so for example to
which extent should it be allowed to
compute profiles of people without an
illegal basis in this sense we have just
well an open community and there's no
legal basis to collect such information
which types of information can freely be
connected and covered and which types of
information should be kept isolated okay
from each other so what what
possibilities to connect these different
knowledge bases should be allowed when
doesn't hei become an uncontrolled
autonomous control system so for example
if you really think now that this is not
only a knowledge collecting device but
it's also proactive can disseminate for
example what what what it knows in such
an AGI system then it's somehow an
autonomous system that has a relatively
strong control about many things but
it's itself somehow not controlled who
should be allowed to few consequences
such an HDI system can draw so in
principle first of all it can draw every
consequence from the sources that of
information it can gain but at a certain
point probably the question is are there
some limitations that should be somehow
involved okay there are some ethical
questions that are I think very
naturally fall falling from this such a
relatively such an HDI system that is
already more
possible and yeah we can postpone
discussion to the later time point thank
you very much all right
I'm incredibly honored to be permitted
to speak to you today because I'm kind
of the odd man out here I'm not an
academic I'm not a researcher I'm a web
developer compared to you guys my
company we play in a sandbox we watch
you building skyscrapers and I just
noticed you had a really cool truck I
had to come check out so that's why I'm
here but the framework that we work with
for building web applications is an open
source framework and I've seen a lot of
parallels between the framework that we
work with which is Drupal and what's
being done with OpenCog and and even
architectural II with other systems in
the AGI frameworks and so I want to talk
about some of those things and
maximizing the power of open source for
AGI my hope is that every one of you
who's working on a software project
would consider open sourcing your work
so that hackers like me can throw in a
weekend here and there and help out and
and I hope to give you some tips that if
you do that that will help you maximize
the opportunities that that affords so
as I said we work with Drupal I've made
some very small core contributions of
Drupal I'm also a member of the drupal
security team drupal is what we call a
content management framework
out-of-the-box content management system
that you can do simple content websites
without writing a single line of code
you could build systems like any of
these that I've listed here mostly
without writing code click together
websites but it also has a robust
framework to it so that developers can
do anything else you can imagine
with the Drupal system for bringing
communities together online to interact
with each other to share content to
collaboratively work on developing
content so here's a bunch of
organizations that use Drupal of course
you know Mozilla Foundation in the White
House but Metallica and the Playboy
bunny there's for Playboy de the German
web site which only go there for the
articles and for the web framework Soho
so you know the question is how can we
make AGI that sexy and get people
interested in working on
IGI frameworks as a hobby I have a
two-minute video from Drees boy tart the
founder of Drupal I'm gonna show that
the end if I have time so why should you
use open source well I think the
audience is ready I mean for one people
who grew up when the Terminator movies
came out are now approaching their 30s
and they think this stuff is cool you
know
you know they've finished University
they've hopefully got stable income so
they can do crazy hobbies that might be
related to your fields that you're
working in there are a hundred and
thirty four thousand and more free apps
in the Android Market these are things
that aren't necessarily monetized that
that are available that's as of April
when the number of free apps in the
Android Market surpassed the App Store
there are 14,000 plus Lego Mindstorm
projects on the Lego Mindstorm community
sites submitted by users just to put out
there and share these are people you
know building with this toy robot right
well look good is that to you guys
working on a higher level AGI well I'll
tell you I did the little tutorial
yesterday the light of tutorial
everything we did in there if you want
to do embody that you could do it on an
NXT robot it's got five sensors and
three actuators you know prove to me how
many more than that you need if you
think there's some specific number you
know there's been a lot of talk of
virtual worlds minecraft has three
thousand user submitted mods people made
it as a hobby in their spare time these
mods for this virtual world they share
on the forum and give to other people to
fruit for free and other people hack on
them and release better versions and
they share them back and forth so I
think there's an audience out there of
people who are interested in all these
things around the peripherals of what
are needed for AGI both you know
robotics and image sensing and language
processing and virtual worlds that that
could be useful to you guys so you know
how would you like to harness you know
0.1 percent of those 134,000 mobile app
developers who are already doing stuff
for you maybe they donate a weekend to
your project I also think open source is
important because it's safer I added
this slide when I saw that pretty much
everybody else in this section is
talking about safety of AGI I think the
best defense against unfriendly or
unsafe AGI is safe AGI and you know what
we need to avoid ultimately isn't
necessarily just unsafe AGI itself but
unsafe AGI with no defenses against
right some people like Hugo de garis say
we're going to get unfriendly anyway so
we need to have the friendly AGI on our
side remember
in Terminator 1 the robots were the bad
guys in Terminator 2 he was the good guy
so you can get a little shorts and a
girl on your side right so you know the
worst case scenario if HDI's developed a
closed source model we've got very few
people who are going to have the
capabilities to fight back against that
AGI in an open source scenario yeah you
might make it a little bit easier for
someone to create an unsafe AGI but
we're gonna have a lot of people with
the capabilities to defend against that
to build a GI that can combat that so is
unfriendly add a GI actually less likely
to occur in a closed model well I think
the burden of proof is on you if you say
that it is another important part of
using open sources it's training the the
next generation you know the the people
who carry the ball forward may not be
the students in your university how many
people go through university program and
then change careers completely so the
people you're investing in that setting
may not be around to carry the ball in a
Drupal community the project lead for
the most recent release Drupal 7 it was
the largest release of Drupal yet was a
girl goo who goes by the name web chick
online and she started out in Drupal
community just hanging out in the forums
building a click together web site did a
little bit of coding learned how to
create a Drupal module for a simple quiz
function and blossomed into an
incredible web developer who ended up
taking the lead for an entire relief
cycle which is a very major project for
Drupal you know for myself I actually
started out working for a non-profit not
as a web developer but they happen to
need a new website and I discovered
rupal it would meet their needs and from
that I actually left my job there they
became my first client I started
freelancing in Drupal and through a
similar process have gotten more and
more involved without any you know
formal training or connection to the to
the origins of the Drupal project so
given that if you can see the value in
open sourcing your project how do you
make all these hobbyist contributors its
crowdsourcing capabilities how you make
them able to create meaningful
contributions to your project first I
think there's an important part of
branding and messaging you know don't
just call your project AGI soft don't
make it about the software you've got to
continue to emphasize that you're
building a community of people with
common interests Drupal got its name
from the Dutch word for village
you know emphasizing the image of
community
Andrey's Boyd hurt the founder went to
create his first site where he's going
to deploy Drupal he intended to register
the the Dutch word for villagers just
dorp
and he must typed it as drop the ROP do
RP and of course the Dutch word that's
the English word dropped the Dutch word
is Drupal that's how Drupal got its name
so the idea of community is built into
the the core of Drupal and it's a
message that's repeated over and over as
you read the documentation that Drupal
is an amazing software project but it's
an amazing software project because it's
an amazing community of people who have
common interests in making the web more
accessible to everyone and another way
you know to foster community is through
unconferences barcamps buffs or
something they've been really important
in the Drupal community even we have
formal get-togethers birds of a feather
session and a sidetrack along from the
formal presentations you have a bunch of
rooms that are available or anybody can
write up on a whiteboard we're going to
do a talk about this they're very
informal and and now when I go to Drupal
events those are the things I go to I
skip the main sessions because that's
where the the real action is so you need
to reduce the barriers to entry if you
want people to be active and productive
in your open-source project so you want
to have really high quality examples one
of the Drupal modules really the only
Drupal module that is not a part of the
Drupal core when you download Drupal
that is developed alongside of core by
the same people is a module called
examples and it's just full of examples
of how to implement the hooks and the
the tools that ripple provides to
developers you need to reach for the
technological lowest common denominator
that can accomplish your goal so an AGI
that might mean using something like
Python instead of Haskell or something
more obscure in in Drupal it is
surprisingly to many people it's more of
a historically been more of a procedural
PHP system as opposed to object oriented
that's not just as a goal for lowest
common denominator it's in part because
PHP for didn't have very good object
orientation support and PHP 5 adoption
was very slow among hosting providers
but it had an interesting side effect in
that I observed that people the computer
science background of course object
oriented makes perfect sense to them
they want to do object oriented but
people coming from a design background
the people who are doing HTML and CSS
they get procedural much more much
easier and so that was kind of a side
effect of other factors that
them to the procedural PHP which now
looking back some people say well we
should move everything to Opie's fast as
we can now that PHP supports it better
but maybe there are reasons not to
because it allows a broader base of
people to contribute who have lesser
skill sets in that area you have to
support all the tools so your source
code management system
Drupal has done a great job originally
with CVS now migrating to get and that
process of migrating to get was a huge
community effort of making sure the
documentation was available and making
sure that people understood how they
could make their contributions available
to the community and so along with that
along with everything else you got to be
available so you know the OpenCog
project has an IRC room that's that's
fairly active and being available for
real-time communication I think is
really important and as project leaders
those are your heading up software
projects I think it's important to
recognize that your most important
people are your best mentors they may
not be your best coders because your
best mentors are going to be bringing up
your next coders all right
then and this might be the most
important point so I'll make this and
then I'm done you want you've got to
make it easy for people to find and
share contribution so it's one thing to
put your code out there for anybody to
look at but you've got to make it easy
for them whatever work they do to give
it back so that other people can find
their work and get it back and get that
self reinforcing system so of course
that requires being modular and thinking
like a framework even if you're not
exactly a framework it's important have
very high standards for your core but
then for your contributions you have
very low standards to open up a
repository set up a network on github or
set up your own you know file a
repository where anybody who wants to or
anybody who meets very basic minimums
can upload stuff in there so it's easy
to get to people know where to find it
you know that one centralized
authoritative repository so no one gets
confused about how to find help so
that's what I've got thank you very much
all right so mine is the last talk in
this series and then we'll move to the
the panel discussion quickly after that
following in the in the vein of the last
talk I'm going to talk about open source
AGI my theme is not just to advocate
open source that's already been done
very well but to deal with the question
well isn't building open source AGI a
kind of recipe for disaster which is
very often posed to me in the context of
OpenCog people say well of course if you
open source your AGI code want some
psychopaths basically fork your code
easy to make an evil closed source fork
of your open source project that will
then blow everybody up or do something
else nasty and I mean I think this is a
this is a it's a more deep and subtle
question that most people recognize I
mean I I don't think we don't aim to
take a sort of Alfred E Newman attitude
above what name worry and just do it and
not think about it at all but I think
that there are many pluses and minuses
to the open open source approach as
opposed to the closed source approach I
mean yeah putting something out there
for everyone is scary on the other hand
elite groups who believe they know best
for the whole human race and want to
save everyone in secrecy according to
their ideas have also caused a lot of
damage and in the history of the human
race and of course each of you may think
yes but my own elite group of me and my
friends would never cause any harm
alright but then I mean that sincerely
of thinking has caused a lot of problems
as well so I don't think open or closed
is particularly a panacea and then
another high-level point of course is
how closed is closed really given that
corporate espionage is everywhere and
everything is online I mean in principle
you could develop an edgy art project in
a sealed chamber inside someone's
basement and keep all the prisoners
there under lock and key with guns to
their heads just like prisoners I meant
programmers and
you you yeah you could do that but in
reality that's not really the way it's
likely to happen like if if a big
company develops an AGI is a closed
project someone else will hire away some
program or someone will quit and start
their own project with the same ideas
they can go to a country that doesn't
care about IP laws I mean it's it the
alternatives aren't just open-source
versus locking the vault in the basement
it's more a matter of how quickly the
information gets diffused where it gets
diffused to and what rate and so forth
so Joel Pettit was one of the lead
programmers on OpenCog in the head of
our project in Hong Kong he and I are in
the midst of writing a paper kind of
going through strategies you could take
assuming you are doing open source AGI
what what can you do to sort of make
sure to not to make sure but to buy to
bias the odds in your favor to bias the
odds in favor of a positive ethical
outcome and the our paper gives
argumentation underlying each of these
now I'm just going to roll through each
of these nine points with a brief
commentary so the first is kind of
obvious you want to engineer into the
system the capability to acquire all
sorts of different ethical knowledge and
we kind of I guess we phrase that as we
did because we don't think that much of
like programming and ethical axioms or
theories into the system we think it's
it's probably more valuable to teach the
system to acquire judgment empathy have
habits of of ethical behavior through
its interaction in the world and you
want to provide ethical interaction and
instruction to the system and you want
to build your system in a way that
that's easy to do so that whoever Forks
the code or downloads the code develops
it and uses it it's it's easy for them
to teach the system in an ethical way I
mean in there in the context of building
virtual worlds or robots or something
that means when you build the world that
the AGI is naturally going to play and
that most people will use it in you want
to be sure there are avenues for ethical
instruction in there just as preschool
teachers think about teaching ethical
behavior to young children alongside
cognitive linguistic skills and and so
forth
this third man is more of a cognitive
architecture point which as I mentioned
in an earlier discussion is a place
where OpenCog diverges intentionally
from human cognitive architecture and we
we're trying to build OpenCog with his
fairly stable hierarchical goal system
where you can supply it with top-level
goals it will learn sub goals from these
and it will systematically try to pursue
its goals now there's no absolute
guarantee of it doing this this is a
real-world system interacting with an
uncertain world and with limited
resources but it's still different from
how the how the human brain works where
the there is no stable hierarchical goal
system and no kind of probabilistically
trying to achieve your goals that's
that's something some of us acquire
culturally but it's not wired into the
human brain in the way that you can do
into into an AI system getting to self
programming as we discussed in the
workshop this morning I mean I think the
risk of kind of runaway self-improving
self reprogramming AGI is kind of
overblown and the other thing that's a
likely thing to happen with a gr systems
in the real world on the other hand I do
think it would be a bad and dangerous
thing if if it were to happen I think
the self programming self modification
at the deep levels we want it to occur
fairly slowly and with humans in the
loop and in the first phases so I think
that's that is a feature of following
the path of human cognitive development
and I think it is a feature not a bug
either I don't necessarily think we
should pursue an approach to AGI where
the system is radically revising itself
all the time right from the beginning
though that's an intrinsically
unpredictable so a more conceptual or
controversial point a number of thinkers
including Francis Halligan at free
university of brussels have written
about what they call the global brain
the emerging collective intelligence of
all of us connected to the internet and
our mobile phones and google and narrow
AI programs and eventually a GI program
to the extent that early-stage AG eyes
are linked into this whole emerging
global information network they're going
to be tied in with us kind of growing up
together with human beings which is sort
of very cool as well it's first to
active on things the alternative being
the idea of an AGI in a box locked in
the basement somewhere evolving in its
own direction which which is another
risk of the isolate approach to AGI
because by and large if if someone's
developing a system of the Box in their
basement for security purposes probably
that system is not engaging with the
whole network of humanity in a rich way
as it as it grows up there's a lot of
different perspectives on AGI and on the
future of AGI and the human race and one
thing I think the futurist community has
not been good at so far is really deeply
communicating with people with divergent
points of view because it's it's so much
easier and more fun to communicate with
people who have about the same point of
view as you do and if somehow as we went
forward there are a way to really engage
in deep interactions with humans with a
huge variety of views and all these
issues we're talking about I think that
would certainly increase the odds of
things going forward positively I mean I
I tried this on my trip to Mongolia
recently to talk about friendly AI would
these people who could barely speak
English living in a yurt in the middle
of the desert and then it didn't make
much progress but it's I mean it's a
it's a difficult thing but it may be
important that as we move forward
getting to a point that I Steve I'm a
hundred likes to talk about creating a
mutually supportive community of a GIS
again this is not a panacea but it seems
based on what I understand about human
intelligence and about game theory and
social interaction if for starters you
had a bunch of somewhat different AG
eyes watching each other and interacting
with each other
that would seem to ride some uncertain
but interesting level of protection
against some of them going nuts and if
you have a bunch of Forks of your open
source AGI all doing stuff in the world
then when someone introduces a new one
there's other AG eyes to watch it and
interact with it which is better than
just having humans doing so
there is a lot of interesting
presentations given today about AGI
ethics and I think it's valuable to be
thinking about it now I also think most
of our ideas about AGI ethics now are
going to seem idiotic 10 or 20 years
from now once we have more experience
building AGI systems so I think that as
in many other areas the theory and the
practice they're going to have to
advance together so we'll have to
advance our ethical understanding as we
understand what these AGI systems can
actually do rather than what we
speculate them to do and perhaps my most
controversial point is say for the last
after a bunch of thinking we came to the
conclusion probably it's safer to
develop AGI sooner rather than later
and in the next 45 seconds I can't
justify this too deeply but but my basic
line of thinking is there's other
technologies developing too and think
about molecular nanotechnology as one
example now if you had a whole economic
infrastructure with advanced synthetic
biology molecular nanotechnology for any
any one of us could just synthesize any
configuration of matter by typing
something into the internet if you
brought an AGI that was powerful into
the world in that circumstance if it was
bad he could wreak a lot of havoc really
quickly whereas if he brought a very
powerful AGI into the world now if a
wand to wreak havoc you know to build
get new hardware for itself we're
supposed to order it online that's to
get someone to build it get a truck to
drive it there I mean then the process
is much slower and there's a lot of
humans in the loop I mean of course if
it was a superhuman god mine they could
circumvent all that but in a lot of
realistic scenarios the more advanced
the rest of the technological
infrastructure is the more rapid it
would be for an AG either went awry to
do a lot of damage so you come to the
conclusion that if AGI developed sooner
relative to other advanced technologies
the risk of a really common catastrophic
damage coming out of it might actually
be lower and that this assumes a
somewhat measured advancement of AGI it
assumes you're not in the scenario
were like you know this minute we have
the OpenCog of today in next minute we
have we have a god mind but so that
we're writing a paper on this I'd hope
to have it ready for this conference but
didn't quite happen but we'll post it
online within the next month or so and
just to recap the theme that's these are
things we think you could do to bias
open source AGI toward friendliness I
don't think there's any guarantee with
an open source or a closed source or any
approach basically and I think that the
merits and demerits of the open source
approach will become clear as things
develop along with a lot of other things
so we can that's the last talk in this
first fraction of the future of AGI
sessions so we're not scheduled for a
half-hour of panel discussion then a
break and more future of a gif
afterwards so all the all the speakers
who just talked and should come sit up
sit up here in the row any questions
joso am I
hello-hello totally attached part of the
answer was contained in your question we
do treat animals well when we care about
them it's much more are short-sighted
intellectual side that sees you know the
immediate game that we might get from
treating them poorly ignoring them
taking advantage of them and it's very
much our hardcoded emotionally optimized
emotions that really say you know the
correct long-term thing to do that
you've evolved to know is that in the
long run you know treating the animals
well ends up being to your benefit far
more often than not you know it's it's
the same thing as the rainforests oh
look we can make tons of money by you
know tearing down the rainforests in the
long run we all pay a price for it there
are all sorts of examples where the
short-sighted clear advantage it turns
out to be really the long-term stupid
thing and a lot of the problem is is
that you know we have semi developed
intellects right now we're intelligent
to a certain degree you know the more
intelligent you are the more time
discounting is not in effect and you
know all of our problems in life are
because we take the short easy way out
you know we see the obvious killer you
know
application and we don't realize that
you know if you treat animals poorly
anyone who doesn't know you who sees you
treating animals poorly is never going
to trust you because they then know how
you treat those who are no less serpent
now it's very possible if there's you
know one and only one AGI they never
want to reproduce you know they're sure
that they'll never be in the submissive
position you know if they're willing to
take on the entire world and that
includes you know what's the probability
that there's other sentient life in the
universe you know do you want to take
that risk I mean I love the risk-averse
approach because you know most of these
you know short-term you know I can win
things are really stupid in the long
term and if you're risk averse hopefully
you're looking and seeing that one my
question
so the question was the major questions
are going to be posed by the threat of
military applications of Robotics how
are we going to control that the
question was actually traded and Ron are
confronted
does a lot of work on robot ethics in
warfare and basically the answer really
is is that the individual or machine
doing the killing needs to be under the
control of a machine or human being not
doing the killing I mean when we have
humans out in the field you know the
first thing they do is they send you to
boot camp they break you down they
reprogram you as a killer but they keep
you in this enforced hierarchical
control architecture whereby
theoretically you know you shouldn't be
able to break loose you should be
following the rules of warfare you know
it doesn't work all the time but that's
sort of the concept and that's you know
a lot of what he's proposing for the
military robots is that you know they
are limited in terms of how much they
can progress they're integrated into
this you know architecture they're not
allowed to actually you know both kill
and be in charge of their own ethics
well thanks for the reference to walkins
you want to add something it sure does
one point I might add in there is that
so novel systems are going to be
developed in a research context before
they're deployed and so and usually
there's a fair lag between the two so if
I was going to look at that I might
consider more military the expansion of
military robotics or something at the
margin creating more demand for machine
learning and AI work
yeah one brief comment on this is I mean
from having done a little bit of work in
that sector as I know mark and maybe
other sort of have as well I mean at a
premium in creating military systems is
that systems will behave according to
doctor and then they'll do what they're
told and this isn't always compatible
with creating systems with the maximum
level of creative intelligence so I I
would find it unlikely that military
robots are going to be the most
intelligent AGI systems at any point in
time just because flexible creative
intelligence is not demanded by that
application as much as by some others so
the answers may be that there by the
time you have the smart killer robots if
you do there will be other even smarter
AGI is out there that will change the
landscape a bit now since military
funding seems to be the main source of
AGI research especially in the u.s. does
anybody else everybody else feel the
same our share your optimism
you know today's weapons are extremely
crude and the whole killing people and
blowing things up we only do that
because we can't exert force in a more
humane way and so I think as technology
advances the whole nature of warfare is
going to change dramatically and we can
create an infrastructure which actually
obeys you know protocols and kind of
universal Constitution I think that's
something that what people would like so
human rights can be embedded into the
infrastructure in general and the whole
thing of killing people doesn't have to
be part of that the US military is
uniquely good it's not killing large
numbers of people but targeting exactly
who they want to kill so it's more
intelligence and precision rather than
lower carnage we do have a board of
utopians here maybe you would want to
expand on that well I want to share a
related thing which is I am very
skeptical ever
for sulfur if we try and control what
they're allowed to have and it can be
much safer and much more realistic if we
imagine we're in a setting where
everything that can be done is going to
be done by someone and so I don't know
every time we see that aloud I just
don't believe it and I think it just
makes me nervous so we should be you
know I like the open source idea because
everyone will be doing lots of different
things we have to have goals that we're
compatible with that and that lead
towards benevolence or safety Nisour
good outcomes get another question there
so this is a whole sequence of things
that are happening so what I was talking
about is mostly the very near term how
can we ensure the program you write
tomorrow doesn't run out of control and
so building in provable constraints on
it sort of is good for that I think what
you're talking about is as systems get
more developed we really want them to
want to behave in an ethical way we want
them to be friendly we want them to
create cooperative in bar environments
just like people do we love cooperating
most of the time and so you know from
that perspective that the system doesn't
want to break out of its own utility
function that's what it wants to do and
so if we you know choose that carefully
so that it is aligned with human values
then it wants to be aligned with our
values but I think that's a little
dangerous
yeah I mean it'd be nice if we could
really understand it it'd be great to
make the very first systems have those
the trouble is as we've heard there are
a lot of philosophical subtleties and so
I don't think we can rely on giving it
you know values that we think will lead
to the right outcome to ensure safety
initially and so that's why I've been
focusing on systems which are
mathematically provably limited in what
they can do and then eventually I think
that can be used to bootstrap the
systems which actually want to behave in
the way that we want the audience maybe
I take couple questions first before we
continue you were the next one
who wants to take this
let's take you tomorrow because I you
know I couldn't agree more I think the
the fact there are implications to our
actions that were limited and
repercussions are are the keys if you or
anyone in here would suddenly be
Superman and not vulnerable you can take
on the whole world that she or she
wanted to I think it would vastly
changed their behavior maybe not at
first if they've started out as a moral
human but but eventually that's really
the concern and sort of the lankan you
know system that's intelligent enough
would be dangerous in that sense now
having a bunch of those kind of fight
each other well then my question would
be what would you know where's humanity
in that equation maybe they'll just wipe
us all out and then deal with themselves
and that's you know if they are on an
equal platform eventually that's at some
level that's way beyond human level then
you know they may end up with the same
predicament but uh but the but I think
the concern you're raises the concern
that I have about super intelligence and
the and if inevitably or the conflict it
might arise from that inevitable
so this was basically not a simple
question but a bunch of topics which you
addressed do you remember the question
or should I summarize it or basically
you expressed lingering doubt that
evolution will be perceived as a good
thing yeah and then we that we create
could create a TI from scratch and you
expressed sympathy for the poor brain
emulation approach yeah and self
modification let's start maybe you want
to take this since it's in sure I was
just gonna say that well I'm sympathetic
to your point about evolution not being
very humane it's not at all clear that
whole brain emulation is a better
approach since if you have a partial
brain scan and you're trying to realize
it the more obvious ways to do this
involve creating numbers of people that
have never existed before each of which
lives for some seconds while you try and
see if that works and many of which are
in brain damaged states
I mean so it seems like any in any
situation we're searching over a large
number of parameters trying to improve
model of brains or new get that sort of
situation and the the numbers might be
small relative to the long-term
population but so there's there's a
possibility that if we had a version of
the animal rights movement or I'm the in
calling for extension of moral status at
such system that would restrict
advancement in some areas but it would
it would seem quite a leap to imagine
that everywhere stopping research into
such a valuable technology when the I
mean people don't don't seem to be
complaining about the treatment of
systems that use reinforcement learning
algorithms today even though you can
make an analogy to very simple organisms
and goal-directed behavior so back to
the question of whether creating kind of
ecosystem with powerful AIS and humans
is preferable to having a single sort of
system that controls it certainly my
private elections definitely go in that
direction and when you look at some of
the game the theory literature you can
create a set up I mean that the image I
have is that will have a kind of
universal Constitution that reflects the
values we care most about that will have
these very powerful systems that are so
powerful that no person could limit them
so the only thing that can limit one of
these systems is another one of those
systems and so if you only have one or
two well then they could collude and do
something if you have lots of them and
you structure it so that if anybody
stops obeying the Constitution that
their neighbors detect that and keep it
keep it in check that feels a lot more
stable to me I've done a number of
mathematical analyses if you have
computationally limited systems there's
a very nice result of name on that the
iterated prisoner's dilemma you can
actually find cooperative solutions and
so a similar kind of thing I think
happens in these systems if they're not
too big that within a range of powers if
they were getting to get in a conflict
it would be so costly for both of them
that there's actually a benefit
in a pure you know economic calculation
to creating a peaceful solution if you
get a lot of them creating all in that
sort of interaction
I believe you can create an ecosystem in
which no even a rogue individual or some
percentage of rogue individuals would
not be able to take it over
probably you have to limit the maximum
size of any entity it's probably a good
idea to do that today to you know
whatever corporations hit a certain size
you cut them to I'm a popular that would
be but what about that's a really good
question you might have requirements to
explain you know your motivations that
balance between freedom to explore
wherever and enough constraint so that
nobody can can sort of run and run amok
that's a challenge you just mentioned
the point that you want to have multiple
entities competing that's why you want
to limit that size yeah well you want to
limit their size so one doesn't take
over and this appeals to a position
where you have conflict between those
entities and we find that if you look at
humankind's that you have large-scale
organizations which are intelligent on
their own that they are competing to and
in this competition not the most banal
abun have been the most successful ones
I mean Stolley was arguably very
successful and Dick Cheney was very
successful and even though modern-day
democracies are kept in check by
WikiLeaks and so on sorry if somebody
listens from some agency but how is it
possible that you all do not need to
reflect on the problems of conflict of
violent conflict between entities which
compete for resources first of if
anybody hasn't seen at the movie the
corporation is quite interesting
corporations are profit maximizing
entities by definition in the United
States and many of them behave in
psychopathic manner in this movie sort
of analyzes corporations as if
they were people what psychological type
would they do and they're Psychopaths a
lot of them on the other hand there are
checks and balances rules that kept
trying to keep them in check and
substance I think the corporation is a
model for the AI like what we're doing
to keep corporations sort of behaving in
positive human ways is exactly the same
issues as we'll have with the AIS
there's a new kind of corporation which
is sort of halfway between a non-profit
and a for-profit that some people are
proposing which they're allowed to
include more than just the profit motive
the the other piece you mentioned was
oak on Flickr so economists have a
notion of coopertition which is a
simultaneous competition for resources
but also there's mutual gains of benefit
synergies from working together and I
think when you have the economy when you
look at it yes there's a lot of
competition but even apparent
competitors you know AMD and Intel both
want to expand the market for the Intel
for the the IAT sex art architecture and
so there are subtle you know balance is
a cooperation and competition that sort
of lead lead things and yeah ok you were
the next one
so use me a notion of bounded
rationality you have pointed out that
rationality itself is a neutral tool
that rationality does not have implicit
goals but can be used to go to goals
okay okay what is the question
yeah okay yeah I think that the game
theory results that Steve was alluding
to use of the notion of bounded
rationality and friendly
I'm in fully defining anything in the
declarative way is pretty tricky this is
a classic AI problem from AI 101 like
how do you define what's a cup versus a
bowl explicitly so to the extent that
friendliness to humans is a sort of
informal natural language folk
psychology concept
I wouldn't expect us to be able to fully
define it in a precise way I'm I'm not
sure it's the kind of thing that you can
fully define I think the Friendly AI
it's um it's a notion that's going to
evolve as a eyes and humanity evolve
sort of tacitly in the same way that
everyday ethics and common sense
morality have evolved mark yeah I was
gonna come I have problems understanding
the whole notion of friendly AI mean one
way to interpret is as the agent or the
system doesn't take actions and in turn
intentionally harm humans right but
that's a very it for me very abstract
now right that's very abstract right so
I mean um I'm sorry
right but it's it it goes again into in
my mind an attempt to interpret this is
through its its goals and again if you
subscribe to reinforcement learning its
rewards with our external internal but
but yeah I have a problem of that and I
were problem with it enforcing it as
these things become more intelligent
it's uh it's a self-referring problem
and I guess my answer to Ben's question
is if you use it like a cup it's a cup
I'm very much a functionalist I mean you
know yeah that molds a cup sure your
cupped hands are a cup you know anything
you use like a cup it's a cup its
function that matters
that's a good question from you'd Kowski
it's that the AG I will treat humanity
in the way in which they want to be
treated
yes
some people don't want to be treated
friendly I guess some people don't want
to be the people they want to do is to
make ourselves more intelligent and
therefore I think you should be a lot of
emesis we're using artificial
intelligence to amplify collective
intelligence for the first decade or two
or maybe three that these Agis are on
the scene humans are still going to be
the major stream cores in the world and
we have to deal I realize we're dealing
in a world where these computers will
not only be used by academics they'll be
used by militaries they'll be used by
criminal organizations they'll be used
by hackers hacking is hacking it's one
place where you're going to be using the
most wildest and craziest of algorithms
to just get into other people's boxes
and twiddle with their minds so I think
the thing we really need to do is make
ourselves more enlightened through
various forms of reflective in cultures
including pens plugging in so I mean
I'll just I'm all in favor of making
humanity more intelligent I just don't
know how to do that I'd be happy to hear
other people yeah yeah I mean again
there's a there's a philosophical
theoretical aspect and there's a
practical one and so I'd be very
interested if anyone wants to comment on
how to practically do that I mean just
one example is Wikipedia I mean no
single person could have done Wikipedia
it's a creation of the whole planet and
lots of people now go there to learn
about things and so it's increased the
intelligence of many people around the
planet it's you know it's not automated
yet but imagine a successor to the
Wikipedia which was really smart and you
do you have the beginnings of a true
global mind in which is sort of a
synergy of people and
and bring it we need it I mean some of
the decisions that people make are so
irrational insane it'd be great if there
were a sort of a system whereby people
could make decisions and that the public
opinion was really driven by what was
true rather than you know what was
convincing do you want to add a European
perspective Cayuga I can add something
on this on this definition problem as a
matter of fact in principle every
concept every important concept that was
now mentioned rationality bounded
rationality morality ethical aspects of
behavior intelligence for example itself
right then and all others kind of all
these terms are just non definable and I
never heard a good definition of what
rationality is I don't never heard a
good definition of what intelligence is
and what happens is and that is what
people essentially doing is I developed
perhaps normative theories reinforcement
learning and then if I maximize my
utility function or my expected future
of further real future reward then I
called it rational intelligence some
logician say everything I can prove and
therefore I behave according to my
proofs and the things I can prove that
is rational or if I you have to write
heuristics if I have heuristic based
rationality content and I behave
according to this nice heuristics then
this behavior is rational and so and you
can do this with all these things so in
principle and I think definitions of
such definitions of such concepts is in
any case not possible and the only thing
we have if we talk about machines</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>