<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2010: Early Test Feedback by Test Prioritisation | Coder Coacher - Coaching Coders</title><meta content="GTAC 2010: Early Test Feedback by Test Prioritisation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2010: Early Test Feedback by Test Prioritisation</b></h2><h5 class="post__date">2010-12-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bMTI7c1AHS4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright ah we decided that we will give
two parts of the talk together so i will
give you about the initial talk about
the ideas behind what we have done and
then robot will come up and explain the
engineering challenges and how this will
fit into the actual practical procedures
so I'll end it over to robot when we are
ready okay this is a slightly different
style of talk because first we are we
will be discussing something that is
very ongoing work and also something
Jeff talked about in a transferring
technology from academia to industry and
I didn't know that Jeff was quoting bulk
but it is also work done by not just
robot and me but also more common who is
leading the crest with I come from so
it's a research center university
college london it's called centre for
research for evolution search and
testing because well has evolution but
it's a software engineering lab really
and but the reason why we have evolution
in the title is because we are quite
passionate about using metaheuristic
search algorithms for engineering
problems so it has wide array of
expertise but we applied Devon testing
so there are people whose doing
automatic test data generation as Jeff
mentioned he actually have such based
technique on the slides with the
mutation testing I'm working on
regression testing but there are people
also doing requirements and also large
search based stubs or static analysis
and other things it's based in very
essential part of London so if you ever
find yourself in London and want to talk
about these things you're always welcome
and the center as i told you is very big
on search based soft engineering the
whole theme is the very obvious fact
that optimization and the research into
trade-offs between different objective
is always very essential to any
in disciplines mechanical civil
engineering and the main idea behind
this case is in the why not soft
engineering you know we are engineering
we are in the engineering discipline so
why can we use them so that all all of
SBS e is to formulate problems in soft
engineering into optimization problems
and then try to apply various techniques
from this rich array of optimization
library they are you know with all the
hundreds of algorithms devised by
evolutionary computation people and all
sorts of other disciplines so we just
need to pick one and apply to your
problems so we have seen some very
interesting results recently so there
was a paper that surprised quite many
people last year which was a one of our
automated pod pitching by westheimer and
stephanie forest last year so what they
did was to use genetic programming to
build a quick patch for a known bug
automatically and it worked quite well
we had a paper about human competitive
results in various areas so they applied
it to regression testing requirements
engineering and the results that this
algorithm babe was also annual
international symposium it's not that
was the sales pitch this is the online
of the talk I'll give it some
motivations and then we will go in to
define what we call dependency coverage
for large systems in shared code
repository and how we optimize the
regression testing problem we have and
some initial result and then I will hand
it over to robot for engineering
challenges so I probably do not need to
motivate you very much about the
problems of regression testing the
definition we use in the academia
regression testing is performed to gain
confidence that's because you cannot
predict the vault but you can only
reveal the existence but you want to
essentially gain confidence than the
recent changes did not actually damage
the existing system
there are very many techniques developed
for this mainly because as the software
evolves your regression so it keeps
growing until it is not feasible to
execute all the tests that you have so
there are three main pillars in the
literature minimization you try to get
away with and get rid of redundant test
cases selection you want to precisely
select those that are relevant to recent
changes on me and then finally the
prioritization idea is to you want to
run what the most important test first
and these are all techniques invented to
deal with increasing cost of regression
testing but I say in the modern
development environment all the tests to
share some dated assumptions so first
they all assume that we have a fixed
release cycle and fixed shipment date
and you are working towards this next
goal and then it also assumes that the
tester or will some cases developers but
all the techniques assumes that you know
the person who's applying these
techniques have the has the ownership of
the entire code basis so for example it
assumes that you can get all the
instrumentation for coverage you can
inspect the source codes that kind of
things which may not always be available
so I sense that there is a new trend
emerging and it is mainly for the world
of web applications and these small apps
that we you know use more and more
frequently it's in our everyday life
nowadays especially in the web
application world software is not
released and shipped you are I mean they
are made available in incremental and
continuous way which means the pressure
to time to market is very enormous and
then we often have distributed
development environment and the source
code is shared over large repositories
and you may not break your code but
someone else's by making changes and
it's very hard to predict when you're
working on your
module when I showed this Lodge to robot
he panicked because no no you know no
one does what a full model anymore and
as soon as I see one on the slides I
know that I can sleep for the next five
minutes but don't worry so this is the
classical model but I will skip it and
then we had agile I'm not so sure you
can represent the whole agile movement
by adding one more edge on the graph but
you know there you go and I think we are
probably moving towards the next
generation of paradigms and what it
means is probably the dis cycle is
getting so short that now everything
almost happens concurrently robot
suggested that um push by Green so you
don't have a fixed shipment or release
date you just try to keep everything
healthy in your repository and then when
everything is fine then you push out the
next release in incremental way so yep
what we mean by which by Green is
probably that the iteration time for
this cycle is now converting 20
everything happens concurrently so
testing should happen while you are
developing and it should help the
development itself so the automation
probably plays a central role here
because otherwise it will be too costly
but it's not just about being
cost-effective meaning in a shorter time
to test but in any guidance we get from
automation should it's possible allow
the tester to measure the trade-off he
gets and by giving the early test
feedback with this measurable trade of
it will enable the developer to achieve
shorter development cycle and decrease
the cost to fix regression folds so what
does it mean in concrete terms for the
developer that automated tool should
guide your testing by prioritizing so
suggesting some tests that you should
run at this precise point so the goal is
to achieve elitist test
feedback and to select a set of tests
that will heal the highest chance of
breaking something but without owning
all the dependency because you are
working in a large repository and then
to be able to observe this
cost-effectiveness trade-off precisely
so what we suggest is a heuristic and
you can see that now i'm coming from
materialistic camp a heuristic that
select test that can be executed within
specific budget we often talk about cost
in terms of times it's we didn't
specified time for a particular code
change and we will base our decision on
testing the dependency relations so you
want to ask did I break anything that's
depending on me history of all failures
so we will assume that what has been
broken in the past probably has high
chance of getting broken again and then
any other information that you care
about and this is one very strong point
about using materialistic search because
anything that can be formulated into
your objective function for your search
algorithm you can optimize so these are
the things that will try to optimize in
this small pilot so first one is the
cost you obviously want to minimize it
so we will depend on a automated testing
framework which has all the metadata
from the past execution of the test and
try to extract the cost of each test you
have in your system the second one is
dependency we will not use the code
coverage as a measure of effectiveness
of your testing mainly because you may
not know all the code level coverage of
your of the modules that spending on you
you may know everything about your
quotes but there are some modules that
are owned by other people and they may
get broken so in the literature for in
immigration testing technology
techniques the most widely used
criterion is probably the structural
coverage but it may not always be
available and we want to
our approach on this large repository
model so instead of code we will focus
on a graph of the dependency model so
what we eventually get is something like
this so this is your module you want you
are developing on this anyone to test
this and if you are a good developer
then you should probably done a lot of
unit testing on this module so this one
and anything else that your module is
depending on has been tested but there
are other targets that are depending on
your module and then eventually there
are some test that has to these modules
and also perhaps your own own module and
we have this structure that looks like a
horn that's what we termed so we will
define a new coverage on this graph so
essentially it is the number of things
in the corners and number of things your
test touches by executing the test so
one added benefit of defining this
dependency coverage if not just to
detect breakage in the modules that
depending on yourself but it also
enables the developers to see outside of
the directory that he's working on so
meaning you I mean if you are testing
your codes then you will probably care
about the quote that you have written
and everything else you are depending on
but it's very hard to see outside your
box and see other modules that are
depending on you but that can have the
most dramatic aftermath if something
goes well if I may say so it makes it
possible to touch all the effective
effective pieces of code in order to
proper I avoid any danger and the third
objective we considered is history so we
have a will head the support from this
large test framework in Google when he
did the pilot so for each test we could
count number of failures in the fixed
amount of time before our testing so
obviously this we want to increase
as well as the dependency coverage we
talked about so that assumptions we are
making for this optimization if you get
higher dependent coverage it's better
because it gives you higher chance of
detecting our failure in arbitrary
location behind you and by including
history we are expecting that in a a
good test which has detected something
before will again probably detect new
failures and then we want to maximize
these two within the given CPU time and
the way we do it is by using alternative
algorithms in this case so i won't go
into details because it has the whole
stash of papers assigned to itself but
essentially what we are doing is a
multi-objective optimization so you get
to minimize your post while trying to
maximize whatever goal that you want to
maximize so this is in actually a
trade-off observed in one of the real
modules in Google's repository so what
is saying is as execution time grows you
will get more and more dependency
coverage and this trade-off is quite
frequently observed in any code whether
its dependence coverage or statement
prints coverage you get in a graph that
looks like that and there are either
point that where the trade-off happens
so the question you should ask yourself
is you know this is often the 8020 rule
so is this enough or we'll see so that's
what we did in this small pilot earlier
this year actually I tend to me in the
Google to leave office and we looked at
some change lists submitted to the
Google's repository there is a small
caveat the past the history data we used
for this pilot is was actually not real
history but some future but at that
point we didn't know how to develop the
data from the infrastructure and it was
quite difficult but at the same time we
thought in a day is nice continually in
this
testing a put in Google so it will
probably work and robot will give you
more explanation on this later but we
did not filter out the flaky test this
is where I will probably spend the most
of my time so this is one of the good
result and you will see again this
familiar shape so on the x axis we have
the CPU time required to your required
execute your tests and on the y axis we
have the dependence coverage and it sort
of flattens out after you spend a
certain amount of time and there are
black dots and read thoughts red ones
are the set of test that detects failure
so if you want to be complete then you
can choose this set which will require
quite a lot of CPU time but also gives
you a high coverage but if you want to
if you are really short of time
something along this region will still
notify you about the break ET that you
just made on the right side we have the
comparison of the CPU times so the
subtotals EPO time is the time required
to execute to all the tests that are in
the corn that we looked at earlier so
this is the time required to execute all
the relevant test for the module that
you are submitting and the middle column
is for the required the CPU time for the
maximum coverage so essentially that's
what it takes to execute at this point
and finally the left one is minimum time
for failure so it's the CPU time
required for executing this point the
problem is you cannot predict where this
printer will lie always but between this
and this you can probably get reasonable
policy so for example you will always
cut this graph in half and pick some
point closest and we have some at the
result that shows similar patterns this
one is quite interesting because
you get a lot of those similar patterns
but only one of them are actually read
it is interesting because all these are
increasing dependency coverage as we
allow them more time but only the
rightmost one actually detected some
fold so it tells you what it tells you
is the test in these lines are actually
touching the dependency but probably not
doing enough to actually detect the
failure whereas if you start to spend a
significant amount of CPU time on this
test you will actually include the test
that detects the failure but this being
metaheuristic and also a testing problem
it's always not so good news so in this
case we require quite a lot of
dependence coverage to get the first
failing test the same here and even none
at all so this is in or something that
we can improve upon but as a summary out
of 24 18 ciel's provided good result
meaning that we should any we were able
to prioritize and select some subset and
they still detected failures every
disappear time required to maximum
coverage it's less than fifty percent of
the Heinrich oil to run everything April
every CPU time to require the first
failure the leftmost red dot it's about
twenty-seven percent and to require the
first failure you only need about
seventy percent of the coverage so it's
again sort of the 8020 rules here so
that was the pilot and ever since we
have been doing this ongoing evaluation
so what we did was we ran a process and
we have been picking random sales to
apply the optimization it had to be
automated so it it actually means that
in out of those graph you have to pick a
point to
select the tests that you will run so we
decided to be very conservative and
allowed algorithm to have fifty percent
of CPU time required for running all the
tests so overall we will always get
fifty percent save in CPU time and this
time we knew a lot more about the
infrastructure and we were able to
filter out flaky tests before starting
the optimization so this is the result
from the ongoing evaluation we have
number of tests here the blue column is
what the tab the testing infrastructure
in Google considered for the submitted
change list and the red column is what
we chose based on the metadata and the
optimization and on average we are
selecting about sixty six percent of the
tab considered so it's a saving of about
one-third in terms of effectiveness
about sixty percent of time the
prioritized test detect breakage and
about forty percent of time it doesn't
this is out of 60 change lives randomly
chosen and the real break data was again
collected from tab so this probably
doesn't look as good as the pilot but we
should consider two point first because
we are filtering out flaky tests it's a
bit hard to detect aviel breakage but
still it's not that far off from the
pilot and we succeed slightly more than
half the times which is better than the
pilot but the reduction is slightly less
we save about one-third but again what
we should remember is we choose a very
conservative policy of allowing fifty
percent of CPU time budget where as the
real failure point the left most reps
that we saw on the graph may be
significantly cheaper than the fifty
percent CPU time budget and this is
where our next work lies so we need to
find a realistic way of
finding this threshold that gives you a
significant saving while being in a
reasonably effective okay so at this
point I'll handle with Robert and he
will give you some details about the
engineering challenges first yeah so one
thing to keep in mind here is that one
of the targets if this is to be for
developer and he's in the state of mind
of developing his software he just want
to run the pesticide activity all the
time and he wants early feedback from
all these woodworking all right so we
can have this and then you can have like
something in the back of your running in
the background ok so can you help me
yeah so this is targeted some of the
engineers or in the flow of developing
and they want to have still the backs
free so they want to find the problems
where you know the did I break someone
else in the integration system so also
for doing this and actually using it in
action there is an artificial constraint
from an academic perspective like these
things are not something that would talk
about probably in
hello so I wouldn't talk about in an
academic conference but some veneers are
very impatient creatures and if I tell
them that hey you're going to have to
wait 10 or 15 minutes for an
organization to finish it will save them
some testing tonight okay why would do I
need to do that we have a search engine
where I can type in a word rape right
size all the web pages and it takes like
under a second so why should I wait 5-10
minutes to my test prioritize so I
couldn't do it you need to do some
processing of line to make this like
appear to be instant basically right
they don't want to run stuff on the
desktop because they have other stuff to
do on the desktop and there is kind of
annoyance of flaky tests a flaky we'll
talk about more in later but this
something goes around Google a lot where
we have false positives basically
something say oh something's broken but
really it's not so you spend time
investigating things and losing time and
productivity another thing that you
don't want to run into is to measure
your change to the system or the change
to whatever you're developing again
something that's already broken I mean
you want to know that your thing that
you're introducing or adding to the
system is nice and clean you don't want
to be affected by whatever else is going
on in the in the system that's evolving
around you so yeah and another thing
that I have experienced from other
priorities that if it present like
information to engineers and developers
and you see them like the facts on the
table they'll be okay so what do you how
do you do this I mean dislikes what is
come from so they won't have some kind
of insight an explanation to what kind
of suggestions who gives and able to
understand it at least on some abstract
level and have some kind of control say
this is really stupid I want to do this
instead if I just say okay wait a minute
I or wait in the actually not a minute
wait half an hour and I'll run all the
tests and figure out this for you this
equals like okay what is it will take
half an hour and then I presented on
what we are doing is okay with that and
doesn't that is very stupid so what do
you do that anyway and okay this is
again ray unscientific because I don't
we do like a story and and this kind of
stuff to the sake of things that I've
been observing working with developers
in my job so the flaky test is something
that you heard the fourteenth Simon
mentioned this as well it comes from
cycling on the terminus of the test
result when it comes from
multi-threading sit around earnest in
your testing you can have timeouts from
resource overload like your stored on
converter cpu running on there is some
resource that you need to retest that
you can't access like test accounts or
something like this so as part of this
work we just off the scene left zurich
we sell it to Cadiz this flakiness is
probably going to be a big problem you
don't want to suggest flaky tests the
developers because it going to make this
to more or less useless so we thought
start looking at how can we cannot
convey turistic to determine if it tests
is flaky or not to some extent we could
get this from manual annotation people
would know care and doing this I know
it's time to be flaky is like a big urs
or something is going to be very hard to
get it right all the time there are also
a lot of places where people haven't
annotated resting a lot of flakiness
going on and you basically looked at
awesome test infrastructure we have at
Google and you can see like history and
see this is a flaky tests and it has a
pattern which looks like this that we
have a usually a cut where it wasn't
person to failing because it's very
unlikely that the developer will catch
up and like fix the problem exactly just
of the one change list or a change in
Excel or submit the repository whereas a
flaky thing looks like more like a
regular pattern like this and we
basically extracted heuristic from very
count the number of transitions here
with the length of one for the failure
like the past failed paths transitions
so while we're developing this was
actually other people in mountain view
in the headquarters would doing similar
work and it turns out we could set up
meetings and collaborate on this and now
this is actually part of the common
infrastructure house is a metric that is
exported it's really good so while we
what they want to use test PlayStation 4
here is to like perform analysis and you
want to do it so it appear
to be super fast so we did while
developers write our code and we want to
do either just conservatory so we had a
good chance of finding a braking test
for them and all precisely say okay I'm
going to go through magic kitchen are
going to have a cough 10 minutes and
give me the best shot at finding stuff
that breaks so I can go back here and
fix them they don't have a less
probability of submitting and finding
out like half an hour or half a day
later that oh by the way you broke
someone else and put an angry email or
something so yeah so if you have this
thing you can run it and you can get an
early feedback and anything else choose
to go ahead and submit it or you can run
the rest of them if you are still
insecure of your if the risk is very
high if you wish I so this is not rocket
science it largely as a small tool that
has basic server but tells like a
component where you actually spy on the
IDE of the develop you see what are you
doing right now and this means that he
can you know start batch processing and
figuring out what you need to eventually
around to validate exchange and we have
something going on here which talks to
our backends which contains all this
nice test metadata and stuff is
collected by our continuing education
system and then when the developers need
this they can used go ahead and stay get
information as appearing as it's super
fast right so this is how it looks on my
screen because I'm like cool i use black
and green actually not it's like that
how the two-loop looks right now is for
the type thing we developed based on
this and sasa command line thing you can
see get a risk estimate here so you see
that okay you just changed it effect
under 29 tests and pentagon libraries
across deflection get the list of what
kind of things that you are modifying
fucose maybe some of these packages up
here or more importance maybe I we might
be really sure that I never break the
build of the new evil validator for
instance because then someone recommend
eating realistic where else
I'm not as concerned about this thing or
something like I'm very I think it's
very unlikely that I will break this so
this is how it looks like now this is
very early bit as i said what i would
like this i look like is something like
this this is why present exists but you
would like to have this breaking down
lon roi test type okay what kind of test
do we have here you a automation test
integration test unit tests and so on if
we had manual tests in our dependency
graph which i think would be interesting
idea to try out we could also have like
manual testing costs here like okay if I
submit this we're going to have so many
metal tests we have to run taking so
many hours and then you have the same
stuff as before and instead of having
the CPU time which have today would have
estimated were dropped I'm saying of
actually this time you have the wait for
this and also things like who is the
people who cares about this thing so
maybe I want to make sure I never break
email because I'm people commonly angry
at me so then I want to prioritize many
more these these tests and so the kind
of options you want to give to these
things okay I have this information and
I want to guide the presentation to take
into consideration like it spread over
these or more integration tests and
focusing on gmail stuff like this right
this is like missionary development here
so this was like pretty tool now look we
look back at what scene described
actually dependency cone problem and so
on or dependency coverage there is some
fundamental things here that can cause
problems that it realized when we're
trying this out in practice that this
dependence relation is kind of
statically defined anyway and it can be
very weak or very strong and you don't
know so if you had like structural
information about code coverage to
enhance this would be better but simply
say like how strong is dependency from
this model to disorder model and make a
better presentation so it happens that
we sometimes think we have dependencies
which is very incidental which is very
not really a big deal say and then we
also have this related problem where we
sometimes hear they have hot spots or
landmines in our dependency graph so
this coded I'm changing right and I
could be pulled in by some of our big
binaries like a search front end or
something and then all of a sudden we
have thousands of UI tests or even more
to run it also lots of work and then we
have a very hard time to know what is a
good thing here what's running water not
run so this is something that the water
would like to have a better dependency
concept to flesh out into a very job and
so I mentioned in my unit in a tool
slide that we have some caching or
absolutely understand at least we have
this encashing over the repository have
and also the systems you have have some
caching internally so there is some
problem with freshness that data that
we're actually going to use in our
organization is going to be a little bit
stale there will be nicer to have but we
can probably live without it the CPU
time the wall time is very hard to
predict if you run in your tests and
building things in cluster because
doesn't involve caching and
fertilization aspects which is hata
model and we need to have like very much
very api's to or build tools to get this
information out another thing which is
related to the previous one is that
actually because we have cashing on many
levels as nathan is an excellent talking
to PG tech talking about this we have
catching on so many levels it could be
that the course is actually conditional
so that if I build these set of things
the test or it becomes cheaper because I
already done lot of the work right so
you can maybe rephrase this this cost of
the test in terms of what things you've
already billed me work already done so
it becomes not a search problem going to
finding this space of actual cost even
that you have done work previously so
wrapping up outcomes of this porridge
that they have done so far
we have this dependence recovery concept
in between Google which is kind of
interesting we have input to Google
teams about various metric presentation
ideas basically here we did an open
source library because it turns out
working with academia in Google is kind
of hard I mean Shin wouldn't have a cool
axis if I would have so we actually
split it up so she knows working on some
couldn't open source library and I was
working on some stuff inside and we made
sure that interfaces for anonymized and
clean and whatever another thing which i
think is the interesting one is that we
have a greater understanding about
google kind of problems in academia so
communication which chain is seeing what
kind of repository horror and what we
have and what what is going on in suguru
can help lots of PhD is to focus on this
problem maybe and you know get some more
academic results of their useful for us
we are potential benefits which we don't
know we might have some times in the
future we can give the developers this
risk estimation thing right in the tools
and said okay if I have this information
before i submit the repository i know
what kind of things that can affect i
can choose where I wanted in my chest
effort and in some cases I can deploy
prioritize manually some ports and not
others you can also have this cpu if you
run is just a piece of meat scenario you
can save you hours basically by not
running all the tests and still getting
a decent warning so to speak before you
run everything in the in the pasta meat
case yeah and that earlier feedback is
another aspect of this so what are we
doing now continuing putting in
evaluating this thing and more random
change lists or changes into a
repository we would like to try
different teristics and try to be even
better obviously these motor signals
that we can come up with maybe one thing
only want to try is this obvious moment
we have historic data or when
thing wrong doing am I gonna change
cause it has to fail in the past that
would be like a really good indication
division on this test again right now we
just haven't like an absolute notion of
ladies test fails a lot and it's not
flaking so don't sleep only want to
prior to us prioritize that in front of
another one yeah so also this problem i
mentioned do is better with dependency
growth and make this tool more useful
more like the vision I had basically
amazed that something that was actually
would like to use and to see what
feedback we get from that and that's
about it questions come up here chin as
we're short of time again we'll have
couple of questions and we request
everybody to just ask but in case you
have a follow-up please talk to chin and
Robert in the break any questions
okay we can't have more okay what would
it is a dependency analysis take into
consideration non-code dependencies such
as you know module a updates a file
which module B then reads changes the
data structure which is not in some
cases yes but not always are you doing
any retrospective analysis to see what
bugs escaped that's very hard to observe
on the current infrastructure because
almost all tests are running probably
all the time so even during the time we
run the optimizations someone else can
actually pick up the test can run it and
may pass or fail so it's hard to isolate
what our protest in exactly done and
it's one of the problems especially well
for me if I want to present it in
academic setting but we're sort of
working on it and actually it poses a
very challenging settle problems I think
because because essentially we are
testing too hard the initiative so you
are not always able to observe what
you're testing has done to the system
but you're always observing someone
else's result as well as a sort of
continuous stream so we need to work on
this to evaluate this more clearly I
think the dependency we actually didn't
analyze it out by ourselves but we
picked it up from the dependency
relations / that's running in Google
system so you can curry and get some
relationship so you give a file and it
will bring your older files that are
depending on it and depending on that
information does the system learn over
time so the effort s proves particularly
useful particularly good at detecting
bolts will it get chosen often or is
that something you've looked at adding
he repeated this if if a test proves
particularly useful of catching bugs is
it more likely to get chosen in the
future does the system learn over time
to also think that's our hope and in
this case the positive feedback route we
have is if a test is particularly good
above average then it will show up as
higher number of past folds detectives
put that by by the test case so that
should be prioritized by maximizing on
that objective so they should have a
higher chance of being selected given
that it fits in the project that we have
yeah and also there is more explicit
napping from file to test and it won't
even more of a learning factor in there
these files actually very good to test
with tea set of tests or something like
this right thank you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>