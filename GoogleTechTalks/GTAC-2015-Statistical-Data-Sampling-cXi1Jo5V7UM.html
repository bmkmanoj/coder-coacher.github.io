<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2015: Statistical Data Sampling | Coder Coacher - Coaching Coders</title><meta content="GTAC 2015: Statistical Data Sampling - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2015: Statistical Data Sampling</b></h2><h5 class="post__date">2015-11-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cXi1Jo5V7UM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're going to kick off the day with
Jalal and been talking about statistical
data sampling so I'm Ben Greenberg and
I'm a graduate student at MIT I'm
concentrating in machine learning I was
an intern at Google for the past two
summers and this past summer I was
working with Joelle on statistical data
sampling which is what we'll be
presenting today hi everybody my name is
Kjell uzuki I am a software engineer at
Google for the last two years before
that I did my PhD in University of
California San Diego on software
maintenance and testing and I've been
working at Google on those topics for
the last two years I've been building
tools so this is my third talk at G tech
I've been in the last two as well let's
see what happens next year if I can find
something to present so we're gonna talk
about today on a noble statistical data
sampling technique on software testing
so I'm gonna start by giving an overview
of an important aspect of testing which
is data so whenever we write test cases
as much as the code is important data is
also equally important because that's
what flows through our system right so
one way of getting test data is
handcrafting it manually what do I mean
by this think about unit tests you have
a deterministic set of inputs and it you
send it to you through your system and
then you have a deterministic set of
outputs which you expect and also
against at the end of your tests so this
is one way of getting your test data
sitting down and putting down these
inputs and outputs another way of
getting product test data is getting it
directly from production so sometimes we
just don't have the time or the ways to
get test data ourselves especially if
it's on a large scale and in those cases
we would like to get it from production
because it's pretty easy so when I say
production just think about it like say
you have a system and you're collecting
logs for
or all of your requests and responses
and this can basically be say 10% of all
of the logs that you collected yesterday
and you can use this on your test today
so and this talk is specifically focused
on sampling from production so we're
going to talk about some problems on
this front and hopefully identify a
solution for this problem so I would
like to show you some examples of the
kinds of tests that you can have if you
use production data for tests so say you
have an integration test and you get
some of your production data as I said
let's say 10% of your logs from
yesterday so what you can do is you can
just run through all of this data
through your system and then you'll get
some kind of output and you can
investigate the output and typical
things that you would look at would be
does your system crash or do you use a
lot of CPU or the use memory does it get
deadlocks so kind of like black box kind
of testing you can do with this data
another example could be a B testing so
there was a talk yesterday on Diffie
which is an open source from tool from
Twitter so it was about a/b testing so
you simply get some production data you
have a current production binary and you
have your new version of the binary and
you can run the same exact data through
these two binaries and they produce some
kind of output and at the end you just
diff these outputs and figure out if
there's anything abnormal some developer
sits and looks and says yeah this looks
normal this is all fine so we can push
it to production
so these are the two kinds of tests that
you can potentially have if you want to
use just production data in your test
cases so something that's extremely
important for these kinds of tests to be
effective in catching bugs is that the
sample that you choose from your
production data should be good so if you
think about all of your data space in
say your production data space and if
you can get a good sample of that then
there's a high likelihood that you
should catch bugs right if if there's a
case where your system will crash
hopefull
if you get a good sample your system
will actually crash and you will catch
that during your testing so assume this
is your this is a representation of your
entire data space and these blue dots
are the samples that you choose from
your entire production data so if you
look at these samples you will see there
are some middle ones like the expected
cases and there are also some boundary
cases which you probably want to test so
let's make an analogy so if let's say
you're a developer writing testing a
function that takes one parameter and
integer right so you would probably test
some positive inputs for it but you'd
probably also test some negative inputs
some boundary inputs like max integer
zero things like that so this is pretty
much very similar to that so you want
expected cases and boundary cases so I'm
gonna do it over to Ben to talk about an
example so now I'm going to try and
ground this a little more in a distinct
example that people might understand say
your data looks like this you have a
bunch of people that you're trying to
represent and each of them has an age a
weight and a height and you might want
to figure out is there any problems in
my system is there anything that is
there any age weight or height that
might crash my binary so you come up
with a reasonable bound for each one the
age might be between zero and a hundred
fifty that's totally reasonable but what
if we have a person with an age of zero
we need to make sure that our system
works for this so you write a unit test
to make sure that if age is equal to
zero nothing bad happens we also might
want a just and zero because who knows
what's happening
maybe you're getting this information
from another product somewhere else and
you just want to know is there any case
where age is gonna break this we might
have age is greater than 150 maybe
someone's giving news that a person's
age is 300 and you need to make sure
that that's not too large for your
system to handle and all this is great
but if you're specifically going through
these cases and saying what do I think
might break my system what I think might
be wrong what about the cases that you
don't think about what if ages no if you
don't think about it you can't
test for it this is why we like the idea
of sampling our data from production so
that we get the actual data that exists
and in an optimal case we'd be able to
cover all of these edge cases whether we
can think about them or not so let's say
that we have 500,000 of these proto
buffers a proto buffer is a Google data
storage mechanism it basically just has
key value pairs so in the example before
age would be the key the value might be
50 so we have 500,000 of these and in 25
of them we have the specific case where
age equals 0 so we're sampling a
production data and the common way that
most a lot of people do it is they'll do
a random sample in this case we want to
know how likely are we to catch this age
equals 0
using a random sample from our data and
the results are basically that we needed
to a we need to take a tremendous chunk
of our data to have a very high
likelihood that we're going to catch one
of these age equals zero cases if we
only take 500 or 5,000 examples we're
most likely gonna miss it even if we do
10% which is what a lot of people in
industry have been doing recently we
still have a 7% chance of missing this
age equals zero case and if you think
about having not just one field not just
three fields but tens of thousands of
fields which is very common in Google
you're almost certainly going to miss
this case if you just take one of these
cases if you just take 10% of your data
so from this we can see that we pretty
much need to take the entire sample in
order to get a reasonable chance that
we're not going to miss anything and we
can be said we can be safe and confident
in our new binary so if you remember the
picture that you all showed you before
we have a production sample we run it
through our integration tests and we get
an output and if we take a massive
sample we take the full five hundred
thousand or four hundred 50 out of the
500 thousand the integration test is
going to take an incredibly long time to
run
if it takes a full day to run say we
can't run it all the time we can't have
it running with every change because it
just takes too long the same thing with
a/b testing now we have the entire
production data running through these
two different binaries we're getting
their outputs and we have this diff but
as Jalal mentioned before a developer is
probably gonna have to go through this
diff and if we use all of our data the
diff is massive developer is gonna have
to waste a ton of time it's gonna be
very tedious work and at the end there's
a good chance that they're just gonna
say yeah it looks good to me because
they don't have time to go through
500,000 diffs so what if we could get a
small sample that covered the entire
data set that was representative of the
entire data space and made sure that we
were catching all of the edge cases the
issue from before is that we both want a
large sample for coverage purposes and a
small sample for run time purposes we
ran our statistical assembler no sorry
so imagine that you have some data and
we wanted to actually sample this data
so the first thing that we want to do is
we want to somehow reduce the feature
space we do this what's called a data
miner this takes all of the features in
your system and reduce them into a more
manageable amount if you have 10,000 it
might reduce them down to 100 something
that we can actually work with and then
we look at our entire new data space in
this lower dimensionality that's easier
to work with and we start with a seed
the seed is where our sample is gonna
grow from this is a random seed but it's
incredibly tiny maybe 7 or 8 out of the
500,000 messages that we started with
and then we look at a new sample and we
say does the sample teach us anything
and if it does we add it to our seed and
continue going but if it does not we
leave it alone
and we continue doing that and adding
data points slowly over time but only
important data points only the data
points that actually give us new
information about what's going on in our
Sam in our sample space and eventually
we populate the entire space we cover
all the edge cases have a good
representative sample and that is what
our data sampler does so let's talk
about some experiments we ran this on
500,000 of our own production photos and
managed to reduce that set to just 284
while covering all of the edge cases
that you'd probably be interested in
testing along with having a smattering
of these middle cases that you'd be
interested in for just basic does this
work at all tests we also ran it on a
sparse or data set so that's what the
600,000 number is these proto's were
incredibly sparse they only had a couple
fields each and you'll notice that we
were able to reduce them significantly
further because our sampler was able to
figure out that there's only a few
things in these proto's that actually
matter and only care about those which
means we don't need as many to cover the
data space and then finally our biggest
success so far was that we ran this on
1.6 billion production proto buffers
reduce it down to 78,000 and this was
from a Google team that was actually
doing this a be testing their tests were
taking about 14 hours to run and they
were running them about once a week
because that's all you can realistically
run a 14 hour test we have them down to
just a couple minutes and they're now
running it on every single submit
so we can guarantee that not only are
these much smaller samples but they
cover all the expected cases and all the
edge cases the tests run much faster and
the diffs are a lot smaller and non
repetitive because we're not taking
proto buffers that look all very similar
right
so now I'm headed back to JA so this is
basically what it's called statistical
data sampling in literature so an
example of this is the election polls
when you're trying to figure out which
candidate is gonna win you go out and
pull a bunch of people in in the country
right and it's very important to ask the
question to the right people so that you
can actually really get a good
representation of the country so that
your success of prediction is high right
so this is kind of a very similar
problem that we are solving here but one
difference is that our feature space is
very big so as Ben mentioned earlier
some of these protocol buffers have tens
of thousands of fields so that gives you
a very high dimensional data space so we
have to really work with this data space
so therefore your features are really
really important so if you don't have
good features to represent your data
space you're not gonna get a good
representation of your data hence as a
conclusion you're not gonna get a good
sample so I can give you some examples
of what these features that we're
talking about are so in that protocol
buffer use so there was there were three
fields all integers so for such fields
we can find the boundaries of such
fields we can say this field is always
between 0 and 15 for example right and
protocol buffers are open source by the
way from Google you can go look it up
online
it's like JSON XML so basically we take
all this data extract features from it
another feature is we look at if a field
is always now never null or it can't it
can be both now and not now so we
capture all of this information with our
features and given 10,000 fields we
might even have 80,000 features because
we look at every single field and we can
actually
tract multiple features for each
specific field and we also have what we
call binary features for example we can
say if this field exists in this
protocol buffer this other field should
also exist so it becomes pretty
complicated very fast so we what I mean
to say is we have a lot of features to
have a good capture of our data space so
in the algorithm that Ben went over we
just showed you one example of a sampler
so we have four of these the reason is
different teams have different needs the
example that we showed you had the
ability to capture both expected like
the middle-ground cases and the boundary
cases but some teams don't really care
about the middle cases they just want
the boundary case right so you can make
small tweaks to the algorithm that we
showed you and we can simply just find
boundary cases so and we can do
basically similar tweaks so that we can
get different kinds of samplers for
different purposes and finally I think
this is a very important point so we in
our experiments we showed you that we
can do this on half a million block
proto's or we can do it on billions of
products the reason is we can paralyze
this with MapReduce it's it really
yields itself to parallelization and
I'll just go over quickly how that
happens so say you have all of your
production data and you basically shard
this meaning you split your data into
approximately equal sized chunks and
then you use a separate data sampler to
process that part of the data and all of
these data samplers are running
independent from each other so they
don't know what proto's they chose like
from each other so they all yield some
temporary sample right the problem with
this is since they are working
independently they can actually have
repetitions right so and we don't want
that so what we do is we take all of
these samples and then run them through
another sampler again and at the end you
get this final sample which is
equivalent to what you would get if you
were to run all of the status serially
using one simple data sampler so this
way you can really increase the number
of your shards depending on how much
data you have and given some time
constraint you can really process all of
your data within that time constrain it
regardless of how much data you have
now after all of this discussion I think
it's a very legitimate question to ask
how do you exactly know your sample is
better than some random sample right how
do you measure that so we talked a lot
about like what happens if you get 10%
of your production data versus what
happens if you use this tool to get a
very small sample that is smart right so
first of all as we mentioned this is
obviously going to be increasing the
speed of your test right your tests are
gonna be running much faster and the
outputs are gonna be a lot smaller but
another way of finding if you're smart
sample is good enough is you can simply
run both the smart sample and the random
sample through the miner because the
miner captures the essence of your data
right and you can dip it and say oh my
random sample was missing this whereas
the smart sample has examples of where
that is missing and this is what what we
did in our experiments and the smart
sample is obviously better and an
implicit or indirect way of doing this
could be branch coverage so when you use
the protocol buffers there's a
capability of generating an API for it
so that you can say in Java simply say
get age get weight so it generates an
API for you so that you can code against
it right so in teams they take some
branch conditions based on these fields
so you can simply run both of these
tools while you're instrumenting your
binary collecting code coverage and see
if you're smart sample gives you better
branch coverage and this is an implicit
or indirect way of measuring if this is
still better and again obviously we did
this and smart sample is giving you
higher branch coverage in our
experiments so that concludes our talk
and we are going to open it up for Q&amp;amp;A
yes guys um so lots of questions
starting off how do you determine
whether a sample introduces new
information so we have we have this
miner that we spoke about briefly right
and what the miner can do is it tells us
what the edge cases for any given sample
is so for an integer because integers
are pretty simple to work with it might
say that currently this sample has
integers between two and seven so then
if on some certain fields so then if we
look at a new sample that has an integer
of size 10 say we know that this
introduces new information because we
can't possibly have value 10 in that
slot yet so we use this to determine
whether or not we're getting new
information from adding a sample and
obviously we are doing this in parallel
like we're not doing one field at a time
right when you give us a new sample we
are looking at all of its fields all
together even if a single field teach
you something new we take that example
so can you tell a bit about the
mechanics of the data sampler ie how
does it work is it a tool that we can
use so so basically this data sampler
again if it so you use the miner
underneath and the sampler is a pretty
similar simple wrapper around it you
just mind it if it's giving you new
information you accept the sample and
you keep growing your seed until you
cannot learn anything anymore and you
process all of the data about is this
something that you can use it's not open
source yet but since protocol buffers
are open source I guess we could open
source this tool as well the miner and
the sampler so yeah it's something that
we will consider definitely what if you
have an edge case that hasn't yet
appeared in production data how can you
determine the production data set is
complete that's a very good question so
this is yet another example of garbage
in garbage out right so if you don't
have good production data first of all
if you don't have good production data
you shouldn't be doing blackbox testing
or a be testing because you know this is
basically just trying to make that
simpler right but yes this is a very
valid question and this always comes up
with the teams that we talk to what
happens is they run this not only on one
day's worth of data they keep running
this until they do the next release so
basically they try to capture a new data
over a period of time so you don't
discard your sample every single day you
just keep it and then you keep adding
more information to it as you have new
features and as soon as you have a new
version of your binary you start again
and that's kind of a good point that he
made also it with a be testing in
general I think it's important to note
that it only really works when you have
a good binary running in production that
has been going for a little while that
you know works because if you don't know
that your production one works comparing
a new one against it doesn't do you much
good
you partially answered this but let's
take another pass how often do you
update the sample set how long does it
take the sampling process take on 1.6
billion entries so I'm gonna answer the
second part of the question so on 1.6
billion I think we did about 100 shards
and it took about four hours so you
could basically just run it every day if
you want it but you can have more shards
as I said you know yeah more shards just
makes a little bit more overhead but
winds up taking less time because each
sampler is going over less information
and out of my own curiosity are you
putting the previously sampled data in
if you run it the next day you can yeah
it's like one of the inputs plus
whatever new days info yes regression
tests are good when they catch errors
before they're in production I suppose a
strategy cannot stand alone can you
please elaborate on balancing this
approach with scripted testing so
obviously you don't do just a be testing
hopefully you hopefully have unit tests
integration tests which is
pre-production tests but it's always a
good idea to also do kind of like
cannery which is what a be testing
usually is done for so yeah I agree
you should totally do pre-production
testing and one really cool thing that
this does is it actually
because we're running the miner anyway
we can actually tell you what a lot of
your education are so we might be able
to tell you a case that you didn't
previously think of and then you could
right go back and write unit tests for
those if that's something that you would
like to cover in a unit test how do you
choose a good sample again the features
are the key pieces of information here
if you don't have good features then
your sample is also not going to be good
so that's kind of the most important
part I think how is the feature set
reduced
feature set oh so we we have this miner
that we keep referring to and basically
what the miner does is it will go over
everything and just figure out what the
edge cases are for each field or if a
field even has edge cases so if you have
like a boolean field it might be true
false and then if it had if both things
are present we just will say okay that
we have full coverage on this when we
have a true and false because there's
really no edge cases whereas for an
integer we might have an upper bound
lower bound for a string we might just
have a list of possible strings
something like that so we basically
reduce it into just each field each
field has something that matters about
it and we're only concentrating on the
things that matter
how do you know the edge cases for
specific domains 0 and 2 to the end
might be edge cases for many scenarios
but one application might have educators
at normal numbers like 2 5 &amp;amp; 9 etc yeah
so this is kind of coming back to the
feature set again so right now we have
unary features which are just features
that you extract on a specific field by
itself without any relation to any other
field we also have binary features where
we capture the relationship between two
different fields but I basically stopped
at two cases because it becomes n square
if you do three it becomes n cubed so it
becomes really intractable very quickly
because as I mentioned some of these
products have thousands of fields and
it's just you can't deal with that kind
of data so this is actually a general
problem about data mining right so when
you're trying to do mining like Big Data
it's very important that you stop some
at some point before things get really
exponentially expensive because if you
try to do that in them so let's just
think about an ideal case so ideally
what you're doing here is you're
modeling the system that actually
consumes this data right so the actual
real representation is the system itself
the source code that you put in what we
are doing here is we are just
abstracting it away by extracting these
features using data mining on a lower
dimensional space let's say right and
you can only go so far and this is a
very valid case there might be cases
where for different fields are very
related when something is let's say five
the other one is supposed to be positive
and the other one is supposed to be true
right so there can be all these
relationships you can have like boolean
logic in your protocol buffers right
like if this field is set and this is
true or if this is the case that is the
case this implies the following right
you can have really complicated
structures but those are very hard to
mind basically so we ought we also
notably have a field that you can set
where instead of just doing ranges for
integers it will keep a list of all the
integers they've seen up to a certain
point so you can set that so ID by
default I think it's three so it will
only
remember up to three values before it
starts doing ranges but you can we've
tested with that as high as a thousand I
believe so little actually if there's a
thousand different integers that could
be possibly there it will keep track of
every single one and try and catch one
case of everyone obviously this grows
our sample but if this is something
you're interested in it is a setting
that we have
so yeah this gives like more discrete
values for your numeric fields as
opposed to over approximating very
quickly and related to that how do you
handle and reduce non numeric text data
so we don't have anything specific for
Strings yet there's research on
automatically mining regular expressions
for Strings we didn't implement that yet
but it's something we want to implement
so yeah and we do have the list the list
thing for Strings Dale just yes that's
that up strange and then we have like
boolean's we have oh this is always true
this is always false it can be either
true or false things like that and we
have null checks this is always now this
is never now have you considered using
these ideas to guide generative testing
libraries such as quick check yes so we
have a project inside Google that is
just starting to take these patterns
when that you find with data mining and
then automatically generate samples so
in some cases you can yeah you can just
go and take your production sample right
but in some other cases you can't do it
because maybe there's a personal
identifying information there it's like
user sensitive so you can't actually
take that in those cases you can just
take these patterns that you mine and
you can automatically generate valid
data right so this is another thing
we're working on on top of the miner
what is the specific criteria for adding
a new sample to the seed that we learn
new information meaning so it's very
simple you mine all of your data and
then you mind this one sample if the
parents that you learned say change even
slightly that means you learn something
new simple as that and we talked about
sampling strategy different sampling
strategies basically choose this
differently that's the major difference
between them it doesn't have a
performance impact search you might have
to enable debugging in the logs I mean
that happens before kind of we run this
tool on stuff right like many companies
just have request/response logging we
don't do it on all of our data we do
like 1% of the entire traffic for one
day for example it depends from team to
team but yeah obviously there's but I
mean this is done selectively right like
you don't run it on all the time you run
it on for like 10 minutes then you turn
it off and you know you do it over the
day so that you have a good uniform
sample and with that I'd like to thank
you thank you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>