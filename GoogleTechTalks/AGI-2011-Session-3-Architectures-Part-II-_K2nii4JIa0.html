<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AGI 2011: Session 3, Architectures Part II | Coder Coacher - Coaching Coders</title><meta content="AGI 2011: Session 3, Architectures Part II - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AGI 2011: Session 3, Architectures Part II</b></h2><h5 class="post__date">2011-09-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_K2nii4JIa0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to welcome you to our second
session on architectures the first
speaker will be Antonio della Tony o
quella thank you so good morning
everyone my talk will be about
problem-solving learning problem solving
skills from demonstration and this talk
is a part is related to earn a European
project humanoids humanoids that learn
social communicative skills by
observation in which my university the
university of Palermo is a partner the
prime contractor is the University of
reggae there are other people and also
in the Indian dis audience so this is a
brief summary of my presentation I will
talk about what does it mean for sensory
motor skill motors and what does he mean
to learn moodus from the low level
modest and high level model and I will
talk briefly about the problem solving
skills and I will preside the
implementation of our architecture in
the case of the well-known game sokoban
sake bomb is a very well-known game
there is also a long history between
soccer ban and hei there be many papers
also by a bomb and also if i remember
well also my goals about something about
Sokoban anyway someone is a is a nice
game so the basic block of our
architectures of our appetizer is our
the sensory motor skill and we
distinguish between forward in the
inverse models of sensory motor skills
while the forward model is a model we're
starting from the state I from the
action it generate it generated the
night
the next state why the inverse model is
a is a model that starting from the
action that the current state and the
the final state it generated at a
possible action so sexy so these models
are very simple modest of sensory motor
skills at the main goal of our
architecture is that this this killer
should be learned by the architecture so
what does what does we mean when we say
we talk about the learning skill model
there is the environmental there is some
attentional attentional control and we
extract pattern from the from the
environment by means of the perception
proposal and we asserted that the more
relevant patterns from the external
environment and we try to generalize in
turn there is a sort of a general over
there is a moody that try to generalize
this pattern in order to extract the
more general patterns related with the
current environment in the previous
section we were talking about a
compression of information this is a
kind of compression or information so
the system tries to suck but meaningful
partner from the external environment
and to generalize this kind of God the
the master plan with that you can see in
the lower part of this picture the
master plan is the sort is a knowledge
whole night by the system so it means
that the master plan at the beginning
starts with a very simple a priori
define at the scale model both forward
and inverse and it contains some kind of
a net goal and in the init fact and also
a very simple ontology describing the
relationship between entities in the
world
but for the system to be successful it
is necessary that these master Fung
should be very very poor very very
simple there should be some sort of
epigenetic knowledge as we can say that
the system that the system hold and add
the rice is the rice should be acquired
by the interaction between the system
and the environment this architecture is
very dry this kind of architecture is
very general and we try to apply we
actually applied it in the Indus aqua
banner in the soccer ball frame I don't
know if all of you know the song koban
and it's a very simple but in a very
very simple game in which there is the
actor the player should push the block
in order to put the Block in day your
own position so for example this the
agent cannot pull the block and so a
block can be a block at there so the
agent cannot push for example two blocks
all together if a block is so for
example the agent cannot push this blog
board and it's a very simple game but
it's very engaging so I spent many hours
of my flight from Palermo to took a to
San Francisco playing this document so
we want to look that we want to see some
learn this kind of skills and we had two
different kind of learning the low level
skill learning which is based on
essentially on motor bubbling or what we
call motor or what we call motor
bubbling it
each the season tries to to interact
with the environment in a random way
essentially so just like the baby board
size that tries to move their arms in a
run away so this is an example of let's
hope that everything is okay okay this
is an example of moto bombing the system
tries d for a very different strategies
by translation it randomly and try to
extract the patterns and to generalize
the scandal buttons while the hike level
skills its base it on imitation learning
we can say or learning by demonstration
what they say stir that if it is more
and more correct to say today eleven
skills are included by chains of a very
simple skills and they are learned by
observing actual so a bio serving people
playing this kind of soccer but in our
case we constrain it all our PhD
students to play soccer burn and we have
a long log of the game in this case
there are six military female and it's
learn and play the twelve trial on
average and we store at the all day all
the games and uses this log in order to
make the system learn the the hug level
skills after this face after the face of
learning the system is able to use the
to use this knowledge acquired by
essentially bye-bye bye-bye in
simulations in indicating in effect the
system is able to solve problems by
using these skills trying diamond
simulation so the system has acquired in
massillon a long list of a very simple
skills and in the situation of time that
the system is able to find that the more
relevant skill by simulating it in a
simulated environment and by decided
which kind of skin is useful it's useful
at the moment this is essentially the
main idea in all of the systems as I
said before the main block is the
anticipation and simulation manager the
anticipation the simulation man so the
season is a those some kind of planning
but the difference of line at the
difference in the season with respect to
the more classic AI planning is that all
these skills are learned and in the
combination of skills are learned by
interaction between the system and the
between the system any environment so
this is the main the main difference
between our system and the more general
assistance so let's see
okay this is a very simple game played
by circle you may notice that these are
the season move this block yeah I'm out
of times so the see Simon we can we can
see again in the last min sighs okay the
system is realized that is not possible
to move the yellow block because it is
blocked by the red block and then before
trying to block these this one move the
block on the under on the right so it is
a very simple form of anticipation in
the sense of the system realize that
it's not possible to move this blue one
block because it is blocked by another
block in the reins on science we
evaluate the system by using the same
people we use our essentially our PhD
students and they and it is a very
simple system evaluation of course but
it is initial last type which we asked
them how they consider that the game
played by by the by the systems and it
was such a good or very good especially
when the system was able to perform some
kind of anticipation so this is my this
is important in my opinion because it
seems that the character the kind of at
the anticipation is is a merry main
point for for for intelligence and in
the next type we would like to continue
in this in this kind of research
essentially by making the massive line
the more simpler the more simple at the
possible so the sister should be able to
learn everything from scratch and to by
interacting with by interacting with the
environment
okay thank you very much our next
speaker will be alessandro volta mari I
think it's this one is it yeah sure okay
thank you so in this talk oh I present
our paper concerning the extent and
extension of cognitive architectures
akhtar with ontology in the context of
visual intelligence system as this work
is a joint work with crystal appeared so
the problem enabling visual intelligence
so basically we are working inside this
minds I program the mai tai programs is
to develop the emachines visual
intelligence were visual intelligence
has to be conceived as the capability to
learn generally applicable in generative
representations of action between
objects in a scene directly from visual
inputs and then reason over those learn
representations with respect to
state-of-the-art machine vision visual
intelligence I want to do something more
I mean when you deal with the visual
intelligence in the contest odom of this
program might I program without trying
to go behind the let's say the state of
the art of object recognition feature
recognition but try to understand which
are the the semantics which is the
semantics behind the actions detected
not just combining the objects but try
to understand the relations between
those objects so the count of relation
between those objects in the sense we
are not just trying to focus on you know
the nouns that we recognize from the
objects in the scene but try to
understand the verse which we use to
describe in a narrative way the oral
action this means that we are not just
adding summing what we are seeing in the
single sequence of scenes but we are
trying to understand
something more so the kind of underlying
concept of structure in this sense for
example if we look at this scene this
could be described by entering to the
car getting in or this is like driving
or leaving the scene but then if you
consider the overall input sorry if you
consider a larger context like this of
course getting in driving leaving the
scene it cannot be together done don't
add up to a story so the context here is
very important very important to
understand which are the relations
between all the objects and the
different directions of the movement for
example in this case and and the concept
of structure behind it so of course this
is a probability this is a complex
problem and we are trying to get a to
propose a solution at eye level okay so
try to combine both a cognitive
mechanisms and semantic context which is
the the framework of these extension of
the cognitive architecture akhtar with
ontologies what does it mean combining
mechanism plus content so if you
consider human visual intelligence as
where background knowledge makes the
difference when dealing with the complex
cognitive tasks like for example actual
recognition in this ambition as
individual that we have seen in order to
get these human level visual
intelligence for artificial system we
think that integrated the knowledge
systems need to be developed in these
systems focus on mechanism of human
cognition like for example control or
learning memory perception and in this
sense we are using attar as the
architecture with focusing on the
mechanism and in concerning content you
need kind of knowledge resources
knowledge representation
like for example oncologists in this
sense you can think of ontology not only
in terms of computational oncologists
like open psychosis humor and so on but
also like computational lexicons which
are very popular like war net and free
net and sown in particular so in the in
the framework of combining mechanism
cognitive mechanisms with semantic
context we focus on actor mechanism and
the so-called M&amp;amp;A ontology which stands
for hybrid ontology for the mind's eye
program in this sense the integration
between these two let's say knowledge
systems a different level of nodes of
course this integration helps to
pinpoint the main roles played by agents
and objects in actions so in the in the
scenes for example disambiguating the
lexical meanings or the verbs and nouns
for the tactile actions recognize the
cognitive patterns of action we are the
temple and conceptual components for
example so we cognitive pattern we mean
like something like the sequence I don't
know walking bending over lower they are
picking up an object and bending and so
on and so forth so sequences like that
that compose for example the pattern
pick up or lift so in order to have an
instance of a pickup event you need a
sequence both temporal conceptual of
specific temporal of offices specific
components okay and so in order to
understand if a specific video like the
one that we have seen evokes or not a
specific pattern we need to look for the
components that compose that pattern and
in order to look for those components
for example we can use a mechanism that
are embedded in akhtar cognitive
architecture like the base level
activation waiting the relevancy of
components
some kind of partial matching like
providing similarity measures between
components for example if you so there
can be from the somatic viewpoint some
verbs are similar and some other are
very different so the fact that if you
see something like lifting so if the
system recognize something like lifting
is similar to pick up or put down is
completely different from pick up and so
on so forth spinning of activation so
assessing the specificity of component
in in context this means that if you
consider patterns of actions like pick
up Barry holding or put down what else
chasing and so on some of these embed
movements by the agent which are common
which are shared okay like walking is a
movement which is very common and can
can occur in different patterns and
spinning of activations means that when
you have a detection of a specific
component this component can activate
different patterns depending on the
importance of the component for the
specific context it would be activated a
pattern more than another and from the
viewpoint of the same knowledge
representation this integration apps to
reason over the logical constraint
underlying actions for example we are
using here non Tala G so hominid
ontology has been implemented in all wl2
so having caught up in negativity is a
kind of description logics so zooming in
you can think about the input so we have
a kind of low-level processing so visual
classifiers that give us as input kind
of object actions and and so called
micro actions which are these are small
components of general actions like for
example we get input like person steps
forward from t1 to t2 person bands
over from 22 23 and so on and so forth
and the output so the recognition of the
action would be like person picks up
hammer okay from T four to six so the
cognitive part this activation of
patterns try to combine together the
different input and to reason overs kind
of high-level a more abstract verbal for
describing the action and basically what
we are doing is so this processing of
the he put a car using a car so like for
example the base level activation is
based on a frequency data from the
American national corpus this means that
all these specific words that describe
micro actions have an activation in the
actor cognitive architecture which is
based on the frequency of these verbs in
in a corpus the partial matching based
on similarity between components and
again this similarity is between verbs
and nouns and similarity is based in
this sense from gross factor measures
and spinning of activation I already
said something about that homina is a
based on framenet foundational ontology
which is called which is Dolce the
stands for the that's quitting ontology
for linguistic OT of engineering and
some temporal reasoning this is what the
example an example you can do can focus
on so you have like a Pew one person one
enter the scene person one is in the
track the track exit left and the person
to run left it is the description of the
video we saw before so together these
these different micro actions can
activate the cognitive pattern of living
in the case of person 1 enter and person
one exit in the car and chasing when
person to enter the scene and run left
which is the same direction in which the
person one in the car is the exiting is
living this is a kind of description a
graphical description of the time course
of activation
so you see that after the first micro
action so after the first component
there is a good activation of enter okay
so p1 enter the scene then when p1 is in
the track you have kind of activation of
of other patterns and at the end of the
4 micro actions adult of the fourth my
correction you see chase as a general
Mack reverb activated concerning the
ontological knowledge just to mention
that you can reason so if the if the
person is entered into the scene and
then the track exit left and the person
is close to the track you can reason
over some kind of a characteristic like
this that p1 pulled the door proceed p 1
is in the truck even though you don't
have any input concerning the pulling of
the door you can reason about that that
you can infer that kind of knowledge so
future work we are working on improving
the interaction with the visual
classifiers in order to provide the top
down into to refine processing we are
trying to work on improving automatic
inferences of relevant features of the
action structures and of course we are
trying to expand the kind of actions
that we are able to recognize right now
with this integrated knowledge system
for augmented the power of the
recognition in the complex activities
okay thank you these are the references
next let's welcome mr. and Miss group I
hope this one is the correct
hi everyone so I'm going to be talking
about some work we have been doing with
integrating sort of state of their
perception with cognitive architectures
this is work done with Cristiano beer
and Tony stance at CMU all right so how
can cognitive architectures support
perception and I'm coming from a high
level cognitive perspective and
generally what you find is cognitive
architectures are good at one of two
things usually high level cognition as
the name suggests in some cases like
epic it's tuned towards perception but
um usually one of the one or the other
they just you know assume facilities are
available and then work with what are
called the products of perception so
what we are looking at is how ok picture
such as act are actually help improve
perception and there are at least three
ways that we're looking at one is
obviously attention you know would we
look next what do I look at what is the
context right right so this is a street
scene and we know that there are people
going there are going to be people on
the sidewalks they're going to be cars
on the road and we can use this
information to actually make perception
better in fact you can see that this is
the output of the state-of-the-art
person detection on the scene and it
detects a couple of people it detects a
couple of things that are not people and
mrs. are one or two people and
information about context like people
don't usually float in the air can help
cut down on these false positives and
then finally we refine the output in
various ways so what I'm going to be
talking about and this talk is the
refinement process using akhtar so
here's a simple example we call it the
checkpoint scenario essentially it's a
check point a B and C or three vehicles
waiting at a checkpoint the checkpoint
itself is not shown and people when they
start they wait at the checkpoint they
get bored they get out of the car they
stand around they walk towards other
cars probably to talk to people and then
they walk back to their car
are they keep walking the goal is to
make predictions about what people are
going to do and eventually when they're
going to do it and detect anomalous
actions in this particular scenario
there's only a single anomalous action
which is fleeing you can guess where our
money comes from people get out of the
car and just keep walking and they don't
you know look back or turn around or
whatever um so what is the input to our
high level cognitive model um it is some
ID vector right now we're using at least
for this paper it's just an ID that an
ID but we've also started using color
and height and eventually even post you
know depending on how fast we want our
system to run there's also a location
information coming in just to mix y
coordinate a word this particular IDs to
be found in this in the scene and both
of these things have errors associated
with them um there are location errors
some Delta error is there always and
there is a one in ten chance that the ID
that is given is not the right ID for
the object so what the model does is
that given this information coming from
perception about an ID and an X Y it
uses that to retrieve from its memory on
what's called an object information
chunk again a chunk in the act our sense
is just a frame which contains value
pairs and in this case this particular
case the frame has the ID information X
Y Delta values on how x and y are
changing and an action slot so it uses
that information to retrieve this this
infamous object chunk and then updates
this junk with the new information it
has about the new X file locations for
this object an action is always based on
the rate of change of X Y for the
scenario so if somebody is standing the
rate of change is very low if they're
walking it's fast and they're kind of
standing around it it bears a little so
and and then the P part of the model is
how does information is
from memory and there are two parts of
Akhtar that I'm going to be focusing on
which is partial matching and blending
and I'll get to both of them in a moment
I just want to say that in general in
akhtar when you retrieve something from
memory use this fight certain
constraints you say I want a chunk of
this type and all I know are the ID
value and let's say the X and the y
value and Akhtar will go through all the
chunks on see which ones match exactly
and from those that match exactly it
picks stone with the highest activation
value and what act how the activation
values calculated etc is in the paper
and in akhtar literature but what is
what happens when you include partial
matching and blending what partial
matching does is it retrieves chunks not
it does not just retrieve chance that
match exactly it retrieves chunks that
match to the greatest degree so there is
a similarity calculation that happens
that we can specify and it calculates
that and not retrieves chunks that are
similar not just those that exactly
match and what blending does is given
those chunks that have been retrieved
instead of picking one that has the
highest activation it generalizes them
essentially avoided averaging depending
on in the last time the activation
values and things like that so so here's
an example I request a chunk from memory
because i have from perception
information that this object a is that
10 comma 10 and you can see memory has
about six junks three of the chunks are
about B B's and 9 10 10 10 11 10 and
then a is at 100 hundred 10 201 or 400
what partial matching does is it looks
at what I asked for which is an object
with the ID a and says look I know you
asked for a but my similarity
calculations tell me that the chunks
were be actually matched this better
than it matches it just because you know
the values of x and y are very different
and blending blends all of that
information those three chunks into the
a 10.3 40 comma 10 and the reason why it
is 10.34 not 10 exactly is because it's
weighted averaging in certain chunks
have higher weights alright and so
that's how kind of it deals with errors
in identifications so for example in a
if you say this it's this object a at 10
comma 10 it says well we think its
object be actually there are also a set
of actions are these they're perceived
actions which are straight forward you
know somebody's in the car they're
standing around the car there walking
you can be more specific and say they're
standing around this particular car
they're walking from car a two-car be
and you can aggregate it that these
actions into sequences and say this
person started out by being in the cars
and they got out they walk to this
particular car and came back and that
can actually be used to predict
sequences of actions for other people
again the anomalies are the flame
behavior so here are a couple of class
that graphs that show the results in the
the top graph shows the the actual path
and the predicted path for objects so
that the blue line is the actual path
coming from reception and the red line
is the predicted threat for an object
and you can see their match fairly close
to the x predicted path what I mean is
when when and when an object information
comes in you can use the delta x and y 2
actually predict where the object is
going to be next and that is what is
retrieved from memory when you ask for a
particular chunk and the bottom graph
actually shows the percentage correct of
predicted actions at each step and you
can see this is this is simply the
output of blending and the the drop-offs
you see our when actions change from you
know somebody's in the car and they
start walking um this is better than a
simpler prediction based on hey what the
person is going to continue doing what
they did earlier on the problem with
that is the delta x and y values change
a lot they fluctuate and it
hard to predict so I unfortunately i
don't have the graph of that but they go
up and down a lot and what blending does
is it smooths over all that and gives a
slightly better result so so that's what
we have done so far and part of the work
that we are doing now is you know using
real-world data so the data we were
using word from simulations we are
working on real-world data like the
example I showed you in the first slide
we are working on adding context to our
systems so actually having the the
cognitive architecture suggests
perception or to look for objects um the
the two pictures in the middle they're
not from our work but from one of our
collaborators but you can see that the
system is actually predicting these are
where people are going to be are going
to be found and it predicts both it uses
both the geometry of the scene as well
as information about the height of
people in the scene and one of the other
things we are doing is actually predict
because we know where people are going
to be we are predicting that as well and
suggesting to perception hey this is
where this object is going to be next
and that's it so there's some references
and this money comes from robotics again
this is why I use a Meg
okay next up is Benjamin joins them okay
I'm from the university of technology
sydney and this is really an ideas paper
about how you might collect and use
large-scale data bases of physical
knowledge my research interest is really
in creating practical artificial
intelligence and specifically practical
artificial common sense and i'm trying
to build artificial intelligence so i'm
not necessarily constrained by
biological plausibility or human-like
intelligence as I see it I my philosophy
is that the human brain and computers
have very very different machinery so
there's no reason to expect that a
computer-based intelligence should
operate in any way like a human
intelligence and I've been working from
the assumption that we understand
computers pretty well and the question
for us is really well how we put
together what we already know about
computers in such a way that
intelligence and mergers and so this
kind of pitcher is sort of inspirational
I'm working from how is it that we as
people can understand this image I mean
it's it's clearly a chair it's also you
could say it's a hand and so it's like a
fusion of these two concepts but it
isn't clearly it isn't a prototypical
chair and it isn't a prototypical hand
it doesn't really fit without with it
doesn't fit neatly into any particular
concept so it doesn't so traditional
methods of AI don't really work well
with this ah here's another example the
Chinese word for minivan happens to be a
bread loaf vehicle and as soon as we can
sort of see where this comes from it
looks like a bread loaf and we could
sort of see how there's this termite
have developed in the Chinese language
but how is that we understand this I
mean bread loaf vehicle and minivan are
very different concepts
and if we think about a bread loaf its
defining characteristic isn't its shape
I mean a bread loaf is really it's
something that's made of bread and it
serves more than one person the shape
isn't a crucial part of it so my
question then is well how do we deal
with how we do with this world where
words like chair in hand have vague
meanings or how do we deal with concepts
when they're used in ways that seem to
violate their fundamental that they're
they're defining principles and so as I
said my research is about practical
artificial intelligence and so what's a
practical solution to this well I look
out look out the the techniques used in
there in computer science and see that
data-driven intelligence so data-driven
a is proving to be a quite a powerful
solution to some of these challenges of
vague and uncertain data if you seems
that if you have enough data and a
fairly simple algorithm you can get some
pretty impressive results so while IBM
Watson made some stupid mistakes it also
from time to time displayed a sparks of
intelligence being in that other popular
web search engine from time to time show
quite uncanny levels of intelligence and
answering difficult questions and then
concept net is a fairly powerful common
sense reasoning system built from a
large numbers of volunteer contributions
and it does useful reasoning and it's
also being used to bootstrap more deep
and intelligent forms of arm deepen more
powerful forms of common sense reasoning
so our perhaps data-driven intelligent
perhaps dodge driven a I can help us in
this physical reasoning problem and
while the purists in the room might
argue well this is not really the way we
should be going I think it's a very
practical solution to the problem of
doing very useful deeper useful
reasoning in the real world and it can
help bootstrap towards are more powerful
forms of artificial intelligence
okay so then how would we given that
we're going to use a given that we're
going to try and follow a data-driven
approach to physical reasoning what
representations would we use and how
would we collect this data in in
previous previous AGA conferences I've
demonstrated I presented a an
architecture that combines logical
reasoning with physical simulation and
of course I'd like to continue using
those rich physical simulations I've
shown here before but um but I must
pragmatically admit that they're a bit
too complicated for this task at hand
they're too rich and we really need
something simple as something as simple
as possible in order to be able to
pragmatically build these large
knowledge bases of physical common sense
so what we want we want a representation
that simple to store and analyze it
needs to be very computable if we're
going to be dealing with large databases
it needs to be simple for people to
understand simple so that people can
contribute these models and also so that
we as developers can embed them into our
AG AI systems and exploit it as much as
possible it should work with existing
AGI systems and existing game engines
and it should be useful I mean we're
doing this for a reason it's not just a
theoretical exercise so ah what I think
really fits this bill is voxel-based
representations imagine if we had a you
could imagine if we had a huge database
of Lego objects objects in the real
world built out of Lego so if we if the
system wanted to know what a house or a
car is like it can just retrieve one of
its many millions of objects we've got a
motorcycle here and either these are
simple representations but they capture
some of the essential properties of
these objects and that this is really
comes to the point of this project this
paper sorry I've built an online tool so
that to invite the public to contribute
models that might be used in art
general intelligence so it's basically a
3d voxel editor and we've got on the
right hand side here a 3d viewer and the
left hand side we've got a 2d editing
surface so I can start by building a
model of a chair here
this mass is not working very well so
you can see I've created the base here
and then we can work our way up through
layers so it kind of works like a 3d
printer if you want to view it that way
so we just build up the layers build
another layer let's give it a seat
I'll ignore that mess back rest and so
it only takes a minute or two to just
add another model to this database of
physical objects and on the way we can
annotate it with knowledge so we can
connect it up to we can connect it up to
textual representation so we can say
that this part of the objects of back
rest and then we see in the notes being
added there we can see how our 3d object
as we've created it so in my experience
this is a kind of fun and useful
interface it's a quick way of collecting
large amounts of data that could be
useful and it it's related to some of
the it can be used in other systems like
been goats or presented a voxel based
representation a voxel-based 3d
environment earlier and this could be
plugged into it it's kind of related to
Minecraft and went wit and so far I've
got a number of submissions they've all
seemed to be quite creative it hasn't
been much yet but then I imagine well
what could we do if we had millions and
millions of these are these 3d objects
in the system and I'll just do a simpler
example referring back to this bread
loaf vehicle so faced with this question
of how do we interpret what's a bread
bread loaf vehicle the system might pull
up its model of a bread loaf and it
might pull up all the models of vehicles
at nose and it can just work through
them and compare how similar they are so
we see that the the minivan the red
minivan happens to have the smallest
error 143 so we'd say using this
similarity measure the minivan is most
similar to a loaf of bread it could also
be used in other ways for example we
could pull up all of our bread loaves
all about minivans and then ask well in
what way are they similar and so a a
color based similarity it's not very
similar but a size and comparing the
size there's there's very little
similarity but if we do a scale
invariant shape comparison like I've
done here we see
that they happen to be very similar and
so this can help make sense of these
physical reasoning problems so this is
really a sort of small and quick example
of what what what's possible and sis are
it's kind of a vision of what might come
we don't really know what we can achieve
out of this until I've collected a we
collected a large database and started
putting it into more serious and big
problems but I hope that gives you an
idea of some of the possibility so
you're welcome to contribute if you'd
like to I'll just go to WM / objects and
feel free to add a few models to help
contribute to artificial general
intelligence everyone has to take off to
say that they will release their work to
the public domain so I'll be able to
share this work to everyone when it's
complete hey thank you i would like to
invite our speakers of this session for
the panel and if you have questions I
would like to ask you to line up again
because i think this method has worked
very well we can give you microphones
then and everybody in the room can hear
your question please come to the front
I guess this question is for the guys
with Akhtar I i have read a little bit
of literature by fellow named jeff
zack's at Washington University on what
he calls event segmentation and I just
wondered if you guys were aware of that
or if you do something similar it's
basically something like humans watch a
video and they're asked to segment the
video into events and his basic thesis
is that when a perceptual prediction
fails it signals an event boundary and
you need a new event or new model to
predict I am not aware of that work but
that is exactly what we are doing so we
have these predictions of what actions
somebody is going to do when they
deviate that's when we know that
something is changed and so that's when
one action ends and another begins so
when you detect a failure is there a
special I guess switch or something that
because in his model they're sort of
like a gate where the model is sort of
protected from change until there's a
violation its predictions and at which
point there are gate opens and allows a
new model to be formed um no but I think
that's partly because of actors memory
um so multiple models can exist and
whatever is highly activated is what is
retrieved at least and that's my so
something new is highly activated and
yes and then that will come in okay
something new and something that has
been retrieved many times is highly
activated and that's what we use okay
yeah concerning our model we provide the
top down kind of ontology-based the
sequence of those components for events
recognition so basically these cognitive
patterns are provided top down and are
used to you know try to detect the
overall sequence in the event and as
as when we say that then we use other
mechanisms in order to if you were doing
it in a very high dimensional input like
a video how would you represent a model
of a person or how much you do that do
you mean with the model of a person um
like all the movements of actions or
something yeah I mean how do you project
what that person will be doing in the
next few seconds yeah well in terms of a
representation okay well basically to to
make these projection and prediction we
use we exploit these patterns so should
you post that you see a person moving
alike walking okay and then maybe
extending the arm the model look for in
the declarative memory look for a
pattern which is constituted by those
two elements and try to understand which
are the the next components in the
various patterns and try to do it to do
it like using spinning of activation and
something like that so it's not just
it's not really a prediction but it's a
kind of you know comparison between the
available patterns which have been
provided by the ontology to them all
right thank you thank you start a
question about the sokoban so you may
you made an assumption that you have a
very good simulation of the game and
then you can you can bootstrap
anticipation from utilizing that
simulation I had a question about maybe
an extension could you also in parallel
learn the simulation model while
learning the anticipate or moves so
maybe through body battling or something
along those line yes if the system is
able to learn by concept by analyzing
other other player the simulation is
used when the system have two to choose
which kind of action do in order to
solve the so Kabam program so the system
try to
assimilate which kind of action and try
to individuated the action there is a
sort of aristocrat is inside the water
that I call the master plan in order to
evaluate these in the independence in
the code implementation these arista
killin by the designer what we want in
the next in next year's one that this
kind of risk should be learned by the by
the Bell assistance and that leads to a
follow-up question and you mentioned
that future work you'd might look at
like you have a scene you make a non
thoughts of where people might be but
you're not going to do would you do
feature extraction of where the
sidewalks are independently and then
you'd say okay it seems like that's a
that's a feedback you you'd say okay I
think these are sidewalks but if they
were then there would be people on them
and I think those are people because
those so maybe you can talk to a little
bit about how that would work um yeah
actually part of that is so one of the
people we collaborate with is works in
robot vision and so they have these
systems that actually do they they they
have an idea of the surface geometry but
they use evidence from where people have
been detected to actually improve that
and then so there is a feedback loop
that they use unfortunately i am not
familiar with the math but i can give
you like references but so for both of
your work its binary you say yes there's
an entrepreneur there's a person there
with probability one or it's not in
probabilistic 0 the output of perception
is a confidence Oh Callie okay oh yeah
and that we use that to you know why is
our decisions okay thanks hi dr.
Cassandra a very Inc it your talk yeah I
have the two questions one question is
what you're so real meaning is a hybrid
ontology what is a hybrid is a real yeah
i'm using hybrid instance that we are
combining
different resources both from the
viewpoint of ontology so in a sense of
formal anthologies like we are expanding
these dolce ontology with some kind of
domain ontology forever to me but we are
also extending the computational
ontology using warnet and frame it so in
this sense i'm i'm using i breathe
instead we are combining different
semantic resources and so this is the so
you all look for lose would need or use
the standard auto Briere you're like
this yeah the mainstream in your hybrid
ontology okay so basically the this
hominid ontology is constituted by a top
layer which is the ontology implemented
in Oh WL in ontology web language and we
expand the top-level ontology with
warnet and framenet of course translated
into owel okay okay so basically the
ontology is formalized in 0 WL but is
composed by both let's say general
formal distinctions like object event
quality agent and so on and populated
with lexical meanings with warnet and
conceptual frames from Frimet everything
in a hybrid resource listen okay second
question is I also look at the head deal
talk is you also use a formal logic to
describe the hybrid oncology what kind
of logic is you use it for this purpose
well as I said we are using a the
ontology web language which is a
description logics oh yeah he has a
formal description yeah yeah what what
kind of that he has a tours yeah yeah
describe your hybrid ontology yeah I
said I bid ontology is coded encoded in
the ontology web language which
is a description logics so the
expressivity of the language is
description logics okay okay thank you
very much thank you hello I have a
question to the last talked actually two
technical questions one is whether you
provide the editor and the viewers open
source and the other is related is
whether you considered plugging a depth
camera or something like connect to it
getting the point clouds which are like
voxels and then just clustering it to
decrease the resolution and getting the
models just from rotating a camera
around something um well twenty first
question on the editor is well it's all
JavaScript so you can look at it the
selassie so it's not great code but the
point isn't really the editor it's the
models that's that it creates and all
the models are going to be public domain
you have to take a book saying releases
to the public domain as for collecting
point clouds that that's certainly a one
thing I was considering well you could
have your good every robot explore the
world and and in autonomous and click
models or even just give it given models
in a controlled environment and that
that's tempting but it limits you to the
kind of things that you can just you can
you can show so you couldn't you
couldn't show it you couldn't model a
house or a skyscraper it also means that
a the advantage of this is that it's
just simple so we can just create many
many models and then add text your tags
on to it so it's trying to find a
compromise between rich detail and
something that's simple and easily
manipulated so I'm going for the
simplicity so what I meant is whether
you could combine like to take a model
from outside and then still edit it in
that's so many possibility here I just
realized that you are also implicitly
capturing our intuitions about objects
you know we feel these are the important
parts of the object which is something
you would not get from just getting the
point cloud and by by using different
colors you
you're basically providing a
segmentation to it as well yeah did you
consider plugging your designer into
something like Minecraft I have
considered but it's I haven't explored
that at all okay so I think you have a
big difference between the two
architecture sessions in some way
because the first architecture session
was more about how people wanted to do
that this is what kind of architecture
their proposed here it's more like what
people want to do with it that is they
present their work starting from tasks
and my question would be do you think it
would be worthwhile to open up the task
sets to a more general audience for
instance make the data public make the
task itself public turn it into
challenge problems so for instance
different approaches can compete with
different architectures in order to
identify people in street scenery or to
play sokoban okay yes definitely should
be a good point and in my opinion all
these time 32 2000 sort of benchmark in
the high in the story of artesia
intelligence I couldn't has not grown so
much actually because typically you
first start with the architecture and
then you try the taste that wealth that
sweet well with your with your
architecture however i can say that one
point should be to try to have some sort
of em instead of putting the tastes and
say say my architecture perform so in so
you architecture performance so and so
and my my impression is that what is
missing in a GI and autonomy GI it is a
sort of software and more software
engineering approach according to which
i may use your modules for my
architectures and you can you can reuse
yourself so I remember that
for example when in the engineering some
years ago in Robocop the rules a
standard the hockey teacher which would
which was the the dogs for sony and a
and old people develop it's also for
that kind of robot architecture and then
there was a fast-growing a kind of
general knowledge it might be what is
mrs. are not the ties but this is a sort
of a standard framework in which people
can we use can reuse software and that
is what is needed is a sort of more
serious software software engineering
approach to AGI this is my point okay
but we do have a lot of engineering
approaches i know of about a dozen true
kids at least and i only knows a small
minority which are open to the general
public that people can contribute that
are used by various groups and so on in
which people try to promote so it might
not be actually the lack of proposals
but the thing is that there is just no
reason to use a particular tool kit if
it doesn't help you to do a particular
task better so in robocop simulation
league you have several frameworks which
people tend to reuse but it's only
because it helps them to do the task
better yeah that's true of course I
don't know yes and the center there
could be a lot of a lot of a possible a
lot of possible task of courses so I
think in the short term yes but in the
long term the advantages to a gif having
competitions is unclear I mean and the
part of it is only because of the
difficulty in defining tasks so for
example if you take let's say a
benchmark as modeling a three-year-old
the problem is that the most interesting
thing about a three year old is that in
10 years will be a teenager right it's
not what they can do now it's in 10
years they can do even more and sort of
capturing that as a test
is is hard I mean it somebody has to
work on it you should look at the
youtube video of Star Wars explained by
the three-year-old I'm not sure if
that's really embed bigger progression
after that I think she can do it all
yeah i think that the one of the so that
the point of opening the data sets and
also trying to evaluate the task is very
important and for example in the context
of this mind's eye program where the
data set are not open of course because
are not funded there are different teams
that compete in trying to solve the
general problem of deviant recognition
right so we are evaluated with respect
to those kind of standards for the for
the event recognition and but the
problem is is not just I mean if you
want to evaluate and this is a general
commented I add also from last days if
you want to evaluate or try to compare
different AGI systems you need to be
able also to map the different
architectures in my in my in my view and
not just at the level of mechanism as I
try to point out in my talk but also add
a level of representation because many
people talk about you know Universal
mental representation may be you or
neuro neurocognitive representation
mental representation but when you try
to understand the semantics behind it
how do you do so is if you want to
evaluate the real power of the
representations you need also some kind
of mapping between the representation so
yeah these are my comments on the rhythm
I think maybe some challenge problems
can provide such mappings we have seen
this excellent example by Benjamin which
harnesses the power of the Internet's to
make people collaborate on things like
this now imagine you have something like
a street people recognition task you
make data set public and it's one of
many challenges which are available on
the on a challenge website and people
can just get in there by a standard API
you use whatever solutions i have and
have the grounding wire the data because
they have to fulfilled
and you can have something like an
automated competition yes angry 11 wore
this if a for example as sometimes ago I
was involved with the in Denise the
competition of recognition of that data
of print and other site of written to
the side and at the at the end of the
story the winner strategy is to try to
find a bag of tricks are intelligent bag
of tricks that it is a very ape that
able to solve this very special kind of
approach there are no artificial general
intention very intelligent bag of tricks
completely focused toward these toward
the problem so this is in my opinion the
risk arriving some kind of tie slit I
know people probably want to go for
lunch but so I'll be quick but I still
got a few minutes okay I've been
thinking quite a lot about requirements
for simulated environments and also real
environments to produce appropriate
challenges for the goals that I'm
interested in and I know that they're
not necessarily the same as everybody
else's I'm primarily trying to
understand myself and that includes
understanding my evolutionary precursors
and that includes understanding some of
the things that branched out from I
evolutionary precursors and I think that
one of the features of the space in
which I and my self age one and two and
whatever and other organisms inhabit is
that it is not much like the kinds of
things that people tend to build into
their AI testbed for instance tile world
where you can only move in for all
directions or Lego which is fine i mean
young children do play with Lego and so
on but if you're interested in processes
well with the becomes a question are you
going to include the process of
constructing the lego thing if you do
then you're getting closer to the world
in which
positive where there are processes going
on all the time and in fact one of the
things that's wrong with almost all
research in vision is that it assumes
that the basic things to identify
something static an object or a
relationship whereas most of what we see
and most of what was most animals er
processes and so if you want to put
processes in then that's going to change
the data structures almost certainly
because I suspect there are things that
going on during the editing process
which aren't there in the simulated
world like the block moving from here to
there and all the intermediate positions
which don't fit into a kind of grid like
structure anyway last point about it is
that if you want to look at processes if
you think of the processes in terms of
how objects as we think of bounded
relatively rigid identity preserving
things change their relationships and
their positions that's much too narrow
because we need processes in which
things are bending things in process in
which things come into relationships
where the parts of the objects are
important so the fact that it's your
fingertips that are trying to grab the
pin that you're picking up is important
rather than that your hand is trying to
pick it up and if you don't know that as
your fingertips then you won't be able
to do debug things that go wrong so I
think we need to find what the
underlying fragments are fragments of
process and fragments of structure and
what's the fright of fragments of change
are and if we can build those into a
nice tool kit then of course we have the
further problem where how they're going
to be represented by a perceiver and
that's another bag of worms as shown and
I'll just leave this as a sort by how we
see impossible objects like the Penrose
triangle and the rotas von triangle
they're clues that human vision system
does not produce 3d models of what's out
there producing something much more
sophisticated in abstract ah I've very
much agree with what you just said um
and but I'll just take it in a more
narrow implied criticism matter I'm
collecting only static objects and I I
reaction to that in just the short term
is really that I think that with enough
enough physical models you
you start capturing some of these
dynamics so you could have a car than a
broken car and a car that smashed and a
car that started and and a person in a
car and you can see that the person is
connected to the steering wheel and so
on so I mean there's a small step
towards this longer vision I also make
share with you okay we have time for a
final question but if you want to remark
it which is something Muhammad which was
that you know there's not it seems like
they're even multiple representations
because and there's this disconnect
between for example some of my you know
some concepts are our idea of
representations of objects are even
platonic in a way but at the same time
when you're working in the real world
trying to pick something up if that
doesn't work anymore like and we seem to
have both of these at the same time
without conflict my response to this
idea of creating benchmarks is well a
lot of benchmarks do already exist
especially in the machine learning
community for things like image
segmentation and I didn't look up the
website i was thinking of before coming
of you but there are large collections
of different problems and then rankings
of different algorithms on these
problems and that sort of thing so I'm
my question is what do you think is
still missing from that sort of thing I
think so essentially what's missing is
that the point but you know it's not
about simulating a three-year-old that
the interesting fact is you know down
the line they're going to be even better
and even for age closer to like symbolic
data sets there is like the laminate
game there's the test that goes on
personal dilemma their competitions that
go on for those things but if we if you
can define a problem well you can create
anything you can create something to
solve it and at that point it's less
interesting it's the vague problems the
problems that have not been defined
solving that is the interesting part so
in if I think of a new problem and tell
you about that problem you can solve it
and that's I think that's the
interesting part in this defining it
seems hard oh sorry Marco wanted the
problem one of the problem is that we
want to capture things that are
difficult to describe in terms of the
decide for example creativity which kind
of data cited do you propose for trying
create DVD in assistance or officer or
something like this so we might be in Ag
I want to capture things that are not
captured that are not easy to describe
in terms of pattern matching or data
classifications last year I had a paper
on a benchmark problem that tries to
capture what I thought was a deficiency
in using a you know data sets and the
the problem was to give a robot at a box
of clothes box of toys it doesn't know
about and it's got to spend time
exploring and playing with those toys
and then after that answer questions
about those toys like what happen would
happen to this toy if I did this to it
or maybe it has to put the toys back
into a toy box and so that that was my
attempt to try and capture this need to
have a controlled situation like a bench
as a benchmark but also allow for
uncertainty and this need to build new
models of things like it's never seen a
it's never seen an iphone before so it
has to figure out how to play with it
it's never it's never seen a rubber ball
it's only seen steel balls before so it
has to build a mall of what's rub
earnest and so on I could have edged in
something like imagine you have seen the
breadbox vehicle you have seen the bread
you have seen the vehicle you have seen
a computer come up with an
interpretation or addition of a breadbox
computer I know I had one back in the
day but okay I do thank you very very
much for your contributions today thanks
for the panel discussion was very
interesting to me and for the nice talks
in the session and have a good lunch and
see you after the ranch</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>