<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Privacy Preserving DataMining | Coder Coacher - Coaching Coders</title><meta content="Privacy Preserving DataMining - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Privacy Preserving DataMining</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0jLI0NiL_HY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let me introduce Matt Logan to you met
and I know each other for about five
years I first learned about his work
through the papers and then I met him a
number of times at different conferences
he was working at AT&amp;amp;T back then and I
was always amazed how Matt is finding
all different applications of
mathematics to networking that people
before could not even imagine yeah and
right now we met to move to his home
city and his home school and work for
University of Adelaide but I hope he
will be coming more frequently to us so
we can perhaps find some common topic
for collaboration and in the
anticipation of that I invited him to
give this talk about his current work
today which is published this talk will
be on Google video so you can download
it later and Matt kindly agreed to give
it to general public for our google
video database ok afternoon thanks for
having me here
apologies if I seem a bit vague I just
got off a 24 hour plane flight a couple
of hours ago so I'd probably go home and
sleep after this I'm gonna talk about
privacy preserving data mining in
particular its application to network
measurements privacy preserving data
mining has been around for a couple of
years now so it's not the algorithms
that I'm going to be presenting that
anew it's primarily the application as
Andre said I've been working I come from
a math background but I've been working
with companies like AT&amp;amp;T for a number of
years now and we've really been doing
internet measurements trying to get
things out of the internet find out
things about what's going on primarily
from a carrier point of view to be
honest so perhaps some of the things I'm
going to be saying aren't particularly
targeted towards the sort of things that
Google does more towards sort of
understanding carrier performance
carrier track but hopefully some of the
ideas will be sort of in common so one
of the things that's something that I've
come to believe very strongly is data is
really the key to understanding how
networks work
you know back when I was doing my PhD
some number of years ago now we sat in
little rooms and we wrote up equations
on blackboards and we built models we
never really looked at the network and
that was a really bad idea these days
you start with data you try and work out
what the data is telling you and then
you start building models trying to
understand what's going on you really
need this data if you want to do any of
the sorts of networking tasks that I've
had to do things like traffic
engineering in particular you need to
have good data the data tells you things
that you didn't know before the other
big change in perspective for me over
the last few years is as Andre said I
left AT&amp;amp;T and I went back to work at the
University I left a lovely big data
source AT&amp;amp;T had wonderful internet
measurement infrastructure part of which
I helped them build and I went away and
I don't have that data anymore and this
can be a bit frustrating but it's not a
situation that is unusual most people in
universities don't have quite the access
to data that people working for
companies - but one of the things that
I've noticed is that even working for
somewhere like AT&amp;amp;T you end up with an
AT&amp;amp;T centric view of the universe
you get to see AT&amp;amp;T data you don't get
to see Sprint though you don't get to
see you know you unit data and this
gives you a very sort of one-sided view
of how networks have run what sort of
traffic they carry what sort of
characteristics those things have so one
of the things I've been thinking about
is how you might get a more large-scale
view of the Internet what's happening on
the Internet what its performance looks
like what its traffic looks like the
trouble is companies don't share
why don't they share well primarily
because they don't want to reveal data
and there are a bunch of reasons for
this some of them are extremely good
some of the data is really private you
mean you don't want to release data
which would tell people what you know
various people have been browsing on the
web people have a fairly large concern
about maintaining their privacy and you
don't want to breach things like that
but honestly part of the concern about
releasing data has nothing to do with
privacy some of the data that I would be
interested in is just the volume of
traffic that's going across the network
whose privacy is infringed by that
companies still don't want to let that
sort of information out part of the
reason is they're worried that it will
give their competitors an edge part of
the reason I think sometimes is they're
worried that that theta will be misused
and this happens from time to time I've
seen a couple of cases but where data
was collected and released and then
someone came along they took that data
and they did something with it that the
data wasn't really capable of supporting
mmm I wrote big advertising blurbs and
the company that released the data said
oh we're not going to do that again so
those sort of things have been a bit
disappointing in terms of releasing data
so the net effect is that no one out
there really sees the whole internet the
internet by its nature of course is
distributed each company has its own
perspective you don't get to see that
whole thing all at once and that's kind
of a sort of thing that I would like to
be able to do see the whole network so
let's look at a couple of really simple
problems how much traffic is there on
the Internet this is a question that I
don't know how to get an answer for at
the moment there are a couple of papers
which talk about it there are a couple
of people out there who quote numbers
but the numbers that I've seen even the
best ones are incredibly where we're at
robbery they're like plus or minus 50%
even if you believe all of their
assumptions and I'm not sure that I do
you know back when it was the NSF that
backbone there were good measurements
then suddenly it became this large
number of companies and those companies
don't share traffic measurements so how
do you find how much traffic there is
there's been the argument made in at
least one paper that part of the origin
of the tech wreck was the fact that the
traffic measurements that people had
were really bad they got blown out of
all proportion people assume that if the
internet was going to double every three
months or whatever the hell it was they
were saying they assumed that it was
growing that fast it didn't matter what
your business model is you could just
you know buy in at the ground floor and
by the time it grew up
you'd be doing fine actually the traffic
probably was growing exponentially but
at nowhere
right there a bunch of other problems
things like detecting distributed
attacks on the network it would be
really nice to have multiples
perspectives to help you do this at the
moment most of the tools that are out
there are based in universities
University Networks you know they have a
little bit of a university network
looking at it the internet very few of
these sort of large scale measurements
involved carriers because the carriers
won't cooperate in these sort of
measurements by and large there are
always exceptions to these sort of
things so the classic example of this
sort of thing outside of the internet
measurement is the CDC in America in
particular who effectively are charged
with discovering new potential health
threats so the example that's getting a
lot of press in Australia these days is
bird flu I don't know how much press it
gets in in the US but it's considered a
pretty big deal in Australia we have a
very large amount of our commerce that
happens with Asia and so it's a lot of
concern about this the data you need to
track these sort of diseases comes from
a whole bunch of different companies and
government organizations and even
non-government organizations like
charities and the data mining
community's been aware of this for a few
years and so they've been using a bunch
of techniques which originally came from
an area called secure distributed
computing which is a real crypto area a
lot of really nice crypto math and more
recently they've been the communities
been calling at privacy preserving data
mining so the goal here is to do data
mining the same sort of stuff we all
think of when we think of data mining
but to be able to do it while preserving
the privacy of the data now there are
there a whole bunch of ways you could do
this one one of which would be to be
very rigorous about you know how you
treat that data not use it in any way
that you're not allowed to but the
strict sense of this privacy preserving
data mining you never have the
capability of seeing into that data and
yet you can do your data mining
operations so this is really neat and to
be honest I'm a novice in this area
I come from an internet measurement
background and I found these techniques
for solving a couple
ones that I'm really interested in so I
think there's some really cool stuff
here the obvious thing that you could do
in this sort of context is to have a
trusted third party who collects the
data it does the data mining and then
conceals the data from any party that
might know have nasty intentions this
works sometimes I'd be lying if it
didn't in Australia in particular I'm
from Australia if you hadn't already
picked it from the accent in Australia
in particular the Australian Bureau of
stats collects information about ISPs
and they collect traffic statistics
broad level traffic statistics so this
is possible to do but it's typically a
really inflexible sort of approach this
sort of problem it's inflexible because
quite often it requires legislation you
need to legislate before these companies
will participate in something like this
and the legislation has to be very
specific and as soon as you make it
specific suddenly you have no
flexibility suddenly you have a new
measurement you want to make you can't
go and make a new law every time you
want to do that so it's not a
particularly great approach even in the
cases where it works and in North
America it hasn't worked as far as I
understand Europe have right who do a
bunch of these sort of measurements
North America nano again the equivalence
just haven't ever finished twice
actually spent a few years working with
people at nano trying to get them to do
these sort of measurements and it just
never happened so let me give you a
couple of really simple problems that
sort of illustrate the sort of things
that you can do with privacy preserving
data mining and then I'll generalize
them until you get more about some of
the sorts of internet problems that I'm
interested in the dining cryptographers
problem is a kind of nice problem the
the idea is you have a bunch of
cryptographers who are having dinner and
when they get to the end of the dinner
the waiter comes off and says someone's
already paid for your bill and the
cryptographers they're a little uneasy
about this because you know could have
been one of them who paid for the bill
but maybe it was the NSA or maybe it was
someone even more
shady than that he's paid for the villa
I don't want to be compromised by any
sort of associations so they try and
work out who paid the bill but they want
to do it in such a way that they don't
have to reveal you know the person who
paid obviously wanted to be private
about it they want to be their own
little deals so they want to find out if
one of them paid but they don't want to
reveal who it was so it's a simple
problem of course they could just try
and bribe the waiter but we have a very
good restaurant and the waiter is not
going to take any amount of bribes in
this particular case so that's one
problem second problem you have a couple
of very rich people and those very rich
people want to know who's the richest
for some reason I have a little
arbitrary example there you can make up
whatever example you like they're rather
secretive so they don't want to tell
each other how much money they have they
just want to work out who's got the most
money this is a very simple problem we
want to do this in a nice way so there
are a bunch of techniques for solving
these sorts of problems
so I have labeled these as primitives
three simple operations that you can do
in a privacy-preserving manner and from
what I've seen you can do just about
everything with these three simple
operations well though I'm sure there
are some problems you can't solve this
way all the problems that I've been
interested in or worried about these
three simple things you can do so let's
have a look at how you might do these
what they are and what you can do with
it so secure distributed summation is a
really useful operation the idea is you
have n parties each of them has a value
VI and they won't work out the sum it's
really simple operation you just want to
do the sum of a bunch of numbers the
point is they want to keep their number
private they don't want anyone else to
be able to learn what their number is so
here's the algorithm for doing that it's
kind of lots of words on that slide I
showed these slides to my postdoc before
I came here he said there's far too many
words on the slides so I went and did a
couple of little pictures of this just
to make it a little bit more obvious
what's going on
you label your you know parties 1 to N
and I'm gonna often call these ISPs
because I'm really oriented towards ISPs
you label them 1 to N and the first one
in the list he makes up a random number
R and he's gonna distribute this random
number R from 0 to n where we believe
our son is in the range 0 to N and can
be a big number it can be your largest
integer if you like we just need to have
some cat so we're going to generate a
random number and then we're gonna add
it to our first value so we end up with
the number s 1 which is V 1 plus a
number R and we do the arithmetic
modding now the interesting thing about
s 1 is it's also going to be randomly
distributed over the interval from 0 to
n so if I told is P 2 my number s 1 it
doesn't give him any information you
can't infer anything about my value from
that it doesn't learn anything because
the number he's got is just a random
number from his point of view so the
next step is you just go round the
circle when you do the same thing at
each step around the circle so say you
get to number 4 you take says value he
adds it twist sorry it's not quite the
same thing we don't keep adding random
numbers in as we go round we just keep
adding our value on we keep doing it
modern the number that we pass on to our
next person in the chain is still going
to be a random number from the point of
view of that that next person and you go
around the circle until you get to the
end and this is like one of those old
math jokes I remember these sort of math
jokes you sort of used to have in high
school where you know you have this
chain of calculations and then at the
end of the chain of calculations they
say and subtract the number you first
thought so that's what you do here you
take your chain of calculations and then
you subtract your random number and the
thing you get out the end is the total
it's a really really simple algorithm
it's so simple it's the sort of thing
that you sort of kick yourself when you
read this and you think well I should
have put that out 5 minutes
really easy it's really nicely it does
have some issues and I'll talk about
Lizzy issues in it in a second you know
obvious issue is the only person who
learns the value in this particular
algorithm is ISP one so he then has to
share that with everyone else and this
requires a degree of honesty in the way
this protocol is conducted we'll talk
about that in a little while let's talk
a bit about application service so the
dining cryptographers is really easy to
solve once you know this hour you just
make the value 0 or 1 you make it 1 if
one of the diners paid zero if he
doesn't given that we're assuming only
one person pay the bill the value is
going to be 0 or 1 so n is 1 you just do
your met with mehta quad 2 and then you
go around your circle and that's it you
find out whether or not you one of your
diners paid or whether someone external
to the group of pathway the problem of
calculating total traffic on the
internet would also be trivial using
this algorithm although there's some
issues in terms of scaling it because
you might have 10,000 participants you
might not necessarily want to do it in a
circle you might want to come up with a
more clever hierarchical scheme for
doing this but still the basic idea is
the same you also have to be a little
careful with Internet traffic
measurements you don't end up double
counting traffic so the way you probably
end up doing this is by setting it so
that you add up the traffic coming in
from non BGP customers which probably
doesn't mean that much for everyone here
but there are ways of setting it up so
that it's not too hard to avoid double
counting of traffic internet health
statistics likewise you know you can
basically get a lot of ISPs these days
make internal measurements of their
performance they send packets across
their network they measure the delays
they measure the packet losses in their
network and they use those measurements
to help fine-tune the network health
improve it help detect problems then you
could take those measurements and you
could use this distributed some to come
up with network wide health metrics
and you could do things like wait then
by the traffic you can see this secure
distributed summation fairly easily
generalizes to doing weighted sum or a
weighted average for that matter you can
do time series algorithms using this
sort of technique you can do the time
series algorithms pre or post doing this
summation as long as they're linear
algorithms but a very large class of the
sort of anomaly detection algorithms
that people are interested in in the
Internet are all linear nonlinear might
make your life a little bit harder one
new thing that we came up with in
particular was applying these two
sketches so how the sketches come into
this well you start thinking about this
and it doesn't take too long to think of
you know a hundred different data sets
that you'd like to do this summation
across let alone when you start thinking
about something like net flow
measurements net flow can generate a
really large amounts of traffic really
high dimensional traffic there's a lot
of information in there performance data
you can get a lot of data about the
structure of performance in networks you
could get a very very high dimensional
data set very very quickly now if you
were then going to do these secure
distributed summations over these high
dimensional data sets
you'd be adding up a lot of traffic
across the whole internet it could get
to be a fairly costly sort of operation
in terms of your overheads so that's
something that I think a lot of ISPs
have been folker but I'd say oh no we
can't do this it's going to involve too
much traffic too much overhead we're not
gonna do that so what a sketch is well
sketches are an approach for reducing
the dimensionality of data sets
effectively typically these days they're
applied to streaming data sets where the
data is coming in in the stream the
analogy is your fire pipe fire hose
where the water is coming through so
quickly that you can't deal with it all
at once so you just do something on the
fly you don't try and store the data you
just process it and keep something
representative of that data as it goes
through there are a lot of papers on
this now and a couple of references I
have here is
is a pathetic sub sample of all the
possible references you could give on
sketches I just want to give you a kind
of a very quick example of how this
thing sort of works so this is a this is
a sketch called the count mem sketch the
idea is you have some sort of stream of
updates so your updates take the form au
where a is a key and U is a value and
what you want to do is keep a big array
VA where the VA element tells you the
total values of which have the the key a
the assumption here is that an is going
to be a large number so the
dimensionality of this data set is large
particularly large in comparison with a
number of updates that you receive so
you might be talking about n being of
the order know a million or something
and the number of updates being more of
the order of thousands so you don't want
to store necessarily this large vector
you certainly don't want to hand this
vector around the whole internet as you
do some sort of summation so what does
the sketch
do well you have this array of D by W
array we're going to call it C and you
have a bunch of random hash functions H
ones HD these hash fractions do you have
to be chosen a little bit carefully but
there's some good techniques doing that
and what these hash functions do is they
map your 1 to n possible keys into a
much smaller space of better one of the
better term I keep calling them keys and
the way they do that is you simply take
your hash function your series of hash
functions from Ike is 1 to D and
whenever you were going to instead of
incrementing this vector V up here you
increment this array for each of these
hash functions then when you want to do
a query when you want to find out VA
what you do is you look for the minimum
across I across your series of hash
functions of this array now I'm not
going to go into the details of why this
is a good thing to do it's not really my
area of speciality my co-author is the
people on on sketches the point is that
there are some really good techniques
out there for reducing the
mention allottee of data the nice thing
about this particular one is that it's
all linear so applying the secure
distributed summations thus captures is
actually it's almost trivial the thing
you just have to recognize is that if
you take the sketch of the union of all
of these updates from a bunch of
different places and labeling the places
by an and the update sequence by eye
then that's equal to the sum over the
sketches from each individual place so
we can do a summation over these
sketches once we've constructed them
from each individual place and we can
use that and and this is a reasonably
general property for a whole lot of
these dimension reducing sort of
techniques question when I think in
terms of things often I'm thinking
traffic and traffic the dimensionality
includes things like port numbers what
application people are using things like
that and that can get very large very
quickly when you talk about performance
it's not quite so big that as soon as
you start trying to do performance from
say city the city you know even in North
America there's a good 20 big cities
that you want to have in some sort of
matrix of performance so twenty by
twenty matrix is 400 places already it's
not even doing an international
performance it's a set of performance
metrics so it's not that hard to come up
with reasonably large sizes of data just
for foot performance but the traffic is
where you start running to the really
big data sets I guess when you're
looking for particular sorts of attacks
you're looking for particular
combinations of things like port numbers
particular sets of IP addresses
particular prefixes there's a whole
range of these sort of combinations of
groupings of traffic that could be
responsible for some sort of anomalous
behavior some
miss configuration or some sort of
attack on the network does it make sense
number of prefixes you're talking about
you know getting on for 200,000 now so
that's that's one of the places it gets
pretty big yeah number of ports is you
know a couple of thousand probably
interest you these sketches so there's a
paper the internet measurement
conference that's three years ago I
guess on using these for detecting
anomalies in traffic and so the basic
idea was detecting things like denial of
service attacks and they were focused on
detecting these within an ISP I can't
remember off the top my head it wasn't
my paper just one I know but you know
you're talking hundreds of thousands
millions perhaps with traffic gets large
very quickly they've been a whole bunch
of papers on how you can do various
dimension reduction techniques on
traffic sketches are not the only
approach I shouldn't try and convey
that's the only thing you can do it's
just a nice way of using these sort of
techniques so I've been a little bit
careless in what I've told you so far
and this kind of deliberate because you
know I like to start out simple but we
have to do a little bit more we have to
consider what sort of security model
we're actually operating under here you
know when I talk about privacy
preserving what sort of privacy am i
preserving what do I expect from the
participants the model that most people
seem to use in this area is a thing that
often gets called the honest paquius
model we should note that there's an
intrinsic assumption of honesty in these
sort of algorithms because you are never
revealing your value I'm never going to
tell anyone my value VI you can lie
about it
there's nothing to stop someone from
just putting in the wrong number
either through incompetence or
deliberate Elevens
and that's very unfortunate but it's an
intrinsic part of this sort of
privacy-preserving approach you have to
assume that the participants are going
to participate in a way which is
basically honest that's that's a big pun
you wouldn't catch their accounting it
probably would have done pretty well
with their traffic you know there are
there are a bunch of places where it's
quite hard to force people to be
competent and I'm more worried about
competence with this sort of thing than
I am necessarily about
so I've heard different points of view
you guys should actually ask Vint Cerf
what the story is I mean he works for
you now doesn't he he can tell you I've
heard different stories about what was
actually said and I wasn't there so you
know it's it's unfortunate certainly a
lot of people believe things about
traffic that weren't true whether or not
it was welcome telling them I can't
really say let me get back on track so
you have this honesty issue you have an
honesty issue here but the idea is that
we're going to allow people even though
they participate in an honest fashion to
be curious so they're gonna be allowed
to perform extra operations to try and
find out whatever they can typically
people will limit these extra operations
to polynomial time algorithms and
primarily that's because some of these
techniques that I'm talking about here
from the crypto literature they assume
things like RSA gonna be used and if you
have the ability to do non polynomial
time algorithms very large algorithms
then you can break RSA and you can go
away and you can do your factoring and
that will break some of these sort of
techniques as well on the other hand we
do allow for things like collusion and
collusion in the simple algorithm I
showed you for secured distributed
summation so far is a really bad thing
we can fix that but let me show you
first this is a really simple example of
how you do collusion here if you have
party J and PI J plus 2 then ha DJ he
made this number so he knows it and
party J plus 2 here's this number from J
plus 1 so he knows that number and if I
subtract those two it's trivial to
extract VJ which would mean that if you
have two parties on either side of one
guy they can collude to find out that
guys
now that's not something that we want to
allow any sort of algorithms the plus
side is there are a whole bunch of
different ways of fixing this so I have
one really simple way here I don't think
this is actually the best way of doing
it the best way is show me a polynomial
secret strength the best way that I know
at least but that would take me another
15 minutes to explain so I've got this
very simple way I don't think this is
necessarily a bad way but I think the
other one is a bit more elegant the
other way of doing it has a few extra
properties it also allows you to correct
for errors it has honest majority
collusion protection and so it's just a
little bit stronger the protection again
but this way is kind of nice what you do
is you take your value you break it up
into shares randomly and then you do a
secure distributed summation on each of
these shares but you do the summation in
a different order so you pick a random
order for each of these summations and
perhaps a random starting point so no
one particular know it has a special
role and they algorithm anymore and you
do this across these guys and then right
at the end you do a normal sum across
each of the sums you've got from each of
your shares so what does this do well it
prevents it obviously presents this sort
of simple collusion that I've described
it prevents a whole lot of coercion
attacks as well like I said I think that
polynomial secret sharing technique has
a little bit of an edge on it but who
take me a little bit longer than I want
to try and explain how it actually works
so another application so into provider
performance measurements
this is where it sort of gets
interesting for me particularly I spent
a number of years of a tea trying to set
up into provider performance
measurements and so I went to various
providers other than AT&amp;amp;T and I talked
with people there who were interested in
measurements and the technical people
who were very you know they were very
keen on this because there are some good
reasons for having into provider
performance measurements a lot of the
problems in big carriers at least happen
at the edge of the network there are
problems with inter domain routing BGP I
don't know if you guys know much about
BGP the BGP is a nightmare to configure
correctly it's really really not
transparent it's really arcane to
configure correctly in most of the
current routers have been gone by really
easy to make mistakes and mistakes are
made on a fairly regular basis so a lot
of the problems happen around the edge
of networks add to that the fact that
peering links are probably the most
congested links in the big providers the
links are connect between these guys
often have the the worst performance and
it's really important to monitor you
think so the technical guys when you go
and talk to them about these sort of
performance monitoring things in to
provide a performance model monitoring
they're really positive they really want
to do it it goes up the ladder up to
some sort of senior management person
and then they don't want to do it so
much they're scared of the way the data
is going to be used and this is like I
said this is because in some cases
they've been burnt these sort of data
sets have been taken out of context and
people have gone and done ratings of ISP
so they've gone you know this is P is
number one this is P is number two this
is P is number three it's not the point
of these measurements at all they quite
often not set up in a way that actually
allows for valid comparisons to be made
they're not commensurate datasets so the
comparisons are not necessarily even
valid comparisons people get very upset
when this happens and they tend to back
away and they tend not to do it again so
this is one of these sort of things
where it's a it's an issue for people to
get these sort of performance
measurements again it's not impossible
in Europe ripe have set up a very nice
measurement interest
structure and eventually we got AT&amp;amp;T to
participate in Europe's right
infrastructure so there are some the
measurements into provider measurements
but as far as I know there's not a
really good large scale into provider
measurements going on in North America
or in Australia for that matter various
research organization set up measurement
infrastructure around the network so
Andre used to work for Keita they do a
huge number of really wide scale
measurements of the internet the trouble
with those is they tend to be focused on
academic networks they tend to be
running across things like Abilene they
tend to be running across in Australia
Arnett the academic research network in
Australia that doesn't tell us much
about the real consumer Internet
unfortunately and it's always a problem
getting good measurements of the
consumer backbone another thing that I
want to emphasize is you want one-way
delay measurements here people sometimes
don't get this but into ISP routing is
fundamentally asymmetric it's absolutely
fundamentally asymmetric I'm not going
to go into details about this but there
is no part of it that is an asymmetric
you know you have to have one link
connecting you to the rest of the
Internet to get a symmetry that's the
way it typically gets set up so one way
delay measurements are really important
so how do you make these sort of
measurements well it's pretty easy in
principle well I shouldn't say easy in
principle a lot of people have done a
lot of good work to make it easy for
people like me to make me sort of
measurements I have a number of
colleagues who spend a lot of time on
doing things like setting up very
careful GPS clocks for these boxes so
that you have good clock synchronization
and so on but I'm not gonna talk about
that today what you do is you just send
packets you have a box in each of these
networks and you send probe packets and
you measure the the performance of these
and these represents samples of the
underlying performance of the network so
actually this is really nice from a
statistical point of view this is a
really nice sampling problem I'm not
going to talk about the sampling problem
either today there's some interesting
maths can do there
want to stick to the one topic boo today
so you see your problem up like this you
have kij probe packets going from ISPI
to j notice k IJ is actually a random
variable one of the reasons for this
there's a good good idea that people had
back in internet measurements a number
of years ago which is possum probing the
idea is you send packets as a Poisson
process you don't send packets going
bang bang bang you send them sort of
bang a bang bang bang as this random
sequence the the original idea for this
was to avoid any chance that you might
accidentally synchronize with some
Network phenomenon so if you had your
measurements and you were sending them
at uniform sampling spaces there's this
potential that there would be some
periodic behavior that you'd synchronize
with now how likely is that I don't know
but in this particular context Poisson
sampling is really useful the reason is
that the Poisson process has this nice
property that you can't anticipate you
can't anticipate when the next probe is
going to happen and that's nice for the
secure distributed computing sort of
problem privacy preserving data mining
because if you send those packets at
regular intervals then the guy receiving
them can kind of work out something
about when they're being sent he can set
up a statistical inference problem to
try and work out what the guy sending
them is doing and work out the delays of
the packets of packet losses by just
looking at what he receives but if you
send them as a Poisson sequence you
can't do that because there's this lack
of anticipation property of Poisson
processes so that's one part of this one
part you have to get right you also you
have to do a few things differently from
the standard way people do these
performance measurements the typical way
people do this is they put timestamps in
the packet so you send the packet with a
time stamp in it the time stamp tells
you the transmit time here and then the
receiver gets the received time takes
the transmit time the received time
subtract one from the other and then he
knows the delays you notice when packets
are lost because he looks at the
sequence numbers well we don't want to
do that either because having sequence
numbers would let us again set up this
nice statistical inference problem
where we could work out what's going on
with this bang so that would be bad so
we're gonna send these packets but the
sender is going to record the transmit
times and he's not going to tell anyone
else and the receiver is gonna get the
received times and he's not going to
tell anyone else these receive time so
that's part of the secrecy obviously the
delay is given by the difference between
them the average delay is just an
average over all of these guys all these
packets that are sent in our sequence I
have a couple of other averages there
that I'm gonna use in a second so if
other if you don't want your ISPs to be
able to make comparisons with other ISPs
well the first thing you're gonna have
to do is give up on some sorts of
measurements you're not going to be able
to you're not gonna be able to report
this number because as soon as you
report this number some some you need is
going to go out and he's gonna put on a
webpage and Google's going to index it
and then someone else is going to come
along and say oh no AT&amp;amp;T is at the top
of that isn't that fantastic someone
else is going to look at it and say
someone else is doing badly and they'll
complain nope you know working in AT&amp;amp;T
we got complaints from people when they
saw these lists they'd say why are you
at the top of his list as I said they're
not always designed for this purpose
so let's limit it to looking at an
average across these ISPs now honestly
that does restrict the utility of these
measurements if you lose something by
doing it but you're still better off
than you were before you had
measurements you still know a hell of a
lot more than you did before that you
know yeah
what exactly do you know there's no time
so what do what do what do you know so
the transmitter knows the transmit time
the receiver knows the receive time
that's all that they know at the moment
so we don't have any way of putting them
together yet we're going to do that in a
second and what we're going to try and
work out is this average this is the
outgoing performance from our network
the average outgoing performance to all
of the other providers we could work out
the incoming performance and it's
trivial operation to do that once I show
you how to do this one but I didn't want
to take up any more space on these
slides and we can't share these
individual measurements so we're not
gonna we're not gonna allow them out of
the box so how do we do this well I just
rearranged my son's a little bit and
write them like this not a big deal
there let's think about it from the
point of view of the transmitter I is
the one who wants to work out d out I so
he's the one who has to get this number
at the end of the day he already knows
the transmit time so he knows this thing
this t bar he wants to find out R Bar
that's just a sum we can use secure
distributed summation to work it out so
at this point this looks trivial this is
all you have to do secure distribute
summation over the received this looks
so easy why am i bothering to give you
another example which is quite this easy
well it gets a little bit hairier when
packets can be lost again packet loss
averages the the average loss percentage
we can work that out using a secure
distribute summation the problem is when
we want to calculate something like
delay I should say I'm calculating
averages here you can use the same
techniques to calculate distributions or
marginals or whatever you like I'm just
focusing on averages to keep everything
simple here
but the problem is when we calculate D
out this average delay we have to censor
out the measurements where the packet
wasn't received where it was lost
otherwise we're gonna really heavily
bias our average delay measurements we
can't tell the other rsps which packets
were lost because that would be telling
them you know a huge amount about our
performance they then start comparing
packet loss percentages and that would
be just as bad as comparing delays so we
can't tell them which packets are lost
we can't put sequence numbers in the
packets I've already said why the
capital sequence I'm just in the packet
we get it we get a statistical inference
problem and that's not right so what do
we do well we're gonna use another
secure distributed operation this one is
to do a dot product so you you guys
little worm what a dot product is it's
just an inner product of two vectors you
have a question why don't know it's lost
you okay this is this is the problem
neither of them know it's lost because
the sender knows he sent a packet the
receiver knows he didn't receive a
packet no one knows the packets been
lost we will fix that in a second let me
let me explain this first I'm not going
to tell you how to do this one I haven't
I have an extra slide if you really
interested I can show you how to do it
but for the purpose of keep of getting
us through this talking you know
nominally well maybe 15 minutes I'm
gonna go through this just what it does
so we want to calculate a dot product
which is just a sum so Bob and our Alice
and Bob are a traditional cryptographers
here and they're the ones who want to do
the things like this so Alice and Bob
have values imb they're both vectors
they want to calculate this sum across
the elements of those vectors now we
could come up with some kind of secure
distributed algorithm here one of the
key things with these algorithms you've
got to remember though is even if the
algorithm is secure
the result is not necessarily because
you're going to share the result so what
if I put B is equal to zero zero zero
one zero zero zero then Bob is gonna
learn AI he's going to learn one of
those values we don't want to allow that
so we have to we can't tell either of
the the two participants a dot B that
would be the algorithm might be secure
but the result will tell us too much
something you always have to be aware of
in the sort of context so what we do is
we split the solution into VA and VB and
we tell Alice VA and we tell Bob VB and
you know the first thing that I thought
when I saw this I was reading through a
pure math sort of crypto paper on how to
do this and I was thinking why the hell
would you want to do that I mean wasn't
the point for them to learn the sum so
if you tell them part of it you know we
deliberately choose these things in a
way such that if you know VA you can't
work out VB what good does VA do you
well this is a really nice example of
the good that VA does it so what we're
gonna do is we're going to put a packet
ID in each packet importantly these
packet IDs are not sequence numbers
we're gonna choose the packet IDs
randomly from some set we make that set
large enough so that it's going to be
larger than kij for all I and J but so
we're going to make it a fairly big set
generally we make it a lot larger
perhaps and then we're gonna pick random
packet IDs out of this set and I'm going
to create an indicator vector and what
the indicator vector does will be
indicated so this is my ID ijk it's one
if packet with idk from I to J is
received so the receiver creates this
vector the receiver knows this vector to
answer your question he knows this
vector he doesn't know whether those
packets were the ones that were sent but
he knows this vector so the calculation
now looks like this so a little bit
different a my is the total number of
packets that we send in all of our
measurement experiments going out of I
so total number that are received
so the transmitter doesn't know a my
transmitter doesn't know I but he does
know t you receive a noise I the
receiver knows R so the receiver can
calculate this bit pretty easily the I
times R this is the bit that we worry
about I times T but if you look
carefully this is just a dot product the
receiver knows I the transmitter knows T
so this is a dot product though so too
many words on this slide I might know
completely too many words the important
bit though is we take our dot product
and we break it into two bits 1 bits
known to the transmitter 1 bits known to
the receiver the bit that the
transmitter knows about he can just take
those values and he can add them up
across all J no problem there the bit of
the receiver noise or the bits of the
receiver noise they can use a secure
distributed summation to add up across
all J the M is you can work out by doing
again a secure distributed summation
across all of the J's and once you've
done this you put these numbers together
the transmitter then has all the bits he
needs to calculate his delay measurement
his delay metric so this to me is is it
really this was what really showed me
why you needed this inner product and
how it could be used you don't use the
inner product just as a direct result
you use those outputs from it
you then sum them across some other set
and that then means that you can share
the result of that sum because you don't
learn any of the particular values
I was gonna tell you a bit about
oblivious tree who wants to learn a bit
about oblivious transfer okay who is
oblivious transfer is good I'm not gonna
I'm not gonna use oblivious transfer
except that oblivious transfer is
actually part of this inner product
operation so I'm not gonna give you a
specific example of oblivious transfer
in the context of internet measurements
but it's part of this inner product that
I've shown you already so I feel
justified in from telling you a bit
about oblivious transfer there are a
bunch of different versions of oblivious
transfer the one I'm going to talk about
I have some references here it's called
one in an oblivious transfer so very
simples conceptually it's a very simple
operation between two parties Alice and
Bob again Alice has a list of numbers a
1 to a n and Bob wants to know one of
those numbers but it doesn't want Alice
to know which number so Bob wants to
know our app officer a be a beta but
Alice mustn't learn beta and equally
Alice doesn't want Bob to learn anything
more than he gets so Alice doesn't want
him to learn anything except a better so
you set the problem up it's a very
simple problem the algorithm for solving
it again a little bit too complicated
for me to go into detail except possibly
if people have questions afterwards but
it's a really neat little trick and
again between this trick and the inner
product and the security forbid
summation there's not a hell of a lot I
think you can't solve in a sort of
context so let me give you one example
of how to use oblivious transfer I
showed you this millionaires problem at
the start let's solve the millionaire's
problem using oblivious transfer
actually the millionaire's problem was
originally proposed by Yau and in a
paper that goes back to I think 1982 so
it's you know good 20 or so years old
and the technique he came up with there
is very clever I like this way of doing
it a little bit more because it's more
generic it's it's no better than the one
yeah
came up with but I can apply this
technique to a whole bunch of problems
whereas the Ouse technique was very
specific to the millionaire's problem
yeah as a regional technique I should
say yeah actually generalized his work
as well so how do we do it well we have
to we have to put again a cap on our
range of possible Wealth's we can just
make that how much money is there in
America but that's a pretty big number
that should be enough a trillion dollars
or something no one has more money on
that and then Alice is gonna make a list
from zero to n and Alice is going to
make them all zeros until you get to
Alice as well and then after that
they're all gonna be once and Bob is
just gonna do an oblivious transfer Bob
is gonna request W be his wealth so you
can see immediately what happens if Bob
gets a zero then bob has less money than
ours and he's gonna be very disappointed
if Bob gets a one then bob has at least
as much money as Alice obviously there
are some issues here in the fact that we
we do have to do some discretization
here when we're making this calculation
no though they'll probably be happy if
they've got the same amount of money
plus or minus a thousand dollars I would
guess you could do it you could reverse
it as well and they could get rid of
that that nasty little equality case by
reversing that the process obviously bob
has to tell Alice about this this comes
again back to the honest but curious
model again there are ways of making it
a little bit more general so you don't
have to rely quite so much on that in
this sort of problem so there's a
solution to the millionaire's problem so
we're getting caught up to the end of my
talk which is probably a good thing
given the time future I think this is a
really cool like I said at the start I
know this stuff is pretty new to me and
I think there's some really really cool
ideas here some really really cool
things you can do particularly I like
the implications for a whole bunch of
problems that previously at least in
internet measurement modeling internet
control
from a point of view of ISPs at least
have been sort of considered as gang
theory problems along the lines of
something like the prisoner's dilemma
those of you who don't know the
prisoner's dilemma is you have two
prisoners that they've been involved in
some crime the police put them in two
cells and they quiz them separately if
the two prisoners both clam up my other
of them tells the police anything the
police don't really have any evidence
they'll have to let them go but if one
of them confesses the other guy is going
to go to jail forever and if both of
them confess then the probably go to
jail maybe not for quite so long so it's
really in their interest to cooperate
the problem is the sort of people you
end up in that sort of situation quite a
far particularly trustworthy and so they
don't cooperate they lie one of them
confesses or possibly both of them
confess and they both end up in jail see
the prisoner's dilemma should I trust my
owner or not there are a bunch of
similar problems where you look at
things like inter-domain traffic
engineering where you have two ISPs
traffic engineering is the process of
balancing your traffic across a network
when you talk about inter domain traffic
engineering if I rebalance my traffic
that affects that other network and
likewise if he rebalances his traffic it
affects my network if you can come up
with an algorithm that optimizes your
joint performance that's great but how
do you trust the other person to play
fairly to do this honestly well there
are a bunch of techniques you can use
here that will allow you to make
measurements to make tests of whether or
not they're behaving correctly it's not
a panacea but perhaps if you can improve
the trust between ISPs there's a whole
lot of these sorts of algorithms that
suddenly become much much more
attractive to them and this one the
long-run reduced costs it will improve
performance and all them the usual nice
things so conclusion
we can do some stuff that I had never
imagined was actually possible and this
is literally true they're a bunch of
these problems into inter-domain
measurement problems that I just didn't
think you could actually solve and it
turns out you can there's been you know
half a dozen papers written by the
internet measurement community on
anonymizing traffic the idea being that
you have to take out you know the
headers of the the traffic you have to
do something to IP addresses all these
anonymization techniques have real
headaches and most of them don't work as
well as they need to to satisfy
companies like 18 T 18 don't release
anonymize traffic the anonymization
techniques aren't good enough for them
to trust these techniques give you a
guaranteed way of doing these sort of
computations that is really really neat
it's one of those sorts of things where
I think it's just waiting to happen so
as I said this is all new to me I'm
looking for people who are interested in
participating or cooperating or doing
some of this sort of stuff so talk to me
afterwards if you have some suggestions
well talk to me now ask questions now I
think this is a good time to ask
questions yep
these objects
yep
yep
yeah what's wrong with that okay there's
nothing in principle wrong with it I
mean it's great to have trusted third
parties that you can work with and as I
said in Europe right fulfill this role
they do a whole bunch of performance
measurements the problems for me in
trusted third parties are the fact that
they're not always easy to find parties
that will fulfill this role in any
particular situation and when you do
quite often the result is very
inflexible for instance you know one of
the things that happens say you want to
share data with your customer and you
want to do it through a trusted third
party this involves writing a couple of
contracts so you run in contract between
two parties okay how many lawyers is
involved with that most the contracts
like that that I've ever been involved
with took six months or something to do
at least as soon as you start including
end parties the the time to do those
negotiations goes up as N squared and
this is one of the things that killed
off the efforts we were trying to do
within nanog to set up these inter
provider performance metrics is that
every person who came to the table have
their own specific sets of requirements
for what would happen with those
measurements how they would be made what
equipment needed to be used because that
was their sort of processes and it just
it meant that you never got anywhere you
know as fast as you made traction with
one sort of thing something else changed
and then you have the issue of well I've
spent my two years I've set up my
measurement infrastructure great now
someone some research has come up with
this brilliant new measurement that's
going to change the way we you know do
things on the Internet
am I going to spend another two years to
set that up it would be really nice to
be able to say we have this bunch of
primitives go away right your operation
in terms of those primitives and we'll
just run it turn it into a kind of API
that people can use so I don't have
anything in principle against the
parties and there's a lot you can do in
that
approach but I think this is better
the people I've talked to so far
optimistic that they're not going along
with it yet so I think it requires a
pretty good job of selling it as well
and you know this is something that I'm
hoping this got a little bit of time on
in the next year or so it's trying to
get a few people at least to participate
in doing this sort of thing you'll be
interesting to see how will they respond
I think you need to cover a few things
very well in terms of you know making
sure they believe the claims that you're
making that privacy and so on but the
the sort of measurements are so useful
to them that I think they will
eventually come on on board that's
that's what I'm hoping at least</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>