<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Chunkfs | Coder Coacher - Coaching Coders</title><meta content="Chunkfs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Chunkfs</b></h2><h5 class="post__date">2008-05-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Tach2CheAc8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Val Henson I have my own
company gage consulting I do specialize
in Linux file systems but I do all sorts
of other systems programming stuff too
especially silent data corruption I love
trying to find out what's going on
silent infection anyway so I have a I
have this a research agenda I've been
going along on for a few years now
called which I'm summarizing is repaired
driven file system design the idea that
it's really important to design your
file system so that you can repair them
quickly as well as access them right
things to them delete things from them
all that all the all your sort of normal
online news so the way I got started on
this initially oh by the way this is the
chunk of s talk it's just starts out
this way because i think it's bigger
than chunk of us so this is a question
that got me started and when i initially
asked this question back in 2002 just
started working on file systems and the
way I answered it was my head was a well
disk capacity will continue to grow but
CPUs will get faster too and then there
will be no problem right because at that
time I thought file system check was CPU
bound which it is not not even close so
when I came back to this in a 2006
something like that I actually did some
math which is very dangerous math can
find out all kinds of interesting things
so first I started out with these some
numbers I grabbed from a seagate paper
which is you know the vendors are the
best reference you can get right now so
they're predict their predictions for
the way that disks are going to change
between 2006 and 2013 are thus capacity
will go up by 16 times which is sort of
the normal expected doubling rate we've
seen for a long time banner is only
going to go up by five times so
bandwidth relative to capacity is going
to be about a third so that's uncool and
the really crazy thing is that seek time
is not going to change it's like some
incredibly minor speed up that are
expecting I had an ax
a really cool personal experience with
this recently one of my friends brought
but an ancient amiga UNIX machine there
was such a thing it ran unix often be
bays I think it was built in 92
something like that and there's a
program you can rent to find out the
performance characteristics of your
drive and it could do about 40 50 60 at
seeks per second there are two different
drives one was doing 41 was doing about
60 your this laptop can do 70 seeks per
second so what is like 15 years we've
got another 10 seeks per second so ouch
ok so I'm using my laptop again because
that's what you do I decided to do some
math to figure out how much filesystem
check time would change between 2006 and
2013 assuming my file system goes as
fast as a desk so you can see the
numbers here I tried to be a i gave cpu
0 seconds now assuming my cpu is
infinitely fast i came up with this
really bad prediction is that my laptop
in 2013 will take 80 minutes to run a
file system check which makes me really
upset I really don't like that it takes
me eight minutes right now to check my
file system this is clearly not the way
to be going the way that file systems
are designed right now on the way that
all for future file system development
is going is such that this this is going
to be the result alright so I'm going to
get to this pretty quickly because I
lately people have been buying this you
can say okay great i have to file system
check is going to take a long time but
i'm gonna have to do it very seldom
because you know just are so reliable
and stuff so there's a bunch of things
that are going to cause you to have to
run file system check the per bit IRA is
going down the per disk i/o rate ra is
not looking so great you can do all
kinds of bad stuff to your disk on
purpose
a failure of layers below block device
interface I think SSD compact flash all
that kind of solid-state storage stuff
really really really bad error rates so
that's even more of an issue than before
and there's always file system boxes if
you can eliminate every other source of
error you're still going to have bugs in
your file system and those are going to
be the hardest to fix and find there's
also even more edge corner case where
the your operating system has bugs and
ends up like stomping on the thing that
you're writing to desk it's not good
okay so yeah which of these broad
categories would lead to put the power
second solitude so unexpected power
cycles is something that the software is
designed to handle so modern file
systems all can handle that to one
degree or another does that answer your
question yeah so this is this is a lot
of people i think i have this in the
next slide we'll see okay i think i have
further comment on this so i'm calling
this fsck time crunch i want to
emphasize right here that fsck is
actually including piles to file system
check and repair it's wonderful if you
can check your file system really
quickly but if you still have take a
long time to repair it we're still
losing presumably we're running running
file system check because there is
something wrong with the file system
okay yes this is the pointy ones make
this so a lot of people when I bring
this to them they say well you know
that's what logging is for logging is
actually just for handling the case of
an unexpected power cycle so and the old
good old days when ffs in HD too you're
just right step to the file system any
old order and when the the if the
machine crashed unexpectedly are there
was a power cycle then the onda state
was kind of undefined and you had to go
and fix it modern file systems use
logging or copy on write something like
that so that they can recover from this
very quickly and deterministically and
get it right so that's that's not what
I'm talking about here when I'm talking
about is actual errors of some sort
um so this is the next objection i get a
lot oh we don't need to do that raid
does that well raid honestly it's
questionable whether it improves
stability I've heard of so many raid
disasters it's like raid isn't tested
very well usually it works and it's when
it doesn't work anymore that things go
wrong there's there's a there's a huge
problem with latent sectors if you lose
a drive and you try to reconstruct it
you suddenly discover that you know five
sectors on your other disks are bad so
it doesn't fix it trust me and so
there's still the file system bug case
so okay so I propose a pair driven file
system design this is the idea from the
very beginning assuming that you're
going to have your file system is going
to break and and you are going to want
to fix it quickly so a bunch of stuff I
think should go into it you know start
up for the beginning thinking about it
this is a hard one for file system
programmers or programmers in general
simple data structures we like to be
really super clever you know it is way
more fun to come up with something
complicated when you start thinking
about things in terms of okay this has
been broken how do I fix it suddenly you
want very very much simpler data
structures the the metric I like to use
is could you repair this by hand you
can't repair it by hand you probably
can't repair automatically in your file
system repair program so we currently
layout data on disk to optimize for
reading or writing or directory
traversal or something like that there's
not much thought given to optimizing it
for the order in which you need to
diverse the data and in order to do
filesystem check so most file systems
lay things out right now so that if
you're doing a file if you're walking
the entire metadata tree for the file
system you end up doing all of these
seeks and dependent reads and things
like that which as we as we know we can
only do 70 86
so that really cuts down cuts down on
the speed so so I forgot to edit this
slide because this is very important
fast incremental file system check and
repair so the incremental part is very
important I just don't see any way to
make file system check go fast unless
you can do it one piece at a time if you
look at the bandwidth on the of the
disks you know that is it's going to go
up by five times capacity is going to go
up by 16 times I mean you're just if you
have to read the same amount of data you
have a very strong limit on how quickly
you can go so the repair part again you
need to be able to repair it quickly and
incrementally so okay all this stuff
people know checksums redundancy
scrubbing you should just have two
copies of your metadata metadata so
it'll so so little overhead on most
normal file systems that you just might
as well keep two copies of it this still
isn't going to help you in the case of a
filesystem bug casino if you have two
copies of the wrong thing it's still the
wrong thing so in this last principle i
think is probably the most generic and
one I would like to see adopted widely
as a metadata isolation so if I screw up
this one piece of metadata how far can
the repercussions of this travel you
know it can can this ruin the the free
space map for the entire file system
okay that's bad want to try to limit
that okay so this is my particular
iteration or instance of these
principles um I is presented as a chunk
of sa file system it's actually more
like a set of techniques or a meta file
system
I am debating whether it should be
implemented on its own or just folded
into new file systems so the idea is you
know great whoo exciting gonna divide
your file system into sort of little
file systems and these are I'm calling
them chunks because that's did you call
there's like three words that you get to
use in storage it's like blocks chunks
and I don't know sectors or something
like that and people just use them over
and over again so i'm using chunk so you
divide these up into independent little
file systems great i can check each one
of them individually the problem is when
you want to share the space and the name
space between them all so how do you
smoosh it all back together there's a
side to do this theorem a bunch of
examples so say this chunk is completely
full I've used up all the space in it
and I have a file with I node X in there
that I want to grow make bigger so I
when I do is I allocate another I doubt
in another chunk and I have forward and
backward back pointers so now when if if
I were to have a problem in this chunk I
could check it repair it and then I
could follow pointers to say out here
and say oh this chunk is fine there's
nothing wrong with it I'll just make
sure that this I know it exists once
it's done nine you know I'm going to go
back and deal with my own business so
that you can use the same man sorry you
can use the same technique for
directories where you know it's unix you
just a drug we use a file say I'm trying
to create a link to or sorry trying to
create a new file just in that directory
just create another extension of the
directory in another chunk and then
allocate the inode there this is the
hardest case I'm including it I don't
expect you to you no reason it out or
anything but this was the most difficult
case of creating a new hard linked to an
existing i know'd inside a full chunk
still works this little continuation i
know'd thing it's actually pretty useful
so
so you can definitely get into a state I
did forget to put those numbers you can
definitely get into a state where you
end up having basically another file
system a normal file system because
there's so many crossed chunk references
and you know maybe like if you there's
the bouncing free space problem where
space frees up in them different chunks
in different orders and you end up
allocating over and over again from
different chunks and good Julian's of
continuation I notes and that's very bad
so you can do this we know how to do
sports files we know how to check to see
if there's already something allocating
that chunk can use it instead so yeah
are the chunks designed to be laid out
in physical block sector groups or so
there's no um you just need to have a
way to encapsulate operation so it's
very easy to check when I'm working on
this trunk am I somehow trying to
reference outside of it so I would just
do it and the way I've implemented is
just it its length right the chunk is
these blocks to this block yeah
oh so um yes so you're saying that uh if
everything's on one disc you only get so
many sikhs anyway before you had it on
multiple discs yeah so it's if you have
multiple discs you should use them and
there's nothing wrong with chunk OFS
that actually tongue confess is very
friendly to having multiple discs
underneath so this is trying to improve
fault boundaries inside of a desk so
that make sense sir okay well what I was
concerned about was then you know if you
have let's say three chunks per disk or
random choice for disc is that as you
have to go back forth between we have
received a little different so you
haven't introduced more seats which will
avenge this guy ah right yes so dividing
things up to up into chunks might make
your accesses be spread out more over
the file system or over the disc and
that would create more seeks and that
would reduce performance that's your say
yes okay um well so this great thing
about the way that file systems are used
is that they have locality and so in
this the problem sorted exists already
it could be worsened by the strict
requirements on references inside of a
chunk so what I did is I took an
existing file system and tried to
simulate you know what what the behavior
would be like if you had to restrict
things inside of a chunk and really it's
it's not it's not things are already
fairly localized when you introduce the
chunking it's only a very small amount
of additional work and seeking around so
basically most most the time when you're
doing operations it's like on a file in
a directory and then your next operation
is on another file in the same directory
and you try to keep these all stuck
together in the same chunk as much as
possible so all of these locality issues
our locality issues you have in normal
file systems and you just use that
inside the chunks as well so
I the other question I had little check
how much of the non allocation meta data
is replicated from the game to the
continuation on non allocation metadata
uh you know as much as you want I would
I windows I would go ahead and copy all
of the head i know'd metadata into the
following i know'd metadata as much as
possible so there's also the idea that
you might have copies duplicates inside
of the the chunk itself but it's just I
mean you might as well you've got
otherwise you're just using the
permissions to represent no information
so i think you should do more
duplication rather than less okay so I
wanted to give sort of a visual
representation of one of the problems
you have to solve with this which I
realize is not in these slides
originally okay so say you have this
arrangement you have a continuation I
know it over here between these two
chunks say bad thing happens so so this
gets somehow smashed which actually i'm
going to show you in the demo so the
question is how do i find this when I'm
doing when I correct this this inode is
going to go away and then i have this
dangling pointer here so how do i find
it there's actually a really simple
solution it's no big deal i just have to
call it out so um you just have to have
a list inside of each chunk of things
that are references from another chunk
so i'm in trunk too and I have something
from chunk 3 reference back and forth so
I just need to have a list of that and
the way i did is i just created
directory a directory to contain all the
references from a particular trunk
inside the trunk
so basically when you have something go
wrong over here in this chunk you check
it you fix it and then you scan all of
the other ones to see that their
references into that chunk are stoked
okay it's fast trust me okay so um I
have a demo which i'm not sure i'm going
to keep our sorry a prototype which i'm
not sure i'm going to keep working on
its implemented on top of the hd2 hd2 is
not a logging file system and if you
shut it down and expectedly bad things
will happen but chunk of that should be
able to deal with that right so it's
kind of putting your money where your
mouth is I have done the file
continuation part I haven't done the
directory part you know I just wanted to
get it working to show people what it's
doing and everything let's not really to
end that works if your file systems
developer you know that map is hard and
it's really hard with this structure as
hell because you have a bunch of
different file systems all contributing
not sure that's the right thing to you
okay so now we're going to do a demo
I know you guys tell me run this earlier
but we're gonna run it again so I'm
copying over a disk image that contains
an already set up chunk FS file system
it's got three chunks in it and each of
them is an ext2 file system and they are
glued together by this VFS level
framework there we go okay so I set them
up and then this is the the the command
that's gonna that's stomping on and i
know that's a part of a file that
crosses chunks i realize i can actually
do this etfs to buy whatever anyway so
I've just corrupted my file system
awesome now I am going through and
checking each of the file systems did I
do yes I corrupted the the first one
over here um filesystem check found that
there is and i know'd
yes there's a link to an inode which is
not allocated correctly so which is what
i just did to it fixed yes now this one
isn't use anymore and then okay
alright so that's where we ended when by
the way Owen important that my demo
worked give it up yep live demos awesome
but I also managed to do this when I was
in Australia too soon ok so my little
dumb script that does the the check says
notes that that that chunk was bad
because we're going to have to come back
later and check everyone else's
references to it but the next thing you
do is you check the second chunk and
that is all fine nothing wrong with it
check the third chunk it is also fine
great so now I'm going to mount them and
look at the list of references to other
chunks so this this is what I was
talking about to get a reference i'm
going to chunk tune and saying what are
all of your references to chunk 1
because one of them might be broken oh
look turns out chunk 1 I know 29 there's
a link there this is a fancy thing to
see if that I note is allocated and it
says no it's not allocated so to the the
chunk of s file system repair script
says ok we'll remove that because it's
dangling it's no longer allocated it's
basically an orphan i know'd ok and then
we check chunk 3 chunk 3 has no
references from chunk wine great we're
done so that and then this mounts it to
make sure this is all part of my hideous
pile of work around to make this work
mount it check if it's eric this is a
bunch of debug output
not big as the file that crossed the
boundary that then I destroyed so we
want to make sure that that's gone
that's gone then we want to check to see
that the that we found and got rid of
this dangling reference from chunk 1 and
that is also gone so that was
incremental file system check and repair
it can be done ok so the some cool stuff
about chanco FS I really like it's got
um it is by design multi-threaded I mean
what you're doing is you are explicitly
calling out the when data operations are
dependent on other ones you can create
and delete files all day long in one
chunk without having to synchronize with
anything going on in any other time
because you know that they have nothing
to do with each other they're not
sharing any data structures they don't
they're not allocating from the same
free space you know they're not working
on the same directory so it's really
cool it's really nice when you're trying
to do one good thing and you get another
really good thing out of it so always a
good time other stuff so you get that
increment online remember yes a
surprising thing is how useful chunks
are for a bunch of file system
operations people want to do shrink grow
and defragment you cannot um everyone
can grow a file system that's not super
hard and like Oh changed my length my
length is now this shrinking is really
hard I think was like the last thing
implemented in CFS I'm not actually sure
that it is implemented because last time
I asked they said we almost have it it's
really hard to have sorry not yet yeah
you know it's hard so basically you're
doing this reverse mapping problem you
have a disk and you want to remove this
part well who's pointing to the blocks
in this part I don't know I'm gonna have
to walk from the top of the file system
down and find out what points to here
which is slow painful but doable except
you have to take the file system offline
because if you're changing things
suddenly you know you can al
from there or you could fence that off
well anyway it's hard with chunk effects
it's really easy who's referencing this
set of blocks while only references
inside of this set of blocks okay so
stop fence it off stop allocating from
it move things you don't have to walk
all the way down from the top of the
tree you just have to walk from the top
of that little chunk and move all of
that stuff you can also do all these
kind of cool things that people keep
coming to me with new ideas for how to
use this where you can defragment your
system by shrinking it and then growing
it again or you can cooler you can have
a different file system layout for each
chunk and if you decide that you want to
change what those layouts are you can
shrink it take away those trunks and
then put them back in a different format
and reuse them so if your file system is
changing use cases from a lot of small
files to a lot of big files you can
reformat that way all right I have this
idea of the the one big file file system
where it has 1i node and the rest of it
is just the file data you could do that
for individual chunks if it worked up
pretty well I it crashed only software
is this pretty cool idea which is
somewhat misunderstood which I thought
was pretty awesome initially but i had
no idea was build building something
that was crashed only fresh only is the
software is this idea of your system has
to deal with crashes happening it has to
behave correctly and it has to recover
quickly so as long as you already have
these requirements you might as well say
that you're going to take advantage of
that and shut down the system by
crashing it and recovering it the so one
this is going to be fast because it has
to be fast and two you're going to be
testing the code for crashing recovery
all the time or just be like Open Office
Cinda unnecessarily recover frequently
but uh rationally software does not mean
that you don't make an effort to save
data I get it written out to desk or
anything like that but yeah so based on
the description it looks like this cause
system is not at all these are over
there no it is about data recovery it's
just acknowledging the fact that that a
data loss occurs and dealing with it
quickly so presciently software I'm
describing what I was just about to say
that that's the misunderstanding about
crash only software that you that people
need to understand rationally software
is not about oh and you know oops i
tried i'm just going to crash now
crushingly software is is saying i am
going to be aware of the fact that i'm
going to crash and try to get things to
operate in such a manner that if i do
crash the the the badness that happens
is minimal so in in non rationally
software you're going to save up a bunch
of changes and then not actually write
them to disk and crashing tlie software
you're going to write them to disk
frequently instead of waiting until the
end when you close the program I'm sure
like Firefox used to do this you change
your preferences and if it crashed
before you actually quit officially it
would not save the freaking preferences
right that was incredibly annoying to
make it crash only you save it right
that not waiting until this um one of my
friends put it this way all of the code
that happens you know in your clothes
function the the explicit shutdown
function it should be there should be a
comment at the top that says wishful
thinking it's like sure you get to run
that if the stars align you know if I
didn't just you know control see that
program that kind of thing so another
another aspect of presciently software
is that it says to build things in small
rebooted pieces so that you can have a
bug or an error or a data problem in one
small piece and then you can restart
that piece by itself everything else
keeps running along and then that part
starts operating again so that's what
the chunks are you can cut them off
fix them unmount them remount them
everything else just keeps going ok I
think okay this is where I admit why
this will never never work and so the
I'm not the only person who thought oh
hey I'll just take much little file
systems and like in reality and
implementing it and glue them together
in in the end I don't think it's a good
way to go long-term there's all these
annoying issues so the logs are a good
one i'm using hd2 I like ext2 ext3
pretty much works but in reality you
should probably have a logging file
system inside of each chunk and then you
have a bunch of logs one per chunk and
logs have to be have to have memory
pinned for then and then you have to
write out to them and and then you
needed a log across all of the chunks
and this ends up looking very bad and
not like something you actually went to
do so I just kind of punted on that
issue and then the next thing I realized
and this is actually has applications
for all kinds of other things that
people are trying to do so the way that
you do a lot of people are trying to do
this kind of thing on lenox we each file
system exports a set of VFS operations
like you know allocating I nodes and
creating directory entries and things
like that and then the VFS 6 sits on top
and does all the permission checks and
things like that well a bunch of people
are trying to get in between the VFS and
those file system operations they're
calling the stacking file systems or
layered file systems chunka fest is only
one of a long line of file systems
trying to do this I finally concluded
that we're doing the wrong thing there's
there's these great little interface
little a function sticking out of the
file systems that are intended for use
by the VFS we shouldn't be using those
they're just not the right operations we
end up having to fake up a bunch of
stuff and pretend that we have weird
structures that we don't actually need
and we don't care about when we're doing
layering file systems so I actually
think that there should be in an
interface inside of the Linux file
system stack that says and this is for
stacking file systems because this is
what stacking file systems really want
they really want to just allocate this
inode they don't want to create a d
entry for this you know
kind of thing I would love to get
somebody to agree with that and start
working on it so so that's the lower
level ops it's about ok questions my
superfast checkup s presentation so
sorry I kind of asked that I breathes I
designing for crashes in my butt so
while admitting that you know when I
asked about you you replicate all the
data to the continuation I know it's the
possibility which you can use them to
recover the corrupt exactly right right
but the downside being that now every
time I do it Ramon it's got a right to
every chunk that contains any of the
data for this file check which for a lot
of files is going to be spread across a
lot of chunks yeah so now I've got a
synchronous operation across six chunks
which could be on the same disk but once
I it doesn't have to be synchronous
there's logging still works so you can
write a log entry that says change this
and then do it in a lazy matter who you
know because the thing about a lot of
this stuff is that when you're when
you're when you're writing redundant
data you don't have to do it right away
it's just it's not an issue of
correctness it's an issue of how big the
window is during which if you lose this
data you also you lose it all overall so
I mean there's a bunch of there is cost
I've done some mathematics on the cost
doesn't look like it's a big deal given
most regular reasonable file systems so
most most files are in most cases fifty
percent of your files are under 4k
they're not even be in more than one
block much less more than one chunk
given those calculations what kind of
chunk size you imagining um I did some
math and showed that uh i think it was a
gigabyte it was fine right and that's so
tiny I mean how long does it take to to
check a gigabyte a second two seconds
and I did some math on okay and then
include checking all of the references
in all of the other chunks and then
assume that it takes a worst-case seek
every single time to get one of those
references and deal with it and it was
like oh then you added two and a half
seconds to the one second check sign
time so you know it really has chunk
size is really dependent on how
localized references are inside of the
file system and references are pretty
localized there's very few hard links um
you know most my smiles are small and
most files are located near the
directory that their reference from I
was really pleased with that result like
wow okay bye it cool I mean what can you
do think I goodbye it seems like then
you'd have if you have your trim size of
a gig by a lot of the files would be
interested in or will probably do with
anything within yep some of them to
spend many many jumps maybe yeah yeah
yeah most notably seems like basically
is false right
they deposit which is those the ones who
care about performance you act when
playing tricks trying to guarantees and
sort of particulars allocation to what
degree will will that so I mean it is
effective it can be effectively
contiguous I mean you lose most of the
the benefit from contiguous allocation
and contiguous allocations of above like
I don't know 256k so at that point it
kind of doesn't matter as long as
they're fairly close yeah yes you can't
have different chunks sizes as well so
so something is to get the big files in
the beach well so one of the nice things
is that all of the all of the algorithms
that you want to use to defragment
things and keep things local and try to
allocate them sequentially sequentially
in chunks are exactly the same
algorithms that we already had to
develop and use in modern-day file
systems so it's I mean you're just
adding a little layer of indirection
between things and existing file systems
you want to lay them all out
sequentially in chunka fest you'll want
to lay them out sequentially inside of a
chunk pecan and be nice to get a
sequential across trunks but it's not a
big deal yeah
do you need a host file system to back
the chunks or um well the thing is is
that once you have a file system inside
each chunk I mean they don't have to be
contiguous they can all be located in
different devices no i wouldn't i don't
i'm a personal personally i'm an
advocate for folding the vm sorry the
volume management layer into the
filesystem layer i think that that
taking a disk interface and exporting
another disk interface above it is very
bad you lose a lot of information you
introduce another layer of indirection
for not a whole lot of purpose so I i
I've failed to mention that butter FS
btrfs is a new Linux file system
copy-on-write check some storage pools
all that kind of stuff and the author
Chris Mason is at least taking very
seriously these concepts and trying to
build them into butter FS so trying to
compartmentalize metadata in such a way
that you know if something goes wrong
here you can repair it without looking
at things outside of it so like
splitting up the free space map is one
of the major things so that may be the
future of these ideas is actually being
inside as part of a file system being
designed in from the beginning like I
would like to see okay I think oh wait
sure
you've underneath thought in any sort of
generational thing kinda like the
generational garbage collector to keep
some chunks is not changing and it seems
like the time to recover is proportional
number of the chungs didn't change uh
yeah well so I actually did some
measurements on that and um most sort of
simulating again this is a chunk most
parts of the file system are idle most
of the time like ninety-eight percent
ninety-eight percent of the time a sort
of thing generally speaking not a lot of
stuff is undergoing any sort of change
but just because it's actually active
and changing doesn't mean that you have
to check it you in this particular
implementation with you see to yes you
do but any smart way of doing it you
would only check things that had a
problem with them or related to
something that had a problem so sup
makes sense okay all right okay I think
we're done questions appreciate you guys
coming
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>