<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Photographic Tech EDU Day 26: Image quality testing and... | Coder Coacher - Coaching Coders</title><meta content="Photographic Tech EDU Day 26: Image quality testing and... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Photographic Tech EDU Day 26: Image quality testing and...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uRtnFIbFO9k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah I'm Norman Koren i'm the creator of
emma test software i like to thank dick
lion and Gary M blur for inviting me
it's quite a thrill to be here in the
center of the high-tech AKA geek world
and i also want to thank Google for
making it possible for me to do this
business because as a small entrepreneur
with not much in the way of an
advertising budget google has enabled me
to communicate ideas to my potential
customers and other interested people
I'm sure there are hundreds of thousands
of small entrepreneurs around the world
who are creating businesses that they
couldn't have created without google so
I'm very appreciative of that anyway I'm
going to be discussing Emma test
software that I've developed for
measuring image quality in any sort of
digital imaging system most commonly
digital cameras but it also works with
scanners scanned the film images and
other devices I'll talk a bit about the
background of Emma test how I got there
then I'm going to go through a number of
the key image quality factors and how
they're measured and then I'll review
the same material discussing the emmitt
test modules it's a modular program that
analyzes a number of tests charts and
I'll kind of go back over that material
here is an example from the internet of
an image that clearly has quality
problems I chose it because this is a
building that I worked in for a while in
Boulder it's the Department of Commerce
NIST building that contains the world's
master clock and I had an office for a
while down the hall from it I always
used to think every time somebody would
say oh I wish they could turn back the
clock I think if only I could hack that
clock down the hallway but I'm not that
good in any case this image shows a
number of image quality issues and these
are
typical of the sort of issues that we
want to be able to diagnose and predict
from testing and so it's not let's see
if I can make this here we go not real
real clear but there are some vertical
lines here that could be flare alight
I'm in fact not but you can see it above
the car clearly I'm not sure what the
source of those lines are the sharpness
is generally poor in this image there
are some jpg artifacts II these are
waviness near boundaries it's easier to
see on a laptop screen and there's very
poor tonal detail in the shadow areas
here the question is is this detail
being captured it's affected by response
curve and flare light and if it is
captured you may want to do some post
processing enhancement on it so anyway
this is an image with a number of issues
I created him a test really to measure
and predict system performance and to
answer questions about for example which
lens is a better lens that photography
nuts have been arguing over forever I
decided I needed a way of answering that
question the way that Emma test works is
you photograph a test chart and then you
analyze it with the am a test software
and I have a bunch of examples some
fairly pretty of what I do one example
is you can photograph the great egg
Macbeth color checker in a scene and
then you can analyze the colorchecker to
see how the cameras white balance
algorithm and also how its exposure
algorithm is working but I have a number
of other test charts and I'll be going
through several of those during the
course of the talk in fact I created
Emma test for serious amateur
photographers I thought that's where the
market would be and I was quite wrong
which turned out to be a good thing yeah
I thought it would be for us guys with
digital slrs who have collections of
lenses and we would want to
find out which lenses are best and under
which conditions which focal lengths in
which apertures they work the best so
the original version of Emma test was
designed to look at you know camera
response curve or dynamic range
sharpness and color accuracy then I
found it was being purchased mainly by
corporate customers which is a good
thing and I've gradually adapted to
their needs although I still have an
Emma test light version that meets the
needs of like serious amateur
photographers the program runs in
compiled matlab you don't need matlab on
your computer because it comes with a
library it's downloaded from Emmet Escom
and sold online and it consists of a
number of independent modules that each
of which measures a test chart and what
I try to do is extract the greatest
information from each test chart and I
find that I'm constantly developing it
adding support for new charts new image
quality measurements about half the
ideas come from my head and have come
from customers probably the better half
from customers now in the talk i'm going
to use an image i took from a wonderful
place called hunts mesa and monument
valley as an example of image quality
degradation i'm not sure how clearly
it'll show up on this screen but in the
power point it should be quite clear and
what I do is on the crop of the image on
the left side I have the original image
that always stays the same on the right
side I have a simulated image quality
degradation that i use to discuss what
this particular issue is some of the
things we have to think about when we're
talking about image quality are is the
quality factor mainly a function of
image capture meaning how the chip
performs and the electronics coming out
of the chip or is it a matter of
post-processing which would include
everything out of the chip including d
mosaicing and any sort of you know
contrast adjustments sharpening and so
on white balance that happen after the
imaging chip another issue that i'm
going to skirt is the issue of objective
measurements versus subjective judgment
which is to say i have a number of
objective measurements sharpless noise
color accuracy and so on i have some
information on how they relate to how
you know customers or clients will
respond to that image but that's a quite
a complex and difficult subject for one
thing different customers are going to
proceed image quality quite differently
so somebody who's doing aerial
reconnaissance is looking at very
different things from somebody
photographing a birthday party and
therefore I've kind of mainly
concentrated on providing the world with
convenient easy to do objective
measurements and I'm hoping that as time
goes on more and more relationships will
be developed between the objective and
the subjective like how good is that
picture a quick summary here I'm not
going to spend much time on this slide
but there are a number of image quality
factors and I'll be discussing these
individually and they are analyzed by
several of the EMA test modules these
images show the type of charts that are
used in each module for example the
slanted edge for sharpness another chart
which we use for sharpness and also to
look at some signal processing artifacts
the good old grey tag macbeth color
checker which is very widely used for
measuring color although we measure a
great many other color charts step
charts which are monochrome charts that
have density steps light fall-off
measures the uniformity of lens and
sensor response
it has quite a number of outputs that
you just photograph a blank surface or
perhaps a integrating sphere and finally
distortion where you photograph a grid
and measure lens distortion we'll get
into a bit more detail sharpness in my
opinion is the probably most important
image quality factor though certainly
not the only one because you can have
very very bad sharp images if they're
you know often color or exposure this
image shows an example of how we measure
sharpness at the top what you see is a
sign pattern you have an original sign
pattern that's modulated sine wave
increasing and spatial frequency a
spatial frequency is not familiar to
most people but it's very similar to
audio frequency aka pitch the better the
response at high spatial frequencies the
sharper the image the more detail you
can see in an image so I'm showing what
happens when you blur this case it's a
simulated lens when you blur a sign
pattern at the high spatial frequencies
of contrast drops similarly this is what
happens when you blur a bar pattern this
is the amplitude of the bar pattern that
drops off in frequency this blue line is
the envelope of the amplitudes of the
sine pattern as you go up in spatial
frequency this response is called the
spatial frequency response of the
imaging system or also called a
modulation transfer function they're
synonymous in practical cases when you
have a good quality test chart they're
the same and this is what we use to
measure lens sharpness this is in a
spatial frequency domain and the nice
thing about working with spatial
frequency response is if you have an
imaging system that consists of several
components you know lens sensor and then
software sharpening and noise reduction
which do opposite things
the multiple of these that the product
is the total response of the system in
spatial domain you can't do anything
like that you'd have to do a convolution
which is extremely awkward and not not
terribly enlightening so this is an
example of the EMA test sfr spatial
frequency response module what you see
in the image on the right is the
unblurred image and an example of a
blurred image this turns out to be
Express these results are expressed in
terms of the spatial frequency response
on the left the black curve is a
relevant curve that describes the system
it rolls off in frequency so that at the
fifty percent point is about point three
to four cycles per pixel I should
explain that there are a number of ways
we can scale spatial frequency cycles
per pixel is a scaling factor that lets
you know how efficiently you're using
the pixels this number point three to
four is typical of a good quality
imaging system the Nyquist frequency
which is a half a cycle per pixel is the
highest frequency at which you can
detect real data anything above this
frequency is essentially garbage or
nonsense it's going to be aliased to a
lower spatial frequency and I won't go
into the detail now but if you're
familiar with digital sampling it's it's
a pretty familiar concept so what we're
looking at is a measured measures of
sharpness and the one that I most
typically use is the fifty percent MTF
where it drops to half the low frequency
value sometimes I use half the peak
value and that number is very similar to
bandwidth
our band with an electrical engineering
so it's it's a good but perhaps not
perfect a number for summarizing
sharpness now you see i have a dashed
red line in this curve on the left that
has to do with something I've done which
is called standardized sharpening and
what that is is a little algorithm I
cooked up that enables me to compare
different cameras I'll discuss
sharpening in just a moment but the key
there is that every digital image at
some stage in its processing needs to be
sharpened I mean the digital sharpening
is one of the key advantages of digital
imaging and you know lenses and I'll
have roelofs an MTF and you can recover
quite a bit of perceptual sharpness by
sharpening cameras have different
degrees of sharpening usually built into
the camera some of them have severe over
sharpening which I'll discuss the
standardized sharpening that I do over
here and it's discussed in a lot of
detail essentially normalizes the image
this particular image is actually under
sharpen it has less than it really
should so that different cameras with
different degrees of sharpness heather
sharpening either boosted or cut so they
can be compared but it's not a perfect
way of comparing cameras it needs to be
taken with a grain or two of salt but it
is somewhat useful for comparing
different cameras let me talk a little
bit about sharpening here in digital
sharpening what you're doing is from
each image pixel you're subtracting a
portion of the neighboring pixels and
that has the net effect of sharpening
the appearance of the image in this case
an edge such as we use for testing the
black would be the unsharpened image the
sharpened image will be the red this has
a little bit of overshoot which
generally is not visually objectionable
but this sharpened image is going to
look a lot better to the eye and you'll
actually be able to see more detail
visually in the sharpened image of
course you can over sharpen an image and
that's pretty common with a compact
digital cameras and when an image is
over sharpened you see a peak here this
is the average edge response so this is
spatial response and you see these peaks
and a lot of inexpensive compact digital
cameras and this peak in the spatial
domain corresponds to this quite large
peak in the frequency domain I often see
peaks of no fifty percent above the
nominal low frequency response or
greater the over sharpening does improve
the measured MTF response so it is a bit
of a problem it's kind of a way of
cheating in making digital cameras look
good so in general one has to say
sharpening beyond a certain amount is
probably going to degrade the image I
happen to think about twenty percent is
a good sharpening number that maybe you
don't want to go beyond and for some
instances maybe less maybe a little more
is suitable a little bit of over
sharpening definitely does not hurt an
image but over sharpening is common
enough and it is something we need to be
concerned about in measuring cameras and
you do need to you know when you develop
an imaging system you need to select an
appropriate sharpening recognizing that
there's going to be a little bit of loss
in the display device so you may want to
go a bit over the theoretical optimum if
you don't consider the display device
well MTF is a measure of the sharpness
of a device one of the things that is of
great interest is to relate the MTF or
spatial frequency response to perceptual
sharpness and in in the test we recently
introduced a measurement called
subjective quality factor or sqf that is
a perceptual measure of how sharp an
image is going to look this is a
measurement that was developed in the
early 1970s at Kodak and has been quite
used by Kodak and polaroid although I
just yesterday talked to a former Kodak
scientist who mentioned some other
measurements it's not a perfect
measurement and none of them are as far
as I know this was a friend of mine who
was a former Polaroid scientist got me
going on the sqf measurement but
essentially what it involves is it's a
way of combining the MTF curve of the
camera the eyes contrast sensitivity
function the human eye is sensitive to
contrast at spatial or angular
frequencies that peak around six to
eight to ten perhaps well six to eight
cycles per degree and so very low
spatial frequency information you don't
really see it's just a variation in
brightness at high spatial frequencies
your I can't resolve it so what's
happening in that that angular range is
important of course now we're talking
angular frequency which has to be
converted into spatial frequency to work
with MTF so what we do is we plot this
SQ f versus image height making some
assumption about viewing distance and I
allow you to pick the assumptions but I
typically assume that the viewing
distance is going to be proportional to
the square root of the image height I'm
thinking in terms of say fine art
exhibition prints you tend to move away
from a really big print but not
linearly with a print hi it's just a
guest and to be able to get a result and
so you come up with a number that plots
sqf the black is for this MTF that
doesn't have the standardized sharpening
it's a bit of an over sharpen system in
fact if this number is too high with sqf
that's an indicator that perhaps there's
too much sharpening and quality may not
really be improving but this is a number
that that does give a reasonable
perceptual measure of sharpness it's
still a number that's unfamiliar to most
people so I'm hoping there will be more
development in relating this to user
preference but I know codec and Polaroid
it have done quite a bit of work with it
and curiously enough popular photography
magazine in their lens tests if you dig
deep into their website you'll discover
that they're using sqf as a measurement
I'm not even sure that the people there
understand what it is or how it's
derived its software that's been handed
down but in fact their numbers are the
same numbers that I use an EMA test
except that it's for the lens only
whereas emmitt test is measuring the
entire imaging system so that kind of
wraps up the sharpness part of the MTF
measurement the one thing I should point
out is that the the image that's used
for measuring the sharpness is a slanted
edge and the slanted edge is light to
dark it does a fair amount of processing
on this edge so that it averages it over
samples it so that you can look at
response above the Nyquist frequency but
one of the key things to note is that
typically close to edges cameras are
doing a great deal of sharpening very
typically if you go away from an edge
the camera will say anything that's here
is noise and it will do noise reduction
which is the opposite of sharpening it's
in fact blurring or low pass filtering
so
the contrast of the edge in real camera
systems with this as i call it nonlinear
signal processing can vary quite a bit
with a contrast of the edge not because
of the sensor orleans yes while we get
into this but uh so you're talking about
this general Eastman some cameras do
luminosities different color
right interest interesting question I
have a little program for printing out
these tests charts and you can print
them out with any of the primaries as
the highlight value of course you can
always put a filter in front of the lens
I'm working now with the RGB and the the
luminance channel the y channel which is
the weighted sum you know mostly green I
have not done this with the like the
luminance and chroma the two chroma
which are not quite uniquely defined but
I do in fact in many cases if you turn
off the standardized sharpening then the
three color channels are highlighted and
you can see them clearly and see what's
happening there or you can in fact look
at say red and black blue and black
which is generally going to give you
quite a bit worse responds with a bayer
type sensor if you have a faux V on
sensor they're going to all be the same
so there are a number of options in Emma
tests that let you test out some of the
fine details and one thing that I'm
committed to is if you have measurements
that you're interested in that unit test
doesn't include well I've got the
interface and the whole structure ready
it's usually quite easy for me to add
options and I found ways of doing it
that I hope don't clutter the input
dialog box too much and I'm always eager
to add more measurements to Emma test to
meet everybody's needs I figure if I
make customers happy maybe I get more
customers hopefully that works okay now
we go on to a measurement that's
actually quite closely related to
sharpness and that's lateral chromatic
aberration that is the color fringing
that you see near the edges of the image
this is typically something that happens
in extreme telephoto retro focus or a
true excuse me extreme telephoto or
extreme wide-angle lenses the retro
focus lens
is in quite a few zooms usually not too
much of this is visible in normal focal
length lenses and what lateral chromatic
aberration is as I say you see it as
color fringing near boundaries oops like
near the mesas here and in fact it isn't
really simple boundary position
adjustments but there's actually quite a
lot of change in the shape of the
transitions this is most pronounced near
the edges of the image typically there
isn't much of this near the middle
although you might see this if there's
some miss registration between pixels my
perceptual measurement of chromatic
aberration which I'm using for now is
the the area in pixels between the
normalized images this goes from 0 to 1
it's normalized horizontal axis is
pixels so the area is actually measured
in pixels typically this increases
linearly with distance from the center
of the lens or roughly so not certainly
not exactly so I also have chromatic
aberration as a percentage of the
distance to the corner but these are
pretty good measurements and lots of
detail on the lateral chromatic
aberration which is something that can
be corrected in software post-processing
noise is another very important image
quality factor that could be equal or
certainly close to sharples in
importance and you know that's the sort
of graininess noise and film was called
grain we just call it noise and digital
imaging it comes from a number of
factors one is simply the number of
photons reaching the site in that case
the noise increases with a square root
of the light level so it the
signal-to-noise ratio goes down with a
square root of the illumination that's a
major factor especially in very small
sensors with small pixels
there's also electronic noise Johnson
noise 1 over F noise and a whole bunch
of other noise factors so we measure
noise in a number of ways in fact I've
recently enhanced the noise measurements
and recent versions of Emma tests so
that you can get several different
definitions of signal to noise ratio as
well as noise measured in pixels noise
normalized to the maximum 255 pixel
level noise normalized to difference
between black and white so that your
kind of normalizing out the contrast
differences of different cameras on the
middle one I have in this case noise
measured in f-stop so it's a noise
that's proportional to the illumination
level or luminance level I you know have
a great many options and of course
they're all described in gory detail on
the website yes noise when i'm looking
at a patch in either the color checker
I'm usually the using the bottom petrol
or the third row or the step chart what
I do is I I isolate the patch the signal
typically is the average the mean level
of the patch the noise is the standard
deviation of the level of the patch but
I do a trick before i do that standard
deviation what i do is i subtract off a
second-order fit to that patch so that
would mean that subtractive number would
have a mean of zero we don't use that
that removes a non uniformity in
illumination which is a big issue
generally from the patch and I do that
subtraction before I take that standard
deviation which is the same thing as RMS
or root mean square noise so then I take
the ratios and as I'd mentioned there
are a number of ways of defining
signal-to-noise ratio this was a problem
in my former field of magnetic recording
channels and the same problem persists
in
imaging you you whenever anyone mentions
signal-to-noise ratio ask them how they
define it and they probably have a
slightly different definition from the
one you're thinking of it I've learned
that you just have to deal with that so
I I will probably be adding more
definitions as people request them in
fact there's one called PSN are used in
video I'll probably add within the next
week or two it's a widely used standard
measurements and there are lots of
papers that will explain why it's not a
very good measurement that's why I
haven't added it so far but people want
it color accuracy is another important
image quality factor and in the color
check module we do an analysis of the
Gir tagme color checker to determine
color accuracy in this case we're
looking at the differences in the a B
plane of la be color space this is a
transformation of our gb that is
relatively perceptually uniform and very
industry standard that gives you a fair
idea of how far off colors are it turns
out the space distance in la be space is
a very first order way of estimating how
wrong or how off colors are there are a
number of other measurements that I
described on the website they all go by
the name Delta e for total change in the
space or Delta e which is just a or
Delta C which is just the change in the
chroma but there are a number of other
measurements that are weighted to try to
get closer to the how the eye perceives
color difference call there are Delta e
94 Delta ECMC none of them in my opinion
are perfect and again I plan to keep
adding options as they become available
or I become aware of them there's a new
measure called Delta e 2000 it's a bit
controversial I don't think it has a big
it's very complex so I haven't added it
yet I don't think it has a big advantage
over the Delta e 94
you so they're quite quite a number of
ways of doing this we also have a new
module called multi charts that analyzes
the color in a number of other color
charts and allows you to look at
segments of charts different regions and
produces quite a number of displays like
they will show you a little bit about
that later a more image quality factors
tonal response and contrast this is one
where there's no good or bad oh one
thing I wanted to mention about color
before I move on accurate color is not
always a pretty color or color that
users prefer sometimes users prefer
distorted colors like they like skies
that are bluer than the cyan that's
typical and more saturated they like
grass that's greener more cyan less
yellow than real grass people tend to
like more saturated skin tones and are
some cultural differences where as I
understand in asians tend to like skin
tones cooled a bit Europeans North
Americans tend to like them warmed a bit
so we all end up looking sort of close
to each other which we sort of do anyway
when we do these adjustments so
depending on markets people may want to
make the algorithms for color or the
reference values which you can change a
little bit different tonal response it's
a pretty important area that relates to
image quality and again perfect tonal
response may not be desirable you do
want to see what a camera is doing how
it responds to the tone for that we use
the step chart a module which analyzes
the step charts that have constant
density steps the tonal response is
shown here on a log plot the average
slope in the highlight to center region
is called the gamma gamma is
pretty much the same thing as the
contrast of the image is an encoding
gamma which is usually around one-half
there's also a display gamma that's 2.24
srgb is the standard for the internet
that there's a slightly different
standard for macintosh computers but
this does tell you the camera's response
typically what you see in cameras is in
the encoding side there isn't a pure
s-curve you typically get a roll off
near the highlights this is for
scientific accuracy not so good but for
pictorial quality a very good thing
because if you have a true linear
response you tend to burn out highlights
very quickly they go pure white you know
all 255 and this does not look good in
images so this type of shoulder on the
response curve over here on the upper
right of the curve that's what you get
in film and I think it's a desirable
thing to have as a part of your image
processing chain in digital processing
you see it and most of the better
cameras dynamic range is another
important issue in fact in digital SLRs
dynamic range is extraordinarily good I
think in ways better even the negative
film and way way better than slide film
that's the range of exposure that a
camera can respond to and produce an
image of a reasonable quality so we
measure that using a transmission test
chart this is an image of the stout 40
41 10 very inexpensive chart that that
you can buy and there are other charts
from image engineering they have a chart
that has a circular pattern that is
definitely a finer charting a lot more
expensive and we analyze both of those
charts and they show the response over a
wide range now the dynamic range is the
range of exposure and I usually use
f-stops which are factors of 2 to
measure the dynamic range whereas 3.32
f-stops are one density unit that's a
log 10 based unit
but the dynamic range is defined at
quality level that is where the
signal-to-noise ratio is better than a
certain amount so this particular curve
which is for a cannon I think it's an
older cannon that doesn't have too many
pixels so it has fairly large pixel size
and older g2 compact camera it's it's
quite a good dynamic range it's 9.75
f-stops at medium I'm able or the
customer mine was able to actually
detect 9.8 f-stops medium dynamic range
means that the f-stop noise is what is
it less than 0.5% high quality is noise
less than point 1 f-stop and at high
quality this old camera has a 6.83
f-stop dynamic range which is really
very good it's a lot better than slide
film in fact there has been a tendency
and a lot of cameras and especially in
camera phones to make everything compact
which means shrinking the pixels you
tend to get a lot more noise when you do
that so the effective dynamic range at
any given quality level tends to be
fairly low with some of the newer
cameras well to get around that people
do a lot of signal processing that can
be removed quite a bit of detail so
there are people I know who really like
the older digital cameras with those big
pixels of course a DSLR has big pixels
big cameras they are and that's what i
like to use for my own work lens flare
is another very important issue and it's
especially important for some of the
applications like yours where you don't
have controlled lighting that image I
showed of the boulder NIST laboratory
you had the predominant light was very
much behind the subject just off the
axis of the camera in those situations
light that bounces
off the lens elements or the barrel on
the inside of the lens can fog the image
that's fog is called veiling glare or
flare light and that's a fairly
important thing to measure there's also
ghost imaging that that you can have
ghost images are kind of non-repeatable
their unique to each lens and so I'm not
attempting it to measure that but to
measure the lens flare and this is like
a typical flare type of thing that you'd
get what I do is it's a part of the step
chart module I measure a kodak q13
grayscale and I have this black hole as
i call it it's actually a cavity lined
with black felt that no light reaches
directly so this is really true black
and I measure that after I use the
results of measuring this chart to
linearize the image I measure the
veiling glare or flare light is the
value of luminance in this black hole
compared to the white region and that's
typically around point2 2.5% and
definitely is something that depends on
the lens quality and you know the really
fine lenses with excellent coatings are
going to have low flare or veiling glare
levels and cheap lenses may have high
flare yes from the illumination source
if you are shooting against
that's much more prone so how do you
better back in your measurement just
remember to repeat the question okay how
do you factor it into the measurement if
the illumination source is near the
image there's so many possible ways of
doing it that I've had to make a simple
approximation I'm really not dealing
with the illumination source but what
I'm doing is in this target this is in
the central part of the image but I try
to make it so that the white background
I mount the q13 on a white background
normally when I'm testing things I'll do
it on a gray or dark background and I
try to make the white background extend
out so that the area goes well outside
the image frame hopefully something
equal to the image frame on the outside
that's pretty hard to do with a
wide-angle lens and the assumption there
is that the fogging part of veiling
glare is sufficiently high that that
this will give me a fairly consistent
and usable number that will give you
some indication of what happens if you
have a light source outside the image
but it's not perfect it is when you do
have a light source especially if its a
bright narrow light you're going to get
the ghost imaging and the thing about
ghost imaging is that's so variable with
different lenses you just have to you
know try out different things with your
lens that's under test there is as far
as I know no industry standard for this
because you know almost any company will
be able to make their lens look good in
some way and other lenses look bad by
setting up a test setup just so so it's
one of those tricky things you just have
to try to figure out what the conditions
of use for your lens are and maybe set
up something there that becomes your own
internal standard my way is have a big
piece of math board make it go away
outside the image hopefully that answers
it yes
quantify that as a percentage of your
eyes respond so how hard is it it's it's
it's pretty easy because I'm measuring
it right next to the kodak grayscale i
can use that to linearize the response
what i found is that almost all cameras
that i've seen their electronic response
is fairly linear as you go down to the
low levels so i'm crossing my fingers a
little bit here but yes i use the
grayscale to linearize the response and
then i measure inevitably much darker in
the black hole than in the 1.95 density
patch 19 here
ooh reference image with lights to there
we go good question the no I don't have
a reference image with the lights turned
off the let me think about that one I
could do that to see what the basic
fogging is this is essentially a
relative measurement right now basically
it's yeah I'm linearizing with a
grayscale this is something to think
about something I may want to add you
know what is the zero level when you
turn off the lights one of the pardon no
I I linearize it for the image under
test and one of the reasons is if you do
a different exposure you're going to
have a whole different set of conditions
you may have different amplification
many cameras have adaptive signal
processing so I use this gray scale for
the linearization and then i extrapolate
down to zero hoping that I get a
reasonable number what I do get in
practice is if you're testing different
lenses or different lighting conditions
with the same camera you will get a very
accurate relative measure I can't
promise you that it will be an absolute
measure that might be the same with a
completely different image sensor but it
is a very good relative measure because
this this is the best way that I know of
to linearize the image for this test and
we can discuss it later exposure
accuracy is based on a couple of
assumptions that when you're using a
reflective step chart the white area has
a reflectance of about ninety percent
and in the so-called theoretical ideal
exposure if you have linear you know a
pure gamma response 106 percent will
saturate the image so I do a bunch of
math
and I use the results from another step
chart or color check to estimate how far
off your exposure is this can be useful
if you have one of these charts in a
scene where you have an auto exposure
algorithm and you want to see how well
the auto exposure is working light
fall-off is a module that you work by
simply photographing a plain white or
gray surface the perfect thing would be
something called an integrating sphere
which is a bit expensive or you can go
to Home Depot and get a globe for a lamp
that might be the really cheap
integrating sphere if you figure out how
to light it right and this measures lens
vignetting lens fall-off in recent
months I've enhanced the light fall-off
module to measure all sorts of
non-uniformities and sensors like local
noise variations and quite a number of
things it produces quite a number of
displays showing how uniform your
response is so it started with lenses it
keeps going on and on as people make
requests lens distortion is the barrel
or pin cushion effect that particularly
wide angle or ultra wide zoom lenses
tend to have and that's important for
architectural photography and such
things for a lot of types of photography
it's not very important very important
for astronomical photography lens
distortion is something that can be
corrected quite easily in
post-processing and I come up with a
number of different coefficients that
you could use like third order and fifth
order and also I measure lens
decentering with a distortion module I
have these backwards one of the really
interesting phenomenon that I've been
looking at recently and this is a module
that's
up on Emma tests now but I still call it
a beta because I haven't finished the
documentation is I I'm very interested
in the signal processing and how in high
contrast regions you get sharpening in
low contrast regions often you have
noise reduction or low pass filtering
sharpening is sort of high pass a high
frequency boost so what I've done is
I've created a chart that can be printed
with the EMA test test chart modules
that has oops varying spatial
frequencies on the horizontal axis and
this is a sine wave pattern in varying
contrast or contrast squared on the
vertical axis and I'm looking now at the
normalized MTF I normalize it to one on
the Left I have many displays that I can
do as a as a function of the image
contrast now if I go back this is a very
good panasonic a compact digital camera
with a leica lens I got it for my wife
so I could steal it when I want to
compact camera and a tie so 80 what you
can see here is that it has quite a bit
of sharpening at the high contrast
levels the sharpening gradually
diminishes and the contrast at the MTF
at low contrast falls off but now if i
go to iso 800 it's quite different
there's very little sharpening even at
high contrast levels and things drop off
much more quickly I'm you know this is
one of the things I've been quite
interested in lightly this chart also
can be used to measure color more ray
which is an issue in some cameras colors
colors that appear because of the
aliasing effect if the lens is too good
for the sensor so this is a new module
that I've recently added and what I've
done now is I've covered
the key image quality factors we're
running out of time anyway I'm going to
take a quick spin now if I have a minute
or two left i hope to take some
questions but a quick spin through the
modules so we have step chart that
measures either transmission or
reflective step charts for tonal
response gamma which is contrast noise
dynamic range for transmission charts
and exposure error we have the color
check module that uses the great AG
macbeth color checker that measures
color accuracy also tonal response gamma
noise and exposure level although with a
bit less detail than the step chart
module a new module relatively new
called multi charts works with a number
of different test charts it has a highly
interactive interface and produces quite
a number of displays including a
rotatable 3d display of color errors in
this case we show it analyzing a portion
of the IT 8.7 chart which is an
inexpensive chart used fairly widely but
I support quite a number of different
charts including the colorchecker SG
distortion and light fall-off we've
discussed sfr we've discussed again sfr
works with a slanted edge image and
produces results for both the average
edge and the NTS so this is spatial
domain frequency domain produces a lot
of summary results that hopefully relate
to perceptual image sharpness and the
final summary is image quality consists
of a number of factors unit test
analyzes a whole bunch of them when I
figure out how to analyze additional
factors I tend to add them as i have
time to add them the key thing to think
in my think about in relation to these
factors is you know some of them are
primarily affected by capture
some such as white balance are mainly
post-processing but of course you have
to get the capture right if you are to
have the material there to work with and
post-processing to bring the image up to
a high quality image the waiting that
you'd give to each factor or the
importance really depends on the
application and the user preference so
that's something that you have to think
about what works for your application
and it is difficult to define single
measures of image quality it's kind of a
holy grail that we may not really get to
but it's something that at least I'm
trying to come up with more simple
measures of how good things look so
that's the end of the talk i think i
have about two minutes are there any
questions one slide back
ok on the overshoot and under shoot what
am I trying to explain ok the black
curves here are the actual response of
the system the dashed red line is with
what i call this standardized sharpening
meaning a way of normalizing different
cameras if I i define what i consider to
be over sharpening and under sharpening
and that involves just so basically the
response at a third the nyquist is the
same as the response at zero frequency
ok the question is is this a response of
signal or a result of signal processing
I cannot measure the intrinsic response
of a system without signal processing so
there's always signal processing
involved which may result in overshoot
or may the signal may be under sharp and
there may be very little so what I've
done with a standardized sharpening
which I often don't use is I've just
normalized that I sharp and under
sharpen the images I I cut down over
sharpen the images it's merely a way
that allows you to compare different
cameras if you're measuring your system
now you probably want to click the
button that turns off the standardized
sharpening and then you know forget
about it it it's it's it's merely it's
usually like four people reviewing
cameras for magazines so they can
compare different ones have some basis
not perfect mind you because you can
hide anything with signal processing and
I'm working on you know learning what
the signal processing is doing anything
more
wish I lenses um fisheye lenses are kind
of tough because of the extreme
distortion you can measure sharpness in
the center of a fisheye lens but I'm not
going to do very well at the edges the
slanted edge algorithm actually removes
second order fit it did it I mean second
order curvature from the image so a
little bit of curvature won't adversely
affect the MTF but a lot of the
measurements are going to have trouble
with fisheye lenses so you have to
linearize them first but you can work on
the center for a noise and a few of
those details or you can use the
circular patterns there's a chart some
circular test charts that will work with
fisheye lenses any others the siemens
star chart is a star type circular
pattern i plan to add support to it in a
month or two as i have time okay I think
our time is up so thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>