<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>End User Customization for the Mobile Web | Coder Coacher - Coaching Coders</title><meta content="End User Customization for the Mobile Web - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>End User Customization for the Mobile Web</b></h2><h5 class="post__date">2009-01-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cxqJoYzv8E4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and today I'm going to talk about some
of the work my group has been doing on
end-user customization for mobile
devices if time permits at the very end
I'll say a couple of words about some
other projects that we have going on in
general I lead the mobile computing
research group in in our department all
right this is probably all hat for all
of you but I mean just I guess you know
mermie bear with me as we all know
mobile devices are challenging and they
present a very challenging environment
for people interested in browsing the
web which will be the focus of this talk
I mean there are many many reasons for
this you know among others the fact that
there is actually significant
variability between the four lenses
provided by mobile devices and I have an
example here where you can see you know
different devices with completely
different display sizes energy is also
very significant issue in mobile
environments because well roughly you're
limited by the size of the battery that
you're willing to carry around bandwidth
is an issue it has been getting better I
mean we've been getting better what are
your connectivity but still the kind of
bandwidth and latency that you see over
a 3g connection are significantly lower
than what you get over let's say
residential dsl connection that is you
know half decent and worse in for many
many people bandwidth the still not
three so either they have a plan that
has a hard cap or they pay you know X
amount of dollars per megabyte and what
all these boils down to is to a fact
with that we actually have to adapt web
content to make browsing on mobile
devices a more enjoyable experience for
the mobile device for a more user I'm
sorry and there are different ways in
which one can adapt content the one that
is used the most today is basically
manual adaptation so you actually pay
someone to maintain
multiple versions of your content for
different devices so you have a desktop
version you have a mobile version and
this is fine it works but it has a very
high human cost in it involves a
significant investment an alternative is
to rely on some kind of automatic
adaptation some kind of automatic
transcoder that adapted it's the data
for you but the problem is that this
automatic trans colors tend to be less
than perfect because they have a very
limited understanding of the data that
they are trying to adapt especially if
I'm thinking of applying you know a
transcoder to pretty much all data that
may come from the web so an example that
we can provide is you know if you're
let's say browsing the web because you
want to buy a mobile phone well this low
fidelity version of Sony AIBO dog is
probably fine because you know you don't
care about buying I Bulldogs but you
know if you get a similar feel a
diversion for phone this might not be
sufficient because you may actually want
to know more about this phone so the
issue is that we want automatic
adaptation but at the same time we want
this adaptation to be tailored to what
the user is going to do with that
content so we want smart automatic
adaptation so today I'm going to
describe two projects that attempt to do
what we call end user customization they
basically put the user in the driver's
seat and say well the user knows what
they want so let lets them you know
adapt the content but you know that
basically just pushes the BOK to the
user so now actually all the effort has
to be put in by the users so what we're
going to see is techniques that are
actually try to limit the amount of
effort that the user has to put in so
that potentially I adapt the content
only once and then I can actually
leverage the adaptation over a long
period of time and that is what the
first project page salaries is all about
and then I'm going to
talk about the second project that we
call Eureka where what we're trying to
do is we're trying to leverage the
effort put in by some people in the
community to benefit other people in the
community specifically if we have maybe
a subset of people adapt the content
then maybe a larger group can actually
take advantage of whatever adaptations
they make so let's start with paige
Taylor and with a very specific example
let's assume that you happen to to be
avid BBC news readers this is a
screenshot taken about a year ago of the
BBC main main website this is how it
looks on a desktop and it's you know
that particular layout is optimized for
the desktop and it works very well the
problem is that if you actually want to
read this page using a mobile device and
just as an illustration I have a PDA
here but the form factor I guess of this
PDA is it's very similar to what you
would get with the current you know an
iphone or a 9th an android phone give
give or take so the problem is that you
know it has a display that is much much
smaller and only a small portion of the
page fits in there so in this case let's
assume that the user is interested in
the top stories that occupy that portion
of the page plus some popular stories
that are in a different side of the page
so in what they'll have to do is they'll
have to do a lot of scrolling and with
something your browser still have also
to do suming in and zooming out and yes
you know this works but you know it gets
in the way of fluid interaction it's
extra effort that you have to do to
accomplish your task which is basically
you want to read those things so what
paige Taylor is is a program that runs
on top of your browser that lets you
customize on the device pages that you
download so what it does it pacifically
let's remove elements so if let's say
I'm only interested in those on in on
those tidbits of the page I can remove
all the other portions I
I don't care about I can actually move
them around I can relocate them and I
can actually resize them so it's easy to
read what's interesting is that paige
Taylor actually records these
customizations and we applies them on
future visits to the same page even as
the page changes even as the content
changes because you know if you go to
the you know you go to the CBC page you
know you show up there 20 minutes later
there'll be new stories so even as the
content changes it will reapply these
customizations so that you get it you
know you get the new version of the page
in you know the shape that you want it
and more so it will actually try to
reapply the customization to other pages
in the same side so an example of that
if you customize you know a specific
page that shows one news item if you
then go to a different username it will
reapply your customization so that you
get them automatically so some
advantages it's a very easy to use tool
I basically you get what you see is what
you know get interface it's very
expressive in the sense that supports
quite a few different other patients
right now we we have it a prototype
that's an extension of the minimal web
browser it's all written in JavaScript
and it's all client base it all runs on
the mobile device you know don't need
the support of a proxy although if you
do have a proxy then the proxy can
potentially do some of these changes and
you know save valletta energy in the
back on the device by not downloading
elements that you're going to crop in
any case
so some of the advantage of this
approach will basically puts the user on
the driving seat so they really get to
modify the page any way they want it and
it's also good for the provider of the
content because they don't have to do
anything there is question is well why
would people do this OH or or when does
it make sense so it turns out actually
customizing the page takes more time
than just taking the pages is and
performing your task because you have to
do even more scrolling and you have to
tell it you know that you want to hide
certain elements and resize other
elements so the total time if I'm if I'm
looking at the time to complete the task
actually goes up because now you're
first customizing and then you're
performing your task so if you're going
to use this content only once this
doesn't make too much sense now for
content that you're going to be coming
back to you over and over or if we can
reuse a customization across similar
content then it makes sense in other
words if this is a page like world that
you're going to be checking every
morning then it's actually okay to maybe
spend you know the first time that you
go to this page you spend five minutes
customizing it because you're going to
reap the benefits of your effort over a
long period of time and the same goes if
let's say I have to go to my a small
piece a small portion of the website if
by customizing a small portion of the
website they then get to basically
browse you know the whole website then
maybe it's it's worth me spending that
effort in its own bath a few pages so I
asked well why does this work because
you're basically saying that you know
reapply customization as a content
underneath changes now the reason why it
works is that because most websites
basically use a very small number of
layouts and this layouts typically
remain unchanged over long periods of
time and by that I mean potentially
months in some occasions even years and
why is this well you know you want to
present a consistent interface to the
user
you know if the site is consistent it's
easier to use it makes possible running
and it's also cost-effective which is
good for us so how does it work well
when the browser renders a page it also
creates a Dom representation of that
page and we use that to record the
modification so for instance in this
very simple you know schematics of a
page we have a paragraph and an image
and let's say the user oops okay let's
say the user changes the size of an
image what we do is we actually record
that change and we record the path
across down where that change was done
so we record that in fact there's a
scene on the image that is in a given
position of the body and why is this
because we want to be able to reapply
the modification even if the change if
the content underneath changes a bit how
does it work well you know next time you
go to the same page we download the page
we render we execute all the JavaScript
that night be in that page and after
that paige Taylor execute and what it
does it basically tries to look for this
path of the content that you modified
and if you didn't if he figures out that
the path is still there it then applies
the modification this works but you know
more often than not there are actually
small changes to the page that make the
naive technique fail and an example of
that is for instance you have your page
and let's say that it was you know
slightly modified there is an
advertisement that was added you know
somewhere on the page most of the pages
unchanged and in fact the users
customization is still valid but you
know if we just try to do I guess that
topological comparison of the two Dom
trees I mean we'll find out that they
are different so instead what we do is
we optimize our search bit so that if we
don't find in this particular case I
mean this is like say the Dom that
results for the modified page if we
don't find a path
the original path in the tree what we do
is we do a search and we basically look
for nearby portions of the page of the
of the Dom tree trying to find out you
know that particular part that that we
modified the problem with that is that
it can basically result in false
positives to so to try to minimize the
amount of false positives we take into
account context so in this example if
you modify the image we also remember
that you know that image was let's say
inside a specific div or after a given
table and we use that context to try to
rule out this false positives in this
example we would find image and will
increase its size alright so question
does it work in practice so we actually
performed an evaluation with two kinds
of evaluations what they call a
quantitative and qualitative evaluation
and we answer a few questions first of
all I mean does it work on a PDA in
denser ecs too can you know normal users
actually take advantage to this to
modify content and the answer is yeah
they're willing to do that how long does
it take to modify a page it really
depends on the complexity of the page
some of the experiments we played with
these were fairly sophisticated pages in
even a very sophisticated page it takes
you about 10 minutes to customize it and
the other question we ask is okay well
given that you're going to have to spend
10 minutes customizing the page well you
know what kind of payback will pay off
you would expect to see you know for
that investment and what most people
told us is that you know as long as the
page remains as there as long as I'm
sorry their customizations remain useful
for about a month they will be willing
to spend 10 mins customizing so the
question that we then obviously try to
answer as well is this actually the case
for common pages do they you know they
customizations you know do
user-generated customizations remain
relevant for at least a month so we
performed two studies what we call a
longevity study that basically try to
answer that question and also a coverage
study which was looking at a slightly
different question which is well if I
modify you know one instance of in a
couple of pages in a website you know
can I reapply those modifications to
other pages to other similar pages and
roughly what kind of coverage of that
website I get by modifying a small
number of pages so to answer first the
longevity experiment what we did is we
downloaded popular pages from amazon BBC
ebay and msn and we actually want to web
archive and we got 16 we basically went
as far back as 16 months and we you know
got its historical it is also a trace of
each of these pages in assets changed
over 16 months we then selected you know
participants basically for for each of
those pages and we gave them the page
just showed up you know in day one
basically six months in in the past and
we told them go ahead and change it
anywhere you want to adopt it any way
you want you know any way you see fit we
then recorded that adaptation so the
users actually adapted it on the mobile
device but to evaluate the longevity
what we did is we left you know we told
them okay we're not going to do that up
against a valuation model device leave
them all device we move them to a
different setting where they had two
monitors in front of them the monitor on
the left showed two windows it showed on
the left side basically here what we
have is the page before their
adaptations on this side we have the
page after it was adapted and this is
basically on the day they perform the
experiment on the second monitor what we
did is we basically show them how the
page changed over time so we basically
you know jumped one week you know in the
future and we said this is what the page
looks like and we reapply the
modifications automatically to
you know the version of that page one
week into the future and we then did
that for you know two weeks a month you
know two months three months and so on
and what the users were asked were to
answer for each time we showed them you
know a new version of the page they had
to answer two questions they had to
answer first of all whether they're
customizations were still desirable okay
whether you know given the uncus to my
version of the page whether it would
still like the customization to be
applied and if that was in fact the case
whether the customization was actually
properly applied and let me just present
results for one of these sites the other
ones are also similar enough if you're
interested you know the end of the talk
I'll have a URL from where you can
download our paper that has much more
you know detailed results so what we saw
is that this is actually the page for
the BBC what we see is on the x-axis we
have time so this is basically the day
the customization was performed five
days after the fact 83 days after the
factory in 249 and you know 500 days
after their you general customization
was performed and on the y-axis we have
people so basically the data
customization was performing you know
all four participants we're happy with
it I mean after all day for from the
constellation you know what's
interesting is you know to even 500 days
after the fact they still think that the
customizations are desirable this means
that basically the layout of the page
hasn't changed that much now the
question is well you know given that the
customizations are desirable you know
let's say up to two hundred and forty
nine days in after they are done you
know for can we actually take advantage
of this and what's interesting is that
for seventy-five percent of our users we
can actually you know for one of the
users actually some of their
customization we're not properly applied
but for three or four in fact all the
customizations they perform were
properly applied up to a year and a half
you know after
you know after the original
customization so that's actually great
because it means that you know users are
able to put some effort to customize
their page and then they can actually
take advantage of that effort over a
very long period of time and for all our
sites this was the case for at least a
month so the second experiment oh by the
way if anyone has a question and wants
to interrupt me please go ahead and do
so don't you don't have to wait to land
right so you know just raise your hand
or you know shoot anyway so the second
experiment we wanted to figure out
whether you know what how many pages in
a website had to customize the pretty
much get access to the whole website and
what we did is we took four other
popular websites and twenty participants
and we asked basically five participants
for each website and we asked them to
customize two pages from each of this
website so if you think about it it's
basically sort of like a portal page and
then and I guess a news item page or in
the case of Flickr you know a specific
page for an album and then a page for a
picture and once they customize that one
page what we did is we downloaded a very
large number of pages from that same
website and we reapply the
customizations and we actually want and
we check them and we saw whether the
customizations were properly applied and
these are results for Amazon so in this
case what we did is we gave them ask
them to customize the page for a CD and
a CD listing and we then apply the same
customizations over a large number of
CPU no other city pages and other CD
listings and what you can see is that in
fact the customizations are reliably
applied you know across similar pages
which is great because what this means
is that when you go to Amazon first time
you go to Amazon you have to customize
you know the first time you go through
Amazon you have to consume I very much
every single page but later on as you
keep visiting other pieces of the Amazon
site the customizations are
automatically applied for you so the
amount or the number of pages that you
have to customize it in a website is
limited it's you know you only have to
customize very few pages and you get to
see the whole side so that's actually
very good some practical considerations
the way I've presented so far I mean it
seems as if you know the users have to
the customizations all at once in fact
actually that's not the case and the way
we see people using paige Taylor is in
sort of an incremental fashion where
they go to the page they do some
customizations they accomplish their
task and you know they go away you know
they come back to the page the following
day they figure out that they want to do
a bit more customizations and so on and
vegetable supports this so the 10
minutes or dimension that people have to
cement customizing the page doesn't you
know it's not something that you have to
pay up front you can you know customized
as little as or as much as you want
another technique that we've explored is
whether we can actually have if we
identify users that are similar I mean
whether they can share template so if
you know your body customized the page
in one way maybe you don't have to start
from scratch and in fact actually the
second part of this talk will describe
much more research into into that
direction all right so just to summarize
this first part of the talk I presented
an approach where we let users customize
on their device and the customizations
are reusable the customizations you know
are basically they reflect the user
interests and they can be reapplied on
the same content or across similar
content on the same page and this works
because the structure of the websites
tends to remain stable even if the
content changes all the time we actually
have a prototype of paige Taylor which
is available for download on that URL
alternatively you can go to my home page
which you know last night of this
presentation will have the URL for my
home page and there is a link to all my
projects and one of them is paige Taylor
and you can get the prototype from there
right now it only runs on on Firefox or
minimal which is really the mobile
version of Firefox all right so we are
very optimistic about the results we've
been getting with paige Taylor we think
it's a very useful tool it's great that
you can reuse the customizations that
you yourself made but at the same time
it still requires significant effort so
the question we asked was well can we
leverage this effort across users so yes
you know it's true you know different
users may want different things but in
reality there is only a limited number
of ways in which people you know browse
content and it's likely that you're not
the only person in the world that wants
the content in a specific way so can we
take advantage then for the fact that
maybe somebody else has already
customize the content so that you don't
have to start from scratch and that's
exactly what the second part of this
talk will will address we have a
technique we called Eureka which
transfer usage aware interactive content
adaptation that is based on this idea of
leveraging customizations across people
it works in the following way basically
the system provides content with random
adaptation if it doesn't have any
history and then lets users change it
this is in now learns from the users
modifications and builds a history
so that next time when somebody asks for
the same content the adaptation is
actually based on this history of you
know modifications made by other people
that have interacted with the content in
in the past now we believe that Eureka
can be applied to a wide set of
adaptations page layout is is one of
them but for simplicity today all
examples that I'm going to provide are
actually based on page on on image
fidelity why image fidelity because it
was actually just easier to build
experiments so that we can actually
value you know get hard numbers about
how good this technique was but we
believe it it can be extended to layout
so let me give you a very simple example
of how this works let's assume that we
have a mobile device that you know
access is the internet or some type of
connection the user downloads page that
has an image embedded in it is this
particular image the full fidelity
version of the image is 40 kilobytes so
instead of accessing the website
directly the user actually goes through
a transcoding proxy the transcoding
proxy gets the image and this is the
first time it has ever served the image
we all have any history for it and so it
applies predefined adaptation rule that
basically transco the image into lower
fidelity representation which in this
case is about one-fourth the size and
serves the user that low fidelity or
presentation well if it happens that
this particular user is actually a fan
of you know it's a beer fan and really
you know is not happy with a low
fidelity representation of you know this
picture of the favorite beer what they
can do is they can tell the system that
they want to improve the quality of the
image by for instance tapping on it so
we have a plugin for firefox that
basically the text of the user taps on
the image and this step is a hint that
the fidelity has to be improved so the
multiplying basically talks to the
laplacian proxy and tells it you know
give me a better version so we send a
better version it's still adapted it's
still not the full fidelity version but
you know it's it's an improvement now if
this is not good enough the user can ask
for a further refinement up to the point
obviously where they get the full
fidelity object what's interesting if
the delectation proxy actually monitors
this exchange and learns from it under
the assumption that well if these are
had to interact with the content it
means that the proxy didn't get it
perfect didn't get it right and we're
going to build a history based on what
we call successful out of patience and
we deem success once the user basically
doesn't ask for improvement so either
they're happier they gave up and we're
going to assume that they are happy now
over a long number of users I mean
hopefully I mean that will eventually be
the case all right so what happens when
user number 2 shows up or actually you
know user number 1 comes back well if
they happen to ask for the same image
well now we have history so we can
actually use this history to do better
so we actually serve the image you know
the 20 kilobyte image under the
assumption that you know that will be
fine if user number 2 is not happy he
can again ask for an improvement and
that will you know help us keep building
our history all right so how do we keep
track of history basically you know
history of adaptations really can be
easily represented as a histogram just
you know under the system is hardwired
this way but you know for illustration
Island let's assume that there are 10
fidelity levels what we basically keep
track is you know the which of those
levels was actually desired by much by
each of the users so the different
histograms that we have here represents
you know how many users want to the
image at the given fidelity level so for
instance this is an example and fidelity
level is on the x-axis where you know
most users like the image at a very high
level of fidelity so this is an image
they were really interested in
presumably this is an example where most
of the users really didn't care that
much about the image or where a low
fidelity version of the image actually
did the trick I mean was sufficient for
accomplishing the task so figuring out
you know how to adapt the image in these
two cases where the history is very
imagine us is very simple you basically
just you know figure out what's the mode
or the average and you just apply that
well as it turns out life is not as
simple as that and there are many
situations where the history is dirty
and the reason for this is roughly
because they're many many web pages that
are actually used for multiple purposes
and where the purpose may actually
affect the way the content should be
adapted and this is an example of that
we have let's say a group of users that
are more than happy with the image at
low fidelity but there is also another
group of users that really want the
image of the much higher fidelity so
what do you do if you serve the image at
high fidelity you know you're going to
be wasting bandwidth for this you you
know for the users that want a locality
one if you serve the image at low
fidelity you're going to force the those
users that want high fidelity to
interact with the system so they're
going to have to basically put more
effort into it and the total time that
it takes them to accomplish the task
will actually go up so the question is
well I mean what should we do and what
we're trying to do here is in some sense
we are trading off user involvement with
bandwidth when we are transferring I
guess when we are doing fidelity
adaptation so optimizing each one of
this too is trivial so let's say that I
want to maximize you know user
involvement or actually it should say
minimize user involvement I'm sorry 17 I
want to limit you know the number of
times that the user actually has to tap
well that's very easy i just you know
provide them you know has level of
fidelity and we're done the only problem
is that that's actually costly it's
going to chew up a lot of bandwidth and
in all likelihood if
if you happen to be using limited
network it's going to result in very
long slow download on the other hand if
what I want to do is save the user money
it's also very easy to do I just give
you you know the lowest fidelity but the
problem is that then the users are going
to have to explicitly ask for
refinements and in total if we look at
well you know what what we really want
to do what we really want to do is we
want to optimize what we call
fulfillment time which is a metric of
how long it takes you to get your job
done you know get the page in a state
where it's good enough for the task that
you are performing and this is really a
trade-off between you know it's a
balance between how much content I
download and how many times you have to
interact with the content to improve it
so ideally what you want to do is you
know download as little content as
possible without forcing any
interactions that of course is is
unfortunately possible because we don't
know a priori exactly what the user will
one so instead what we have is we
developed what we call a personalized
adaptation schedule which is a policy
that basically looks at the history of
the adaptation the amount of bandwidth
that is currently available and how long
it takes users to interact and based on
that builds a strategy an optimal
strategy for downloading the content so
it's basically saying well to bring an
object to the fidelity the user will
eventually want they are actually two
elements that that I have to take into
account the fact that the more content
our download you know that chews up
basically on the time and every time the
user interacts that chews up a couple of
seconds as well so the total time will
be download time plus interaction time
and what we want to do is you want to
optimize that so what we do is we have a
problem that basically does dynamic
programming
and looks at the history and figures out
you know what is the best strategy
roughly you know what's the first
reality that if I give you that feel it
it's more likely that you know that's
the one you're going to be happy in
otherwise if you interact once it's
going to jump to the next more likely
fidelity and over a large number of
users is it's it's it's optimal so how
do we evaluate this policy well we ask
we brought participants in into our lab
and we asked them to use a PDA over a
cdma 1x network to download content and
to perform tasks now the PDA was
communicating with our allocation proxy
which was downgrading the validity of
images we turned prediction off because
what we wanted to do was get a sense of
what were the Fidelity's that people
really wanted okay so it's sort of in
learning mode and we basically ask users
to browse over a couple of sites i'm
going to present results for two and we
gave them specific tasks to perform over
those sites and what they had to do is
they had to tap on the images too low to
raise their fidelity until they were
good enough to perform certain tasks
what we then do is we get all these
traces and we feed them to a simulator
and we look at how well different
allocation policies managed to match the
ultimate fidelity is requested by by the
users so obviously the best you can do
is an Oracle that basically guesses what
the user will want and that will give
you the best you know time fulfillment
I'm that you can expect so let me show
you some results of this experiment
there are two graphs there for two
different sites one where basically all
the users are performing the same tasks
so the history is imaginas and another
one where actually users are performing
different tasks so we have I believe
three different tasks so the history is
dirty so let's see what we have on the
x-axis we have three different policies
on the y-axis we
have time in seconds so the first bar
shows how long it takes you to finish
the task if there is no adaptations no
no transcoding whatsoever and we see
that that takes about 60 seconds we then
see that if we do I'm sorry Oracle which
basically guesses we know exactly what
the users want if we just serve them you
know that fidelity this is you know how
long it takes to do the task so there's
basically significant savings you know
you go from 60 seconds to 20 seconds
that obviously is you know the ideal we
obviously won't get as well as that we
then see that the simple policy that
looks at the history and calculates a
mean and serves that fill it a level
gets in this particular case actually
fairly close to to the optimal and our
policy that looks at both interaction
time as well as you know history and
bandwidth you know achieve similar
performance so this means that when the
users are performing only one task you
know just a very simple policy that does
mean on history does fairly well on the
other hand when the history is dirty
which is what we are seeing here that's
actually not the case if again so this
is you know unadapted and this is mean
if you do the wrong adaptation if you're
just considering you know history but
not the cost of interaction which is
what min does as it turns out you can
end up in a situation where in fact the
total time to complete the task is
actually higher why because you're
downloading yes I mean you save a bit on
the downloading but then the users
actually have to interact so any savings
on the time that you know basically you
got from reducing the amount of content
that you got you lose because the users
actually have to tap and have to
interact have to ask for for
improvements on the other hand we see
that task can actually adapt to the
situation and performs much much better
and really approaches the performance of
the Oracle policy i mean we're within
just a few percentages
of the Oracle policy and the reason is
because well this policy gets an optimal
schedule an optimal strategy for roughly
you know set of Fidelity's that is going
to jump through to get the based on on
the history and based on how expensive
it is to interact another example of how
this strategy works and how it actually
adapts to variations in the
instantaneous bandwidth is what we have
here so we have the same this is
experiment i just presented you know in
the last slides again multiple tasks
this is when you're downloading / 1x
bandwidth which is about 90 kilo bits
per second this is know what we saw at
the time we didn't have 3g in canada so
what we did here is we sort of simulated
the equivalent of 3g and wheelies dsl 3
400 kilo bits per second which is
actually what you get with you know a
video or I thing edges in fact actually
a little bit lower than that and what we
see is that obviously the times go down
you know the unadapted time how goes it
goes down because you have a faster
network what's interesting is that if
you have your policy that just looks at
history and it's of Livius of the
immediate bandwidth that these are is
you know getting in fact situation got
in a bill becomes much worse because you
know the user is still required to
interact quite a bit so you actually end
up hurting the user more than helping
him on the other hand because has takes
into account both you know how long it's
going to take me to download stuff as
much as you know how long it takes the
user to actually correct the wrong
decision it becomes much more aggressive
and in effect still does adaptation but
not as much because it knows that if it
gets it wrong there is going to be a
higher penalty percentage wise and
because of that it's able to better
adapt itself to the situation where we
have a better network it still does
better than no allocation and it
definitely does much much better
than an approach that is oblivious to to
the bandwidth that the user sees it is
actually quite important because even
with 3g today what we see with wireless
networks is that the bandwidth changes
quite a bit depending on where you are
and it can change monitor you know very
momentarily especially if you like let's
say you're you know riding on a car or
you know moving so you may think that
you know 3g basically fixes you know
with 3g we won't need adaptation yeah
that's true as long as you actually
happen to have a good connection but in
reality over the course of the day your
connectivity changes and as such you
know it would be nice if the underlying
system you know realize this and adapted
itself all right
so I'm just going to quickly skip over
the next few slides and hopefully we'll
get a few questions so this is all nice
and good but we figured that we could
actually improve it even better we could
you know do 11 more improvement by
leveraging the fact that many websites
actually involve tad many tasks on the
website involved many different objects
so the question is well can we actually
find correlations in adaptation
requirements of different objects so
that you don't have to improve each
individual object in other words if I
figure out that you know the two images
both either have to show in low fidelity
or high fidelity if we improve one of
them can I basically you know take that
into account and automatically you
improve the reality of the other image
and when does this happen this is you
know one example of the apple store and
this is an actual string shot of the
apple store and let's say that this is a
user that is interested in buying an
ipod now that's the case they care about
the top images but they don't care about
the bottom images okay so if the system
basically presented them you know
everything you know at low fidelity and
if I somehow figure out that you know
these are let's say taps on the iphone
they eventually will tap on every single
image because they want to see all of
them at high fidelity maybe I can
actually leverage that and the moment
they tap on the iPhone I basic improve
the affinity of all damages but they
don't do so for the images that they
don't care about so this is effectively
an on a classification problem in some
sense we know that the content supports
different usages and when the user shows
up we don't necessarily know what is
they are going to do with the content so
what we are going to do is that we're
going to probe the user we're going to
sometimes force them to give us some
feedback and we're going to use that
feedback to classify them to throw to
figure out you know what is that the
trying to do and based on that you know
this
okay if we figure out that you know this
content is in access for three different
tasks and each of the task you know
requires images at certain fidelity's we
figure out which one of those tasks they
want and based on that you know we give
them what they want so let me show you
how this works more than the performance
results so basically we have an example
here where we have let's say a web page
that has four images and we somehow
figure out that there are actually three
tasks that are supported by this website
and each of these tables represents a
task and what we have here is for each
of the images we see what is average
reality that you know people doing task
want one people doing task 21 people
doing test 31 and we also know the
minimum and the maximum fidelity that
people doing those tasks want and this
is based on basically a k-means
algorithm it's just clustering so what
we want when the user share a new user
shows up we don't really know if you
think of each of this tables is
basically a sub community we don't know
you know what grouping the user belongs
so what we want to do is we want to
elicit some feedback from them to drop
as many groupings as fast as we can so
we can reliably say well you know this
is the group that they belong to so that
we can better adapt the content so in
this case what we do first and assuming
you know this is magically let's say
that this is what the user will want so
that's the targets that we're here now
what the system how does it that well
first we don't know anything about the
user so what we do is we actually figure
out what's the of all the groups which
one is just slow the lowest maximum
fidelity that they want and that's what
we provide let's assume then that the
user actually sees this content and
basically tells us that the first image
is actually too low of fidelity and
raises the fair lady well now we have
one more bit of information
we know that maximum facility for for
image one is not good enough which
basically means that the user cannot
belong to this subgrouping so we can
just drop it and we recalculate our
predictions so again we take the maxim
you know the smallest maximum which in
this case basically means that image 2
is single O's 293 goes to 6 and 4 goes
to 1 well the user interacts again in
this case let's say with image 3 and
tell us well 6 is not good enough which
basically tells us that they don't
belong to this other community and at
that point we can automatically raise
the theology of you know we have
identified them we have classified them
and we can raise the fidelity and this
works very well what you are seeing here
are measurements of basically how many
groupings so we take the data and we
basically do clustering into how many
clusters this is how many interactions
are needed to complete the task and
that's how much bandwidth and the policy
I just showed you is this one the blue
one and what you can see is that we are
able to identify a nice sweet spot where
basically we have we can complete the
task with very few interactions and
consuming very little bandwidth
so in summary you know the second part
of the top but I presented is an
approach that learn how to adapt content
by monitoring the feedback it gets by
letting users fix you know wrong
adaptation predictions made by the
system I showed you how that can be
leveraged you know across users and how
in fact we can actually correlate the
location requirements of different
images so that we can automatically
classify users and again reduce the
amount of input that the user has to put
in in order to adapt all images or all
content in a website that URL over there
we don't have a fully working eureka
prototype but that URL over there has
pointers to all the papers with which
published in that on this project in
much more details so I'll spend maybe
one minute just talking about other work
that my group is doing we've done a lot
of work on accurate techniques for
indoor localization so GPS is wonderful
and you know I used it to get here and
sure you all use it pretty much every
day but you know amount you step indoors
it stops working actually Monty don't
have line of sight 24 satellites it
stops working well and in reality people
actually have done studies and I figure
out that most people in fact spend over
ninety percent of their time indoors so
in the places where we spend most of our
time strip it doesn't work and will it
be great if we had a localization
technology that also worked indoors and
you know that's you know one interesting
form that we've been looking at and the
approach that we've taken is one where
what we do is we leverage the existing
cell towers that you know are deployed
for communication we leverage them for
localization we don't do triangulation
which is you know what the towers
already do
instead what we do is we do an approach
that's based on surveying so if I want
to localize people let's say in this
office what I do is I walk around and I
collect measurements of radio towers
that they can hear and each measurement
includes you know the identities of the
towers I can hear and the signal
strength or the phase delay depending on
whether you're using cdma or gsm and a
build a database with those measurements
and later on when they want to localize
someone well they take a measurement and
in my database I look for measurement
that the similar and then I do some
interpolation and this actually works
fairly well you can localize somebody
within you know three to five meters in
indoors and you can actually for a tall
building you can actually tell which
floor they are which is you know
critical for emergency response if you
think about it when you call nine on one
today from your cell phone if you happen
to be indoors you know bell and rogers
you know they can't really tell where
you are you know in many many cases
basically they know where you are within
them about kilo meter radios which if
you're having a heart attack that's
that's actually pretty bad on the other
hand if you know the new what floor in
the building you wear well at least they
can dispatch you know emergency
personnel the den can canvass you know
that part of the floor and look for you
we don't know the work where we leverage
similar techniques to allow devices to
pair securely so wireless communication
is great because there are no kami
basically you can just go anywhere and
use it i mean the other problem is the
problem that has is that you don't know
since you can't see wireless
communication there is no cable you can
have any certainty about who you're
communicating with so this project is
developing techniques that give you some
assurances that your devices are
actually talking to devices that are
closed as opposed to attackers that may
be using powerful directional antennas
and may be very far away and finally a
newest project and i'll be very happy to
talk to people about this tries to make
the life of social user
in mobile environments better by
automatically adapting the behavior of
web applications to the context of the
user and specifically to who's around me
as an example of that if you run into
somebody and you want to schedule a
meeting with them you know when you
bring your calendar up on your mobile
device it automatically figures out who
surround you and adapt your calendar to
show you no available entries of the
people around you and things like that
and again sorry for being that cryptic
but i'll be happy to you know give you
much more details about that project so
finally you know although i sort of
presented this work as if it was my own
my own doing in reality nothing could be
further from from the truth you know I'm
just advisor in reality all of the hard
work and sweat that went into making
this project a reality really goes to my
very talented students and other faculty
members with whom I worked and you know
those are some some of their names my
email is there as well as my website and
you know feel free to send email if you
have questions and I'll be more than
happy to take your questions and you
know thanks for for listening for this
round this presentation yes
of course first what Milton going to get
to mentions ornithologist well that's
it's a civil tween yes let me just
repeat the question so on the internal
localization what do you need to do to
to actually localize people on the
device itself well you need access to
we've actually played both with cdma and
gsm for our gsm prototypes you need
access to signal strength information
and this information is available to the
phone that you know the phone uses us to
do a handoff it's typically not provided
to the application so what you typically
have to do is you have to hack into the
phone and get access to it we've
actually done so with you know some
models of gsm phones but you know our
hope is that in the future phone
manufacturers will see the value of this
information and will actually make it
you know available I mean it's there
they just choose not to not to bring it
up for cdma turns out CDMA networks are
much more dynamic than gsm networks the
size of the cell changes over time and
that affects the fingerprinting so if
your fingerprinting is based on signal
strength it just doesn't work for cdma
on the other hand cdma towers are very
tightly synchronized which means that
you can actually get very reliable
timing measurements and you can use the
time of flight to do fingerprinting so
in that case what you need is what we
call a PN offset which is really a fancy
way of saying a measurement of you know
time flight which is also available to
the phone and again but it's buried deep
down in
in the US so whether you need to send
that information to centralized database
is a matter of implementation so if you
know and current phones it's it's easy
to get the phone now within an SD card
then you can you know load on that
things or a gigabytes so you could
presumably have a very large you know
database fingerprint database on the
phone itself and do the localization the
phone itself alternatively what you
could do is you could you know upload
the fingerprint op oh somebody has to do
the surveying for sure yeah I mean
that's the problem with this approach
that somebody has to serve an
environment and collect the fingerprints
people and also included have looked at
alternatives where we use let's say ray
tracing to try to you know given an
environment and roughly where the large
obstacles are and the radio sources we
try to pre-compute fingerprints and
we've done other approaches where what
we do is we assume that we have a small
number of devices where we know they're
local another position and we use the
fingerprints to estimate relative
distances and that lets you do
localization not with the curious ease
the fingerprinting but in Oh better than
what you do now without having to serve
an environment but yeah there is no
magic you know somebody
scentless information that they also
have their GPS information which is very
easy buzz maybe to get many people
absolutely yeah absolutely you can do
that and in fact actually there are
companies that are already doing that
for Wi-Fi data so if you have a
smartphone with gps and Wi-Fi you can
you know download this piece of software
and it what it will do is it you know
once in awhile it basically do wife has
discoveries and it tax them with GPS
data and it uploads it and yes they have
algorithms that basically reason because
you have noise there and obviously
people can this report so yeah yeah
absolutely the different ways in which
you can build this database
like I mean I understand that once I've
modified from page and someone else
looks at it and they see my
modifications they may get a benefit
that how how easy to the users actually
understand the UI in your prototype for
modifying the page in in the first
project sure we did some limited
usability studies so the modifications
were actually tried were performed by
non-technical users these are gues
University students they were not
initially in computer science actually
most of them were not in computer
science we basically gave them about the
five minute you know session where we
describe how the prototype works and
we're able to figure how to use it so
the prototype is very easy to use that
there is no calling involved it's
basically to adapt the page all that you
need to do is you know tap on a given
item that you want a menu shows up and
then you have to tell it whether you
want to resize move crop delete and
things like that so we have fairly
confident that normal users you know
non-technical users can actually use it
we haven't done long term user study
where we actually give devices to people
and we tell them you know go and play
with this for two months and let us know
you know what you think two months from
now that that part we haven't done so we
haven't done a longitudinal study in
that sense
but is there a mentality to it like
there's a mode where I ought to find the
content browsing yes oh absolutely i'm
sorry should have started by that there
is a top level menu item that you
basically our bottom that you click on
it and you go into basically editing
mode and you come in and out of it yes
yeah I otherwise would be very confusing
sorry should have sound better thanks
for that clarification Saturday three or
five transformations we we have four for
the first project for Paige seller so
delete resize both images and text and
move roughly those are the three things
and you know it's it's very simple but
surprisingly powerful well it's really
hide any other than maybe I this it is
hide really in the item is still there
because if you completely delete it and
there is a JavaScript which in many
modern pages I mean there's tons of
JavaScript it just breaks yeah they can
undo I mean there is also bottom there
that basically says you know undo all
modifications and it basically brings a
page to its original state
I would think it's a small number I mean
if I have to guess I would think it's a
small number which is really the
motivation for the second part of the
talk so i would think it's a small
number if you look at if yeah if you can
somehow patch the modifications i mean
because let's say which in our case
actually when we say resize you know
it's it's it's not free form resize it's
basically jumping between you know
specific sizes so yeah i mean i would
expect to see a small number of
modifications 10 is probably even an
upper limit my guess is you know it's
probably an 8020 rule right i mean most
of the users will probably be happy with
you know one specific and then you have
two or three more but this is pure
speculation I don't have the data to
back it
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>