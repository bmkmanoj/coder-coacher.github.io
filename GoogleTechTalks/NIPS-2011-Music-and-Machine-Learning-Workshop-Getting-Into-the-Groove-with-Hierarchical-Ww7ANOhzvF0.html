<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Music and Machine Learning Workshop: Getting Into the Groove with Hierarchical.. | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Music and Machine Learning Workshop: Getting Into the Groove with Hierarchical.. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Music and Machine Learning Workshop: Getting Into the Groove with Hierarchical..</b></h2><h5 class="post__date">2012-02-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ww7ANOhzvF0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright so then
about one is through we're communicating
to the idea music is next year's so are
the music of deal with recording
recording messages and I introduce an
algorithm I talk about a system called
search by groove that we've been
implemented and give me some results
from that alright so first of all what
is groove but we probably read through
all of those references i thought what i
do instead is give you a more pictorial
so this background rhythm sets up
actually on the twice-over
with Liam Gallagher
the festival right
today's gonna be the way
my back seat Cummings it something
studio by now you should've somehow
realized buddy gotta do I don't believe
it everybody feel the way I do about you
now
so what I wanted to do by introducing
that is the guitarist starts off German
speaking German pigeon gentle pink
ginger money comes in today is that the
beat Dave doesn't right then the drugs
come in individual so everybody is
different for their ripping off the same
route the same background rhythm it's
not the rhythm of the melody is not
written at the base but there's some
underlying constant background rhythm
that everything else hands up and our
hypothesis is this is a very common
phenomenon amusing and so we're trying
to design a system to identify that kind
of background rhythm and then to
exploited to do searching and matching
and so groove has multiple meanings it's
definitely caused by rhythmically kind
of stuff but it could be the voice as
well to Debbie Wesley sings it in a
rhythmic way so it's not necessarily
just about what instruments being played
is how the instrument is being used
listing is written it repulsive rhythmic
feel comes from mckinsey jeff build
talks about so I mean
there's a whole bunch of articles
together we'll talk about rooms they're
all talking about something slightly
different but it seems same seems to
point back to for some underlying
background in rhythm alright so let's go
to specific example this is a
spectrogram of a particular rhythm a
particular drum p that's used over and
over again it's called the amen break
and if you know it you'll recognize it
you don't know if you listen to it when
I play an example of it will start to
hear it I'll give you an example of it
so I gotta give so this is a kind of a
background rhythm
Bobby Brantley so this is a spectrogram
corvina pop bubble bubble bubble up
seizing snare drums with wife and stuff
hit bass drum is the strictest of our
proven system houses now if we wanted to
represent rhythm a high level kind of
way you might think of music notation
has been affected for this right here's
our rhythm now this is the last part but
except if we clap that out it doesn't
feel like what I just played what you
need instead is the boom yeah that's
right so our second hypothesis is that
not only other his background rhythm but
you really need to express the rhythm
across some funny change by temple
change so without that you would just
get a series of essentially a tour
sixteenth notes but it says change in
the tablets actually created the room so
what we go for biz with looking at
recordings we're trying to set out these
different camera elements of the rhythm
and look consistent happiness and these
are always counter a joint tambra rhythm
elephants and we're going to do this by
using a component analysis where we
break the spectrum up into different
frequency profiles that corresponds to
consistent frequency Ward freezing
permissions
and these have associated tight profiles
that are essentially on sets of
different machines so we're going to do
this using essentially matrix
factorization and as a Lardon it was in
history and actually your job history
went back to the 1960s and really cool
to see that someone pencil factory
Chanel factorization is way back I wish
and stuff using ICA where we basically
took the fact same factorization model
that's been used since where we take a
frequency profile it's operates in this
direction the time function operates in
that direction and the matrix outer
product of those two vectors forms and
matrix and you stack a bunch of these
together that different spectra brain
slices of an unwise spectrogram pounds
and get kind of the separated layers
under them there's a bunch of different
way some early stuff that's just based
on the SDGs of latest stuff is based on
NM economic non-negative matrix
factorization is a reverb the current
algorithm of choice for non-negative
matrix factorization is this thing
called probabilistic late component
analysis or essentially this is your
spectrogram it consists of mixed
components you can model this as the sum
of a bunch of slices so you think of
your spectrogram has been a bunch of
layers that are songs together what
we're going to try to do is access those
those layers unmix those layers
essentially and we're going to constrain
the model by saying it's composed of a
vertical function in our first
approximation and the horizontal
function so this would be a spectral
profile and that is a time profile and
this spectrogram extends over some
period of time for example
a whole song this could be three or four
minutes long you actually did
transactive this this works for housing
section is this paper okay and here's
another thing that we do is that the if
this is a this is a profound constant Q
transform of the a membrane and we seen
this earth there's a lot of narrow kind
of found information in there as well as
the wife and information what we do is
we essentially take a first few kept
strong coefficients of his mode
frequency transform through time and we
lifter it so it sort of removing the
high capital coefficient so left with a
load capsule coefficients and then we
map back to the frequency domain from
the capsule of the day to get a smooth
version of the spectrum is really just
expressing terminal information it's not
expressing pitch information we don't
want to really separate out layers of
pictures with my trip way out things
that like forward spectral profiles
tambra in a voice wipe out of the
excitation for percussion for is them
the reason why we do this in own
operating the kept stored in May is that
our model assumes additive components
and because we're in the log domain let
me go to spectrum our components are
additive in the way that we need them to
do to work
so we map back to the invert the capital
transform once we've lifted removed the
high capital coefficients and lastly
representation working with and so what
we do those we work with a whole track
and we use the entire temporal context
we're actually interested in the tiny
evolution of these low cumin seeds
spectra and this is the 12 up to filter
back of the front and back in back to
seize constant Q for a transfer okay so
this is our method this is what we're
going to do to try and access some kind
of background rhythmic information and
then deduce this invention so we're
going to expect this chamber time
distribution which is the low Q can see
konstantin very transformative
subscribers and then for each track in a
collection for working with collections
of many thousands of tracks or hundred
thousands of tracks they're going to
extract this probabilistic latent
component analysis but we're going to
extend it by applying a hierarchical
version of the way we're going to extend
is we're going to estimate a universal
Tamra model they're going to take the W
functions which are these which are the
extracted Layton's the frequency
profilers going to take just the W
functions throw away time for now and
we're going to stack them for all of the
tracks in our database so if I extract
10 or 20 different frequency profiles /
track I'm going to concatenate them
across a whole database and I'll end up
with several millions concatenated then
what we're going to do is we're going to
estimate another plza on just the
stacked w functions and we're going to
get a global model of these temporal
functions and now what we can do is we
can go back and we can re estimate the
total functions for each trap using the
global basis of universal background
model and what this solves for us is you
have these time functions / track what
it solves is the correspondence problem
if I have a time function from one
tracking a time from
from another track how do I know that
they belong to something that sounds the
same it's a saying tambra collagen but I
know because their maps for this
universe a temple model and then our
rhythm features are essentially needs
per track 20 amplitude functions that
belong to each of these frequency
functions and just to try and make that
a little bit more pictorial I didn't
have infinite I think almost pictures of
us like this why I do you'll be doing
nice pictures and it's a step up from it
but basically it comes down to this that
our factorization is a there's a prior
probability for each slice as
essentially the amount of energy that
that slice accounts or in the entire
spectrum there's a frequency profile to
this diet and a time profile is let go
when you take the outer product of these
things by this with a scalar Friday you
reconstruct one sense as a successful do
it and we do this for each track and
estimate a global models and this is
what our profiles don't like this what
frequency component looks like this is
our amplitude profile of psychic and and
kind of guess by looking at which
frequencies are a play which what type
cousin instrument that might be SD mmm
break their the universal term remodel
then basically our notation is we just
stack the frequency functions given each
latent index so that there are ki latent
slices for each track I and we're just
stacking them all together on this horse
Universal basis only the functions
universal races tell functions givers
essentially these Universal few places
did so as a second-level plc a SS
hierarchical then what we do is we need
to map these back to each trap so we
have these Universal basis functions
what we actually do is we adapt the
universal basis function slightly apart
of each track so that if the universal
basis functions differ a little bit
let's say this one across monstrous
snare drum a snare drum and a particular
sample trap is slightly different
reversal basis function then we adapt it
so slightly so that it looks more like
individual tracks basis which is still
referencing the universal function is
locally adapted that gives us a better
estimate of the amplitude time
distribution which is this guy for a
finger trap and it's not a house on a
position estimate the amplitude time
resolution for that tracks a latent
component channelreserve local
temperature alright so we've built a
search by proof system and essentially
how this works is that we're looking for
similarity of using content by this
background rule we do this layton
feature extraction of these universal
term of functions and these time
functions we do some tricks there's a
there's a sparsity constraints fastness
constraints response if you can strain
on the number of components racetrack
using an entropic prior which basically
means that we just set some global
parameter which is responsive since very
hand and we rediscover an optimal
optimal number of components in each
track so we don't have to set how many
components were looking for it adapts to
each throw this world later a second
thing that we do is we look at the
sparseness of the time functions that we
get back and we only keep those that are
above us as certain sparsity threshold
meaning that we're not interested in
time functions that are fascist state
knows we want time functions that are
for rhythmic information so we kind of
do a we reject things before service
varsity threshold so some basic net
tricks that go on so we worked with this
database and easiness the issue guide to
electronic music I can just give you if
you haven't seen this on the internet
please check it out it's a completely
insane website it's fantastic but
basically if we go here we just listen
to a little bit of the doghouse and just
play you some one thing about one or two
of these things employer
so since I a history of electronic music
in a couple of thousand tracks or
something I'll give you an com
contrasting and this is apparently from
the same sub-genre an electronic music
and don't see there are 170 different
genres initiative guide and they're all
really this out exactly what we were
sort of what what we do what we do
instead of using 170 styles and pavement
these 4 mar 3 8 tracks we actually gave
these tracks to some professional DJ now
in new york and said please label these
tracks into whatever categories one by
their rhythmic feel and you know try not
to give too many categories between
three and five will be good and having a
dumb no category so we made this
incredibly hard test for ourselves which
is you know how if we label up a data
set like that can we present a system
with one of the tracks and do we recover
the other tracks that they're labeled in
the same by the same category player
professional deep DJ and having set the
bar pretty high is the results that we
get being so for retrieval for a recall
rate of five percent and ten percent and
twenty percent we're comparing ourselves
to doing a sub bound only decomposition
so we take an opportunistic band
decomposition of the spectrum of just
used the amplitude outputs as as the
temporal measure we see that we get a
divergence of five percent consistently
for rear evil right by using hplc a
versus sub back and this is an early
resolve this is working in progress and
we have to do a lot better but we sort
of rethinking about how your evaluation
but is early evidence that we're moving
in the right direction by diver
it sometime so our summary is that
because we've got embedded components in
a mixture herbal so we want to match
using these embedded components in their
temporal context audio is mixtures so
use latent components or something else
I did some mixtures I mean Universal
tambra model so that we can compare time
functions from different tracks these
are there's a Tamra model that gives us
the tambra time groove based
representation and we have a simple
metric space in time dot product thing
so thanks to these</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>