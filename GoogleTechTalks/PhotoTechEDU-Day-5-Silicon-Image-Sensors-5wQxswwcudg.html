<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoTechEDU Day 5:  Silicon Image Sensors | Coder Coacher - Coaching Coders</title><meta content="PhotoTechEDU Day 5:  Silicon Image Sensors - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PhotoTechEDU Day 5:  Silicon Image Sensors</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5wQxswwcudg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay thanks everybody for coming to
lecture five of the photographic
technology series I'll be given the
lecture today I'll start off by thanking
Ian for doing it last week when I was
out of town I'm not going to talk about
these books and references but they're
in the notes in case you want to go back
and look I've got a few examples from
most of these books in the talk and
searching around for good reference
material I found this place at York
University that has a whole lot of
interesting papers online so if you need
to know more about sensors it's that's a
good resource so it's all in the notes
so let's start talking about about
silicon today's topic is really all
about the interaction of light with
silicon and how we make images out of it
silicon is this it's this magical
substance silicon crystals have this
this beautiful tetrahedral symmetry they
have a valence of four which puts them
in the periodic table right below carbon
which is the the stuff of life and
silicon behaves quite differently from
carbon when you when you get this kind
of crystalline symmetry with carbon you
have a diamond you can make
semiconductors out of that but it's a
little harder to work and more expensive
then silicon carbon also makes double
bonds and chains that that make life
type molecules and silicon doesn't do
that it's too big to make double bond so
it's a very different animal but it
makes these nice crystals and one of my
historical side lines that I want to
introduce today is this guy Gilbert
Newton Lewis who in 1902 in this memo he
was working on the the structure of
atoms and the arrangement of electrons
and atoms before anyone understood about
quantum mechanics or Bohr's atomic model
of orbits and all that stuff and there
was this notion from the periodic table
that maybe atoms were arranged in groups
of eight or shells or something and he
drew these pictures in which he showed
silicon with alternate vertices of a
cube occupied by electrons and that
gives you that tetrahedral symmetry and
that's in fact the way they arrange
themselves in in crystals so some I'm
not sure how much was known about the
crystal structure back in 1902 but he
was I think he was he was pretty
prescient he was a he was at UC Berkeley
he did a lot of the early
work on physical chemistry and I think I
don't know if he invented the word
valence or not but he wrote the book
called valence in 1923 a few years later
he coined the word photon with this this
quote here in a paper which is his
thesis was that photons are conserved it
could be neither created nor destroyed
which turns out not to be the right
concept but in terms of quanta of like
he's the guy that named it Photon so
he's my random sideline of the day did I
skip one no so how do photo diodes work
when when light enters the silicon
crystal it it creates what's called a
photo current if you know anything about
diodes you know that with you if you put
a positive voltage on the anode you get
a current and what I've drawn here has a
positive voltage on the cathode relative
to the anode so the diode is back biased
there's no current it can't you can't
get any current that way unless you put
light into the diode and then the light
kind of disrupts the normal current
equilibrium and causes some current to
flow and it's a reverse current as well
if you don't bias the thing and you just
let it sit there with its terminals open
that current will will cause the the
anode to become positive until the thing
is slightly forward biased and then you
can actually get energy out of it that's
the way photovoltaics work but we run
these things reverse biased
unlike photovoltaic cells we don't get
energy out of them we just collect
signals out of them so we're going to
talk about how you make sensor chips out
of these photo diodes physically what
goes on in there is a there's a junction
between a p-doped region of
semiconductor and an endo Prejean of
semiconductor I got this this figure to
represent the book analog VLSI circuits
and principles in which Toby dobrik and
and your Cramer wrote a chapter on on
photo sensors when a when a photon
enters the crystal it it can be absorbed
and disrupt the perfect crystalline
lattice in which all electrons have a
place in the valence shell that's shared
between different silicon atoms so
normally there's you know every atom has
its place but when light goes in there
and disrupts that and atom get
every electron has its place and light
disrupts that an electron gets kicked
out and it becomes free to wander around
and it leaves behind the hole that it
got kicked out of and that hole can also
wander around by taking in electrons
from neighboring atoms so you get these
two mobile charge carriers once
positively charged and ones negatively
charged when you set up the reverse bias
condition in the photodiode there's an
electric field in there that tends to
separate these positive and negative
charges so they can't get back together
and annihilate each other and that's
where the photo current comes from so
you can you can collect them and count
them if the ideal detector would would
collect and count these electron hole
pairs and tell you how many there were
and where they came from because each
one of those pairs corresponds to a
photon having been absorbed you can't
really build that ideal sensor because
you can't tell exactly where it happened
you can only collect in discrete regions
and you don't need to collect both the
electrons and the holes you collect one
or the other that'll tell you how many
so usually we build structures that
collect the electrons typical
cross-section of a silicon a silicon
chip that does this it's got a pretty
simple photodiode it's just a region
that's doped n-type through the surface
here in a p-type substrate so this
bottom of this area is the PN Junction
that is the photosensitive area and
light enters the silicon gets absorbed
pretty near the surface usually within
the top couple of microns and then
there's other circuitry on there that's
usually under a metal light shield so
that it doesn't get light because you
don't you don't want light doing things
other than what you want so they shield
some areas and put these PN Junction in
other areas so you can look at how much
how many electrons do you get and the
answer is you get an electron per photon
so you need to know how many photons you
get and it's pretty much the case that
any photon that enters the silicon will
generate an electron for you and you'll
collect most of those unless they're
even generated so deep that you're that
you're doping profile loses them to the
substrate or something so once the
photon gets into the silicon you get you
have a very
conversion efficiency you get almost one
electron per photon so you need to be
able to to estimate how much charge
you're going to get you need to be able
to know how many photons are getting
into the silicon and there's some
efficiency problems and getting them in
there but once they're in there you get
almost one-to-one to compute the number
of photons you can do that from the
energy if you know how many watts per
square centimeter and you know the
wavelength and you know what period of
time you're exposing for and so on it's
it's pretty straightforward to compute
the energy at each wavelength and from
that energy you can compute a number of
photons by knowing what the energy of
each photon is and that energy comes
from the rule that plunk developed and
1902 or so I forget when it was maybe
was 1899 energy equals H nu Planck's
constant times the frequency of that
light which is the same as H times the
speed of light divided by the wavelength
of that photon and you can get Planck's
constant out of any books and you can
get it either in units of Joule seconds
or electron volts seconds depending on
whether you want to work with standard
SI units of energy or or the electron
volt unit of energy which is a kind of
convenient one when you're working with
electrons because the the electron volt
is the amount of energy that it takes to
move one electron across a potential
gradient of one volt with this formula
you can compute other things about the
interaction of light with silicon as
well for example if you know that
silicon has a bandgap energy of one
point one two electron volts which you
can find in any book about silicon you
can you can say how what what wavelength
of light has enough energy to cause an
electron to to pick up that energy and
cross the band gap between the
conduction band and the valence band on
the silicon this is what it this is what
it means to disrupt the crystalline
structure and kick that electron out of
this stable orbit it has to have at
least enough energy to cross the bandgap
so you can work out the wavelength from
Planck's constant times speed of light
divided by that energy plugging in the
Planck's constant in the form that has
the electron volt seconds and you just
do the math and you get eleven hundred
and ten nanometers that's
a longer wavelength than what's visible
and in fact when the light gets to be
that long or longer the silicon is
completely transparent to it because the
the quantum mechanics of the silicon
crystal doesn't allow that photon to be
absorbed there's no place for that
energy to go because there's no
intermediate states between the
conduction band and the valence band so
that those long silicon wavelengths go
right through you can't make a good long
wave infrared detector with silicon you
can make you can make a detector for
near-infrared things between 700 and
1,000 nanometers say although it gets
absorbed quite deeply as we'll see this
is a plot that may be a little bit too
dark to see very well but it shows
different colors of light entering at
the top surface of the silicon and it's
each one is drawn with an exponential
decay with the right space constant to
simulate the the rate of absorption and
silicon and you can see the blue light
is mostly absorbed very near the surface
green light a little deeper and red
light deeper yet and there's sort of a
an exponential curve of depth versus
wavelength here and that as you go off a
little further here it just goes to
infinite because the there's no
absorption at all of the long infrared
most sensors don't don't use this
wavelength dependent absorption but they
make detectors that are close to the
surface usually around a micron or so
and stuff that gets absorbed much deeper
than that gets lost so there's some
fall-off of efficiency and the longer
wet red wavelengths which doesn't hurt
you that much because it you want us you
want to roll off toward the long
wavelengths anyway to get a good good
color accuracy if you measure all three
of these separately you can do even
better of course yeah question
yeah the question is why do you need an
infrared filter if the silicon doesn't
tend to absorb and respond to infrared
you need it for that region between 700
and a thousand nanometers because the
stuff that's it's just longer than red
here the silicon is still quite
sensitive to you can see some of it will
be absorbed high enough up to affect you
and the the cutoff of human vision is
fairly sharp there and there's a lot of
things that you that you look at that
look black or dark like dark cloth dyes
and so on that reflect very little
visible light but they reflect a lot of
light just longer than what you can see
so those things would look start to look
red or maroon colored if you didn't
filter that out yeah there's no gaps I'm
sorry these are just lines I drew out
here but the band gap so the band gap is
not a place it's a that's I didn't
really talk about that diagram but this
is an energy space where you go from the
conduction band to the valence band at
different energy levels and biasing the
two sides of the photodiode puts them at
different levels which is why it's bent
through the junction here when the when
the photon gets absorbed and the
electron jumps it actually has to take
on a different momentum as well and it
tends to go scooting off someplace but
there's no there's no place associated
with this jump this happens anywhere in
the Pierces pure acrylic and silicon
crystal
so it can be it can be at the surface
where your dope 10-plus it could be in
the middle of the junction where it's
essentially not doped it can be in the
substrate where it's p-type the the
crystal it doesn't know these dopants in
here really don't affect the absorption
much at all so if it gets absorbed
pretty deep there's still a good chance
that that electron will wander up across
the junction and be collected there's
some chance it'll wander down here and
not be collected so so if you shine a
light on silicon a perfectly steady
constant electric field if you will the
interaction events the photons that's
the the sort of random quantum
mechanical effect of what happens from
what we think of as steady illumination
gives you these independent absorption
events and if you if you do a little
simulation I just ran a simulation here
where I ran a thousand trials where I
had a mean of ten independent events per
trial and I did a histogram of how many
of those events I got in each trial so
think of these as photons per pixel say
where the the intensity is such that the
mean number of photons per pixel is ten
but on each trial you get a different
number so you get this for this random
distribution because each of those
events is independent and even though
there's a mean of ten you might only get
five sometimes you get fifteen or twenty
sometimes and so you get these
distributions that are Poisson
distributed which means that they have
this they can't go below zero on this
end they have a longest tail on this
side and they have a variance equal to
their mean that's kind of a nice magical
property of a Poisson distribution so
the variance is ten in this case if the
mean is ten and that means that the
standard deviation the square root of
the variance is about three point one
six so you can very quickly estimate the
noise and this is called shot noise it's
the it's the noise of the the quantum
mechanical randomness involved in
responding to a perfectly constant light
level so this is the unavoidable ideal
behavior of a sensor if you can do
anywhere near this good you're doing
good
so how do you take this photodiode and
make a sensor out of it and there's
there's two ways I'll say a few words
about each one is the charge-coupled
device and the other is the CMOS circuit
this is a example of what's done in CMOS
and in fact the the C + CMOS is
completely irrelevant here because it
means complementary symmetry or
complementary transistor types and that
means n-type transistors and p-type
transistors but the circuit only has M
type transistors and it has three of
them and it has this back biased
photodiode the the C the complementary
part is typically used around the edges
of the sensor to build better amplifiers
and things but these kinds of circuits
were actually first made written about
tested in 1968 and they were not CMOS
they were just n Maus CMOS I think
either wasn't invented or wasn't
certainly wasn't common yet so the way
this works is you us-cert true on the
reset line which turns on the reset
transistor and back bias is this
photodiode by pulling it up to the reset
voltage then you let go turn that off
and you expose it to light and as you
expose it to light you get this photo
current which is putting putting
electrons collecting electrons on this
node thereby lowering its voltage so you
pull that thing high and during light
exposure it comes down at a rate
proportional to the light intensity but
randomly in steps of one electron at a
time of course when you're done exposing
it and you're ready to read it out you
assert true on the row select line which
turns on this transistor it connects
this amplifier as a source follower
transistor to follow the diodes voltage
out onto the column line so that's a
that's a gain of something less than one
from that voltage to the column voltage
and then you have circuits at the bottom
of the column line that buffer and
amplify that again and serialize it and
get it off chip so what's going on in
the pixel is very simple thing you reset
the thing you wait then you read it out
you can find a lot of pictures like this
of array architectures your question
over here yes so the question is to get
to get uniform voltage per electron or
electron to voltage conversion gain do
you need uniform capacitance and yes you
do the the capacitance at this node is
the primary thing that affects the
voltage per electron gain and then the
gains of these transistors and the gains
of subsequent amplifiers also affect it
it turns out that area capacitances and
these Maas processes are probably the
the best controlled man-made thing in
the world so they're they're pretty
constant they're not completely constant
you can get typically in these things
probably less than one percent RMS
fluctuation which is low enough that you
usually don't see it that's one thing
that people often think about correcting
for you can make a map of the gains of
each pixel and correct for it and in my
experience that's never necessary and
almost never done and in fact it just
doesn't matter I took this array
architecture drawing from a phobia on
part that I'm familiar with it's got the
three layer photo diodes and a three of
circuits something like what I showed
you although a little different to read
out each each of the three I think if
they give it a stacked up pixel sensor
circuits it's got on the two sides what
are called row readout control and row
reset control and go back to the pixel
circuit diagram and show you what those
do so I've drawn the row line and the
column line as oriented and rows and
columns I didn't really say what the
reset line does but typically that's
also oriented as a row so that you can
you can independently reset each row of
the sensor so you can have a scanning
process that's controlled here in the
reset control that's running down
resetting the rows and another process
over here that's running down and
reading them out and you can control the
phasing between those and it works a lot
like the opening and closing shutters in
a focal-plane shutter you can control
the exposure time by the time difference
between the row reset and the road
process that's called an electronic
rolling shutter readout and that's
something that's quite common in CMOS
type sensors and it's something that's
it's much harder to do or essentially
impossible to do in a CCD although they
have other tricks for electronic
shuttering most cameras don't don't work
that way they use I think I said this
before probably in the first lecture
they use mechanical shutters because
this electronic rolling shutter is a
little bit slow and will give you
motion artifacts but in the in the low
end for example cell phone cameras they
do pretty much all work this way at the
bottom the column lines come in to some
kind of a column readout control and
multiplexing and amplification and A to
D conversion structure and there's a lot
of there's a lot of variety in here
about exactly how people do things
that's the part I won't be saying very
much about because it is it's it's it's
just our cane art you probably don't
need to know about it although from time
to time it it'll come up and bite you
one way or another like if you let light
shine on these circuits if they're not
well shielded that'll do something
interesting and then there's usually
some kind of a digital control block
that lets you program internal registers
and control the mode the exposure time
the readout rates the clock phasing all
kinds of stuff like that okay so Ian
introduced some of the noise sources
last week and I'm going to tell you just
a little bit more about that but it's
only the first line here the photon shot
noise is this I want to remind you again
this is a this is an ideal noise this is
something that comes from physics and
quantum mechanics and you can't do
better than that so that the ideal in a
sensor is to make all the other noises
small compared to the one that's really
fundamental that you can't reduce the
second one the reset noise at the photo
diode capacitance which has a charge
variance called
charge variance equal to KT C where K is
Boltzmann's constant T is absolute
temperature and C is the capacitance
it's often called a KT see noise because
of that formula and that's that's a
noise you get here that's when you when
you turn this switch on to pull it up to
a voltage and then you turn it off your
your
left with some random number of
electrons on the capacitance of that
node that's that's called a reset noise
it doesn't depend on anything non-ideal
about this switch a perfectly ideal
switch here will do the same thing you
can model this using thermal noise
formulas for resistors by putting a
resistor there and letting it's in value
go to infinity and so on but it's easier
to think of it as a property in the
capacitance itself when the switch is on
and you're holding the capacity that
capacitor or a certain electric
potential electrons are randomly coming
and going all the time and when you turn
the switch off you've got some random
number of electrons in there and so
you've got some random amount of charge
and some random voltage that you're left
with that's not quite equal to the
voltage that you held on it before you
open the switch so that that has a
charge variance of ktc a voltage
variance of kt divided by c and that'll
from that you can compute the standard
deviation in either charge or or voltage
you can also look at the variance as the
number of electrons there if you use the
thermal voltage thermal voltage is KT
over Q Boltzmann's constant times
absolute temperature divided by the
charge of an electron is about 26
millivolts and the way to remember the
noise on a capacitor like this is it's
the it's the number of electrons that
corresponds to charging that capacitor
through 26 millivolts it's it's I'm
sorry it's not that much noise it's the
shot noise associated with that much
charge so if you take that number of
electrons that's the variance of the
number of electrons or the noise so
that's a way to remember it and compute
it from uneasier to remember number
because 26 millivolts to me has always
been easier to remember than Boltzmann's
constant or the charge of the electron
so there's all these other noises you've
got the photo response non-uniformity
that's when we just had a question about
basically different gain variations that
at high signal levels when the shot
noise relative shot noise is low the PRN
you can be the dominant noise but it's
still not usually enough to to bother
you you've got dark offset fixed
patterns and that's that's a big deal in
CMOS because these
these source follower transistors have
random threshold variations and they
drift so there they're kind of all over
the map and so you need a way to to
correct for those you don't have the
same problem in CC DS say a bit about
that in a minute
you've got all the readout amplifiers
they have thermal noise they have
flicker noise or one over F noise you've
got leakage in the photo diodes which is
called dark current and there's a shot
noise associated with the dark current
so there's a it was like a fixed pattern
of dark current that you might be able
to estimate and subtract but you still
have the randomness of it it's not quite
repeatable you have the quantization
noise from the analog to digital
converters which has a variance 1/12 of
a LSB squared that is the standard
deviation as somewhere between a third
and a fourth of the step size of the
hata D converter you have the
differential nonlinearities that is to
say the unequal step sizes of the a to D
converter adds some more noise and
you've got all kinds of pickup of stray
electromagnetic interference from all
these fast digital things that you tend
to put near your sensor so you have to
worry about all these things and the the
way you worry about them typically as
you convert them all back to input
referred electrons because electrons is
sort of the easiest unit to do things in
you don't have to worry when you talk
about electrons you don't have to tell
someone what gains you've already
applied at which point in the system
you've measured it so you refer it back
to electrons at the input at the
photodiode convert all your noises into
that space and then you can compare them
and when you compare them only the big
ones matter because you variances add
and and then you do a square root to get
back to a standard deviation assuming
they're independent noises so you you
can you can plot all that against the
signal level since a lot of these things
are level dependent in particular the
shot noise is signal dependent some of
the other ones are as well like the P R
and you the photo response
non-uniformity gives you a noise
proportional to the signal shot noise
proportional to the square root of the
signal it's on yeah question here how
accurate is it to say that they're
independent usually quite accurate there
if you have several noises that are that
are interdependent
they're probably from a common source
and you probably ought to model them as
one thing and then but you know to a
very good approximation you can just add
variances of things that are of
different types you can also convert
these signals using games like micro
volts per electron at the output of your
analog section or digital number LS B's
per electron at the output of your a to
D converter and compare things that way
it's good to have those numbers in mind
sometimes now this this reset noise is a
big deal and is the reason why many
generations of CMOS sensors were not
quite noise competitive with CCD sensors
because in a in a CCD there's an easy
way to cancel it at one place in a in a
CMOS sensor it's harder you have to
cancel it at the first amplifier that
converts charge to voltage and that's
inside the pixel and the way that's done
is they add a fourth transistor called
the transfer transistor and they operate
this pixel in a mode that that requires
a special kind of photodiode here called
a fully depleted photodiode which when
you when you pull it up through this
reset path it it doesn't just hold it at
a certain voltage it actually takes all
of the free electrons out so that when
you're done with the reset process there
are zero free electrons at that note so
that's called fully depleted which that
that layer-cake in the silicon that had
an n-type region where normally
electrons predominate all of the
electrons have been removed from that
region so it has to be a very lightly
doped it has to be pinned at the surface
with a small p-type layer to prevent
surface effects messing it up and so on
and this was invented by some
combination of guys that at Kodak photo
bit and Motorola depending on who you
ask but working together boy less so
this this is a typical for transistor
pixel you'll find that in a lot of
modern CMOS sensors in it the way you
read this out you after you've
accumulated you you reset this to where
there's no free electrons you accumulate
photo charge here then when it's time to
read it out you reset this node again
without resetting that one
that puts a that puts a random reset
noise here you read it out then you turn
on the transfer switch and in a way that
moves all of that collected charge onto
that node and then you read it out again
so that second read has the same reset
noise as the first read you don't reset
it again in between so you've read that
node twice with and without the signal
charge and when you take the difference
the reset noise cancels and you're left
with the signal so you need down at the
bottom here what they call correlated
double sampling amplifier that takes
that sample twice and takes the
difference and those those two readings
are correlated in terms of the noise and
so the double sampling of a correlated
signal allows you to cancel that noise
so that's that's very important in terms
of getting a low noise floor because the
reset noise is otherwise going to be
typically on the order of somewhere
between 20 and 100 electrons depending
on where you are and has Ian mentioned
last week some modern CMOS sensors get
down to three or four electrons this is
a plot that shows what happens at the
bottom if you have very low noise from
say all of the sources other than shot
noise if you plot as a function of the
signal level in electrons what the noise
is that's the red curves there's a
there's a diagonal here that's a it's a
square root
slope of 1/2 on the log log plot that
represents the shot noise and then if
you have one electron of readout noise
or three electrons or ten electrons it
adds variance to that and pushes those
curves up when your when your signal is
low and has negligible effect when your
signal is high the signal-to-noise ratio
which is the ratio between the signal
electrons and the noise electrons starts
at that same square root relationship
and comes down with noise it's the
square root because against that magical
property of a Poisson distribution that
the signal-to-noise ratio is equal to
the noise and electrons so the point
here is that even small differences in
the readout noise can matter when you're
in a very low SNR region or a low signal
region and it may determine the floor of
what you can see let me see if I get a
picture of that yeah can you see the eye
charts here
last week even pointed out that to read
small text like four or five pixels you
need a signal to noise ratio of about
four and I checked that carefully and I
have to say I do agree that's exactly
right
the numbers here are a little bit
different because I'm working at a much
smaller signal-to-noise ratio but much
larger letters so the the letters you
can resolve here are about twelve pixels
high and if you down sample those by
averaging to about four or five pixels
high
it improves the signal-to-noise ratio by
the square root of the number of pixels
you averaged together and you get up to
about the number inset so I think you
can see here that these letters are a
little bit easier to read than these
letters although you can still see them
in there and the difference here is
between two electrons of readout noise
and four electrons of readout noise so
for big enough stuff that you're trying
to look at the noise floor can be quite
low in this case I'm illustrating it
with a signal of four electrons think
the contrast between zero electrons of
signal in the dark letters and four
electrons in the white background this
is a this is a carefully simulated plus
own distribution plus a Gaussian
distribution of noise added to it so
this is where I said I was going to
quibble a little bit with um that he
thought that three electrons was low
enough that it wouldn't matter and I
think it does matter if you're trying to
push toward things that are not so tiny
it doesn't matter in the readability of
four pixel text but it matters in being
able to resolve some larger size things
in the noise floor of your of your
pictures let's look a little bit about
CC DS yeah
sure well the Poisson distribution is
what you get from shot noise the
distributions you get from the other
noise sources you don't know necessarily
what they're going to be but typically
Gaussian is a is a good model okay
here's a here's a charge coupled device
complicated pictures here but this is
just an example of what you find in a
datasheet when they're trying to explain
how it works and this one is a Kodak
inter line transfer CCD inter line
transfer is a method of doing electronic
shuttering in which you shield a region
between the lines of the sensor between
lines of photodiodes you have these
shielded regions where you can store the
charge and while you read it out so the
light can continue to shine on your
sensor while you're reading it without
corrupting your image too much sort of
works to do this they they have to
shield this charge read out region
charge coupled device means that you're
you're actually reading out the charges
the photo charges themselves they get
shifted out on they don't get converted
to a voltage like they do in the CMOS
pixel so you you collect them on a
photodiode at some point you transfer
them into this shielded charge-coupled
device which in cross-section is shown
with a light shield on it here and then
they come out one end of the chip on the
edge of the chip where you can serialize
them
that was that was Kodak here's a drawing
from micron showing similar things which
they're actually comparing CCD and CMOS
architectures as far as I know they only
make CMOS devices but they've drawn both
having pretty nicely here this shows a
photo diode and a masked shift register
you shift from the photodiode into the
mask thing and then you clock out into
these readout strip readout structures
there's another horizontal
charge-coupled device across here and
then up to an em up put em player and
you can do the correlated double
sampling in one place out here because
you get all the charges with their reset
noise and everything right out to the
edge then you can with a single
differencing circuit instead of one per
column like you need it in the in the
CMOS you can get a good signal out of
their in their CMOS chips they've drawn
at the bottom of the columns what they
call analog signal processors
and what's down here varies a lot as I
mentioned some some things are very
simple we made chips with just very
passive multiplexers here you can also
have active gain per column you can have
an analog to digital converter per
column which is what micron does and a
lot of their chips there's no there's no
real simple way to compare CMOS and CCD
and say which ones better they're just
they're just different and you have to
look at the the details of each chip in
each architecture in each application to
see which one suits you better yeah does
the CCD reset when you shift it out
usually you shift like zero charge
buckets in from this edge as you're
reading out that way so there's a reset
process but but resetting during readout
may not be what you want you really want
to reset just before you do your
exposure so you may have to shift it
some more do it again to clean it out
just before you want to use it and that
can introduce some delay into the into
the process when you push the button to
take a picture you may need to spend a
few milliseconds cleaning it out first
yeah question
okay right so you're commenting on this
small print that I wasn't really gonna
pay any attention to from micron which
say they say the big advantage of CMOS
is it's made in a standard CMOS process
you can integrate a lot of stuff on the
chip like A to D converters and so on
that is a big advantage and says C CDs
are optimized for lower noise and
improved image quality but they require
additional clock signals converters and
controls off the chip that's true too
but I think we've gotten to the point
where CMOS with with CMOS that you can
you can match C CDs image quality
there's still a lot of CMOS being made
it's not going to die anytime soon
it's like film there's still a lot of
film being made that's not going to die
anytime soon either but I think the
trend is toward replacement with CMOS
it's cheaper more flexible and just as
good there's a lot of CCD still being
made the trend is toward more CMOS yeah
yeah
okay so Ian points out that in the in
the Canon 5d and other other DSLRs that
use CMOS sensors they typically do not
have very much integrated on the chip
with the sensor it's a it's a great big
chunk of silicon with a simple analog
output it doesn't even have a 2 D
converters on it it doesn't have much
control on it and so on it's the same
way with some some other CMOS sensors
that I've worked on that when you're
doing the really big ones where yield is
a is a big factor you really don't want
to use your silicon area for the stuff
that you could do in a separate chip
you're you're pushed to make the biggest
cost-effective sensor you can so you
want to you want to use all that silicon
for sensing and if you can do your A to
D converters and other stuff off chip
you do that it's just different
economics and the high-end from what it
is in the low-end due primarily to yield
and also somewhat to the actual physical
size limitations of the semiconductor
fabrication equipment that makes it
harder to make the chip bigger sometimes
they some of the sizes that are used or
right up against the limits of what the
manufacturing equipment can do appear
okay the question is why in the the
really large medium format back so they
still using CCD instead of CMOS and I
believe the region the reason there is
stitching the to get a given pixel size
you don't need a smaller feature in CCD
so what what you might do here in a 0.25
micron process you might do here in a 1
micron process and with those with those
larger processes it's cheaper to get fab
and it's much easier to do stitching so
you can take a reticle and step it and
make pieces line up and make really big
sensors that way with the very fine line
stuff it's it's very hard to do
stitching and you can't it's hard to get
lithography that'll do point to 5 micron
over a full frame size area and bigger
so it's basically a feature size issue
of the fabrication technology Arbus yeah
how does the bit shifting work so what's
being shifted here's not bits it's
packets of charge and that's not
something I was going to go into but
it's it's basically this this thing that
says parallel clocks there are there are
gates that overlay the silicon that you
pulse them up and down and in a pattern
that pumps the charge along by
attracting it to the higher voltage
gates and repelling it from lower and
it's a it's like a systolic thing you'd
you see you pump the charge through but
manipulating voltages there's lots of
books on C CDs that'll explain that to
UT in in the codec sensors in
cross-section there's there's a little
lens lit called a micro lens and a color
filter over each photodiode and they
have these light shielded areas and the
the function of the lens lit is to get
most of the light to go into the
photodiode rather than bouncing it off
the light shield and wasting it that's a
another picture from their datasheet and
they also show quantum efficiency of
that thing with and without the the
micro lens and the the quantum
efficiency is electrons per photon so
how many how many electrons you get per
photon that was incident at the front of
your sensor and with the filters you
lose quite a bit with micro lenses some
reflect off and at the interface to the
silicon
more reflects off so there's a lot of
ways that photons avoid getting into
your sensor but if you add micro lenses
you can increase the efficiency from
about 16 percent peak to about 40
percent peak so a factor of two and a
half and your question up here first
due to the noise
yeah so what's happening out here past
800 nanometers is the question all three
of the color filters have are
essentially transparent out here this
falling slope is due to the fact that
these photons are being absorbed deeper
and deeper and so fewer of those
electrons make it up to the photodiode
so the infrared cut filter will be back
here somewhere and all of those effects
won't matter your question back
yeah as Christian is is it possible to
make the read out well smaller than the
capture well so they don't take up as
much space that's a that's a good
question
and yes it's certainly possible but if
you make if you make the the charge well
in this vertical CCD shifting structure
much smaller than the photodiode
it's going to have less capacitance it's
not going to hold all the charge so
there's a there's a natural balance that
you want you want to be able to hold all
the charge here that you can collect
here or pretty nearly the same and
there's some adjustment on that using
overflow control structure voltages and
so on now the trouble with these micro
lens 'lets is if when you get off axis
some of that light that if this is the
sensitive photodiode region some of that
light will miss it so you have a
fall-off of quantum efficiency with the
angle of the incident light and you can
find a plot for that in the datasheet
that shows for the red green and blue
sensors how they fall off with angle
this is down to about half at 12 degrees
of angle in the horizontal direction on
the chip and the angle in the other
direction it hardly falls off at all
because you have these long skinny photo
diodes and in one direction you don't
miss them when you get off so you get
this very asymmetric kind of vignetting
effect where the the amount of light
collected is very angle sensitive and
one dimension and not at all in the
other so you have to correct for that
I've got about ten minutes left I want
to say a little bit about the sampling
and aliasing effects that you get you
you put an array of these things
together and each one of them is picking
up light from a small area and averaging
it over that small area the area can be
defined by the photodiode or by the
micro lens that concentrates the light
into the photodiode so as a test pattern
we're going to use this thing called
ozone plate which is just increasing
frequencies from the center you make a
you you computer radius and make a phase
that's quadratic and radius and take the
sine of it you can make this kind of
thing real easy in a few minutes in
MATLAB and then you can do some sampling
of it or see what it see what it does so
here I sampled it by just drawing lines
over it in power
so I've got a set of lines here and a
set of lines here so literally I've got
that same picture in the back and drew
lines over it so I haven't done anything
I haven't done any computation here but
I hope you can all see circular patterns
in here could everyone see that yeah
hard to see from where I am they're too
big so these low frequencies that are
generated from high frequencies the
pattern underlying has high frequencies
in here but you're generating low
frequencies because sometimes the little
areas that aren't obscured see the dark
part of the curve like here and
sometimes I see the like part of the
curve like here so it's it's taking a
higher frequency and aliasing it to our
lower frequency and you get these really
cool Moray patterns this is something
that's very often a problem when you
sample an optical image with a with a
grid of sensors and this in this case
the the active area of the sensor would
be the size of the little openings
between the lines here if the opening
was a little bigger you'd get a little
bit less of this pattern but you'd still
get it if you take that same test image
but you blur it so there's much less of
the high frequencies here you can still
see it's it's kind of there but it's
rapidly falling off due to a Gaussian
blur that I applied here and you put
those same lines over it you can see
that there's there's still some aliasing
you can see in this region a little bit
of kind of backward circles and things
but there's much less and especially as
the frequency gets higher the amount of
signal is so low that the the aliased
copy of it is so small that you can't
see it so this is the effect of of a pre
blur an anti-aliasing filter that's
almost always used in in color Bayer
filter type cameras so this the so I
like to demonstrate sampling and Maury
patterns to know no computation just
draw lines on a picture the spatial
frequency of the optical system tends to
have some roll-off anyway due to lens
diffraction and aberration so as a
function of frequency it'll be coming
down and the filters that are added the
anti-aliasing filters that blur it more
tend to have a null because the way they
work is a
create two copies of the image close to
each other and you get destructive
interference at certain certain
frequencies so if the if the frequency
that's aliasing down to low frequencies
is somewhere near this null that'll
that'll reduce the aliasing down to low
frequencies to near zero and you get rid
of low frequencies means large scale so
you get rid of the large scale more a
patterns by by putting this notch near
your sample frequency and you can you
can control where that notch goes and
the fact that it comes back up again is
usually not a big deal because the lens
is continuing to fall out there so very
simple filters actually are fairly
effective at reducing aliasing at least
in monochrome systems in color it's more
complicated when you overlay these red
green blue mosaics on the zone plate
which again I've done here by just how
did I do this when I might have cheated
on this one did some computation in
MATLAB to make the picture but basically
at each pixel I've I've got only a red
green or blue response equal to what the
grayscale of the original image was
underneath and and averaged in little
square regions and you can see these
really prominent color artifacts and
spatial artifacts and so on so you
really don't want those high frequencies
getting into your picture and of course
there are reconstruction algorithms that
will partially get rid of those what's
hard to do so completely this is a
close-up of the region where you have
the the frequency of the original test
pattern is hitting every other column
and so it's here it's hitting primarily
the the green and red and here it's
primarily hitting the green and blue
until you get these low frequency color
variations if you're in Photoshop and
you just tell it to desaturate and give
you a grayscale image you'll get
something like this where that aliasing
structure has has now been converted to
a a luminance art effect of a fine kind
of you know grid like or maze-like
pattern you want to be careful about
about that kind of thing on the other
hand if you if you convert to grayscale
Damali by knowing the relative ratios of
red green and blue at each place and and
computing a grayscale in a consistent
way you can get back to the same pattern
you would have gotten with with more the
monochrome sensor so if you know the
chroma you know the color at any given
region
you can use that to correctly infer the
luminance but of course you don't know
the chroma you've only got a sub sampled
version of the chroma so what you tend
to do in these algorithms is trade-off
between smoothing the chroma and trying
to get a local chroma estimate that you
can use to improve the luminance versus
versus the kind of luminance hashey
artifacts that you get when that
estimate is not right so so there's a
lot of tricks and and you can do
different tricks in different regions
and try to infer boundaries between them
and so forth which is why the whole
thing about reconstructing images from
color mosaic samples is such a big
inactive field for the last 10 years or
so that book by Holst that I showed you
on sampling and data fidelity and so on
has this table in it that talks about
all that all the different kinds of
elements that typically we just call
pixels and what they've said a picture a
pixel is a picture element is this it's
a pretty conventional definition which
means which is it it's one sample
created by a detector it's they treat a
pixel as an information element and
that's that's true it's probably the
oldest and best definition but it's
certainly not the only way it's used and
in fact all of these other things data
elements and display elements and
resolution elements and scene elements
these are all called pixels in various
contexts as are the physical units of
sensing apparatus and display apparatus
so I throw this in just because it
represents this book and there's a lot
of really thoughtful thinking about
aliasing and stuff in there and that's
kind of interesting stuff
Ian introduced you last week to the
notion of the luminosity function that
converts power to sort of visibility of
light and this is a table I just copied
out of the Wikipedia that shows some of
the different units
radiometry is a field of measurement of
radiant power and before I say too much
about that they're there the curves and
this is photometer II another table and
photometry is the measurement of power
weighted in a way that corresponds to
how humans perceive brightness so
there's a whole parallel set of units
and measurements and the relationship
between them is this spectral weighting
the the black curve the solid black
curve is the official 19 actually it
predates it predates the 1931 CIE color
space work I think it's 1923 it's called
the V lambda curve and it was a huge
landmark in visual perception and
standardization to have this curve
defined it's been redefined by various
tests since then and you can find
publications of data with slight
Corrections at the short wavelength end
where the the the wavelength the the
apparent brightness of these short
wavelengths was underestimated and the
standard so they're these sort of
revised standards you can use if you
prefer but usually the the old original
one is used and if you're really in the
dark this other curve represents topic
visibility function but that's that's
not usually relevant to color perception
color photography and so on
I made this curve from the data set put
it in the Wikipedia so you can find it
on this article read all about these
different units and get the references
to the data set if you want them these
functions are really important if you
want to do things like convert units
that people tell you that a light meter
measures like Lux what's a Lux well Lux
is lumens per square meter that's the
light coming off of a surface reflected
from a surface or emitted from a surface
if you don't know how much how many
photons you're going to get from that
you have to convert back to energy units
and that's where the radiometry units
come in you need to know how much energy
at what wavelength and to do that you
have to you have to take this curve into
account knowing the wavelength to get
back to the energy the the watts per
square meter
this guy here the radial accidents to
figure out how much energy is coming off
that surface you can you can then
there's simple formulas that'll map that
into the illuminance on the focal plane
of your camera and you can figure out
how many watts per square centimeter and
the focal plane and convert that to you
know joules per exposure time per pixel
and figure out how many joules you got
and then if you know the if you know the
spectrum you can you can convert that
back to how many photons you're
absorbing so we started off with
equations for getting number of photons
from an amount of energy and these are
all the the kinds of things you need to
get the amounts of energy from things
that your light meter measures or from
specifications that somebody tells you
like volts per Lux second sensitivity of
your sensor and things like that and I
think that's about it yeah
so we're there I've got through all my
slides today yeah and next week we're
going to talk about the processing
pipeline in a camera that takes the data
off of these sensors and and goes
through and tries to generate an image
from it and it's it's actually turning
into a kind of a logical sequence
because we had ROM talking about optical
image formation and ian's about getting
the image onto the sensor and getting a
good quality image and you know good
optical noise considerations and now we
know we've gotten through what the
sensor does and next week we'll say what
to do with that data to make a
photograph out of it thanks
any questions while you're here here's
one the Gaussian blur filter that I
mentioned for anti-aliasing I just used
photoshop's filter they have under their
filters menu they have a thing called
blur and under that they have one called
Gaussian blur which lets you change the
size it was I did that in the digital
domain iya Oh in the optical domain what
filter is it okay so I didn't I didn't
draw a picture but I showed the MTF plot
of what the filter is it's not a
Gaussian blur exactly it's the one that
had the notch in it that notch comes
from a layered birefringence crystal
usually two layers with different
orientations that takes the image in and
it splits it into two polarization
States and puts them out at different
places and thereby by making two copies
of the image or four copies you get kind
of a blur it's not at all like a
Gaussian blur but it has the intended
effect of knocking out some high
frequencies yeah hi
cycling sensors
see that
oh yeah good question so high ass
because of the rectangular shape of the
photodiode in a ccd with the inter line
transfer they'll do more averaging in
one dimension and less than the other
and you should be able to see that in a
difference in horizontal and vertical
Moray patterns and yeah that's true in
fact that's why they usually measure
horizontal and vertical resolution
separately that usually get slightly
different numbers but the more a pattern
is actually probably a more sensitive
way to see that it depends to because it
that also puts a notch in the transfer
function it depends on where that knotch
falls relative to what a filter does as
to whether you're going to see the
difference or not but yeah it's it is a
measurable difference it can exist in
CMOS sometimes I have rectangular photo
diodes and sometimes you do see the
difference yes yeah
yes question is do modern digital SLRs
use anti-aliasing filters and yes they
all do the exceptions were some of the
Kodak cameras Kodak had at least one
camera I think of 14 n that didn't have
an anti-aliasing filter alex says the
canon 10 d doesn't have it either I am
pretty sure that's not correct but we
can look it up and he's positive it
doesn't have it so I don't know a camera
without a high-res camera without an
anti-aliasing filter is going to make
horrible color artifacts and I'm pretty
sure all current DSLRs have
anti-aliasing filters and almost all
medium-sized point-and-shoots
it's in the very low end like the cell
phone cameras they don't they get enough
blur from the lens that they don't need
it
so yeah good question in the fobian
sensor with the stacked photo sensors do
they need an anti-aliasing filter and
the answer is sort of yes and no there's
they don't use a separate anti-aliasing
filter what they do is they rely on the
fact that the the fill factor with the
photodiode is near a hundred percent
which which if you work out the
transform of you know 2d Fourier
transform of a square it puts a notch
at exactly the frequency that would
alien to DC so it's when you can get to
100 percent fill factor it's kind of an
ideal compromise in terms of preventing
large-scale morei patterns and it has a
shape of a roll-off that's very much
like what you can get with that's
actually little better than what you can
get with the typical lithium niobate
anti-aliasing filters so it's it's hard
to do much better than getting a hundred
percent fill factor you can't get a
hundred percent fill factor in a color
mosaic chip because you know the red
only has one fourth of the area
available so it's it's half in each
dimension which means that that notch is
a factor of two higher than where you
want it which is just way too far out to
do you any good so there's a huge
difference in the anti-aliasing effect
of micro lenses on a monochrome or fovea
on type sensor from what it is on a
color mosaic type sensor and that that
difference means you don't need a
separate anti-aliasing filter yeah Peter
okay so is angle of incidence a problem
on C my sensors it is a problem it's
just that that one I showed was probably
a worse problem than any other one I've
seen somewhat worse what what Olympus
did in defining their Four Thirds system
digital camera family as they they
decreed that the chief ray angle at the
corners of the field should never be
more than ten degrees or something like
that and that that puts a constraint on
the family of lenses designed for that
family of cameras other people haven't
been able to do that they've designed
cameras it had to work with existing
lens families which means they've had to
tolerate larger chief ray angles and
generally you don't know what the chief
ray angle is when you put a lens on the
camera so they've had to make it
reasonably flat response and so they've
they've engineered it to to not fall off
too much at reasonable angles but it's
it's it is true that CMOS does have that
same problem sometimes they offset the
micro lenses a little bit to optimize
for a particular angle and then you get
some range around there that works
pretty well do CMOS usually have micro
lenses yes almost always
you could see a full-frame Nikon camera
sometime yes if if Nikon changes what
they've been telling everybody which is
that they're never going to do that yep
and we're gonna talk about the fobian
sensor I can do it's kind of a problem I
know too much I can't talk about
everything about it but and I don't
think I want to try to do a full lecture
on it but I mentioned it from time to
time when it's differences are relevant
and if there's more you want to know let
me know ok thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>