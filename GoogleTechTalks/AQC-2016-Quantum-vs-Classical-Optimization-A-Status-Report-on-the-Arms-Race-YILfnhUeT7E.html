<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AQC - 2016 Quantum vs. Classical Optimization - A Status Report on the Arms Race | Coder Coacher - Coaching Coders</title><meta content="AQC - 2016 Quantum vs. Classical Optimization - A Status Report on the Arms Race - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>AQC - 2016 Quantum vs. Classical Optimization - A Status Report on the Arms Race</b></h2><h5 class="post__date">2016-10-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YILfnhUeT7E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay please be seated
I'm Boris Altschuler from Columbia
University and I'm supposed to share
this session and the first speaker will
be Helmut Katz grabber from Texas A&amp;amp;M
and it's about quantum versus classical
optimization thank you it's a pleasure
to be here what I'd like to do today is
give you a status update on quantum vs.
classical optimization and because right
now we have the copa de las américas in
the euro cup I figured I'll keep tabs on
how things are going with a little
scoreboard throughout my presentation
now the results that I will be showing
you and this is a slide that I always
like to show are really at a crossroads
from statistical physics quantum
information theory and high-performance
computing and you will see today that
statistical physics is actually quite
important when you want to study pontem
computing related problems in especially
if you want to optimize complex systems
now there's a lot of questions we would
like to have answers to
for example can quantum annealer be
faster than classical computers do
applications exist where quantum
annealing excels with so far studied
mostly random spin class problems and so
far results have been rather
inconclusive enough of course what are
the limitations of transverse field
quantum annealing are there things that
we know ahead of time will be very hard
to overcome if we use this type of
optimization technique and so what we've
been doing is with developing state of
the art classical solvers in an attempt
to raise the bar for quantum annealer
but at the same time to also create
benchmark problems for quantum annealer
we also refined a little bit the
definition of quantum speed-up and I'll
get to this later in this talk just so
that we can do a slightly fair
comparison and one of the things that
really we are doing mostly is using
statistical physics to analyze benchmark
problems and in particular we're trying
to predict if a problem is well suited
for quantum annealing or not now this
sounds almost crazy that we're
trying to predict something but
hopefully we'll be able to show you an
example where it actually seems to work
this was done with a lot of people
sergio bazzill sara gay and Hartmut at
google as well as Phyllis hums the
d-wave Alejandro and Salvatore Marc
mandra Massage definitional in Leipzig
and a bunch of guys that work with me as
you see it's usually hot in Texas so
this is how we dress most of the year
now why do we actually need new
computing technologies there was a
beautiful editorial in Nature earlier
this year entitled beyond Moore's law
and I think we all know Moore's law I
think that something we can assume it's
basically says that the transistor count
will double roughly every two years and
this trend has been going on for a very
very long time you can just send a
straight line through that and you see
it's still going this is great news but
there is a piece of information that is
often swept under the rug and that piece
of information is that the clock speed
has stalled since about a decade and a
half or so so if you look now at the
clock speed of the processors it's
basically a constant meaning that you
have reaching fabrication limits
you can't go much faster so the only way
to pack more transistors into a chip is
just to add more cores the other thing
that a lot of people don't think about
is that if we ever want to build an
exaflop machine we will need gigawatt
power and so it's a little bit like you
know electricity for a town or running a
computer which is going to be a very
difficult issue to tackle and so this
has led to many investments Google IBM
Microsoft and many others and of course
the big question is is quantum the
technology of the future now we do have
now commercially available quantum
devices there's all kinds of things
however the current state of the art
when it comes to computing our special
purpose computers and I think we all
know exactly which machine I'm talking
about it is the big black box from
d-wave systems Inc as you know it's a
special-purpose quantum annealer so
there's only one thing it can do it can
optimize problems mapable on its
topology and there have been sold a few
of them and
I think that this is still a
groundbreaking result building such a
device with so many qubits is not easy
I've heard it will come out with a 2,000
cubits chip in the near future and so
this is in my opinion a huge
technological feat now what can you do
well it can minimize problems in Cuba
format in other words if you write it as
a quadratic form something like an Ising
model once you can encode it onto the
fix topology of the chip known as
chimera and within the machine
constraints for example position and if
you want to see some of the ideas that
we have been developing about how to
improve chimera itself I encourage you
to look at the poster by mark Novotny
later today now one thing that I want to
talk about very briefly is how the
Machine optimizes and this is important
because what I will do is I will do an
analysis of the performance of the
machine and for this I should remind you
how it works and actually the way it
optimizes is sequentially now you might
wonder why is there a big axon here and
the reason is that the way the d-wave
optimizes was developed about 7,000
years ago in the Neolithic era and the
Germans of course discovered that if you
anneal a piece of metal it's more
malleable you can make a better ax and
you can more efficiently bash each
other's heads in now we don't want to do
this here but basically we want to just
use the same approach numerically and
this is known as simulated annealing you
have a Hamiltonian or a cost function
you stochastically sample it once the
system is thermalized you cool it and
you do this according to some annealing
schedule for example here a linear
schedule and then you hope to find the
optimum in fact if you are neil
infinitely slow there is a theorem that
proves that you should find the optimum
however that is of course not very
efficient now when you have a problem
with a rough energy landscape say like a
spin glass or a complex optimization
problem then simulated annealing has the
potential of getting stuck in any of
those metastable States and to overcome
this what people have done in previous
studies is very simple you just do
multiple restarts in other words you
rain down on the landscape until you hit
the jackpot this allows you to do
statistics and then
basically figure out if you found the
optimum or not at least with a given
probability now the d-wave device does
basically the same using quantum
annealing this is what we all hear
except that instead of punching thermal
fluctuations what you actually do is you
just quench quantum fluctuations it's
basically the same thing now the nice
thing is that you're not limited to a
local search and potentially if barriers
are thin enough your system might be
able to tunnel through the barrier and
find the optimum and the way it's done
in deal with 2x is transversal quantum
annealing where you just apply a
transverse field doesn't commute and
there was a beautiful animation or
earlier by Brazil that showed this and
then basically you reduce these quantum
fluctuations D according to some given
annealing protocol now there have been
many many attempts to see if this
machine can do something that classical
computers cannot do and I think that the
most recent results that are most
promising and they were presented in
detail earlier by Brazil are the recent
results by the Google / NASA team where
they showed that there can be a very
large speed up for very carefully
tailored problems I'll show you again
the results and I'll just focus on the
50 percentile for simplicity and of
course the result is simple to summarize
if you compare first of all just
absolute numbers this here is around 10
to 13 this is about 10 to the 5 so you
have roughly at 10 to the 8th speed up
for the largest system size of 945 or so
qubits
however this offset is not the most
important thing the most important thing
is actually a slope of this curve
because I synthetically this is what is
going to determine who wins the race and
you see very nicely that the simulated
thermal annealing has a much steeper
slope that either quantum Monte Carlo or
the d-wave device so this is in my
opinion really the first strong
signature that quantum approaches scale
better than classical approaches and
what this means is 1 0 for quantum now
let me see a little bit more about
Google's weak strong cluster instances
as I said was you'll mention this
already before I
just want to look at it from the spin
glass point-of-view and that is you have
what they call weak strong clusters
basically they're all ferromagnetic li
coupled you have one field that is
stronger than the other and this cluster
here is just basically kind of like the
Siamese twin to this one and this one
over here then connects to the rest of
the system in the network now the
important thing is to zoom out as far as
you can and just look at this rectangle
as one object and if you look at it what
you get is basically a spin glass with
positive and negative interactions on
some kind of network topology okay
the only job that these weak strong
clusters have is to basically fool at
thermal annealing into a into a
metastable state quantum annealer
however might be able to overcome this
problem now there are some potential
issues if you compare now quantum
annealing to simulated annealing
well simulator annealing is known to be
an extremely poor optimization method
there's far more state-of-the-art
algorithms out there and don't ask me
for a reference this is just for me
doing this for the last 10 15 years
some monte carlo methods are extremely
inefficient the second you turn on
strong uniform fields and the reason is
that flipping individual spins becomes a
very hard task so we figured let's do a
fair comparison and use what is known as
population annealing Monte Carlo instead
of simulated annealing some computer
scientists here might notice particle
swarm method and the idea here is that
we want to use something that is
sequential in other words you have a
control parameter either quantum
fluctuations or temperature that you
tune from a large to a small value and
at the same time we know that population
annealing is pretty much unaffected by
external fields which is phenomenal if
you want to simulate spin classes so how
does this population healing work in a
nutshell well let me remind you
simulated annealing you easily get stuck
you run it in repetition mode and what
you then get basically is the optimum in
population annealing you do the same
except that you run all these runs in
parallel at the same time and at each
temperature step you
call the population according to the
Boltzmann distribution what this means
is if some of these instances are stuck
in a metastable state they get killed
and replaced by fitter instances that
actually are getting to the true optimum
and so what this means is that basically
your effort gets reshuffled and then you
get to the optimum much better so again
the way it works is you just perform
capital R simulated annealing runs in
parallel ok typically RS say 10 to the 5
10 to the 6 it's a very large number and
then at each temperature step you do a
resampling of these guys you kill off
some and you repopulate with others so
think of it as a little bit as a mixture
of a genetic algorithm with simulated
annealing and then you do a few a
handful of Monte Carlo sweeps just to
make sure that Boltzmann is happy now if
you simulate spin glasses at a finite
temperature you will find that
population annealing is as efficient at
the state-of-the-art parallel tempering
Montecarlo it's an extremely good method
the other nice thing is that the
numerical effort is the exact same as
simulated annealing the resampling step
is really very small overhead so it's
fair to compare population annealing to
simulated annealing and you can just
switch off on this side the resampling
step and then you just have simulated
annealing run in parallel and it
outperforms simulated annealing in
repetition mode now I can demonstrate
this to you what you see here is data
for a three dimensional spin glass with
512 sides and a thousand side so roughly
the size of the d-wave and the d-wave 2x
okay and on the horizontal axis you have
the population size or the number of
times you ran simulated annealing in
locked n base and this is the percentage
of solved problems and you see that if
we run a population of 10 to the 5 or we
do the simulated annealing 10 to the 5
times on these problems then population
annealing finds all solutions simulated
annealing only finds about a third if we
increase the system size by a factor of
2 you should simulated annealing wasn't
able to solve anything at this size
problem where population annealing
clearly does solve still all problems so
it is a much more power
full method and it is sequential so
let's toss this into the whole picture
and I just highlighted the line for
population annealing these are again the
results by Google and you see very
nicely that the slope of course is less
steep than simulated annealing but still
far from the scaling we got with quantum
Monte Carlo or the d-wave 2x so one
could see this as additional evidence of
some quantum advantage in other words to
zero now population annealing is just
one method what if we now open the
drawer and pull out all possible methods
that we can think of basically a long
laundry list of algorithms and see how
they compare against all of those and so
before I show you the list of algorithms
I'd like to just very briefly refine the
notion of quantum speed-up such that the
comparison later is a little bit more
apparent until now in the paper by
runoff at all in 2014 it was introduced
that we have provable quantum speed-up
in other words you can prove that there
is no classic algorithm that will do
better say growver search and were
strong quantum speed-up where we
compared to the best known classical
algorithm say Shor's algorithm you have
potential quantum speed-up where it
compared to a set of algorithms and
these of course can be short-lived until
somebody finds a better method and then
there was what is called limited quantum
speed-up where we just look at speed-up
over a classical counterpart and so we
decided to take limited quantum speed-up
and just refine it ever so slightly we
first introduced limited tailored
quantum speed-up what does it mean it
means that you know the structure of the
problem so you're not receiving a
problem which has no structure you can
look at it and say ha if I use this
trick I can abuse the structure and find
the optimum faster prime example the
hams de freitas lb algorithm which
basically exploits a three structure of
chimera then we have limited non tailor
quantum speed-up and I would call this
the gold standard these are generic
algorithms they assume nothing about the
system but they are the fastest that you
can find and then there is limited
sequential quantum speed-up
which basically is a sequential method
like population annealing simulated
annealing quantum annealing and the
motivation for us to do this was to
basically perform a better description
of the benchmarks so we went through
this long list of algorithms we looked
at Taylor at once
for example the hums of the fractals lb
algorithm a super spin approximation
that James you who will talk after me
introduced which is basically something
rather brutal you take each of the weak
strong clusters and you call it one spin
in other words you just take this and
replace it by that and instead of
solving a problem with nine hundred and
something variables you're not solving a
problem with maybe twelve variables
obviously you'll find the solution
instantly okay and then there is a
hybrid cluster method by Salvatore
Eamonn Drive in Tralee etc then you have
the non tailored algorithms for example
population anneal language basically is
sequential but it's not tailored
parallel tempering with as energetic
cluster moves is the solver that we use
and then we use some other things like
replica Monte Carlo and then of course
we compare the simulated annealing and
quantum Monte Carlo let me show you the
results and ask you to not try to
discern what is what in this spaghetti
plot okay these are the simulations with
all these algorithms and I just
highlight the two lines one is again the
d-wave result or the on the machine and
this is quantum Monte Carlo and you see
that this here is simulated annealing
population annealing but all the other
ones are pretty much comparable to the
performance of the d-wave device now we
decided to donor synthetic scaling in
other words exclude the small system
sizes the reason for this being that it
seems that there is a double scaling in
the d-wave due to noise in Salvatori
mandra will give a talk this afternoon
at 3:30 actually addressing this point
in detail with this beautiful simple
model that he developed to address this
now because we're doing an asymptotic
scaling again all that matters are the
slopes we're going to try to fit an
exponential with some coefficient B and
we also try to do this with a polynomial
pre factor to normalize it and all that
matters is this exponent B because that
will tell you what the slope is of this
individual line
and what you see here are these
exponents B now let me walk you through
it there is red blobs and blue blobs
forget about the blue blobs this is
where we assume there is a polynomial in
front the red blobs are this coefficient
B here for the different algorithms
simulated in linked population annealing
deal with two quantum Monte Carlo hybrid
cluster replica Monte Carlo parallel
tempering HFS super spinner London and
the important thing now is I should
color those in and of course a smaller
value of B means a better scaling in
other words a faster method and you can
see very nicely I drew a line here
through the d-wave results that within
the category of sequential algorithms
clearly quantum Monte Carlo and D Way
sorry I draw a line to come to Monte
Carlo comes when to call in d-wave
outperform population annealing and
simulated annealing however when you
even use non tailored algorithms in
other words generic methods that we
developed you see very nicely that they
perform better so in other words I would
say that what we have here is sequential
quantum speed-up and so what this means
is 1 0 1 to 2 now the next important
question is does the scaling persist for
large system sizes and this is going to
be a little bit technical ok this is
where the statistical physicist in me
comes into the picture and it starts
with something that Hart would mention
this morning this P of Q thing and so
what we do is we need to find an order
parameter for a spin class and the
natural order parameter for a spin class
is the overlap between two copies of the
system why is that if you look at a
ferromagnet and you lower the
temperature close to 0 then what you
will see is that either all spins are up
or all spins are down so you just look
at the picture in your right away the
system is ordered in a spin class the
spins will freeze randomly in space and
so if you just look at a snapshot you
will not be able to tell if it's in the
ordered or disordered phase so what you
do is you simulate two copies of the
system with the same disorder and then
you compare spin by spin and we call
this quantity the spin overlap Q and you
see that if this
Papi's agree then for low temperatures
you should go to one model lilies
normalization and for high temperatures
goes to zero so it's a natural order
parameter for the problem now you can
show you cannot measure the distribution
of this quantity and again I'd like to
remind you of the ferromagnet if you
look at a ferromagnet at very low
temperatures and you look at the
distribution of the magnetization either
the spins will be all up or all down in
other words you will get two Delta peaks
at roughly plus minus one okay but if
you don't increase the temperature
magnetization will average out to zero
and due to the central limit theorem you
will just get a Gaussian centered around
zero in a spin glass you have the same
behavior at high temperatures
however at very low temperatures you can
have dominant states that form and form
different basins in the energy landscape
in other words instead of just having
one peak at plus minus one you can
actually have a multitude of peaks and
the position of these peaks here you
have three examples red black and blue
strongly depends on the disorder that
you chose for that particular problem
and so we've been staring at these
functions for a long time and if you
have no linear terms in the field it's
been reversal symmetric so it's enough
to just look at half of the distribution
and you can show that if there are many
features then likely you will have
something corresponds to a wide barrier
and I don't have time to go into all the
details of this and when you see thin
sharp features then likely you will have
thin barriers that show the potential
for tunneling and I'd like to also
mention that if you typically have a
single wide feature then it could be a
problem without any features what I mean
by this basically something like a
ferromagnet something parabolic and
simple model or some little bumps now
I've swept a lot of details under the
rug but we did very careful studies here
in this first paper with Buju fears back
and you can very nicely show that this
complexity of this function also
correlates very well with the
autocorrelation times of
state-of-the-art algorithms like
parallel tempering so we went back now
and measured these quantity for the
google instances and
we found is typically two types of
problems one that looks like this a
single sharp needle peak that is away
from one and the other one basically
looks like a tooth with maybe two or
three peaks that are really close
together and so what this means is and
this is based on more than just looking
at pictures that whenever you have a
very thin peak you have either a thin or
no dominant barrier and typically the
Hamming distance for that is about less
than 16s and we know that d-wave will
have some tunnel tunneling advantage if
the barriers thin enough however when
you have double or triple features like
this that overlap the barriers are too
thick to see any tunneling and so what
we can now do is we can plot the
fraction of problems that have a thin
versus a thick barrier and what you see
very nicely is that with increasing
system size the fraction of problems
with white barriers becomes dominating
in other words the bigger you make the
system the more the spin glass backbone
will dominate as buzina set in the
morning and you will eventually lose
your scaling in the quantum atman
advantage will vanish for large n so
sadly we're tied good now I've showed
you these results and now the question
of course is can we use this for
something else and what I'll show you
now are very new results that we have
just produced and I'll just show you one
case study because Jeng will show you a
whole palette of problems that we've
analyzed and this is very weighted
partial max at it's just a typical
constraint satisfaction problem max that
is the problem where you want to
determine the number the maximum number
of clauses which satisfies a given
boolean formula and weighted partial
means that you satisfy all clauses and
maximize the sum of the weights of the
soft classes and you can encode this
into a cube or like Hamiltonian and then
basically solve this with whatever
solver you want to solve now what does
the P of Q look like for this very
particular problem well you get
something that has many spikes and when
you have many many thin spikes it's
suggestive that if there is some kind of
time
you should be able to tunnel through
these spikes and so what we did is we
sent these problems to Sergei Yakov
he ran them using simulator annealing
than quantum Monte Carlo and what you
will see in the next slide is basically
the speed-up of quantum annealing over
simulated annealing measure in Monte
Carlo sweeps and what you see very
nicely is that first of all this is a
logarithmic scale and for about 90
percent of the problems you need it
about a factor of thousand less Monte
Carlo sweeps in quantum Monte Carlo than
in classical Monte Carlo in other words
you are able to predict that this type
of problem would be perfectly suited to
run on a quantum type architecture in
the mean time we've tested of course
everything we can get our hands on
random spin glasses well they're random
but if you mind the data according to
the features again you can show speed-up
same thing for vertex covers 3sat fourth
graph partitioning circuit fault
diagnosis etc etc so in other words we
have known opportunity to basically go
in and decide ahead of time if a problem
might benefit of quantum speed-up and
this is really great because it comes
back to what Harwood said you can do a
run a test of the problem and of course
there's still a lot to be learned here
and then decide if you farm it to a
silicon type machine or to a quantum
device and again if you want to learn
more about all these things here just
stick around for the talk right after
mine so I would say given this
information it's a two three now the
last thing I want to address is
something very new that we did and this
is what we call fair sampling and so
first sampling you know everybody gets a
beer basically is very simply defined as
the ability of an algorithm to find all
solutions to some degenerate problem
with roughly the same probability so if
you run your algorithm and you know the
problem has 17 different solutions and
you run this ten thousand times and you
make a histogram you want to make sure
that these 17 bins are roughly filled
the same model OOP Estonian these
fluctuations that means your algorithm
can find all solutions to allow
and not just the minimum and this is
very important because sometimes
solutions are more important than the
minimum think for example at SAP filters
where you need many uncorrelated
solutions hash that or if you want to do
contingency testing or for example if
your problem has some other constraints
that are not part of the optimization
procedure think solving a Traveling
Salesman problem but then it happens
that your trucks are part in a place
where you're not right now so you would
like to have a better solution that
matches this additional constraint so
the gold standard for any optimizer is
find the ground-state fast and reliable
a much stronger test is find all
minimizing coagulation zyk we probably
in back in 2008 2009 I did a very nice
study with Yoshiki Matsuda in either
Tashi Nisha Mori
where we looked at first at a toy model
of five spins your dashed line means
anti-ferromagnetic solid lines
paramagnetic and basically what we found
is that this has three the general
solutions model of spin reversal
symmetry and when you now look at the
probability of finding these three
solutions as a function of annealing
time if you have a transversal quantum
annealer what you find is that solutions
two and three you get 50% of the time
but solution number one is exponentially
suppressed similarly if you now say well
let's use a more complex driving
Hamiltonian like the one shown over here
suddenly this weird behavior goes away
and you find all three solutions with
1/3 probability which suggests that the
transverse field quantum annealing might
be biased so we thought let's do a more
thorough test and use a model that has a
really nasty collection of solutions and
that's the so-called fully frustrated
easing model it's just an Ising model
but every second row here is anti
ferromagnetic and you see now that every
plug head this model has no disorder but
every parklet is frustrated so a system
with 36 spins already has about 45,000
different the general ground states and
when we ran this again using quantum
annealing you see very nicely that some
states are
exponentially suppressed okay however if
you were to do this with classical
methods this is not the case so what
about quantum annealer well we ran
experiments on the D wave and we chose
our disorder to be in five six seven
plus minus and we modded those
interactions in a way that there is no
floppy spins and we can very carefully
control the degeneracy to be 3 times 2
to the K so 6 12 24 48 etc okay
and then for fixed C and K we studied
the distribution of rank ground States
now I don't want to bother you with the
details of showing every single plot I
can just show you one characteristic
plot here these are experiments run on
the d-wave for different number of
ground States and you see very nicely
that again the data are biased in other
words there is an exponential
suppression of certain states and this
is of course is very bad news because
this means that transfer free quantum
annealing is an exponentially by a
sampler and this also suggests that we
need to think beyond transverse field so
this leads basically to a tie but a tie
in quantum mechanics is nothing else
than a coherent superposition so we're
basically stuck with this and with this
I'd like to thank you for your attention
questions comments so they we have
microphones now so if somebody doesn't
want to come all the way up Boston and I
ain't your microphone thank you I can
see that this method would work for
multi spin interactions for multi spin
terms in the Hamiltonian and I wonder if
you have a feel for how much harder the
calculations get as you add more and
more multi qubit terms so I don't have a
feel as to how much harder to get but
they do get harder I can quantify it at
the moment okay thank you
so you finding that the distribution of
solutions is not uniform when you use
the transverse field Ising model the
transverse adiabatic algorithm but I
think what would happen if you varied
the paths I mean I would bet that if you
had a randomly sampled your paths you
would you would flatten out that
distribution what I can tell you is that
so we have not I mean obviously in the D
way if you cannot really do that and and
in some ways my point was basically we
need to think about you know more clever
things better driving Hamiltonians
the influence of thermal noise say with
this quantum parallel tempering better
path etc but these results as we
mentioned were done with many gauges
so despite applying gauges to the system
the bias was still there so my guess is
that unless you happen to find the
optimal path which is a measure zero
it's not gonna do much for you yeah yeah
sure but but I don't think that it would
help too much agree with you gauge
transformation is the classical
operation there are changing paths the
quantum operation takes you to different
corners of of the potential so I and
also the second comment is I would
encourage people to listen to d-wave
talks because
we can do multiple paths and we have
seen the effect of that in optimization
I encourage people to listen to those
oh cool that is news to me so I'll look
forward to that is so I know what the
d-wave machine that there's some
fuzziness in each individual coupling
from systematic random variations etc
how do you know how much that affects
the fair sampling well if anything I
would expect it to help you see because
if some fluctuation or error is applied
basically you are kind of breaking that
bias introduced by the strong
transversal at the initial of in a
beginning of the anneal so I mean
obviously in the end the questions are
still solving the right problem because
of fluctuations but I think that for
something like fair sampling and we
haven't tested in detail we can of
course simulate this any type of
fluctuation will likely be of a slight
help in my opinion
great talk Helmut thanks very much
so we've also been looking at the
backbone of samples coming from the
system you mentioned during your talk
you could imagine using this as an allow
analysis technique to determine which
instances to send the quantum annealer
but one of the things we've observed is
that you know getting a good estimate of
what that sort of spectrum of the
backroom look look
looks like can actually be very complex
and in fact we've been finding that
using the quantum annealer to try to
like heuristic aliy estimate what that
is is is actually one of the fastest
methods for that
do you agree No
sorry I just had to say that yeah so so
I mean this is one metric we also have
other metrics that we have developed and
that we're currently testing especially
based with population annealing what I
like about this is this is just you know
a feature of the problem and not the
algorithm but we can also use algorithms
to basically get a hand on how
complicated a problem could potentially
be as at least in a sequential for a
sequential method and that is at least a
fair comparison now to account to Manila
and they're the experiments we have done
with that metric that is extremely quick
to measure we have promising results
suggesting that there is a strong
correlation in fact even stronger
correlation with then with this P of Q
but having said that and this is
something that I always keep saying
there is really very little to no
understanding about this being glass
physics especially when it comes to the
order parameter distribution and
everything that we've learned so far has
just been literally often by trial and
error over the last two years you know
by tailoring a problem do a particular
physics outcome and then seeing what it
looks like and then using that to go on
problems where we don't know the outcome
and try to do predictions so there is a
lot to be learned from all kinds of
points of view back to the benchmarking
of the weighted partial max at problems
and the speed-up that was predicted
given the fact that cuba's are
equivalent to weighted max 2sat
i was curious to know what model of
weighted Max had problems that you try
yeah it's a complicated thing you see I
wanted to suite the details on the rug
Zhang is gonna show you more about this
in the next talk but there are some X
ORS in there and yeah it's not just a
standard max 2sat but at least it's a
whole class of problem you see before we
were saying we here are spin glasses
look these five are fast these 100 are
slow here we have something where it
doesn't matter what we generate it's
almost always fast
maybe last one and short question you
said that a fair comparison is only if
you come I mean it feels fair if you
compare the quantum annealer to a non
tailored approach now I definitely
understand that feeling but usually you
know the architecture of the quantum
annealer so you know it feels like in
the real world we'll always be able to
come up with a tailored approach I agree
and I disagree with that so we can
produce problems on the Camaro
architecture that we can predict will be
extremely hard for quantum annealing and
also for tailored algorithms like the
HFS method so you see there is ways and
if you look at the problem yes you know
the structure but you just feel random
bonds you cannot really discern what we
did to it yet it will be really really
hard thank you very much again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>