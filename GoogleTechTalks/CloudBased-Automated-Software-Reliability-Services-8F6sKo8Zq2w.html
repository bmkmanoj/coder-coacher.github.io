<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Cloud-Based Automated Software Reliability Services | Coder Coacher - Coaching Coders</title><meta content="Cloud-Based Automated Software Reliability Services - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Cloud-Based Automated Software Reliability Services</b></h2><h5 class="post__date">2010-08-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8F6sKo8Zq2w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm going to tell you what little I know
this guy he went to MIT and Stanford
these two kind of small sort of third
tier schools you may or may not have
heard of them I'm not sure so I'm not
quite sure about this guy's pedigree but
but he'll have to do on a recent suite
of European universities I think I
visited seven of them over about a five
day period back in May George's
University and George's team really
stood out I went I visited some of the
top universities in France Switzerland
Germany and the work down there was just
really first-rate they're not only doing
testing they're building some tools that
are solving some really important
problems so so i expect that through our
office in Zurich and through mine up
here in kirkland we're going to be doing
a lot of work with with dr. ken dia and
his team hopefully hiring some of the
students and supporting some of this
research so I'm going to hand it over to
George in a second but I want you guys
to send me any feedback from this so I
can get it to George obviously ask
questions as we go through but if you
have any specific interest in any parts
of this work please pass that feedback
to me because I'm kind of taking it on
myself to coordinate a lot of our
outreach with with universities so
having said that George all over to you
and hopefully will be a kind audience to
you hopefully not thanks James for the
intro can people hear me if I speak from
this distance yes Oh James can hear me
and he's probably pretty far away so
it's pretty good so well good afternoon
thank you for inviting me to give this
talk as James said a lot of the work we
do is very much applied we like to pick
our problems from the real world I
myself have spent a fair amount of time
in industry and I you know if I can't
justify the work we're doing with a very
clear problem then I don't feel
comfortable so i will share today some
perspectives on software reliability
these are
some of these thoughts are half-baked
some of the thoughts are baked a little
more and my goal is that with the
feedback that will get from you we can
refine this further and make sure we're
working on relevant problems and so the
the problem that we're looking at today
in in the present is that a lot of the
when it comes to the reliability of the
software we use there's little choice
but really take it on faith and together
with my students I'm trying to help fix
that and I'll illustrate with a kind of
a classic example which I think we've
all been through say you administer a
bunch of home pcs and you need to
install a device driver for the network
card used in these species and what you
need to do is you take this driver which
is really just a bunch of machine code
that was written by typically by some
teenage programmer that you've never met
and then you install this machine code
this unknown machine code inside your
operating system and you allow it to run
at the highest privilege level and when
I talk about acts of faith this is an
example right this is what i mean by an
act of faith because if this random
teenager walked up to you on the street
and asked you for the keys to your car
you probably would not give your keys to
this teenager but somehow you're willing
to give this code the keys to your
computer which probably is more valuable
than your car but we don't necessarily
think of it in these terms so what is
the state of the art defense that we
have today for these kind of things well
it's a pop-up window which i now you can
read what what's on and i'm sure you've
seen this quite a number of times which
is a warning and it says well you you
might screw up your system with what
you're about to do and here are two
options do it anyway or don't install
the software so it's really your choice
either way you can't safely move ahead
because if you don't install the
software you can't use the hardware now
wouldn't it be great if instead of this
you could go to some independent website
upload the device driver tell the
website what kind of software you just
uploaded it and then just to push the
test button and then automatically the
service explores all the paths looks for
bad behaviour things like race
conditions data corruption deadlock ants
on and then for each bug found it allows
you to download it click on the button
and you can download an executable
replay trace and this executor replay
trace is basically a proof that the bug
is real and it's reproducible so this
would be nice now this is not science
fiction we actually developed a
technique that can do such automated
testing of proprietary drivers I will
not talk about it in a lot of detail
today whoa I just did something wrong
okay so this is good to do you know
repeat them to get the message even
better so on anyway what I what I
clicked away was a reference to this
paper that explains how we combine an
automated test generation technique
called symbolic execution with
virtualization and dynamic binary
translation to provide this kind of
completely automated testing device
drivers on x86 binaries now the for
those of you familiar with symbolic
execution you know that this allows you
to explore the execution paths through
the code and then look for bad behaviour
along these along this path with no
human assistance right and that's
important and if you'd like more details
on this work i'll be happy to talk about
it during a Q&amp;amp;A session now i should
mention that this RPL 8099 example that
i'm showing you here is actually one of
the most popular network device drivers
has been shipping with windows for i
think over 10 years and it is microsoft
certified yet we did find these bugs
the bug shown here are two bodies found
in this driver and a bunch of other
microsoft certified drivers as well so
this is an example we're installing
drivers constitutes an act of faith
because we know they have these bugs at
least now we do yet we still use them
and I know what the problem is the five
at the bottom of the screen is cut off
so I don't know if there's something I
need to do about that or the AV folks
can do something about it but we're
missing the bottom out of the slide ok
so moving right along I I venture to say
that the software industry is really a
faith based industry to a large extent
today so programmers kind of write code
and the best of us also do write some
tests but the essentially we write code
and then kind of hope for the best users
take it on faith that the vendors have
performed thorough testing and third
there's really no way for consumers to
assess the reliability of a piece of
software before they use it buy it
install it anything like that so this is
a very strange industry and now
fortunately this isn't working out so
well because if you look at there's a
survey here which has found that over
the last few decades the density of bugs
has kind of stayed the same you know in
bugs per thousand lines of code and
depend on whom you ask well you know I
tango from one to ten production quality
software but at the same time what
happen is that the volume of code is
increasing exponentially so constant
density increasing volume this means
that we have more and more bugs floating
around and so clearly this faith based
approach is not working so what we need
is a disruptive technology that will
enable us to improve software quality by
an order of magnitude so we're attacking
this in two ways first we're working on
developing practical techniques for
automated testing and debugging so
there's a lot of research on this topic
but very little of it actually is
practical meaning it runs on large scale
software so the first part is
practically the second one is what I'd
really like to do is offer reliability
as a service and what I mean by this is
make software testing and debugging as
easy as web mail it should be
inexpensive fully automated accessible
to everyone and then if we provide this
as a service we want to develop the
techniques that can actually harness the
resources of massive commodity clusters
to actually provide this service very
quickly make it very fast as I said
we're huge fans of automated test
generation technique like model checking
and symbolic execution unfortunately
these techniques do suffer from path
explosion we're all familiar with that
the number of paths through program is
typically exponential in the size of the
program and so these techniques if you
run them especially if you try to run
them on large pieces of software they
ball neck on cpu run out of memory quite
quickly before actually can get any
decent coverage on more than a few
thousand lines of code and as it turns
out these are also very hard to paralyze
so we can't just throw cluster hardware
at the problem our goal is to make this
technique scale and accessible so what
I'd like to do today is talk at a high
level about the for reliability services
we have in mind and then dive in depth
into one of them namely automated
debugging now in terms of services I've
already shown you an example of testing
as a service for device drivers the one
with the website and such a device could
really empower consumers to test the
software they use before installing it
so we think of it as the home edition of
testing as a service beyond this
developers would also like to use this
so we think of reliability service
offered to developers both in terms of
testing the developer edition and
automated debugging and finally a
reliability certification service which
would act as a sort of underwriters labs
for the software industry now whereas
the Home Edition used a web browser to
interact with directly with the service
developers would use the ID as an
interface to deserve it so that while
they're writing their code the service
does continuous testing in the
background of that code continuously
pulling code from the developer
exercising the paths through it and then
checking for the desired properties hold
along these paths now continuous testing
as we are all aware is is very useful
because it catches bugs as the software
is being developed right so you want to
catch these things early on because
that's when they're cheap to fix and
that's when you know everything still in
your head because you wrote the code now
if we use this if we do this as a
service then it can become a lot more
powerful than anything we could do on
our workstations right because the cloud
the cluster has much bigger resources so
we can test much deeper we can look for
much more so you know systemic
properties along these paths so my hope
is that this form of continuous testing
that actually runs in the background
will one day become as indispensable as
for instance automated spell checking
right today in word processor right as
you type your word well if it's the
wrong word or it doesn't isn't spelled
right it tells you so hopefully we'll
have something like that we have taken
one first small step toward this we have
a preliminary prototype running on ec2
but there's still a lot of work left to
do on this of cluster based continuous
automated testing the reason we are
investing effort in this is because if
we get to the level where this is you
know just as quick as of on the fly
spell checking then we as programmers
can spend more time thinking about
system level properties and less abouts
of the low-level stuff that can be
automatically checked for right so we
use the human brain for stuff that
humans are really needed and whatever
machines can do
we delegate to machines another
reliability service helps debug deployed
code namely code that is out there
running in the field and then you get
bug reports from it writing a the
service-oriented set up such as Google
this may happen in a slightly different
form but for for things like platforms
Android and Chrome OS and so on you
would imagine having this situation as
well so this service would take a bug
report and information about the program
that is being debugged and send that
over to these debugging service which
will then produce what we refer to as an
explanation for the rest of this talk
it's basically a it walks you through
how that program could have exhibited
the bug shown in the report and then
this explanation is actually something
you can plug into a classic debugger and
just know that if you're in gdb just say
you know run or next ended it you can
just step through it and you see the bug
happening without you having to think
about how it could have come about so we
built a system that synthesizes this
kind of an explanation for bugs entirely
automatically based solely on a bug
report and in the second part of my talk
I will describe in more detail exactly
how we do that and now for the final
service we want to build a certification
service this provides an objective
assessment of a software products
quality you can imagine this service of
going out there are thoroughly testing
software products using these automated
techniques publishing the results and
thereby enabling consumers to
differentiate products based on their
reliability something you cannot really
do today at most you can read you like a
cnet report or something like that and
you know the same way you no longer buy
a car that has not undergone official
crash testing I hope you will still
refuse to use software does not carry a
quality certification perhaps something
like that if we
our way so software companies compete on
measurable things they compete on
performance because it can be measured
through benchmarks they compete on
features because you can check them off
a list now certification services will
hopefully add another measurable thing
to that the companies can compete on
which is reliability and at the moment
we haven't actually built this system
but we're gathering support to provide
it as a public certification service so
we've seen for reliability services
testing for home users testing for
debugging for developers the
certification service before I move on
to the second part of the talk you know
should actually ask the question is this
just about slapping a web interface on
top of existing techniques and you know
say the symbolic execution or model
checking and the answer is no these
techniques still have substantial
challenges to overcome not only is half
explosion of problem but you know if you
run some of these you you will see that
you know constraint solvers bottleneck
you know you end up spending a lot of
CPU time constraint solvers they blow up
in terms of memory if you want to put
this in a cluster that uses commodity
interconnects anytime you have to ship
state from one node to another it's
expensive so this isn't a trivially
paralyzed able problem unfortunately I
wish we could do with MapReduce the
world would be a better place so we're
you know the the the proof that this is
a hard problem is that after more than a
decade of research on the topic the
parallel checking community has still to
my knowledge not come up with a
cluster-based peril model checker that
can actually work efficiently so let me
show you how we tackle some of these
problems in the case of automated
debugging I will describe first how we
cast the problem of debugging as a
search problem then I will show you how
we combine static analysis with symbolic
execution to
help developers debug their code and
i'll show you some some results and to
motivate this let's see what is the
state of the art in debugging standalone
programs if you work on your computer
and you use Windows or you know any
other reasonably large piece of software
you may get one of these which is kind
of a you know the system hangs you can
end application and then I guess not
like this which says well you know now I
have prepared a bug report some
automated way of telling the vendor what
happened and i'm about to send it off
and and then you click send and it you
know there's the bug report produced by
the software itself and i'm not picking
on microsoft software my mac happily
gets into these situations all the time
now once you have this bug report it
will will be sent off to some big
debugging factory where human is kind of
toil away in cubicles and racking their
brains trying to figure out how that bug
report could come about how did the
program go from the start state to the
end state that is captured in the bug
report when the classic approach is
really to try over and over again to
reproduce the bug hope to catch in the
debugger look at the code try to
understand how it happened and what we
want is to really try to automate that
to become to get a little more specific
what is it that typically we need in
order to reproduce one of these problems
well it'd be nice to figure out what the
program inputs were or could have been
for that program to hang or to that
locker to crash how it interacting with
the environment you know say what
network packets might have received what
system calls it made and what the
results were and what was the thread
schedule and if you can guess these
three things about what happened in
production then you have a pretty good
chance to actually reproduce the bug on
your own workstation and then figure out
what's broken now in terms of research
the state of the art is to record many
of these things in production and often
virtual machine based approaches are
used for this and the goal of these
tools is to reproduce the buggy
execution exactly as it happened at the
customer site of course you have to pay
the cost of performance overhead in
production now you know the clear the
classic approach is not working we have
direct experience of that and the
surveys for instance this fairly recent
one from Microsoft on 2008 shows that a
lot of bugs take months to diagnose now
what I do want to point out that is that
I expect things to actually get worse
modern software is increasingly paranoid
and the hardware that this software runs
on is increasingly parallel as well so
the number of possible executions that
can occur out in the field is growing
dramatically and while our brains aren't
necessarily growing at the same rate so
there will be more and more diverse
executions which will make it more and
more difficult to debug so there'll be
more guesswork and as a result well you
know they'll take longer to find the
fixes and push them out so the problem
is that programmers get turned into sort
of bug detectives and then these
mysteries that they need to solve are
becoming more intricate as the software
became more complex in the hard ways
behind more peril so be really nice to
automate this detective work and we
developed a technique called execution
synthesis that similar to what you've
seen what I've shown you earlier in
terms of debugging service will take a
bug report and a program and then
produce this explanation that you can
plug into the debugger and we can do
this with no run time tracing no program
modifications and no runtime overhead
and we've tried it out on for real on
you know some database software web
servers
etc so let's so dive now into how we
actually do this I've been talking about
bug explanations and this tool produces
bugga explanations these are really
three sets of items as I was mentioning
program inputs the interactions with the
environment and this is mainly the
interaction with the environment happens
through libraries and system calls so
these are the things that would like to
be able to produce and the thread
schedule and all of these that led to
the field report a bug to manifest of
course the bug report does not contain
these things the bug report just contain
the the end contains the end result so
how do we get them or the idea is to
reconstruct them and obviously something
doesn't quite compute here because you
can't there's some information lost but
the key insight that actually makes this
possible is that as a programmer we
don't really need to replay the exact
same execution that occurred at the
customer site what we need is some
feasible execution that exhibit the same
bug ok someone didn't like what i said
and leaves up like that
is there anything I can do to the blue
button voila you should always have a
blue button with me that seems to work
so we don't need to replay the execution
we need to replay the bug and to replay
the bug there's a number of possible
executions that can do that and with
execution synthesis we want to
synthesize one such execution that makes
the problem a lot simpler and this makes
sense because the developers really want
an explanation of the bug not
necessarily of how it actually ran and
the customer side in fact you may not
even be able to run the same execution
that occurred at a customer site because
its environment dependent so if you see
how the bug manifests then you can see
the causality chain that causes the bug
and this eliminates the guesswork that I
was referring to earlier so you don't
have to do any more detective work and
if you can deterministically observe the
buggy behavior in a debugger then you
can employ classic techniques to fix it
you can do step-by-step execution data
structure dumps look at look at the
program state and so on so this is the
key enabler of execution synthesis this
distinction between replaying the
execution and the bug so one way to do
this is to think of debugging as a
search problem there is there are many
paths through a program of an infinite
number of paths and we want to find a
path that takes from the start of the
program down to the actual bug so the
natural thing to do is to explore all
the paths through the program and then
when you hit the bug you found the path
you need of course when you have an even
number of paths there's somewhat of a
problem with with this approach but for
now suspend your disbelief and let's
look at an example
this code here is a snippet from sequel
light it's and it's an a real bug it's a
deadlock bug and for those of you who
can read the code real quickly you will
see that there's a possibility that this
lock is acquired out of order so you do
a lock m1 m2 and then if you unlock em
one you will lock em one after having
locked em too so you have a you have a
locking version relative to other thread
that may be going through a different
path so that's how you can deadlock so
what we might want to do is let's say we
execute the first statement then we get
to this branch and this branch in terms
of paths creates two possible paths
right one for the den branch and another
one for the else branch and so we can
capture this as shown here we put the
condition in a box and then we say well
we have a true branch and the false
branch then we move on this is another
if condition and this also now branches
the execution and it will branch both
this part of the execution and this part
of the execution and and then we move
along and we get to the next predicate
next if branch which itself will also
split the brand will split the execution
and so on and so forth so this is one
way to explore all the paths by very
systematically and exhaustively through
the program and this is at the heart of
symbolic execution now obviously the
problem here is that you know we just
had three branches and this tree now
already has 2 to the 2 to the 3 8
possible paths so this blows up so this
naive approach conceptually works it
doesn't it will work for small programs
it will not work for real programs and
I've just shown me the example of the
number of paths been exponential but if
you think about the thread schedule as
well that adds another dimension of
complexity to the problem so even though
the approach won't
work let's try to build on it what we
need is to really be clever about how we
do this search how you know maybe we can
explore parts of the tree and not worry
about the other parts of the tree so we
can solve this the search problem more
efficiently and if we think of it as
search we have to now more precisely
define what the goal of this earth is
and so far i've talked about bug reports
but really what we want is this end
state and a bug report this has to be
extracted from the bug report so the
goal more formally speaking is at uppal
B and C where B is a basic block which
is where the bug actually manifested so
finally makes your program crashed the
basic block where it maybe even the
program statement on which you crashed
and see is a condition that needs to
hold for that bug to actually occur so
for example if you have a crash due to a
null pointer dereference this is the
simplest possible case then what you
need is the program counter of the
instruction that actually caused the
null pointer dereference and the
condition is that pointer is null so you
can see how this you know you can easily
pull this out of a core dump little more
complicated if you go to a deadlock
there you need the program counters of
every thread involved in the deadlock
and you need to capture the cycle
between the mutexes that were held by
the threads in the case of a race
condition it becomes even nastier
because you can have a race condition
that occurs over here and then it causes
a problem down here what we need is the
program counter or counters of the
detection site where the actual problem
was observed and then you know if for
instance it was caught by an assert
would use as the condition than negated
assert so you see or four different
classes of bugs you have different B's
and C's
and ESD which is the tool that
implements this technique extract these
goals automatically for crashes hangs
wrong output failures from the core dump
and also for bugs in external libraries
so now that we have the goal this BB and
see how do we find a path to it
efficiently so remember the the simple
naive approach is to do you know this of
exhaustive search and then hope to hit
the the goal this blows up so instead
execution synthesis uses static analysis
to work backward from the goal and
identify what we refer to as
intermediate goals these form a skeleton
that will then guide the forward dynamic
analysis to search from the start state
to the first intermediate goal then from
that go to the next one and the next and
so on and these searches actually
constitute an envelope for the past the
potential paths that lead to the goal so
this static analysis part is is really
crucial and so this is because it trims
down the search space considered
considerable okay so what we've seen so
far that we've cast the debugging
problem as a search problem the naive
search does not terminate so we break
down this search into a few interment
into smaller searches some then let me
dive deeper into how we do the static
and us and the dynamic analysis so we
need to look for these intermediate
goals these red dots and we work
backwards from the the goal this B and C
so the first step is to use the control
flow graph and look for what we refer to
as critical edges critical edges are
edges in the control flow graph such as
this one which must be traversed by any
path that reaches the goal so here you
can see that in order to get to this
goal you have to go through that edge
there's no other way so we find these
critical address
and you see how working backwards you'd
find these these edges now the branch
that corresponds to the critical edge
will I mean first we have to decide
obviously we're taking the true branch
or the false branch which is easy to do
because you know you know based on the
edge one that is and then for each
variable involved in the condition
inside that branch we find sets of
instructions further up that are
reaching definitions for those variables
inside the inside the condition and then
we look for combinations of instructions
from these sets that would give the
branch condition the desired value so
this is kind of a dataflow analysis that
for which we use the data flow graph and
then based on this we can construct the
intermediate goals which are really
instructions with static guarantees that
if you execute it then the critical edge
would be followed right and then of
course these basic blocks that contain
the reaching definitions you can mark
them as intermediate goals and if you
have more of them than you marketing to
meet a goal as a disjunction you know we
can go into more details later if you'd
like so let's see an example there's
again our sequel light code and here we
establish this as a goal because you
know if when this deadlocked well there
was a thread that tried to acquire m1
and it couldn't and it was stuck because
some other thread held it and then if
you look backwards you will see that for
that lock and one statement to actually
be feasible you need this condition to
hold true and part of that conditioner
holding true means that the mode
variable has to be set to mod y and then
you go backwards and you look where does
the mode where is the mode variable set
to mod y and at this point you have an
intermediate goal now it's a very simple
example just to illustrate the ideas you
can probably into it there's a lot more
analysis that goes into this but this is
fairly standard static analysis so we
actually
you know we didn't invent this let's see
the blue button Oh awesome I'll take
this remote with me this is great so
once we work backwards to find these
intermediate goals we just hop from one
to the next two in the forward dynamic
analysis how does the forward dynamic
analysis work well for those of you who
do not know about symbolic execution I
will tear in 30 seconds what the idea is
here when you run a program you can run
with concrete inputs you know acid an
actual integer number 5 or some string
or something like that or you can pass
it what are called symbolic careful
these are inputs that could take on any
value and that's okay let's actually I'm
enjoying this so these take on any value
and you can think of them as
unconstrained inputs all right so
something that is you know that you pass
it value 5 it's constrained to be 5 I
can pass it some value that is
unconstrained could take on say any
integer value by doing that we can then
explore every branch instruction
independently of what that value is and
then remember a lot of each path what is
the constraint that has to hold on that
particular variable or those variables
along that path so for instance down
here see on this true branch what we
know is that mode is constrained to be
mod y and I DX is constrained to be one
so those are called path constraints and
each branch condition imposes a
constraint on the variable involved in
that condition now as we're building
this tree remember we don't want to
necessarily explore the entire tree all
the time actually it would be a bad idea
to do that but any given point in time
during the exploration we have this
frontier these are unexplored
on the program and the question is which
state to explore next if you choose
smartly you will only explore the state
that will get you to the goal right so
if you had serve an Oracle that told you
exactly what to choose you just go
straight through the tree and find the
goal I you're done so what we want to do
is serve well that's pretty hard to pull
off but let's see if we can get close
and that he we use a variety of
heuristics but the key metric in
execution synthesis is this notion of
proximity which is a tight lower bound
on the number of instructions required
to reach the goal from the current place
in the program proximity is computed
based on the control flow graph and
given a current location in the program
for instance if the goal is inside the
same procedure the same function it's
kind of easy you just choose the
shortest path from where you are to
there in the control flow graph and that
is a tight lower bound so you use that
as a heuristic for how close a different
state is to to the goal if the goal is
outside the procedure becomes a little
more complicated because there's various
ways you might reach the goal for
instance you might return from the
current function and returned from the
next one up and then follow it through
say the second function of the call
stack so there to compute the proximity
heuristic we actually compute the this
distance metric the proximity metric
from where we are to the nearest return
instrument and then apply this
computation to the other functions in
the stack so it's basically how long
will it take me to return and then once
I return how long will it take me to
reach the reach the destination and this
this this proximity ristic then guides
us in which state along that frontier to
choose and think of I mean you realize
this is simply an optimization right
this is not a correctness guarantee of
any sort but it helps guide the search
towards the states that are closer to
the goal and keeps us away from the ones
that go
off you know into other sub trees that
were not interested in so we used this
heuristic we choose the state we
searched through the tree and once we
found the path then there's something
for us to do now how do we know that we
have found an interesting path while the
program counter now equals the be in our
goal right and the constraints along the
path on the variables are consistent
with the sea condition in the goal so
for instance if we want a pointer to be
null then if the sea condition if if the
constraints on the path allowed the
pointer to be null then it means that it
isn't it possible to reach that location
with owner the pointer being known so we
don't get too far from the mic but say
we got down here somewhere actually
let's see yeah so down here would be the
actual goal in that code example we had
then you just go backwards up in the
tree take the path constraints along
each edge put them in a big conjunction
and feed this to a constraint solver and
the constraint solver will then come up
with assignments to the various
variables involved that will make the
path constraints along that path true
and that solution will give you the
inputs and will take you will give you
the values required by conditions such
as this one where you make you know for
instance system calls or library calls
what value has to be returned to enable
that particular path and that you're
pretty much done if this is a single
threaded program now the problem is well
with multi-threaded programs which is
more and more the case in in practice we
also need to synthesize a thread
schedule because certain sequential
paths may not enter leave the way would
like them to link early so real quickly
the idea here is that if we have two
different threads we want to interleave
thread such that they follow the desired
path to the goal and we use for this
search we use heuristics that are
specific for each type of bug and to
illustrate let me give you the example
of deadlocks think of the schedule which
basically says you know how the thread
interleave think of that as yet another
symbolic variable and what we're trying
to do is solve for that symbolic
variable and this this schedule is kind
of a bit vector with each bit saying
that you know at this particular
opportunity for preemption there was a
preemption or there wasn't a preemption
so just on off and this long string of
bits is a symbolic variable now if we
just did the search blindly of course it
would take forever but we can use clues
from the core dump in the case of a
deadlock for instance the call stacks
are a very very powerful clue because we
see the threads that got stuck in the
deadlock what calls they made and and
these the interleaving of these calls
tells it right because we we see what
locks they hold we see where these locks
were acquired and so this tells us where
would be interesting preemption points
to put in more generally when you're
trying to find paths to explain deadlox
you really just need to put preemptions
before and after the synchronization
calls because if you preempt anywhere
before a synchronization call it's kind
of all the same right in the absence of
race conditions so if you have no race
conditions then it doesn't matter where
you've been preempted the bottom line is
that someone else may get to that
synchronization call before you do some
other friend so we use this heuristic we
use the cost access clues and putting
this together with what you've seen on
the previous slide we now have the input
the values we see from system calls that
came out of the the constraint solver
and the thread schedule that we actually
modeled as this bit vac
and we actually found the the particular
values for these bits so now what we
have this stuff ideally you know we want
we have the original binary of the
program and we would like to connect to
it with a debugger and just see the
thing happens see the problem happen so
to do this we have a custom shame
library which plays back the right
inputs that were found with execution
synthesis intercepts all the system
calls and feeds the synthetic return
values and you can imagine how this you
know so builds a shell around the
program and really makes the program
feel alone makes the program have the
illusion that things are happening
exactly in the right way for that bug to
be triggered and for the thread schedule
for instance in the case of in the case
of deadlocks the shim library would
intercept all the POSIX thread calls and
feed you know do the right timing you
know hold threads up to let others get
ahead and so on to enforce the synthetic
schedule and the schedule shows up as a
set of dependencies between instructions
in different threads it just says you
know this instruction has to run before
this one and then this one before this
other one and so on so the program has
no idea it is not talking to the real
world it's behaving just as if it would
behave if it were in the real world and
we can see that in the debugger now I'm
I'm insisting on this original binary
story because as you well know build
processes in the real world are fairly
complicated and you know there's also
flags and there's also things that
happen to the code in the build process
and then what you really want I mean
it's the binary that it that experienced
the failure and you may do analysis on
the source code but because of some
optimization in between you actually
can't find an explanation because the
explanation is actually in in this
process so we want to be able to make
this really be visible in the buyer
itself
so this is how execution synthesis does
so this detective work that I mentioned
what we did was we used backward static
analysis to help the forward symbolic
execution stay within sort of the bounds
of what's strictly necessary we found an
execution and we played it back using
this shim library and this playback is
hundred percent deterministic because
any source of external non determinism
is eliminated with this ship library
well does it work we built a prototype
that works today mostly for C programs
but there's no technical reason why it
cannot work for other kinds of programs
and here are some of the results we we
like to work on real system so we looked
at a few of them sequel light is an
embedded database that is used in a
variety of products how can L is a
networking library ght PD is a web
server and then these are a bunch of
unix utilities and then we went out and
looked for actual bugs reported against
these real systems and i said well can
we take the bug report and actually come
up with an explanation automatically oh
i should also mention that we want it to
span a wide range of sizes so this
actually spans three orders of magnitude
sequel light is about a hundred thousand
lines of code and it goes all the way
down to you know some of these little
uniques utilities there are a few
hundreds of lines of code so we'll see
how this effects running time and so on
the kind of bugs we found we focused on
the ones that felt a little more
interesting was for instance this one in
sequel light was the deadlock but that I
showed you earlier so it results in a
hand this is another deadlock bugging
Hawkin l this one's actually was a
security vulnerability was a buffer
overflow that would eventually call
the web server to crash and then most of
these are the segfault type of type of
bugs and so we're an ESD or prototype on
these and the execution synthesis time
was was pretty fast it the longest one
took on the order of three minutes and
the reason it's fast is because the
heuristics really help the search so
this is not a guarantee that for all the
bugs it will always be this fast because
these are heuristics so there are corner
cases where it might take longer we we
are actually our cells are trying to
understand a little better how these
heuristics work but for these bugs it
worked out quite well but another thing
I should point out is that it is for
these kind of tools it's quite important
that they work for shared libraries as
well not just for standalone programs
but for instance equal light I mention
it's an embedded database so this is
something that is used in Firefox it's
used in the iphone it's used in Mac OS X
it's used in nokia symbian OS using
skype just all over the place so all of
these programs out there might deadlock
if they tickle that bug that i just
showed so you fix a bug in a shared
library and then you have to impact on a
much wider range of programs now we
researchers one of the things we do is
we compared to some of the existing
state of the art and also my my
practical mind says like well the state
of the art is some super duper
programmer well hard to compare to that
and probably would win I mean it's
hardly know in a few minutes to it takes
few minutes just to read the bug report
but what we there are no tools to my
knowledge there are no tools that can
take a bug report and figured an
explanation for you but what we did was
we took two research tools that were
recently published one or two years ago
one is the CLE symbolic execution engine
which is used as a bug finding tool
and the chats bug finding tool which so
CLE doesn't work for multi-threaded
program and chess is a tool that finds
bugs in multi-threaded programs so we
kind of put these two things together to
build this this super tool that finds
you know looks for bugs in programs so
we compared to that now this is not a
fair comparison because theyre bug
finding tools so they're not really
focused on finding a path to a certain
place but as i said there's no tool that
i know that does that so this is the
closest we could come and by doing the
comparison will actually see what the
benefit is of these these optimizations
that we put in so this is a graph that
shows along the y-axis on a log scale
you see that time the path synthesis
time basically the time it takes to find
a path in the case of ESD it makes sense
to call it synthesis in the case of the
other tools not necessary so and then we
ran it on the various bugs that I showed
you earlier and in blue you see ESD and
the red and the yellow are or green
whatever that color is are two different
strategies for this super tool that I
was referring to that was the
combination of clean chess okay so we
want to see how the search strategy
influences things and so we tried two
different sorry one is plain DFS and the
other one is a random path which is
actually the best performing one in the
original systems and what you see here
is that in one hour which is what we
have up here this tool the super tool
was unable to find the path I mean
something it's not really surprising
because the number of tasks is so great
so it ran for an hour it still hadn't
found a path and then by comparison ESD
was able to find the path as we've seen
earlier in three minutes or less but
then we said well okay we've got to give
this tool a chance the super tool a
chance so what we did was
took the LS utility which is a small
units utility and we introduced
synthetic bugs we actually put in some
null pointer dereferences in a bunch of
places and that was findable by the this
tool is Clete chess combination and yes
d also found those bugs luckily and but
it did so in an order of magnitude less
time so the these heuristics and the
static analysis that helps guide the
search it does actually pay off so to
summarize I've shown you how we take a
bug report fitted into this execution
synthesis story use a combination of
static analysis and symbolic execution
to produce an explanation which consists
of a code path the program inputs and
thread schedule and this can then be fed
into your classic debugger and replay
and remember the key inside that allowed
us to do this is the realization that we
don't have to replay the execution that
occurred in the field but just replay
the bug and any execution that evidence
is the bug will do so we cast the
debugging problem as a search problem
for this B and C and this technique is
really one of the this is behind one of
the services that I was talking to you
about earlier the automated debugging
which is targeted at developers now
that's so we're getting to the end of my
talk in closing I want to emphasize that
we really think that if we are to reduce
bug density by an order of magnitude
which is what the goal was that I
mentioned at the beginning we really
need to engage both technical and
non-technical forces so on the on the
one hand we must develop techniques that
automate testing and debugging such as
the one that I showed you today and make
them work for real
not just for twat software and on the
other hand we really need to find ways
to enable free markets to promote
competition based on reliability because
otherwise well you know who will really
care I think that economies of scale
allow us to make automated software
testing inexpensive and accessible to
everyone if we provide these other
service you realize there's a lot a lot
of redundancy in testing that can be
taken advantage of to actually make this
a feasible proposition and then if we
drive the cost of testing down that will
level the playing field and allow even
the smallest programming teams to match
the testing resources like the ones that
you have here you know your massive
clusters vast amounts of very smart
people even the little guys can have
access to that kind of test quality so
maybe in this way well one day say
goodbye to this or faith-based approach
to software reliability and one the day
we do that I'll be a happy man so thank
you for your attention and I will be
more than happy to take questions so
good idea cluster the people yeah
I have a question I have a guest in
Georgia someone else as an ass asking
one already go ahead okay so it seems
like you're you're running this on kind
of native Native compiled binary so if
you have you any experience of doing
this with web applications running
inside a browser and what that would
take we haven't looked at that yet
that's a kirkland or I guess so I I can
speculate that I actually think that it
may be easier from the point of view of
the technique because having access to
the actual code you know if you're if
we're going to do this on a JavaScript
type of web app then that would probably
work you know we'd have a lot richer
information that we can leverage in this
kind of analysis but at the same time
and I was sharing this with some people
was meeting earlier today we've had to
do tremendous amounts of engineering to
actually get this to work on real code I
mean it is you know there's all these
little things that you have to take care
of which are not too pure research they
are just stuff to get it to work so I'm
saying this has a caveat to my answer
for JavaScript because who knows what
would have to solve once we you know
take a look at this kind of web apps but
at least from a conceptual point of view
it seems that it would be advantageous
to try those out we haven't yet but
hopefully we will okay thanks George hey
we're getting thrown out of our room up
here so we're going to have to sign off
okay play it safe thank you
yes yes so the question is whether the
work we've done has taught us anything
has given us any insight into what
developers could do to make the code
more debuggable or easier to test or
anything that is that or to make our our
techniques work better with their code
and I think that is a great question
there are many things that can be done
and the truth is that the I mean I'm not
a programming languages person or a
static analysis person I I hail from an
operating systems background but as I
understand the the community has
introduced many clever ways for instance
to annotate one's code that would help
this kind of tools you know clearly if
you don't use pointers both things would
be so much easier you don't have to
worry about pointer aliasing so there
are these there's a lot of work out
there I don't I can't even represented
accurately in you know two minutes but
what we've tried to do is of push it to
the extreme and say well what if we are
to maximize the productivity of the
programmer and say that well you the
programer don't have to do anything just
have to think about how you write your
code and you know just write the best
code you can and let the tools to the
rest I think the most practical
solutions are probably somewhere in
between you know where you do inform the
tools a little bit about what it is that
is happening or at least you know you
provide invariants you can you know the
more assert statements you put in the
code the easier it is for the tools to
you know understand okay so if if I
there's no way I could have gone down
this path because you know this assert
me I couldn't have been there so you had
all these things help there was I think
a question in the back yes
so the question is whether our
heuristics described in the in the case
of deadlocks with the heuristic where we
said we introduce preemption points
before and after mutex operations
whether this assumes in any way that
mutexes are used correctly is that
correct so the answer is no it doesn't
make that assumption because the correct
so by using new tactics correctly you
mean acquiring them in a certain order
or you know not doing double release
double acquisitions ah so that that's
good so in that case we talked about
what I would call a race condition where
because you have something that should
be a critical section not be protected
by mutexes then you actually may access
shared state in a way that you know
really depends on the scheduling the
output may be eunos of an uncontrolled
effect of whether this thing comes
before or after and the heuristic i
described works in the case of there
being no race conditions you have very
correctly point out that in the case of
race conditions this is problematic and
the tool actually does do a dynamic race
detection largely based on the eraser
algorithm whereby during the search it
looks for opportunities for races and if
there are parts in the code where there
could be a race it actually explores
those those alternatives as well so the
complete way to describe this would have
been to say that we introduced
preemption points before and after mutex
operations as well as in locations that
on the search path we actually find
opportunities for races so that's a way
to augment this and there are also if
you are looking for instance for a bug
that's caused by a race condition then
you can employ other algorithms for race
tection to to do that yes and that's a
wonderful question the question is
whether we can actually think of the bug
report as something that trims out a lot
of the false positives that static
analysis would find if we were to just
run the static analysis on the program
itself and so the bug report kind of
gives us clues to eliminate also
possibility that a static analyzer may
find that are not true possibilities is
that a correct representation of the
question okay so that's a wonderful
question because that's in some sense
exactly what happens if you do not have
so you could run the static analyzer
just as you know part of your testing
process and typically you will get a lot
of warnings now depend on which tool you
use how complete and sound it is it
cetera et cetera you typically get a
very large rate of false positives
especially the larger the code base is
use pointers things get really crazy and
the bug report tells you I'm only
interested in those say races that occur
on the path to this end state and I'm
not interested in any of the others that
you might find and so that by itself
because now it it sort of imposes a
restriction on the path through this
entire space that you're interested in
static analysis can be a lot more
focused the co static analysis aims to
be complete so he tries to reason about
all the paths but given an end-state
many of these paths cannot get there so
the bug report conceptually filters how
many of the false positives now how to
cross that conceptual bridge and
actually bring it into a tool that's
probably what we've done with execution
synthesis I think that's very perceptive
it's a way to really leverage that
information any other questions here or
in the ether yes a great question what
is the state of the art in industry with
respect to certifying software for
example Microsoft assigning various
pieces of software that it ships that go
undergo a certain certification process
so first of all let me say Microsoft
does have some extremely advanced tools
to do both static analysis and symbolic
execution some of the the greatest work
that we build on as well actually did
come out of Microsoft and Microsoft
Research and to to give you an example
let's talk about for instance the I
think it's called the windows hardware
certification program or something like
that where you actually you write a
device driver and then it's actually put
through a testing process at the end of
which if the drivers of emerges
successfully micra will put its stamp of
approval and when you install that
driver you will not get that pop-up
warning that I showed you and so that's
an example of this there has been very
interesting work in particular there's a
tool called slam which was published and
to about three years ago or so that uses
model checking to test device drivers
and at the time this was a very advanced
tool because it worked
or actual c-code actual driver code one
of the reasons for which our tool DDT
found bugs in Windows certified drivers
is that to make the approach that slam
has practical it does have to cut some
corners and those corners that you cut
are unexplored paths and to give you an
example slam will take the driver and
test it in isolation of the rest of the
system around it so the entire windows
kernel API has been modeled actually
manually people actually wrote models
for the various calls that can be
plugged into the model checker and these
models by necessity are incomplete
because Andy actually took three years
to write to model the entire API we do
not rely on modeling in the DDT tool we
actually run everything natively we're
on the drivers inside windows inside
this virtual machine so the technique is
slightly different but these tools are
very powerful I don't know how I know
what the statistics are about how many
bugs they do find but they're not
complete and I think the certification
is really not a guarantee it's not
something that says this software is
guaranteed to be free of bugs it just
says we put it through our stringent
test process and it's not the random
teenager who wrote the code that you
have to trust you have to trust our
Microsoft's test process and well as we
all know you know there's no perfect
testing so you know despite it being
very stringent there's still things that
fall through
so the following question is you know
don't want we always have to actually
trust the test suite in order to assert
that something is free of bugs and I
think that is true ultimately what a bug
is will have to be stated by someone the
kind of bug we looked for and that I
used in this talk are things that
everyone would agree are bad things I
mean who would argue that a deadlock is
an intended behavior well nobody you
know a null pointer dereference these
are bad things no one has to formulate
them formally no one has to argue about
whether they are not a bug so there
there's nothing to trust but ultimately
you will have to trust the definition of
a bug in the certification service that
I presented our intent is really to run
these automated tools and look in a
first stage for this kind of lower level
bugs and provide you guarantee that for
instance this device driver is free of
race conditions there are no race
condition well sir there's no deadlocks
there none of this none of that so these
are guarantees that the current tools
cannot provide they don't scale and
further down the road you remember those
this developer edition where you would
actually write you know you have your ID
and you're writing up your code and it's
testing it in the background one of the
things I didn't talk about is that I'd
like the ID to allow you as you write
the code to specify properties of
interest so while you're writing the
code you realize oh yeah I could write
an assert that you know check a certain
property now asserts are very local type
of properties but can imagine more
global properties about how many locks
are being held or how many threads are
running at a time and then if you had an
easy way to specify that you the
programmer that gets pushed to the
service and it can be checked and then
you can actually get a guarantee that
that property will hold along all paths
and this kind of guarantees amount to
formal proofs which are are still very
difficult to do and I
don't think the certification service
will be able to do it will be able to
provide formal proofs of this sort but
will be able to provide guarantees for
certain types of bugs for you know under
when the code operates under certain
conditions and so on so forth yes I see
so I will try to rephrase the question
and then you'll tell me if I'm going
down the right path so the question is
whether in the case of testing as a
service for consumers how would this
service be able to test for properties
that the consumers are interested in how
the consumers communicate these
properties to the service because for
the developers you know we just talked
about writing a search we talked about
you know the execution synthesis takes
it the goal well how do consumers do
that is that correct
the kind of properties to look for how
do we decide what properties to look for
just based on a search in the code good
question we haven't thought about going
from asserts to properties that are
looked for by the end user version of
testing at a service but what we
envisioned for that is something close
for now we can look for the kind of bugs
that everyone agrees upon than our bad
things and I also think that having
something like Wikipedia style or null
style contributions of such predicates
you know by people like yourselves we
might say oh you know here's something
that's really important for device
drivers not to violate you know there's
a particular type of Colonel call that
shouldn't be done after some other type
of callers on so I can imagine through
this open environment of contributing to
a database of properties that are
important in a first stage I think you
know there's there's just a small set of
properties that obviously you know are
still being violated so whilst we use of
eradicate those there will be more and
more sophisticated properties you could
put into this database and perhaps you
know these if they're say a commercial
entity that provides this testing as a
service they might tap into this open
database or maybe this database becomes
a competitive advantage you know they
say oh we have a better database than
the other company that offers this and
personally I'd like to see an open
database everyone can contribute to but
I think that would be one way to go
about it we haven't thought about
automated techniques I'm sure they exist
but you know I'll just speculate about
it at this point
anything else all right well thank you
all and thank you for the great question</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>