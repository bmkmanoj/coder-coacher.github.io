<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning and Inference for Hierarchically Split PCFGs | Coder Coacher - Coaching Coders</title><meta content="Learning and Inference for Hierarchically Split PCFGs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning and Inference for Hierarchically Split PCFGs</b></h2><h5 class="post__date">2008-02-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Y3YI2UjAgkw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is a Petrov and I'll be talking
after that anything with my adviser dang
time the goal is going to be to take
sentences and produce parse trees for
them and we'll assume that we have so we
want to do this for many reasons that I
don't need to explain to you and we'll
assume that we have a bunch of labeled
examples a tree bank that we'll start
from and then we'll try to learn a
context-free grammar and I will be
talking first about how to learn the
grammar and then the second part of the
talk is gonna be about if doing
efficient inference so the problem or
the challenge in designing a grammar for
parsing is that the categories in the
tree bank are too coarse to model the
complex underlying process for example
if you look at those two noun phrases
then we have in the sentence one is in
subject position and the other one in
object position and they have very
different distributions so in order to
design a grammar that captures these
phenomena you need to refine the
observed categories and typically this
has been done in kind of manually by
either doing structural notation form of
parent imitation for example if you know
that the parent of this known phrase is
the sentence symbol then it's very
unlikely that this is going to be the
subject noun phrase and if the noun
phrase is under a verb phrase then it's
probably a object another way of
capturing this is by lexical icing the
grammar where you propagate information
from the bottom up in form of head words
this causes the grammar size to get much
bigger and you need sophisticated
smoothing techniques to deal with
sparsity in this work we'll try to do
something different where we say we have
K number of categories and we'll try to
learn automatically what these
categories are with the e/m algorithm
clustering them into hopefully
meaningful categories and we'll start
with the bare bones structure of an
x-bar grammar where we have just
observed categories in a tree bank and
we'll keep refining the categories in
our
in a hierarchical way allocating the
complexity only where we need it so
after his introduction the first part is
gonna be learning what we want to do
okay
there's some previous work in this my
adviser when he was a grad student at
Stanford and spent a bunch of time we
finding those gramm grammars first by
annotating them with structure and
information in form of parent and
sibling annotation and then manually
doing kind of an e/m type hill climbing
where he hand split the categories and
linguistically meaningful subcategories
and checked whether this helps actually
for parsing performance but after a
while the performance started to level
out because he we started running out of
ideas and this is a very tedious process
that has to be redone for every language
so it couldn't be scaled up to parsing
any arbitrary language then a few years
later and so starting from a naive tree
bank that is just right off of the tree
bank you can get up to 86% parsing
accuracy where state of the art is about
90 91 nowadays I need to do and then a
few years later there was some work on
automatically inducing these
subcategories where each category is
split into K different versions and the
algorithm is employed to learn those
categories but the problem with this
approach is that because every category
is split into the same time in the same
number of sub categories for binary
rules like this one the number of
parameters was cubic lis in the
cardinality of the latent variables and
so you quickly reach the limits of what
fits in memory and parsing is very slow
so what we would like to do but they
achieve the same performance roughly and
what we would like to do is take the
best thing of both for goals we would
like to allocate the splits only where
we need them but automatically learn
where to allocate them and the resulting
grammar should be compact and capture
many of the features that are present in
the data and everything should be done
automatically so that we don't need a
linguist to sit down and specify those
categories how are we gonna do this well
we'll start with these parts trees and
the first thing to note is that because
the brackets the whole structure is
observed and the base category is
already observed we don't need to run
the general inside-outside algorithm but
we can instead run a simplified version
that is more like the forward backward
algorithm for hmm we don't have to come
yeah one pass up and pass down computing
inside outside scores that are
restricted to conform with this observe
structure so that the algorithm is
linear in the number of words rather
than cubic and therefore very fast if we
do this we can start with a grammar that
has about 100 Cybercom categories and
then when we have two sub categories per
category we improve from 64 to 76 and
keep going
this is what previous worked it and at
some point we run out of memory and also
patience to run it so I when we have
split every category in 16 sub States
performance level Celtic about 86% and
our contributions will be 3 fault well
present hierarchical training scheme
that allows us to converge to a better
local optimum because en is a local
method and can
stuck in a suboptimal solution well do
the splitting in an adaptive way where
we allocate the splits only where needed
and finally to prevent overfitting we'll
smooth our parameters and this will lead
to state-of-the-art parsing performance
on a variety of languages so
yeah yes please yeah
so we the easiest thing you can do is to
just strip off the Hindu say you can
just do the therapy decoding and then
just erase the labels in the second part
I'll be talking about inference and it
turns out it's better to try to
marginalize out this certain annotation
you cannot do it exact because it's
actually complete but there's a
variational method that gives good
performance but I'll talk about it in
the second point so we're just yeah
we're just interested in recovering the
observed categories they're hidden
categories that we induce are just gonna
help us model this better but we're not
interested in that per se though they
are linguistic things are critical
micellar show okay so actually there's
the first example if we look at what
happens when we take a category the
determinant category these are the three
most likely words when the categories
unsplit and when we go to a grandma with
four categories we might end up with
something like that this is a real
example when we want to learn in grammar
with eight categories the way weaker she
do it in this naive version was to just
start from the same one and tried
inducing all the categories at once but
what is much better is to take the
category split it in two and then take
each of those categories split it into
bread add a little bit of random noise
and random algorithm initialize from
this previous local maximum
so this minimizes the number of training
iterations that you need to do and it
turns out that you converge to two
better local optimum in terms of log
likelihood and also in parsing accuracy
and what is nice about this is that the
splits that we learn they very often
emerge in the same way so here we have
learned that the determinant category in
fact has determiners but also
demonstrative and in there
linguistically somewhat
friend and they tend to occur in
different situations and this split
occurs always at first and then when we
go one step further we learn that there
is indefinite and definite determiners
and that there is quite a bit current
that the monstrous and quantificational
elements and those patterns are often
observed it the categories that we learn
are linguistically sensible sometimes
they're they're syntactically motivated
sometimes semantically often a mixture
of both so if we look at parsing
performance we see how at the beginning
both methods exactly the same but then
already with four subcategories because
we initially it's because position we
tend to converge to better local optima
and parsing performance increases by
about one percent when we do this
hierarchical training
but no matter whether we do this
training in a hierarchical way splitting
some categories is just silly for
example there's a common category and
there's no point in splitting it those
trivial examples can be captured by hand
but then they're okay overstating will
occur eventually for any category as
long as you keep splitting and for
example we can look at the determine
attack again what I've just zoomed in on
the left branch we have two indefinite
definite determiners and then when we go
to one when we do one more split round
we see that we end up with two
categories on the one side where we have
sentence initial and sentence me do
distinctions but then these two
categories look quite similar you might
say they are the same and this is
already obviously think we don't really
know what that along statistics are so
it's kind of hard to judge from a human
perspective and we don't want to have a
human in the loop so we would like to
have some way of splitting the
categories and evaluating how good the
split was because it is very hard to say
which category to split next we follow
this approach where we split everything
and then revisit it and merge it back
and so for this thing we might say well
these categories it
and contribute much we want to merge
them back but other categories might
might survive
so some animation okay yeah so we'll
split we'll look at the the split and
then merge it back this split on the
other side we might keep it
so who have rather than a complete split
tree will have this rugged tree and the
way that we'll evaluate how useful the
split was inspire simple likelihood
ratio where we compute the likelihood of
the data with the split reversed over
the likelihood of the data without the
split and we'll do a simple
approximation where we just treat we
compute the likelihood locally just for
a given node in a parse tree so that we
don't need to reparse the whole training
set and this gives us them a controlled
way of deciding how much how many of the
splits to keep we can sort this bliss by
their likelihood gain and reverse a
fraction of them and we found that if we
reverse half of the splits we don't lose
much likelihood and we don't lose
anything in parsing accuracy so we fix
it at 50% and didn't really experiment
more on the side
so when you contribution for a direction
when you have you level yeah so now we
have one possibility that you have one
bit and then you may decide which bit is
least we have some consequent work on
discriminative parsing where we treat
each split as independent feature and
there you can induce these features and
depending on the context we don't
explicitly merge them back but you might
just say well I want to have the feature
for the global one and for this category
I don't care about this particular split
and depending on the context you might
want to condition back on there are a
lot of features and in some other
context you might go back to the very
general category I haven't thought about
it too much in the in the generative
case what we did try was to kind of
enforce rather than just letting each
category kind of each thing be split
independently when you think about so
this our lexical categories because it's
easier to display but when you have
binary rules your split so to get binary
rules you take on Harry when you
binarize it and you have to spec bone
structure so we tried fixing particular
bits along the backbone because we
thought that we want to propagate
information along the whole backbone and
it turned out that restricting the
grammar in this way
performance it seems to be better to
just let the em do their own thing yeah
yeah so we take this and then we add to
the parameter and then we run we do it
on training data we could use data but
it seems to work fine with the training
data in fact we experimented with a
bunch of different ways of kind of
merging things back and it seems to be
quite robust make the same kind of
splits tend to merge if you merge too
much back you end up hurting your
parsing performance but then in the next
round you introduce the same spit again
and so you're kind of doing the same
work over and over again and so that's
why we fix it at 50% where we don't do
is any parsing performance
yeah well we just said well like this
category like 1 2 3 4 you can just
interpret them as like a binary but like
you have two binary features and then
you just say the first like the odd
features cannot change along the
backbone of the like offer of rules that
have resultant that that are coming from
binary seing a bigger yeah yeah and we
experimented also it had binarization
right left organization and it seemed
that just doing simple right branch Inc
was fine though the branching Direction
seems to matter a little bit for some
other languages ok so if you look at
parsing performance what happens is that
we don't lose anything in if we look at
number of like split rounds we don't
lose anything in person performance but
what we can do is that we we can now do
two more iterations of splitting so that
some categories get much more refined in
this improved parts in performance by
another percent however what you might
now get start start worrying about is
that at this point we split some
categories into up to 64 categories
subcarrier so we can look at it and for
the phrase of subcategories we see that
every category for each for every
category some splits have been reversed
so it seems like we're reaching a point
where performance is calm level of even
if we did more splits and also what was
interesting to notice that the
complexity has been allocated in a
reasonable way categories that are
frequent and have a lot of structure
have been split a lot while other things
that are either rare
or don't have a lot of variation because
they always produce the same thing have
not been split much and in the lexical
categories we can also see that the most
heavily split categories are the open
classes of linking nouns and adjectives
and in fact the proper nouns have you
split them to succeed three
subcategories so even there there was a
merge no there's no this automatically
learned that commas should not be split
because they always produce the same
thing there's no gain in likelihood if
you split them but and in in work that
I've done since then we have actually
scaled it up to do more and one or two
more split rounds and performance has
leveled off so it seems like we've
extracted as much of the data after yeah
structure as there is for this type of
model yes I'll show some lexical
examples and there is a bunch of
examples in the paper about kind of
structures that how you learn how to
propagate parent information but they're
kind of hard to analyze because there's
so many things going on in the grammar
so we we have a small interface where
you can click through parse trees and
rules to see how things propagate but
it's kind of hard to analyze but what I
wanted to allude to is that one problem
that this model has is we're splitting
we're taking these observed categories
and splitting them in a large number of
sub categories so this heavy splitting
can lead to overfitting because now we
have these 64 or 63 in reality
independent categories that are treated
as just completely separate symbols of
the grammar there's nothing saying that
all these things aren't proper nouns in
the model so one way of doing tying them
together and
sharing statistics is to do what we call
parents moving where the parent is now
not the parent in the tree but the
unsplit category and we experimented
with different ways of doing this but it
turned out that it doesn't matter how
you smooth it's important that you
smooth so a simple linear smoothing
where you just have one parameter which
is set to zero point one and says that
you believe to what your statistics say
to ninety percent and the rest comes
from your siblings and we do that after
you count of VM we computing you maximum
not forget parameters and smooth them
towards the parent gives a lot more
robustness so when we look at the result
well so yes I mean in the room so
overall five so what we do is we we just
look at the parent of the rule so for a
fixed parent eggs that's like a thanks
is the subcategory we take all the
siblings of the like four particular
configuration of these we take all the
other combinations so they're tied to
their to the head of the rule so all
rules that have like there's a wireless
you here like there's subcategories for
the B and the C but so there will be
yeah so for all those rules that have a
X on the right on the left hand side and
some type of B and some type of seat
there will be smoother together so the
rules so if we have a 0 goes to V 0 C 0
this will be smooth it with all rules
that go a 0 goes to be x cy they will be
put together like the parent is kept
fixed and then all the siblings
like that maybe I've convinced that it's
far yeah so because so I what I want to
do this so this is less specialized yeah
but so I mean what we could do is we
could smooth towards the kind of
completely unsplit rule yeah and I don't
think we try that we we always so fun
for the lexical categories there's no
difference because it's just one
direction and then for you never in
binary productions that would make a
difference so we could average out the
whole thing but we always if this could
be better
yes oh that's what we were trying to
rebel together the two always now we're
just doing this but the idea is that
when you so you're trying to build a
constituent over something so if you
just smooth over like the left-hand-side
kind of when you compute the score of
the left-hand side of the parent like
this moving will like will have taken
effect so when you combine that with
something else like it's already the
parameter for this Constitution is
already gonna be smooth like you don't
need make otherwise if you started
smoothing more you you would start
making double smoothing like n p0 and p1
and then you have like different rules
yeah but yeah once you build this you
always treat them like use this in
something else you always use them as
MPs no no no we keep them as n p0 so
maybe I can write on the board
so when we start up npep
they'll be some parameter P this is the
inspect rule that is in the tree Bank so
what we create is then at 0 goes to NP 0
v p 0 and then 0 o stuff and p 0 1 0
kind of all combinations and so on there
we'll so in the first split when the
categories are split in two we split the
rule into eight things and when we apply
the smoothing we will smooth those four
and those four together and where we say
well if you want to produce an S 0 it is
important what type of NP didn t P it is
but it is not like there should be some
I mean the smoothing kind of makes them
a little bit less different so you're
more tolerant like you're willing to you
when you were yeah when you construct
this and sewer from NP s and V piece you
want to kind of share statistics across
the right hand sides so you care about
what particular parent you have but
you're a little bit flexible on the
right hand side because you will know
that when you have a lot of splits you
will be fitting nice and over fitting
your data so and then that's why we just
split on the left hand side because we
say well the next time wherever s 0 is
on the right hand side in some rule it's
gonna get smooth it there because we
apply it always to the right hand side
so we don't need to smooth like we could
smooth the whole thing together but this
way we kind of have a little bit less
little bit more control
yeah yeah yeah I mean yeah we just do it
in a linear way right start off with
something that's up someone and the next
time it's going to someone about and if
we look at performance we see that at
the beginning when things are when you
have just two sub categories and things
are truly distinct something actually
hurts you a little bit because you're
just trying to separate them and put
something you put the things back
together but then towards the end you
you gain 1% and accuracy in a bit and
yeah so at the time of the paper we
couldn't do more splits and it looked
like the gap is getting bigger and we
can go even further we scaled it up and
unfortunately just leveled off but it is
also interesting to see four other tree
banks so we have run it on other
languages since then where you have less
training data so this is for English
where you have a million words for
German and Chinese you have 300,000 or
500,000 words performance actually drops
off here and if you don't do smoothing
it actually starts dropping because
you're overfitting
so this moving is is crucial but doesn't
yeah we try different ways and it just
spilling your way it works fine there is
I think in the current version there's
two there's one for the rules and one
for the lexical productions but at the
beginning I think these numbers are with
just one like we try to give us a few
parameters
no it's it's that's manual just like we
try point zero one point one and point
two and point one more fine and we fix
it and left it so some examples of what
is being learned we see for proper nouns
we observe things like months first
names middle names last names then what
goes first and second and make and
compounds for the personal pronouns we
learn casing we we have some nominative
and some X it is from personal pronouns
yeah it's just it's you can argue that
it's just yeah
how useful this is but it it's nice to
see that the things that you learn on
actually interpretable and we stick the
meaningful and when you look at the
rules you see that many of the things
that my adviser did as a grad student by
like manually splitting things in
linguistically motivated voice they
emerge naturally and I'm yeah I'm more
of a computer scientist than an English
sir yes that was good
okay so now that we have talked about
learning we should think a little bit
about inference because one thing that I
omitted because it was not really
favorable is that parsing the
developments that took about a minute
per sentence and so if you want to do
parsing in any type of application this
is useless so we need to think about how
to make it faster and the problem is
that even though the grammars are small
in the number of categories because the
number of parameters grows cubically
they're very densely connected and you
need to sum over many things for our
maximum many things with me when you're
doing inference so
we'll borrow some ideas so we find home
contrasted grammar introduced as a
result here with a grammar a treatment
they grabbed and produced base say with
the various columns of China what's this
so I think it's roughly the same as the
comments and trainings because of the
lexicalization which makes things like
lots of parameters so if you need
smoothing but our grammar is just kind
of slower to parse with because you have
fewer constituents constituent types but
the same number of parameters so when
you want to construct a constituent you
need to loop over many things but in the
lexicalized grammar you know the head
word so you just have this one
particular constituent that can go there
and so that's the problem with this kind
of approach though in the meantime the
discriminative grammar said we have
their orders of magnitude smaller so it
yeah so this thing has around one to two
million parameters and the
discriminative ones have like 50,000
parameters only and have the same person
performance but that's not gonna be part
of the stock but I can talk about it
later
so we'll borrow some ideas from previous
work on to speed up parsing and prints
and the main idea is course to find
parsing where you take your tree bank
and you X estimate refined grammar for
example the previous work used
lexicalized grammars in our case these
grammars were we just split grammars and
then in addition to that you have a
coarse grammar that is much more compact
and faster parse with the only thing
that you then need to do is establish
some type of correspondence between the
symbols of these grammars for example
saying that we can just strip off the
lexicalization or strip off this hidden
annotation it's in our case and then
we'll parse with the coarse grammar
we'll compute the posterior likelihood
of constituents and prune away unlikely
concision so that when we parse with a
refined grammar we can skip over many
types so this is a pretty cure
it's not guaranteed to give us the
correct result according to our model
but in practice it works really well and
like you have a threshold that you need
to set for each so we have a global
threshold that we use for all pruning
passes and we set it by tuning it so
that on a development said that we parse
we don't make any parsing errors but we
don't know whether this means if we
don't make any particular assume that on
the real test set and what we do is we
go through the constituents computer
what's your probability and if we have
like we might say Q P so very unlikely
but in peace and VPS are likely
according to the course grammar so when
we go to the refined one we can just
skip over the refined versions of the QP
the quantity phrase so this idea if we
apply it allows us to cut down the first
thing time significantly
but we're still too slow for practical
applications however what we can do is
we can do it in a hierarchical way we
can instead of going from the courses
version to the most three five one we
can do several rounds of pruning we
start with the courses one and then
prune away one symbol then we go to a
Catalan programmer then has two
subcategories and there we can split
already over the skip over these symbols
but we need to compute the probabilities
for ap1 and ap2
we might say well in p2 and bp1 are not
very likely so in subsequent passes
everything that's a refined version of
those categories can be skipped over and
we can keep doing this until we have
reached the final parsing model where we
then do exhaust farthing but just using
those symbols one question that arises
is where to get the intermediate
grammars from one thing that comes
immediately to mind as well in training
we're producing them already we we're
splitting each mechanism iteratively
so we can just keep them around and use
them for pruning and in fact
is what it's by well again with the
threshold tune so if we don't make any
sector on it development set we are down
to 35 minutes now and this becomes
reasonable but we can do better and the
realization here is that when we train
things we initialize each round with the
previous grammar and had a bit of random
noise but then we let things drift
there is nothing constraining them to be
the the new subcategories to be
refinements of the previous category so
we initialize we add nice and then we
just keep doing a.m. and then we end up
with something where for example this
the word these there was the second
highest work here has now drifted and
become a subcategory here with the
dominant tag so if we use the previous
grammar for pruning we need to be more
careful and kind of with setting the
thresholds if we have grammars that are
approximations of our most refined
grammar then we could potentially prune
more and one way of obtaining these
grammars is by projecting back the most
refined one so we take our initial
grammar we do our learning procedure but
then instead of keeping the intermediate
grammars we throw them away and we
compute projected versions of
with most refined grammar that are as
close as possible given the smaller set
of parameters to the to the most refined
one and this will allow us to prune to
tighten the threshold pruning threshold
and four more so how can we do this well
to decide on the non-terminal set it is
very easy for example if we have a
grammar that has been split in two the
projected version one of the projected
versions will have several it can be for
example just ripping off all the delays
annotation what is trickier is how to
compute the parameters of this coarser
grammar and the problem is that we want
to take all these rules and map them to
a smaller set of rules
we cannot just add these parameters up
because we need to know kind of waiting
for the left-hand side we need to know
how frequent s1 is relative to s 2 so
that we can do a weighted combination
and the way to do this is by estimating
the grammar rather than from a tree bank
we can estimate it from an infinite tree
distribution that is induced by our most
refined grammar and it's looks kind of
complicated but what it boils down to is
that we need to instead of just counting
parameters since we don't have the real
grammar with interest induced
redistribution we need to compute
expectations over these rules and over
their left-hand sides and there is a
simple recursion where we start off
saying that the count of a symbol up to
depth 0 c0 so for an it's 1 if there's a
mole is two roots and 1 0 otherwise
and then at depth I plus 1 we can
recurse to taking accounts from the
previous depth and then kind of
expanding the tree using the rules in
our grammar and this gives us expected
counts for symbols up to certain depth
if you run this process for a few rounds
the fixed by these equations converged
pretty quickly and after 25 iterations
they're quite stable and in fact most
trees in the tree Bank have a depth of
the most 25 so it seems to have captured
this probability of how long the
sentences are so then we can use these
expectations or counts to compute the
true probabilities of the coarser
versions when we do this we can tighten
the threshold without making a searcher
and down to 15 minutes and this is 50
minutes for 1,500 sentences it's a
couple sentences per second and in fact
if we were willing to make a small error
of X 0.1 percent in parsing accuracy
which is like a
on three or four sentences we can cut
this down into like seven minutes which
is four or five sentences a second and
now becomes practical if we look where
the time is spent we see that now the
first pass with the course is Graham is
actually the most expensive one and if
we really wanted to find unit we could
compute even more abstract versions of
the original grandma where we conflate
the tops of categories and saying well
noun phrase and verb phrases are very
different in some sense but for a kind
of course structure there might be
sufficient if and behaves similarly and
compute even more attractive versions
and speak this more up but for now this
is what we do we can visualize this
process where we look at the bracket
posteriors black means that a
constitution spanning kind of over the
triangle it's underneath it is is likely
we're summing over the type of
constituents and so after the first pass
with the simplest grammar we see that
many things already ruled out in the
grammar says now there's it's impossible
to put something over there this span
particularly when there's Thomas
involved there a good hint of whether a
precision can span over this and then
when we do the pruning can a Harkaway
there should be a video this coming up
so we start and then we see how certain
regions of the list
the chart get pruned away and sparse to
fight and at the end the final chart is
extremely sparse there's some ambiguity
still preserved for example how this
phrase here is it that at which level it
attaches but most of the other things
have been already sorted out and so even
though the grammar is big with we have
kind of achieved this what the
lexicalization also does is that over a
certain span where relatively certain
about what type it is but not like
there's just some small variations
possible so that then we can do the
final pass where we do our variational
approximation and compute the best tree
yeah the chart is passed on and can I
expand that because you have more
variation but the nice thing is that
yeah many of the things don't need to be
expanded because they're already moved
off completely so in each subsequent
pass you don't need to do that much work
anymore there's a different so if you do
you could do the same one but I actually
find you on that so at each level you do
a slightly different one it seemed at
the beginning you need to be a bit more
generous because you don't want to the
grammar is just very bad so you don't
want to prune that much but then in
later stages you can be actually quite
aggressive because the grammars I'm good
enough to say one part that I'm not
going to be talking about too much as I
see because I thought I wouldn't have
time is what your question was what do
we do with this little annotation our
grant what we're interested in is parse
trees over these course categories but
what our grammar gives us are so-called
derivations over these little categories
and so there can be many derivations
that correspond to the same parse tree
and in practice we would like to sum out
the hidden variables but we cannot
because it is np-complete and there are
different things that you could do one
thing is to just do Viterbi parsing and
settle for the best derivation which
works okay but it's not the best thing
and you shouldn't probably do it because
the other things are simple a better
thing is to extract an N best list and
then compute the exact likelihood of
those trees and we rank them it helps a
little bit but the best thing is to that
we have found so far as to do kind of a
variational approximation to the
posterior distribution for this given
sentence so if you kind of construct a
sentence specific pcfg that gives you
probabilities over on these constituents
and in practice it just turns out to
running inside outside and then one
extra pass and that is very quick so
it's not slower but it makes about 1 to
1.5 percent difference and it's crucial
for getting a state-of-the-art
performance so you know results in terms
of efficiency we compare it to charniak
and Johnson's person which is arguably
the state-of-the-art parser in English
we are a little bit faster we are a
little bit more accurate and our purses
in Java rather than in C so it's if we
ported it to see it would be probably
even faster in terms of accuracy
compared to the generative component
without the ranking because the real
ranking can be applied to our stuff oh
yeah yeah yeah so it's kind of like the
maximizing
recall but yeah and so for English we're
doing better than the generative
component and then if we rank our trees
we end up with about the same
performance is they and that the thing
is that because we do so much pruning
our and best lists are not better than
theirs
since we have pruned away many of the
almost right trees and but one of the
beauties of this approach is that
there's nothing that's English specific
in here there were very few parameters
in in it and in fact we can just keep
them fixed and run the model on any
language it has a tree bank and the the
hardest part of this experiment was
getting the the tree banks and licenses
but then once we had them we ran it on
German and we all perform the previous
state of the art model by a lot mostly
because there has been just much less
work on German and on Chinese we also do
much better than the previous work we
have run it and in the mean time also on
French and I'm gonna run it on Arabic in
the next week and French is also better
so it's a seems to be a generic approach
that works well for for a variety of
languages that have very different
linguistic properties so some kind of
not future work is already in the past
but some work that we done in the last
year to extend this is to use the same
idea to learn structured models for
phone recognition you can start off with
simple phones and then instead of
splitting them manually into try phones
and clustering them you can just try to
induce the structure and it works fairly
well we have done a different version of
this where instead of doing the split
and merge heuristic we had a infinite
model with sparse prior and
unfortunately even though it looked nice
around paper and thin the math it is
slightly worse in performance but it's a
kind of a cleaner model and currently
I'm working on a discriminative version
which at the moment is as accurate but
an order of magnitude sparser so to
conclude I thought about splitting
merged learning of hidden structures and
I showed
Reiko training procedure that combined
allows us to do an adaptive splitting
and combined with parameter smoothing
gets very good accuracies and on the
inference side we talked about
projections of these models to coarser
versions to speed up the parsing and
about marginalizing out the hidden
structure I just had one slide on that
and it works for many languages so the
parser is on our webpage and if you want
to play with it we have two questions
that's my question and it's something
that I've been meaning to do I've done
only the extreme case of erasing
everything in the setting access
everywhere and trying to recover
bracketing structure the problem that I
ran in through there was that the
grammar is pro-pro very quickly since
they're cubic and the number of yeah
they're late in categories so and
because they were so densely connected
there were kind of too many things going
on at the same time and it converts to
just some very poor local maximum so I
think you don't need a sparse prior you
need to initialize it with a little bit
more information one thing that I wanted
to try to start kind of conflating
categories that are rare or similar and
see how much we could damage the grammar
without hurting parsing performance and
then later also what types of other
structures we can learn because I'm
interested in using this for machine
translation for example and I'm not sure
that the categories that linguists have
specified for the English tree Bank are
the optimal ones for machine translation
maybe we can learn some more useful ones
automatically
okay oh really
question is how much can you cripple the
prevent irritation I'm still oh you mean
just like put noise and not have well so
that's what I was just saying that I
have done only the extreme experiment of
raising everything but I would like to
see how much we can actually damage it
without losing parsing accuracy but
that's something that I need to do two
questions so first it always sort of
bothered me that you never learned
English by reading The Wall Street
Journal right so why not train your
model first on very short sentences with
extremely simple parse trees and then
from that grammar you know go to more
complicated sentences and try that sort
of an incremental training and see
whether things change at all or not wow
that's I mean there's work on kind of
self-training for parsing and the result
there was that if you take a parser
train it on the tree bank then parse a
bunch of sentences and use those as your
training data your parsing performance
suffers because you're kind of
reinforcing the errors that you make but
if you parse upon if you have a parsing
component and a rear-engine component
then you can parse external sentences
re-rank them and train your parts around
that data and then it actually helps
because you are then kind of having the
additional information from the rear
anchor that kind of feeds back into into
the parser but it's some work that
Charney acted it brown but you needed to
parse a lot of sentences like minions or
sentences but yeah kind of a
bootstrapping approach especially for
resource-poor languages where you don't
have that such a big tree bank there'll
be something interesting to explore
which was 3d within the realm of the
tree bank you could split the sentences
according to London train first on very
short sentences pick that grammar use it
um I don't think it would help much
I mean you might converge to a better
local optimum because I mean yeah mr.
local search but so going back to the
very question range so maybe what's
happening when you write you start with
all X's is that you just overwhelm I got
people sentences if you started with
small short sentences maybe you've made
a few splits like Oh a discovers num
nouns and verbs say all right yeah
without being overwhelmed by comprising
which they are embedded and then once
you've done that you can okay yeah if
yours if you're starting kind of if
you're trying to learn the categories in
the first place if you don't assume that
they're already observed then yeah I
think that might be a good way to go
does this have any promise so everything
here is the true bank is very
well-formed sentences sanitized text and
all that but you know the real world is
a little different and I'm wondering you
know whether this has any promise in
adopting the parser to some data that is
not so close to so one thing that we
want to do soon is in collaboration with
XE it was part speech data and to see
how it does there and whether we can
adapt it to syntactic language modeling
and infer speech and then I guess in the
fall when John Blitzer comes he's very
much interested in the main either
patient such as parsing biotech store
it's another thanks let's out of the
main because typically performance goes
down a lot but we haven't tried much in
that direction yet so you did the great
structure grammar right can you consider
doing similar thing for dependency I I
haven't done anything on dependency and
crime iris
I guess after philosophy my advisors
philosophies that you can extract
dependencies from a constituency parser
and in some early experiments the
dependencies extracted from the chariot
parts that we're better than the
state-of-the-art dependency parsers so
if you want dependencies just use this
and extract the dependency cell for it a
bold statement and dependency parsing is
typically much faster so that well I'm
not completely posited I just haven't
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>