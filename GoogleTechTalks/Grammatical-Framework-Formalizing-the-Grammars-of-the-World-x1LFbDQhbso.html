<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Grammatical Framework: Formalizing the Grammars of the World | Coder Coacher - Coaching Coders</title><meta content="Grammatical Framework: Formalizing the Grammars of the World - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Grammatical Framework: Formalizing the Grammars of the World</b></h2><h5 class="post__date">2016-09-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/x1LFbDQhbso" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everybody into this typo so it's my
pleasure today to introduce our new
round thumb so RN is a professor of
computer science Chalmers University of
Gothenburg this mission is to formalize
the grammars of the world and that is
what we will talk about today thanks you
and thanks everyone for joining thanks
Michele and you one for organizing it
it's great to be here and now I'm going
to tell you everything about the project
that has been going on for over 18 years
and I will not assume that you know
anything about it if you need advance so
this is my challenge today starting with
submission so it's of course a
paraphrase of googles mission of hope of
making all information available but
it's a little bit more specialized so
our mission is to formalize the grammars
of the world and make them available for
computer applications formalized means
that we present grammars as machine
readable code and there is a web page
it's an open source project and it has
been going on since 1998 when it started
that Xerox Research Center Europe in
Grenoble so grammars if you are inside
natural language processing so maybe you
know that nowadays grammars are seen as
something that requires a PhD in
linguistics and you have to work five
years still the result will be brittle
and therefore this is an obsolete
technology which has been replaced by
statistics where you only need Bachelor
in computer science you don't need to
work five years you just need to
eight five weeks till the training has
been true the result is robust and
therefore that's the state-of-the-art in
natural language processing so why still
grammars and now I will show some bit
provocative examples but everyone who
has been in a statistical especially
phrase based machine translation will
immediately know that these are genuine
examples that are hard to are still hard
even for the state-of-the-art in machine
translation the system that we all know
one-club so grammaticality is the first
thing that is difficult to get right so
if you know German you see that the
first translation is is fine but the
second one is not right because the
agreement from the noun to the
determiner doesn't extend across the
objective that lies in between so this
is a common problem with long distance
dependencies that effects for instance
in Graham based models but also many
other things has having to do with the
spare with the sparseness of data and
other things in the technology and there
are of course ways of of solving this
even in the statistical world but I will
show how at least recognize this is a
problem of grammar so now you might
think that this is a just a little
cosmetic thing to get the agreement
right here it isn't it doesn't disturb
our understanding the meaning and maybe
the languages will actually develop so
that such things will be cleaned out
because it's just a nuisance for the
learn or sub German for instance to keep
track of this but let's take the second
example which also has to do with the
long distance dependency so the first
one so Umbreon is the verb for kill and
if there is an object in between so then
this is fine but if the object is more
than one word so you will probably miss
the connection with in print and um and
then you get done
halation which is not correct maybe even
dangerous if you are for instance if you
are spying someone's mail and trying to
find potential threats for security
third one also related kovorix so this
is from my native language finish and
you have the word for beer in four of
the other sixteen forms that come from
the number case combinations and the
fourth one here actually this is not an
uncommon form at all but it is just not
what's not given in the data although
this is well known so if you have a
morphological analyzer you would of
course recognize this as a form of beer
so this is morphological coverage due to
sparseness of data and there's syntactic
coverage which is related but a bit more
kind of more difficult to solve maybe so
we like beer
we will not like beer that should be
it's just different tents of the same
sentence and then you form a question
with the same things and then you get
something which is completely yeah let's
say wrong precision here so is there
anyone who knows both Swedish and
Chinese this is an interesting pair
because you see even if you don't know
Swedish you'll see that these are two
different sentences and you have exactly
the same sentence in Chinese and what is
the relation so which one is is any of
those correct let's try the same example
into English ok still the same so maybe
this work in that doesn't really
contribute to the meaning at all but if
you try with another noun far instead of
more then you see that it actually you
might think that the
in place exactly the negation that's
what it is so you might when you see
this example so you might think that
that well the translation seems to be
correct but but here actually well what
you get here is an translation which
gives you an idea it is about your
mother and Swedish so it does convey an
idea but actually it says the opposite
with you it was supposed to me and we
still claim it that grammars are one way
of solving this so why so because we can
understand grammars we can understand
exactly what's going on in each
individual translation and also in the
program that makes the translation so
the models are written by humans and
they are human understandable and
readable you can inspect and understood
stand them you can debug them for
instance if for some reason the negation
is missing or a morphological form and
then you can recompile them and you
don't need to wait for five weeks but
you run the compiler for a few minutes
and the whole system will be up and
running again with the bug fixed so the
purpose for which we have been mainly
using this this for domain specific
tasks and this is a very and maybe less
problematic use for grammars so you deal
with the language which where you only
need to deal with hundreds or maybe
thousands of concepts that you name me
need to translate for instance for
instance that one's product catalog or
or something like that
very specific and you can give your
grammars so that you can reach rather
high precision and this is something
that we have been doing with with G F
for many years but this is quite
different from what what for instance
made the mainstream
machine translation system ours are
doing so it's something where a consumer
can take a web page and then throw it
into the system and then see what this
means in the users own language this
system has to comply with or cope with
millions of concepts but typically the
precision is somewhere below 50 if you
think in terms of the blur score for
instance and there is no system that can
combine high precision with high
coverage and we have been concentrating
on on high coverage sorry high precision
because we think this is something that
you can you can use in producer tasks if
the system is not meant for a consumer
who is browsing the web but for someone
who is publishing their information on
the web and want to get reliable
translations and I will later say if we
have time so we will go into the
consumer task in gif as well but first
let's show an example that producer
tasks so here is something that we did
in the digital grammars company so it's
a system that translates technical
descriptions of buildings from an
accessibility point of view and produces
the webpages and I could show many other
examples but that should be enough now
so that was with a grammar we can do
this in a in a productive way this kind
of domain adapted precise systems which
are very multilingual as I will also
show but the results can also be very
compact so we have a kind of a flagship
of the project is the G of offline
translator which is a mobile app where
you can translate among 15 languages and
it's available for both Android and iOS
it was originally for for Android and
the colors I will show later
they show the confidence we have when we
inspect how the translation was formed
so the level of analysis that you have
and all of this system is so compact
that it actually fits on about certified
megabytes with the 16 modules for 15
languages and I will say why it's 16 and
at least it so it used to be so that you
you have you can download Google or by
the translator offline but you get a
much bigger system onboard and in
practice it's not very easy to have all
of these languages onboard at the same
time so compactness is maybe for the
common people so it's the most the
clearest advantage of grammars because
they can compress the information so
much like orders of magnitude compared
to storing for instance that statistical
phrase tables or even neural networks
okay but this said I should also say
that this looks nice when with these
examples but on the average Google
Translate is still much better in terms
of general overall quality I will also
come back to that so grammars are good
we want them maybe you are convinced by
now so how we make them work we still
have this PhD and 10 10 5 years of work
and so on and I think this is very much
due to the way in which grammars were
viewed earlier so grammars were seen as
linguistic theories where you have a
huge definition of the grammatical
sentence in English or something like
that as Chomsky declared the program and
they are big monoliths worse and nothing
bad with that so we have some very
sophisticated grammars and formalisms
that are for that purpose but tf2
have a justification for existence at a
rather different starting point so it
was based on ideas from compiler
construction rather than linguistic
theory because compilers are of course a
success story for grammars all of the
compilers have a grammar of the
programming language something they
analyzed it content you get exactly the
right translation into machine code so
we viewed them more like not definitions
of grammaticality because that's I think
quite widely acknowledged to be brittle
and very difficult to achieve because
all grammars leak but the grammars are
more like transducers so they analyze
the text and then they produce something
and this was also in part inspired by
one of the well known grammar systems
namely Xerox finite state tool that many
of you might know so it was exactly made
in order to write transducers that do
some specific tasks with the language so
that's why the whole GF is not a
monolith grammar but it's a framework
for building grammars and I think there
are at least thousands of grammars that
have been built and for many languages
hundreds of grammars the other group of
kind of firm lessons learned the GF was
on the programming language side so we
see grabbers as programs and they should
have a proper modern programming
language so that for instance you are
not programmed with copy and paste and
maybe macros if you are very abstract
but you try to do something else you
don't I think I show this is that you
have functional programming which allows
a very compact and abstract way of
writing grammars there is static type
checking
instead of programmers discipline so
that you know that your grammar will
work
runtime and this also supports
distributed work so that grammars can be
built collaboratively with people who
are not all experts whereas
traditionally grammar writing was very
much expert PhD level work so another
thing which is kind of independent of
this is that gif was from the beginning
meant to be a multilingual grammar
formalism which is fits with this
transduction idea whereas traditionally
computational grammars were written as
monolingual grammars for one grammar at
a time although there are some quite
substantial grammar packages that have
parallel grammars but they are just
again written parallel by the
programmers having a certain discipline
so there is a book about us that you
might read in English or in Chinese so
what is the the result so we promise
that you can go to the gif cloud there
is an IDE for interactive developmental
environment for writing grammars in gif
and publishing the results on the cloud
as well if you want and you can get done
in minutes writing your first grammars
if you start learning gif more seriously
then you will spend some hours in
writing up what we call the food's
application grammar which has some
restauraunt phrases and this is not very
far from what we have to a commercial
application grammars like the one that I
thought about that I showed about the
accessibility database so you basically
write it for some days at least for
every new language going up in the units
of time so there is a full resource
grammar which is the one that gives you
the full morphological inflection
and a very comprehensive syntax
combinations in packs which typically
takes a few months and typically is
nowadays done as a master's thesis and
in between there is mini resource camera
which basically gives us a signature of
each language so that they can do useful
things with the language but not yet
quite in the full scale so the resource
camera was meant to be an library to be
used for these commercial application
grammars and other academic application
grammars and in recent years we thought
that well this could maybe the next step
would be a wide coverage grammar a kind
of grammar which you can use in the
consumer mode so that you can use it for
analyzing and generating all kinds of of
text and what is the expected unit of
time here I think this is you might
expect something here and this will not
be true because it turned out that we
can build a wide coverage grammar on top
of the full resource grammar in some
days by adding two things adding a large
lexicon which we usually have obtained
from from resources like word Nets or
Wiktionary and then adding some
robustness into this so we'll come back
to so as a result we have this grammar
grammatical framework community and on
different continents with more than
2,000 members I think and the coverage
of different kind of coverage in
different languages countries here is a
list of the languages that we are having
the resource camel ivory and in the
white color agent and there are some in
progress
and basically why this estimators are so
successful is that many people find this
a very rewarding kind of work - for
instance a computer science master
student who hasn't thought about
linguistics before but who likes
compilers and has learned them and likes
programming and then starts looking at
his or her native language and then
realizes wow this is very interesting so
a recent comment that I received is that
once I got into it was obviously a lot
of fun a challenging task which is also
rewarding and so grammars have been
written by undergraduate PhD students
professors translators hackers from many
fields of expertise I think in this
order of frequency so the most common
one is computer scientists and then
mathematicians and linguists we have at
summer schools every other year that
every time gather about 30 to 40 people
and we will have two summer schools next
year so to say the first time we will
have two summer schools so if you are
interested please join and now this is
was to be a technical talk so I should
tell you how it works and I said it's a
modeled by compilers so in particular
think about the multi source
multi-target compiler like GCC to some
extent the guinot compiler collection so
you can have different source languages
and different target languages and what
you have in between is the abstract
syntax that formalizes the essential
semantics that is common to these
languages and then most of the compiler
is written in the abstract syntax level
and then you have a front end on the
back end and just to give a little
example of how how the how this task is
so if you are compared compiling java to
java virtual machine you have to do
things like going from the expression on
the left to this bytecode on the right
and if you don't nothing about it so it
looks like a very difficult task and the
first thing you have to understand this
word alignments so which word in the
source language corresponds to which
word and the target language and you see
that here and maybe you already knew it
that Java Virtual Machine is the postfix
or a stack machine so that you first
have the operands 1 &amp;amp; 2 and then you
have the 3 and then you multiply 2 &amp;amp; 3
and then you add the plus and of course
when you the expressions grow more
complex these word alignments can become
more and more messy kind of long
distance and crossing and so on and how
you deal with this in a compiler so
that's something is called syntax
directed translation where you have an
abstract syntax tree which is common to
both of these notations and then you
know how to parse the expression on the
left to the abstract syntax tree and how
to generate the byte code from the
abstract syntax tree okay and how is the
here is the first bits of GF code so
here is how you write it
you have an abstract you have separate
abstract syntax and concrete syntax
definitions so in abstract syntax you
define functions so add is a two-place
function that takes two expressions and
and builds an expression and in the
concrete syntax you have linearization
rules which says how the abstract syntax
trees are linearized into code by
concatenation so you concatenate the
linearization of X with the plus sign
and then with Y and in GBM you first
have the first operand on the second
operand and and the byte for the
addition and this is the idea that we
use for natural language as well so we
call it compiling natural language we
have abstract syntax and then we when we
translate from Chinese to French then we
parse from Chinese to abstraction that
generate fringe and unlike in typical
compilers all of these translations are
reversible so that we can use it in both
directions so it's a decompiler at the
same time as the compiler so if you
think of the translation task which is
to translate from every language to
every other language so here you see why
it can be made so compact because we
just have the abstract syntax used as an
inter lingua so the next thing is are we
actually do this for natural language
and let's look at at the very kind of
the first rule that you learn if you are
learning syntax and linguistics which is
that you build a noun phrase you build a
sentence from a noun phrase mode and the
verb phrase we call this function the
predication and then you build the verb
phrase from a to place verb and it's
complement noun phrase that's called
complementation and the abstract syntax
3 port john loves mirror it comes out
like this so have predication applied to
John and the complementation to love and
marry so this is the abstract syntax for
john loves Mary in this kind of analysis
and then your challenge is how you again
in traditional linguistics so how you
fit this index to the different World
Order's subject-verb-object like an
English subject object verb like in
Latin or verb subject object like in in
Arabic and you see that the f16 tax 3
was the same for all of the languages
but the parse trees come out quite
different and the word alignment also
indicates this so I think we know how to
do SVO and sov because this is actually
the same thing as we did with Java and
JVM because SVO is the in fixed language
and sov is the postfix language so we
have a very similar linearization rules
as we had for in the compiler case
however to deal with VSO we have to
leave this so what we are doing here was
very close to what is known as
synchronic context-free grammars but now
in order to give the same abstract
syntax for natural languages it has
turned out to be too weak a model so we
need something a bit more powerful than
context-free grammars and we defer we
actually had two things which turn out
to be just one thing under the hood but
the main idea is that the linearization
does not only produce strings but
records so when we build the complement
the bird praise by complementation from
a verb and an object we do not put them
into a string what we store them
separately in a record and then when we
use it in predication then we can take
them apart and put them together with
that subject and VSO is done another
thing that we want to know is that
different languages have different
different the lists of inflectional
forms very different sizes so English
for instance has only five verb forms
and for regular verbs two of them merge
and this is how you write the inflection
table for the abstract syntax object
club which you might now think as the
semantic object for the verb or the word
stems of a blog in them in its meaning
that we use in john loves Mary in German
I count it just to generate 86 different
forms because it contains all the party
spoils that you might need so a very
different inflection table but still the
same abstract syntax word sense love for
libel and in Chinese you only need one
word so I and of course you wouldn't
like to build your interlingua abstract
syntax in such a way that it makes room
for 86 web forms because that would be
complete nonsense for for Chinese but
you just want to have the semantics in
there
sex index and by using these variables
in the concrete syntax in stead of just
strings you can make this possible
agreement Chinese is again very nice
here so if you say I love her it's a
variety and if you said she loves me
then it's a DA I whoa
okay you just permute over words but if
you do the same in English and you
actually no single word is retained but
you have to inflect all of the words
this is not a very rational language but
of course we don't want to change it but
we have to keep it the way it is so we
have to describe agreement in the
concrete syntax we don't need to make
the Chinese grammar more complicated
just because English has agreement so we
keep the abstract syntax is pret and
compo and we do that work in the English
concrete syntax so we have a table with
the nominative and accusative forms for
she and her and then we have the
agreement feature which tells you what
platform to choose something the
predication rules tells you how how the
subject agreement is passed to the verb
phrase and of course our language is
more complex than than English for
instance German so German has this basic
variable word order where the word order
is different in the main clause in
inverted clauses and in subordinate
clauses and we can also deal with this
in the common abstract syntax but doing
the work in the in the concrete syntax
by using these tables and records so
basically the main multilingual
abstraction is that we have the abstract
syntax which has a three structure
defined the consisted constituency and
the word census and then we have the
concrete syntax which s defines what the
words are what order they come in how
they inflict and how they agree and yet
sixteen bucks is the same for the
language this and the concrete syntax is
very so to make this happen we need as I
said something more powerful than
synchronous context-free grammars namely
we need parallel multiple context-free
grammars which are grammars over tuples
rather than strings p.m. C of T is a
very nice formalism so it's actually
it's a multi context-sensitive it's not
fully context-sensitive but it still has
a polynomial parsing and that's why it's
quite nice to use but it's a very low
level so it's very tedious to write BM
CFG grammars there are some approaches
where they are automatically draw
derived from from from corpora and we
have also worked with dos but basically
have made it easy to write parallel
multiple context-free grammars which are
quite can be quite subtle and quite
complicated but we solve this by using
functional programming static type
checking and the module system and all
of those are compiled away when you go
to the portable Rama format the PM CFG
binary that we actually can then put on
the phone I also mentioned type theory
here actually that's what we use in the
abstract syntax so it's for defining
different levels of semantic precision
and I will come back to that but so it's
not only pgf is actually it's a type
theory plus this parallel multiple
context-free grammar and did I have here
okay so we had this added expressive
power compared to synchronous
context-free grammars which I I think I
that was well covered so you have
inflection tables and records which are
then encoded as tuples in the PM CFG so
when you write your GF grammar it has
lots of modules some of which you write
yourself and some of which come from
standard libraries then your energy of
compiler which is a rather heavy program
written in house code running on a PC
nowadays maybe in the future you can run
it on your mobile phone but the the
target is to get something that you can
put on on your on your phone small
enough for that so that's how we use
grammar so one word so lonely we ship
the grammar product to to the Doozers
and in the form of the pgf the portable
gramophone so the resource camera
library the library that defines the
morphology and syntax for over 30
language seas it has the morphologies
usually for most languages it's
represented as so-called smart paradigms
where the idea is that you like you do
in grammar books so they tell you very
carefully what kinds of words suffixes
generate what kinds of exceptions from
the regular inflection or what paradigm
you use for a certain ending over for
words that have a certain endings so we
have this for the many parts of speech
and then you can build a lexicon by just
giving one form and this is both useful
manual and automatic lexicon building so
for instance we if we're taking
Wiktionary x' and they usually don't
have or not always they don't always
have morphological information so we can
infer most of that automatically by
using these mark smart paradigms more
precisely so depending on a language we
can infer up to 97% of the inflection
tables correctly from such just one form
and if we give another form then we are
always very close to 100 so it makes
lexicon building even manually but also
if you extract this from data so it
makes it feasible
then that was morphology and in syntax
we have combination functions and for
many users for instance we also of
course have the spread and Compal
functions but it can be more intuitive
to use this function that takes the
subject and the object at the same time
is the verb so this is one of the
functions in the resource grammar API
and now for instance if you take the
apply this to he and I and and the verb
on bring'em in German which you have
defined by using the paradigms then you
you will get an inflection table for the
clause which inflected in all different
tendencies and negations and word orders
and so on
tens of forms here and there is an API
documentation which we maintain by
generating it for all of the languages
included at for instance if you are
localizing a social media saying that
someone likes something then this is how
you do it for English and Swedish and
Italian and in Italian you moreover have
to switch the subject and the object but
that's no problem so here you have an
example of a semantic abstract syntax
concept like which is more abstract than
just saying that you have a sentence
with a subject number predicate so when
you have this it generates you all kinds
of possible uses of the like like which
in Italian is the verb be a chair with
different word orders and inflections
and so on so I wanted to show a little
bit more of the resource grammar now and
it's an incremental parser where you can
write sentences so that the next word is
predicted by the grammar so these are
like the fridge magnets which you might
know but they change dynamically
according to the grammar so these are
what I want is put here the
the black cat sees us okay and you can
you can continue with some some adverb
or some other things but now what I
wanted to show is that you can
illustrate the abstract syntax which
looks like this
you can see the word alignment which
look like this you can also see the
concrete syntax tree which we show in as
a dependency tree and now this is mapped
to Universal dependencies that you might
be familiar with can be useful or you
can also see the phrase structure tree
and then finally you can see the full
inflection tables of the sentences with
all their tenses and other variations so
this is what you can play with what it
shows about the things that we can do
with the grammar and how it works in a
multilingual way oh so now I would was
to yeah say about this abstract syntax
tree so it is language independent
ignoring morphology and world order
semantics its types your decal am
determined it's non lossy it can
generate the other trees and the surface
forms so basically you can see parse
trees or phrase lakhs of trees and
dependency trees is two different two
different projections of the have 16 tax
trees and there are algorithms how you
get the trees from the abstract syntax
trees so here is how you get the parse
tree so you start with with an abstract
syntax tree for the black cut sees us
and then you link it with french words
and then you get the parse tree and the
dependency tracy is a little bit more
interesting so if you have the abstract
syntax tree so the universal
dependencies here correspond to
different argument places so for
instance the bread vp you know that the
subject is in the first argument at the
head
in the second one you annotate a tree
and then you start can throw away the
abstract syntax functions and then you
follow links for each of the words
finding its head so that you get the
dependency tree as a projection from the
abstract syntax tree by throwing out
some things and you can do the same for
French as well so I don't go more into
this but we could say that there is a
lot of in common in that kind of
philosophy of gf and universal
dependencies namely that we want to have
the same a syntactic representation for
each line for many languages and this is
one of the main research directions that
we are doing now but this takes us to
the last part which is scaling up so I
said that we can reach precision with
lower coverage but actually we want to
reach higher coverage by sacrificing
some precision and we do this in an
internal way to keep the grammars
compact and manageable and highly
multilingual so if you know what what
triangle so this is an adaptation of
that to the Intel in while setting so
the Green translations use the semantics
in telling what the devil was syntactic
one and the red one is chunking inter
lingual
so for instance if you have my daughter
is hungry so in the semantic level this
is a predicate hungry which is then
applied to possessive of oil and
daughter on the syntactic level this has
an adjective out predication my daughter
and hungry and that the chunking level
it just has some chunks for each of the
words and if you translate from these
abstract syntax tree compositionality
to french french for instance in the
semantic translation you get a nice
translation because it actually doesn't
translate word the word or structure by
structure but it says that my daughter
has hunger whereas using an adjective as
in the yellow translation is a bit
awkward and then in the
direct translation you even lose the
agreement basically you use this for
different parts of the or different
sentences so the translator always tries
to find the best level of translation
and here are some examples of what you
do so for instance I think the jello
translation of here is particularly
strange because it's something that is
grammatically correct but it really
doesn't give you the right sense and I
will not show demo of this I would just
say that we are below state-of-the-art
statistical machine translation on the
white coverage segment of course we are
working on that at the same time as we
are of course also working on the other
levels and here is another little piece
of evaluation where we saw that in the
semantic level we can do actually quite
well bearers there is another evaluation
from a free text where we do much worse
than the state-of-the-art statistical
machine translation maybe the thing that
we find cool is that we can done this at
all with grammars and even and device
such as a mobile phone
here are some other blur scores from our
commercial case with accessibility
database so to come back to the
translation would say that that gif has
disadvantages of guaranteeing
grammatical correctness it has feedback
with trees and colors it's less
dependent on language data for instance
we have all of the inflection forms of
Finnish words although there is
certainly no corpus where they all occur
and the multilingual systems can be made
compact in size whereas we are way
behind in non compositional idioms and
in contextual disambiguation and these
are cases for hybrid methods where we
are also working on the future trends
are that we of course there is more and
more interest for this controlled
language or the producer
quality tasks like localization fixed
robots data-driven documentation
precision translation and also some
business so that's why we've started the
company and on the research side you
have this hybridization where you can
think of a gif driven a hybrid where you
have a better statistical disambiguation
than what we have by now and also
grammar writing automation by machine
learning and other methods or we also
hope that that people working in
statistical frameworks might be able to
exploit gif for instance there is an
idea of using a factored model on
abstract syntax which should be quite
general and then there is a with this
you the universal dependency so we have
used this for bootstrapping tree banks
for languages where there is not yet the
treatment because we can take them for
instance an english stream bank and just
translate that to another language tree
back so I will not show this example
what is still to accomplish is to
formalize the grammars of the world we
are constantly working on more language
is why the cover it's better quality and
then usability in other context and
framework the take-home points so gif is
for grammar engineering without tears
it has multilingual 'ti via abstract
syntax the model is compiling natural
language and an application is inter
lingual translation in particular by
domain adaptation to reach high quality
and we currently provide languages for a
little bit over 30 languages but this is
growing and most of this stuff can be
found on the grammatical framework
webpage thank you
yep cheering you started by showing some
examples where is that these two cool
approaches I think you imagine abstract
what types of problems the song hi how
some typical example is where you fail
yeah so I have prepared this I just took
a random text well first there are two
phrases that are very good but then I
took a random text from our course
webpages and you see all kinds of
problems here so one is lexical coverage
we didn't have the for any reason okay
so here is two problems in one so there
is there was something unknown and
therefore select was was treated as an
adjective erroneously and all of the
translation just goes completely but
this I thought is quite good but there
is one word missing from our lexicon
just randomly and again so you can see
there is on the bottom of this there is
try Google Translate and I can promise
you that it will be much better than
what we had so I mentioned that there is
one particular thing which is a
disambiguation where we do do not so
well but the other thing is is very much
kind of random things missing from here
and there in the lexicon ambiguity yes
so we deal with it by basically we are
we are let's have an example okay oh
this is a bit strange so you basically
have different syntax trees that gives
you different translations and here for
instance the U has three possible
translations in German we display them
all and we rank them statistically
initially but again if you are working
in a production task then you want the
user to be able to pick the right
translation so the last mile is
interactive so what was the first words
I hear the beginning of your question
yes some token yeah so basically we
technically do everything into tokens
like hombre Mian so we have two tokens
and then when they are glued like dusty
ramesha on bringt then there is an
analyzer that it organizes it so it's
it's a part of the grammar or it can be
a separate process we have used both
approaches yes right right so this is
actually what we do in Finnish but we
don't generate all of the force because
there are so many and to store them even
in a compact
with some compression would be expensive
so we actually divide them into tokens
and then we use those with some penalty
in the speed and maybe in ambiguity the
formalism as a way as a way to catch it
so you can do this by using the record
sent and put in different parts of the
of the sentence that may turn out to be
a part so you put them in different
fields in the record and then you
combine them so I think we have dealt
with with most of the classical examples
and give the most exercises the students
because they are it's very good to look
at those and make sure that we can deal
with them and that each new grammar
grammarian knows how to how to deal with
them
it is quite straightforward I would say
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>