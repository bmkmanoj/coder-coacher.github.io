<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Recent Developments in Deep Learning | Coder Coacher - Coaching Coders</title><meta content="Recent Developments in Deep Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Recent Developments in Deep Learning</b></h2><h5 class="post__date">2010-03-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VdIURAu1-aU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's my pleasure to introduce you
Anton who's a pioneer in machine
learning in neural nets and more
recently deep architectures and I think
that's gonna be the topic today okay so
I gave a talk here a couple of years ago
and the first ten minutes there will be
an overview of what I said there and
then I'll talk about the new stuff the
new stuff consists of a better learning
module and I'll show you it learns
better at all sorts of different things
like learning how images transform
learning how people walk and learning
object recognition so the basic learning
module consists of some variables that
represent things like pixels and these
will be binary variables for now some
variables that represent these are
latent variables so they're also going
to be binary and there's a bipartite
connectivity so these guys don't connect
to each other and that makes it very
easy if I give you the states of the
visible variables to infer the states of
the hidden variables they're all
independent given the visible variables
because it's an undirected graph and the
inference procedure just says the
probability of turning on hidden unit HJ
given this visible vector V is the
logistic function of the total input it
gets from the visible units so very
simple to infer the hidden variables I'm
given the hidden variables we can also
infer the visible variables very simply
and if we want to if we put some weights
on the connections and we want to know
what this model believes we can just go
backwards and forwards inferring all the
hidden variables in parallel then all
the visible ones do that for a long time
and then you'll see examples of the
kinds of things it likes to believe and
the aim of learning is going to be to
get it to like to believe the kinds of
things that actually happened so this
thing is governed by an energy function
that is given the weights on the
connections the energy of a visible plus
a vector is the sum over all connections
of the weight if both the visible and
hidden unit are active so when I pick
some of the future detector active you
add in the weight and if it's a big
positive weight that's low-energy which
is good so it's a happy Network
this has nice derivatives if you
differentiate respect to the weights you
get this product of the visible and
hidden activity and so that derivatives
going to show up a lot in the learning
because that derivative is how you
change the energy of a combined
configuration of visible in hidden units
the probability of a combined
configuration given the energy function
is e to the minus the energy of that
combined configuration normalized by the
partition function and if you want to
know the probability of a particular
visible vector you have to sum over all
the hidden vectors that might go with it
and that's the probability of a visible
vector if you want to change the weights
to make this probability higher you
always need to lower the energies of
combinations of a visible vector and a
hidden vector that would like to go with
it and raise the energies of all other
combinations so you decrease the
competition the correct maximum right
hood learning rule that is if I want to
change the weights so as to increase the
log probability that this network would
generate the vector V when I let it just
sort of fantasize the things it likes to
believe in is a nice simple form it's
just the difference of two correlations
so even though it depends on all the
other weights it shows up as this
difference of correlations and what you
do is you take your data you activate
the hidden units they're stochastic
binary units you then reconstruct a
great we can start to activate so this
is a Markov chain you run it for a long
time till you've forgotten where you
started and then you measure the
correlation there right with the
correlation here and what you're really
doing is saying by changing the weights
in proportion to that I'm lowering the
energy of this visible vector with
whatever hidden vector it shows and by
doing the opposite here I'm raising the
energy of things I fantasize and so what
I'm trying to do is believe in the data
and not believe in what the model
believes it eventually this correlation
will be the same as that one in which
case nothing I'll have them because
it'll believe in the data or hopefully
it turns out you can get a much quicker
learning algorithm where you just go on
down and up again then you take this
difference of correlations then
justifying that is hard but
main justification is it works and it's
quick
the reason these modules interesting the
main reason they're interesting is you
can stack them up that is for
complicated reasons I'm not going to go
into it works very well to train a
module then take the activities of the
feature detectors treat them as though
they were data and train another module
on top of that so the first module is
trying to model what's going on in the
pixels by using these feature detectors
and the future textures will tend to be
highly correlated the second model is
trying to model the correlations among
feature detectors and you can guarantee
that if you do that right every time you
go up a level you get a better model of
the data actually you can guarantee that
the first time you got a level for
further levels all you can guarantee is
that there's a bound on how good your
model of the data is and every time you
add another level that band improves if
you had it right having got this
guarantee that something good is
happening is we have more levels
we then violate all the conditions of
mathematics and just add more levels in
the sort of ad hoc way because we know
good things are going to happen and then
we justify it by the fact that good
things do happen this allows us to learn
many layers of features vectors entirely
unsupervised just to model a structure
in the data once we've done that you
can't get that accepted in a machine
learning conference because you have to
do discrimination to be accepted a
machine learning conference so once
you've done that you add some decision
units to the top and you learn the
connections discriminative leap between
the top layer features and the decision
units and then if you want you can go
back and fine-tune all of the
connections using back propagation that
overcomes the limit of back propagation
which is there's not much information in
the label and it can only learn on label
data these things can learn on large
amounts of unlabeled data after they've
learned then you add these units at the
top and back propagate from a small
amount of label data and that's not
designing the feature detectors anymore
as you probably know Google designing
feature detectors is the art of things
and you'd like to design feature
detectors based on what's in the data
not based on having to produce labeled
data
so the idea back propagation was
designed your feature detectors so
you're good at getting the right answer
the idea here is design your feature
detectors to be good at modeling
whatever's going on in the data once
you've done that just ever so slightly
fine-tune them so you're better getting
the right answer but don't try and use
the answer to design feature detectors
and yoshio Benji's lab has done lots of
work showing that this gives you better
minima than just doing back propagation
and what's more minimally and completely
different part of the space so just to
summarize this section I think this is
the most important slide in the talk
because it says what's wrong with nearly
all machine learning up till a few years
ago what people in machine learning
would try and do is learn the mapping
from an image to a label and that would
be a fine thing to do if you felt that
images and labels arose in the following
way the stuff and it gives rise to
images and then the images give rise to
the labels and given the image the
labels don't depend on the stuff but you
don't really believe that you only
believe that if you label something like
the parity of the pixels in the image
what you really believe is the stuff
that gives rise to images and then the
labels that go with images are because
of the stuff not because of the image so
there's a cow in a field and you say cow
now if I just say cow to you you don't
know whether the cow is brown or black
or upright or dead or far away if I show
you an image of the cow you know all
those things so this is a very high
bandwidth path this is a very low
bandwidth path and the right way to
associate labels with images is to first
learn to invert this high bandwidth path
and we can clearly do that because
vision works basically the first order
you look out there and you see things
and it's not like it might be a cow it
might be an elephant it might be a
lecture theatre basically you get it
right nearly all the time and so we can
invert that pathway having learned to do
that we can then learn what things are
called but you get the concept of a cow
not from the name but from seeing what's
going on in the world and that's what
we're doing and then later associating
the label
now I need to do one slight modification
to the basic module which is I had
binary units as the observables now we
want to have linear units with Gaussian
noise so we just changed the energy
function a bit and the energy now says
I've got a kind of parabolic containment
here each of these linear visible units
has a bias which is like his mean and it
would like to sit here I'm moving away
from that costs it in energy the
parabola is the negative log of a
Gaussian so costs it and then the input
that comes from the hidden units this is
just VI hjw IJ but the V's have to be
scaled by the standard deviation of the
Gaussian there the if I ask if I
differentiate that with respect to a
visible activity then what I get is H J
W IJ divided by the Sigma R I and that's
like an energy gradient and what the
visible unit does when you reconstruct
is it tries to compromise between
wanting to sit around here and wanting
to satisfy this energy gradient so it
goes to the place where these two
gradients are equal and opposite and you
have that's the most likely value and
then you have Gaussian noise around
there so with that small modification we
can now deal with real valued data with
binary latent variables and we have an
efficient learning algorithm that's an
approximation to maximum likelihood and
so we can apply it to something so
there's a nice speech recognition task
that's been well organized by the speech
people where there's an old database
called timet it's got a very well
defined task for phone recognition where
what you have to do is you're given a
short window speech you have to predict
the distribution the probability for the
central frame of the various different
phones actually each phone is modeled by
a three-state hmm sort of beginning
middle and end so you have to predict
for each frame is it is it the beginning
middle or end of each of the possible
phones
there's 183 of those things if you give
it a good distribution there that sort
of focuses on the right thing then all
the post-processing
will give you back where the phoneme
boundaries should be and what your phone
error rate is and that's all very
standard some people use try phone
models
we're using by phone models which isn't
quite as powerful um so now we can test
how good we are at taking 11 frames of
speech it's 10 milliseconds per frame
but each frames looking at like 25
milliseconds of speech and predicting
the phone of the middle frame we use the
standard speech representation which is
Mel capsule coefficients the 13 of those
then there differences in differences of
differences and we feed him into one of
these deep nets so here's your input 11
frames are 39 coefficients and then I
was away when the student did this and
he actually believed what I said so he
thought adding lots and lots of hidden
layers was a good idea I've stopped it
too but he added lots of hidden layers
all unsupervised
so all these green connections are
learned without any use of the labels he
used a bottleneck there so the number of
red connections will be relatively small
these are not these have to be learned
using discriminative information and now
you back propagate the correct answers
through this whole net for about a day
on a GPU board or a month on a core and
it does very well that is the best phone
error rate he got was 23% but the
important thing is whatever
configuration you used however many
hidden layers as long as were plenty and
whatever width and whether you use this
bottleneck or not he gets between 23 and
24 percent so it's very robust to the
exact details of how many layers and how
wide they are and the best previous
result on timet for things that didn't
use speaker adaptation was 24.4% and
that was a verging together lots of
models so this is good yep four million
weights so we're only training one two
three one two three we're training you
know about 20 million weights 20 million
weights is about 2% of a cubic
millimeter of cortex okay so this is a
tiny brain
probably all you need for her name
recognition they start with the
differences and double differences of
the MSE C's if you're going into a thing
that could learn to do that itself if
you want to do that's a very good
question because you asked that again at
the end it's an extremely good question
because the reason they put the
difference is in double differences so
they can model the data with a diagonal
covariance matrix we are diagonal
covariance model big and you can't model
the fact that over time two things tend
to be very much the same without
modeling covariances unless you actually
put the differences in through the data
and the model of differences directly so
it allows you to use a model that can't
cope with covariances later on we're
going to show your model at can cope
with covariances and then we're going to
do what dick Lyons always said you
should do which is throw away the market
full representation and use a better
representation of speech yeah you said
that to me last time I visited okay so
the new idea is to use a better kind of
module this module already works pretty
well right you know it does well at
phoneme recognition it does well at lots
of other things it can't model
multiplicative interactions very well it
can model anything and with enough
training data but it's not happy
modeling multiplies and multiplies it
all over the place I'll show you a bunch
of places where you need multiply so
here's the sort of main example of why
you need multiplies suppose I want to
from a high level description of an
object the name of the shape and its
pose its size position orientation
suppose I want to generate the parts of
an object and I want them to be related
correctly to each other I could use very
accurate top-down model that says
knowing the square knowing these post
parameters I generate each piece in
exactly the right position then we
require high bandwidth or I could be
sloppy and I could say I'm going to
generate this side and that sort of is a
representation of a distribution of
where this slide might be and I'll
generate corners and other sides and
they're all a bit sloppy and if I picked
one thing from each distribution it
wouldn't make a nice square but I could
also top down specify how
these things should be pieced together
in effect I can specify a Markov random
field that says what goes with what and
then I can clean this up knowing these
distributions and pick a square like
that of course I might sometimes pick us
where there's a slightly different
orientation or a slightly different size
but it'll be a nice clean square because
I know how things go together and so
that's a much more powerful kind of
generative model and that's what we want
to learn to do and so we're going to
need hidden units up here to specify
interactions between visible units here
as opposed to just specifying input to
visible units there's an analogy for
this which is if I'm an officer and
there's a bunch of soldiers and I want
them to stand in a square I could get
out my GPS and I could say soldier
number one stand at these GPS
coordinates and soldier number two
stoned at these GPS coordinates and if I
use enough digits I'll get a nice neat
rectangle or you could say soldier
number one stand roughly around here and
then soldier number two hold your arm
out and stand this distance from soldier
number one and that's a much better way
to get a neat rectangle it requires far
less communication so what you're doing
is you're downloading roughly where
people should stand and then how they
should relate to each other but you have
to specify the relations not just where
they should be that's what we'd like in
a powerful hierarchical generative model
so we're going to aim to get units in
one layer to say how units and the layer
below should laterally interact when
you're generating it's going to turn out
you don't need to worry about those
little lateral attractions when you're
recognizing when you're generating you
do to do that we're going to need things
called third order Boltzmann machines
which have three-way interactions
so Tara Sinofsky pointed out a long time
ago
that we have an energy function like
this where this was V and this was H but
these just binary variables and we could
perfectly well write down an energy
function like this where three things
interact and we have a three-way wait
and if you think about these three
things now k the state of K is acting
like a switch when K is on you
effectively have this wait between I and
J when K is off this wait disappears
and it happens every which way because
it's symmetric so using an energy
function like this we can allow one
thing to specify how two other things
should interact so each hidden unit can
specify a whole Markov random field over
the pixels if you want but that sort of
begins to make you worry because a
Markov random field has a lot of
parameters in it and if you start
counting indices here if you have n of
these and n of those none of those you
get n cubed of these parameters which is
rather a lot if you were willing to use
n cube parameters you can now make
networks that look like this suppose I
have two images and I want to model how
images transform over time and let's
suppose I'm just moving random dots
around have a pattern of round dots and
I translate it
well if I see that dot and I see that
dot that's some evidence for a
particular translation and so if I put a
big positive weight there this triangles
Minter ups and that big three-way weight
then when this and this are on they'll
say it's very good to have this guy on
that'll be a nice low energy state if I
also see this pair of dots
I'll get more votes than if this guy
should be on and I'll turn this guy on
if however this picks I went to here I
vote for this guy and if this pixel also
went to there but for this guy so these
guys are going to represent coherent
translations of the image and it's going
to be able to use these three-way
weights to take two images and extract
hidden units to represent the coherent
translation and also be able to take the
pre image and the translation and
compute which pixels should be on here
now what we're going to do is take that
basic model and we're going to factorize
it we're gonna say
I've got these three-way weights so I
got too many of them so I'm going to
represent each three-way weight as the
product of three two-way things I'm
going to introduce these factors and
each factor is going to have these these
this many parameters which is just per
factor it's just a linear number of
parameters if I have about n factors I
end up with only N squared of these
parameters weights and if you think
about how pixels transform in an image
they don't do random permutations it's
not this pixel cos n that one gives you
big cells do sort of consistent things
so I don't really need any key
parameters because I'm just trying to
model these fairly consistent
transformations which there's a limited
number I should be able to with many
less parameters and this is a way to do
so that's going to be our new energy
function leaving out the bias terms one
way of thinking about how I'm modeling a
weight is I want these this tensor of
three way weights if I take an outer
product of two vectors like this I'll
get a matrix that has Rank 1 if I take a
3 or outer product I'll get a tensor
that has Rank 1 and if I now add up a
bunch of tensors like that so each
factor now each F specifies a Rank 1
tensor by adding up a bunch of them I
can model any tensor I like if I use n
squared factors if I use any n factors I
can model nice regular tensors but I
can't model arbitrary permutations and
that's what we want if you are so how
does inference work now inference is
still very simple in this model so
here's a factor here's the weights
connecting it to say the preimage here's
the way it's connecting it to the post
image here's the weights connecting it
to the hidden units and to do inference
what I do is this suppose I only had
that one factor I would multiply the
pixels by these weights add all that up
so I get a sum at this vertex I do the
same K I get a sum at this vertex then I
multiply these two sums together to get
a message there I'm gonna sent to the
hidden
it's and as that message goes to the
hidden unit I multiply it by the weight
on that connection and so what the
hidden unit will see is this weight
times the product of these two sums and
that is the derivative of the energy
with respect to the state of this hidden
unit which is what it needs to know to
decide whether to be on or off he wants
to go into whatever state will lower the
energy and all the hidden units remain
independent even though I've got these
multiplies now so this is much better
than putting in another stochastic
binary unit here if I put a stochastic
binary unit in here the hidden units
would cease to be independent and
inference will get tough but this way
with a deterministic factor that's
taking a product of these two sums
inference remains easy the learning also
remains easy so this is the message that
goes from factor F to hidden unit age
and that message is the product that we
got at those two lower vertices the
product of the songs the in computer on
the pre image in the post image and the
way you learn the weight on the
connection from factory after hidden
unit h is by changing the weight so as
to lower the energy when you're looking
at data and raise the energy when you're
constructing things for the model or
just reconstructing things from the
hidden units you got from data and those
energy derivatives just looked like this
they're just the product of the state of
the hidden unit and the message that
goes to it when you're looking at data
and the state of the hidden unit and the
message goes to it when you're looking
at samples from the model or
reconstructions so it's still a nice
pairwise learning rule so everything is
pairwise still so you might fit it in a
brain now if we look at what one of
these factors does when I show random
dot patterns that translate then we can
look at the weights connecting it to the
preimage and that's a pattern of weights
where white is a big positive weight
blacks of ignatiev weight so that would
have learned a grating connecting it to
the preimage and this will have learned
a grating connecting it to the post
image and with a hundred factors i'll
show you what roland learned
so those 100 factors connecting these
are the receptive fields of factors in
the preimage and remember is looking at
translating dots and these are the
factors in the post image and you see
it's basically learned the Fourier basis
and it's learn to translate things by
about 90 degrees and that's a very good
way of handling translation
mathematicians say things like the
Fourier basis is the natural basis for
modeling translation I don't really know
what that means but this learnt the
Fourier basis so I'm happy if you give
the rotations it'll learn a different
basis so this is a basis it learns for
rotations you see it learns about yin
and yang here oops that's the basis for
rotations one other thing you could do
is train it just on single dot patterns
that are translating in a coherent way
and then test it on two overlaid dot
patterns that are translating in
different directions it's never seen
that before it's only been trained on
coherent motion but we're going to test
it on transpose called transparent
motion in order to see what it thinks
when we're training unsupervised there's
no labels anywhere we never tell it what
the motions are we need some we're
seeing what it's thinking so we add a
second hidden layer that looks at the
hidden units representing
transformations and is fairly sparse so
the units in that second hidden layer
will be tuned to particular directions
of motion and then to see what it's
thinking we take the directions those
units like weighted by how active those
units are and that'll tell you what
direction it thinks it's seeing now when
you show it transparent motion and you
look at those units in the second hidden
layer if the two motions are within
about 30 degrees it sees a single motion
at the average direction if they're
beyond about 30 degrees it sees two
different motions and what's more
they're repelled from each other that's
exactly what happens with people and so
this is exactly how the brain works
okay um there's gonna be a lot of that
kind of reasoning in this talk I'm gonna
go on to time series models now so we'd
like to model not just static images for
example we'd like to model video to be
and we're gonna try something a bit
simpler when people do time series
models you would nearly always like to
have a distributed nonlinear
representation but that's hard to learn
so people tend to do dumb things like
hidden Markov models or linear dynamical
systems which either give up on the
distributed or on the nonlinear but are
easy to do inference well we're going to
come up with this something that has the
distributed and the nonlinear and is
easy to do inference but the learning
algorithm isn't quite right but it's
good enough it's just an approximation
to make someone yet and the inference
also is ignoring the future and just
basing things on the past so here's a
basic module and this is with just
two-way interactions this is the
restrictive Boltzmann machine with
visible units and hidden units here are
the previous visible frames these are
all going to be linear units and so
these blue connections are conditioning
the current visible values on previous
observed values in a linear way so
that's called an autoregressive model
the hidden units here are going to be
binary hidden units they also
conditioned on the previous visible
frames and learning is easy in this
model what you do is you take your
observed data and then given the current
visible frame and given the previous
visible frames you get input to the
hidden units they're all independent
given the data so you can separately
decide what states they should be in
once you fix States for them you know
reconstruct the current frame using the
input you're getting from previous
frames and using the top-down input
you're getting from the hidden units
after reconstructing you then activate
the hidden units again then you take the
difference in the pair Y statistics with
data here and with reconstructions here
to learn these weights
and you take the difference on
activities of these guys we're data with
reconstructions to get a signal that you
can use to learn these weights or these
weights so it's learnings
straightforward and it just depends on
differences and you can learn a model
like this after you've learned it you
can generate from the model by taking
some previous frames these inputs the
conditioning inputs in effect fix the
biases of these to depend on the
previous frame so they're sort of
dynamic biases and then with these
biases fixed you just go backwards and
forwards for a while and then pick a
frame there and that's your next frame
you generated then you keep going so we
can generate from the model once it's
learned so we can see what it believes
sorry no we're going to give back more
steps in time I I just got lazy with the
powerful now one direction we could go
from here is to do high-level models
that is having learned this model where
these hidden units are all independent
given the data we could say well what I
done is I've turned visible frames into
hidden frames now and it turns out you
can get a better model if you take these
hidden frames a model what's going on
here and now you put in conditioning
connections between the hidden frames
and more hidden units that don't have
conditioning in it they don't interact
with other hidden units and you know in
this model then you can prove that if
you do this right then you'll get a
better model of the original sequences
or you'll improve a band on the model of
the original sequences so you can learn
lots of layers like that and when you
have more layers it generates better but
I'm going to go in a different direction
I'm going to show you how to do it with
3-way connections and we're going to
apply it to motion capture data so you
put reflective markers on the joint you
have lots of infrared cameras you figure
out where the joints are in space you
know the shape of the body so you go
backwards through that to figure out the
joint angles and then a frame of data is
going to consist of 50 numbers about 50
numbers which are joint angles and the
translations and rotations of the base
of the spine okay so imagine we've got
one of those mannequins you've seen art
shop windows
we got a pin stuck in the base of his
spine and we can move around and rotate
him using this pin and he can also
wiggle his legs and arms okay and what
we want him to do is as we move him
around we want him to wiggle his legs
and arms so his foot appears to be
stationary on the ground and he appears
to be walking and he better wiggle his
leg just right as we translate his
pelvis otherwise his foot will appear to
skid on the ground and we're going to
model him we can do our hierarchical
model like I just showed you or we can
do a 3d model like this where we
condition on six earlier frames here's a
current visible frame here's your basic
Boltzmann machine except that it's now
one of these 3-way things where these
are factors
and we have a one-of n style variable so
we have data and we tell it the style
when we're training it so that's sort of
semi-supervised it learns to convert
that one of n representation into a
bunch of real value features and then it
uses these real value features as one of
the inputs to a factor and what the
factors are really doing is saying these
real value features are modulating the
weight matrices that you use for
conditioning and also this weight matrix
that you use in your thoroughly
nonlinear model so these are modulating
an auto regressive model that's very
different from switching between Auto
regressive models it's much more
powerful so we're going to have data of
someone walking in various different
styles a style of walking
yes yeah yeah the weights on the
connections will tell you which frame is
coming from right in the earlier model
there were two blue lines their
different matrices and they have
different weights on it just skipped all
the weight right
you just yes we in other words there's
direct connections from all six previous
frames to the country for determining
the current frame well the were when you
were computing what the fifth frame is
doing right but when we're computing
this frame we have direct connections
okay so we're now going to train this
model it's relatively easy to train
especially on a GPU board and then we're
going to generate from it so we can see
sort of what it learned and we can judge
if it's doing well by whether the feet
slip on the ground
oh I
we'll get there here's a normal walk
maybe Vista willing okay so as generated
from the model he's deciding which
direction to turn in and he's deciding
you know he needs to make the outside
leg go further than the inside leg and
so on if we we have one model but if we
flip the style label to say gangly
teenager he definitely looks awkward
right we've all been there I think this
is a computer science student my main
reason for thinking that is if you ask
him to do a graceful walk it looks like
this and that's definitely c-3po you
know I think this was a student unlikely
but he's very good you can ask him talk
softly like a cat we're asking the model
at present right and the model looks
pretty much like the real data the real
data obviously the feet are planted
better but notice he can slow down and
speed up again
what are aggressive models can't do
things like that what our aggressive
models have a biggest eigenvalue that's
either bigger than one in which case
they explode or it's smaller than one in
which case they die and the way you keep
them alive is by keep you keep injecting
random noise so they stay alive and
that's like making a horse walk by
taking a dead horse and jiggling it it's
kind of it's not good
now he doesn't have any model of the
physics so in order to do these kinds of
stumbles I had to be stumbled similar to
that in the data but when he stopped and
which stumble he did when he's entirely
determining we could make him do a sexy
work B you're probably not interested in
that you want dinosaur to chickens
where's dinosaur to chicken oh no that's
dinosaur and chicken that's a blend well
maybe a switch he's got quite a lot of
foot scared s is probably a blend this
is doing a sexy walk and then you flip
the label to normal then you flip it
back to sexy it's never seen any
transitions but because on one model it
can do reasonable transitions
so you have these hundreds styled
variables you decouple those from the
one event style and just make up new
styles but yep yep now you can also give
it many more labels when you train it
you can give it speed stride length all
sorts of things then you can control it
very well okay so it can learn time
series at least 450 dimensional data and
obviously what want to do is apply that
to video but we haven't done that yet
except for some very simple cases
the last thing I'm going to show you is
the most complicated use of these three
models one way of thinking of it so
that's similar to the previous uses is
that we take an image you will make two
copies of it but they have to be the
same and then we insist the weights that
go from a factor to this copy are the
same as the weights to go from the
factor to this company so if i equals j
wi f equals WJ f inference is still easy
in fact the inference here will consist
of you take these pixels times these
weights to get a weighted sum and then
you square it because this is going to
be the same weighted sum so inference
consists of take a linear filter ax
square its output and send it by these
weights to the hidden units that's
exactly the model called the oriented
energy model
she was right kind of linear filter so
that's been proposed both by vision
people by a dulce and bergen a long time
ago in the 80s and by neuroscientists so
you're a scientist to try to take simple
cells I point vaguely of that and look
at what polynomial their output is of
their input and yang Dan at Berkeley
says it's between one point seven and
two point three and that's means two
right so this looks quite like models
that were proposed for quite different
reasons and it just drops out of taking
a threeway energy model and factorizing
it the advantage we have is that we have
a learning algorithm for all these
weights now when we have a generative
model
so now we can model covariances between
pixels and the reason that's good
is well here's one reason it's good
suppose I asked you to define a vertical
edge most people will say well a
vertical edge is something that's light
on this side and dark on that side
well let me know maybe it's light on
this side and dark on that side but you
know well it could be lighter P and dark
down there and darker P and light down
there okay oh it could be a texture inch
it's getting cold might actually be
disparity edge well the matter should be
motion this side no motion that side
that's a vertical edge too so vertical
edges are big assortment of things and
what all those statements have in common
is a vertical edge is something where
you shouldn't do horizontal
interpolation generally an image
horizontal interpolation works very well
a pixel is the average of its right and
left neighbors pretty accurately almost
all the time occasionally it breaks down
and the place it breaks down is where
there's a vertical edge so a real
abstract definition of a vertebral edge
is a break down of horizontal
interpolation and that's what our models
are going to do a hidden unit is going
to be putting in interpolation and it's
actually neither turn off so it's sort
of worth logic when that breaks down
it's going to turn off so one more thing
it is this if this hidden unit here is
on it puts in a weight between pixel I
in pics LJ that's equal to this weight
times this weight tons this weight okay
since these okay
so these are controlling effectively the
Markov random field between the pixels
so we can model covariances mostly
because the hidden units are creating
correlations between the visible units
reconstruction is now more difficult we
could reconstruct one image given the
other image like we did with motion but
if you want to reconstruct them both and
make them identical it gets to be harder
so we have to use a different method
called hybrid Monte Carlo essentially
you start where the data was and let it
wander away from where it was but
keeping both images the same and I'm not
going to go into hybrid Monte Carlo but
it works just fine for doing the
learning and the hybrid Monte Carlo is
used just to get the reconstructions
then the learning algorithm is just the
same as before
and what we're going to do is we're
going to have some hidden units that are
using these three-way interactions to
model covariance between pixels and
other hidden units is just modeling the
means and so we call for meaning
covariance we call this Maccabi M um
here's an example of what happens after
its learned on black and white images
here's an image patch here's its
reconstruction of the image patch if you
don't add noise which is very good from
the mean and covariance hidden units
here's the stochastic reconstruction
which is also pretty good but now we're
going to do something funny we're going
to take the activations of the
covariance units the things that are
modeling which pixels are the same as
which other pixels and we're going to
keep those but we're going to take the
activations of the mean unit so we're
going to throw those away and pretend
that the means for the pixels look like
this well let's take this one for us we
tell all the pixels have the same value
except these which much darker and it
now tries to make that information about
means fit in with this information right
covariances which is that these guys
should be the same but very different
from these guys and so it comes up with
a reconstruction that looks like that
where you see it's taking this dog -
blurred it across this region here if we
just give it four dots like that and the
covariance matrix we got from there
it'll blur those dots out to make an
image that looks quite like that one
so this is very like what's called the
kind of watercolor model of images where
you know about where the boundaries are
and you just sort of roughly sketching
the colors of the regions and it all
looks fine to us because we sort of
slave the color boundaries to the actual
where the edges are if you reverse the
colors of these it produces the reverse
image because the covariance doesn't
care at all about the signs of things if
you look at the filters at your lungs
the mean units which are for sort of
coloring in regions learn these blurry
filters and by taking some combination
of a few dozen of those you can make
more or less whatever colors you like
anywhere so they're very blur they're
smooth blurry and multicolored and you
can make roughly right colors the
covariance units learn something
completely different so these are what
the filters learn and you'll see that
those factors they learn high frequency
black and white edges and then a small
number of them turn into low frequency
color edges that are either red green or
yellow blue and what's more when you
make it from a topographic map using a
technique I'll describe on the next
slide you get this color blob this low
frequency color blob in with the low
frequency black and white filters and
that's just what you've seen a monkey's
brain pretty much if you're going to
among his brain you see these high
frequency filters whose orientation
changes smoothly as you go through the
cortex tangentially and you see these
low-frequency color blobs and most
neuroscientists thought that at least
must be innate what this is saying is no
just the structure of images is and the
idea of forming a topographic map is
enough to get this that doesn't mean
it's not an age it just means it doesn't
need to be so the way we get the
topographic map is by there's global
connectivity from the pixels to the
factors so the fact is really our
learning local filters and the local
filters start off colored and gradually
learn to be exactly black and white then
there's local connectivity between the
factors and the hidden units so one of
these hidden units will connect to a
little square of
factors and that induces a topography
here and the energy function is such
that when you turn off one of these
hidden units to say smoothness no longer
applies you pay a penalty and you'd
rather just pay the penalty once and so
two factors are going to come on at the
same time it's best to connect them to
the same hidden unit so you only pay the
penalty once and so that'll cause
similar factors to go to similar places
in here and we get a topographic map for
people who know about modeling images so
far as I know nobody has yet produced a
good model of patches of color images
that is a generative model that
generates stuff that looks like the real
data so here's a model that was learned
on sixteen by sixteen color images from
the Berkeley database and here's things
generated from the model and they look
pretty similar
now it's partly a trick is that the
color balance here is like the color
balancer and that makes you think
they're similar but it's partly real I
mean most of these are smooth patches of
roughly uniform color as are most of
these there's a few more of these are
smooth than those but you also get these
things where you get fairly sharp edges
so you get smoothness then a sharp edge
then more smoothness like you're doing
the real data you can get things like
corners here we're not quite there yet
but this is the best model areas of
patches of color images and it's cuz
it's modeling both the covariance and
the means so it's capable of saying
what's the same as what as well as what
the intensities are you can apply it for
doing recognition so this is a difficult
object recognition task where there's 80
million unlabeled training images not
all of these classes but of thousands
and thousands of classes that were
collected by people at MIT it's called
the tiny images database they're 32 by
32 color images but it's surprising what
you can see in a 32 by 32 color image
and since the biggest model we're going
to use has about 100 million connections
that's about point one of the cubic
memory of cortex in terms of the number
of parameters and so we have to somehow
give our computer model some way of
keeping up with the brain which has a
lot more hardware and so we do it by
giving a very small
we say suppose the input was only 32 by
32 maybe we can actually do something
reasonable there so as you'll see there
are a lot of variation if you look at
birds that's a close-up of the ostrich
this is a much more typical picture of a
bird and it's hard to tell the
difference between these ten categories
particularly things like deer and horse
we deliberately chose some very similar
categories like truck and car and deer
and horse people are pretty good at this
people won't make very many errors
that's partly because these were hand
labelled by people so that even even
people make some errors we only have
50,000 training examples 5,000 of each
class and 10,000 test examples because
we had to hand label them but we have a
lot of untrained unlabeled data so we
can do all this pre training on lots of
unlabeled data and then take our
covariance units and our mean units and
just try doing multinomial logistic
regression on top of those or maybe add
another hidden low and do it on top of
that
so what mark arena and Zeta actually did
since you've worked in Yan lakhan's lab
he actually took smaller patches and
under model and then strode them across
the image and replicated them so sort of
semi convolutional and then took the
hidden units of all of these little
patches and just concatenated them to
make a great big vector of 11,000 hidden
units which are both the means and the
and the covariances and then we're going
to use that as our features and see how
well we can do and we're going to
compare it with various other methods so
the sort of first comparison you just
take the pixels and do logistic
regression on the pixels decide on the
ten classes you get 36 percent right if
you take gist features which are
developed by 12 then people that MIT
which I meant to capture what's going on
in the image quite well but they're
fairly low dimensional you get 54
percent so they're much better than
pixels if you take a normal RB m which
has linear units with Gaussian noise as
input variables and then binary hidden
units
and then use those binary hidden units
to do classification you get 60% if you
use one of these our BMS with both the
units like these ones for doing the
means and then these units with the
three-way interactions for modeling
covariances you get 69% as long as you
use a lot of these factors and if you
then learn an extra hidden layer of
8,000 units so notice that times that's
100 million so the next 100 million
connections you learn there but that's
fine because it's unsupervised and you
just learn it on lots of data you get up
to 72 percent and that's the best
results so far on this database one
final thing you can take this model that
was developed for image patches and the
student who'd be doing phoneme
recognition just took that code and
applied it to log spectrograms
which is sort of not close to what dick
would like to see you're not using all
this more capital stuff which is
designed to throw away stuff you think
you don't need and get rid of lots of
correlations instead you're going to
take data that has lots of correlations
in but we got a model that can deal with
that stuff now and the first thing
George tried on February 20th which was
four layers of a thousand hidden units
on top of this got 22.7 current present
correct which was the record for phone
recognition on the timet database where
you're not trying to do urn model
adapted to each speaker and then a week
later when he'd adapted a bit and used
more frames he was down to 21.6% so this
all this stuff was designed to do vision
it wasn't designed to do phonemes and if
we treat phoneme recognition as just a
vision problem on the log spectrogram we
can wipe out the speech case at least on
small vocabulary another student is now
at Microsoft seeing if this will work on
big vocabulary as well yes yeah
we you know we can give them new better
tools we can give them new and better
tools
so here's phoneme recognition over the
years back prop from the 80s got twenty
six one one person correct over the next
twenty years or so they got that down to
twenty four point four percent using
methods that weren't neural inspired so
I call them artificial we've now got
down to twenty one point six percent an
estimate of human performance is about
fifteen percent
I don't know much about how they did
this estimate I'm afraid but we're about
we're nearly a third of the way from
artificial to real um so we need two
more ideas and with their ok I'm done
yeah
you've broken the world record
by simply using my seven layer deep
forward network with background GPU with
lots and lots of cycles yes he did
indeed announce that what he didn't
announce was his God a spectacular
result he gets down to 35 errors what he
didn't announce was there's two tricks
involved one trick is to use a big net
with lots of layers and a GPU board that
tree by itself won't give you 35 errors
there's a second trick which was sort of
pioneered by people of Microsoft in fact
which is to put a lot of work into
producing distortions of the data so you
have lots and lots of labeled data so
you take a labelled image of a2 and you
just store it in clever ways that make
it still look like a - but be translated
so people can get down to about 40
errors good
so dicks already patented that so you
get down to you can get ant about 40
errors by doing these distortions what
he did was even better distortions or
more of them and a much bigger net and a
GPU and got from 40 to 35 which isn't
impressive because it's hard to make any
progress there but it won't work unless
you have lots of labeled data and what's
the disguised thing is the work went
into if you look in the paper it's all
very straightforward it's just back prop
except when you get to the section of
how they generated all this extra label
data where there's very careful things
like if it's a 1 or a 7 they only rotate
it a certain number of degrees but if
it's something else they rotate it more
degrees I'm actually the referee for
this paper but I don't mind him knowing
I think it's very important work but he
should emphasize that they had to have
label data to do that and they had to
put work into distortions so for me the
lesson of that paper is when we had
small computers you should put your
effort into things like weight
constraints so you don't have too many
parameters because you only got a small
computer as computers get bigger and
faster you can transfer your effort from
instead of tying the weights together
like yam was doing in the early days put
your effort into generating more
distortions
so you can inject your prior knowledge
in the form of distortions and that's
much less computationally efficient but
with a big computer it's fine and it's
more flexible so I think that's the
lesson of that paper
seems like you've invented some kind of
cortex here that has the expected
property that it does vision it'll do
sound yep other problems you're going to
apply it to you um maybe be quicker to
say the problems we're not going to
apply it okay I can't think of any I
mean okay let me say what the main
limitation of this is for vision we've
got at least 10 billion neurons for
doing vision with or at least a billion
anyway probably 10 billion and even
though we got that many neurons and
about 10 to the 13 connections for doing
vision we still have a retina that's got
a very small fovea the size of my
thumbnail at arm's length and so we
still take almost everything and don't
look at it and the essence of vision is
not to look at almost everything
intelligently and that's why you get all
these funny illusions where you don't
see things we have to do that in these
models these models are completely crazy
and all the computer vision is
completely crazy almost all of it
because they take a uniform resolution
image and quite a big one like a
thousand by a thousand and they try and
deal with it all at once with filters
all over the image and if they're going
to do selection they either do it by
running of their face detector
everywhere with no intelligence or they
do sort of interest point detection at a
very low level to decide what to attend
to what we do is we fixate somewhere
then on the basis of what our retina
gives us with these big pixels around
the edges and small pixels in the middle
we sort of decide what we're seeing and
where to look next and by the second or
third fixation we're exciting very
intelligently and the essence of it is
that vision is sampling it's not
processing everything and that's
completely missing from what I said now
in order to do that you have to be able
to take what you saw and where you saw
it and combine them and that's a
multiplying so this module that can do
multiplies is very good at combining
what some wears to integrate information
over time and that's one of the things
we're working on now but that's probably
the biggest thing missing but that is an
example of how
module is quite good but now it's never
good enough so you have to put it
together over time and use it many times
and that's what sequential reasoning and
all this stuff are so basically as soon
as people become sequential we're not
modeling that at all
we're modeling what you can do in 100
milliseconds and so that's what's
missing but I believe that to model that
sequential stuff we need to understand
what is the sequence of is the sequence
of these very powerful operations and
we're in a better shape now to try and
model sequential AI than we were if we
didn't know what a primitive operation
is if he's thought of primitive
operation was just deciding where the
two symbols are the same we're going to
be out of luck for understanding how
people do sequential stuff
I know I'm still processing that people
look I'm making the point that people
find quantifies quite difficult I would
love to do that I have not got a clue
how to do it and you will notice that in
old fashioned AI that you used to point
out to neural net people but you can't
do qualifies so forget it
nowadays when they all do graphical
models they don't actually mention that
anymore because their graphical models
have difficulty with it to some people's
cut from all Zeus to Russell and people
do right yeah some people do but most of
the graphical models of like five years
ago don't do qualifies either and so a
pretty good division line would be what
you can do without having to deal with
really sophisticated problems like that
I would love to know how we deal with
that but I don't so yeah I'm gonna give
up on that right now in Timnath thats
what we haven't image all the examples
we have labels it still is a big wind to
do the pre-training well the earliest
mid Hoover hasn't tried with all his
distortions doing pre-training now I
have a student called Vinod Mehra who's
just produced a thesis where he tries
things like that he tries distortions
our own list and he uses special
distortions of his own and the fact is
distortions help a lot but if you do
pre-training that helps some more too
and Benji's results Yoshio Benji's
results suggests that pre-training will
get used to a different part of the
space even if you have all this label
data soak only one thing that needs to
be done is to try the pre-training in
combined with all these labels you don't
have to have the pre-training but i bet
you it still helps and I'll bet you it's
more efficient to it's faster because if
we change relatively fast you don't have
to learn a very good model yet and lots
of rich features and starting from there
I think you'll do better than he does
just starting from random and faster
that's just a prediction you might even
get down to 34 hours the problem with M
NIST is the error rates so low you can't
get significance Tim it is really nice
that way they designed it well so you
get high error rates so you can see
differences so I didn't understand the
question we have a limited time window
they cannot deal with they come model
house right
but if sort of what happened 15 times
steps ago really tells you what should
happen now and it only tells you what
should happen now it doesn't tell you
what's shamon in the intermediate 14
time steps it just contains information
across 15 time steps without having a
signature at smaller timescales
you can't pick up on that because it's
not got a hidden forward backward
algorithm a forward back around
potentially could pick up on that alert
actually can't not over a long time
scale mode unless you say that there's a
memory involved where you go back to a
previous owner gets more complicated
right now it is true that when you build
the multi-level one which you can do
with the three connections as well as
with the truer connections at every
level you're getting a bigger time span
because you get your time window so it's
going further back into the past of each
level so you get a bit like that but
that's just sort of linear
unlabeled data you need to train each of
the different levels and how it would
change like is it just linear with the
number of weights or as you go up levels
two things change okay so I have one
sort of important thing to say about
that which is that if you're modeling
high dimensional data and you're trying
to build an unsupervised model of the
data you need many less training
examples than you would have thought if
you're used to discriminative learning
when you're doing discriminative
learning there's typically very few bits
per training case to constrain the
parameters the amount of constraint you
get on your parameters for a training
case is the number of bits it takes to
specify the answer not the number of
bits to take specify the input so with n
lists you get three point three bits per
case if you're modeling the image the
number of bits per case is the number of
its it takes to specify the image which
is about a hundred bits so you need far
fewer cases per parameter another way of
saying it is your modeling much richer
things and so each case is giving you
much more information so actually we can
typically model many more parameters
than we have training cases and
discourage if people aren't used to that
many less parameters and we have pixels
many more than training cases and in
fact he you only used about two million
cases for doing the image stuff and it
wasn't enough it was overfitting he
should have used more but he was fitting
a hundred million parameters but the
basically the only rule of thumb is many
less parameters for the number of total
number of pixels in your training data
but you can typically use many more
parameters in the number of training
cases and you can't do that with normal
distributed learning now if you do do
that when you start discriminative
training it quickly improves things and
then very quickly over fits so you have
to stop it early</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>