<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deferred-Acceptance Auctions: Worst-Case Approximation Guarantees | Coder Coacher - Coaching Coders</title><meta content="Deferred-Acceptance Auctions: Worst-Case Approximation Guarantees - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deferred-Acceptance Auctions: Worst-Case Approximation Guarantees</b></h2><h5 class="post__date">2017-10-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NI7I8ezo1go" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you
so it was more than but this same
spectrum was not more valuable to mobile
phones it was for broadcasters so the
goal of this was to first run the
reverse auctions
to that I'm old I'll go and buy the
wipers and this they find this class of
options that I'm going to be defined
importantly and I will throughout this
talk is to I'm alike
but also tries to convince them to tell
us what this value is that they don't
have incentive to lie and to give you a
concrete example it's kind of abstract
let's make it a bit more specific
single-minded bidders is one example
that many people have studied in the
past where the provider in this case
owns a set of indivisible items a set of
things like the spectrum frequency say
and the buyers are single minded in this
sense that each buyer wants a specific
subset of these actors so this is a kind
of communal option where each buyer
means a specific bundle of these items
and for value for these items is what
the VI is so essentially saying I have
advised you if I get this set of items
behind us VI if I get anything less than
that my value 0 so I and you nothing
other than that subsidy also you can
think of it as let's say you have these
4 items over here and you have a bitter
saying I want items a and B so you can
think of this as an edge we're going to
represent this as imagine this talk so
you have this this bitter over Hugh says
you know what I want items a and B and
my value for it is 4
you can have another one who says I want
B &amp;amp; B and my value is 1 a third one who
says I want a C and B and you can think
of it as a hyper edge in this graph and
my value is 6 and what I mentioned is in
this example just going back to the
definition I mentioned before our goal
is going to be to find what subset and
we serve give them what they want
aiming to maximize the sum of their
values the social worker so that's what
we mean by efficiency we want to choose
the subset of these winners that we
serve to maximize some of the ways if
you think about it here the best
solution in this example will be to just
serve that last guy this is my u 6 and
therefore we get that semester but in
general as you can imagine this is a
heartbroken np-hard course another
example would be a knapsack option where
you have an asset constraint think of a
clean it so everybody people interested
in a seat or two seats maybe a family
who wants four seats in the plane so
everybody has a size and you have a
knapsack constraint you cannot just fit
everyone you have to choose a subset of
people to fit of course you would want
to maximize social wearing this since
you want to fit the people that have the
highest value so this is exact
not a problem but now remember they add
the difficulty that you don't know these
values so it's harder than the knapsack
code because you need to incentivize
people to tell you about these values
and all both of these examples have this
probability that they're binary in the
sense you can either accept someone or
reject them you can either give them the
surface or give them no service so an
important distinction we're going to be
making in this song is about that and an
example which how many of you have
closed your heart is AdWords option it's
a third example or then is not true so
it's not finally so you have advertisers
on one side bidding for online search
keywords so their position if you search
for moving companies their slopes and
there's a different leap through for a
different slot in each advertiser has a
value for a click right so different
positions in this and in this case would
give you a different click-through rate
so therefore it's not like I either
accept you and I give you H 1 or I
reject you instead I choose what level
of service you're going to get and
that's kind of different than than the
previous two examples and I'm going to
come back to these but before that let
me give you an overview of what we're
going to cover in this talk so I'm going
to define these different acceptance
auctions which the benefit of these
auctions are that they're they give rise
to very practical and very simple
mechanisms for the bidders it's very
easy for the bidders to participate and
they have impressive incentive property
so in other words they give very very
strong incentives to people to tell the
truth tell you what their value is right
and even if you don't care about
auctions or incentives you can think of
them as backward greedy algorithms so
this is essentially an algorithm that
has this backward structure I'm going to
explain what this means so even as an
algorithm designer at the end of the day
it boils down to solving an algorithmic
question having designed a backward
greedy algorithm that is efficient an
important limitation that these auctions
had the way that Milgram and Man Segal
defined them is that they only applied
to these binary settings that I
mentioned the first two examples either
accept or reject so the first half of
the talk is going to be our first paper
from EC 2004
where we essentially studied these
binary settings and trying to understand
some fundamental examples to what extent
can we actually get social web
approximation using these options and
the second half is a very recent
extension so that actually we generalize
this framework to settings where you
don't have just binary outcome so you
can actually have multiple levels of
service and we also study what you can
do within that make sense okay so let's
get back to the binary sitting so the
first time of the talk is gonna be about
the binary setting so remember again
this single-minded bidders example you
have a set of bidders that one subset of
items you have to choose which subsets
to serve and as I mentioned this is an
np-hard problem so even if you didn't if
you knew the values that's a hard
problem to solve it's even harder if you
don't know the value set up somehow need
to convince people to to study them in
fact the way to think about this is this
is called the non single-minded
community or your auction you know this
these constraints you know what subsets
of the participants you can satisfy at
the same time but you do not know what
their values are right what you can
choose as an output is who wins who
actually gets what they wanted who do
you serve and prices payments how much
should they pay and this is your tool to
get the incentives right there's no
other way of it essentially the payments
allow you to convince people to tell you
the truth so you have to choose those
payments the combination of the
allocation and the payments in a careful
way and prove that somehow the
incentives are there so people shouldn't
be lying and the objectives are going to
be to incentivize every person to report
the truth and we're going to
approximately maximize efficiency in
this sense so which is standard computer
science in the worst case approximation
sets so we want the optimal social
welfare should be no more than Rho times
the social welfare that our algorithm
our mechanism at use right so we want
all to be as small as possible on any
given instance we want to give this
worst case tank and those of you who are
familiar with this with auctions might
think of sure VCG would actually do that
so we know that there exists this
mechanism known as BCG which actually
maximizes
so actually achieves ROI equals one and
gives you the right payments so that you
get not only optimal social welfare but
also you get strategy proof Ness at the
same time people should do not have
recently unilaterally deviating back but
as I mentioned if only you could solve
this problem optimally then you could
apply this but it's np-hard so we cannot
solve the underlying problem ultimately
and therefore VCG is not applicable here
because the moment you do not achieve
the exact ultimately this whole thing
breaks down so the moment you have some
error in your optimization then VCG
doesn't work and in fact we're going to
see there's other issues with this issue
as well but this is one and in fact this
exact limitation that I'm describing
here was the motivating reason that work
that gave rise to one of the first
actually probably the first mechanism
algorithmic mechanism design paper by
lemon in 2002 where they said okay let's
look at the setting single-minded
bidders let's try to solve this come up
with a truthful mechanism people should
tell the truth and they and we give an
approximation of the optimal children
this is just that this simple greedy
algorithm we're the same
just take the bins ask-ask everyone
spaniel so they report their values and
we order them based on this simple
scoring function their value divided by
the square root of the size of their
bundle in other words the more the
higher value your board the better you
look the higher the score is for you but
the higher the more things you ask for
the lower your score is and this howell
summarizes the attractiveness of
businesses the higher this number is we
just you come earlier in this ordering
and what this mechanism says is just
order the bids in this order and then
accept them greedy just accept the one
with a high score that means some other
people that have overlapped with that
have to be rejected and then we keep
going but just accepting really based on
that order and reject the ones that are
in conflict with things you've already
accept and you charge everyone the
threshold price I'm not going to insist
too much on prices I'm gonna be thinking
about algorithms most of the time here
but for the sake of completeness
threshold price is the minimum value
that the agent could have reported and
still survive until the end
so everyone who survives until the end
is the minimum value they could have
recorded and still survive so so just
worst it's not the case that everything
else works so if you just divide by the
size that will not work
anything for that means they don't do to
these voice eyes so I guess I haven't
even defined I guess why what I mean by
works and it's going to be an
approximation guarantee which is going
to be the rest of the of the slide so in
particular this world will you make me
some strategy proof that has nothing to
do with the science that was going to be
the case anyway so in fact any monotone
any way that you score them based on
increasing in the value if you use
threshold price that would work so that
part is not does not emphasize but it
also guarantees these approximation
guarantees meaning that the
approximation factor of Social Welfare
is Big O of thee where these and maximum
bundle sites this overall the bundle we
have what's the maximum size that's the
then you get it Big O of the
approximation with this and you get a
route the approximation where m is the
number of items that first one the DI
books to mention is that me some he
wouldn't achieve if you messed up was
that was their route over there
there's a second or not you so the
second one is I think a bit more easier
to get and what makes this a very what
made this a fundamentally very
interesting result is that in fact
that's the best approximation you could
hope for even if you didn't have the
private information part even if you
knew everything inside me hard to get
better than this right so in other words
that fundamental result by LOL i'm sean
paper said you know what this optimal
approximation there we go the ended big
over and i can achieve it even if i
don't know the baggage right so was that
best approximation ratio known before el
oso it's a hardness hardness matching
hardness results so there's thank you
later
or was it already oh you're saying that
that we can achieve the approximation
somehow no I think that was already
known that we can
and that was through different I believe
so yeah I believe it wasn't achieved by
greedy like yeah so this is a very
simple algorithm that achieves the same
personation that you could ever hope for
so that was that was one of the
fundamental papers in mechanism design
algorithm and let's try anything just to
make this because I so I I spent so
although this is an old result the first
half of the talk I thought I should go
through this so I understand what the
great acceptance options are so you
maybe can appreciate the generalization
that we reach okay so in order to
explain this let's look at a specific
example let's verify that we understand
what the forward greedy algorithm does
and let's say we have these four bidders
over here so there's four items and each
one each one of the bidders wants two
items right it'd be for this one CB a C
and B T right so they want different
items over there and the question is
what would the four well would this
mechanism will just solve it like just
to verify you agree in this so all of
them have the same size so actually the
denominator in this piece doesn't matter
so you would essentially rank them by
value and accept them in that order but
first therefore except this guy CV was a
value five therefore once the mechanism
accepts this these two other ones have
to be rejected and then a B is also
accept that's what the mechanism will do
and the prices can you see what the
prices would be just also verify this
what are the threshold prices what is
the minimum value this a B guy could
have reported and still survived
is it floor so think about what if he
said zero he would still survive right
because CB would still win first and
therefore you know there's a B is just
accepted because you know CV was and in
fact the same is true for CD the minimum
price C because I recorded and one is
again zero so he would have won anyway
because of so one of them wins because
of the other and this is important part
of the talk in the sense that this is a
property of many mechanisms if you have
this probably used fresh hole prices
they would both win and pay zero so
although
it's true that this is strategy booth
nobody wants to change their bid in life
can you see of a way where the two
losers could coordinate and win
so if they said okay let's agree so the
two losers get together in the talk and
they say is there a way we can actually
beat the system six one zero so six in
fact if they just say suffered for their
safety so six and zero I think one of
them would have a high threshold price
than their share that they don't say so
because of the symmetry if you think
about it they've both been really kind
you know said if you would get together
and say you know what just let's solve
it infinity we will win and for the
exact same reason will be nothing for it
right so if these two guys get together
and say let's say they both bid nine so
they would win a nothing so definitely
there's strong incentive to form a
coalition and and lie this way and this
is a property not only for it really has
but also BCG would have it would have
the exact same outcome if you think
about this you would be the exact same
thing same prices so in important option
settings you care about this to this
strong incentive to deviate as a
coalition and the main theme of this
paper is what if we did this backwards
what if rather than a forward we do when
we do the backward really algorithm
let's see what it will do that so you
would first reject so rather than
accepting weekly would reject really
right so you would reject one of these
two bidders that's it rejects there's
one baby then reject this one and then
at the end it accepts the remaining two
right because they're there so it kind
of backwards finds its way into the
feasibility so it rejects while things
are not feasible it keeps rejecting so
it would reject the Lisa being people so
you said things for you function
what would the threshold bids then what
would these guys have to pay for the
Cindy and ap guy what is the threshold
price point so yes one so both of them
have to be won and in fact what that
paper by the deferred acceptance auction
papers show that if you do it that
it's not only truthful it's also a group
strategy
there's no coalition that can get
together deviate and benefit so it's
much much stronger it's kinda cute that
if you just do that same thing before
think backwards that just works okay so
that's the whole focus of this talk
backward really algorithms because if
you do it backwards it's much better can
you get efficiency that way that's
that's the trick it's hard to in fact
get efficiency okay so now let me
formally define what this set of
different acceptance options is so you
can think about you know what
essentially has an algorithm you don't
care about prices it just defines the
price it's a set of algorithms initially
everybody's active so a sub zero is then
all the a people at stage zero are
active and then it's a sequence of
stages where while you cannot accept all
the ones that are still active a sub
teams on stage Steve there's it's
infeasible accept them all what you do
is reject the one with the lowest score
right just like we did before so you
check the better one who has the lowest
score and so therefore in the next stage
you have one bit or less and of course
everything depends on what is this score
function what can it depend on so the
only thing so this function the scoring
function and depend on any information
you can change from round to round it
can be adapted we can depend on any
public information the only thing it
cannot depend on is on the values of the
other videos so for bidder I I each
round I have a score a number that
summarizes the attractiveness of that
bitter and this court can depend on any
information what happened the past on
crus is still active but not on the
values of the people that are still AK
it cannot in any way depend on this
information and as long as you do that
and at the end a threshold prices
doesn't matter that there's there are
prices just think of that last thing
that there exists Isis that make this
groups trying to prove in a bunch of
other properties I'm going to discuss so
from an algorithmic perspective the
question is here's a class of algorithms
what can you do with them right so you
get to choose the scoring function we
discussed before finely divided by
square root of
is clearly one of those right so it only
depends for every bidder his score
depends on me on his own value and on
public information the size of his bid
it can be much more complicated to that
but the important thing is my score
cannot depend on other people's values
and at the end of the day we summarize
everyone's attractiveness into a score
rank them reject the worst I move to the
next stage news four's reject or sky
next stage and we keep doing that until
we get a feasible substance does that
make sense so this is the class of
algorithms that will commence ago
defined and the question was you know
what can you do but before we go there
let's verify also what are the nice
properties you get if you manage to do
that so I said it has impressive
incentive properties so it uses in
computer science you would interpret
that as backward greedy algorithms
adaptive backward for the algorithms
because they can change from round to
round and they can redeem said they
probably such that ascending flow
auction implementation whatever you get
you can implement it as an ascending
auction everybody sees a price that
keeps going up from round around and
adjust at some point quit when the value
that price is higher what than what
they're willing to pay so this is very
very important in many high stakes
options because you don't need to reveal
you don't really need to bid report any
private information just look at the
price what another clock when this book
is too high you just say ok I'm out
alright so this is a much more practical
so the other definition I mean the
definition you said that you contain
this quote from Round Rock correct it's
going to go down or up a score is
connected to the clock in it's strange
way it's not the same thing right so the
coffee is your threshold price up to
that point yeah but if it changes or
downward yeah exactly because remember
your threshold price never drops so if
you survive around at every point as you
survive rounds your threshold price
might if anything increase I saw at any
given point your learner what happens to
the score in the next row yes yeah
because your threshold price was I had
to survive that third round even if I'm
in the sixes overall
you actually get that there's no group
of feeders that can collaborate and all
benefit from that deviation and yet a
recent official obvious Reggie proved
miss and if you've seen this as a recent
definition by Stanford economist again
where it says essentially even someone
who cannot trust doesn't need to say Oh
BCG yes I did the math it actually is
truthful I should say the truth is even
a naive person can actually see that
it's better for them to tell the truth
so this is formally defined as obviously
strategy group and finally in some sense
it has privacy preservation in this end
said you don't really need to spit you
don't need to report any value you just
see the prices go up and the winners for
instance to never reveal how much they
valued things maybe had a huge value for
something no one needs to know about
this if I win I want for whatever all
they know is how much I paid okay so
these are very very important properties
that really motivate understanding these
options better and this is I know this
is what they used in this twenty billion
dollar spectrum auction that just took
place in the properties appreciation for
degree do not possess in general so
therefore this this is a class of
options that you will need to use so
what we study in that first paper is and
we actually get these nice can that's
the obvious first question right we have
this fundamental paper by lemma McCallum
and show home that actually managed do
that can we do that to using backward
greedy algorithms and the first thing
that the most natural thing I think to
try first is that same scoring function
that value by the strange square root of
the size that works for the forward
greeting let's try it out for the
backward and let's see an example of how
it would perform let's see let's
consider this instance right so you have
these items so again the vertices of the
items the edges are bits each edges of
the permitter and the weights on the
edges are the bidders okay so this is an
example a specific instance what would
the forward really let's remind
ourselves list again so it will accept
this guy over here right first then that
we mean they will have to reject this
guy and accept this guy reject this guy
accept this guy
reject this guy so this is actually
really good if you look at this in this
instance is actually the best you could
ever hope to do let's see what you would
do in the backward we and you guess what
we do in the back
exactly so we'll start by rejecting this
one it would reject this one it will
reject this one I checked this one and
so on it would keep rejecting until it
all makes sense one and you get man I
can make this half as long as I want so
it would fail miserably right and the
main takeaway point here is that it's
not even maximal if you do it backwards
so if you greet Lee except you know that
at the end of the day you definitely at
least get a maximal solution there's no
more things you could add without
creating some feasibility issues when
you go backwards you kind of trying to
find your way into the feasibility
region but that means you might mean you
rejected something at the beginning that
at the end of the day you didn't have to
so it's actually much more challenging
algorithmically to get minimal
summations this way which to me sounds
exciting because we have this amazing
and said that properties to try to
understand the reasons and try and
understand discussed functions and it's
clearly algorithmically more challenging
so it's kind of is a challenge from an
algorithmic perspective so in fact this
kind of a path example we could fix it
because if you look at it what I want
stupid thing that I did is I just said I
just look at divining the skirt besides
clearly the people on the ends are not
causing as much conflict are the people
as the people in the middle so people in
the middle lesson share crossing through
conflicts so you can somehow use the
conflict numbering to your score
function you know say something like the
higher the number of conflicts I have
without their active bidders the lower
my score so say something like the value
divided by the number of conflicts and
in fact in this kind of an example this
would fix things so if you have just the
path if you do something like their
value divided by I think it was number
of complex times conflicts plus one
don't ask me why but this is what works
this would actually fix this example but
it was it would only fix this one so our
first result is that there's no
different except constructions of the
form value divided by size 25 our area
times number of complex to any power
that can achieve the desired operations
so nothing in that simple for even if we
add conflicts this is the family a large
family of scoring functions that want to
join we have pounds lower bound showing
it would do worse it wouldn't achieve
those bounds
makes sense so essentially in order to
achieve that same if we ever hope to
achieve these these guarantees we would
need to come up with something different
something noble and this is what we did
so the main result of that first paper
ICI 2014 paper starts from the initial
hydrograph with these hyper edges and
has this first scoring set of scoring
functions which prunes the graph to
extract paths out of it and I'm not
going to explain and don't have enough
time to explain what this I'm happy to
talk about this more later on it if you
if you want but in someone we call it a
locally highest bid mechanism it just
looks at the graph the structure it
focuses on a specific item and only
rejects locally so to avoid this just
rejecting in this arbitrary way all over
the place
it focuses on an item rejects locally
and then tentatively accepts whatever
was achieved locally and then extracts a
sense essentially a path out of the
graph and then moves on to the same
thing and extracts the sequence of that
how you actually implement this as a
scoring function that is valid compared
to the that is in the deferred
acceptance option framework is not
obvious and then once you have that then
you then it's a path you remember I told
you that then you can do something with
value divided by conflicts once you're
here that sees once you've done this you
can actually get a feasible hypergraph
without losing much more and what we
showed in that paper is that the first
mechanism the locally has been actually
achieves in the approximation and we can
also get through them the reason you see
fill that is under the root there's also
a log n that we had to face but not
simultaneously so you can get so so you
can get within log V also you can get at
simultaneously the same up again yes
essentially if you see you can check if
these larger than root them sure so it's
I guess you can do a similar thing is
there so you see the structure you see
how how large V is if it's more than u
very good you pick your okay so
essentially the message of this first
half is that what we show back then was
that you can actually match the
possible guarantee is not only using
strategy movement which is what lemma
McCallum Sean did but also using tricky
backward options backward greedy
algorithms like the ones the deferred
acceptance options and the important
thing is although that sounds I told you
all this is so strange this weighting
function that's all these creepy things
remember that's all in the back end in
the front then it's a simple ascending
option so nobody needs to know how
complete which is another thing I like
about these classes it might be
complicated in the back end but no
participant really needs to know what's
happening in the back and they just see
12 prices they going up at some point
equipped right so it's all this
complication is under the hood it's not
affecting okay so now moving on to the
second half of the talk so this was kind
of restrictive right so as I mentioned
this add more the auctions example
doesn't fit into that framework
everything I told Yousuf I said you
start with a set of feasible things and
you start rejecting this there's no kind
of middle ground it's not like all you
know what I'll kind of accept you it's
reject so all everything they had
defined doesn't apply more genuine and
restricted to these settings so you have
binary constraints so the recent paper
with Don Jamieson and thin essentially
tries to generalize the deferred
acceptance option framework so we can
actually use it in much more general
classes of options and we define this
generalized class of generalize
different acceptance options where just
like before we have a scoring function
that doesn't change it's exactly the
same thing we have a scoring function
for every player that is adaptive
changes from round trap and so on the
new thing is we add a clinching function
so we have this function GI which at any
given round tells you how much if you
were to be we don't call it rejected in
work we call it finalize if you were to
be finalized right now GFI tells you
what you should be getting what level of
service you've clinched already right so
essentially you start we get 0 at the
beginning but as you stick around
through the auction this pinching
function goes up and says oh no if you
were to be finalized now you already got
the cycle oh now you got two items or
three items or whatever the type of
services right so gradually gets better
and better so when you finalize a bitter
with the law your final is a bitter
again with the lowest score but
he gets the level of service dictated by
the clinch and that clinching function
is also non decreasing so you cannot by
definition clinch the level of service
means you're not going to lose it in the
future so you have that level sir so it
only gets better and better and again at
any point when you cleans your level of
service you pay your threshold price at
that point so you survive up to this
point you connection level of service
that means that's how much your
threshold at that point is what you pay
for that level service so you get
different prices for different levels of
service and importantly we show we prove
for each one of the properties I
mentioned before that we maintain all of
those products
so our generalization works more and in
much in much Richard family of instances
and it has all these nice properties
still there in fact and this is to the
single-minded so single-minded is binary
so I'm saying let's go beyond that let's
add words options thank so you don't
have generations of building a single
parameter you what is it single single
parameter yes so most so here so we have
in that paper I'm not going to mention
we have a multi parameter setting in
multi-units options we have multiple yet
may have a decreasing large house and so
on but yes so this the standard setting
is just like in AdWords options right
you have a value for a clinger so your
service your level of service then your
value for different levels of service is
implied by that parameter and with the
good rating and I'm gonna steal some
example in scheduling so later on ok so
so so the important thing to understand
here is apart from the scoring
continuity of this clinching function
and what is going to be from an
algorithmic perspective again if you
don't care about the incentives at all
from an algorithmic perspective what I
found really exciting here is designing
this
gauging function means you can promise
things so if someone's finalizes they
cannot get more than the claims you
function they get whatever they clinched
so in order to get good approximation
you try to promise think of country
functions as promises you try to promise
to people good level of service and
early on but at the same time you better
make sure this is feasible you know
you're running against the wall if you
promise too much then you might hit a
wall you might not be able to better be
feasible so there's this additional
which was not there before that your
promises have to mean thing it has to be
feasible whatever your promise has to be
and at the same time you're kind of as
you promise things do restricting
yourself in the de flexibility and of
what how can you divide up welfare later
on so let's make it a bit more comfy
let's go to the AdWords setting to give
you a specific example here so in the
AdWords setting the thing that that
actually works here we can actually get
fuel so she went through with all the
additional guarantees in fact what we're
going to show is that VCG is a different
except that's option in this set in this
specific setting in fact for those of
you who know for any polling major
constraint if you know what a pony major
constraint is this is just a special
case of one major constraint what we
show is that for any pony major
constraint there exists a deferred
acceptance option that achieves the
optimal socially okay so let's take to
this example so what I'm saying is in
this case the scoring function is very
simple just your value and your pinching
function is to click the rate of this
slot as a function of how many people
are still active so initially when this
in this example say we have these three
bidders initially there's three
activators of this a some T's has three
people in it so everybody's guaranteed
nothing zero victory right so at the
beginning I have nothing so let's
simulate this right so what would happen
so in the first round so we start and we
look at who has the lowest score it's
this bit over here right so initially as
I said here everybody has a clinching
function of zero so this guy has a
lowest score we finalize him and
therefore he gets clicked through search
it's nothing but then the moment he is
finalized a sub T becomes increases it
reduces by one and therefore everybody
clinches 5%
credit so the moment a subsystem on this
guy's finalized the set of active
bidders shrinks by one therefore
everybody else clinches one level
service and then this guy is the next
lowest bidder he's finalized at five in
other words he gets the swell and then
the last guy clinches another level of
service and this is a boy example this
is a
example as I told you even beyond this
in general volumetric constraint
settings you can get optimal solutions
this way but just to verify what it
means what the clinching function means
I can essentially define this function
that changes over time and only
increases okay so let's see another
example that it doesn't make sense any
questions on this so this is just very
funny that what we did in some sense
this is also kind of Sanitation
we came up with this new framework it
better make sense in reasonable settings
so the previous frame one the first
thing they verified is it works for
matrix oh are multiple levels of service
we verifying worse for only matrix right
so it's kind of a first time in check
that we actually have a good so what is
the weak strategy weak group strategy
prove this video so instead they cannot
all strictly benefits by I suppose like
two and three suppose they all reduce
their bids by 10% so you can have some
of them so there's no external transfer
transfers that's what it means is
suppose they all reduce their bids by
10% they all get cheap except for the
last one that's their size this case I
haven't thought about this exact last
example but you know it has to be
someone who's so yeah I guess for you
maybe then one relevant thing is this
well VCG which is not generalized second
price but yeah BCG in this setting is
actually we could stretch proof and as
it can be implemented as an ascending
option just assigned resolve from what
we did that in fact all these nice
properties you get for BCG on any major
industry okay now for a more interesting
which was the main result of the new
apart from the definition of the
monument self the real test was this so
we tried to solve without a how rich is
it now we have multiple levels of
service how rich is a family of things
we can describe this way and we thought
about scheduling let's try this is a
well study promised let's try to
consider scheduling right so we have M
identical machines so let's say a b c d
e NF these white columns are the
machines and each age in our owns a job
and of course this is abstract the
machine could be actual server an actual
job but it could also be that just tasks
you're assigning to people or anything
and the job of each of agents job and
sorry the size of agent Heights job is
piece online and the scheduler
guarantees that all jobs will completely
will complete by something like me think
of a kind of a cloud computing setting
where I have only servers I'm saying by
the end of the day you will be scheduled
but of course different people have
value for actually they care about their
jobs they rather have the complete
earlier right so they're essentially so
every every agent has a value bi for
each minute or second or whatever of
delay that he can avoid right so you
guarantee that by the end of the day you
all be scheduled by but people have
value for being scheduled and better so
in this case by the way the level of
service is the completion time right how
soon will your job complete it can be
the end of an a it could be earlier so
the different levels of service are the
different levels of different completion
times I can guarantee for so if CI is
the completion type of job I our goal
which is a standard scheduling problem
people study is to minimize the social
cost in other words the value so the
weight of completion side so you want to
minimize the weight of completion time
so this is like the social cost right so
that if I reduce your completion than by
one let's say one minute then you reduce
your cost by VI so I would this is
essentially saying I want to schedule
things I'm going to find the schedule
who goes where you know to minimize the
way the completion time and again this
is an inventory problem even if you knew
everybody's values if you wanted to
minimize that objective that's a hard
problem so the machines are identical
jobs have a different time but they have
different processing times and they have
different weights right so trying to
solve this problem is actually a bit
hard even if you knew the information so
now we're asking can we do it as a
different acceptance option and the
thing I want you to think about here is
what would this look like right so now
you know the definition of a generalize
deferred acceptance function we have
this clinching function which starts at
the essentially a low level service and
I start promising things and you imagine
what this will look like so essentially
the completion times I promised at the
beginning Sensei I guarantee you all
by the end of the day and then I start
promising more mice as people are
finalized I start promising more and
more better properties right so I'm kind
of scheduling people top to bottom
rather than many if those of you have
seen scheduling algorithms its standard
the standard thing is of course start
bottom up start scheduling them or in a
centralized manner either okay so let's
say I think yes so one thing if you if
you were doing it as a fourth as an
algorithm one thing you might have
thought about like at least get to
happen that's very well known for this
program is the following simple really
algorithm order the jobs based on a non
increasing value over processing time
again those of you have looked in this
evening this is also known as this
mystery ratio so it's the if the if
everyone is the same value then it's
shirtless job first right
so you you're the jobs this way and you
schedule them in that order on the least
busy machine at each step in other words
let's say these are the processing times
of the jobs that I have available and
let's say I border them based on that
I'm not showing the weights but let's
say I order them based on the way you
look I would say jum one assignment here
job to us I mean they're three here or
five initiatives obvious which ones at
least busy the tricky thing is not when
I erase the seventh and look at the
machine which is least busy so this
one's because I schedule it here eight
here lying there so on so this is a very
simple a scheduling algorithm and what
we know about this is a very basically
in this latest if you do it that way you
get in 1.2 approximation right so you
can get this missed processing algorithm
and get yourself at one point
approximation offered but again now this
is bottom up this is giving people
bottom up we have a much trickier task
at hand we want to start from the top
and start scheduling backwards and
remember I could say oh you know what I
promise everyone really good processing
time but I can I better make sure I
broke my promises you know I actually
find the schedule at the end of the day
where
if you take the forward algorithm is it
monotonically that's it
so it remains fresher prices yes so
anything that's monotone in your value
the higher the value the merrier
then fresh surprises should be first so
it gives truthful one point two but it
doesn't give three weak exciting as all
the other issues it doesn't give me an
ascending I mean it's a figure I saw
send the option there's no way how do
you you know it for a sending option you
have to start backwards you have to
improve like the longer you stick around
then hire you more you pay so
essentially so what I'm going to define
a theme of day is going to be
implementable as a clock auction
everybody sees a price and the longer
they stick around the better their
completion forms but how do you do that
I mean so I have couple of minutes so I
want you to just offer this because I
spend a lot of time try and improve this
and I think I just show it and just a
complication of it that you want to
start high up all you know in order to
define the clinching function in the
score is everybody else's size you do
not know they're bad okay so so you have
everybody everybody's size but you don't
know the value where do you start from
where do you schedule the last guy so
let's say in the beginning you you're
going to start with finalizing that
first guy who has the lowest score so
let's say this phone I use that same
thing at the end of the they going to
use something like that but don't worry
about the score as much as the clinching
function where do I finalize the first
person how high you know essentially
what do I promise so I want to make that
promise only seeing this solve the
processing times or the processing times
of everyone that's not scheduled yet so
I know essentially so it would look
something like this right so at each
stage T a job is finalized and it is CI
which equals which is implied by the
caliche let's say the function say
something like that and maybe this
improves for others and you start doing
something like this right so you start
scheduling people about let's say you
reach this point so you've scheduled
some people some people have not been
scheduled yet the question is where
where it doesn't so you know how much
space you have here left the scheduled
people
but then we're but the thing is looking
forward the promise you're going to make
as to not in any way depend on people's
weights so you cannot say oh what would
the forward really algorithm do I'll
just do the same thing because I don't
know their weights so I have to make a
decision disregarding the weights of the
people that come later on yeah I mean
where would you start so you know there
is the worst most allocation among all
the others so you know to make it
feasible you could always say I don't
like something huge they all started B
because the other promised their
rightful no I'm saying these much it's
infinite let's say you have an infinite
race that's not really the constraint so
the constraint is if you start we late
to make sure you have visibility the
year automation factor will be really
bad so if you start really late make
sure you're feasible then your
observation will be suffer so yeah
that's so the question is how so the
first thing that comes to mind at least
I suppose you have to do something with
it makes like if I were to sum the
process let's say ideally packed
everyone
I still haven't scheduled in here how
much height do they need I better allow
that much that's kind of a lower bound
I better not pack them this but also
there's they're not divisible you cannot
just move things across different
machines we have like chumps there's a
manage on the console so maybe the
highest weight job actually is a huge
one so you have to make sure you leave
space for that because maybe it has
infinite value and you don't schedule it
first you suffer immensely so you better
use space where that guy fit so it's
kind of that's what I kind of thinking
this is a very noble type of
optimization that you're trying to
promise things but also in truth
visibility you have a constraint if
you're packing things but also you may
want to make sure you get the
approximation in the constraints at the
same time
but anyway so so the thing that we
eventually tried yeah so if if the
clinching function is if the completion
times that I allow is too large then I
get a bad approximation if it's too
small then I might end up with any
feasible and before
and not in any way depend on the values
of the people that I have so I guess I
took enough time to try there let you
think about this the thing we actually
did is we're saying let M sub T be the
fractional makes them of the people that
have not been scheduled so this is the
set of people that have not been
scheduled if I sum of the processing
times and divide by the number of
machines this is kind of the lower bound
there's no way I can get better that so
even if I were to ultimately impact
everyone else that's the best completion
time I can ever guarantee for the last
time and the P bar is the maximum
processing time among the people that
have not scheduled it's overall because
there's also as I mentioned it's
unfashionable if there's some huge guy
over there I cannot really promise
somehow it will change what I can I can
do so what we said the the goal that we
set where we finalize people is the
fraction way span plus two times the
maximum percent so saying let's start
with this minimum thing and give it some
leeway of to max and processing times
and let's start from that doesn't matter
so and essentially by the way because
you start packing things you might not
be able to actually get your goal
because maybe it's higher than what is
so you might not be feasible to schedule
something up there but you just make it
as late as possible up to that point
let me make it more crystal and the way
you would look like so you start there
you schedule the first guy in one based
on his processing time since this guy
was reduces M sub D for sure by his
processing time divided by M so that
drops in fact if he is the guy with the
maximum size it might that might drop so
right but this always drops and then you
schedule someone else and you keep going
like that keeps getting so the tricky
thing though is that you might leave
some open space here right so you start
packing and it's not clear why you know
now maybe there's an efficiency because
you didn't pack everything there
something sticking out from below right
so it's not like everything here is
always free so it's not obvious that
this would work either
but yeah so I'm not I don't have enough
time to cook these things but it works
and I'm going to talk about the
approximation visibility was what took
us a little time to prove it wasn't
obvious that this will not keep going
too fast
too low for us to be able to pack
everything and in fact this is this is
tight in some sense let me show you in
what sense is this tight let's say I
start with this set of examples right so
I have these processing times so the
first thing my score would tell me to
finalize is a hundred seconds then it's
50 175 180 7.5 100 and so on right so
let's say we start from here when I
schedule the job that has size 100 then
this drops by how much by a hundred
divided by 2 right because I have just
two machines it drops to this a hundred
divided by two that's how much my make
span my fractional make span drops and
then I schedule the next guy who was
exactly 50 so he's facto right there and
now my next job so it drops in by half
so always it drops by half of whatever
is right because this P t-bar will not
change that's right and they're going
into this it goes by half and you will
notice that this keeps lagging right the
redline keeps staying further and
further back and I'm still packing
things here and eventually you see that
it's always kind of getting closer and
closer to the limit of why this in fact
you can show that in the limit if you
were to save you have alpha times NT
plus beta times 2 PT if you did alphabet
anything less than 1 so if you decrease
them then it would be infeasible so it's
exactly typing in that combination in
that sense and also what so there is all
that we get is this so the D algorithm
the actual algorithm that we have is at
each stage if there exists a job I so we
have we have to make that that tricky
decision hew there is this a job whose
processing time is greater that have to
make span then you have to do something
else then we finalize the job with that
largest size otherwise we just use a
scoring function
we scheduled the finalists jug so that
it's CIT so what I meant to be Marissa
other just as large as possible but no
more than their gold and we proved this
approximation so we actually get so
unlike some of the previous results that
I showed this actually gives you a small
constant factor approximation for an
np-hard problem using this this class of
options the way to think about it really
what it does is it we managed to prove
that our scheduling that and this is why
we need this trick over here that first
if whatever the forward greedy algorithm
does we make sure that point wise every
job is not delayed by more than a factor
of four so whatever forward does we do
no more no worse than four times that so
how you can guarantee backwards that you
know because you don't know what
position this guy would have been in the
forward you know you don't know what the
structure therefore would have been but
somehow we managed to show then no guy
no job is going to be more than four
times away so this is exactly four times
that initial approximation that I showed
so we use a factored form okay this is
my last slide so to summarize apart from
the first part where I talked about the
single mind that there's a bunch of
results I didn't show it what the
limitations that you can go there not
getting more than a logarithmic and so
on there's a lot of results in both
papers I didn't mention so some of the
results in this more recent one that
I've mentioned so I mentioned a poly
metric constraints and the job
scheduling I didn't mention there we
also consider multiple nav set
constraints and we get algorithmic which
in fact is Ultima we also consider back
to our next question about single
parameters so most of our is also single
parameter but we also consider multi
unit options in the latest paper we
define a generalization toward that to
which essentially treats its marginal as
a different bidder somehow that's the
way the thing about each marginal that
you report we treat it as a different
person and we get a for log an
approximation we don't know if this is
tight so this is one of the main event
problems understanding whether but it's
kind of you know we try to get lower
bounds but imagine getting lower ones
for which one of the properties maybe
for regroup strategy poufiness say
there's you cannot do better than bog
it's kind of hard to argue like we tried
lower bounds for a group size you proof
this but it was not easy how to get
there you know we have the
positive results we're thinking about
whether we can actually prove it in
their circle open directions what I
really like about this this setting is
that it's so new then not that many
people have managed this a lot of
low-hanging fruit for instance there's
many interesting special classes of
single-mindedness we just said worst a
single a special case would be interval
bears and say you just want is you want
I so if if you want to schedule a job
and you want from when I know two to
four you want this server right that's
against single-minded you want that you
can think of seconds as items and your
single-minded bitter but it's not like
an arbitrary structure it's not not
material - ray it's like interval
bitters and for that one for instance we
can we can improve but there's different
other constraints you can consider and
both in the binary ending the multi
parameter settings is this is just the
first problems that came tomorrow
and even more generally we have not used
priors all of our results are in the
worst in so if you know a distribution
about diviners you can do much better
and this is public information but
there's no different except this options
that use priors and finally one that I'm
actively working on is all of the the
statement that we have is about
deterministic mechanisms if you allow
randomization is luckily you can
actually potentially do much better than
that so randomization is not art so that
one obvious thing is just randomize and
decide what option to use so that do you
want effect a set of properties but a
trickier thing in a more compelling
thing would be to randomize seeing the
value so as you go somehow been you know
I kind of baked into the scoring
function some randomization it's not
obvious what come this randomization
depend on without breaking so
generalizing the framework to include
normalization is I think a very
important that's it thank you very much
so fun questions in the spectrum option
where they first define this differed
accepted yeah so there's also good
properties but if they also look at what
score function would give good
efficiency so the interesting thing is
that they actually which I think so so
in the real setting they had some
special cases I couldn't I don't think
they they use exam you know they
approximate incentive profits I don't
think the user as a guideline but they
didn't exactly get all these nice
properties but it's funny they actually
use the square root somewhere in the
scoring function it's like you know in
that sense you have those constraints
over you know if broadcaster is in a
specific city you cannot assign the same
frequency to a broadcaster that isn't
the same kind of vicinity right so so if
so really what they're doing is they're
trying to buy off some of the
frequencies making sure that whatever's
left they can redistribute impact so the
constraints are kind of these kinds of
constraints that you cannot have two
broadcasters in the same and they're
using I think something like value
divided by number of kind of conflicts
like that square root and I asked why
and they said well that's there was that
paper so there's some justification
because the interesting thing unlike
what we're doing here is we're saying
let's design out you know like there's
no we just choose which one it is in
that setting even there's a game even
before you get the option in game in
deciding what option we're going to use
some people want this option the other
people like that option right so as a
mobile provider I might actually prefer
one option to the other so they're kind
of low being to decide which one of the
options to use and therefore just say
you know what there's a paper that uses
square root we'll just go with that is
kind of I think just solve their
problems in the sense that they could
just say we're not leaving we're not
accepting any ones sure but if you look
at that some graph problem conflicts and
so on yeah you could look you could do
similar math to figure out what's the
best score yeah yeah I think they just
saw in reality because you know these
are worse case opportunities in reality
they said that they kept getting in all
the simulations proximation so they they
said you know the actual instances that
they got it actually seemed greedy
in fact so Milgram so so when I
presented this in the in their seminar
so one thing he asked was he was really
interested in this really liked things
while they work how do you parameterize
how close rather how closely in nature
you are because you know for me to it's
greedy works so he was really interested
in finding approximations to the matrix
structure that might explain why did
this work in our case there must be some
structure that allows it to work what is
it that works but there is something in
the real raff that actually makes it
much easier than worse these examples
okay turn your single-minded case isn't
truthful against lying above this head
take if I say bigger said no so as I
mentioned the whole thing is single
parameter so the way they defined it was
not only binary was single Brown I mean
you're only private values your your
your value your fit your set is not
private so that's another so another
generalization as I mentioned the only
multi parameter things that exist for
deferred acceptance options is his last
thing over here
so what you're saying is essentially a
two parameter setting where you report
your value and your size we have no idea
I know the things we already have do not
work so and you're right that actually
lemon okay I'm short on paper actually
works even for that so that forward
greedy works even if you're a bundle is
private but yeah so that's another open
question we we proposed at the end of
the first like paper could we design
something but you need a different
framework to prove that you still don't
get the Frederick M group strategy proof
issues right so you need to prove that
there's a way to use these two
parameters without creating any issues
or the either that or just come up with
a specific auction and say I don't know
about a framework but this one actually
has this board but there's no known good
point yeah so this was the first ever
observation algorithm for scheduling
with I two types well we finally missing
the outcome
inv when I could have gotten the process
for survival times so if you ask to get
envy freeness this one is too much to us
will just collapse the schedule to UK so
using their people because essentially
they were willing to pay the price that
means it on in me each other that's why
you get you say in between synthesis
with a pricing to say if I if I relaxed
so if I knew the values and just if I
knew everything cry achieve when we
famous in a more efficient way I guess
I imagine that I mean even in you study
right suppose you collapsed your statue
yeah whether that gives you an NB free
well with the crisis me though right so
there's also prices that you need to
address we keep the prices if you keep
the prices I guess everyone's still
happy meaning they pay less than daily
but it's not obvious that yeah it's not
obvious at all but yeah when it comes to
delays I mean people have looked at the
laser guessing in coordination mechanism
settings right so when you want to
create in sense where people can choose
what machine they'll be scheduled on and
although I guess it seems somewhat
unappealing to introduce delays at first
sight you can always just use those
times to do something else right this is
just saying I have a sort of bitters I'm
into cells but it's not like this is the
only thing you're doing with your
servers you can also have you know what
during those idle time we'll just from
my own processes or something so yeah it
kind of just addresses the incentives
for a specific group of people as long
as there is no kind of contamination
between the people you're dressing and
what you're going to do with the idle
time then that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>