<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Content-Based Image and Video Retrieval | Coder Coacher - Coaching Coders</title><meta content="Content-Based Image and Video Retrieval - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Content-Based Image and Video Retrieval</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6nOHh52GcuU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">face
santee satyam got his PhD from the
University of Maryland has been just opt
ins university system comprising 30 is a
world expert in language following
techniques in many different areas
speech recognition machine translation
language processing and today it looks
like us
and this sitive has
so you have to cook again thanks to date
and like the little pieces
especially if there is something which
is not here on the fly
so the images / like ooh it's
essentially it's about providing access
to give it to the video based
essentially a texture like another way
and it comes to things like collusion
purpose
people have shown over time there in a
few ships in the compact each born
decent access to news time videos just
by doing some recognition and concluded
search of the transmitters and then
there are other cases for example images
on the web app is very well
image you today
the other
and
is the property so often times when we
see a 30 second one min news clip there
are definitely several minutes or even
hours of video behind it the raw footage
and then the production team looks
through it and finds it puts it together
if there's usually a director / label so
if you're looking at the original
footage it's very hard to search and
because they lured you to go get out so
those we kind of things which are
interesting and he's going to be working
on this and I saw this problem I said it
is not so what are people handed this to
make before so what happened is the big
problem was are you sort of research are
usually results in each comment datasets
so a lot of people have published in
something forth into renovations the
current images in either in the stock
photograph from the whistle draw or
something and they have CDs 100
people
and these are many seem to get your
hands up because often here to buy the
software to get the picture and I
couldn't even find the software right No
Deal images courtesy of some colleagues
in berkeley to sort of some figure out
some way
so
within this and
and then these photographs a caption
manually and you see examples for
example than 100 pictures about polar
bear the gentle word pairs of smell and
then another hundred pictures of flowers
pictures of horses eternity and so these
are groups of pictures and people often
what they do is this will make a trading
desk partition put my name into the clay
every text and i'll show you how that in
sometimes be problematic but i think the
way these things have been done the
training in their second too much like
each other and so people often get
dramatically good results of this data
set which don't then scare off other so
when we I our community started looking
at it some years ago they essentially
did what we are doing da la gusta they
said okay you 38 plus a standing
benchmark test test conditions
so the Higgins items will give you are
on this mist feel like a click
so this is very much modeled after the
doctors unassuming might be everyone's
familiar with in this ad hoc task yes
the mix of the web works as I system so
the way it works is like there's a huge
collection of documents and that various
versions of both and typically they get
information analyst retired information
about this for a CIA or NSA to come in
and say okay think of something you're
looking for it somebody
just finding out something about this
disease because the plan
get out
that and so the show is allowed in
formulated when they have a certain
chemical chain give us a couple of key
words give us a one sentence description
and then because we had it will say our
causes of the disease and the symptoms
am NOT interested in like relieved by
pathology of
the disease first controller patients
respectful and then the search engine
starts and typically 6,800 people
participated that's gangsta they are
applied to use this information the
specification will try to get a question
as collection and post-op a privately as
other systems miss puts together
everybody's enough to retrieve retrieve
document and the person who are looking
for the information then Wilson actually
makes assessments saying yes mr.
introduced me to this is a real
information read there's a rave user
it's not too far removed from what might
actually have expanders
so that's the pastor model and so they
did the same thing they got going off
the news broadcast and then all be out
of here before because the way spoken up
earlier there are several hundreds of
hollywood video from new sources say
that maybe see some top use another
system talk in some industries great
news and last year they added arabic and
chinese buffet
so think of a bunch of video of
commercial vehicle we just
and then that's what they said this is
we do this half-ass perspective of
somebody give me shortstop into the
buildings and roads or one of the men
said something like give me shots of
flux so show me to the density of and so
these were visual very stunning illusion
x-ray and they divided up the task a
complete component powers one of them is
just short confrontation if you have
running video when you know that the
camera shop has changed and there is
everything
this is not pregnant you also have to
pull an optically in the way this type
of america i missed in the middle this
caused it I wonder feature detection and
people have worked the object
recognition have you think of this as
object again finding images with God I
great video with someone latex find me
video with a truck passing by on the
back coming
this cot and what is available through
list that two years ago they gave me
collection will turn about 34,000 key
frames and he famous or up a middle
initial
I think and then we have a similar
collection held out for testing purposes
and then last year with other
collections fighting over 7,000 training
very large how do you measure things
again the community took the attitude
that we don't use value the slicked-back
so if I wasn't election commission is my
second means of the cards will be ranked
list and at every position will give me
a video clip is a car I'm going to see
how many about that point have come so
if for the first one second Inquisition
at that point in person the second
America the third is not a far the
fourth one has a car so that point
Inquisition is 50% it was cool
yep obvious lives you're out of 32 you
mean speed up store that it's p.m. yeah
now going to make a destroyer manga all
right good so we speed up so this is the
usual stuff and then I mean the label
that convention is Rico hopefully your
decisions just an average ever recall
and so these are typical queries used in
this high level feature detection task
so they have and like you know some of
these I really don't understand we have
things like they have isn't exactly new
subject face new subject
the English tell you which is which
perhaps
for the things I'm doing at least in
2005 the revised concept mr. liquid and
I about the concept this positive it's
not that we should play sports in
progress use of monkeys what the scales
so this is a typical in 20 of feature
that wouldn't detect okay so so far so
good so when I walk into this problem it
was June of 2004 and for those of you
don't know every summer in Johns Hopkins
we have a son a workshop where we get
about ten even though three teams which
one interesting is a thousand three
months and leave it to the ground all
over the world half of them researchers
forgiven for cyclin people was to take
they made segments into regions and each
the mix region sort of quantized it for
whatever visual properties you're
looking for into some discreet for
carefully so what happens is the image
now look sling bag of words and its
caption which comes in the girl will get
your image also looks the new value is
treated like a translation problem one
language is the language of blocks as
they orbit so these would love for this
device to engage with the image regions
and the other language is the language
of words and they just cleaned up the
standard machine translation system and
it seems
and images training I honest now these
guys from UMass they took a slightly
different they said we treat this like a
cost I wish I are so the query is in
words and the documents are in laws like
English Chinese and they pulled out some
ideas they have been use it for which I
are again the same thing that desperate
lies the images and actually said each
image in your training collection which
happens to be annotated Smith's a big J
is the size of the frame which we treat
this as a sample whose relevance or
closeness to the test image they made
that one apparently not practically is
given by this row and this for example
in case of cross-language ir will be
based on some sort of translation
so you say the surveillance Chinese and
lures English I have attained in English
berrykin Chinese document was like
a Chinese burger English
so here's the similarity between you two
dummies and then I simply look at what
the concepts are being present in the
top middle image that a flavorless jay
and i'll just use the presence of
concepts in j times the similarity of
jade I you come up with what might be
pleasantly it so they call this the
relevance model and in particular the
similarity can be computed using any
curve and in fact if you wanna get away
from this discretization of the image
regions you can simply one of the image
features as continuous value so they get
this began this means with some of these
diseases so this was the extent of the
artist event of the summer
right so this is what we proposed a nice
idea however saying who was at the time
in class independent the us-cuba screen
and we said let's think of the image as
a stochastic process which is generated
by an underlying hidden process the
heating process takes batteries we will
go category of words things so if I'm
going in caption and image and let's say
my caption would Cabul early has 3 500
words it would be very much and I simply
think of a Markov chain that takes
values in this set and if I'm given the
caption of the image I know which states
were visited out of the 300 possible
states so this particular image was
captioned with the building sky outdoors
of God by the way this is what comes
with the data set so for example they
didn't bother to label the person in the
image just have to live with it and and
then you basically think of the images
being stochastically generated by this
underlying process and to not to assume
that the processor independent we can
put in some transition probability so
it's a fairly simplistic model and in
fact if you think really carefully this
does look like a gaussian mixture model
if i try to cycle model the mission
density with councils so closely all
right so a quick note on what these
visual features are in other words what
is being observed in this image right so
one of the decisions we made it we just
said we'll segment the image not bad
agent but if the fixed length blocks 35
or 50 by 50 blocks and then visual
features are extracted from each block
and these visual features were things
like the color movements and the for
reasons that I'm not an expert at I can
just tell you they were moved into this
lav color space where apparently
perception is more linear and the value
of the colors and so on and essentially
it says the mean of the red of the
illness of the anus of the penis
a little block if you thinking rbg it
would be the moon Retina screen that
green Nestle and then the variances on
it what's the extract sort of visual
features and so for example there are
three la and B components and each of
them have four moments so this would be
a 12 dimensional vector of real value
things right and similarly there was
they can we extract oriented edge
histogram so you have a little filter
that says is there an edge in this
direction and the output of the filter
is quantized to four or eight levels and
so you say the strength of an edge in
this particular in this little sub
images so much it's 1 through 8 0 0
through 7 and then there's not a fifth
of its is there an edge this way is
there a vertical edges on this article
edge and you get a bunch of EDS
strengths and that's another backer and
assessment and then similarly you look
at the texture of an image by asking
yourself how often are two neighboring
pixels in a gray scale version of the
image the same level and that gives you
some sort of so all of these are vector
valued quantity then we just concatenate
all of them and get like an AP
dimensional vector so that's the feature
extracted or of each little sub image
and again we didn't do extraction of
these features which have just got them
from our colleagues at IBM Korea and God
who happened to be at the summer
workshop and he happens to know how to
do something
sorry at the example of the car and sigh
and guy hmmm
the processing how the outcome of us
going to send different
and then getting an infrared
the system we have right now it would
just hope that it had seen enough
infrared images in its frame that
recognize it so in that sense it's fair
enough it's it's it's just a statistical
matching kind of an attitude or simple
yes it's not doing anything you would
close to object recognition and in fact
and that's partly this is one of my
criticisms of some of the other words
that I just mentioned about taking the
corel draw pictures I've seen people and
Stefan here will tell me it's true that
people show images where the computer
says its Scottish far as if there is
some hope in hell that this computer
actually recognizes a Scottish farm it
knows how it's different from English
well since you have to
analysis yep but you're making
inferences about an image based on its
annotation right what one wonders is
that if you did not
well any images is there a very or is
there some sort of unsafe goal behind
the research that if you did enough of
analyzing entity images then when you
received an unannotated image and you
try to do some pictures
physical properties of the future
expression mash up with all of the
annotated future chapters from the other
one and then be able to reason about the
onion opinion I'm wondering if that is
if that is a goal that you see beyond
the current research or current research
focuses only on the medicinal properties
animated images that precisely is to go
and let me maybe articulated in slightly
blotted or bigger terms I come from a
background in speech recognition I
worked on it for seven years old record
for this problem and that we've seen
this that like you get a number of
speakers to speak in some big
transcribes it and update it and then
you do want to go and recognize a new
speaker typically in the different
acoustic conditions and as you go
further and further away from your
annotated speech the performance gets
worse and worse so if it's the same
speaker a way great especially in the
same microphone you change microphones
universe it takes the world is the model
is not happy right you're sitting up
what happens to it to a static model in
this when the property purchase price
taker is called so on right so so what
we do in speech recognition we've made a
lot of progress in the last year's as we
have been adaptable and in this top
towards the end I will show you a little
bit of adaptation but it's still a face
static nature in this enzyme adapting to
video source so in principle if you gave
me lots of images with one kind of
imaging device in other words the
different imaging device I have sort of
a toehold into where I would go next in
order to adapt models build everything
one device through so they're absolutely
Adams devote your goal is to do this
both with supervised make a little bit
later from the new source and also
unsupervised what if I just give you 80
new stores what would you do my hope is
that there's enough similarity that I
can start bootstrapping in an
unsupervised yes that is the code we
haven't gotten there today I won't show
you the result is obedience occasionally
but my hope comes from the fact that you
can able to do the speech it everything
i'll show you will be speech model
system all right okay so adaptation will
happen and did it happen in third half
the dog and so so these are the features
and this is typically what they look
like in casing on a visual the enemy
moments like this let's play then these
are the edges that work texas of your
parents make you say this image has
neighborhood correlation for charity hi
wake up it's another feature so here's a
little bit of a mathematical formalism
so the hmm say is the image i which is
made up of these p image locks x 12 x t
each of which is like a katie
dimensional vector it has a likelihood
given its caption or let go and this
this this probability is essentially the
joint probability of the image and the
underlying states at the Markov chain
went through it reducing these parts I
just marginalize out with these where is
a single the C goes into restricting the
set of states that the Markov chain
takes value so this is the supervisor of
the family
you actually have a caption for the
image and the hoff here is that if I see
sky and many captions and then some of
them might in class well as I see moving
I'll have a common density for what the
image block meant to look like given sky
and that hopefully by some
self-organization this conditional
density will try to capture what the sky
visually looks like given that the
underlying state is the word sky and
this will tell me what sky appears next
to most of the time maybe other blocks
called sky or maybe other blocks or
pleasing circle so that's the hook over
here and this is your standard hmm yep
you say so we will be bordering in those
states ok now the the way I've been
doing with this to the HMMs the states
are fully connected so you can go from
any word the caption for any other words
so at this point the modern doesn't
district for example in alternate blocks
go through skype building sky building
water and nothing in the model stops in
the hope is that there is enough
similarity between sky blocks in
building blocks of the self-organization
and take over and give it the right
thing yes ok so well the second lens the
same in the first I guess this is
happening
okay so in this particular case we just
scan the box is exactly right one of the
students had mentioned later on is
working on a more if specially organized
way of going through the blocks way to
adjacent blocks first so are even doing
region-based reversal of blocks can be
inverted a certain that isn't so right
now you just go lick your ass escape and
each density image block is modeled as a
mixture of gases and this is simply what
we had at hand so given us enough gang
images you can we use scan logarithms
the baggage algorithm estimate these
parameters of use and the Sigma's and
there is an each particular image like
an each particular concept has a mixture
of gaussians two modalities so if there
are multiple modes if you have like you
already in these guys and blue daytime
skies in great cloudy skies you have a
number of gases to play that so you
hopefully cover all of them they're
using multiple nasty okay so that sort
of the set up and the number of
parameters is basically the number of
states which is number concepts you try
to label x number of gaussians in the
mixture for each state times of course
these are objective I will be excellent
the dimension so this is sort of these
kind of their model that has been very
popular tradition okay so what do you do
once you have such a model then you're
going to give image you want now and
this is a standard equation that comes
out of this HTML for lose the
probability that you went through some
states see and by the way what you do
for decoding is if you had 100 concepts
in your vocabulary so that your 182
states you create a big hmm where you
can go from any of the hundreds case by
the other even though in training you
are restricted to going through the 405
words that name which was labeled with
now during decoding you let it go to any
of them and then you get a standard
probability out of this model what is
the probability that a time T time man
is like a special image block that at
block t you the Block B was generated by
a underlying concept sky or tiger or
three or whatever horse so this
probability comes out of this so this is
all state sequences always of traversing
the hmm if you had a hundred words in
your vocabulary then all hundred to the
pjs of going through it which go through
the particular word see if I'm feet of
space location fee and this is of course
just the probability of a Tulpa and your
summit Hall Pass which go to see at time
T and this is where the Sun Hall Pass
which is imagine coming hi and this
simply gives you the posterior
and then we say that the probability
that the image I contains a concept
little C is simply the probability that
it meant through this concept at some
point in the image its if sky is present
anywhere in the image on the level in
the sky and the probability that the
label sky is good is include aslam make
sense all right so so we can also do
this by the way this sum over all these
things can be fairly well approximated
by essentially the RF max typically if
you have distributions which are very
far away from each other the most likely
heart has such a high probability
compared to everybody else that the sum
over all paths is dominated by the
biggest term we can very often kill me
so people in speech come to sleep every
approximation and by the way all this
thing comes about very efficiently with
computations are very efficient because
in my dynamic program that you can write
for solving suppose okay so this is the
basic setup so why did we go into it
because other than I were working on
speech coming years we knew how to
speech recognition systems and we need
many other things that one could does be
honest so the first thing is of course
the hmm design how many states do you
want to have how many gaussians do you
want to have how do you earn extra
weight the Gaussian wave there's a lot
of sort of experience of history behind
of this is done and so we said okay
let's try to bring all that in their
language model in mini model when you do
speech recognition who care about what
words are likely to fall over other
words instead let's not think about what
blocks are likely follow what other
blocks so maybe sky is not immediately
followed by water may be there has to be
some ground in between we get water or
whatever it might be or that if you've
seen plane than the property of seeing
sky is high but on the other hand if you
have seen I don't know
water the problems you've seen both
might be high on our plate and so on so
those are kind of dependencies between
the underlying states which the Markov
chain captures and the speech recognizer
that would happen in my view of language
modeling so your survey let's see if you
can drink some of that another little
known engineering hack the fish turns
out to be interesting in speech
recognition is that we likelihood
computed by this hmm and the underlying
transition probability of a Markov chain
of language modeling properties they
live in many different spaces so because
this is a Gaussian likelihood of a a two
dimensional vector you get log
likelihoods of like you know minus 10 or
minus 12 which means ETA minus 12 is the
probability of this particular X
compared to that you get word properties
of point2 in point three it so somehow
they don't go very well together so
standard hat in engineering is flatly we
just a probability insulation pipe
please let me lose that and then this is
the point that we were referring earlier
in speech recognition we've done a lot
of work in enacting a system train of
one set of speakers to a new speaker or
new acoustic environment so we said okay
let's take assistant rain on images from
one video server set in an African
original sources so these are backup are
here though which I'm going to go in the
next in 15 minutes
and if you're more interested in one
than other than interrupting or not
right so these are four things with them
all right so numbers there are lots of
numbers let me just help you go through
them so the first issue was counting
gaussian mixture can polish a valid
usual empirical just tried out part of
us see what works and so for the Quran
Vegas I told you we had five thousand
images which are divided into putting I
wanted in training it I rented for
testing so that it turns out having fun
gap series is the goodness for track we
have many more samples to train on I
think something like 14,000 images with
something by the way we did divide it
into a training test partition even
within the 44,000 so this is training on
some of them and you can actually go
much higher than 20 you can see it sort
of gets good at 20 in the other little
trick is that when you're training these
gas in mixtures very often the the
probability rather the estimate of the
variance of anyone Gaussian might become
small you started to overfit so there's
a standard vein speech to control this
overfitting you essentially they're
going to put a floor of the variance you
say the variance can get syllabus and
the variance floor is typically some
fraction of the overall variance of the
data so we tried a bunch of that muncher
having a fairly low variance works well
and those of you work on speech
recognition compared to speech where
this would have been a good number for
images in this comes out these are just
details in case somebody fill you knows
this okay and so these are the kind of
numbers we get so this is the mean
average precision of Oh point one nine
what it means is an average twenty
percent of the ring cleaned images
actually have what you're looking for
and in case of crack it's a little bit
lower at seventeen percent and what do
we compare them in is this good is the
demand what is it well like I said the
state of the art when we went into this
lab some other still summer a year and a
half ago were these two things the
machine translation one done by doing
the loo and Bernard and force it and so
on and the relevance model done by the
Francona mythology so one of the nice
things about the Sun workshop by the way
is that both these people were there and
we were all working on the exactly the
same training and guest said it exactly
the same sort of data that we have and
so those nice to be able to compare all
these things next to each so this turned
out be one of the nicer experience in
the summer and in particular the machine
translation model gets and we have which
position of about point 15 and so this
is considerably better than that and the
relevance model is 0 point 26 and this
is actually a not quite we model i
showed you will in fact some enhancement
of it so this is clearly much better
than and these are all the gorilla
beaches so then it comes to
state-of-the-art we're far behind this
one which should be enough is okay so
far so good on the track the story is a
little bit different it turned out the
district ask is a whole lot harder part
of it may be that the video quality is
much lower because the kernel image is a
fairly high resolution will track you
make nature
video so that might be part of it the
other might be well that in case of the
credit limit is very clear your pictures
of bears your pictures of flowers your
pictures of buildings in case of tech
you're looking for cars in regular news
footage you're looking for people so it
tends to be a harder problem is people
so here Everage position completely
empty system is much better and in fact
we're quite competitive with this yeah
emphasis to Moscow so I can claim the
negative decent results and of course
there's twenty percent good of course
not that can you get one correct image
or the top 50 to the top ten most people
doing search would say that's not good
enough you need at least half a dozen
pits in there so we said okay we
continue working on but the good thing
is that this already looks good so if i
wanted to show you good results this is
what i would show you so remember i said
this is on the training in test of the
44,000 break images that are in house so
there's also the mist actual track
selection of 13,000 so now we ran these
queries so 11 of the 17 word states now
hmm so you'll actually be free for them
we didn't have labeled data for the
other 76 there things like Madeleine
Albright and samuel l jackson which for
which we didn't actually labeled images
at all together we're not only valuator
now on this portion of mean average
precision is very present just as before
and these are the kind of images again
so the top line is weather news so all
five of them happen to be done is the
second one is horses and luckily enough
all five happy to be horses right the
third one is football and i'm sure this
will actually under obviously playing
tennis but hey the system is looking at
color texture of this great football in
fact we this is just dumb luck and i
have my own personal guests about why
this might have
I think there's this funny image of blue
which appears only when they create
these synthetic maps which is not there
in any other part of the broadcast so
this system whether maths weather news
means funny blue in the background so
don't don't think that these systems are
smart and know what's actually happening
in there doing statistical matching and
this happens to be down your way of
managing these are precision recall
girls over here so the precision scales
close to one till pretty far into the
latest so we pick up mostly when it so
that's the nature of this model shiver
right okay we looked at modeling these
word co-occurrence probability so if i
see grass and trees in the image
unlikely to see tigers or the other way
around if i see a tiger it better be in
grass and leaves and not in sky and
water ability or whatever it is so you
would like to model co-occurrence of
caption words so we built a couple of
models one thing we said forget about
them using a form transition
probabilities that's case a case be
trying to model co-occurrence of words
and your training captions and billboard
because a bigram model and third let's
just allow captions which are actually
see in other words if i've never seen a
common road and building together it's
not allowed as a possible paths through
many children's only parts which have
actually seen in training a lot there's
nothing that my captions coming out will
only be captions in the training his
room but i do in the end compute Asteria
probability that image this image block
came out of this particular concept yep
the previous ourselves from extracting
the black ones are uniform in with
whatever Angeles and you see that at the
minute that show you results to this
pathology so one of them will match up
with uniform and the other we match up
with fighters and this third one turns
out that this is modeling sort of the
brutality like all these things tend to
occur together don't get the whole
caption all fours is it trips right so
this is sort of a finite-state glamorous
you well and it's a weighted financial
planner we get simply by how often did
this configuration of Captain words
appear and based on that computer
castelia projects it doesn't mean that
you will not get probabilities for
anything that can happen together what
does it mean is that if you do with
every decoding you get only one of the
captions actually sign okay how does
this do well a fair bit better so
remember I have a point one six of seven
point 1934 the carrel in the last night
that was the magnet statistics so in
case of the corral data the word
co-occurrence is improve a lot so you go
up from point 1 56 or point 16 2.19
three it's like a 20 25 percent relative
improvement and the reason is that these
gorillas images are of that sort so you
see polar bear eyes Scott you see
butterfly trees grass flowers so like in
a very very strong please so as soon as
you know some words you can find the
other things as soon as you found some
objects into that show the other object
is that even if there's a faintest
visual signature this really crude sort
of visual feature so that's the kind of
thing you get over here with the track I
feel that the concepts are much more
scattered so there's out building
buildings out there in mountains once
you're out there you don't know whether
you can see public accountant so one
tell you all about the others perhaps
building in Mountain tell you that it's
outdoors but not yet so the correlations
are a whole lot weaker and on drag we
couldn't get much of a gain by trying to
model co-occurrence of these concepts
and even the concept second of their
allegedly organizing some sort of
ontology but I haven't figured out a way
of exploiting their ontology of the fact
that several things are special cases or
appear with certainty so this is
relevant language man the next thing we
did is scaling of these Gaussian like
pets remember this was the standard
equation the posterior probability that
I went through some states see an IP
would be computed like this all I'm
doing is I'm taking this likelihood that
comes out of the hmm calculation and
flattening it so k is usually a big
number 2 5 10 20 so you take a very
sharp that's it you flatten it up right
and why do we do this I've no good
answer but it seems to work great in
speech recognition so we said let's try
to be here and so here's a dataset
either the carrel set which has 375 very
words this is this hmm state space angle
there or the click when they decide when
we hit 75 with them and in this case I
use the full caption or the bagra melon
because in the previous flight remember
those were the best and for the track
adjustment with the bagra melon and
again there's a substantial improvement
in case of the corral and this mine mind
you is now starting to catch up with the
best numbers are apples so the best
known was going to six and and for the
cliq bit again very small improvements
so I think we have a lot of work still
to be over here so at this point we
decided let's focus on the drag task it
is definitely the hard
the two and we went on to that for me
right so the next step is adaptation so
very quick two minute tutorial on how
speech recognition design application so
remember this is the Gaussian density
model of the hmm probability making a
property of the image I given that
you're going through some states see and
this is the Jade training image so the
way you do hom cleaning is you to
maximum that you're claiming you
maximize this lie clear of all the
training data in other words the muse
and the signals of the Gaussian
parameters there's one in vector for
every mixture component of every state
there's one covariance matrix for every
mixture component of every state so
there's a whole family and you estimate
everything together over all the
training images to come up with the
maximum likelihood estimates of the
kitchen and parameters so what you're
doing ml allowed as you say well we
won't give these models all this freedom
of having their open official means and
variances if I have a good guess of the
means and variances from some previous
system I'll simply look at a fine
transforms of the me in fact some of the
people who just left they developed this
endeavor an SSRI and some other
chemicals and so now you're just
estimating a transformation matrix a and
the Vice vector V to update the Gaussian
meals and so what this means as this
model is lots of degrees of freedom do
drugs from data but if I have a little
bit of new video from the new source not
enough to retrain my entire hmm I just
escalate the matrices envy matrix chain
vector B so this is called MLL our
maximum likely linear regression and the
other thing to do is to say okay if I
don't have a lot of data what about
about setting my model that thing to do
is just something kind of case yet so
let's say this probability the
probability that
I was in state see at time T in the jet
training image is gamma DJ of see then
how do you normally do the updates of
the edge event parameters it's like just
by finding the mean of a bunch of
samples so I saw sample eckstein coming
out of state see with this probability
so there's like a fractional bound so
this is the samples that came out of the
state x number of samples that emergency
so this is like your it should remind
you of the way you usually do estimates
of needs right so this comes out of
summation of calculation because work so
what you doing map estimation is you say
okay let's add tau times in no need
let's add a count of power to do now
it's like saying that if I don't have
enough samples to estimating the B
pretend i have seen cows amples of my
ordinary so that if i see lots of
samples of the new data it will
overwhelm my old estimate and give you a
new one on the other and if i've seen
very little data from the system that
I've estimate for the tough time
estimated parameter how kind of default
now again these are in case you haven't
seen them if you're an expert in blazing
influenced I stuff right okay so we did
this for video game we have 13 sources
so we decided to model each of them the
way you model a speaker and we said this
is supervised so we have 13 individual
video sources valid data collection and
so we went ahead and did this hmm
training so what we did is be sort of
built up the HMMs first for everything
like all the images of all the sources
for all the concepts and then we chopped
up the data into the 13 million subsets
one from each source and re estimated
the hmm parameters on each subset so now
I have a separate recognizer for CNN and
a separate one for a pc and separate one
for Xinhua news and separate work for Al
Jazeera and so on right it's just a
recognizing the same sort of concepts
but their parameters are now teamed up
to a particular source so here this was
the unadapted system in by the way now
I've moved over to the 10,000 in five
flex set that's why the baseline numbers
a little bit as have the number of
queries so the way that 175 trek task
was set up there were 39 concepts
labeled in all of the training data of
which 10 were no night of time to be the
ones will be evaluated on the test data
so we took the training data I chopped
it up in the way they can be development
in the tuning set or whatever training
of the developer tuning set and then
these are trained on the training set
and evaluated on the fuming set off the
training league so again there's a
separation between training tests but
all of this comprises the from the next
point of view this was our training data
and they were going to give us a
separate test in asset later on which
I'll show you results so on this one on
all 39 topics around 39 queries the
average 23% the 10 that were going to be
evaluated on actually turn out to be a
little harder and you'll see later on
why because you see specifically what
they are now that it's a little bit
lower but the bottom line is this map
adaptation gives you significant
improvements in both cases and by the
way whenever I say significant I mean
both statistically significant under a
two-tailed t-test and also significant
in the sense that when you look at the
top view images you can tell that you're
getting better imaging so both a
qualitative feel like thing and this and
this one gives you almost mode so it
turns out m ll RS to constrain you do
have enough data I need source that you
can be more detailed estimation of your
parameters right yeah
ceragon reads and vs whispers yeah but
in the results i'm showing you we only
happened the woman's written test the
variances and we could do it we just
haven't had time to get around to doing
it and in fact this thing was send out
just end of this remember when was a
deadline point to remember something and
then I'm not going to communication so
stupid alright so we're getting some
games out of adaptation all right good
next the next big thing that has
happened in speech recognition is people
have figured out how to do
discriminative training of these hidden
Markov models so the idea don't just
maximize the likelihood of the training
data the annotated images that you see
or the annotated transcript speech that
you see try to maximize the likelihood
ratio between the speech and it's like
it's correct transcription in the speech
and its most likely wrong transcriptions
save transform finally likely ratio
criterion to estimate parameter London
like people call this maximum mutual
information estimation as opposed to
maximum likelihood estimation so under
the papers it can be fairly standard by
now right and so we said okay we should
do something with that but in Silicon of
the the training of this hmm parameters
using mm I'll it was a fair bit of code
development and so on and we said okay
we'll get to it eventually but for now
let's do something quick and dirty and
what we did is we said just take all the
training images and pick a concert sky
and set aside the training images that
have sky and take the images that don't
have sky train one hmm with only images
having sky another one with image is not
having sky and now the underlying
interpretation of the state is no more
individual concepts it's just some
hidden variable which helps you model
the image so we've lost that underlined
interpretation of the state lab HMMs now
for sky one estimated an image is having
skies
perhaps back and then when i get a
testament i simply compute it slightly
under both and I classification if it's
sufficiently high I say the image has
skies it's not a dozen and if you want
you can rank images based on button on
they have sky animation is likely
prediction right so that's where agree
with that so if you want to retrieve
images for Tiger you complete all test
images for Tiger all estimates for a
model with no Tigers take you like of
iterations or right so that's the symbol
and in this one this is sort of a little
bit more formulism so we clean up one
set of HMMs only from images containing
the concept see another set of hmm true
image is not containing the concept see
so the states of this hmm and the states
of this achievement nah nothing to do
with each other with this beautiful set
of states and gassing parameters and
then when they get a test image we
simply compute it slightly under both
these models take the ratio and he has a
nice part if you care about
implementation since these marginal
probabilities are well approximated by
this max the best part in the HTML what
this means that the 500 concepts have
200 different hmm and I need to
calculate the likely testing agenda at
least 200 different hmm i don't need to
run the schematic different times but i
can do is i can stack up all these HMMs
and parallels composing achievements
gives you another hm now and then the
way can arrange it is you can do a
there's a version of you typically
thinks they give me not cool slightly
200 most likely us and what you get is
you get sort of a path through each of
the 200 HTML so in one shot you get all
these likelihoods for all kinds of pants
rockers so the decoding time is just the
same as before and although you're not
calculating likelihoods of to be you hmm
so it comes out fully fashioned so we
really have the ability so it scales
very nicely done nicely but we can index
I think there are 30,000 75,000
keyframes in under an hour maybe 40
minutes or so so very scalable self and
this again it seems to work fairly well
so if you remember these were the
numbers for the 39 concepts altogether
the subset of 10 with the adaptive
system now there are two different ways
of sort of making this discriminative
model why does it just separate with a
time two sets of images and trade models
bleach the others to train a common
model this is what I say background in
it and then take the model estimated
that way and then we train them further
on different states so if you know hmm
is you know that initialization is
sometimes important and there's another
instance of it so if you initialize more
carefully you get a much bigger jump
again another twenty-five percent so
improvement in the total performance and
these are sort of our best and worst
rating so we've got it got a long way
there are few other things that oh yes
let me do this let me let me go through
this very quickly this is yet another
model which another student God is
working on and he said we've been
ignoring all this audio that goes with
this broadcast video so why do we do
that let's factor in it well once you
have this hmm now I not into the
discriminative framework
let's go back to the old one when and
light states and meaningless concept so
let's just assume that the image vector
as well as a bag of words was edited by
the state so now you have the image
vectors and the world records so each
word back there is a bag of words you
son copy p.m. 25 and I see other people
the audience awaits quit this job so I
want to tell you how it's done and
essentially you now have a to
observation process or a pair of
observations or a stream of observation
and the underlying states if the
calculation is going to pool of
different and Brock was able to crank
was through and these are the kind of
numbers he gets so this was the average
position point 1891 a little earlier and
in the text gives him or gives us a bit
of an improvement again this is
significant and so what we're able to do
is model boats what kinds of image
features are seen and what words are
being spoken by the news anchor this
mind you is a set of this automatic
speech recognition and in case of
Chinese and Arabic it's actually
followed by automatic translation the
segment of garbage a piston
and here you can see why this guess it
is a little bit harder so there's one
concept prisoner on which we completely
helpless just come here and find the
reason there is like there are very few
training example that's the statistical
no real reason but even otherwise it's
very hard to tell when the person in the
image is a prisoner it's sort of
something very deep this model is no
hope for getting it in the near future
unless you give it a million example
give me just the prisoners and then
million other people but not so many all
right so these are the kind of images
you get in this one of their Trek
mid-2005 task so for building bigger
these five at the top and this one is
actually the roof of a building and you
are and these would happen to be
buildings cars our model learn that if
you have a nice clear background with
some stuff in the middle must be a car
so this one happens not to be a car by
the way we got credit for this because
the trek database the evaluations for
the whole shop and this is the key frame
in the shark or the shot actually had a
card so what I'm showing you it's sort
of a representative key frame from the
video so don't go by this this was
picked automatically there was a car in
there the likelihood this being high may
have been because of one of the other
stranger US flag these for a great
because they actually have flies in the
background the ceremonies sports we had
tremendous precision on sports one
hundred percent till several images and
part of the reason is that these are all
from the Chinese source that we have
again this must be something to do with
the color schemes and in order
waterscape you see both blue sky BET's
kind back down to spine
again this may have to do with like I
said don't start listen aiding the
listing will actually understanding the
end it's just doing matching in this
clever back and education okay other
results because this is the summary so
when we started what we had at the end
of last summer would have given us this
on the black mask the visual plus tax
give a slight improvement and we
discriminated like your ratio based on
give us this and we were comparing all
the while with that colleagues at IBM
will actually help guess which traffic
research effort and their best system in
2005 this is by the mainland I had
somewhere in its ideal angle I ain't got
enough that's the skill of so we are
starting to be the people who happens so
let me just conclude by saying we've had
some success applying ideas from speech
recognition into this class we are
getting a clear exceeding here in the
yard I don't want a very strong claims
because in the latest track competition
it's not supposed to be a competition
the latest track evaluation they were a
group they were a couple groups my BM
and Belgium Holland who used all sorts
of fancy money past systems with SVM's
and knows what going to take me from
much booty all the papers who got much
better than us so why should you point
you do there Oh point three or sides
that a good so but they have all sorts
of fancy processing on top of whatever
anyway there are many things from speech
that we haven't used it in fact i said
my biggest out is that we can actually
build adaptive systems which won't
require lots of label data of the kind
of video or image
run across a few examples and maybe more
example towel just to complete
bootstrapped without the poop is all the
guys in the next developed so this is
our hope for the future thank you
questions comments jokes right thank you
yep you have to think about improving
this further if what is it is important
lower level features or the module
itself so I don't I dependence ok
question is what's more important the
low-level features of your body I'm a
model guy so I'm going to pay attention
on you to the model but I have no
illusions that that's where the shoe
pinches the big thing that these things
is essentially you made it somebody just
asked you know what makes I sky right
what makes it track the track so we have
to have features which are invading
these features are very lazy so other
people in mind University are working
one making up more fancy features you
know edge detector multi scale
representations trying to factor
case is closed in this manner there's a
North quirk of their own which I'm not
on it man it is give me good features
give me in gaining features give me
features which will be the same value as
soon as as a face in the image no matter
whether its spell it or dark side down
till did and you can give me that I can
find the face what particular say that's
easy but now that I think is a favorite
of modeling left on top of it because
description you'll always be wise and we
have to make inferences essentially from
Nigeria so the modeling part takes care
of that and then a favorite of which
might go hand in hand with the model
mine doesn't have to be compartmentalize
so the answer is but I haven't time
opportunity I mean I think questions not
apply</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>