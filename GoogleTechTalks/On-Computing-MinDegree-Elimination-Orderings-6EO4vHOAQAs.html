<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>On Computing Min-Degree Elimination Orderings | Coder Coacher - Coaching Coders</title><meta content="On Computing Min-Degree Elimination Orderings - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>On Computing Min-Degree Elimination Orderings</b></h2><h5 class="post__date">2018-01-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6EO4vHOAQAs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">triptank from georgia technology richard
this gives the at CMU then spent some
time at midn is working on the design
and analysis of efficient algorithms
well thanks for the invitation
introduction kuba so today I'll talk
about computing in degree orderings and
this work is joint with a couple of
people at CMU and up co2 at Georgia Tech
so Massey of Harvick and sort of Salani
are both grad students at Georgia Tech
Karen Miller is at CMU and routine wanna
action country are his well current and
former students okay
plan for the talk is I'll talk a little
bit about what is mean degree ordering
and what is the motivation for the
problem I'll talk about how to sketch
decrease in implicit state so the Gauss
elimination state of the graph using l0
estimators and then to finish I would
like to file a report about l0 estimator
discuss about some of the issues Ren
trying to be using them theoretically
but we don't really see them practical
so the problem that's really motivating
all of this is what I like to call the
oldest open problem in algorithm design
but one of the oldest which is I want to
solve a linear system faster than Kozma
managed so there has been a variety of
methods designed for both the general
and specialized cases so there are
things like pass matrix multiplication
iterative method there are context
article matrices which is that you are
you you assume that your matrices are
sum of large low rank objects and from
that you get to things like fast
multipole and then you get what I call
their FFT type algorithms whole bunch of
methods designed for this but for this
top what I will focus on is surgeon is a
general symmetric positive-definite
matrix which means that the method that
we really can use is cos elimination so
the idea of Gauss elimination is Ivor's
I have a bunch of equations and what I
try to do is I could have repeatedly
remove
variables by expressing equation one
variable in terms of the other variables
and it's and replacing all occurrences
of that variable with other ones and
what its leads to at least two basically
adding and subtracting collisions from
each other which as you might imagine
the equations have in the case of sparse
equation so equations with zeros in
different locations you get a lot of 0
plus 0 which means that you get a lot of
other entrants so you get what's called
its fill which by its own essentially
defines the entire field comment world
of scientific computing which tries to
come natori Oly measure this increasing
fill using graph theoretic and other
combinatorial techniques so you get this
example on the top right which is that
if i do it if i start off with a matrix
a with a star city party so you can
think of this as just the non zeros of
the eirick now zero in the matrix there
is a dot and each other towards how to
alaska factorization which is a specific
way of running Gauss elimination you get
the you get to that method on the right
and the reason is called fill is
essentially your Europe if you can be
thought of as I'm filling in I'm just
doing some kind of coloring and I'm just
putting plotting extra entries into this
matrix and the degree in a new matrix
this is a specialized permits called a
fill degree this is just once I once I
remove some vertices what her de tigre
self everything else and the assumption
that I will make here is that this
things are working in the absence of
numerical values so these almonds they
don't know what a numerical structure
they've been actually getting a very
good a good motivation for this which is
that for something like interior point
method your you want to create one
ordering which then you do all the
eliminations and the numerical
structures I could changing at all steps
so it make sense to think about
generating a one ordering that
guarantees you that there won't be a
zero here and the reason that such
orderings are very useful is that they
like to do things like layer the cash
out there they like to know in advance
how much memory and so on you can be
using so it lets you optimize directly
the cash structure of your all word and
etc one question if you had the matrix a
was symmetric and so you're able to talk
about cholesky is this focusing on
symmetric matrices or
yeah the entire thing is symmetric
positive semi-definite matrices so I'm
not focusing on null spaces I'm not and
basically I'm going quite a data view
that as combinatorial as possible and
then asymmetry also lets me have it also
the symmetry gives me the rep the
correspondence to undirected graphs
positive semi definite I don't have
zeros in there so I don't have a deal
like no ranked objects I'm having to
deal with no spaces separately I see
it's just trying to avoid the numerix
because this whole area of like tracking
non zero is one way to deal with it it's
just you just come to completely assume
away there's all the enemy mobs going up
I will do something slightly different
the next slide which is that if you do
want to do something involving your
merits there is a very exciting line of
recent works what's called sparse salsa
nomination which is that our structured
matrices they are able to improve
balance for something that looks very
much like incomplete velocity so you
sexually show that for for something
that's for a class of matrices known as
graphical assets which are very much
which basic text is connected to graphs
one step further in Adeje assume that
the numerical structure actually
corresponds with my combinatorial
structure and essentially the algorithm
is randomly permute the vertices
eliminate the vertex add some nonzero
zakaria
among its neighbors randomly repeat and
they're able to prove that this provably
gives you all that runs in nearly linear
time and the picture on the bottom left
once against that
eliminations is that fill picture and
the reason that such a primitive under
the painting one further aside is that
there's also a hole so this kind of
routine is getting used a lot in modern
algorithmic graph theory in that there's
kind of proved nearly linear time
solvers for linear systems having to do
with grass I mean first there was a
theoretical result says they can do
nearly linear time now there's actually
a package called the passing JL that I
could give you access to reasonably fast
solvers I mean
they would have beat like 1 million
sighs system is able to beat a lot of
existing packages it's able to beat
things like not too great and so on
electric to things like elliptic systems
electric do a spectral clustering heat
kernels and also logic do things like
flows matching image processor of course
oh wait where does that help but I will
tell you that basically the real world
grass PageRank beats all of these which
actually there is some there is a big
degree of truth to it because there's a
good reason which is that random loss
for well-conditioned system actually has
a lot in common with submariner systems
there's a way to interpret certain types
of iterative method is just sampling a
large number of length effects so the
the general problem with taking the kind
of correspondence between graphs and
matrices further is that recently they
are worse I'm also some hardness results
let's say set for things like what's
called linear elasticity problems which
is basically I have a 3ds shape I want
to know how hog deforms for things like
two commodity flow which is also a
fundamental automatic promise not to
unlike max flow also for things like
image T total variation image denoising
oh by the way that has to be the
isotropic version not the anisotropic
and as a propagates just max flow so for
problems a very much graph like it's
actually tricky to actually solve the
linear systems from them efficiently so
the linear system from them actually
complete for general symmetric pulses we
said they're semi definite linear
systems so that was kind of one of our
motivations for thinking about just
solving a generic system of course the
other motivation is that we were trying
to implement the King Saturday one
algorithm and we arrived at some point
we still had to switch to a direct
method and switching the direct method
is still calm brings up the question of
how do you efficiently compute a good
elimination ordering and how to
implement code elimination or and how to
get the concepts they were always in to
talk with a algorithm that does the
exact method that doesn't do any
randomization
get tough with all this cash optimizers
and so on so that leads to that led us
to think about these things come
completely from a comment for your
perspective from which there is a very
nice characterization of theory so
infinitely advertised what it
corresponds to is corresponds to getting
rid of that vertex and a Maui's
neighborhood putting out a click so
adding a click to it everything that is
adjacent so the one way to think about
this is I remove the vertex view and I
connect all with neighbors VI BJ and the
problem with this view of things with
this company the combinatorial view of
things is that they're like a hardness
results here so this is the area where
we can no more hardness results than
algorithms in optimal ordering is
altering that would create at least new
edges that either the least no edges or
the total work in creating all the edges
are several ways of defining off there's
actually other works there's a I said a
whole lie on work that says that every
version of this in the user access is
np-hard
ok so what people resort to is some is a
heuristic forming degree heuristic the I
gave a minty realistic that I always
eliminate the vertex that has the
minimum fill degree sounds very simple
and it's known to and in practice this
works pretty well for things up for
basically any sparse matrices at most
parts matrices with say 10 to the 4 10
to the 5 noun zeros this works
reasonably well again fill degree is was
I eliminate some vertices right I have
some new entries it's just a degree
ended in a setting with the new entries
so just every time I go comma matrix I
look for the cheapest variable to
eliminate but it may change during the
time product yes do you favor take a
role can change this yeah this is
basically constant makes this problem
interesting is that the degrees are
continuously being updated and this is
actually used this is used correction
here regards miles basically give me a
correction here is actually not using
lapack it's using sweet spots is using
sweets bars which is basically the
scarce version that is for tents
matrices this is basically the tool
underneath the matrix vector solves any
things like MATLAB Trulia and so on so
there's a lot of algorithm that I could
call them in degree heuristic of course
the more interesting questions can
actually get parole I got approval
approximations to the minimum fill and
since I can't get all that efficient and
nearly optimal in the sense that it
produces almost optimal filled except
that the fact that mean degree runs so
fast meant that it kind of makes sense
for us to even to call so go study the
mean degree problem by itself and the
reason that mean degree problem by
itself is what it was interesting to
study this problem is that there are
some there are some very interesting
algorithms involved involving what's
called the fill graph or intermediate
state of Gauss elimination so there is a
result by Gilbert and then Peyton which
is kind of one of the foundational
resulting comment or sanity computing
which says that with Union fact and
given any ordering I can actually
compute the fill of each vertex so how
many new neighbors you create for every
vertex you can actually do that in time
that M inverse album of M and universe a
comment is coming from Union fact sowing
time that's nearly linear in the number
of nonzero entries you can actually go
and compute exactly how much fill you
generate at all steps and note that the
amount of fill your generators N squared
total work you do might be as higher
than Q but there does exist these highly
efficient algorithm using data
structures that's able to implicitly
look at the process of elimination and
actually compute you how much fill you
generate across all the steps and it's
based on the following representation so
it still takes the original graph it
doesn't look at them so the reason I
keep on saying things like fill degree
and fill edges is because I should never
actually constructed the world I want to
do this all implicit so in the original
graph I have my remaining vertices which
are marked to be green I have my
eliminated bursaries which I mark as
read and you
so in the fill graph there is a fill
edge between U and B if and only if
there is a path through the red vertices
that connect if I now if there is a path
among red vertices that connect them up
and this representation is is very well
by the way just a reminder the pair of
connected remaining vertices that no
mine's a fill neighbor and also top lies
as a filter group and the what makes
this very E and what makes this useful
is that now the first thing I can do is
I can go contact all the kinetic
components of the redbirds
for the vertices that I've removed right
the other eye all I want is up I want a
path that goes from a green to our red
to another green so I can contact them I
can get basically the problem of so all
the fill edges now are just pairs of
neighbors of the red vertices and
because the contraction does not
increase the edge cut this is what I
call the component graph also is a
sparse graph with M edges and the
problem of maintaining min degree
century can be viewed as maintaining
this kind of component rock dynamically
and so so surprisingly when we look at
this problem we found out there was
actually very few results I'm how to
effectively compute computer mean degree
ordering I had a big type of here I'm
sorry I made a picture look more common
I made a problem look more complete than
that thing thing should be so the naive
algorithm is and cubed but the problem
with the naive algorithm is that you
actually spent and squared memory and
the square memory for any large route
that there's a killer so if you stick to
like nearly linear memory the best
algorithm practically gave me degree
orderings now as the multiple mean
degree ordering this isn't this prancing
and squared M time sorry what was the m
inverse Ackermann of him was the so this
is given a watering compute how much
fill their pokey so this is for a
particular ordering like once you've
given the ordering layout the graph
which is also the reason it's very
useful to generate new such an order
there's one more which is called Andy
but with no idea whether you look what
are you does like hey I'm he's not even
guaranteed
produce your miracle ordering this
produces your a Oran cut another mean
degree ordering and as it turned out
there's actually a very good reason that
there aren't too many good algorithm
ports which is that turns out the main
degree problem I mean so this with
hindsight this is with this is this is
with hindsight with a work that's about
fifteen years after both the book like
the work on like MMB and AMD this is
basically the the hardness in P this is
kind of the core Hungarian hard missing
P this is due to Williams and it's
called orthogonal benefits problem the
orthogonal vector problem what it states
is I have a bunch of lens log squared n
vectors and I want to know is any pair
of the more thought now there exists a
pair of vectors all these vectors are 0
1 as well that there exist a pair of 0 1
vector such that they don't have a one
income and the other way to think about
the orthogonal vector problem so you can
think about the orthogonal vector you
can think of each dimension as a
separate click and that just becomes
does D clicks cover all pairs of IJ you
know inspired vertices what are you're
essentially looking for you're looking
for for IJ that have nouns in your
thought product if and only if they both
have a 1 in in some dimension so each
time a just becomes a creek and then a
pair of them has a nonzero dot product
if I donate in some damage or in some
creeks they're both there so it leads to
is actually a graph that immediately
corresponds to Domino better problems or
the partial state of Gauss elimination
what you do is just you just for each
dimension you create eliminate vertex
you put edges from the eliminated vertex
to the remaining vertices for the
affordable entries that are one there
and what this proves is a proof set
basically finding a vertex of main
degree
finally awareness of integrating
anything sub-quadratic time is actually
strong exponential time hypothesis hard
it proves that it is very unlikely that
is even there's a sub quadratic
algorithm for this well we also
that we also show up I mean so this is
actually given a partial state and this
partial state is very unlikely because
all the red vertices at a very high
degree so then we went through a lot of
work trying to create such a state
so in our reduction what we actually
gave is that we gave that it's unlikely
for there to be a sub m to the 4/3
algorithm for finding a minimum degree
order so even in the exact case there's
a lot of space for further working that
we actually show that wow I mean we
don't even rule out of every 1.5 time
over the I mean if you ask till other
essentially if you ask others were
working on this I think people like
Matthew they believe that mmm is opt
people like Matthew Farva who is a
student GT and I think myself I believe
there's M to the 1.5 I'm not sure what
ringing please but I think I also think
that it's 1.5 like where you have a I
mean it's almost enough starting start a
betting pool amount the idea among the
authors involved and if you're
interested in this a very high talk
about it offline but there seems to
still there still gap even in the exact
case excuse me
what do you mean by saying that by being
paradox of mimicry is wrong ETH heart a
strong it means that if I give you a
partially in in the native state so give
you a graph on these graphs with red and
green vertices so you want to say
getting better than quadratic would be
said hard but yeah in time faster than
yeah yeah but how exactly is the problem
statement so this is just okay I think I
skipped a step here that's easy thanks
for calling me out on that so the so the
reason that this is this is the case is
that if you give me one of these graphs
right I ask you does there exist a green
vertex with fill degree is less than n
minus one so so if yes so the better
problem is basically come on to asking
English pilgrim is every single at
present
so finding min degree right that's
basicly asking is the magnetic rate at
minus 1 or less than a minus 1
I'd escaped like I skipped that step
here so I want to check if every edge is
present and that's equivalent to asking
is my minimum degree n minus 1 or a
minus 2 because the graph is represented
as a sum of cliques
yeah ok so what we do instead is is that
we're based on this realization that
exactly mean degree is hard what we do
is we do two things we first give an
output sensitive algorithm because
usually when you do running degree the
max degree of what your pivot tends to
be fairly small it's not it's not going
to be as high as M if you do it as high
as n your matter got run then starts
elimination so we give our running high
that's roughly a Delta this is ignoring
mark factors and then we also give
what's called a epsilon approximate
greedy mean degree and this definition
is kind of interesting in that what we
say is that the epsilon approximate
greedy what it means is just that
instead of always pivoting on the vertex
a minimum degree we pivot on the vertex
whose degrees we pick up any vertex
which degrees and lesser or equal to 1
plus epsilon times the minimum degree
and to do this we use a whole bunch of
tools we use sketching sampling
randomization I will say that the way we
combine them and the issues that we've
run into we're accusing these sketches
is actually probably more interesting
than a problem we solve itself and I'll
get to that hopefully about 15 minutes
so heavy try to climb in this heuristic
sin actual elimination of verses from
there so the problem with a lot of the
also
Foreman's you mean by an amount of fill
or no I just mean like total running
time if you just implemented heuristic
or we're gonna lose if we just matter
total running time with like a
straight-up implementation we're going
to lose and part of the reason we're
going to lose is that the exactly
elimination they're much faster because
they're able to lay the vertices in
memory much better there's cash backing
that goes and the part of the biggest
reason you'll want the elimination
ordering is you really want to pack the
cash but if you're the epilogue they're
taking n squared M type of time and
you're getting much under that then for
large instances they're actually
experiencing that worst case bound oh
you're not you're not experiencing that
about a lot of cases that's the other
thing because it what the N squared M is
really it's a sum of degrees squared or
something of that type so of for a lot
of if you start off the graph sparse
chances aren't amount of work that does
it's probably not much bigger than the
actual this is also there the reason
that this has entreat being humble
I mean it's sufficient other runtime
bottleneck people came up with AMD AMD
is known to write n times M not the
bound that we know for AMD and for AMD
the thing is because I mean these are
the mention typically take more than
quadratic time anyways so because
there's analysis per elimination right
although there will be M log to the 5
this is amortized this is the both of
these although there is quite a bit of
amortize so let me first talk about the
exact outwit which is the Delta cap
Tower and the key idea is basically this
idea from Cohan called
l0 sketches this was originally used to
estimate the size of reachable sets in
wrap so this is the original application
of this is given a bag give it a tag I
want to know for each vertex
approximately how many things I'm able
to reach and the philtrum is not too
different out that you know I can create
another copy of all the remaining
vertices
point them also the remaining to the to
the remove vertices today the native
vertices and they have the eliminated
vertices point back to all the remaining
vertices and then those things
unreachable in two steps that's exactly
the one that if I orient all the edges
downward it's exactly all the edges that
I can all the vertices on the top level
that can reach my current vertex on the
bottom and the weight the way that l0
sketches work
it's just each of the remaining vertices
you get a random very get a random value
X U and the static algorithm for l0
sketches just that every vertex then
propagates along the minimum of
everything that can go to it essentially
for every where does you track the
minimum random number that can go to it
and the key idea in analyzing l0
sketches so there's so that way that
propagates is that on first on a second
level then X each each component vertex
tracks the min of everything that could
reach it and then the third level so
that basically all the remaining
vertices things then look at all the
component neighbors and compute this
value the reason this works is a minimum
ignores duplicates so in fact what
you're doing is that this generates an
also bite because of the symmetry of
randomization this generates you a
uniform random entry from everything
could reach you so in other words if if
a vertex has filled agree Delta that's
what we filled agree by the way so this
is if a vertex has the only thing that's
able to reach it they're just like
straight up Coulomb coupon collector it
ensures that if I have maintained Delta
log and copies of l0 sketch is infer
that we get all the neighbors it's okay
to explain the random thing in where is
it ready so if I have a set and then
every number picks a uniform between 0
and 1 right just by symmetry just by
symmetry the minimizer is random amount
set okay so nothing in the independent
adjusting for each guy it's a random
yeah for each one it's a rendering 0 1
and then I propagate these along to
realistic sets and the minimum the
reason that
zero takes the min is that it ensures
that if I have two copies they they're
just both the same so with a little bit
more turn off we can to get approximate
degree by having a lot and copies of
this so there's a little more work to do
to to get this to get this to go through
later structured sense because when you
remove a vertex you what you need to do
when you pivot out of vertex we need to
first you need to remove your label from
all your neighbors labels and then you
need to contract together the lists of
all the remaining birds you can become
merge all those see vertices into a
single vertex and then what we do is
eager propagation which is that anytime
you change your ax min changes you just
inform all your neighbors of it and this
appears to be degrees squared except
there's a little bit of amortize
analysis you can do speeding up here a
little bit because I want to get to the
next part and the main idea is just that
if I merge a piece of size two there's a
backup something how backwards analysis
which is that you can actually analyze
the expected number of propagations that
emerge from m1 merging a set of size m 1
to a set of size of size and 2 it's
going to incur on everybody
yeah is that you can think about the
randomization so because we have a see
fixed sequence of mergers you can think
about these murders as i have emerged
now i go we generate all my randomness
and i see how momentum is effects and
how it cost the number propagates you
have expected update cost and then you
set up amortize analysis you set up a
protective function which is your
standard your standard amortize
potential function for merging things
which is size of the set times the log
of the size of the set and you can show
that the expected cost is about order M
log and over all the mergers there is
slightly tricky knees because there is
deletions going on here so with the
deletions is actually possible to have a
single set that's actually gonna be
that's gonna cause a lot of informs that
you could have a single set
something like n informs it's not like
the case with merging smaller into
bigger where you'll guarantee that
anything coming causes spot log and
influence so if there's a slight
trickiness but Emmett has analysis and
backwards analysis does all of this okay
so now a questions about the data
structure I'm kind of skipped through a
bit of amortize analysis because the
main gist is that because these things
are generated randomly it's able to get
a very good amortize one time and then
the better how to use it so once again
we're back without your coupon collector
so l0 sketch which for every word has
generates your random neighbor among
still member if I ran demand tree among
its fill neighbors if our vertex has
degree Delta then you're guaranteed
after generating something like order
Delta log and copies ensure that you get
all the neighbors so you can also the
other thing is that the the main degree
sequence after some lexicographical
tiebreaking that unique so this is a
sequence so this is very important is
that the sequence you actually want to
do pivots on is that you can dependent
about all the randomness and that
ensures that you have a fixed to
deterministic sequence that you start
with and then you have apply all the
previously discussed advertised analysis
and you can even get things as you can
get a bounder that's like Delta M log n
use because just because you maintain
causal copies of this is actually Delta
M log squared n you can even adapt if
you choose Delta so there are graphs
such as the square mesh or the cube grid
where the Delta values only goes up
towards the end you can keep get
balanced or something like some of the
other I or something of that form so the
more interesting thing is what happens
if you apply this approximately so
within the same paper by : : didn't do
this to just compute as reachable set
size Col Israel actually was trying to
approximate assign reachable sets and
there the result there without is that
if I take the if I take a whole bunch of
copies and I
that one minus one over e quanta which
is that if I take a bunch of numbers and
I take a certain quantile so if I take
log 10 of these that I take 1 minus 1
will be time logging I think of the case
smallest if woman has one we use a bit
of a calm type number thing of median
median also works but it's concentrate
around something slightly different that
I could serve as a surrogate for 1 over
X sorry it's a it's a concentrated
around yet it's concentrated around the
size oh yeah if I have X copies of this
0 1 variable that quanta is gonna be
concentrating on one around 1 over X so
what I can do is I can take the take a
whole bunch of copies and for each words
I can take a whole bunch of copies of
this l0 estimator and then each vertex
can track 1 minus 1 over ye or like to
think about median across all these
copies and invert it and that's going to
give me a approximation to the fill
degree of this vertex cost this is like
my cube 1000 perhaps on squared and the
result actually what it gives you is
actually gives you a data structure that
approximately buckets all the vertices
based on like fill degrees so for a
fixed sequence of pivots
I could actually produce just directly
using l0 estimators actually can produce
you a bucketing of all the vertices so
you actually produce use the grouping I
mean so what's the in particular this
does produces it produces you a Berta's
of approximate minimum degree we're done
right and this is where
I can file my bug report questions
before I continue or those who haven't
mentioned this bug joke can someone
guess what the bug is this point so what
I'm gonna do is I'm gonna try to
generate a approximate redeeming degree
sequence from this so this data
structure already produces me the
approximate bucketing of the vertices by
their fill degree and all of the do
writers I just take the minimum I pivot
out and the problem is that this input
series so the next vertex exhibit this
choice of the pivot is that it depends
on the random is using the sketches so
the random is in all the l0 sketches
that actually decides what the minimizer
is it decides the pocketing so I get
into this issue that it breaks the
requirement of my data structure which
is that my data structure says that all
my pivots must be independent of the
randomness of my data structure my
half-day sequence needs to be
independent of my data structure and
there are steps in this I mean I thought
oh I'm fortunate I went through the l0
sketch and assets very quickly but both
the coupon collector upon hire estimator
and also the expected one time analysis
actually strongly hinge upon
independence so in fact I would actually
conjecture that the following algorithm
works this is so this is actually so the
other kind of really have really
frustrating about this is that we
actually couldn't find a case where we
could break it in fact we could even
break the simpler version of this
algorithm which is that each remaining
Burnette just picks a bunch of pick one
label uniform random being 0 1 and then
Y propagate everybody is trying to learn
a lot in minimum I was I tried to order
log n minimum I just break I just break
ties by just picking the one whose
medians bigger or I can do 1 1 y -1 will
be a quanta in fact what we couldn't
even
so I mean as a bug report writer I
should give you the minimum failure
example such a analysis even when we try
to use this to approximate the size of
sets which we couldn't we couldn't find
a dependent data structure like we
couldn't find a dependent sequence of
operations like I see was the operation
that depends on what is the main size
set and then does Mirjam and so forth
set what this corresponds to is each
element gets a random number between 0
and 1
each set tracks and smallest login'
labels and i would say a set with bigger
median of these labels is smaller doing
particularly all than that we could not
break yeah do the one we could not break
so first of all this is something we
can't analyzing what's called the
oblivious adversary model which is the
formal name for my update sequences
independent of my randomness and improve
ly the routine oh let's always just take
this set two sets of smallest size I've
merged them right it's something breaks
it's probably that that that that one's
going to break we ran a lot of
experiments I mean had a whole bunch of
flights last summer where I could run
these experiments off and empirically
what that God is that logins about 400
to 500 always gets get to gives you to
approximation for the minimizer so you
can just take one of these sets there
writing things you can just write and
then you can see what's the worst scum
pair to any given time in particularly
one thing we did is we just took like if
you initialize all the sets of size 1
and you start merging where you should
be getting twos and fours and so on
that's the particular instance that we
try to do this with don't not approve
this and the other thing I was a little
unfortunate is that at some point we
actually thought we broke it because a
hundred copies of this does break up so
400 or 500 else a very unfortunate
number so in even practically there's a
question how can we get verges of l0
estimator with better constants can we
get like a number of copies needed for
something like 50 to 100 which is soo
because this for five hundred comes from
the constant it comes from over epsilon
squared times x like a Union bound over
all N squared copies like it we actually
see this
long ferry fairly significantly and this
has got us on to the wrong track because
for a long time we actually thought that
hey like the actual oh the actual
oblivious adversary data structure
actually doesn't work so why'd you have
to design something new which i think is
my excuse for what you're about to see
next which is significantly more
complicated way of dealing with this so
before I continue I should say little
few more words about the oblivious
adversary model this is a very popular
model using dynamic graph algorithms in
over the last five years basically as
soon as people started putting sketching
in conjunction with data structures I
started getting used as I mean the paper
I really got this as an idea started was
a result by Catherine King and on Troy
that was this deter this worst-case
Polly longtime dynamic connectivity data
structure and the idea is that the
adversary can choose the graph and the
sequence of updates but what I cannot do
it cannot choose these updates
adaptively based on the randomness
that's guiding the algorithm and this is
a list
refer to that actually follow this model
so things like connectivity by
connectivity maximal maximal matching
dynamic spanners dynamic spanners
actually predates the definition of this
model and actually the people who did
the dynamics patterns were actually kind
of responsible for also kind of
responsible for doing this for matchings
as well but the spanner paper were
actually particularly frustrating agreed
because they don't even say what model
they're in so you had actually go
through every paper and look at how the
randomness actually gets analyzed and I
mean the last ones also grasp are
suffice so right now there exists a lot
of graph theoretic data structures that
make this kind of assumption and to
illustrate what's the problem about this
let me give you a simple example where a
potato pointer that works under the
ugliest adversary model but it's very
easy to bring on groups with adaptive
adversary once again with an estimate
the size of a set and a way that I can
ask them in the size of a set is I just
pick a random subset of size log N and
the way that estimate the size of s is I
just say take intersection with s and
scale the number back up this can be
proven to be a epsilon and additive
approximation is guaranteed to work
under under a oblivious adversary and
the adaptive scheme that breaks this is
I start off with two sets that's both
all the elements I repeated delete an
element from B and I only keep that
division if the size of deep the
estimated size of P does not change so
what this has the effect of is this
essentially has the effect of I call go
to BI giving everything in here that is
not in K and the specifics the I theta
for sure that works under out of
oblivious adversary
extremely badly under a adaptive
adversary but then again I mean this is
the reason that we actually did
experiments we try to break the sets
there any things but also very
surprising we couldn't break it so the
distinction between these two models
seems much finer than what we thought
was the case as well
the natural fix to us is 10 we try to
introduce some noise so notice that this
this is problematic only because as soon
as we delete we're able to know whether
we have completely messed up like
whether we have found a element in hey
so I mean I don't know I don't know I
don't know much about differential
privacy but somehow the feeling here is
I'm really kind of what I'm really
trying to do is that they don't for them
but really trying to hide cave from you
if there's if anyone knows some
connection there I'll be very happy to
chat about that essentially what the
data third needs to do is in use to
handle curries while hiding okay so the
way that we deal with this is that we
developed a third meeting that's able to
given a single fill graph state using
complete independent randomness
estimated film degree of a vertex tool
so locking accuracy in time that is
essentially log in I mean there's a
degree term there but you can ignore a
degree term and then we define a
deterministic sequence based on this
third estimation so we what we want to
do is we want to de correlate the pivot
sequence and the data right so what we
do is we can find a sequence based on
his estimate degree sequence and the
good thing about such a sequence is that
because we still have this data for you
that gives the approximate buckety
because we know that everything in the
higher buckets right there there's no
way they are the argument we can remove
those it suffices to just evaluate on
everybody that's in the smallest bucket
and but still there's a problem there
which is this bucket can be everything
so then again
another natural fix what if we just pick
randomly from the smallest bucket and
then as then run estimate agree on that
or we just take the smallest bucket and
pick a random entry from it there's
still a problem which is that because
there is error in the data structure
which bucket the vertex falls into is
still dependent on the randomness so
such a scheme still has dependencies
between what's Amos what it returns as
the pivot and but the randomness in the
data structure
I still haven't resolved this dependency
issue so we got fed up and we just
introduced some some we started trying
to format this problem as I want to
introduce another distributional noise
to my estimate degree and the property
that I want this noise to have is that I
want to get it I want to have a
distribution Delta basically this kind
of shift or decay such that it
guarantees that no matter for any set of
approximate degrees so the s-mint degree
sequence for any set of values of
approximate degree I need to guarantee
you that one minus Delta U once I hit
with one minus Delta U it guarantees
that hey I don't perturb things too much
and that the majority of things there's
only very few things that's close to the
minimum this if it's close to minimum
that I can just go estimate and to just
just go call the Esmond degree thing on
Almont and for all intensive purposes we
need to think about this set of
approximate degrees as deterministic so
this Delta U there's actually there's
also very interesting connection here
which is that this Delta U needs to be
what's called pseudo deterministic is
that I do not I'm actually not even
going to generate all in the Delta use
the goal is I want to avoid generating
you all the Delta use so I need to have
all my random is to actually pseudo
deterministic in that all my random list
needs to be a function of time and you
but not be ordering in which I call as
my degree you and epsilon this is like
easily doable with random number
generator I'll take a seat corresponding
to that takes a seat corresponding to
the time the vertex
and a few other things but this is
fairly different than how we usually
think about our notion of independent
random is as well and the way we do it
is it's called is we use property of the
exponential distribution which other we
take our Delta U to be
Absalon times exponential random
variable divided by order log n and this
this div this division by log n
guarantees that with high probability
just by Union bound over a bunch of
exponential distribution guarantees at
the max perturbations at most epsilon
and here we use another important
property of the exponential distribution
which is that it's memoryless the
memorylessness of the exponential
distribution what it guarantees is that
if I condition I experiment about a if a
condition of my exponential distribution
being greater than Y that distribution
once I subtract a Y away from it it's
still exponential distribution in
particular what that implies is that the
expected number of things that's within
water one on the maximum is constant so
what I can do now is within each bucket
I can try to implicitly generate the
exponential distributions what I do that
generate the max I can make the second
max and so on until I'm a couple of
excellence off from the max and just
forget about the rest and then pick them
out of the buckets and there's still one
more issue with it if I take all the
buckets I take all the buckets and I
pick one I pick one candidate from each
bucket and it is called approximate
degree on them there's still one more
problem which is that I can have
basically everything else is being a
very small degree fine I have one bucket
with a very large degree and that one
bucket will get any value at every step
so what we actually have to show is that
we have to have to take one so we take
all the Delta use we generate from every
bucket we globally trim everybody based
on the estimates based on the estimate
that they don't hurt who gave us we go
opal and go trim that so we're on e we
think 1 plus epsilon a constant factor
of the global min
and then we show that if we do end up
getting to this point if a vertex does
survive to this point by the
memorylessness to be property of the
exponential we're guarantee that if we
do call estimate degree on it we remove
it with constant probability and this
then gets finished off with amortize
analysis so to summarize what I've try
to do here is that I gave a very a very
food force way of artificially
correlating the output of a data
structure with the input and this is
done through a third data structure
which by which I define the sequence but
then the with the third day but the
third day torture the reason that I use
the third data structure is that it
causes completely local so I can still
use the approximant bucketing produced
by my global data structure to limit the
cost of invoking the third data like I'm
not calling up processes we everywhere
I'm just calling it on the vertices
that's close to the top of the bucket
and there we use the additional property
that the randomness of guarantee that
that cost is small so to finish want to
mention a few that reference for for
future Network future work I think this
issue with Olivia's vs. active adversary
is that your very real issue the way I
imagine this with a to further people is
that it's only a it's a it's a question
of how long will take until somebody
bites a wrong paper using by messing up
monies or something's actually came
pretty close to doing this like on a
week ago as well your W finding new
standards for worse crossbar suppliers
and so the question there is is there
general method can we develop some
fairly general technique for describing
what does it mean to be adaptive against
random is adaptive against T adapter
against random is adaptive against the
output are there ways of defining how
much adaptivity we're allowing what I
think but even for this problem in
degree apart I mean different I think we
understand it in particular I really
think that one should be able to prove
that
based el0 skeptics work for four main
degrees that will probably require a
fairly a much more significant
Sofia randomization post it wouldn't
guarantee one little shorter paper like
if you look at like the truth we put
together I mean it's I mean what it was
enough not fun writing this but if we
really nice to see a proof of the
elements that's probably actually work
and also even for exactly maybe we can
we close the gap between the upper lower
bounds can we better basically better
approximate mean degree can we get
better exactly degree as well and that's
all I have thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>