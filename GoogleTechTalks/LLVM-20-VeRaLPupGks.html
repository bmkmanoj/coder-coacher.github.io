<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>LLVM 2.0 | Coder Coacher - Coaching Coders</title><meta content="LLVM 2.0 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>LLVM 2.0</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VeRaLPupGks" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everyone my name is Ian Taylor I manage
the compiler team here at Google and
today we're here to welcome Chris Latner
from Apple Chris is the lead developer
technical lead etc of LLVM a free
compiler originally from University of
Illinois he has a team at Apple working
on this compiler they've done a lot of
very interesting technology and he's
here to talk about LLVM for us thanks
Sam
so Mikey and said I'm here to talk again
who here knows what LLVM is already
that's pretty good who here's actually
downloaded and compiled it ok so I'm
going to talk to slides of just
high-level overview of what it is and
what we do great
so OPM is basically a compiler right ok
so what does that mean today we focus on
two major things we focus on an
optimizer and a code generator
components for a VM and a front end and
the optimizer in code generator is very
similar to the GCC optimizing cogenitor
in some ways it's SSA based the back end
targets many different architectures I
think there's 10 major ones the front
end is based on GCC and so it's GCC 4.0
front end and the way that we compile
code is that when you when the front end
parses and converts the trees to Gimple
we then take the Gimple turn into lv miR
from that point we use the LVM
optimizations and the vm code generator
and that's basically how it works if
you're develop C compiler that's a
pretty good way to go some of the
interesting things about LVM is not
really related to GCC LVM can be used
for a lot of different things L games
actually built with the library based
design and so you can take the various
libraries in LVM plug them into
different things apply them in
interesting different ways for example
LVM supports link time optimization it
has since LOV m 1.0 and the way this
works is basically the libraries throw
the optimizer get linked into the linker
and get used to the link time and they
operate on the LOV mir just like the
standard compile time optimizer so it
does another interesting thing about LVM
is that the code generator is actually
pretty modular and one of the features
of sports is chicken
inflation and so LVM doesn't require you
to use check compilation but it supports
it and so one of the some of the early
interests in Alabama has been because of
JIT compilation and so companies like
Apple and Adobe have used this in some
of the products that they're shipping a
lot of other companies use LVM for other
things if you're interested there's a
webpage that talks about it the big
feature of LVM that people generally
like is that it's really easy to
understand it's easy to use it's
well-documented and there's a good
community built around it so if you have
questions about it you're using it for
some interesting new crazy thing we
didn't think about there's constantly
people asking questions I have this idea
what do I do what's the best way to
implement this and people are generally
really happy to help out right this is
like a lot of open source communities
and LVM by the way is open source it's
under BSD license and that's basically
elevator in a nutshell so today my talk
is going to be about two different
things
one is the LVM 2.0 release which is the
first dot oh we've had since the first
one and after that I'll talk about what
we're currently working on and the stuff
that's coming up and and I'll focus on
three main things I'll talk about LVM
2.1 which is plan to be out in September
I'll talk about LVM GCC 4.2 which is our
front end ported to 4.2 and I'll talk
about some new C front-end work that's
affectionately known as clang right now
that's the code name so and if you have
any questions then feel free to stop me
and ask me at any time this is I'm happy
to keep this very informal so I just
want to say one thing about questions
this will be going out on Google Video
so if you want to ask a question that's
in any way Google confidential please
hold it till the end of the talk and
talk to Chris afterward so first up I
want to talk about what's new and LVM
2.0 and new is relative because 2.0 came
out in May right so the the major thing
that LVM 2.0 was focused on is
implementing missing features we're
missing means something that common
software packages rely on the LVM 1.9
and before didn't provide and so for
example there's a whole raft of elf
features like symbol visibility
thread-local storage you simply do this
and stuff like that that wasn't very
important early on but now that people
are trying to build things like QT or
Mozilla
that suddenly becomes really important
it uses that all the time other features
are target specific for example on x86
LVM 2.0 brought now a full support for
MMX we already had us to see support and
other things but MMX is the you know
unloved bastard child with vector
extensions for x86 and code needs it and
we didn't support it now we do but also
brought pic support for Linux reg
parameters things like this there's a
bunch of other miscellaneous things for
example pragma pack soft float support
for targets without FP use precompiled
header support other features like that
these are basically you know key
features that GC is supported for a long
time this is you know fleshing out the
feature set of LVM to make sure that
people can download and compile random
stuff right and it works LVM 2.0 is one
of the is the first release of LVM you
can take something like QT or mozilla
build out of the box and it just works
right before you would have to do little
hacks to avoid reg crammed parameters or
things like that so now you don't have
to do that looking forward there's still
some things that are missing in the
missing features category chief among
those is c++ exceptions so this is
currently work in progress
people in the open-source community are
very interested in this for me
personally it's not a driving thing that
i have to have but it I agree it's very
important for completeness and will
probably have this with LOV M 2.1 the
other big thing is long double we don't
support all the myriad of long double
formats out there this is also important
we will also add this it'll probably be
2.1 or 2.2 something like that of course
being open-source if somebody comes and
really asked to have it and implement
themselves it'll happen a lot sooner
right
there's also a couple other
miscellaneous things like built on
apply' and nested taking the address of
a nested function again when people want
that we implement it but there isn't
high demand so besides implementing just
missing features there's also some other
useful user visible improvements in all
of IAM 2.0 particularly the optimizer is
significantly better than 1.9 so we have
rear materialisation support simpler
materialization scored in the registrar
gator we have the ability promo unions
into registers so if you take a vector
and you have a union with a vector and
array of floats in it you store into
those floats and use the vector
well now that will become a P and
certain W instruction and SSE for
example there's our switch statement
lowering there's better register
pressure minimization algorithms stuff
like that in addition to just better
code there's also a few other
miscellaneous things so compile times
better we now can turn LVM intermediate
representation to ms IL something that
everybody's always wanted and the ADA
community is actually very interesting
LVM and they've been contributing
support for the lesser-known GCC tree
features that we didn't support as well
and so eat it with LVM actually works
pretty well
what 2.0 as long as you don't take the
address of an STD at a function because
that would die so because this is a more
technical audience I didn't just want to
blast you guys with features because
that's not what's interesting I want to
talk about some of the specific things
we changed in LLVM 2.0 that are
interesting from the compiler design
standpoint and so the one interesting
aspect of LLVM is that the ir has two
main forms it has two main on disk forms
as the human readable text format which
means that you can hack out LVM ir in
via it also has a compressed binary
format which is very useful when you're
the length time optimizers talking your
compiler things like that right and so
up until 2.0 we've always maintained
backwards compatibility of these files
and so if you had an l vm 1.3 ll file
for example l vm 1.9 would read it well
since we had this nice you know 2.0
number we could justify dumping that
backwards compatibility auto operating
support and make some more interesting
changes we want to make for a long time
so I want to talk about two of these one
which is signless types and one which is
arbitrary precision integers so in l vm
1.9 the type system in LEM is very
similar to C or like or Gimple if you
have an integer type it turns out that
we would just represent
as assigned int or an unsigned int or
sign long or an unsigned long something
that you know is has a fixed size but
the sign this is encoded explicitly into
the type you know m to point out what
the change is to get rid of the sign
from the type and change the operations
themselves to be signed and so so
someone like RTL you suddenly now have
16-bit integers 64-bit integers and you
have esta view t'v instead of just
divide which changes behavior based on
the type right so why do we do this
there's a couple of reasons one is that
the ir itself is smaller right so now we
don't have to have casts to convert from
into unsigned in right
not having int unsigned int casts make
people optimizers happier because they
don't you know look for two instructions
they're supposed to be back to back but
oops there's a cast in between them that
disables transformation things like that
right it also makes it smaller if you
care about memory footprint of your
compiler about link time authorization
things like that the other good thing
about this is it makes the operations
themselves more explicit a great example
that was hated before at least by me was
cast so cast in c can mean a whole bunch
of different logical operations
depending on the source and destination
types so if it's an integer 2 integer
cast and source assigned well that's a
sign extension well if it's a measure to
float and the source of sign then that's
a sign you know whatever it's there's
all there's actually 10 or 12 different
logical operations and the nice thing
about this changes that makes that all
explicit so you have a sign extended
instruction it's very easy to analyze
and transform the other bigger driver is
that it exposes more it more closely
matches what actually is happening in
the program and so I'm gonna give you a
contrived example but this does actually
happen when you start talking about
induction variable analysis and
insertion and illumination so this is a
simple example where you have basically
an integer coming in you convert it to a
sign integer you do two adds you add
four to both of those you cast it back
to sign you do subtraction okay and so
in LVM 1.9 the IR is basically the
identical thing to what you get in C
right you have a cast that changes the
sign this you
a couple of ads you have another cast
and subtract right this is a very
literal translation this is very similar
what Gimple would give you incidentally
this is what the LLVM textual
representation looks like it's basically
SSA machine code looking operations so
when we went to 2.0 of course these
casts which just changed sign go away
instead of int and unsigned int now we
have just a 32-bit integer right
and so the casts go away the bigger
impact of this though is that now
suddenly these ads are obviously
redundant right before you had adding a
and B where they had ads of a different
type well your you can teach your dozen
C elimination of stuff they handle that
but really you want to just say is it
doing the same operation and the problem
was is that these two instructions did
the same operation but they look
different to the optimizer because the
types were different and the type didn't
matter for this operation at all if we
were divided that would be a different
thing right and so with this the
optimizer can trivially delete one of
these then it decides this is a
subtractive X from itself that's zero
while so the whole function or from zero
this is just again a contrived example
that shows why getting sign out of the
IR was a good thing for us and you don't
lose anything from an analysis
perspective so the other thing this led
us to do is extend the type system to
allow arbitrary precision integers and
so in LVM 2.0 and beyond you can now
declare your favorite I 123 bit integer
right and so you can do operations on
these weird long or really short integer
types and the optimizer fully respects
that all the transformations are updated
to do the right thing with them this is
this work was driven by the EDA industry
and the people doing hardware synthesis
from LOV Mir so for them the reason they
wanted this is that well at 13 bit
multiplier is significantly cheaper than
a 16-bit multiplier or you know being
able to squeeze you know 32-bit divide
down to 24 bits is a big deal for them
right for most people the big win for
this was 128 the integers on 64-bit
platforms and the LVM transformations
and things you know support any type on
any system
which is very nice from the
implementation standpoint doing this is
real pain because up until now we had
all these bit twiddling authorizations
and other things they all use the UN
6040 which was definitely big enough to
represent all the interesting and
integer values right well that's not
true anymore and so now we have this APN
class which abstracts that is extremely
efficient if your types are 64 bits or
blue if it's above that then it handles
them gracefully but not efficiently but
this actually ended up making the code a
lot cleaner because now you didn't have
to handle the idea of I'm doing 8-bit
integer arithmetic within a 64 bit
container for example you just declare
it as 8-bit value and it does the right
thing yeah so the code generally
question yeah the question was does the
code generator do the right thing if you
give it a thousand 24-bit integer and
the answer is no it does not do sensible
things right now it could be extended to
do that we support arbitrary with
vectors in the same way so if you
declare a 128 element vector of floats
we do correctly turn into four element
float operations on SSE for example and
so you can do the same similar kinds of
things so until now the front end the
optimizer support these the code
generator has marginal support but not
wonderful so that's basically all I
really wanted to talk to you about with
LVM 2.0 if you have more specific
questions I'm happy to answer them but a
lot of the interesting stuff is
happening in the future and the hell's
maymays so long ago I mean who keeps
track of any of that stuff right so the
three things I wanted to talk about were
LVM 2.1 which is our next release LVM
GCC 4.2 which is upgrading LVM GCC to
4.2 strangely enough and a new sea front
end that we're working on which i
mentioned before so I love um 2.1 is
slated to come out in September one of
the interesting things about the LVN
project is that we cranked out releases
every three to six months and this has a
number of advantages for us we don't
have to worry about you know sub dot
releases because if we just release
something and your feature doesn't make
it in a new
we'll be coming out really soon anyway
so it's not a big deal the hard part
about this is getting developers to
understand and and cope with this and we
do this two ways mainly incremental
development is a really hard thing for
some people but it's really important
this this lets us do development on Main
Line instead of on development branches
which means that people are more
constantly testing work being developed
by other people on different projects
and we don't have these periods of
instability where huge changes slam back
into the mainline tree for example so
the end result of this is that even
though LVM 2.0 came out in May and
September we'll have a new release out
and we just keep pumping them out so one
disadvantage of this is that each
release has correspondingly fewer
features because it's shorter time span
that elohìm 2.1 will probably include
dwarf zero-cost exceptions for c++ some
more Imbros for the optimizer and
significantly some big speed ups for the
composer in particular the to
optimizations related that store
elimination and load transformations are
being revamped and they used to take 40%
of compile time in certain cases and now
they'll take next in no time so this
will make users happier so if you're all
hacking on all of em today you have
plenty of time to get your changes in
and and ready ready for the next release
so the next the the second big project
underway within the LOV immunity is
upgrading LVM GCC to 4.2 and so this is
the obvious merge process of taking LVM
GCC 4.0 which is based on the apple 4.0
GCC tree and upgrading it to the apple
4.2 GCC tree and we use the apple tree
instead of the FSF tree just because it
has more features and has more stuff and
a lot of Apple people are working on it
the most significant interesting things
about 4.2 for us because we're not using
the optimizer we're not using code
generator remember from GCC our front of
features like OpenMP the new Fortran
front-end the ADA improvements which
have actually happened the ADA and
Fortran communities in LLVM are very
very very happy about 4.2 and can't wait
for us to get there
other features in the new recursive
descent parser for C again since we
skipped 4.1 there's there's a bunch of
new features in there so this work just
started like a month ago the statuses
that it was merged in the compiler
builds all the LLVM stuff seems to work
in terms of it's hooked up right so now
it's a matter of turning through the
bugs finding out the you know
constructor change you know semantics
from 4.0 4.2 and updating that and you
know working through all the bugs and
get to work so if I had to guess I would
guess that this will be right by 2.2
time frame so which will be three months
after September so say December January
something like that
of course it goes faster with help so
the third the third piece is the more is
what I've been working on most in the
last couple months and it's a new front
end for LLVM and so this is a new front
end it's been released open source
recently it's under the LVM BSD license
so it's a new front end for c4 oh yeah
so the big question of course is why
would you bother to do this right we
already have LLVM GCC LVM GCC is great
support C C++ objective-c eight of
Fortran Java you know all the stuff
right why why are we doing this well the
answer has multiple different components
right one component is that at Apple we
really care about IDs and unfortunately
GCC does a really bad job at servicing
the IDE GCC does a good job in
compilation it does a really bad job at
parsing code really fast and finding all
the definitions and uses of variables so
then I double click on something it
jumps to the correct definition in the
correct scope right this is kind of like
the doxygen style of analysis right well
the hard part about this is that you
want to do it extremely fast because you
want it to be up-to-date as users typing
in their window right
GCC just architectural not built up
built to support this kind of use
another thing is that GCC is really not
set up to export an ast so the clients
like source analysis tools can take that
ast grind on a little bit and then
output some interesting information
about it there's been work to do that in
the GCC community
but it certainly it doesn't seem to have
taken off very well and there's other
problems with that the second major
reason is that GCC just doesn't preserve
enough information in particular we're
interested in full column number
information in GCC has two different
ways of map of representing locations
and neither of them actually preserve
all call number information they both
saturate if you go too far to the right
for example the they dynamically adjust
so it's unlikely to happen but it's
still a problem the other more
significant problem is that as it purses
GCC passes everything through full right
and fold does all kinds of interesting
useful wonderful simplifications but
they st the trees that you get out don't
look anything like the source code at
the end necessarily and you can't
disable this either because if you do
that then the parser dies in mysterious
strange ways because it assumes that
basic things are getting folded which is
extremely frustrating and so if you're
writing something that wants to you know
look for all uses of x and the source
code contains x minus x and you go in
you're factoring tool to rename x and it
misses the X minus X that occurs
somewhere the users program won't build
right that that's a real big problem and
coupled with the fact that if you're
trying to rename X and you can't find it
because it's in a weird column that's
another big problem
so another more another point to see
this more easy to argue about it's less
empirical is that the GCC front end is
difficult to work with and this is a
personal opinion but I've heard this a
lot from people the big there's two big
two major problems one is that if you're
new to GCC the time to ramp up is
horrible it's very painful GCC is very
complicated interconnected system and
getting up to speed requires
understanding a lot of it right you
can't just learn a small piece of it to
me that's a huge problem
the second issue which is becoming less
of an issue as time goes on is that
there's a lot of politics around GCC for
example LTO wasn't allowed to happen for
a very long time again that's changing
but a combination of politics and the
implementation being really hairy make
it really difficult to make
any meaningful change so for example
switching GCC to use signless integers
or arbitrary-precision integers is
pretty much impossible I'm not saying it
is impossible but it's certainly very
very difficult right that's not
something you can do in a single release
cycle you know with a series of
incremental changes and finally another
big issue is the GCC is very slow and
it's very memory hungry and I'll talk
about this a little bit more in a few
slides the combination of these things
say that we want to re-evaluate the way
front-ends work the way friends interact
with the optimizer in the backend part
of a compiler and you know see if we can
do something better so looking for our
goals for this R then not to solve all
of world hunger and all the world's
problems and change the way that
everything works is we have very
specific concrete goals one is that we
want a unified parser for C C++ and
Objective C the GCC
parsers are split between C and C++
which means that a language extension
like Objective C needs to be implement
in both places right if you have a
unified parser besides sharing some now
logic and making each one a little bit
slower perhaps it makes it so it's more
difficult to extend both with a single
extension the other feature is that we
won't have good diagnostics coming out
of the front end the front end a front
end of a compiler has two main features
well assuming the compiler compiles your
code correctly there there are two areas
where a front end can contribute to the
end user it can either be fast or it can
produce good Diagnostics or hopefully
both right but aside from that all you
want from is to do the right thing I
guess it can also support all the
features you want which is also
important but these are our focus and
another important feature of this is
that we want to have GCC compatibility
because there's a bazillion lines of C++
and C there there's a ton of C code out
there in the world and we want to be
able support it which includes all the
nasty and gnarly extensions and
weirdness so we plan to build this the
same way that we've built the rest of
LVM we have a library based design where
we have
you know cleanly broken up the various
pieces of the front end so we have you
know lexer preprocessor parts parser
parts we have semantic analysis parts we
have converters to convert from a STS
tell them they're all separate libraries
what's that okay so the the good thing
about libraries among among other things
are that it's very easy to dive in
understand them and move on because you
don't have to understand the whole
system so again we want to be
multi-purpose here and this is talking
to you before in an IDE you have
multiple different clients you want to
be able to support them all well that's
a primary goal of ours and of course we
want to be fast right
and so this both includes compiling code
fast having a little memory footprint
but also allowing interesting
optimizations like caching lazy
evaluation supporting multi-threading
within the compiler things like this
which are very difficult to retrofit
into an existing compiler that wasn't
built support those so with all those
goals
obviously there's there must be
something we're not trying to do right
so non goals are that we're not trying
to support anything except for C family
languages and so we are building a new
AST s but we're not going to try to
shoehorn Java into using Ras T's right
if you want to build a Java front-end
what a Java and for LVM that'd be fine
that'd be great but build your own AST s
we're not they're not the same Java and
C look the same perhaps but they're
totally different languages likewise
with Ada or Fortran completely different
languages we already have an
intermediate representation that's good
for analyzing and optimizing code Oviatt
the main LVM error yeah ir there's no
reason for this front-end to do the job
of some other front-end zs two's another
non goal is that we're not trying to or
planning to obsolete GCC one of the
common criticisms I hear is that we're
trying to kill GCC which of course is
completely wrong right GCC has a lot of
strengths that LVM can't touch right
there's a lot of things you see we'll
always be able to do the LVM will never
be able to do because it's not in our
interest it's not what we're trying to
do LVM and GCC are two different
projects which have some
amount of overlap and people tend to
focus on the overlap but there are
inherently different projects with
different communities trying to do
different things and so we're not trying
to obsolete GCC and that's not a goal
trying to solve problems so Amos on on
with the actual concrete details this
already exists you can download it at a
subversion so when you run on the
command line it looks just like a
compiler we have GCC compatible options
for the most part it's written in C++ so
like the rest of LOV M it's written in a
tasteful subset of C++ and so we don't
do all the web crazy stuff that you can
do people have commented often that our
code is not as horrible as people expect
C++ code to be so I if you're afraid of
that check it out take a look at the
code so when you run it it looks just
like a compiler as I mentioned before we
have full column column information and
so if you have a problem we can give you
a carrot diagnostic right this is
similar to a lot of other compilers out
there where a prince the line of code
and it prints the location on the line
with the problem in addition to just
being able to analyze code we can also
we also have a bunch of options for
performance measurement so for example
one of the features of the front end is
that it's architected as libraries
there's two different libraries for
parsing and for semantic analysis what
this means is that you can parse code
without building an AST at all
and so this parse no op option says
parse the code don't build an AST don't
do any semantic analysis stuff well
rather do as the minimal amount of
semantic analysis which is basically
keeping track of type Tufts and their
scope and you don't have to build an ASC
just purse right well the interesting
thing about this is that if you can
separate those two you can actually
measure how much time you so in person
versus how much time you spend building
an AST and doing the type checking stuff
so we have a number of options you know
pre process but don't print the output
because I won't try and measure is how
much disk IO am i doing how much macro
expansion my doing stuff like that
another nice features we have a pretty
printer that actually generates for the
most part legal C code and so if you
give it a C file
it will give you the macro expanded CAS
T that comes out of it and so it's just
what's that yeah this this is compatible
so I mean the the C file looks the same
probably but actually the the input C
file had some macros that got expanded I
didn't show that but you could compile
that is that the question the question
was was it is the output compile or yes
so there's another feature so for our
deejay new test suite we don't build all
kinds of stuff into the tickle our
tickle code is very very simple it just
says run this program and the checker
for whether or not Diagnostics are in a
minute on the right lines of suffer
actually built on the tool itself and so
if you run with this parse ast check for
example to run the Perseus no purse
building ast and then check that the
diagnostic submitted are correct and so
you know on this test case where this is
not legal C code if I say I want an
error that's some weird string and I run
the checker on it it will say AHA on
line one you expected this but it didn't
happen
you didn't expect this but this did
happen and so you got an expected
identifier and so one of the cute things
about this is it because it's built with
all the same infrastructure as the rest
of the compiler this expected error
annotation actually occurs after an
escape new line and that's fine because
it's using the lexer the lexer knows
about escaping eli's it knows about
trigraphs knows about all that horrible
stuff and so that that kind of thing
continues to work right it also meant
that we didn't have to build yet another
thing that scans through c code I
mentioned Diagnostics before one of the
nice things about this is it gives you
good diagnostics here's again a simple
example here's some code as a struct has
a couple functions and two lines of code
in this function body when you compile
this with GCC you get two errors they
say there's an invalid type argument in
arystar that's true and it's talking
about this some a dot X but given this
error message I have no idea what it's
talking about
it's invalid I just know it is invalid
the second thing is it says I have
invalid operations binary plus right
again I have no idea why they're invalid
I have no idea which binary plus is
talking about right if you run this
through clang so here I took took away
the source code so you don't have to see
the source code and these are the
Diagnostics you get out it doesn't give
you color coded Diagnostics but it could
and in an IDE you could draw a nice
little boxes or whatever right but here
says you know at that unary star with
the sub expression your indirection
requires a pointer operon right it's not
that it's invalid for an arbitrary
reason it's it requires a pointer give
me a pointer that's that's the problem
right and it says that it and it is an
int so now we know exactly what's wrong
we you know we can fix that
here again the second line it tells us
exactly which plus it is and especially
when you get nested complicated stuff
coming out of macros it's really useful
to have an indication of what some
expressions is talking about
particularly to get it heavily
parenthesized right and so it actually
tells you exactly which sub expressions
and that's the right thing there and it
says it gives you the two types right
now giving types in Diagnostics is
something that is relatively easy to do
right GCC does this sometimes for
certain things it doesn't have to do
these but it doesn't give it for others
the problem though is that types can be
arbitrarily complicated right and so
every I'm sure everybody has done C++
programming has seen the compiler spit
out ah you got a basic string of tartar
traits of this is that and you know
whatever right this is basically because
GCC doesn't in completely Java tracking
type def information well by the time it
goes to give a diagnostic it has lost
the fact that you're using a type death
and so we don't support C++ well enough
to handle basic string of char yet but
this does occur and C also and we do
actually retain type def information
well we propagated through the ast we
efficiently can prepare types and do all
that kind of stuff so this is something
we think is really important especially
having to suffer with writing C++
programs for
there's other minor features so for
example each diagnostic has a unique ID
so you can control them on a per
instance basis you can group them things
like that the location tracking is
actually better than calm information
because if a token came from a macro
expansion we both know where the macro
expansion is and where the token came
from for example and so we actually
tracked that information which requires
more bookkeeping but it's very important
for certain clients like refactoring if
you want to refactor code knowing that
something came from a macro expansion is
really important if you need to throttle
back what you're doing handle it so
there any questions so far so where are
we right so this is still very early
work you're not going to download this
and build a Linux distro with it so the
again it's broken up into a series of
libraries and so the lexer and
preprocessor already well done we have
one of the people who co-developed the
GC C preprocessor scrutinizing
everything and he's convinced that it's
doing the right thing and all of his
crazy test cases the C parser is about
95% complete which is good I think C 99
so and the good new extensions as well
the major thing we're missing is for in
good new style and line azzam for
example we don't build an ast node for
it we just parse it and keep going -
things like that semantic analysis is
not so well complete I'd say it's about
80% complete so this includes omitting
all the errors when you have type
mismatches and stuff like that we're
continuing to work on this this is
basically largely going to the spec and
make sure making sure you implement it
all well the nice thing about this is
there is a spec well except for the
extensions but there is a spec and so
you can be sure that you're doing the
right thing and you're employing it
correctly the harder partner where our
warnings write warnings require a lot
more fuzzy kind of you know I'm trying
to help the user even though the codes
not invalid but it's really questionable
right and so that's something that we're
a lot we're not nearly as far along
we recognize that it's important but we
haven't had much emphasis put on it yet
also cogeneration is a is very early on
you can do a lot of arithmetic you could
do scalars vectors arrays loops you
can't do crazy things like Struck's
I mean I mean really who uses those yeah
they should use scalars so we're
obviously continuing to work on this but
as we do this we're continuing to say
what can we do and how are we doing on
those pieces right and so how do we
evaluate ourselves how what interesting
things can we apply this to and so one
easy thing to do is compile time
performance right because compile time
performance is something was very
important to us and it's easy to
quantify so to me at least there are
three interesting areas where you'd want
to look at compile time performance and
these are kind of ranked in order of
hardness I guess when you're building a
release build so I want to do
performance analysis I want to ship into
a customer I want to you know give it to
my internal users they can live on it
whatever you're doing early spilled
where you spend or where your turn on
all your optimizations if you work on a
piece of code you're sitting in the
editor doing compile debug edit compile
debug edit called a bug edit come out as
well get it over and over and over and
over again okay if you're working if you
have a source analysis tool so for
example you're doing indexing where
you're trying to find definitions of
uses of some variable the user type some
code and then double clicks or whatever
the action is on X you want to jump to
the right X immediately even though they
haven't even hit save yet right well
this means that you have to be parsing
it incrementally extremely fast the time
constraints are really really hard
because the users editing their code and
you're supposed to be parsing it now it
might not even be valid so of these
three cases the front-end is really only
interesting in two of those right in the
in the first case all the time spent in
the optimizer it's been the code
generator spending you know the system
tools like that
so the front-end doesn't have a big
impact on that you know the development
build obviously the front-end is
critical because you're compiling for at
least one third of those cycles right
and for the source analysis tools even
more so and so I'm gonna start talking
about these two yeah so the question is
is that if you want to optimize your
front-end where is this pangas time what
is the problem right so if we if you
look at a typical application on a Mac
and Mac is particularly bad but this
happens on every platform to some degree
particularly when you're using libraries
what we end up seeing is that there's
the application consists of a core of
you know a couple dozen or a couple of
hundred source files these files are
marginally big there may be a couple
thousand lines of code maybe ten
thousand lines of code Wow
you know but so the code that makes up
the application isn't that big the
problem is is that they include a lot of
libraries and these libraries have these
gigantic headers on the Mac in
particular developers are encouraged to
include the Umbrella header for the
subsystem that they're using so for
example carbon is the C interface the
mac GUI api stuff and it includes a
whole bunch of sub header files cocoa is
the objective-c version of this same
thing it's it's an umbrella header that
includes a whole bunch of other headers
right if you're using a an XML parser or
embedded web browser thing you know
you'll likely have similar things and so
the problem of course is that headers
are huge if you take your 10,000 line
program that pound includes carbon odd
age your 10,000 lines of code are a
pittance compared to the amount of code
the compiler has to deal with
Carbon dot H for example pulls in 558
different header files all of which have
to be looked up on the file system
through multiple include pass which
probably have dozens of entries on them
because they probably have dozens of
entries on the the the text for the
headers alone are twelve point three
megabytes twelve point three megabytes
just for the headers right and out of
this you get thousands of function
declarations enums you get tons of type
des you get all these macros all the
stuff that gets pulled in and then they
have there are four functions or
whatever and the compiler stuff right
and so compile time when you're looking
at least to debug
execute compile turnaround cycle is
largely dominated by the person because
the parser asked to type-check analyze
look at all of this code if there's an
error in a system header the compiler is
required to tell you about right even
though there's probably not but if you
did some crazy dash D you know in so
something else you're going to get lots
of errors and the compiler is compelled
to tell you about that so it actually
has to look at and analyze the entire
header all this contents the optimizer
on there and it's really got it easy
because after the front end runs it says
okay here's the for functions and maybe
an inline function from the header you
have to compile go for it well it grinds
on those and thus done right the front
end has very different time space
trade-offs and other constraints here
and so if you look at this header like I
said before it's about 12 Meg's of input
source after you run through the
preprocessor the preprocessor source is
only three and half mix right and so
people have observed that lectures and
pre-processors are very performance
sensitive right and this is exactly why
the preprocessor is confronted with in
this case four times as much information
as the parser right the parser and the
the lexer piece only has to deal with
you know three and a half mix so when we
look at the memory usage of our ast s
compared to these it's actually pretty
good this breaks down our AST s by a
couple of different flavors so we have
the actual identifier hash table for
example we have declarations identifiers
and characters and statements of
expressions and because it's a header
file there aren't a lot of statements
and expressions right because those are
only an inline functions mostly enums
mostly in function definitions stuff
like that and so this explains why
decals are significantly more than
statements or expressions in this case
and headers are generally like that if
you compare this so one of the the
interesting things we found is that
we're actually pretty good for space
because even though our ast s contain
fully and saturated type def information
will completely track source location
information for any you know you know
for any binary plus we know exactly
where the extents of the left-hand side
and the right-hand sides which is
required to emit these nice Diagnostics
even though we're tracking a lot
information our ast is only about 30%
larger than the pre-process source which
we think is very good of course if for
the people who work on GCC you can guess
that GCC trees are not don't fare nearly
so well
GC sees the the historical design of
it's actually kind of bad for this case
and this is one of the worst cases for
GCC because specifically GCC puts a lot
of back-end information into the trees
right you have RTL fields you have
alignment fields you have all the stuff
that really only the code generator
looks at you have deckle assembler
string all the stuff right well this is
a header file the header file isn't
using any of the stuff or is using a
tiny proportion of this stuff because
most of the stuff never makes it through
to the code generator right and so all
this stuff and space and all this extra
data is just wasted right it's just
whole it's just zeros in your address
space and this is one reason why it's
really hard to use GCC for a refactoring
tool for example refactoring needs to
have a global view of your entire
program all of the declarations all of
everything at once to be able to find
out if I renamed X where do I need to
update it right things like that and
memory is a big problem so another
another interesting experiment is
compile time right that's the natural
thing because we can separate the
semantic analysis from the parsing
itself we can actually measure
pre-processing parsing and the semantic
analysis ast building pieces in
isolation and so we found that
pre-processing unsurprisingly is the
biggest chunk of our time spent on
carbon dot H with 0.16 seconds the
parser is actually really really fast
because the parser is completely trivial
right I mean parser is basically taking
tokens out of the preprocessor and
changing state it's not really doing
that much and the time it spends is
proportional to the number of tokens
that come out of the out of these
headers not proportional a number of
macro expansions or
the 12 megabytes of source code that
come in and anything like that so the
parser is actually really fast and
really simple the semantic analyzer on
the other hand has a different place
where spends all this time and that's
actually building these ast s and so
allocating all this memory for the STS
is expensive takes time doing the type
check is expensive it takes time of
course relative to the parser the parser
is very simple and it runs basically
within cache except for the preprocessor
this is C code so we've also looked at
some objective-c code but since we're
doing semantic analysis that requires
semantic analyzing C++ our C++ support
is minimal it's for bool static casts a
couple of really trivial things but not
nearly enough to be able to do anything
interesting and not nearly enough to get
interesting performance metrics so so
the time to analyze find your filesystem
stuff for 558 files is all included in
the preprocessor and so the preprocessor
has to scan the file system it has to do
it it it has to Lex
scan the filesystem handle macro
expansions do all that stuff so the
preprocessor is the nasty performance
sensitive piece so if you look at the
percentage breakdown we're spending 35
percent of our time they st building
type checking stuff and 65 percent of
our time and pre-processing again this
is for a massive header file right if
you have 12 megabytes of source code
that actually contains statements and
declarations and expressions and other
stuff I'm sure would be weighted more
towards the samac analyzer if you look
at GCC 4 and here this is Apple GCC 4
because it's what I happen to have
available GCC for actually has very
similar performance characteristics in
that it spends a whole lot of time free
processing and so the preprocessor is
about you know twice as slow as the
clanging preprocessor but again GCC
builds all of these trees all these
trees take a lot of time to allocate
manipulate transform analyze type check
all that stuff and so partially because
the trees are bigger
juicy slower GC also has questionable
algorithmic decisions but I won't talk
about that very much so the question is
how easy is it to add a new type
qualifier it's actually pretty easy the
nice thing about it is in one of our
goals is that if you know see here you
know C++ or you know the language that
you're trying to deal with the STS are
very obvious so we have for example type
qualifiers we have an integer type we
have unions and structs as first class
things in the type you know in the class
hierarchy for the ast s we have all the
things that map directly to see because
we're not trying to be a unified AST for
all languages and so basically to add a
new type qualifier or add a new thing
you basically find the pieces that are
handling the existing type of qualifiers
add yours update the things look at
those and it's pretty straightforward
and oh yeah you can change the rules you
can do whatever you want I mean the
source codes there right so the problem
with that is that there are a lot of
rules that have interrelated pieces and
we do have good predicates for what is
an integer constant expression for
example if you want change to the
definition of what is an integer
constant expression as you do in one
place things like that so keep going
so if you look at the numbers we're
about two and a half times faster than
GC on this one test case and again it's
a big header it's very important for
certain people case but it's definitely
the worst case for GCC the problem is is
that for us the worst case for GCC is
impacting our customers on daily basis
right so it's somewhat contrived but
it's real and it's impacting customers
so while two and a half times as good we
actually want to have significantly more
than this and I have no numbers to back
up 10x but I believe you can do
significantly better than this and the
observation is that this isn't using so
we're not using precompiled headers
we're not using any tricks right when
you start using tricks like precompiled
headers when you start caching
aggressively using multi throwing other
things like
he gets significantly better speed ups
one of the things we're interested in
again is an IDE and in the context of
ninety e by the time you've compiled
your whole program and are about to run
it well your IDE should know everything
there is to know about your program so
that when you go to click on your factor
it has all the information it needs
right and so a combination of being
intelligent caching doing things in the
background all the stuff can contribute
to an experience that feels a lot better
and following that once you have these
ast you can do a lot of interesting
things right clicking on X and going to
this definition is one example I know
there's you know source code refactoring
if you have persistent s T's you don't
need to bug information because the
debug information captures just a tiny
subset of what the STS do one of the
things we're very good with in the album
community is knowing how to serialize IR
and st is just a tree triage ir where
LVM ir is more of a linear ir but it's
the same kind of idea right and so if
you can make it efficient to access and
process lazily then you can do great
things so this is all wonderful if
you're sitting on your desktop machine
building stuff the question is is what
about parallel builds and I mentioned
before that if you're building it o for
all right the friend doesn't really
matter
well that's not entirely true right
because if you're at some place that has
gigantic clusters of machines you might
be trying to distribute your builds
across all these machines and so to do
this one popular tools a tool called DCC
the way to CC works is it's actually
architected to drop in your make file so
you drop in you use CC equals to c c and
instead of invoking GCC your make files
invoke to c c and then you say make tes
j 147 and you get a lot of things
happening in parallel right well so the
way dis CC works is that it actually pre
processes all of your source on the
local machine sends the data file across
to the slaves which then optimizing
compiler okay so this is imperative if
you don't own the file system you don't
have a unified file system because you
your to get the same binary out at the
end you need matching system headers
application headers library headers all
of these things and game was all in sync
is very difficult by pre-processing
before you send it across you solve that
completely and you just have to worry
about having the same
your tools which is easier to control
because those can be installed locally
the problem though is that your
preprocessor becomes a significant
significant scalability problem this is
a quote from a paper and by Martin Poole
the DCCC guy and he says dis EC tends to
you know stop scaling at 10 to 20
processors depending on the flavor of
your code now if you're dealing with the
Apple code where you have ridiculously
huge headers it doesn't scale nearly
this well right because you're doing all
your pre-processing on your single
machine and so if your single machine
can only pre process you know for four
times as fast as the back end machines
can compile well you're limited heavily
by that so the question then is what can
you do if you have huge clusters of
machines languor right well the GC
community and Google certainly has
helped with this has actually looked at
optimizing GCC's preprocessor right and
so I just was discussing this with
somebody before and they said oh well
GCC 4.0 sucks and especially Apple GCC
because it has all kinds of nasty
horrible stuff in it and just see 422 is
much better right so and in fact these
right right GC 4.2 is significantly
better than GCC 4.0 and what I measured
here is I took the four biggest
benchmarks that expect CPU 2006 just
because they're well-known they're
available people can get them and took
the biggest in terms of the
pre-processed output and so if you look
at is a lank bmk for example to compile
this sucker you have to end up
processing 291 Meg's of source code a
lot of which comes from headers right
it's this big C++ app like Waze deal to
includes big pieces of boost and things
like that right and so if you look at
this you know the speed-up that you get
from going to GCC for 4.2 versus the
Apple 4.0 is about 6.9 percent which is
pretty good right that means you scale
6.9 percent better
so before I claimed of course the dart
preprocessor significant faster so if
you compare 4.2 to our preprocessor
which again is very conformant and you
can use it independently of the parser
which isn't done yet and the
preprocessor is
we actually are significantly faster
right across these programs for example
you know you get between a 58% speed up
and a 48% speed up what that means is
that you can diss ICI and distribute
your work to twice as many machines
right your scalability is twice as good
and so across these four machines you
know you can average speed up at 58%
this is just for changing the dis CC
step that invokes a preprocessor to
invoke the clanging preprocessor instead
of the GC preprocessor no other changes
right you're still farming it out
finally with GCC you're not using LLVM
at all other than the pre-processing
stage right and suddenly a scale twice
is twice as well which is pretty
powerful right that's that's a very
useful deliverable thing that you know
our users can benefit from but again the
the point of this framework isn't yeah
question so the question is is if you
pre process locally and then move it
remotely don't you lose all the
information about where the tokens came
from and stuff like that so the answer
is yes you do lose information about
macro expansions but pound line markers
do tell you the line number and within
the line assuming there's no macro
expansions the column numbers are
retained so you're fine
in that respect or they're mostly
retained in practice GCC doesn't do
anything with call numbers
well except that's an exaggeration in
practice GCC generally doesn't really do
anything with call numbers and as long
as you have pound line markers you'll
get good good good debug information and
so if your back-end is compiling at GCC
it doesn't matter so dis EC is only
really interesting if you are doing just
bulk compilation anyway and so it's not
a I'm interacting with today ID kind of
tool at least to date could be extend in
the future so the question is is then
you know if can we change the rules what
can we do that's actually better than
just dropping in a place in the
preprocessor right well a clear
observation is that all of these or
these pre-processing runs
all doing very similar pieces of work
right they're using same headers were
largely overlapping pieces of headers
they're all groveling around the file
system to look up the same you know
however many hundreds of headers that
have to be looked up to the same search
pass were very similar search paths not
necessarily identical right can we do
something that's you know built to
accelerate this and the answer is what
we should because you know we said that
this is a reusable framework and you can
use the libraries to do crazy stuff
right and of course I'm not gonna tell
you the answers know strangely and so
you know giving this talk I had the
inkling that Google people care about
the CC and so I hacked together a little
test that says well let's just do the
very simple basic approach of caching
file contents for headers and file
system lookups for the directory
traversal stuff right I'm not going to
try to do precompiled headers I'm not
going to try to do you know caching
macro expansions or any of this crazy
stuff just do the very very basic stuff
because I only had a couple days and get
this going and see what happens right
because I'm curious and so the idea here
is that say you have a tool like to see
see where you're presented with
you know pre process this this you have
this big work list of stuff to do
instead of forking exact and exacting
processes to do this because this is the
library based design you just link it
into your tool and you just pre process
some one after another right and what
happens well the answer is the good
things happen right so the the three
bars here are the left bar is GC for
point to the middle bar is the the
clanging number I told you showed you
before and the third bar is the number
with just caching filesystem related
information right and what I found is
that this is a huge speed increase and
for obvious reasons right I mean you get
between a four point two times speed-up
in a two point six times speed up which
is three point three times average
unsurprisingly what ends up happening
here is that the system time drops right
for the biggest ones I like BMK for
example system time on my Core 2 Duo
went from nine point eight seconds to
point eight eight seconds
right so that's a factor of ten
reduction in and map stats you know all
this kind of stuff right you still have
a lot of i/o because you're writing out
291 Meg's of output right so it's
nonzero but it scales much better as you
add source files to your application
right because as you add source files or
headers to your application now the cost
per process it grows layered basically
linear with respect to the amount of
sorcery not with respect to the source
plus times all the headers number of
source files times all the headers so
this is a very this is a very good thing
another interesting thing is that in the
GCC community there's been recent work
on F directives only right which
basically says well the preprocessor
does a ton of work to keep track of
macro expansions which you know looking
up the identifiers and the big identify
our hash table in the sky for example it
takes a lot of time if you're not
tracking pound of fines and expanding
them you can go much faster if you and
pound of finds almost never really not
not never but almost never are involved
with pound include expansion other other
things like that and so you can get by
without having to do a lot of the work
that the preprocessor has to do that's
not related to the file system yeah I
know so and so I think within the
context of GCC this was a 30% speed up
in pre-processing time in some cases
right which is huge well if you apply
that to this right our our system time
is tiny so if you reduce the user time
by 30% now you're talking about you know
six seconds on the biggest one or
something on that order well if you
reduce something that is you know
already four times faster by 30% or 33%
now you're talking about a 12x speed up
over GCC that's 12 times more
scalability than before because that's
coming straight off the top of user time
that's just worked on by the
preprocessor you don't have to do any
more you just you let the slave machines
dealing with all the grinding stuff does
that make sense so this is just a hack I
threw together it seems to work really
well but it's not integrated SEC so
people can't just take it and use it if
you have threads numbers of files laying
around the you want free process it
works great and you can use
right now most of us don't but the point
of this is to show you that you know
with a framework based library based
design you can build interesting neat
stuff in a couple of days right and
again this is a couple hundred lines of
code if that it took more time gathering
numbers and figuring out which programs
were big and how to which options you
need to compile them all this stuff that
it did actually build the code so that
is my talk so in brief I discuss LVM 2.0
LVM 2.0 came out in May it has a bunch
of new features and if you're send
optimizers our back-end code Jen's stuff
definitely check it out it works really
well we're using it for a lot of
different interesting projects LVM to a
point once coming on in September and so
that will bring the next wave of of
missing features like Siebel plus
exceptions something that a lot of
people really want and optimizer
improvements cogent improvements and
optimizer speeds ups elevated GCC 4.2
will probably be out sometime by at the
end of the year and will probably
maintain 4.0 and 4.2 and parallel for
some time and then eventually 4.0 will
fade away in the background the new C
front-end is very early on again it can
parse tremendous amounts of C code and
it's really fast but it doesn't go all
semantic analysis and coach n is again
15% a long way right and it's the easy
15% so fortunately I've written this is
my fourth ast to LVM translator so I'm
very confident that it will work and
have definitely some experience there
but again it's very early if anybody is
interested or wants to help out or check
it out you can do so the main I love you
my page is there and my really ugly
horrible web page I threw together in
about ten minutes is available here so
that's about it any have any questions
has a compiler internally serial what do
you mean it is not currently
multi-threaded the so there's different
answers to your question I guess so the
LLVM optimizer itself has this concept
of a past manager the past manager is
actually built to enable it to be
multi-threaded but nobody has and so
pieces the optimization pipeline our
function at a time and so it'd be really
easy to enhance that to optimize and
code gen a function at a time for
example that's an easy hack nobody's
bothered to do it yet and the front end
is not threaded but the components are
built to be threaded or they're in the
if you want to synchronize synchronize
outside before you call into it kind of
design and so one of the things we're
interested in looking at is incremental
parsing on multiple threads to support
things like this again it hasn't been
done but I think I can and we're kind of
designing it assuming that we will at
some point so yes there's no answer that
question before yes the next one okay
okay so the question is if I want to
write Java front for LVM how does that
interact with the garbage collector
particularly when the garbage collector
will scan the stack and find all the
pointers the answer is really easy in
the LVM supports that directly you
it supports accurate garbage collection
and supports identifying all the
pointers on the stack metadata so you
thus the intermediate David more have
you go to the store information that's
only in the front end in other words can
you store type information that you
could then report into your front end
that you're recording let's the other so
the question is is if so is this in the
context of C or in Java I think y'all
you know we go get so yeah so if you're
right so the question is if you were
building a Java front and in Java when
you say import of java.awt window
whatever you import a class file which
contains metadata about the class
doesn't actually contain the class do we
support this the answer is no the reason
for that is that C doesn't work that way
we're not trying to support C in the
same context right if you're to reuse
any pieces of our front-end it would be
the the buffer management calm line
number tracking stuff all of those kinds
of pieces you wouldn't be able to track
you wouldn't be able to use the ast is
the type information any of that kind of
stuff that said we do plan on being able
to write out the STS to disk we haven't
done so yet but it's a no-brainer we
definitely want to do that support PCH
for whole program ast s support all
these different things and for that when
we get to it we definitely will be doing
basically a module system similar to the
proposals for a module system in C++
that have been talked about at the
Standards Committee me yeah
hold it code state population so the
question is is anybody using LVM for
multi stage programming language is
where you have eval and quoted code and
things like that so the answer is I
don't know or I don't know of anybody
who is except that LVM is heavily used
for things like dynamic specialization
runtime code generation
it's used by for various scripting
languages so the question isn't really
have they it's could they and the answer
for kadai is yes they could because all
the pieces you can run it at compile
time you can run at runtime and Alvin
was designed that way as a set of
libraries so yeah yeah there's a the
question is does LVM have any chance of
working as across compiler the answer is
yes in fact it does so very directly one
of the nice things about the ovm targets
is that they're all they can all
function as dynamically loadable plug-in
things and by default for example LVM
one of the developer tools is called LLC
LLC is basically all the the targets
linked together into one executable so
you can take an LVN byte code file and
say run through the alphabet in route to
the MIPS or whatever without having to
reconfigure rebuild any of your stuff
when it comes to ovm GCC LBM GCC doesn't
work that way hey okay i'm GCC you have
to configure it for the tart of the host
and target and it does definitely
support that but you can't just say you
know okay i want to target this now the
new front-end does support having
multiple targets built-in and supports
linking to the multiple LVM targets and
so does the right thing there yeah
so the question is do we parse the
expected error markers and the parser
and the answer's no what we do is the so
the tool that analyzes diagnostics and
make sure diagnostic is this we're
supposed to and doesn't exist where it's
not supposed to does two things one is
when it starts up it runs the parser and
it installs a special error handler so
that it captures and buffers all the
Diagnostics that come out so now it
knows what the parser actually produces
on that input and after that it runs the
lexer again in keep comments mode which
is the GCC capital C option right and so
it basically relaxes the file when it
relaxes the file it gets all the
comments as comment tokens and it just
looks at the comment tokens and so the
person itself doesn't look at those this
tool which is used for the test suite
explicitly relaxes it and because it's
using lexer it doesn't have to deal with
you know figuring out what slash star
comments we know that stuff but also
gets full call number line number all
that information for free
so so the question is have we ever had a
bug in the lexer which point you pause
and so the answer is yes
what's thing got fix and and they the
second part was they caused an error not
to be caught and we haven't had that
happen yet so the comment parser has
been and it's well established it it
doesn't change very much so
it is it is vectorized and has other
cute things in it but it hasn't been a
problem so the question is can you
translate Java bytecode and tell vm
bytecode and vice versa the answer is
you can translate LVM byte code into ms
IL bytecode which is not java it is
actually more general than java and it's
actually unsafe Anas al bytecode you can
translate some java with the gcj
front-end for GCC but nobody's done the
work to make it work really right when
you do that you lose portability of java
and so the l vm that comes out of a C
front end is necessarily tied to the
dialect of C you're compiling for and so
if you compile on an x86 box or if you
compile targeting in xa6 box the
preprocessor strips out everything not
related x86 from the input token stream
right and so the bytecode file coming
out of LVM even though a VM can
represent portable programs is
unfortunately tied to x86 or whatever so
given a from scratch written Java friend
you could do the right thing and you can
make it work with nobody's done that
yeah yeah
so so the question is is what flavor of
inline assembler do we accept do we
expect to support and the answer is
twofold in LVM GCC weeks we currently
accept GCC style inline assembly and
code warrior Microsoft style in the new
front-end I'm sure we will have to do
both so we don't support either really
yet and we person anything useful with
it so the question was so you want to
write your own java friend in Java or
Python friend and Python we already
talked about let's talk about Python so
there there's this project called pi pi
which is a Python foreign and written
Python it actually targets alluvium it
can output LVM code or C or bunch or or
Java bytecode or something their fastest
Rodriguez with LLVM currently so there's
two ways of doing it in Python it's
really easy to bind to C class libraries
and stuff like that so I think that's
what they're doing but elven has a fully
functional one percent correct text form
and so for example we have somebody who
wrote a scheme front end by you know
rank scheme front and ski-in scheme he
just spits out a big dll file with all
the code at the output and then just
send that through all the existing tools
we have so just you know turns into run
this command pipe it in the Southern
Command pipe it in this other command
output a test file run GC on it to some
blue tur that kind of thing so either
way you can either bind to the C++
classes if you want directly which works
really well or output a text file and if
you bind directly to C++ classes it
works great because you can dump out the
text file at any point so you don't lose
anything by doing guys
so the question is is LV and both and
optimized are in a linker and the answer
is yes it is both it has so the link to
our physicians story is actually very
different than the GCC one so elion
obviously it has an optimizer it also
has a linker that works on its own IR
files and so when you invoke the link
time optimizer piece it links together
and it understands visibility or it
understands a weak linkage it
understands all that stuff right so if
you're given two ll file so you can lick
them together and does the right thing
the problem is is the LVM does not want
to be a native system linker that knows
about archives and knows about you know
the funny rules for when you're supposed
to pull the data file out of an archive
it doesn't want to know about all the
dot o file format types and elf
vs. maka versus whatever peak off so it
doesn't want to get involved with thing
in that stuff also the native like has
to do a bunch of work to determine
symbol visibility you know oh if this is
visibility:hidden then at this point it
gets marked internal you know and we
could do optimizations right stuff like
that so the way the LLVM link time after
nodes or works is that there's two
models one works similarly to the GCC
designed model where it's a standalone
tool you invoke the santalune tool does
the linker it knows some linker options
it tries to make it work but in practice
it doesn't work very well people drop it
in and inevitably they're using some
bizarre weird feature from get from
Guinea linker or something and it works
and people don't like it right the other
option is that we have a Lib LTO dynamic
library and the way the apple linker
works is that when you invoke the linker
just user bin LD on a bunch of files the
linker just starts popping I'm open and
saying AHA this is in the case with
apple and mock o o file great keep going
here's the maka will file here's not go
file here what the heck is that I don't
know what that is
this looks like an error right so on the
air path of the linker then it says oh
well I wonder if I have a shared library
sitting in this specific spot in the
file system is so I'll open it aha I
have a library that's great okay do you
know what this is and so it acid in this
is aha well the library which is an
LVM live LTO library says yes I know
exactly what that is I can handle it
just leave to me right and so the linker
keeps going and it opens all the data
files summer native data file summer LVM
they might be LVM and native o files
mixed within an archive if it doesn't
matter the the linker then asked the the
the showed library for simple references
symbols that are used symbols that are
fine stuff like that
so they can do archive resolution of
symbols and pull the right things out of
archives and handle all that crap sorry
too not crap liquors are very important
just not my thing
sorry yeah so it does all the hard work
right when it's done is decided these
are all the data files I need to process
it then says ok labelled tou go optimize
cogent and handle all the LVM pieces and
so that runs the LV optimizers it runs
the LVM linker between those audio files
runs the album code generator the output
is a native dot o file which the linker
then opens and contains linking with the
rest of the stuff and so the answer
there is that all you have to do to use
live LTO if your linker supports it is
to just come by with it before if you
compile with it before you and the make
file it says run user bin LD it'll still
work right no matter what weird crazy
features they're using which is very
nice and very powerful yep I hand back
another file because the lycra or eNOS I
read though so so it and it's nice if
your compiler can directly output a data
file but if it doesn't you just invoke
the assembler and stuff like that so
yeah any other questions
all right</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>