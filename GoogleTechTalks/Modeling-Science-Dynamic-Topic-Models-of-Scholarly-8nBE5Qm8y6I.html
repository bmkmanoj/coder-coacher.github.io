<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Modeling Science:  Dynamic Topic Models of Scholarly... | Coder Coacher - Coaching Coders</title><meta content="Modeling Science:  Dynamic Topic Models of Scholarly... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Modeling Science:  Dynamic Topic Models of Scholarly...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8nBE5Qm8y6I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're very pleased today to have David
fly with us from Princeton University I
think a lot of people here already know
Dave and his work especially on
literally allocation and he did his PhD
at berkeley and there's several folks
here who probably I should ask for more
stories from dive alone but since then
he also did a postdoc dawn lafferty at
CMU and there's some joint work with him
and he's now for the past four semesters
now we're starting your field semester
professor Princeton University so we're
very happy to have Dave here we'll be
talking about modeling signs okay thanks
Mira should I stand up here is that we
come to a consensus or okay great my
name is Dave I'm going to talk about
modeling science this is joint work with
John Lafferty and it's um it's a little
long so you know I I'd prefer to have to
cut it short and everybody be kind of
understanding things so that's okay so
feel free to interrupt if you don't
understand something even though it's
long okay so what this projects about
our data our science magazine from 1882
2002 courtesy of JSTOR so JSTOR does a
process that I think you know well here
where they take original volumes of
journal articles they scan them they run
OCR software on them and then they index
the resulting noisy ASCII text so that
scholars and scientists can search
through hundreds of years of scholarly
literature so in the journal Science
here are some examples of articles from
the journal Science this one's about
poisoning by ice cream this process
results in 130,000 documents corpus that
has 76 million words anywhere else this
is a big number here it's like a toy
data set so the idea is that we want to
take this collection and discover the
hidden thematic structure that lives
inside it with I'm sorry that's awkward
with what are called topic models these
are hierarchical probabilistic models of
text that use multinomial distributions
over words to to capture these hidden
themes and then we want to figure out
ways to use this structure for to do
things like browsing search
similarity assessment between these
articles the basic idea is that you know
jstor takes these these volumes they
scan them and they just have a big bag
of documents there's no structure
there's no metadata there's no keywords
nobody has typed in oh this is this
place in the hierarchy and people want
to take articles from the 1800s in the
1950s and understand where they sit in
the broader setting and so that's what
we're trying to do here ok so what topic
models can do our things like
automatically discover topics from a
collection of documents automatically
label previously unlabeled images model
what I'll talk about today the evolution
of topics over time model the way these
topics these groups of words that go
together the way the topics go together
how they how they map and and so on and
so what I want to talk about today is
first I'm going to introduce late and
dear Ashley allocation which is kind of
a the simplest topic model and then
extend it to dynamic topic models that
will go in some detail and then finally
discuss correlated topic models just
very briefly just to kind of show you a
picture but so will focus on dynamic
topic models and quickly review late and
fiercely allocation but if no one
understands then we I'm happy to even
stop here I could even stop now okay
so the idea here is that we're going to
take our documents and we're going to
we're going to model them
probabilistically probabilistic modeling
what we do is we treat our data as
observations that arise from some
generative process that includes things
that we can't observe so in the case of
modeling document collections the things
we can observe are the underlying
thematic structure of the collection so
that's posing the model and then the
second thing you want to do of course is
to take your the things you do observe
the documents and infer what that hidden
structure is we do that with posterior
inference and so in the case here it's
asking what are the topics that describe
this collection of documents that I want
to analyze finally of course we want to
then situate new data into the estimated
model for example how does this query or
new document fit into the estimated
topic structure and then use that to do
things like information retrieval or
classification or just something vague
like browsing and organizing ok so the
intuition behind Layton dear Ashley
allocation will call it lda and I'm
sorry for linear discriminant analysis
people but the intuition behind lda is
this it's that documents exhibit
multiple topics so here's an example
document from our collection this is
called seeking life spare genetic
necessities and this is an article about
asking how many genes does an organism
need to survive in evolutionary terms
and what I've done here by hand is just
highlighted different words so I've
highlighted words like organisms survive
life words about evolutionary biology in
pink I've highlighted words like jeans
genomes sequenced words about genetics
in yellow and I've highlighted words
like predictions computer analysis
computation where it's about data
analysis and statistics in I guess blue
but it looks like gray so I've
highlighted these words and you can
imagine that this article the words in
this article combine words about
evolutionary biology genetics and
statistical analysis so the idea is to
then cast this intuition into a
generative probabilistic process and
that process is as follows we let's
pretend that living somewhere are 100
distributions over words and some of
them have words like genetics with high
probability some have words
evolutionary biology with high
probability some have words about data
analysis with high probability and some
have words about other stuff with high
probability that happened not to be in
this document so holding those fixed we
generate each document by first choosing
a little distribution over those
distributions over words which we call
topic so if there are a hundred topics
then here the x-axis are 100 elements
and here I've decided that the blue
topic has this probability the yellow
topic is this probability the pink topic
has this probability okay that's the
first step of this imaginary process and
then what we do is for each word in the
collection we choose one of those topic
indexes indices so here I chose the blue
one then we look up what distribution
over words the blue coin refers to and
then we choose a word from that
distribution so here I chose the word
analysis similarly I choose the yellow
topic here and I looked up what
distribution over words it came from and
I choose the word genome or the pink
topic and I choose the word organism and
the idea is we repeat this process for
every word in our document beginning
with randomly selecting a distribution
over these distributions and then for
each word choosing one of those
distributions over words and drawing the
word from that distribution okay is that
clear right and we repeat this process
for every document so another document
that's not about this stuff first
chooses a different distribution over
the topics and then draws its words from
those topics and the idea is that this
happens for each document okay but of
course in reality we only observe the
documents we don't get to see all this
hidden structure so the goal is to infer
this underlying topic structure to take
a collection of articles figure out what
topics each word came from what
distribution over topics those articles
came from and figure out what are the
distributions over words that those
topics correspond to automatically okay
so and and so to describe our
probability model i'm going to use
graphical models i know probably many of
you are familiar with them but just to
quickly remind you a graphical model is
a graph where nodes are random variables
edges denote possible dependence between
random variables and observed variables
are shaded so here I've observed x1
through xn and y is hidden
okay and in I'm going to use this plate
notation a lot where replicated
structure can be short-handed by drawing
a box around a variable and giving it a
little index and writing down how many
replications of that random variable we
had so this graphical model structure is
shorthand by this okay in a graphical
model the structure of the graph defines
a pattern of conditional dependence
between the ensemble of random variables
that the nodes represent so for example
in this graph it corresponds to this
joint distribution the Joint
Distribution on y and x 13 x n which can
be factorized as the probability of y x
conditionally independent the
probability of each of the X random
variables okay so this graph defines
this factorization it's not necessarily
true for every joint distribution here
but it's got to factorize like this for
this graph okay so that generative
process that I described for you looks
like this as a graphical model so let me
just I'll go through it the way I talked
about it before but pointing at these at
these nodes instead so to begin with we
choose our k topics RK distributions
over words so each of these bata kaise
is a distribution over all the words in
the vocabulary so let's say k is 100 I'm
going to start doing this let's say K is
100 some of these betas are going to
have genes words with high probability
some are going to have data words with
high probability and so on but so we
begin our process by choosing all those
from a distribution over them which is a
dear Ashley then for each document
that's this D plate here we first choose
our distribution over those topics so if
there's a hundred topics then theta is
that little cartoon histogram that I had
on the right side of the document theta
is the yellow topic with some
probability the blue topic with some
probability the pink topic with some
probability and that gets chosen chosen
randomly from a distribution over
distributions which is the dearest lake
that's why we have deera schley ok so
that's how we start then for each word
that's this end plate within the deep
late because we have D documents in each
documents say has n words I know in
reality
have different numbers of words but just
let's assume that they don't right now
for each word we first choose a topic
indicator Z from our distribution over
topic so this is like choosing the
yellow coin okay that's just an
indicator of which topic i'm going to
choose the word from so say that's topic
number 35 then choose the word i look up
the 35th topic the 35th distribution
over words and I draw the word from that
topic okay so say here I again I have
this combination of topics of genetics
evolutionary biology and data sometimes
I'm going to choose the genetics topic
and draw genetics words sometimes I'm
going to choose the evolutionary biology
topic and draw evolutionary biology
words and so on and so you can see that
this describes how one might have
generated that document and for each
document in the collection I choose a
different distribution over these topics
and generate different combinations so
others might be partly about
evolutionary biology but they're about
evolutionary biology and combination
with other things okay any questions
about this generative process yeah not
sure Alex excellent bars
I don't believe so he's chosen this way
right i mean i don't believe that
documents are written this way either
but that's a good point so in any model
and that's going to be one of the themes
of this talk in any model we're making
major majorly bad assumptions about our
data but the purpose is to be able to
get at some of these salient themes and
so the dirah shoelace seems okay for
doing that although later we're going to
actually relax that assumption but
that's a good point so all of these all
of these decisions that we've made in
building this model contain assumptions
in them that are either we know about or
don't and understanding how those
assumptions affect our analysis is part
of this activity yeah the math and the
computation that's right so the D really
has this nice property that again we're
going to relax it later on at some point
but the D really has this nice property
that it's conjugate to the multinomial
so say I knew this topic indicator then
the posterior distribution of theta the
distribution of theta given this topic
indicator would still be a dear Ashley
and that really simplifies the
computation later on that's why we
choose the dear slain these are good
questions other questions okay so like I
said before we only observe you can see
in this we have you know all these nodes
here but the only thing that's shaded
are the enth word in the deeth document
what we have is a big collection of
documents that are divided up into which
words are in which documents and so our
goal is to infer all this other
structure around it essentially reverse
that generative process for each
document we want to look at what the /
where topic assignment is we want to
look at the per document topic
proportions and very what we're very
interested in is looking at what are
these distributions over words that
generated this you can think of this as
like we're reducing the dimension of the
of this document collection which has
dimension number of words into a
collection of distributions of
well into into K dimensional vectors
will see that later in a second okay so
here is this posterior distribution the
probability of theta and z given
everything else even up just a part of
that posterior and even computing this
is intractable but there have been
several approximation techniques that
have been developed in recent years mean
field variational methods is what we'll
be using today but also people have
developed expectation propagation
collapsed gibbs sampling and something
I'm excited about eya and some
colleagues did collapsed variational
inference which kind of takes the best
of these two worlds and it's a very
efficient inference technique so all
this is to say we can't really compute
that hidden structure that we want to
but we can approximate it and that's
what we're going to do okay so let's
look at some examples so let's just take
10 years of our science collection which
is about 17,000 documents 11 million
words and a vocabulary size of 20,000
where we've removed stop words and rare
words to again speed up the computation
so stop words of course are words like
of and then but I don't tell you that in
rare words are words that occur fewer
than five times in this case okay we fit
a 100 topic lda model using mean field
variational inference
how did
so so what I did was I looked at my
articles and I computed the vocabulary I
looked at how much words occurred in
those articles and I also computed how
many times each one occurred and so if
something occurred less than five times
I just removed it from the vocabulary
it's as though that article didn't
contain that term the number of topics
it is right now it is here yeah in a
different models okay all right so I ok
so that article that i showed you before
actually wasn't in the collection that
i'm looking at it it's we're going to
think of it as a test article and what
we can do is we can take that article
yuri do they believe i have a list i
downloaded it in two thousand so it's
not a great list okay so here's a same
article from before again seeking life's
bear genetic necessities and what i can
do is i can so let me stop first I take
all the other articles and I fit one of
these models so I get my topics I get
all this structure out of it and I know
what the what the 100 distributions over
words that generated those articles are
now I take this new article and I do
posterior inference so I ask ok what is
the say I hold those topics fixed what
is this hidden structure of this article
in other words what topics generated the
words in this article and you can look
at the posterior distribution the
distribution of the topic proportions
given the words here so this is like a
the inferred histogram from from this
article that the cartoon histogram that
I drew this is the inference of the of
that histogram if we fit a model with
real data when we fit a model with real
data and what you can see is that out of
the 100 topics only a handful of them
seem to have been activated yeah your
arms influence but regardless the
estimation process itself so you decided
the door
topics that's right is there any sort of
topic information supervision type of
supervision from none yeah I should have
stress fully on super funds yeah yeah so
let's just go back to the graphical
model remember the only thing that we
observe is are the words of each
document this is all the information we
have so everything else is inferred
those 100 distributions over words are
inferred that's a good point so there's
no that we don't encode anything like oh
there's this thing called genetics
there's this thing called evolutionary
biology we don't encode that at all into
this model okay and so here what you can
see is that just a handful of those
topics have been activated so it says
well I know the topics and I I see all
these words and I think okay these words
came from this handful of topics and
moreover we can then look at so remember
each of those topics is a distribution
over words and we can look at at we can
order the words by their probability
under the topic and we can look at the
first few and what you can see is that
they correspond to things that we
recognize as genetics evolutionary
biology diseases and computer analysis
and again remember these ideas weren't
previously encoded into the model just
by looking at the patterns of word
co-occurrences they they come out of the
analysis yeah this is nervous
it's the approximate in the in the exact
model from how you have I'd like to know
exactly how to do it with the exact book
you have some distribution
proportions that's right so that's right
so oh just to be clear what I did so
what we're interested in is the
probability of theta that's the
histogram given the document okay hold
hold the topics fixed okay i took i
estimated that distribution i took its
mean that mean is a histogram and that's
what I illustrated okay right so that
was the quick review of lda we can kind
of take all these documents we can
analyze them we can learn these
distributions over words seem to
correspond to topics so this is a
powerful model for one visualizing this
hidden thematic structure in a large
corpus although I want to be cautious
it's you don't want to over interpret
these things and say oh well you don't
want to make like decisions about
whether or not you should approve a drug
based on these kinds of inferences but
for visualizing some kind of structure
that might be useful to somebody trying
to sort through these documents lda is
useful and for generalizing new data to
fit into that structure now kind of
where this sits in in in previous work
it's a it's a mixed membership model in
statistics that that's what it's called
meaning instead of belonging to a single
topic like a finite mixture model would
the documents can belong to multiple
topics through this through this topic
histogram and the idea that the words
are grouped but each one can come from
different topics and this really builds
on the work of lsi and pls i which were
alternative ways to do this this is kind
of more of a fully Bayesian or fully
probabilistic version of those of those
models okay and now it's nice about
casting this in a graphical model
setting is that graphical models one are
modular which means we can embed them in
more complicated models and that's what
we're going to do next but to there's
something general about these graphical
models in the sense that I'll go back to
it again in the sense that this data
generating distribution can be changed
okay so for example people have used
exactly this model instead of modeling
words as grouped and documents modeling
pixel
grouped in images where instead of
generating words from topix this is a
distribution over say color histogram
values and they're generating pixels
from different color histogram topics
okay this this data generating
distribution can be changed to whatever
you like and the underlying inference
algorithms for dealing with this
complicated mixed membership structure
remain the same that's what i mean by
general yeah so this could be extended
to any conjugate expenditure yeah that's
right have you paid room
a little bit with images what actually
the data generating distribution doesn't
necessarily have to be conjugate to
anything it's and but the the dearest
lay structure to its left that stays the
same in what I'm discussing now that
makes sense yeah okay although we didn't
talk about it much variational inference
is very fast and it allows us to analyze
these kinds of large data sets and
finally if you want to play with this
model you can download some code at my
website and see that you could use to
analyze this data that's how I analyze
this data Sam just a sort of general
question both the variational methods
and the mcmc methods because they never
really mix have the property that they
sort of find one mode one plausible
posterior mode yeah approximated and
their strength comes from the fact that
not only you get a sort of point
estimate but you get some indication of
the volume the mode that allows you to
do sensible
how do you sing concert yeah but from a
statisticians point of view these things
that you're estimating are really
nuisance parameters and you know if I
run it once and I get some set of topics
and I run it again I get a different set
of topics that's okay because in
principle there should be sort of
integrated out so the question is what's
the sort of quantitative measure of
success here what's the goal like I
should preface it by saying I think
these are great and I'd have done a lot
of this yeah exactly as you say it's
incredibly useful for interacting with
the data but from a statistical point of
view it seems like any particular model
that you fit has no real status because
really those are just excuses firmers so
what how would you measure how well
these things are doing how would you
state what the goal is because I think
if you go to Professor defense mayor on
no no I think you're right keep going
Sam yeah um that's a good large question
that keeps me up at night now so in in
in this paper what we did was say well
how can we measure how good these models
are we can measure them by how well they
fit data and so we hear what we said was
if we look at held out data in other
words take a collection take a part of
the collection and pretend like you
haven't observed it and ask what
probability does your model assigned to
that unobserved part that's a measure of
how good the model is but that doesn't
really satisfy you it shouldn't because
what kind of task is that I you don't
wake up in the morning and decide I want
to assign high probability to things
that I see and so you know what I would
i have come to kind of think is that
these unsupervised models what they
provide are useful descriptive
statistics of data that you can't get
otherwise and so what you'll see in the
next section of the talk a little bit is
how you might use these kinds of models
to build browsers of exploring data
that's otherwise unstructured I mean
while it's not a rigorous statistical
question the fact is that JSTOR has
scanned all these documents they have
them all in no order at all and there's
no topic labels to them and you have to
do something to organize that if you
want to build a browser for someone to
say oh I'm reading this article about
astronomy hey here are some other
articles about astronomy that might be
interesting clear example given a
document find the most similar documents
in your repository yeah we're going
explain a scene and different motors and
kind of us people to say okay which
which really is more similar to do right
evaluated that way we haven't evaluated
that but but but yeah that's I'm going
to show that that kind of measure of
similarity in the next part of the talk
we should probably let you get that's
another part of the song because I
expect these questions are going to
apply just as well oh yeah yeah see I
mean these are these are good questions
so right but the third I did want to say
one more thing which is something that
I'm looking into now is developing these
models where there's some response
variable associated with each topic and
you want to predict that and suddenly
it's so nice because I can sleep again
because I know that I can measure how
good two models are at predicting a
response in a way that's satisfying but
I don't think that we should belittle
the sort of vagueness of useful
descriptive statistics of unorganized
data because we all want to organize
data isn't that yeah and man why are you
asking that question here because you're
on sabbatical alright so next thing I
want to talk about our dynamic topic
models so this goes a little bit to your
question where where you know you can
you can draw this pretty picture and you
can say well what what are the
assumptions here you know this is a
seems like a reasonable thing but there
are assumptions here and and and are
those assumptions appropriate so one
assumption that this picture is making
is that the documents are exchangeable
what what is exchangeable mean in terms
of probability distributions it means if
I reorder my documents and ask what's
the probability of those documents
shuffle
old under my model it's going to have
exactly the same probability no matter
what ordering I give it okay and so in
other words the order of the data
doesn't matter but in the case of what
we're trying to do here with science
this is really quite restrictive so
here's an example here are two articles
from our complete corpus this one's from
1890 called instantaneous photography
and it's a nice article and this is from
1977 called infrared reflectance in leaf
sitting neotropical frauds okay now
these two articles are really both about
photography right in 1890 there was some
science of photography and somebody
wrote this article in fact it was
probably the beginning of photography
and luckily their cousin was around to
take pictures of throwing the discus and
and in 1977 somebody's photography was
about taking pictures of these frogs in
tropical places and and a whole
different kind of technology was needed
to do that but if you're interested in
photography you might be interested in
both these articles now the problem is
that these are about the same topic but
they're really not exchangeable you
can't imagine that this article could
have come before this article and so
what we wanted to do was develop a topic
model where the topics evolved over time
I should say that this work was funded
by you guys so thanks and so the idea is
yeah mark the vocabulary of a particular
topic is going to change of it exactly
exactly so if I want to talk about
photography and I'm a guy in 1890 I'm
going to use a whole different set of
words than you in the year nineteen
seventy-seven or talking about
photography okay so the idea is pretty
simple we're going to divide the corpus
in two sequential slices by year so
we're going to say okay you know all the
articles in 1890 slice at91 slice 1892
slice and so on and we're going to
assume that each slices documents are
exchangeable so I don't care if
something was written in September of
1890 versus January of 1890 but I do
care if it was written in 1890 versus
1960 and then we're going to allow these
topic distributions over words
to evolve from slice to slice okay so as
a graphical model you're now experts
this is what it looks like okay so let
me kind of unpack this for you a little
bit if you turn your head this way you
recognize each slice is the same LD a
model that we just described where each
document comes from a distribution over
topics choose a topic choose a word and
so on okay and here are the K topics for
that slice so this is 1890 now in 1891
what happens is the topics march forward
one year and they change it a little bit
and now here is the LDA model in 1891
and in eighteen ninety two and all the
way up to 2000 and to use that the
population of topics is identically you
can't have that you can't have a new
topic springing into existence halfway
through it models that by simply not
using it until a certain time but yes it
doesn't make that assumption so that
topic has zero zero probability up until
the first atomic bomb and then
yeah but but that's actually a
limitation of this of this model but but
but he gave in when Pablo just said
because the alphas are me they're alphas
are constant over time but in the
posterior it just simply won't use a
topic if it's got enough enough momentum
later on that occasion but as a
generative model it doesn't know about
the idea of topics appearing and direct
or just knows how to hijack a convict
yes yes yes exactly so right and that's
a and that is a limitation with this
model it you think that there are a
hundred topics that somehow genet right
laser is existed in 1890 but they just
didn't nobody just started talking about
them until nineteen fifty yeah but a
failure as one topic actually mutating
into another topic over time it seems
like that's also possible birth right
that can theoretically happen here
although we didn't see it there are some
kind of funny hyper parameters let me
get to the model and I can tell you
where that could happen instead he'll be
like oh chemistry physics nope nope this
is the simplest possible dynamic topic
model we could think of where there's
just k topics remember we don't know
anything about chemistry or physics
because we don't know yet what the
collection is but there's just K topics
and each one involves separately over
time that's the assumption okay the
question is and this relates again to
this criticism of the deer schley is
that how are we going to model topics
evolving over time the deer schley isn't
really necessarily to amenable to that
and so what we did was we took something
called a logistic normal distribution
which is a distribution that basically
you take a multivariate Gaussian draw a
vector from it exponentiate that vector
and then map it on to the simplex so
that it sums to one and that gives you a
mapping from a Gaussian random variable
to the simplex to the dist to the set of
vectors it's a positive vectors at some
21 and so we did was we combine that
with a state space model a basically a
dynamic a dynamic model to build the
notion of a distribution over words that
changes over time so the idea is this is
bad notation but beta before was a
distribution over words now beta is
just this real valued vector so beta T k
so this is the teeth topic sorry the
cake topic at time T so in nineteen
sixty this is one of the topics a
distribution over my words given that
same topic in 1959 is going to be
distributed as a normal whose mean is
that topic in 1959 although when I say
topic I mean real valued and then some
little covariance which which represents
how much it can move from year to year
okay and then what we do the probability
of a word given this beta value is just
don't worry about this if you don't
understand it it's basically the way of
mapping this to a probability
distribution over words okay so it's
just a little Gaussian random walk in
topic space for each word so if I have
20,000 words and a topic then I have
20,000 little chains marching forward in
time and words like I don't know
old-fashioned words for camera could
lose steam and words that are as new
words for camera appear it gained steam
and we have this process for each topic
hey draw it as a graphical model like
this where you see that each top the
topic in each time oh look at that I
reverse the indices depends on the one
previous it's clear yeah we could
estimate Sigma but we just set it to
something small and this answers your
quest oh not really but this is the
hyper parameter I'm talking about if
Sigma is too large then the model will
be able to explain new topics by just
saying oh yeah I know it was these words
but then they all change because it's so
variant varied and now it's these words
and so the if Sigma was too large then
the model would be able to do that but
if you just set Sigma to be pretty small
or put a prior on Sigma that makes it
want to be small then you don't have
that problem
variation over time so it seems like
that might be tricky Lisa it's sensitive
if you make it too small let me show you
what happens and I'll answer that
question later on mark punk okay so our
goal as usual is to compute the
posterior distribution all we observe
hiding that are these words and we want
to compute all of the hidden structure
but now that hidden structure is even
more than before it's everything over
all the years it's all the topics and
for each year it's all of this hidden
structure in for each document okay and
again exact inference is impossible if
you don't believe me just think that at
each time slice we have an Lda model for
which exact inference is impossible and
moreover actually in this case whereas
things like Gibbs sampling are practical
for lda gibbs sampling is not practical
one because of how much data we have and
two because we've lost this conjugacy
property that made the math easy and
gibbs sampling really relies on that to
be efficient so you'd have to do some
kind of Metropolis Hastings thing and
that involves tuning a whole bunch of
other stuff so here we use we did
variational inference again to fit for
all of these articles this entire time
series of topics and I can go into that
in more detail oh go into a little
detail now but really more details at
the end if you want it so let me just
talk a little bit about what variational
inference is quickly so so you know and
might want to learn about it okay so
forgetting everything we're what we're
after is a distribution of these hidden
variables given our observations and
what things like Gibbs sampling an mcmc
that we've mentioned a few times today
do our define Markov chains on those
hidden variables such that the limiting
distribution of those chains is the
distribution we care about and then you
approximate the distribution you care
about by looking at empirical
distribution of samples from that chain
the problem is you don't really know
when it got to its limiting distribution
so you just guess or use some heuristics
variational methods is an alternative
approach to a
approximating that distribution and the
idea is you define another family of
distributions q on the same space of the
distribution you care about here it's
the latent thematic structure then you
find the member of that family of
distributions that's closest in KL
divergence to the true posterior so here
p is the true posterior that we care
about and q is this family of
distributions that we can search over to
find the member that's closest to pee
and the idea is that if you define a
simple family of distributions which
doesn't contain your actual posterior
then this optimization problem can be
you can find a local optimum of this
optimization problem so you're kind of
trading off not knowing when the chain
is converged to the true thing that you
care about to not knowing what
assumptions you're making by simplifying
your family of distributions q but at
least knowing when you get to a local
optimum okay and if you're interested we
can go into mathematical details at the
end of this talk if anyone's interested
okay so now instead of analyzing ten
years of science we're going to analyze
the entire collection of JSTOR science
1882 2002 and just to remind you there's
no reliable punctuation metadata
references or anything like that we
really have to take this kind of
statistical approach where these are
just a bunch of words it's just a bunch
of ASCII text and I'm going to try to
find structure in it here we have a
vocabulary of 30,000 terms this is
actually a little incorrect week we do
have 30,000 terms but we compute them in
a little complicated way I can tell you
about it later in short the data are 76
million words and 130,000 articles ok so
here again is one of these articles this
is called sequencing the genome fast
from 2000 and again I can do so I can
look at the two thousand LD a model and
ask for the topics of this article is
about and here again is that histogram
where a handful of the article of the
topic seem to be activated and again we
can look at the top words from those
articles and see that ok here are words
about genetics it's capture that again
here are words about devices and techno
gee it's captured that and here are
words about data analysis yeah question
how long did you take to build that
model how long does it take to do prints
on one article inference on an article
is very fast I can give you a kind of
order of order of 30 seconds maybe 15
fitting this model I think it was like
24 hours but this is all I usually don't
have to talk about this or people
usually aren't interested but this is
all very parallelizable so it's this has
this kind of am flavor where you can
spread out your e step across many many
nodes and then combine them to create
your model and then spread out your e
step again across many nodes and combine
them and so if you can do that then then
you could fit these very quickly I've
heard you guys can do that so and so
another thing we can do with this model
is now we can take this topic right here
is hear these words device devices
materials current but this is of course
the year 2000 and we can ask okay what
did that distribution over words look
like over time and hear what I've done
is I've looked at the top 10 words from
that topic topic 35 or whatever it might
be and I've listed them at each decade
so there really are distributions over
words through each decade but I've just
taken each decade and said okay here in
two thousand we have device devices
materials current gate and as you go
back in time you can see that the model
seems to have captured something
something salient about these documents
through time so it starts out by talking
about electric machine power engine
steam iron battery and through time it
ends up talking about device devices
materials current gate I don't I want to
caution against over interpreting these
kinds of things you don't want a
historian to look at this and say this
is definitive evidence that blah blah
blah blah blah but nonetheless we can
kind of casually agree that these things
are related to each other that the way
one talks about machines engines steam
and iron and batteries in 1880
one talks about devices current silicon
technology and materials in two thousand
but remember we're casually agreeing to
that what's that what happens to two
since yeah yeah tubes went out of style
unfortunately what that well they're
still there 1970s / now because of tubes
amps tube amps yeah alright so the other
thing we can do is take a single word
from from one of these topics and ask ok
let's look at its probability over time
and this is going to this is the second
part of your forward thinking questions
so here I've taken two topics I really
shouldn't name them because I want to
stress that these topics have nothing
they're not named in advance we don't
know anything about physics or
neuroscience in advance but I took two
topics one which kind of looks like
physics and one which kind of looks like
neuroscience and I've plotted words
through time of those topics so here in
the physics topic you see words like
force kind of loose theme over time
words like relativity peak and words
like laser peak later on in life and
here in the neuroscience you see words
like neuron going down so I nerve going
down neuron going up and then an oxygen
kind of peeking here oxygen is
interesting for two reasons one of which
is funny and the funny reason is that I
kind of was confused by this and I
looked up why oxygen was peaking and
basically there were all these crazy
experiments in the 50s and 60s I think
where they would deprive people of
oxygen and then ask them questions like
have them solve problems and see how
they did it was crazy and the not funny
reason but still interesting is that
this captures a point I want to make
about these models which is that they
can somehow capture polysemy so it's not
as though the word oxygen is less
popular later on people are still using
the word oxygen in droves but the thing
was it wasn't used within the context of
neuroscience anymore and so oxygen
actually has low probability
you're even though it has a high
probability and other topics and it has
high probability here and right so the
same word can have high probability or
low probability in different topics it
and and yeah I'm losing articulate Ness
so keep it at that Mark's point
relativity wasn't invented until
somewhere around here yet you see that
it's slowly ramping up to that now it's
not as though Einstein is thinking about
it and like he's like got these ideas
it's this is the Sigma parameter again
it only lets you move a little bit and
so to explain the frequency of
relativity here it needs to it needs to
say okay well it actually moved a little
bit of yeah it's over smoothing that's
one word for it so right and something
we'd like to do actually is embellish
this model make it more realistic to
have a model of not just topics
occurring but of words occurring at some
point because really that's that seems
to model more what's what's happening in
the data okay so I want to now go back
to this point that this is all fun but
what's it good for and what I said it
might be good for is useful descriptive
statistics of data that otherwise
doesn't have any structure and here I
want to show that in this similarity
metric that you mentioned so one thing
we can do with this with this these
hidden variables so remember theta are
the topic proportions those histograms
we can ask let's take the theta for one
article and let's take the theta for
another article and ask how similar are
those Thetas to each other ok and a and
these are each distributions over our
hundred topics and so we can use the
hell injured distance which is a
symmetric distance between distributions
as a measure of how similar to articles
are so I'm an article and you're an
article and we each have our own topics
and our similarity is defined by the
similarity between our topic proportions
okay so you can see of course theta is a
hidden variable so we take its
expectation given the different
documents so WI is a document WJ is a
doc
given those what's the expected distance
between their topic proportions and a
model okay so basically what we're doing
is we're using the latent structure to
define similarity now I think this is
particularly interesting in the dynamic
topic model because time has now been
factored out because remember I might be
an article from 1950 and you might be an
article from 1910 now to get your theta
you used a totally different set of
topics than I did but yet we're
comparing these two Thetas because we
believe that these Thetas have something
to do with each other so this is kind of
like a time corrected document
similarity metric so here's an example
this is an article called the brain of
the arraign it's from 1880 and basically
this article is about the scientist who
found an orangutang bopped him on the
head cut him open drew what he saw sent
it to Thomas Edison who was the founder
of science and got it published ok now
we take this article and we look at what
are the posterior probability
proportions here and here we have our
distribution over the 100 topics and you
can see a handful of them are activated
now in the entire 130,000 article
collection we can ask what's the most
similar article to this article okay
this is all the articles spanning the
120 years and the most similar article
to this article is representation of the
visual fields on the medial wall of the
occipital parietal cortex in the owl
monkey from 1976 which arguably is about
exactly the same thing only this
scientist had to use a totally different
language for describing what he saw when
he popped the monkey on the head and and
a classical similarity metric won't find
any similarity between these two
articles because it's based only on word
use and word use drastically changed
between eighteen eighty and 1976 but
nonetheless we might as scholars and
scientists using JSTOR actually be
interested in these two articles being
determined to be about the same kinds of
thing
so all this can be put together in a
browser yeah does that mean the one of
your hundred topics is dedicated to the
bumpy monkeys on the head oh no these
are both I think that this big one is
like neuroscience and this little one is
monkeys everyone but I'm not sure when I
don't know I just joke around so um but
but not yeah it's basically the
neuroscience thing but but the it's
funny when we when we look at these
pictures we and and say look at the
words with high probability we take a
lot of stake and what's the biggest one
but actually this similarity uses quite
a bit these other ones that's what
differentiates these two articles from
each other rather than any other
neuroscience articles because it really
takes into account the structure
everywhere else okay oh yeah so what you
can do then is take all this hidden
structure again useful descriptive
statistics and build a browser of this
otherwise totally unstructured
collection of documents so here's
another example so we did this we're
here at the top you see an article
automatic analysis theme generation and
summarization of machine readable texts
okay this is an article from 1994 by
Salton and allen &amp;amp; buckley and i
actually who's the last author on this
someone from google yeah yeah it is I
shouldn't have covered him up but
nonetheless so yeah so this is what kind
of a classical article about doing the
things that we like to do and what we
can do is we can say okay here's this
article and when you click on this you
get to the original article here are the
three topics that it's about so these
are the topics of highest probability in
those topic proportions you can see one
is about computers and data one's about
library science and then one is just
this weird topic two three four
different single that kind of captures
other words quantitative words looks
like and so so that's one thing you can
look at and then here we have a list of
other articles in the collection in
order of their similarity by that time
corrected similarity metric that I just
described and you can see that the top
article is global text matching for
information retrieval that's another
article by two of the same author salt
in Buckley so that makes sense and
that's something that a classical or
traditional similarity metric would find
but not too far down the list in 1962
simple and rapid method for the coding
of punched cards and here's a figure
from that and that's an article
basically about in the 60s taking punch
cards and encoding authors and subjects
and titles or I guess not titles just
authors and subjects on these punch
cards arguably the same type of
technology that's being developed here
again a totally different language
totally different use of words but if
you're looking for the the article
similar to this you're going to want to
read this article and then again it's
still in the top 10 we have from 1899
the storing of pamphlets a fascinating
article about the problem of having
pamphlets all over your office and
needing to store them places and again
arguably this is about scientists trying
to deal with all their information and
this is about scientists trying to deal
with all their information to and this
is a good article it's basically about
how you should get a box and put them in
the box all right
just aside on there and getting back to
be very question about how you evaluate
this stuff I remember coming back from a
conference and showing this paper and I
think it was the Jordan fly to Amon
single and then said you know it's
really interesting to take a look at
this and he looked at it look at the
evaluation section he said uh perplexity
yeah that's what we always measure but
we couldn't get me voters in truck we
didn't even try but yeah but yeah but
yeah I mean that perplexity is basically
held out likelihood although that's what
I'm going to show now but there's no
labels here so you can't do precision
you can invent some tats you can bet
grab a few students to kind of female
labor something in kind of user testing
something I'm talking with some
psychologists about of these user
studies that you're talking about where
you could do something like say take a
bunch of articles and divide them in
half and ask someone to match the two
halves and then see if you can do as
well as people at matching the two
halves that might be a better measure of
how useful it is I mean you're and he
are right that perplexity is not
satisfying because you just might not
care if if down the line something that
gives us better worse perplexity
provides just as good a browsing
experience or whatever it is we care
about then who cares about perplexity
but I don't think that you would get
that I think if we did a user study with
the similarities up I don't think you
would get the same kind of the same kind
of output right the easiest thing to do
is if you have two algorithms each one
will give you kind of most similar
documents a top three of them matches
and then kind of you know hide which
which is coming for agent and have a
bunch of users say and they should tell
you this is better than this oh this is
worse than this but do we really want to
be the business of inventing a problem
to evaluate the solution we've already
developed and I think dave has an
interesting
at a solution
well
well one thing I don't have is users so
for me that kind of infrastructure is a
bit more difficult to implement but I do
think it would be very very interesting
so so in so one thing I mentioned this
polysemy thing Tom Griffiths was a
cognitive scientists did some work on
correlating this with the sore eye and
people's notion of of words that go
together and found correlation so that's
another alternative measure of how good
the model is ultimately the model is
good if I think JSTOR wants to use this
kind of thing yeah I guess that's really
yeah yeah but at some point you want
people say aha we beat Goliath I'll with
the 10 strip right here here's their
mother who's our motive inch lift look
we beat them on the electric yeah I
agree yeah yeah Sam was kind of talking
about which is if you think of the month
was nuisance parameters or something
that is really
right and so there's this issue of
stability if you were running on
multiple times on different samples of
the data so what extent are you
generating the same talk yes you guys
want to know I just had a conversation
with someone about this like two weeks
ago and I think that's a great idea to
understand the correlation between the
topics found by different subsets of the
data as a measure of how good it is
because somehow if you are doing these
kinds of descriptive statistics than
what you want is exactly what you said
you want stability you want to know that
if I didn't happen to see these articles
I would have gotten the same results
yeah and that's a that's exactly right
these data are all from the journal
science so they say I have an article
about baseball not synthesized at all
yeah maybe invited on item so now you're
said malice is article the models I'm
presenting now can't really do that but
there are there are this other class of
models called nonparametric Bayesian
models that are that that have that idea
of built into them that here's an
article it's about something new and
those are that I've worked on those as
well those are those are worth
considering yeah maybe you're selling
yourself a little short because you
could evaluate the marginal
log-likelihood are approximated for such
a document to exist at such a time in
the collection absolutely you can find
that you could do outlier detection yeah
yeah you could kind of look at the
likelihood and say oh that likely its
way down there everything else is
punched over here absolutely just a
proportion of words that are into a
carpenter you would tell you it's fun
all right yeah there's always an easy
hack but no one get a lot if you have to
go yeah
I decided formation with the author's
name is a word none so so what JSTOR
does is they really just take the
journals scan them and then plop the raw
a ski down in a file so there's no
nobody has marked up nobody has marked
up authors or citations I mean all these
things are very yeah yeah nothing all
these things are very useful they now
have bibliography information the way
they did it was by farming it out to
people to do by hand so they have some
bibliography information exactly you
want to cap these are all hidden as
patterns and this just raw data no
that's something we haven't mentioned
that all the words are assumed to be of
equal weight
right right right and again tricky
Python coding could get more information
out of this perhaps but the the point is
that you can actually get a lot out of
just collections of words grouped into
documents okay
yeah yeah I think looking at stability
is important and I also think that well
at the end I'll elutes just some things
that I think are needed in this model so
we're running out of time well we've
we've invalidated this graph so I can
skip it but since I worked hard on it
i'll just say quickly that what this is
is so we can ask how much better is this
dynamic topic model then the fully
exchangeable topic model you know is it
really fitting the data better and this
is negative log likelihood of a held
outset actually so not exactly a held
outset what we've done is we've said
okay imagine I've seen the articles from
eighteen eighty to nineteen ten okay now
i can do one of two things i can fit an
Lda model to those articles and then ask
what's the probability of the articles
in 1911 or i can fit a dynamic topic
model to those articles and ask what's
the held out probability of the articles
in 1911 so a better model of prediction
will assign higher probability to the to
the articles of 1911 and so lower
numbers here are better and what you can
see is that the dynamic topic model
always provides a better fit and I'm
going to ask you to do something that is
impossible which is ignore the shape of
this curve so I made a mistake and I
didn't normalize for the number of words
there's different numbers of words in
each of these years and so that affects
of course additively the negative log
likelihood and so this shape is
meaningless what's meaningful is that
the dynamic topic model is always lower
than the fully exchangeable model
all right this is held out likelihood so
more parameters should mean overfitting
in a sense although they're not exactly
parameters because we've we've
regularized via this dynamic process
right because essentially in both models
we have a prior over over the topics and
so one is d really prior here it's a
dynamic prior that's essentially what
we're comparing other than that the
generating process is the same oh that's
a good we didn't we didn't look at that
so that's it that would be a great thing
to look at to take the model fit from
eighteen eighty to nineteen ten and then
fit from 1911 to 2002 and see how yeah
it's great idea the fact that day so
suppose I gave their day just 1905 to
1910 yeah just everybody maybe does the
fact that the dynamical that has more
historic information would that help it
in any way yeah so I tried that I went
just one year back and the LDA
perplexity just the LD a negative log
likelihood just shot through the roof it
was terrible yeah so in other words lda
might be being hurt by giving it more
information as what you're saying from
further back ya know I we investigated
that it it's off this chart okay so
right let's just go to that so
complicated stuff leads to a another
model where again so one of these one of
the problems with the model like you
mentioned this dear Ashley over the
topic proportion so if a documents about
genetics and evolutionary biology then
and we're putting a dear Ashley prior on
that that's actually saying seeing
genetics and seeing evolutionary biology
are totally independent but we all know
that that's not true that these topics
if they are topics that we're
discovering are going to have some kind
of correlation structure and the deer
schley actually prevents us from
modeling that because there's a hidden
assumption in the deerslayer which is
that these components these these these
slots in that cartoon histogram are are
almost independent they're slightly
negatively correlated because it all has
to sum to one so one thing can't be
really big without something else
getting really small but there you can't
you can't model a correlation structure
that that geology happens with astronomy
for example and so we relax that again
with this logistic normal where we draw
a multivariate Gaussian there instead
and from that to make a long story short
you can find a map of these topics so
here we have our topics as before and
now the size is proportional to how
often they occurred in the corpus and
this is from 20 years of science so this
is an exchangeable model but one where
these topics are mapped and you can see
things like you know we have
neuroscience somewhere up here and we
have biology over here and we have
geology over here and so we can capture
this mapping and again this might be a
useful description for someone who wants
to browse if they are interested in this
species in force and then they say oh I
really should read all these articles
about dinosaurs too okay so I know we're
out of time the whole point of this one
these probabilistic topic models provide
useful descriptive statistics for
analyzing and understanding the latent
structure of large text collections by
building browsers by organizing what's
otherwise unstructured and this isn't
limited to just scientific articles
there's lots of unstructured data for
example I think about query data with a
dynamic topic model that over say 20
years you might want to analyze try
and patterns in query data and these
kinds of models can help you do that
more generally probabilistic graphical
models are useful way to express our
assumptions about the hidden structure
of otherwise complicated data and to
figure out what that hidden structure
might be under that model but of course
paying attention to the fact that we are
actually making a lot of assumptions
about it all of this posterior inference
on such large data sets is thanks to
variational methods and to answer your
question so what are the kinds of next
things that we want to do with this well
there's all kinds of problems with that
dynamic topic model so one is choosing
the number of topics so in el dÃ­a
there's this nice nonparametric Bayesian
formalism that lets us model the notion
of oh here's a new topic and it just
appeared and we can do that within this
probabilistic setting now it doesn't
quite work in the dynamic topic model
because of this assumption that the
topics exist for all time and are
changing even if we're not using them
and so what we're thinking about is is
adapting what are called birth-death
processes where you imagine a topic
appearing and disappearing but then
building that into the model and
building all the inference machinery is
not easy something else we're working on
our continuous time dynamic topic models
so somehow if I write an article on
December thirty first nineteen
sixty-seven why should that be in the
1967 model rather than the 1968 model
and so we're working on models where
each document has a time stamp which
they do and we want to build one of
these topic models that that smoothly
changes the topics over time rather than
having these rather arbitrary discrete
points topic models for prediction this
is what I mentioned helps me sleep
because now we have documents and we
have something we want to predict them
we have a way of actually evaluating it
without a long discussion which I
enjoyed and finally something another
interesting I think this is really
interesting so I mean I that's I think
this is interesting I mean it I want to
do this which is inferring the impact of
a document so there the idea is this see
in the exchange will model you don't
have this luxury but
lalala okay it's not going to be good to
be here but anyways ok so right least in
the exchange will topic my lead on this
luxury but here we have this notion that
we have an ordering among the documents
and we could say that look when Einstein
wrote this article in 1917 that turned
out to be a very influential article and
we can model that with a hidden variable
a hidden variable that says this is the
influence of this article on the future
and basically this is a bibliography
free method of assessing how much the
word use of this article affected the
word use in the future and so we're
working on a model where each document
is endowed with its own impact basically
and then you can determine in a
posterior sense how influential was this
article or news article or query or
whatever on what happened subsequently
and this lets you take things like
bibliometrics which really were
restricted to analyzing citations and
analyze lots of other kinds of
sequential collections like emails or
queries using the same idea of impact so
that's something another thing connor if
they're anima but yeah citations
analysis would if you gather with this
movie
data sources well there's been good work
on that partly by Dave and and and yeah
so I'm not working on that personally
but but yeah so one thing nice about
these models talk to Dave about this is
that there you can add structure and and
because it's in this graphical models
formalism you can kind of add other
variables for things like citations or
authors people have worked on what are
called author topic models where they've
added those kinds of information and use
the authorship to again influence the
the topic proportions and things like
that yeah so my question would be having
this very shamokin our simulation
interest but it makes it easier or more
efficient I think that the author topic
model could be fit with variational
methods just as easily yeah there's a
real deep connection between in the
fully conjugate case the coordinate
ascent variational algorithm and the
Gibbs updates I can show you them
afterwards if you want to have a slide
about that okay yeah all the full
correlation matrix ago talk good so yeah
I didn't mention that so that kind of
saying so yeah we we do and to get a
sparse one what we use is this lasso
trick by bumin in my chosen which is to
find sparse correlation matrices via a
lasso penalty and so that's how we
that's how we got this kind of nice
sparse graph from our basically 100 by
100 correlation matrix that's how we
find the zeros in the inverse
correlation matrix
the universe cool yes far as the inverse
correlation because that denotes the
dependency structure other question up
Thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>