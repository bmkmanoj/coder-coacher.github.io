<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>We have it easy, but do we have it right? | Coder Coacher - Coaching Coders</title><meta content="We have it easy, but do we have it right? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>We have it easy, but do we have it right?</b></h2><h5 class="post__date">2008-11-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DKVRkfXrBpg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you all very much for joining us
performance tech talk today I'm Brad
Chen and it's a pleasure to introduce a
mere dhawan from the University of
Colorado although he seems to know more
people here than me so I'm not sure I
should waste too much time of his time
since it's his talk but but yeah very
good so mayor please take it away thanks
a lot Brad and thanks all for coming so
I'm going to talk about work that I've
been doing with my students and
colleagues at unity of Colorado and one
colleague at IBM Research now before we
start most work in experimental systems
one of the first steps that we do is to
identify bottlenecks so for example
let's suppose we want to improve commute
times in an area and so what we might do
is we might give out a number of surveys
to drivers and we asked them that at the
beginning of the commute they should
record or at the beginning of each
segment of the commute they should
record the starting time do the commute
and then the commute ends they should
record the end time and since we collect
data from a number of drivers hopefully
our data is representative of what
really happens now the next step that we
often do after finding bottlenecks is
that we act on the data and in this
particular case we might realize that
maybe we should build a bridge across
this lake and thus avoid this bend which
is particularly slow for the commute now
this bridge allows you hopefully to do
your commute faster but what if the data
is wrong what if we have identified the
wrong bottleneck in that case we end up
with a bridge that nobody needs now how
could the data possibly be wrong let's
suppose this is what happens in reality
when no one is measuring anything
so you do your commute and then after
the rush hour ends there is a train that
scheduled to grow across the road now
then you actually do the observations
the collection of the data delays the
commuters ever so slightly so that now
rather than finishing their commutes
before the train comes they get to the
train tracks they have to wait for the
train to pass and then they finish their
commute a consequence of this is that
this commute looks a lot slower than it
would have if you were not observing the
commuters this is an example of a
phenomena that's known as the observer
effect and it comes about because the
act of observing a system might change
the system now this particular study
it's an easy study to conduct we gave
out a number of surveys and then left it
up to the individual drivers to the
individual commuters to fill out the
forms and to use their stopwatches
however it's not a right survey because
it changed the data now if you look at
this a bit more abstractly this red
block here represents the prefix of the
commute until you get to the train
tracks we can move this prefix around as
long as you don't get to the train
tracks while the Train is still there it
is not a problem but if you move it so
that you get to the train tracks while
the Train is there you suffer a
significant delay and thus your commute
time is much longer now so far I've been
talking about trains and you're probably
wondering if you've come to the wrong
talk well not
let's think about computer systems and
instead of a commute prefix imagine this
red block is a hot object the object
might be a data object or it might be a
code object and this barrier here this
wall represents a page boundary just as
with the commute I can move this object
around but at one point I will end up
straddling the page boundary so rather
than fitting on one page I now fit this
object on two pages and when that
happens your program will probably run
much slower because you have additional
paging activity now why do objects move
around when you collect data imagine
this is our program and this is an
important loop and this here gives the
memory addresses of the various parts of
the program now in order to collect data
at the very least you need to add a
little bit of instrumentation at the
beginning of the program to initialize
your data collection and then you may
need to add some more instrumentation at
the end of the program to record the
data that you collected now what has
happened is that as a consequence of
adding this instrumentation we have
shifted code around and because we have
shifted code around as we saw in the
previous slide a hot loop that used to
fit on a single page might now fit on
two pages
so in this slide i'm going to show you
that this phenomena actually happens in
practice i'm presenting the data for
just one benchmark in this graph and
then i'll show you all the other
programs that i have used and what we
are doing in this case is that we are
collecting data using hardware
performance monitors so hardware
performance monitors are perhaps the
cheapest way to collect data from a
computer system because the hardware
does all the hard work of doing the
increments all you have to do as i
showed you on the previous slide might
be to initialize the counters at the
beginning and then read them out when
you need their values along the x-axis
as you go to the right we collect more
and more hardware performance monitors
on this particular machine you can
collect up to 18 monitors at the same
time and so all of these can be
collected simultaneously by the hardware
the y axis gives us the ratio of the
execution time when you collect only one
metric which is cycles / when you
collect n matrix where n is the number
on the x-axis the thing that is
surprising to note for this program is
that going from one metric to two metric
can actually speed things up by about
seven percent and going from two to
three can actually slow things down by
about seven percent in other words the
shifting of the code as a result of
collecting hardware metrics
significantly and actually chaotically
changes the performance of this
application yes please
you don't have to modify the code to do
any incrementation of the counters but
you still need to initialize the
counters and read out their values at
some point okay so this is data for just
one program and now i'm showing you data
for the entire spec int 2006 benchmark
suite so these numbers identify programs
now the interesting thing to note here
is that for some programs add encounters
can degrade performance improve
performance and for others it actually
degrades and these two programs are the
most extreme so this one sees a change
in nearly twenty percent as a result of
collecting data but if you eliminate
these two programs there's still quite a
lot of variation along the order of
three or four percent for these other
programs in other words the
insignificant overhead of collecting
hardware performance monitors can also
change data this is not intuitive this
is not what you would expect now in this
graph i'm showing you not hardware
metrics but software metrics now
software metrics as you would expect are
more work and so to collect software
metrics we need to go and modify the
program to increment counters so for
each of these programs we identified up
to a hundred different events that could
be interesting for performance analysis
and for each event whenever it happens
we increment a counter for that event so
for example if this is a database
application then at the end you might
know how many times each transaction
happened so there's one of these violins
for each benchmark program and the y
axis gives you the execution time
then you collect no software metrics
divided by the execution time then you
do collect software metrics now this is
a lot of data so if we summarize the
data using these violins the while in
plot is a very powerful plot it can
capture quite a few things in a very
compact picture so let me explain it to
you if you look at a while and sideways
it basically gives you a histogram so it
gives you a distribution of values so
for example this fat area here indicates
that we see about a five percent change
in performance for a majority of the
time and then there's this tale that is
extends on either end the white point in
the middle gives you the median that we
observed each while in represents the
data for collecting one software metric
to software metric three software metric
all the way up to a hundred software
metrics the interesting thing to observe
is that these violins are incredibly
tall so for example this while and
extends from about 12 point 82 in other
words we are seeing an 18-percent swing
and performance as a result of
collecting software metrics now you
might wonder if you have added a lot of
instrumentation to collect the software
metrics than we expect a big change in
performance well it actually turns out
that that's not the case here the
numbers above the x-axis tell you how
many instructions we added dynamically
for collecting our data so the most
extreme case is about 2% in other words
all the increments that we added to
collect software metrics change the
instruction count dynamic instruction
count by up to
two percent and oftentimes by much less
the thing to observe here is that the
big swing and execution time we see is
disproportionate to the instruction
count that we have added so in other
words a very very small change in the
instruction count causes a big change in
performance so we cannot expect that if
our instruction overhead is small our
observer effect is small it may actually
be huge now where does this observer
effect come from what happens is that
venue dad instrumentation it pushes code
or data through some boundary and
unfortunately these boundaries are
everywhere in a computer system for
example you might push it from one cache
block to a different cache block you
might exceed the cache size you might go
over a page boundary you might go over
TLB and so on so forth so these
boundaries are everywhere in the
computer system and by moving things
around who knows which boundary you're
going to push things over unfortunately
this is not the worst of the problem the
worst of the problem is yet to come so
let's continue with our commute example
and now that we have built our bridge we
want to ask is this bridge a good idea
was it effective in doing what we
intended it to do and so you might
device a study where you ask your
co-workers so you and your coworker
might say I can get to my office in half
the time and from that you might draw
broad conclusions like hey the bridge
cuts my commute in half now if you
believe that study then perhaps I can
sell you yet another bridge the pro
that study is that it's easy to do I
just pulled a bunch of my co-workers but
it's not right it's wrong and the reason
why it's wrong is that it's potentially
biased your co-workers may not represent
the entire population of commuters and
in the context of a computer system your
experimental setup on which you do your
experiments may not represent the whole
population of experimental setups and so
if you do your experiments in one or a
few setups you might end up heavily
biasing your results you might end up
with the results that are biased towards
your setup and this particular phenomena
is called measurement bias now does this
actually arise in practice for this set
of experiments what we are doing is that
we are comparing two things much like we
were talking about comparing a commute
without the bridge and a commute with
the bridge and the two things we picked
just for interest is the performance of
the code produced by GCC so2
optimization level versus GCC's 03
optimization level so the y axis gives
you this ratio and the way this ratio is
if you see numbers above 1 it means 03
is beneficial / 02 and if you see
numbers below 1 you see that oh 3
performs worse than 0 2 as before we
have one violin for each benchmark
program the first most interest and what
I'm doing in this case to change my
experimental setup is that I'm changing
environment variables so these are unix
environment variables and what we did is
we created a dummy variable and that
variable has nothing to do with your
program
and all we are doing is that changing
its size now what that does is that the
environment variables show up under the
Oro Verde depending on how you look at
it over the stack the execution runtime
stack of the program and so changing the
environment variables shifts the stack
around and doing this completely
innocuous thing so we change it from
zero to 4096 bytes we see we see huge
swings in performance and worse 44 of
the program's their violins straddle 1.0
in other words depending on the
environment variable you happen to pick
you might conclude that oh 3 is
beneficial or you might conclude that oh
3 degrades performance you come to
completely contradictory conclusions now
this has significant consequence so for
example let's suppose I set up an
experiment on my machine and I run it
and then I ask a colleague to run the
same experiment on my machine if that
person has slightly different
environment they may get very different
results and indeed we tried that and
that's what happens ok so the setting of
irrelevant environment variables leads
to contradictory conclusions the ends up
biasing the result one way or the other
here's another instantiation of that the
grip the graph follows the same format
and what I've done in this graph is I've
changed the link order so the order of
the dot 0 files to the UNIX LD linker so
I've just reordered them there's nothing
else that I have done once again as you
can imagine changing the link order
changes code layout and of course you
end up with interactions with the memory
and other subsystems of the computer and
we see 432 randomly generated link
orders each while in summarizes the data
we got
or one benchmark program we see that
these violins are huge by changing the
link order we can change the performance
of these applications by close to twenty
percent okay and many of these violins
straddle 1.0 in other words not only do
you come to a different conclusion about
the magnitude of the speed up but you
might come to completely contradictory
conclusions you might think that it's
beneficial but it's not depending on the
link order that you picked now I don't
show you that data here but you might
ask what if we pick the link order that
the makefile has for these programs and
we actually tried that the default link
order which is what specified in the
spec make files or the alphabetical link
order neither of them are the best link
order in other words you can speed up
your program potentially by after twenty
percent by just moving the link order
around and seeing what happens now
whether this measurement bias come from
so as I mentioned the environment
variable the link order they affect code
and data layout and of course these are
not the only sources of measurement bias
there are many other sources of
measurement bias so for example some of
them are domain-independent so to give
you an example the temperature in the
room may affect whether your machine
favors you know compute bound
applications versus memory bound
applications but unfortunately along
with the domain independent sources of
bias there are also domain dependent
sources of bias so for example the heap
size for a garbage collected system
people now know that depending on the
heap size you pick you may come to
completely contradictory conclusions
about which garbage collector is the
better garbage collector so there are so
many sources of measurement bias domain
dependent and independent that is not
something we can easily suppress
now the next thing to ask is is
measurement bias predictable if it's
predictable then maybe we can get around
it somehow so what this does is that it
takes the violin for one of the
benchmarks pearl bench and it gives you
all the points that lie within that
violin and the data for the other
benchmarks are similar so i omit that
and the x axis gives you the environment
sighs so basically the size of this
dummy environment variable and the y
axis gives you the execution time of 02
/ 03 so the ratio that we have been
looking at all along the thing to note
here is that adding an environment
variable can make all three look better
such as here it looks seven percent
better than 02 or it can make it look
much worse and here it looks about 9%
worse than 02 so adding environment
variables does not have an easily
discernible predictable behavior we
cannot predict this behavior and act on
it now what about different
architectures so if for example we
picked one link order and one
architecture and we pick the best link
order on that architecture by trying all
the possible link orders would it give
us the best or link order on another
architecture so here along the x-axis I
give you the execution time for a
pentium 4 workstation and along the
y-axis i give you the execution time for
a core to work station and we produce
these binaries so that we could run them
unchanged on both the machine so it's
the exact same binary on both the
machines and each point represents one
link order the point here that I have
circled because it's the lowest point
along the y axis gives us the best link
order for the core 2 machine the point
here because it's the leftmost point
along the x-axis
gives us the best link order for the
pentium 4 machine in other words the
best link order for one machine may not
be the best link order for another
machine so you cannot just pick one link
order which performs the best in your
environment and expect the results to
carry over to other environments now we
have so far used GCC for all of our
experiments and you might be wondering
if this behavior is an artifact of GCC
in particular since we are using intel
machines if you use an Intel compiler
maybe the Intel compiler designers are
smarter and maybe they'll know how to do
things right for that machine and so in
these graphs I present data for the link
orders for the Intel compiler not GCC
and once again we see we see huge sings
in performance in other words this
phenomena is not an artifact of
something that GCC does intel's c
compiler suffers from exactly the same
phenomenon now are these phenomena is
caused by a poor methodology that we
might have followed actually we followed
the best methodology we could find or
come up with we use the spec cpu 2006
see benchmarks and we use train inputs
because they're already quite long and
we needed to run tens of thousands of
runs we used the best practices so we
tweaked our instrumentation so it was as
lightweight as we could come up with it
we ran our machine experiments on
lightly loaded machines used only local
disk there was no network activity etc
we did 15 month for each experiment so
even though I haven't shown it on the
graphs earlier actually I have shown it
but you might have missed it there are
we computed confidence intervals for
every run so we have 15 runs for each
experiment and the confidence intervals
are very very tight and the reason why
you haven't noticed is that the error
bars are so tight that they're almost
invisible and we reproduce these
experiments on four different
architectures and two compilers so we
conclude that we actually followed as
good a practice that one could follow
now in in the past when I have talked
about this phenomena I've talked about
you know you move code or data and it
hits some barrier and it interacts
poorly with it but I haven't told you
what barrier it hits and it actually
turns out that to figure that out is
incredibly difficult we have figured a
few of those things out but in general
it is incredibly difficult in particular
to figure out what exactly is happening
in the underlying machine you need two
things first of all you need to have a
deep knowledge of what that computer
hardware actually does and you need to
be able to collect data from the
computer hardware that allows you to
test your hypothesis unfortunately
Hardware designers do not reveal many
details of their hardware and there are
many things you would like to collect
that they don't allow you to collect so
to give you a brief example for some of
the phenomena that I've showed you one
of the culprits might be the loop stream
detector which detects loops and latches
them into the instruction queue but
unfortunately because Intel does not
reveal all details about the algorithm
that the LSD uses we can't confirm that
that's actually what's happening also
Intel does not make available metrics
that measure what the LSD is doing in an
execution so we can't see what it's
actually doing actually leave it at that
so we can guess what is happening but it
will be very very hard to pinpoint the
exact underlying cause
now how do we as computer scientists
deal with these issues so in order to
get a sense for that we surveyed papers
from top systems conferences including
asp los packed pldi and cgo and we
looked at all the papers in 2007 for
these three conferences and for asp loss
we looked at 2008 there was no asp loss
in 2007 so we couldn't do that and off
the 133 papers we looked at 88 papers
had an evaluation section so potentially
those are papers that might suffer from
the phenomena that I'm describing and so
then we asked a question for those 88
papers what do they do with observer
effect and with measurement bias now 36
of those papers used simulations the
great thing about simulations is that
you can avoid the observer effect
because of the simulator you can add
things so that you don't end up causing
an observer effect unfortunately you
might still suffer from measurement bias
so what we did in this graph is that we
took the m5 simulator which is one of
the most prominent public domain
architectural simulators out there from
Michigan and we use its accurate oh 3
CPU model and we tried many different
link orders so we did the same link
order experiment that I described
earlier and the x axis gives you the
normalized speed up or the speed up that
we got of 03 against 02 and the y axis
tells you how many times you observed a
particular speed ups and in some sense
this is a histogram that gives you a
distribution of the results we see that
even in the simulator if you take the
rightmost point and the leftmost point
we saw
almost a 20-percent swing in performance
due to changing link orders in other
words simulators may be immune to
observer effect but they are not immune
to measurement bias it's something you
still have to deal with now of the 88
papers that had an evaluation section 83
papers use more than one workload now
the idea there is that if you have a
diverse workload if you have many
different things in your workload
different things in your workload and
you compute averages it may be that for
one benchmark you may be biased in this
way for another benchmark you might be
biased in that way and you hope that if
you have a large enough workload set
then the biases will cancel each other
out so you report average numbers so
what we did here is in this graph i'm
showing you numbers that are averaged
over all the spec into 2006 programs
plus the 3 C programs in spec FP 2006 so
it's 12 benchmarks and the x axis gives
you the speed up but this is the speed
up that is an averaged over the entire
benchmark suite if it was the case that
the benchmark suite was large enough to
cancel out bias then we would see a very
tight distribution instead we see a
broad distribution that spans about
seven percent in other words it could be
that if you use enough programs it will
cancel out bias but at least the 12
standard benchmarks that we used were
not enough to cancel out bias now now
that we have looked at what CS systems
researchers do we should look at what
other sciences do
these phenomena of course are not
specific to computer science other
Sciences have dealt with them for for
hundreds of years there's basically two
techniques that other sciences follow
the first technique is that they use
many different measurement setups and
then they use statistical techniques
such as the t-test to derive conclusions
from the many different measurement
setups and the hope again is that by
using a diverse enough set of
measurement setups your bias will cancel
itself out the second thing that they do
is causality analysis the main idea
behind causality analysis is that you're
not trying to get correct data as a
matter of fact you recognize that
correct data is impossible to get in
general but what you are trying to do is
to determine if the conclusions you have
drawn from the data are correct even if
the data itself may be tainted so I'll
demonstrate how we have been using these
techniques by giving you two examples so
for the many measurement setups we do
set up randomization so rather than
collecting data for example using just
one environment variable setting in one
link order we collect data with many
many many different experimental setups
and then we draw the distributions so in
these vile ends these points here the
circles they represent what you get if
you collect data in only one measurement
setup so this is what we saw in the past
and the x-axis adds hardware metrics so
this is going back to our adding one
hardware metric at a time example and
these violins show the distribution that
we get and the interesting thing is that
if he compared these distributions
they're nearly
identical even though the individual
points are quite far apart from each
other so here these two points are like
twelve or thirteen percent apart and so
by looking at these distributions we can
conclude that it's unlikely that the
hardware data collection is actually
affecting performance it was just a
outcome of the observer effect or
measurement bias so we compare these
distributions using T tests now this
particular technique is popular in the
social sciences right so for example
when when they they go out to PO who's
going to be the next presidential who's
going to the next president they don't
go to just one small town and ask them
because that's going to be biased
instead they go to lots of different
diverse communities and try to extract
information from there and that's
exactly what we're trying to do here but
of course the difficulty with this
approach is it's not perfect and as a
matter of fact there's numerous examples
in social sciences and in Medical
Sciences where they encountered exactly
the same problem and the problem is that
the effectiveness depends on how
representative your setups are if
despite the fact that you try many
setups if your setups are not
representative then you cannot draw any
broad conclusions causality analysis
works as follows the first thing that
you do is you analyze the data to arrive
at some hypothesis now remember this
data might be tainted it might be
affected by the observer effect so for
example you might come to this causal
chain so this causal chain says that
when you change the size of the
environment it causes a change in the
address of the stack which in turn
changes performance so let's suppose
this is our hypothesis so the next step
of causal analysis is to perform an
intervention in other words you find a
in your causal chain and you break that
link so in this case we broke this link
and the way we broke this link is by
adding a directive that made sure that
the stack always started at the same
address no matter what size of
environment variables be used and so if
with this intervention we expect that
now changing the environment variables
should have no effect on performance and
that's what we have to check now it
actually turns out that for most
programs this hypothesis was the correct
hypothesis for one program this
hypothesis was not the correct
hypothesis so this rejected the
hypothesis and then we had to come up
with another hypothesis which we were
then able to validate and of course I'm
happy to talk about that if you have if
you're interested in that so you can ask
if you like alright alright so this
approach is popular in the sciences but
it's a very very difficult approach to
do because to come up with a link that
you can break is entirely non-trivial
it's not so easy to break this link
without also affecting lots of other
things now I'm going to conclude with a
call to action things that we should try
to do as a community the first thing is
that I think in a lot of computer
systems research we have been giving a
strong preference to new algorithms over
insightful evaluations and we need to
recognize that to actually do an
insightful evaluation that really
explains some phenomena correctly is
just as hard as coming up with yet
another new algorithm and so we need to
recognize that by a cultural change we
should reward careful experimentation
the second is we need better
were closed well okay I need better work
loads at Google you probably don't need
better work loads but one of the issues
that I demonstrated is that at least the
benchmarks that be used they were not
diverse enough to cancel out measurement
bias so we need workloads that are more
diverse we need more information on
micro processors and son not too long
ago open sourced one of the spark
designs and that's great because now if
I have a question on how its loop stream
detector or whatever it has works I can
look it up it might be painful but at
least I have a way of looking it up but
with Intel when it does not release
details there is nothing that I can do
at least in academia and finally we need
much better hardware support we need to
have data for all the key components so
for example in the core two there's a
new feature called the loop stream
detector that is heavily advertised but
there is no detail that Intel reveals on
exactly how this thing works and we
cannot actually there are no hardware
metrics that are available to me with
which I can measure that particular
component also it would be nice to have
a mapping from hardware events to
software events and this is also
starting to appear in microprocessor so
for example the precise event based
sampling allows you to do some of that
but it only does it for a handful of
metrics and be nice if we could do it
for everything so what are the key
lessons from this talk first of all just
because you have a small amount of
instrumentation does not mean you have a
small observer effect I hope I've
convinced you that changing things by an
insignificant amount can have a huge
observer effect second observer effect
and measurement bias they are
unpredictable we can't tell if adding
one more environment variable will make
it better or worse their common place we
have seen this in all the programs you
have tried and on forma on for
architectures one of which was a
simulator
one and it's large enough to office Kate
data I mean a lot of in our literature
survey we found that the average speed
up that papers reported was around nine
percent given that we are seeing twenty
percent swings due to changing link
orders how do you trust your nine
percent speed up now finally other
experimental sciences they have expended
a lot of effort to work around this
phenomenon and we have ignored most of
those techniques we've had it easy but I
think we haven't had it right we have
been collecting data that doesn't make
sense necessarily now I will conclude
with the enumeration of some of the
other things that I work on in case
anybody has interest in talking to me
about that so I have a number of
projects that have to do with
optimizations there's blind
optimizations where you're trying to
optimize things without actually
understanding what the microprocessor
does for exploiting hardware features
there's a project on algorithmic
optimizations where we use both exactly
two systems one a compile-time system
and one a runtime system for finding
opportunities to improve performance at
an algorithmic level we are also doing a
lot of work on using techniques from
nonlinear dynamics so computer systems
are nonlinear system then they're
actually turns out to be their chaotic
systems and so we're using techniques
from that domain for validating
simulators and for understanding the
performance of computer systems and i
also have interest in software
engineering where you're working on an
improvement to refactorings call program
metamorphosis and i'll stop here and
take any questions Neil
so for the measurement bias unlike the
other sciences we have the benefit that
we can actually you know fix the problem
in the sense that you gave a link water
example that seems to be another avenue
for optimization same with stack
alignment and so on and so forth so when
you say that we should try many
different experimental setups part of
that involves knowing the problems that
you want to absolute produce and it
seems like if you know that's half the
battle that if we knew the problems that
be some project somewhere working on
factoring that out so we always pick the
best one for that particular axis of
change like so if do you have any
suggestions for how we actually identify
these things that are causing the you
know the bias that's a good question and
basically the answer to that is that I
don't but I don't feel bad about it
because the sciences that have been
doing it for hundreds of years don't
either this is a very very difficult
problem so to give you an example I read
a medical study which what it did was it
looked at 49 papers that were in the top
medical journals and they were each
heavily cited so they each had at least
a thousand citations and they found that
later studies that used more diverse
populations and larger populations found
that a third of those studies were sorry
thirteen percent of the studies were
actually incorrect and then there was
another 13 or so percent that were found
to have made exaggerated claims and the
reason for those things was that the
initial studies had not used had ignored
a particular confounding variable and
that confounding variable is what we
have to find basically so I think I
don't have a quick answer to this and
the answer is that if we as a community
start to value these kinds of things
then as a community we will discover
over time many of these things that we
can either use to do better evaluation
or as you say better optimization I
really viewed most of your slides rather
than be
disappointing is you know jotting down
new projects that somebody ought to be
working on great as if there's a
20-percent swing that's great that is
awesome right that's awesome so
regarding that I have a couple comments
so the link order you can use the l
other two other the object files in
specific orders using the topological
sort sorry I'm not hearing you two
there's a utility called el order uh-huh
mutual you know all of your you know a
list of object files according to the
topological sort of the symbol
dependencies and then there are other
compilers and tools that will reorder at
the function boundaries or even the
basic block boundaries so those will
control that variable which will reduce
the variance because it's been common
that whenever you have uncontrolled
aspect like for example the alignment or
things like that once you start
controlling it the variance tends to get
reduced so i think you're being very
optimistic and the reason for that is an
optimism is good but but i think the
reason for this is that these computer
systems are just mind numbing lee
complex okay so you're doing your link
or your reordering stuff you say okay
I'm gonna make sure every loop starts on
a 16-byte alignment okay but maybe it's
not the particular loop it's something
inside the loop so the issue is that
there are so many different hardware
structures and they owe you you have to
kind of play well with every one of them
in order to squeeze the best performance
out of it that I think any kind of
static technique it's going to be
unrealistic I think know what I'm saying
is I'm not gonna I'm not saying you will
eliminate all the variants right but as
you control more and more you will have
reduced the very the range of the
variance because that is typically when
you have totally uncontrolled stuff you
will see random scatter all over the
place and once you start controlling it
you will see in reuse range you can't
write
asian what's a wet bread which is the
whole point of doing the experiment is
to actually discover something right
right about discover something you want
to control it that's what what I'd say
as you're probably more problem you
should control then without just letting
it you know girl you couldn't control
him but as I showed right the link order
that works well on one microprocessor
does not work well on another
microprocessor right so you do your
experiments on your workstation
controlling everything right and then
you tell your friends or write in your
paper that I controlled you know I fix
this link order i use this basic block
ordering i use this environment
variables but they're using a slightly
different microprocessor they're not
going to be able to reproduce your
results right but as a practitioner what
we care about this we know exact the
platform we are going to deploy exact
processor we can control just stuff so
we want to find the all the best
combination right that we can get the
past performance at all right so we want
to find out those all those control
control over stuff and we want to
control it right by you know careful
parties five bottles yeah only modules
even the optical encoder you still
random because it the real issues is
much no finer grain right so even if you
control that level it stills there's
random stuff so you want to control more
and more see the thing is as Neil said
it allows me if you know exactly what
environment you're gonna run on and you
have plenty of patience then there are
searches that you can do some kind of
health climbing to try to find a good
link order right but if anything changes
in your environment so for example your
dynamically linking to a library and
somebody installs a slightly newer
version of the library that's two bytes
bigger to bite smaller all bets are off
so it's the obits are not off so soon
okay sure
and drive away my client is the address
chips and the preference proclamations
my favorite in tariqa Stan you just blew
it away so I'm gonna have to file
another custom Oh are there other
questions so what happened if you
combine two variables so link of the end
invalid variables then they have a
larger effect then individual but it's
not predictable or additive but so we
see up to twenty percent yeah so the
stays up to twenty percent no it goes
higher than that but it's not additive
so their example and the thighs no
additive but how much higher we've seen
up to thirty thirty-five percent it goes
it goes very high because if there is
one valuable we can easily change we can
try to optimize it for one is for 41 the
amsa collection and hopeful the best I
mean we need to calibrate every time we
change a bit but it may be war file
you're absolutely right other questions
yeah I had one question for you a mirror
um I think if you look over the last 20
years one could argue that computer
systems are getting more susceptible to
this effect I mean you know 25 years ago
we were running on 78 vax 11-7 80s they
barely had a cache instructions retired
in a single cycle so I think
repeatability was maybe less of a
problem and so over that time frame
repeatability or these kinds of issues
perhaps have become worse I'm wondering
if you accept that assertion and at and
if you do then whether you're in the in
the literature your surveyed have you
seen any improvement in methodology or
are we all just irresponsible all right
so so let let me answer both parts I
think your intuition is probably okay
that things have become more complex now
and so these problems are more common
but the math doesn't necessarily bear
that out so in particular as I mentioned
we have been using nonlinear dynamics
from chaos theory and we are finding
that even when we simulate really old
outdated machines even they exhibit
chaotic behavior so now it's a man it's
kind of like a spitting contest were you
more chaotic than I am right or is the
current machine so in some sense even
they had the potential to have these
wild and crazy things but intuitively
you're probably right that maybe it was
a less frequent event in terms of
methodological improvements I mean when
you and I were in graduate school right
there was a lot of papers out there that
were just appalling for example they
would simulate the number of cache
misses and it's all each cache miss
costs six cycles and we multiply it out
and we get something useful out of it
and I think around the time that we were
doing this stuff we had already started
to realize that that was simply not
going to work because each cache miss
has a very very different cost and each
tlv mess has a very different cost so I
think that things have improved in that
people are not making linear assumptions
so people at least acknowledging that
computer systems are not linear but I
think that there are some of these
things which we still known take care of
and obviously you can't really fault us
we are young science compared to the
other sciences that actually do know how
to deal with this
to say that hardware manufacturers
should as it may not just have to make
things as fast as possible narrogin
radius to get yeah that's an excellent
point I was talking to you see people
hmm all right so the question is that if
this variation is such a problem
shouldn't hardware manufacturers try to
build machines that are more predictable
or yeah so you're absolutely right and I
was talking to this great engineered
Intel just last week and what he
confirms the phenomena that we were
seeing and what he told us was that what
they're trying to do is you know what I
the methodology I described where you
run many setups they're doing stinks
similar to that but they're not doing it
to get measurements they're doing this
on future generation hardware so they
have a simulator for a future generation
hardware and they try all of these
things and if they see wild and crazy
stuff then they try to say hey let's
identify the particular feature and
let's tweak it so that it doesn't give
that behavior even historically you see
phenomena like that that we have seen
right so for example the Pentium 4 has
the infamous trace cache which I think
almost anybody who's used that machine
for performance analysis realizes that
that it makes a mountain out of a
molehill I mean little change you end up
with trashing the trace cache or not and
as we see that the trace cache has
largely disappeared from Intel's own
micro processors and you know at one
point when the Pentium 4 was out i was
asking intel engineer so how in the
world do you do any performance
evaluation on a pentium 4 and this as we
don't use a pentium 4 so so i think that
this is one of those things that has
been biting them also and they're trying
to deal with it but i think the bottom
line is that i think it's a bit naive to
expect that we can eliminate these
things we might be able to reduce the
likelihood but we will not eliminate it
because these computer systems they are
nonlinear they are chaotic and they are
going to exhibit these nonlinearities if
you tickle them in just the right way or
just the wrong way
question okay so uh I worked on ia64 in
the past and I 64 has already there's a
lot of background noise yes sorry so
I've worked on ia64 in the past and a I
64 has a very complex performance
counters 325 events on the McKinley
chips yeah yeah so the problem there was
that people don't or it's very hard to
find out what those events actually
measure yeah so even if you do get the
measurements you still don't understand
exactly what they do and what they mean
exactly so even if Intel were to give us
better instrumentation into all the
components that you have I'm wondering
how accessible those still would be that
that is an outstanding question and I
think that to do look at that one should
look at the the power for for example
you know IBM does a much better job of
sort of documenting their hardware
performance monitors and explaining them
and you're absolutely right i mean just
giving you I mean if you've ever looked
at those performance counters and you
look at their names they mean nothing
right and and so you're absolutely right
they not only do they have to give you
the counters but they have to tell you
what they actually mean you're
absolutely right let's think Amir</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>