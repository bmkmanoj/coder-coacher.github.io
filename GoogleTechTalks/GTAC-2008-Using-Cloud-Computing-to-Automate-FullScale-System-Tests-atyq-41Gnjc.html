<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2008: Using Cloud Computing to Automate Full-Scale System Tests | Coder Coacher - Coaching Coders</title><meta content="GTAC 2008: Using Cloud Computing to Automate Full-Scale System Tests - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2008: Using Cloud Computing to Automate Full-Scale System Tests</b></h2><h5 class="post__date">2008-10-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/atyq-41Gnjc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's buy the next 15 minutes
about sharing our experience in building
application helping users to automate
their release process the presentation
goes as follows I'll I'm probably
preaching to convert it here but just to
make sure on the same page or not I will
see I'll spend a bit of time talking
about the problem domain as the problem
we're trying to solve then we'll go
through some lessons learn from past
experiences especially in using grid
computing to do some of the automation
we're doing and then I'll hit the proud
the core of the talk which is how we're
using cloud computing and the solutions
and issues that we found with this sent
out I'll conclude so what's the problem
well the problem is is that nowadays and
still software or releasing software is
is risky stressful and expensive and
it's risky because unfortunately we
would get it wrong often a lot of
software gets either projects get
cancelled or they they don't make it in
time or they exceed the exit costs or
don't meet the expectation and it's
stressful and then this is a I mean for
my personal experience if you walk
around the software team about really
software you can probably smell and
taste the the stress the level of
anxiety sometimes and that's a moment
where you probably want people to be
very relaxed and make sure they don't
introduce last-minute problems into the
air the release process and it's
expensive because of all of this effort
required and when you get it wrong then
it costs not only in money but also in
time in terms of of human energy and and
that's wrong I don't think it is
required to be to be this way so what
can we do about that so let's looking at
the end some of the the root cause as
we're writing software that are our
environment our target environment
or user environment is often
heterogeneous so we'll have to tackle
targeting different operating systems or
different flavors of the same operating
system depending if our if our
application is end to end or it's taking
part in a more complex orchestration we
need to understand how our software's is
going to be used and make sure we
understand that we're probably not
writing software in a vacuum so we might
have software that's been released
already in production that needs to be
in lincoln tank we need to understand
how that that evolves we have new stuff
that is about to be released or or we
have new ideas where we're working on
and all these pieces are in motion and
we need to understand and follow all of
those in order we're talking about
confidence a little earlier and it's
very important that we understand and we
have the right confidence in all of
those moving pieces the best way to do
that is to test right that's the best
way to to to gain confidence but as we
have all of those pieces flying around
we are our needs for test bed for for an
environment in which to test starts to
explode and then we hit limitations we
have limitations in terms of sis admins
so that are able to configure and
reconfigure your test environment and
and sometimes as well resources how many
machines do we have to actually keep
track with with what we're trying to do
and then we when you start scaling up
and hit the distributed systems or NT
architectures then then it gets it gets
even worse and it's at that level as
this explosion starts that the cloud is
actually going to be useful another way
of looking at this problem is is the
would recover the procurement hassle we
need to be able to find a problem and
investigate the problem quickly and have
quick turnaround ok so this here's a
little you know example hey boss we have
a problem and it could be anything like
maybe there's a race condition that on a
certain case so
census will will blow up or the
performance going to nosedive if these
strange things happen together and often
the solution is while we need machines
okay so if you're lucky the machine room
has a few idle machines that can be
tapped at and configured and you're good
to go my experience is that it's rarely
the case sorry if you're lucky there's a
few boxes in the corner of the machine
room they need to be unwrapped and plug
into the racks configured powered up
you're good to go it's more likely that
you're going to have to go through a
procurement procurement loop which means
days weeks and months if you're not if
you're lucky which means but by the time
that the machines are the tools are in
place either the problem is in
production oh you spend a lot of time
doing Gedanken experiment like to figure
out is it really a problem or not all of
this is a distraction from adding value
to 2 to the system being creative that's
a risk and on the other side because
it's takes so long to get get told the
machines even if you don't need them
anymore you hug them you never know we
might actually need them at the later
date and because if everybody starts to
do that then we end up with a slow
frustrating on collaborative animal and
wasteful way of doing things and that
that is wrong so to tackle some of that
a lot of people of starting to the
automation and then I think we're good
testimony of this and this is probably
the order in which people tend to start
their automation walk in in life so they
start with a build so the automator
build there's no reason for not doing
this anymore there's loads of good tools
to do that about their ultimate their
unit tests we're talking about capturing
a unit test I'm talking about the
execution of the of the process now you
package your your software you install
it automatically you make sure that you
can actually not only package it but the
package
install and as you go down you you want
to start deploying those packages
configure then put them into a system
state in case of an n-tier architecture
a distributed system then n-tier
machines and then once that is set up
and working as a coherent system and you
want to test that and these as you go
down that route it becomes harder there
are more pieces in motion and it's those
two or three last steps at that we at
six squared are are interested in so as
it is it's easy to do these things at
the class level because your system at a
class level is a very small system as it
gets bigger and more pieces it becomes
much more difficult a different way of
looking at this is the team dynamics
when we talk about release ability well
there's there's a in order to be at the
end here where we're good to go there's
a lot of people have contributed to this
you have your developers it tested
integrators or certification people in
QA and you probably have those actors
whether you have specialized teams or
you have multidisciplinary teams you're
going to have those types these types of
knowledge exists I think necessarily in
order to capture the different steps
that will form your pipeline that will
end up with something that you can put
into production
so this release dynamics it comes back
to two it's a complicated affair because
there's a this knowledge of her
disability is spread across a large
number of stakeholders and as all of
those pieces are in motion we need to be
able to capture this nostalogie release
ability as time goes on however if this
knowledge is spread across it often is a
small team or even an individual who
actually will you know press the red
button says go go into production okay
so we want to be able to kind of
rationalize this this this a little bit
so the ultimate goal is is is a full
system test automation okay with the the
end result of being able to release with
confidence and this this confidence i
think is a very powerful word that that
captures well what we're trying to do
and here's a quote from Peter Cabarrus
had the chance to work with Peter at the
open grid forum for a couple of years
and here's a slide he presented and I
thought it was quite appropriate to this
this talk so automate automate automate
humans are slow and expensive not to
mention unreliable and lazy that's what
makes us so much more fun than machines
but it cripples testing okay I think I
thought it was interesting so we have to
be careful so looking at some solutions
from from this this very quick analysis
what we want to do is be able to capture
people's worries now all these different
actors that are contributing to this
pipeline we want to be able to capture
this in some form and then automate that
that pipelines from a single button or a
single trigger or whatever able to say
are we able to go to the end of that
that arrow and are we able to release
are we are we able to to go to
production or if anything has been
injected in this pipeline in the
meantime can we actually know it does it
break that and if it does why and let's
let's fix that so the solution to this
is is both its its tooling but it's also
cultural as soon as you talk about
changing the way people what process is
about changing the way people do things
and there's a cultural effect that we
aren't we need to understand and work
with but as we go down the the evolution
you know the arrow is over showing going
down the tool support actually decreases
and this is this is where we we we try
to contribute so I've described a little
bit the the probe line that the problem
that I'm we're trying to address now
we'll start to look at how we do that so
this is us 6 squared we're a limited
company based in Geneva Switzerland I
took that picture from my office just
before we left that's not true it was
raining so we're trying to to focus on
that part where so few people are able
to automate the the installation
deployment and system test part that's
what are the focus of our application
development is is that at the moment and
this is what I'm going to I'm going to
go with before doing that for a couple
of years before creating six squared I
worked at CERN on the ethics project
where we on it was funded by the
European Commission it's the equivalent
of the NSF she went in in the states
where we we showed possible the vision
that delivering a service I mean a web
service and a set of tools is actually a
viable solution to automate that that
that process okay so it was done so
delivering a service software as a
service with a back end which would
manage resources for the users so the
users don't have to manage their own
resource we offer the resources as a
service and it was using a grid grid
middleware and the best beauty where for
this this project we found was Condor
developed by the University of
wisconsin-madison epitope virus is far
post part of that project with a layer
on top college
so you would configure your on your
build process you press the button and
then a grid engine will actually deploy
jobs and your your build process and
test procedures would fire up in the
background and you be told given a
report when it is finished the main user
of this this project this build and test
project was the eg project or the unit
in grid for a science probably the
biggest grid deployment at least public
academic grid the deployment in the
world there's a lot of numbers here but
you know 250 sites over 50 countries
this is an old slide with you know
10,000 user scientists grouped in 150
different scientific collaboration
working together sharing results ideas
data and resources so a reasonably big
deployment it's used by by several
different scientific communities and to
power this grid the they used a software
layer called G light so G light is about
300 components group in 20 20 different
subsystems two million lines of code
written in many language and the the
build process and in another interesting
aspect is there was like several dozens
of developers all over the world
contributing to this middleware effort
and the build process was that was
captured in in ethics this project I was
I was mentioning where the dependencies
between different components and a
grouping were capturing a tool and for
every component the to latex to would
call out to make auto tools or end files
to actually build the the specific
component and generate the packages ok
so let's look at some of the lessons
learned from using this this grid
middleware powered ethics tool applied
to this project as we rolled in this
this new process we thought out
simplicity matters ok if it's not simple
it's not fun ok it will be a diss track
from what people are doing every day and
that so that people will just resist
okay and speed matters as well in a
sense that everybody wants to I think
recognize they contribute to a global
effort to streamline the overall process
but if it's too detrimental to their own
work if it slows them down too much
again they'll start to resist that
change so it's important that they
understand the global contribution but
it needs to pay off for them as well so
the return on investment from a personal
level is important so that you know me
included I have a benefit from using a
new process so that these were important
lessons another project that I want to
talk about briefly is the diligent
project used ethics service as well to
run to automate but focus more on
testing so diligent is a digital library
built on top of the EGD grid and it
basically create it uses the grid to
build indexes so it basically just goes
through a large set of rich documents
and Dillon Texas so that you can have
quick queries by our web form online and
it was using as I said the the dige grid
behind to actually crunch to this data
so it's 40 different services sitting on
five core services and they use ethics
for to automate its its deployment test
so as every time they wanted to test a
single service now we're in system test
mode they needed to put the five core
services in a certain state or at a
certain version level okay because all
of those pieces are are in motion and
are being given up so they had to kind
of control and ask some sysadmin is
responsible for the testbed to put the
test bed in a certain state so that they
could test their service and as this
thing started to to be tested in anger
and then overloaded very quickly the the
people dealing with the five core
services and that became unmanageable so
we propose to them to use the this this
service to automate part of the the
deployment of those core services so
that on demand they could start to
configure automatically the test beds
that they could perform the assistant
test on that so what are the lessons
learned from from doing this by the way
I've put some references at the end so
if you want to have more information
about those those projects you'll be
able to go to find that information and
something that that is only natural and
comes across often when you start to
talk about automation is that yeah but
our software is not designed for
automation you know it's just going to
take too long it's too expensive we
found out that that's was the case in
this case it was not thought the
software was not thought to be automated
but we started to see positive return on
investment very quickly by automating a
subset of that department so you don't
have to be able to deploy the entire
thing you know from the word go to see
positive return on investment and that's
interesting because it gives us a lower
threshold a cheaper way to start
automating and CNC see positive feedback
we're also seeing that as you start to
automatically deploy more than one
machine when things go wrong then
troubleshooting becomes tricky if you're
not able to directly access the machine
okay so if you only in the case of a
grid environment where you only see your
your report coming back the thing failed
and here are your logs it'll go through
your logs do I figure out what's going
on and it's trial and error and that can
be time-consuming this was not possible
because it's not a criticism on the
Condor middleware it's the way we used
it I'm sure there must be a different
way to do that but being able to
actually get at the machine would have
been a time a time saver so trying to
analyze this at what i call the
sociology of automation I think industry
and academia alike I think consensus is
much better than you know your VP
dropping a piano on your head from the
fifth floor at least that's the way it
feels when he headlines on your head you
know this is the way we're going to do
things and
oh ok I think reaching consensus is a
much more productive way to do things
but we have to be careful about it okay
so we have to be as little intrusive as
possible and be clever about where we
start to automate the process to show
you know a dramatic improvement very
quickly so that people say oh that's
interesting that that works for me yeah
it works for the better good of mankind
but it works for me that's cool i'm
going to keep on doing more of that ok
and recognize the depending on
what's the context where you start
working if it's powered you know the
team is under a lot of pressure fear
exists and fear of changes is very
powerful no change is visible you can
always track down now you took the
decision six months ago so people will
just go you know you asked me to do
something different where status quo is
stealth nope someone else will take that
decision and this is something we need
to to understand as we as we as we push
these things forward the goal is to
unchain creativity ok maximize the
amount of time we spend being creative
but there's a cost the cost is to
automate once and then maintaining it
and it's a non zero cost but the benefit
is amazing benefit is we become creative
again we don't have to do all of these
manual steps that we have to do every
time then we get tired we get it wrong
and it creates a chain reaction of
issues especially as you get closer to
the release date ok so we want to views
of the machines as much as possible i
think we're far from the silent
revolution before the machines become
self-aware f will have other issues to
treat we can keep on doing that for
quite a while and as a no basic agile
principle you know we have to slow down
to speed up but we have to be careful
not to slow down to too much for too
long and make sure that we show
potential for speed up as soon as
possible and then real speed up and that
takes and means courage so we need to
start little by little and an
incremental approach ok and automate
with the
the goal of of reduce reducing
turbulence making things easier and more
fluid as we go as we go along and if
it's not simple it just won't work so
what's the scope well I think the scope
is is to get full benefit is ready to go
from build package install deploy test
in a synthetic environment synthetic and
I mean a safe environment that will not
cross fertilize with an already running
system okay we want that to be in in a
safe bubble but yet the environment
needs to be as representative as
possible of your real production
environment and then you know so that
means means you want to do this as as
often as possible for you know times a
series of releases or a series of pieces
in motion which means you're going to
have many many test beds and that's
where the cloud helps because it's going
to allow us to do to do that in an easy
way so I mentioned a little bit what the
problem is who we are what we've done in
the past you know to put us on the right
track and I'm going to talk about the
cloud computing how that specific
technology actually enables and some of
the issues that the test informations
have so cloud this year is the latest
buzzword and it's used to actually mean
many things which is great you know grid
computing went into that in 2000 1000 to
was a cool thing to be in which is which
is okay and we have a wide range of
definitions and I don't want to go into
the semantic definition that's not my
thing when I say cloud i mean amazon ok
the amazon type of cloud as opposed to
for example another definition would be
maybe the google app engine is often
label under the cloud umbrella but it
does it has a different take at it and
it's not necessarily appropriate to what
we're trying to do so no criticism one
on Google App Engine is probably cool
because there's Python hundred
so what we want is a the reason why
Amazon is the glamazon cloud is
interesting in this case because we have
under men resources and storage okay and
every one of the reason why it's so
popular its ease of use okay cloud is
delivering in this context because it's
easy to use it's easy to use because it
exposes it's it's a rest interface with
HTTPS and restful web services is just
wonderful okay if you haven't been
across those that there's a reference at
the end it's a book from Ruby and
Richardson and it's a it's if you if you
suffered with decom and corbin in the
past or big web services so place up web
services you're going to find this
refreshing I hope it's not then just
give me a call we'll talk about anyway
so it's not a topic of this discussion
so I'll resist carry on but at ease of
use of the interface of the cloud of
Amazon is key and it's in its success
the other aspect of it is run time in
runtime environment because it's using
virtualization and I resonate much very
much with our keynote speaker this
morning about the the keenness of
virtualization it provides hardware
virtualization which means it gives full
control of the user so the user has full
control of their machine because they
are virtualized and that isolate
completely the infrastructure providers
from what the users are doing because
there was this abstraction in between
the hypervisor that provides instruction
between the the host and the virtualized
environment and that is great because it
may keeps both sides of the layer happy
and elasticity amazon has a
pay-as-you-go policy so you only use
what you need and you pay for only would
you use okay and and this promise of
relax it a cct comes from the fact that
in the case of amazon and those others
there's like a go grid and there's a
nexus i think that are interesting in
betweens and Amazon and Google App
Engine approach to cloud computing but
would they say is they you know use our
stuff will we can buy hardware
faster than venue which means that the
system remains elastic there's no
concept of a queue so nine taken in the
system as a user and that that gets rid
of a lot of the problems that we
sometimes see in grid computing okay so
ease of use virtualization elasticity
these are the key features looking at
the specific services that at we're
using the first one is easy to say it's
the elastic computing cloud so I said it
was based on virtualization it's
actually using Zen at the moment and
it's interesting because now everybody's
fighting that that part of the world so
Microsoft practice about is about to
announce their their new cloud solution
amazon is saying hi but we support
windows as well so have to see how that
works but most of their stuff at the
moment is a it's based on then the user
basically request a virtual machine so
it says it is store their virtual
machine or either using a public key
available virtual machine stored in in
s3 then the simple storage service i'll
talk about in a minute and then the
amazon will just basically boot that
machine for you voila and if you
configure it right then you can actually
log in as root and then you're you've
got full control and you can use a rest
or a soap interface to to control that
the service that we're using that's
interesting in this context is the
simple storage service it's basically a
very simple hierarchical approach to
storage with recall bucket so their
containers and objects you can put a CLS
and metadata on the objects and an
interesting service that came across
came along this summer is the elastic
block store so that you can prepare data
and the data that is stored in s3 can be
actually mounted as a local disk into
one of your instance that's cool because
if you have large data sets that are
used in some of your tests and you can
decouple the image and the data so you
don't have to carry that data with every
image you want you can actually compose
that at one time that's interesting
so why is this an enabler for what we're
doing well virtualization as I said
provides the clean layer which means for
maybe the first time the user has really
full control over there there is an
environment through a managed service so
they don't have to control the hardware
but they have full control once once
they're running and that's that's nice
but it it also means that the the new
building block when you're deploying
you're you're you're composing and
you're building your your system at
runtime is a virtual image so that's
that's your new building block the
pay-as-you-go approach is interesting
because it means you can start can start
slowly and as your need increase in
terms of richness of scenarios or
specific need for you know there's a
group you gotta get a problem boss we
have a problem then you can actually
request the resources as you need going
back to the ease of use and the
interesting technical technological
choices that Amazon light is that the
use rest and rest is HTTP that's the
language of the web so you end up having
a very simple way to integrate that
service with existing services which is
one of the reason for example you have a
lot of you have a plugins in Firefox
allows you to start stop monitor even
login into a machine and follow a follow
on with what's going on now all of that
is possible because they made a right
choice of exposing their services with
through HTTP which means when you start
to integrate this type of technology
into an existing process you don't have
to take on the world you don't have to
take everything on its head you know
it's it's it's not viral and then
something we suffered in the diligent
project when something what goes goes
wrong during your deployment it's okay
you just freeze your deployment you can
log in and investigate directly your
live system and trace through as you go
along
and lastly that means that now we've
freed up the the development team from
having to manage their own test
infrastructure only that's an elastic
infrastructure so you can grow and
shrink as you need it so a a more
complex solution would that would
include the ability to learn to capture
the image composition so create a
language so that you can capture the
generation of your images and
parameterize your deployment so it will
become reusable building blocks so we
want to be able to capture recipe to
build your images and bill your
deployments and here's a the same image
I showed before but this time showing a
little bit the process and we're
proposing where as we go through the
process we're able to now save stage so
source code becomes packages becomes
work and then added with an installation
procedure we can generate automatically
virtual images we can parameterize the
deployment and then compose those images
into a deployment model that can be
executed and you can fire system test at
that get the results in a releasing
machines that when you're when you're
finished so looking at it in running
slightly late son I'm going to speed up
a bit looking at how we would work in
real life you would basically create
what I call it a little bubble inside
the inside a cloud will you deploy your
machines they do whatever they want
during that time you can deploy
variation of this of this deployment in
parallel one is finished so it gets
recycle the machine go away to be used
another day at the same time you could
have a completely different deployment
scenario you're running in parallel on
the 9th what's interesting here is that
all of those deployments I isolated yet
they're in a representative environment
okay and resources only use when they
need they needed and at the end just
things things go away if we have a quick
look at what could happen what might
happen inside one of those
balls depending if you if your software
is designed to actually come up from the
ground from from from 0 up on its own or
not we have a very simple API it allows
you to actually synchronize things so
that if say you have a test suite that
relies as a dependency at runtime
dependency on a service that service has
a dependency on a database or you want
to make sure that database up before the
service comes up before the test we can
fire ok so the our API will will will
make sure that the synchronization takes
place so that the service is wait for
each other before they're there they're
authorized to fully configure and
execute and once the test we just finish
it says ok I'm good to go we wrap up and
the the report is sent back to the user
so it's not all white and fluffy there
are some issues amazon is using Zen for
its virtualization technology and then
is paravirtualization as opposed to full
virtualization which means the
virtualized OS is actually made aware of
being virtualized ok which means there
are constraints on on the type of
colonel supports that that you can do
and as you start to play with virtual
images because it's so easy to bake new
images you can easily get overwhelmed
with who what's this one made of and
what's the difference between this one
so that that can actually become become
confusing that's not seattle by the way
and so the silver lining where
virtualization technology there's a new
kid on the block called kvm that
provides full virtualization so that you
don't have to tweak your your OS being
virtualized however requires some
hardware extensions in the chipset but
intel na + d recognize that and are
rolling out new chips now that have
these extensions so as time goes i think
we're going to see a shift between at
least from i hope from amazon as well
but we're going to go for from
privatization to for visualization and
for the virtualization image
proliferation problem the idea is to as
we're proposing is to create
these images from from high-level meta
data so that a human can read the recipe
that is used to generate a virtual image
so can a computer so can a program and
then that that I could alleviate this
problem so I've spoken a lot about the
actual Amazon service but what's
actually more exciting than the
commercial service at Amazon proposes is
the technical technical article
technological sorry choices that they
made behind they did it took behind and
talking about this to some some some
potential users and customers some are
feeling a bit on KGC about having their
no trade secrets running on a foreign
network so there is a need for perhaps
standardizing or learning from the
Amazon experience and then allow making
it more more easier to for people to
roll out their private cloud there what
I call an inner cloud so it's really a
call out for open source distribution
and open source implementation of these
types of technology so can we can more
easily reuse this type of technology
into our process automation so as
conclusions um I hope I've convinced you
that it is possible nowadays to 20
tonight a full scale system testing from
build package deploy and insistent test
it's a bit more difficult than unit
testing but it's it's well worth it and
some of the benefits we see it is a low
stress level than turbulence during the
software release cycles okay by doing
more automation at that level and more
of the entire cycle we keeps you humans
more creative that's very important and
then we that means we get the machines
do what they're good at and this is
possible by a fusion of cloud computing
and the right framework and management
layer on top and I think it's Eric gamma
talked about test infection I'm very
much test infected but with this it
means the test infection defends because
now it's it's including more of the
release process
so here are the references I I promised
I just want to quickly acknowledge the
work of paducah virus I I mentioned him
already and Guillermo Diaz and pedal and
Radha were working with me on the
diligent project in two summers ago we
did this work that actually showed up
show that it's possible to use these
types of technologies to show positive
benefits at the system test level so I'm
very glad happy that we did that work
together we're using this today with
this 9 a min and cows name well thank
you very much for a first thing i think
i've been i've been authorized to take a
few questions so fire at will yes I just
go back to the private cloud or the
inner clown yeah how far and prop you to
do of projects that are out there that
are being worked on or how far is I
actually I wish I could be more open
about this week we're discussing this
with a few partners to team up to
actually provide those types of
interface we haven't firmed up those
those collaborations yet so there are a
few things but it's not like taken run
with it so okay well let's have a let's
have a drink after that I will tell you
where we are but I just want to be
careful about about this but that are
some interesting we're going on
eventually what we would like is really
to for people to focus on not
necessarily exactly the Amazon interface
but that level of semantics and that
level of offering so that we don't have
to rewrite our app every time the cloud
change but there's some some stuff
happening there as well other questions
yes at the back
the cost of running an Amazon easy to
exceed 24 hours a day seven days a week
it's roughly the same as hiring out of
hardware real partner from somewhere
like care networks or something like
that what do you do to keep costs down
when you're using well that's a great
question the question is the background
is about cost of Amazon the fact that if
you it's so just and then how do you
it's like the original mobile phone
contract so you were you would you know
you talked five seconds you're you're
charged a minute and at the moment it's
a bit the same you charge per hour so
for a small instance it's ten cents then
you an us cents so it's 72 bucks per
month for one machine photon a small
instance and then you have different
types of of instance with more cores
more memory and so on it's a problem and
then we actually are hitting that as as
you as we as we get our tests that are
much more performant and so on you end
up having idle idle time from from on
the Amazon side that that you we would
like to reduce to be paying for per
second or a perfection of minutes so
what you could do is you you could reuse
some of that time to actually restart a
machine that doesn't restart a counter
and then put it in a different state and
then carry on that's something you could
do we don't at this point in time we say
well we we bite it and say it's going to
cost you ten cents to start a machine
whether you use it for one or 59 minutes
but I mean personally I think it's
something that i'd like to see go away
did
what's the flexibility is that I can say
for this minute I want that image that
this storage this context now the minute
after I just forget it I want these five
machines together in a minute after well
I want to rerun that thing but I don't
want one client I want a thousand
clients because I've got this race
condition I want explore its this
flexibility that you need virtualization
and in the service we have in front of
that would we did Alexis intestacy i was
i was giving yes sure
if I agree completely with a multi-tier
system you want to bring a client
systems all together and you want to
treat them as a group what do I have to
currently right scripting or so you can
say I want five machines like it like it
what you want okay yeah so the question
is good yeah so the question is can we
can we bring those machines together as
a group and then can we control the
order in which they come up right
so when you start an Amazon machines
together you can you have control over
the network configuration but certain
level the network configuration of those
machines so you can tell which one will
see which ones and and so on you can
also provide excellent visibility is
only one of those machines is externally
visible there is no such thing as in the
Amazon interface to control the
dependencies between those machines as
part of the same in senshi ation process
and this is where our tools come in that
are as little intrusive as possible to
say you know in the boot up sequence we
try to be as light as possible to allow
this use case I was describing to say
make sure that the test suite fires up
only after the services that that need
to have been fired up and then exchanged
the minimum amount of meta of meta
information between those so that the
test we will find the service and the
service will find a database because all
of that is dynamic we can't predict up
front the the names there's a service
called the elastic IP so that you have a
pool of fixed IP that you can assign but
in a dynamic way of when you're in a
dynamic environment where you deploying
a lot of those machines you would you
would starve your your static ip's so
that's where there's a requirement for a
set of lightweight services that sit on
top of cloud to manage bringing your
system from from nothing to a running
system so it's a combination of those
two things will answer your question
chip would pleasure thank you yes in the
back
okay I didn't mention that but just
before leaving leaving cern i wrote a
comparative study between grid and cloud
and I it sits on our website because
there's a link to to this comparative
study and I've tried to bring different
information I could find one of them is
an interesting section in this and this
report on on performance the the Amazon
is using a strange strange unit compete
you compute unit to to guide the user in
their choice of instances i mentioned
ten cents is a simple instance then you
have a larger larger instances and so on
so that that defines that the promised
performance profile of that machine okay
but in terms of networking we have very
little information about how Amazon is
putting their their their their back-end
together we know that from reverse
engineering and doing doing black box
experiments that there are data farm and
their their server farms are separate if
you start two or several images together
you're likely to have a close proximity
network wise and you can you get more or
less one gigabit per second connectivity
between those instances and a little
less between the storage farm and so on
so the performances is good okay
assuming that that that you accept the
the configuration choices they made the
way they deal with storage and compute
okay thank you yeah wow I never make it
to the coffee now so let's have a coffee
and then we talk other questions no is
that it
okay thank you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>