<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Simple interactive 3D modeling for all | Coder Coacher - Coaching Coders</title><meta content="Simple interactive 3D modeling for all - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Simple interactive 3D modeling for all</b></h2><h5 class="post__date">2008-06-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YEM-XAeY4K0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning it's my good fortune today
to introduce Anton bandhan hangul a
researcher that Google is supporting
through the google research award
program he's a director of the
Australian Centre for visual
technologies and a professor of computer
vision at the University of Adelaide he
rocked siggraph last year with a video
trace and now it's a chance for you guys
to drink it in and Tom Vanden angler
great thanks Lance thanks for coming so
yeah this is me Anton van den hangul
I've got three jobs as you can see so
video trace came out of a need to
generate geometry from video sequences
for a completely different project there
are a bunch of ways that you can
generate geometry out of video from
completely automatic methods down to
just using something like Max or 3d
studio max admire or something like that
to just kind of wrench the geometry out
of nothing and adjust it until it
matches your images but we couldn't find
anything that really made it easy to
generate geometry that actually
accurately matched what you see in the
video so we came up with video trace so
the idea of video traces that you take
absolutely well absolutely pretty much
any video right so we have used this
sequence for a while because it's that
you know there's nothing special about
it it's and it's an old pal it's not
even HD camera it the guy who took it
was riding a bike at the time you know
it's it bounce it bounces around and
it's not there's no particular camera
motion you know it's really ordinary
general video and we want to model this
object which is in it and the model the
objects got specularity zon it and it's
reflective
and it's curved and just kind of a
difficult thing to model so the video
trace process is pretty much just
sketching out the boundaries of the
polygons that you want to model right so
that and every time one of these things
turns blue it's become a 3d shape right
it's fit to the image information so
video trace allows you to exploit the
automatic analysis of video in order to
create these models so it kind of gives
you a handle on all of that automated
automated analysis that people have been
working on for so long and you know it's
really that easy you don't you don't
need to spend six months at a training
course to be able to draw a model like
that you do need to be able to
understand what a you know what a model
is and that each of these things is
going to pretty much turn out to be a
polygon in your model but apart from
that you don't need any really
particular skills so what comes out of
that is a 3d model of the object you're
interested in which looks like that it's
not nobody's going to go and build
another car out of that you know it's
not that level of model but it's a low
polygon count model that you can use for
all sorts of things including cutting
and pasting objects between sequences so
you see that the old one occludes the
new one and the new one includes the old
ones that both 3d and the both rendered
in the right spot and as the camera
bounces around the new one sticks and
the oh you know you can deform them both
do whatever you want all the kinds of
things that people take for granted with
images video trace lets you do with
video and you know there's a whole bunch
of other applications for video
processing that come out of video tres
so this is some video we just download
off stock video site we didn't take it
which was only relevant because it means
that it's absolutely ordinary video it's
actually surprisingly how to focus there
are compression artifacts up here but
nonetheless we can track it draw over it
and generate the kind of model that we
need and this is kind of the video
choice process right you you draw what
you're after you look at it in another
image to make sure that you're actually
getting the 3d that you want and you
know go back and edit it if you like you
can move things around in 3d but you can
also adjust the outline and after a few
minutes or modeling you've come up with
the you know the accuracy of model you
need to maybe reshoot the video from a
different camera angle if it's not too
far from the original camera angle or to
insert synthetic geometry in the scene
so you know in order to render those
textures on to those sales you've got to
have a pretty accurate idea of what the
shape of the sales are and if you look
at the original video there's no points
on here right there's no feature points
that would give you that information the
reason that you can achieve that so no
automated method is going to be able to
do it the reason that we can do it is
because you know what the shape is
because the person who modeled it knows
what the shape is and there's enough
kind of points for you to stick it to so
the combination of the automated
analysis and the human pudding in the
high-level information gives you the
accuracy of model that you need and if
it doesn't give you the accuracy of
model that you need you can go back and
fix it so you regain complete control so
the idea is not that it takes power away
from you like a lot of these automated
methods you push the big red button and
hopefully the model that you want comes
out of it but it may well not come out
of it video trace gives you complete
control so you can to the point that you
can model stuff that's not there if you
want it so you know you regain full
role of the modeling process there's
also a bit of semantic information that
you're putting in there as well at least
kind of that sort of stuff because you
specify what you want to model so in
this case you want to model this
building not the trees behind it and in
the case of the car before you want to
model the car not the environment around
it there's no way you can do that
automatically if you want to model of
the car and you try to use an automatic
method it's going to wind up merging it
into the background because the
difference between the car and the
background is something you've got to
recognize you know that's a kind of
human distinction
ok
so 3d modeling turns out to be critical
to a whole bunch of applications and
every time I talk about video trace
somebody comes up with a new application
that we hadn't thought of so the obvious
ones are special effects and computer
games and those sorts of things but
there are also a bunch of applications
in mining a lot of the interest we're
getting off the website is from people
doing architecture not necessarily
architects themselves but people who
want to model bits of from small to
large bits of cities so that they can
put their new architecture in and see
what it looks like next to everything
else so people just building houses and
want to know whether the you know the
council wants to know whether your third
floor window is going to be looking
straight into the backyard of the house
next door well just what it's going to
look like where the shadows are going to
fall that sort of thing they're a bunch
of defense applications there's a lot of
people looking at urban planning people
keep sending us point clouds to model
large bits of cities anywhere that you
wind up with image data and point data
you can use video trace to extract it
and get the high level model that you
need and it is a high level model you
know it's not it's not the kind of model
you're going to put into a instant
fabricating machine to generate yourself
a another land cruiser it's a it's a low
polygon count model and it pretty much
generates as many polygons as you can be
bothered putting in there all right so
um people are getting more visually
sophisticated there are more and more
special effects on TV and video and even
more so on the web now but there's also
more 3d being generated so video is
perhaps not 3d but it's it's at least
you know there's 3d in there you can't
treat it as a set of 2d images they're
interrelated but there are also more
laser scanners and people putting all
sorts of different scanners on the
Front's of video cameras and
sort of thing to try to figure out 3d
the the amount of 3ds going through the
roof but the interfaces to most of these
modeling programs are usually pretty
hard to fathom at least I find them hard
to fathom you know you you're stuck
trying to do at least 3d things with a
2d interface so your mouse is a 2d
interface and your screen is a 2d
interface and you're stuck kind of
holding down shift ctrl alt you know
right button trying to spin it in the
jet explain something like that all of
the interactions in video traces are 2d
and conspicuously so like consciously so
we've really made an effort to make sure
that everything you're doing makes sense
and you can just think about it as a 2d
operation when you're doing you don't
need to kind of figure out what the
projection is that you're using at the
moment so it's supposed to be really
usable so it means that people who
haven't had access to 3d before can
model what they're after but also that
people who have can do it quicker you
know 3d hasn't been all that democratic
really there's a very limited number of
people who can really model stuff in 3d
so the number of people who can put
their houses into Google Earth is
relatively limited so they are low
polygon count models there and you can
generate as many polygons as you can be
bothered with but we're talking hundreds
to thousands so the kinds of
applications for those models rather
than we are really accurate high polygon
count models are things like putting
your own objects into a video game there
are more and more video games now where
you can not just edit levels but edit
characters and edit are the bits the
geometry that go into these games and
they all have to be low polygon count
models because they're all coming out of
a database they need to be rendered
quickly and if you're going to share
them they need to be sent around the
world that sort of thing if you want to
put your couch into second life or one
of those other virtual environments
again it needs to be transmitted around
the world so it needs to be a low
polygon count model and a lot of the
stuff you see in second life like some
of its been generated using a proper
modeling package and looks who's
complicated geometry but there's a whole
lot of geometry in there that's shoebox
models and that sort of thing because
they're the tools that people have
access to if you're not going to learn
to use Mac's then you're stuck with that
kind that really basic sort of modeling
paradigm there are you know good reasons
to put your house into Google Earth but
the one that I'm actually most
interested in is video editing not not
the first thing you think about in terms
of application of low polygon count
models but if you so most people have
got a program on their machines which
does something like Photoshop right so
you can take pixels out of one image and
you can put them into another one so if
you want to remove somebody from your
you're happy snaps or maybe you want to
put somebody in to your happy snaps most
of us can achieve it but you can't do it
for a video unless you buy and learn a
really complicated package there's no
video shop well actually I think there
is something called video shot but
there's no Photoshop equivalent for a
video so what we did in that one of
those first sequences was cut the car
out and park another model of it next to
itself those sorts of operations need 3d
models so if you want to cut and paste
between sequences you need to model the
geometry and if you want to remove
someone from your home videos if you've
got some crazy uncle that you wish you
hadn't invited to christmas then you
need the geometry because you need to be
able to replace them with something and
it needs to stick right it needs to look
real which you can't do as a bunch of
images if you take each image out of
your video sequence you do each image
individually then it doesn't work as a
video because it keeps flashing it keeps
changing between images so video editing
i think is one of the more interesting
applications for this so if you happen
to own a truck this one seems to be
owned by bob middleton at least here's
the guy with his name on the window or
so and you know if you want a truck like
that you probably want to put your name
in the window as well if you want to
drive your truck around a video game
then with video trace it's a relatively
straightforward kind of process you take
some video you model your truck and you
put it into your video games editor and
a lot of these video games have editors
that you can use to create geometry from
scratch or import other models into and
then you know the editor allows you to
do things like specify where the wheels
are are and that sort of thing and I
think it insists that you put artillery
on the back that bulb doesn't actually
have artillery on the back of this truck
but you know so it means that if you you
know if you have such a truck you can
drive it around a video game or even
maybe you want to drive into it in a
video game or whatever there are bunch
of applications in special effects this
is a bit of video that we took from the
our art gallery if you want to model
this so this is modeled as a bunch of
extrusions but if you want to make this
happen so the animations done in max
right we just modeled it in video trace
and put it into max but in order to make
that happen you've gotta not only model
the chair you've got to model the plinth
then you've got to mold the background
so that when the chairs gone you can
replace it with your model and there's
actually an artifact in there you can
see the point at which the model just
kind of up here somewhere it's a bit of
an artifact or you can see where the
model is overlaying the real image
okay and video editing requires models
so if you just take so Bob wants to take
his truck and park it next to his SUV in
his home video then you can't just take
the pixels from one video and chop them
into the other right because this is
what you get in the you know this is
even we've tried to we put a bit of
effort into trying to make it work so
that this video actually will swung from
the swung the other way song from left
to right the first time so we reversed
it so that at least the camera is kind
of going the same direction but you can
see what the problems are it doesn't
stick to the ground for the stuff for a
start and the one in the background so
the the SUV is turning a lot quicker
than the front and it's not just a
question even if you stuck like you even
if you could stick this to the ground it
doesn't solve the problem because it
needs to occlude the background at the
beginning and then the SUV needs to
occlude the truck at the end right so
you need a model of both the truck and
the SUV um and yet you need you also
need to be able to render them from
different camera angles so the camera
angles in the video you want it to go
into a different the camera angles that
you want to take them from so you need
to be able to generate new camera angles
which again requires a model so if Bob
wants to model this truck in video trace
at least it's a relatively
straightforward kind of a process
and then we need to model the space that
is going into because this geometry here
is going to overlap the new geometry we
need to model that and we don't need to
model it accurately we just need to
model it enough to make it overlap
properly we built a little interface
with these kind of handles here so you
can put the new object into the into the
original sequence and we actually render
it in max all you need to do is kind of
export it as a collection into max and
re-render it max has much better
renderer than we do all right so
modeling architecture is something we
get a lot of traffic for a lot of hits
on the website a lot of people emailing
us about modeling architecture we're
working with a company called skyworks
that sent us this video of Copenhagen so
that's the end of the modeling process
it takes a couple of minutes to generate
that level of model there I hope that
sometimes I flipped that inside out
myself I hope you see that as the right
way around but it's you know it's not a
bad kind of a model for that amount of
effort and really the quality the model
is limited by the number of pixels
you've got there so skyworks are sent us
this aerial video but we're in the
process of modeling Amsterdam with them
so the central bit of Amsterdam and
there they've put a camera on a boat and
driven it around Amsterdam so that we
can get all the textures and get more
pixels on the Front's of the buildings
that everybody actually cares about
no sorry this is only one image of a
video this is it's an aerial video and
we use maybe five seconds of aerial
video to generate that you have to get
the 3d from somewhere sorry the question
was whether this is just one image or
it's a more and it's it's video so the
idea of video trace is that you use a
kind of 2d interface which is based on
an image to access that 3d information
and that 3d information can either come
out of analyzing a video so you get the
3d out of the video or it can come out
of a point cloud generated by a laser
scanner or any other source of 3d that
you can find so this is just another bit
of architecture an attractive bit of
architecture that we remodeled recently
but the reason to show you that is so
that if I can grab it OOP Google Earth
can't find a connection to the server
Wow is in order to be able to put it
into so we just modeled it texted it and
rendered it into the Google Earth
and this is the 3d model as you know
it's a relatively detailed 3d model it's
still relatively low polygon count like
this is that all of this detail here
just comes out of the texture there's no
geometry modeling of that rough surface
all of this is just one polygon but
these things are modeled individually
and it's a reasonably accurate kind of a
3d model
just get the point grant away and you
can do it out of images as well so these
are two input images of one of the
buildings in Adelaide which we you can
you can generate the same 3d you do the
same stereoscopic analysis in order to
get the 3d information and then model
the geometry as you can see there's not
a lot of the rest of Adelaide that's
been modeled the hills are there and I
think that only two bits that have been
modeled the ones that we've done so the
the process so what you do with video
trace is to go out and take some video
of whatever it is that you want to model
and then run it through a camera tracker
and a camera tracker is a program that
does structure and motion analysis so
what it does is to track 2d features
through the video sequence so
identifiable points correspondences
through the video sequence and by
analyzing the projections of multiple 3d
points into a set of 2d images you can
figure out what the properties of that
of those projections are like there's
only there's a limited set of camera
positions that could have given rise to
the images that you've that you've got
and once you've got those camera
positions then you can reverse the
projection process and figure out what
the 3d is ok so that's what camera
tracker does and there are there are
three commercial camera trackers and one
free one voodoo was the free one if you
want to go try to then you put the the
result of that into video trace and do
the modeling and export it to your
application in reality the thing that
usually takes the most time out of that
process is the first step and then the
second step you know takes you half as
long and doing it in video traces
very fast okay so the idea with video
traces that we precompute as much as
possible so that when you're interacting
with it there's no delays and a lot of
the stuff that's behind video trace is
relatively standard computer vision but
the novelty is in making it accessible
so that you can do it quickly so you can
interact with it quickly so the
structure from motion happens first and
all of that gets stored so that you can
interact with it the other thing we do
is divide the video into super pixels
super pixels are groups of pixels that
have the same sorts of properties that
are located next to each other and so
you can operate on them as a whole
rather than as individual pixels that
speeds things up as well and then you
once you've done as much pre-computation
as you possibly can you interact with
that and try to in order that you can
exploit all of that stuff that we've
done previously so camera tracking the
structure from motion process gives you
what most importantly it gives you the
camera parameters so it gives you the
projection properties of the camera
corresponding to every image in your
video sequence okay so it tells you
where the camera was and what the
projection matrix is so the way that 3d
gets transformed into 2d on your image
plane for every image but once you've
got that then you can take your
corresponding points projecting back out
into space and get the 3d and you know
structure from motion you could also do
the same thing with a laser scanner so
you if your laser scanner generates an
image and it sends out light and
measures how long it takes to get back
in order to get a 3d point cloud the
result is the same you've got a you've
got a point cloud registered to a 2d
image you can go and draw on the image
and yeah it's critical to the
interpretation process that the whole
thing is based on structure promotion so
and this is what you get out of
structure promotion so this is the point
cloud that we get out of
applying structure for motion to that
SUV footage and this is where the camera
is for each of those images you see that
what you get out of that is an idea of
that kind of the blue thing tells you it
gives you an idea of what the focal
length was and the other to tell you
what the image plane is so that you
pretty much got all of the projection
information and that's critical because
you need to be able to interpret the 2d
interactions as having come 3d effect so
you need to understand the relationship
between the 2d and 3d ok so the video
trace interactions are always growing
we're implementing new things all the
time the fundamental and most useful
thing is just to be able to draw
straight lines and close sets of
straight lines define planar polygons
which you can then go back and edit to
change the outline of and it turns out
to really be the most powerful
interaction you can have just drawing
straight lines and coming up with
polygons curves there are kind of two
sets of curves one of them is to change
the outline of a predefined plane of
polygons and the other one is a 3d curve
so you can just draw one line in 3d and
have it define sorry one line in 2d and
have it define a 3d curve on your object
mirroring is also mirroring an extrusion
or both well both turned out to be more
powerful than more useful than I
expected mirroring and extrusion
particularly allow you to model stuff
you can't see it turns out that in a lot
of video you don't get the full 360 or
you don't get all of the footage you
need to be able to model the whole
object but mirroring an extrusion allow
you to make to kind of imply things
about the bits of the geometry that you
can't see so for that truck for instance
you only ever get to see one side of the
truck the other side is just duplicated
from the first which means that the
textures are backwards but there are
still textures there
and the latest one we've been doing is
dense meshing so when you don't want to
have to specify every polygon by hand so
if you're doing a rough texture or
windows or that sort of thing where
there's a lot of geometry there and you
want some control over it but you don't
want to actually have to place each
polygon so the kind of fundamental thing
is that we're fitting planar surfaces so
we're fitting polygons so we use a
specified boundary so what you're doing
when you're using that kind of bit of
video trace is specifying a 2d
projection of a 3d polygon into the
image and that's obviously like really
under constrained specification so there
are infinitely many planes that will put
give you that 2d projection and you need
to find which one of them it is and you
need to find it robustly and repeatedly
and so that people get what they want
but you also need to be able to give
them control in case they don't get what
they want and it all needs to happen
fast so that people can see the see the
effects of what they're doing so we use
a fitting process so it's it's kind of
least squares fitting underneath there
but we use a form of pre-emptive ransack
to do that fitting pre-emptive ransack
so ransack is a kind of extension of
Lee's medium squares fitting so you you
trying a bunch of hypotheses and then
evaluating each hypothesis against the
data and the idea with pre-emptive
ransack is that you do simple tests
first and that they should be they
should have a low false negative rate so
that you can run quick simple tests to
throw away as much of your data as
possible but you can guarantee you will
pretty much guarantee that they're not
going to throw away than when you want
and by running a hierarchy of those
tests we can do it in the kind of time
that's actually available
so we run the simplest and most robust
robust test first and they tend to be
the 2d ones rather than the 3d ones so
sorry the 3d ones are out in the 2d ones
the ones that relate to the point cloud
rather than the edges and that sort of
thing you know it edges are really
strong q but they tend to generate a lot
of there's a lot of noise and they're
really hard to line up exactly where you
want them where's the 3d you are a lot
more robust so if you've got your image
plane and you're barely visible center
of projection down here and you specify
the boundary of some polygons on the
image plane then you can smear pretty
much all you need to specify that
polygon of the three raise a projection
that go through the vertices but any
plane out here any bounded triangular
bounded plane in 3d that has its
vertices on these locations on those
rays is going to project to that shape
and there are obviously infinitely many
of them so we need to figure out which
one we actually want and this you know
there's no point getting it wrong so the
big extra clue that we've got are these
reconstructed 3d points and they're
noisy there's that on that jeep example
there's a specularity that moves across
the window and surfaces are reflective
and curved and so you're going to
generate false tracks all over the place
and they'll be points above and below
the surface because there's a you know
this stuff reflected in the windows as
well and it's a you know it's a tough
problem there's if you just drape a mesh
over the point cloud you get something
that nobody will recognize as the object
or after so the fitting has to be robust
and flexible so but it is the object
points which really now the thing down
so we fit to those first and you don't
necessarily get a lot of them there if
you're like you're defining one polygon
it's probably one planar area that
you're looking at
it probably doesn't have often it
doesn't have a lot of texture in it and
so you're trying to fit to a
surprisingly small number of points
quite often so these are the tests well
a bunch of the tests that happen anyway
so the first thing is to look for
support from the point cloud and you can
do that quickly you know you can
evaluate a plane hypothesis against the
point cloud relatively quickly but it
needs to be robust we're still using a
Huber distance on it which probably
isn't the most sophisticated thing to do
but it works we need to test that it
reproject swith in the new image
boundaries so if you start to look at
this object from another image so that
the the way that you test these things
is to draw it in one and then look at it
in another image and that's really where
the 3d comes from you want to know that
the object re projects within the image
boundaries if you've got a hypothesis
that doesn't reproject into any of the
other images then it's probably not the
right one there are constraints on
relative edge length and face size that
just means that we're checking that the
plane hypothesis actually makes sense
you know it's not kind of some really
obtuse angle thing we do color and
histogram matching on the faces if the
if the plane hypothesis actually falls
on an object then when you reproject
when you look at that plane in a bunch
of images they should all be
reproductions of that same object and so
the color histogram shouldn't change too
much you get some color histograms you
know there'll be some change in color
because you're you know angle to the
lighting and that sort of thing has
changed but if you hypothesize a plane
that's just in space it doesn't
correspond to an object when you look at
it from different angles it will be
overlapping different objects and your
color histograms will be completely
different
we also do color matching on edge
projections and check that the
reprojection into neighboring images
isn't self occluding again just kind of
sanity them is just a sanity test really
okay so 2d curves the it turns out that
what kinda that just drawing big
polygons and ordering polygons anyway
and editing their boundaries is a really
powerful mechanism for specifying these
shapes I wouldn't have picked that
originally but so the 2d curve process
just means that you're taking these
polygons and shaping the edges and if
the edge your shape happens to is a
boundary between two polygons then the
polygons with the normal closest to the
camera angle gets a curved boundary and
the up and the other a polygon one's up
faceted into a curve 3d curves so with
this process he's just drawn on there a
bunch of curves by that exists in 3d
just by drawing one line okay so there's
no with there actually is in that
previous one and pre an existing polygon
but these this 3d curve interaction
happens by a process it's a lot like
doing the methods that are used for
dense surface reconstruction but it's
just happening along a line so by
drawing that out has actually specified
this as a curve in 3d not in 2d and you
can see that because this shape here is
is curved and it's building that up
using well it comes down to photo
consistency
it's doing rather than feature based
analysis is doing photo consistency
which is a bunch of using the idea that
the true shape should have the same
appearance and all the images okay
mirroring turns out to be really
powerful so that red line there is just
specifying the mirror plane so that
you're indicating that all this geometry
exists on the other side of the car as
well and it's a powerful way to
duplicate geometry without having to
draw it all over again so the red line
doesn't need to be drawn all that
accurately because what you're doing is
saying that the the mirror plane roughly
intersects that spot and mirror plane
hypotheses are generated out of the
geometry that you've already specified
by looking for symmetry in the bits of
geometry you've already specified
because you know you need to specify all
of the bonnet before you can model any
of it and so you can use that
information to hypothesize a mirror
plane and extrusion so that's an
extrusion they're extrusion turns out to
be a really powerful mechanism not only
because a lot of things that are made by
people are made by extruding things but
also just because they seem to have
profiles that are the same along an axis
so they're not quite generalized
cylinders but they're you know that's
kind of the idea and modeling that
geometry there it really it's a process
of three minutes perhaps four minutes
okay and the latest thing we've been
doing is dense surface reconstruction so
if you've got surfaces where you don't
want to have to specify every polygon
but you still want control over them
what's happening here is that there's a
polygon we've specified and we mesh it
and then indicate which bit of it we
want to interact with and then you can
just push it in and you can see as it's
being pushed it's snapping two spots
that are supported by the image set it's
not just a surface getting pushed in
there it's every surface hypothesis as
you push in is getting reproject it into
the images and evaluated to see whether
it's photo consistent or not so a shape
is photo consistent if when re projected
into the original images it it matches
the information that's there okay so
this I mean you could model this
explicitly in video trace you could go
in and you could draw a polygon in and
try to model all these things but it's a
whole lot easier just to mesh it and
push it in you see it kind of snap to
different spots until you get the one
you want and generate a model like that
that hole in the texture there is
because you never see that in the
original image set and if you don't get
you know you do that and you don't get
exactly what you want then we've also
got these other tools which allow you to
paint constraints and as you're doing it
you're interacting with the graph
underneath there in order to compromise
between the image based analysis and the
human interaction so you still get full
control so that's not the highest
fidelity model of the Colosseum that
anyone's ever generated but is probably
the quickest
alright so we are constantly extending
video trace the most of what we're doing
at the moment is putting new
interactions in we're constantly well
being bombarded with new applications
and we're working on those as well we're
particularly working to support more
point cloud data and point clouds like
from laser scanners where you don't get
an image set something we're looking at
just trying to render the points in a
way that you can sensibly interact with
a 2d projection and get something that
gives you the information you need to be
able to do this kind of thing is I mean
Laser laser clouds points of laser data
give you the same have the same sort of
problems we're looking at Interactive's
structure promotion so the structure
from motion process is a critical part
of the video choice process you've got
to run the video through camera tracker
which does the structure promotion
structure promotion works most of the
time and that you know the the
sensitivity of this process to the
quality of the data is all in the camera
tracker if if what comes out of the
camera tracker if the camera tracker
works then video trace is going to work
if the camera tracker fails then video
trace has got nothing to work with so
and the reason that the camera tracker
would fail would be because you move the
camera too fast and you've got blurred
frames in there that it can't track
through or you know particular kinds of
scenes or just in general camera
tracking is a relatively reliable
process but which could be improved
through being able to interact with it
and particularly put in other
information that you already know so you
know that bits of the world or at right
angles and that this thing is four
meters tall and that sort of stuff but
yeah the one that I'm most interested in
is video shop trying to figure out how
to do the kinds of things that people do
with maybe not maybe not Photoshop it's
very
educator program but at least an
ordinary image editing kind of a process
to video so at the moment we've got
we're working on grabbing appearance
models out of real video so you've got
you take a video you have an object in
there that has some particular
appearance and is lit in a you know what
you know whatever the lighting is for
that scene we want to insert some tetek
geometry in there that has the same
appearance that means capturing there's
no point just capturing the material
properties you've got to get the
lighting as well and how they interact
and we were capturing both of those at
once and yeah there's a lot of stuff
going on in video shop okay that's it
any questions yeah I was just curious
where you grab the texture for all
messages from a single day image or do
you both so there is there are times
when you want to do both so the
automatic method is just to grab it from
everywhere and we go through each
polygon in turn and try to figure out
what the best images to get that polygon
from the texture for that polygon from
but sometimes that means that although
it might look like it's going to be the
best image there's something in the way
and you can go back and specify /
polygon that you want it to be textured
from this image but we are working on
ways of doing that automatically and
building up better textures over a bunch
of images
two questions was it a coincidence that
the SUV and the truck had a lot of
texture on them like the shadow of a
tree and all the stickers is that a
requirement it was a coincidence but I'm
not sure how it would work if there's no
texture if you take a an object which
has you know some curving homogeneous
object with no features on it then the
camera track is not going to give you
any points and you're going to be yeah
you're going to be stuck with doing the
process that is on the Opera House which
is trying to find bits that you can
actually get geometry out of and
extended so kind of you can night is not
interpolating but you know the way the
Opera House works is you can find curves
that you can you can actually get the
geometry out of automatically and you
can interpolate between those you can
tell the system that this is one
continuous curved surface yeah that's
actually a my second question what
happens for silhouette edges that are
not really the stationary in 3d but yeah
in your images say we don't have a
solution for soda there are a bunch of
surfaces like if you look at a sphere
then the occluding contour the boundary
is not you're not looking every time you
move you're looking at a different point
and there's we don't have a solution for
that that is one of the things we're
looking at at the moment
sorry that just quickly what is the
assumption you have about the
sophistication of the user is the system
able to tell the user they're being an
idiot not at the moment we try not to
but yeah at the moment it's it does
assume that you understand what a model
is like that you understand that you're
trying to generate something in 3d and
what it means to make a model out of
book out of polygons and if it does
exactly what you tell it to and so the
downside of having it having the power
to full control is that you can just
model garbage garbage in garbage out so
how crucial is it to have the actual
video as opposed to just multiple
snapshots of the same scene in I presume
different stages of your pipeline might
be affected by the fact that you don't
have such a dense sampling room I
renault so the we did that one at
siggraph and that was taken from i was i
8 to 12 images and the one the kind of
that building and a delay the slightly
orange build it's just a difficulty of
tracking versus mad yeah so really the
video is for the sake of the camera
tracker not video tracks don't it's kind
of helps the interface a bit because it
means that you most video was really
taken in a horizontal plane and you're
effectively kind of spinning the object
you're looking at so if it's video it
means you've got more points of view
that you can look at the object from but
and if you've only got two images then
you're stuck with those two viewpoints
you've got to model from those two
viewpoints but apart from that it
doesn't like the interface will change
if you have the just a multiple view set
up I mean right now you have just a
single window and you are kind of
playing back and forth aligning we just
traded as video so we just go you just
step between the
the images yeah in your screencast of
the app you keep moving between useful
tools at the magnifying glass to adjust
the angle that you're viewing the image
have you considered using your building
blocks on laptops that have built-in
video cameras to head track the user and
as they cream their head around three
projects one of the applications that
we're working on at the moment is in
augmented reality so exactly that
problem so in augmented reality you
usually said there are head trackers and
people know exactly where they are in a
3d environment but and you want to
insert synthetic geometry in but you
don't have the existing geometry so that
your synthetic geometry always sits in
front of the existing geometry and
there's some really interesting ideas
about modeling from augmented reality so
that instead of switching between
existing views you could go around to a
different spot you know I haven't got
this part of the model and it could tell
you to go walk around there and take
that bit of video but yeah we haven't
got a head tracker in there is the
software available or is it Larsen it's
on beta at the month and it's and it's
at that kind of level it's a technology
demonstrator at the moment but yeah it
is it you can download it somewhere I
can give it to you yeah
the primitives that you showed were
basically polygons and nerves and
surfaces have you thought of higher
level primitives like buildings or yeah
a whole car at once so we actually
started out with higher level primitives
and came down to this because we wound
up generating models that look like
shoebox world and this process is much
more flexible but there are times when
you actually know what you're looking
for and you want to fit that in there
yeah we've been talking to a lot of
people and all you really need is an
appearance mower you need to be able to
generate a set of tests that you can
apply so through that kind of
hierarchical process and that's not
difficult i mean you just you can
evaluate a more complex model against
the point cloud and against the image
set to be able to do the same kind of
process we need to figure out what the
interactions where if you want to put a
photocopier in you don't want to have to
draw an entire photocopier to kind of
specify what the outline is you need to
figure out what the interaction is going
to be but once you've got that then the
same process can be used so a lot of the
examples you should you mentioned that
the video sequence is basically like
rotating the object in space right if
you're if you're panning around if
you're panning around it are there
situate like how would your system react
if you'd had a video sequence where
instead of sort of rotating around the
object you were kind of zooming into it
or or have you have like sort of an
awkward camera path around the object
you do need to get 3d out of it and so
if your optical axes are all parallel
and you're going in a straight line then
you're not getting any 3d the same thing
if you stand you've put camera on a
tripod and rotate you're not getting any
3d so there's nothing to model from but
it really doesn't take a lot of
translation side to side to get you the
3d you need I mean your eyes are only
that far apart and we're all doing it so
yeah the specific use case I'm thinking
of actually is my own
house and that the houses are very
closely packed to one another so i can't
i can't pan around my house because it's
occluded by the by my neighbor's houses
yeah so I would the only video sequence
I would be able to get is by zoomed in
and maybe walked really close to the
side of my building my house right but
then and then I could zoom back out once
i got to my backyard but whole side of
my house you might be able to take a set
of photos that give you as long as you
can get some sort of 3d if your house is
at the end of a tunnel then you're
stuffed apart from that you probably be
all right okay thank you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>