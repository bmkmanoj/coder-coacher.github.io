<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building a More Efficient Ruby Interpreter | Coder Coacher - Coaching Coders</title><meta content="Building a More Efficient Ruby Interpreter - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building a More Efficient Ruby Interpreter</b></h2><h5 class="post__date">2009-12-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ghLCtCwAKqQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I guys wear fusion we're Ruby VM and
Ruby web application deployment shop in
Amsterdam today we're going to talk a
little bit about building a more
efficient Ruby interpreter with me or
today's when we lie I myself in Dingley
but we first before we're going to talk
about building a more efficient Ruby
interpreter it might be interesting to
talk live it up Ruby itself so for those
who are not familiar with Ruby it's a
dynamic language highly it's actually
highly dynamic strongly typed you can
you have closures and other features
like that as well it's resembling to
python somewhat you could say on its
little growing language by the time of
this presentation actually there there's
estimate there's an estimate about
400,000 Ruby programmers out there and
expect this number to grow to five
million by 2013 so yeah so it's still
growing rapidly and we believe that this
is in part actually to thank two reels
which is a popular web application
development framework which allows you
to create stunning web applications in a
Model View controller manner so there
are several Ruby implementations out
there and the main one or the best
well-known one and most widely deployed
one is actually Matson's Ruby
interpreter also abbreviated as MRI it
was named after its creator yukito
Matsumoto and even though it does a
great job in many things it has some
issues though it has a reputation of
being slow like some other virtual
machines and you know it has a
reputation of being a memory hog
basically so we wanted to make Ruby
better and in particular we wanted to
make it better for servers because you
know we enjoy web development and we
enjoy working with rails so in 2007 we
started hacking on the Ruby garbage
collector in order to you know at least
make it more efficient for you know
budget servers that we had to use back
then nowadays that's a totally different
story but so yeah we started hacking on
the garbage collector and this resulted
in a series of patches that we
eventually mold it into a usable product
and this usable product is called Ruby
Enterprise Edition and today Ruby
Enterprise Edition includes various
patches from other contributors as well
who share this passion on making a MRI
suitable for server environments so in
essence you can basically
consider Ruby interpretation as being an
MRI optimized for server environments so
now let's talk a little bit about
actually tweaking the garbage collector
and I'd like to if only donors to do
that so hi guys so our initial
motivation for making the Ruby garbage
Craig's copy-on-write friendly is to
optimize the memory usage of rails but
before we discuss that let's see how our
rails applications work so every Ruby or
actually real application has an
architecture that resembles something
like this there are multiple reels
processes and there's a single web
server or maybe multiple web servers and
load balancers or proxies in from and
the web surfers it will forward HTTP
requests to one of the reels processor
so here we see it working and then one
of those reels processes will generate a
response and then this response is then
sent back to the web browser and if you
look inside the reels processes then you
will see that each reels process is
actually a single threaded and handles
one concurring requests so if you want
to achieve true concurrency then you
have to run multiple reels processes and
this is actually no longer strictly true
because since Rios you got two real
sense becomes red save so these days you
can actually run multiple threads in a
single reels process but what we talked
about here is still true for most setups
out there and if you look at how much
the single hello world rails application
weights then it actually weights about
25 to 30 megabytes and memory usage
increases linearly which each process so
if you want hi concurrency and you have
to spend lots of reels process then
memory usage can quickly add up so we
looked at how we can reduce memory usage
well make it less bloated there are
several ways to do that for example you
could optimize rails or you could
optimize Ruby but both of these ways are
very tedious you have to put a lot of
work in them so try staying maybe you
can cheat maybe can
use the fork system call and copyright
and that works like this suppose that
you have a parent process with some data
say a ferry bill Haywood value 42 and
then you fork a child process and then
issue initially that child process will
share all of its memory with the parent
process so if you access to the value of
a in shell process then you're actually
referring to the same memory page as the
one in the parent process and when
neither the parent or the child rights
to that memory page then that page is
copied by the operating system and then
written to we say that this page is made
dirty and the child now still shares
most of its memory with the parent but
not that one memory page that is just
wrote to and our past experience with
dynamic languages mostly pearl has shown
that most memory is actually occupied by
storing code for storing the Pearl
option for example and what pearl
already uses copy-on-write to save
memory and it works like this it loves
as many perl modules in the apache
parent process as possible then when
apache forks child processes the memory
occupied by those pre-loaded perl
modules will be shared among all apache
worker processes and i'm without can we
do the same thing with rails well it
really depends on how much memory in a
real supplication is occupied by
actually storing realz code so let's see
a hello world in Ruby it's about half a
megabyte memory so it's RSS is 15
megabytes and about 1 megabyte of that
is shared memory so we end up with about
half a megabyte of actual unshared
memory and if we load in the real sly
berries that's then we end up with a
process that it's about 25 megabytes and
that's already with shared memory taken
into account so we had already measured
at a hello world rails application it's
about 25 to 30 megabytes so it seems
plausible that we can use copying right
to
to save memory well but there's a
problem because unfortunately Ruby's
garbage collector it's not quite copying
right friendly and every time that Ruby
runs its garbage collector then all
memory pages will be written to and that
causes copyin right so if you take a
look at Ruby's garbage collector it's
actually a symbol marking sweep system
and here's an example suppose that you
create a full object and then you refer
to it from a local variable which is
part of the route said and then you
create bar object and you change the
reference to bar then when the garbage
collector is info the garbage collector
will follow all pointers that's
reachable from route set and then it
marks all objects that it encounters
while doing this and this marking is
done by setting the FL mark flag in on a
bit field inside the object and then
next comes the sweep face of garbage
collection and in this phase garbage
collector will 30 objects that are not
marked because apparently you cannot
reach them anywhere from within the
program and the thing is setting this FL
mark thing inside the object it will
actually make the entire memory page of
that object dirty because it writes to
that page and everything that's nearby
is affected too so the rubiaceae knows
they are also garbage collected and so
they too will be copied when you run the
garbage collection and the fix to make
sure that this doesn't happen is to move
to marking data away from the bit field
inside objects and to a separate memory
region like marking table this sounds
easy but actually trickier than one
might think and if we go back to the
example with the processes then you see
for example a parent process and child
process and they refer to some bar data
in this top bar there could be real AST
notes then whenever the garbage
collector runs the garbage collector
will mark all those is you know
and this causes copy and right so you
end up with a copy of bharatiya data
that's actually identical you don't
really need to create this copy just
wastes memory and it's all because of
this FL marketing so when trying to make
the garbage collector copyright friendly
we encounter some Catholics for example
like how to measure the dirty memory
pages in the first place because on
Linux we have sets prox that self such
as maps and this is the fruit of out
made by the kernel allows us to inspect
a process a total private learning
memory but there are no tools see which
individual pages are dirty and other
operating systems are even worse because
they don't even seem to allow for you in
the total private the remember usage or
of processes so reducing the dirty pages
involves a lot of guesswork and we use
the following test script to measure the
effectiveness of copy and write because
what this has script does is it first
loads reals and then after loading rails
it runs the garbage collector so that
whatever garbage that was created during
all this time and that garbage is not
freed in the child and then it Forks the
child process and child process it runs
the garbage collector again to see
whether the garbage collector is
copyright friendly and then it measures
the process private dirty RSS so during
our first attempt at making garbage
collector copyrighted friendly we did
not know how much effect it was going to
have we initially use a hash table to
implement mark table and everything that
is in this table is considered marked
luckily Ruby had a built-in shading hash
table implementations though we do not
have to write our own and talk about to
make it all work but we eventually
succeeded and our surprise it not only
saved about fifteen to twenty percent
memory in the child process so we were a
little bit of disappointed and the
garbage collector also became about
thirty to forty percent slower
so we try to optimize this thing and it
turns out that mark table itself is very
large and we did not expect this when
real slow that are about 150,000 objects
even after garbage collection so each
hash table entry occupies about four
words and on x86 this is about 16 bytes
and if you count in Malik overhead to
that's advice then you end up with 24
bytes per entry and multiply multiply
that by the number of objects that we
have then we get about three and a half
megabytes and the hash tables bucket it
allows a depth of 5 before resizing so
if we count in that over at two then we
end up with about this number 307
megabytes and that's just to mark the
objects and what's more this 327
megabytes consists of many small objects
so this oldest memory is actually not
returned to the operating system when
you freedom it's not owned by Malik so
how the result of all this is whenever
you run the garbage collector it
generates 327 megabytes of 30 pages and
then we realize yeah we did not really
need a full hash table because we just
want to know what an object is mark so
we just need a set and a hash table
entry consists of these members you have
hash key record next and we can't get
rid of record because we were only
mapping object addresses to true and
hash is only used to speed up hash table
resizing so we can get rid of that too
in return for making resizing a little
bit slower and then this new data
structure we call it pointer set an
entry is not only 16 bytes on x86 and
that's including malik overhead so if
you multiply that by number of objects
then and count in the bucket over at two
then we end up with about two dot 4
megabytes of memory so the hash table
I'm sorry I mean the the garbage
collector now only used to that four
megabytes of men
Martin table and the garbage collection
speed the not change but the copyright
efficiency went up to about thirty
percent so this is definitely an
improvement but three percent is still a
bit not very good so we try to optimize
this even further and if you recall that
all the set entries are allocated with
Malik and we don't that are there's a
lot of them then we thought well maybe
you can optimize this by using a memory
pool by using memory pool we not only
get rid of the malach space overhead but
it also allows fast allocation because
each entry in the poll has a constant
size and this allows our pool to use a
simpler algorithm than Malick does and
the pool is allocated with a map so that
old memory can be released back to the
OS and the results were pre encouraging
because copy-on-write savings went up to
about forty percent and we even got a
15-percent performance improvement so
this is really nice but yeah we were not
satisfied yet because we thought yeah
this can be better and then we thought
maybe we can save even more memory by
not using a set but actually a bit field
as Mark table so if you look at how Ruby
objects are stored in memory Ruby
objects they are allocated on so called
Ruby heaps and Ruby heaps are not the
same as the system he that Malik uses
but Ruby hips are themselves allocated
on the system here so a ruby he consists
of multiple equally sized slots and each
slot is capable of storing a single Ruby
object and we could add a bit field at
the beginning of each Ruby heap and this
bit field is then used as mark table so
this bit field would have the same
number of bits as the number of slots in
the Ruby hip and a one in the midfield
it just means that a particular object
at that location in that hip is marked
or not so not all slots are occupied by
objects there were about
250 thousand objects in our test script
after loading rails and after running
the garbage collector and but even so
all together the Whitfield's only
consume thirty one megabyte sorry 31
kilobytes of memory and that's a far far
cry from all previous attempts with this
copyright savings went up to seventy
percent and garbage collector
performance also improved by sixty
percent compared to previous attempt so
this is a lot better and the seventy
percent savings that we saw in our test
script is a theoretical maximum because
real life applications they usually
create a love garbage for their own
stuff so the savings and practice are a
little less spectacular at this point we
were able to save about fifteen to
twenty percent memory in real life
applications on average against a five
percent overall performance hit but we
can do even better because there's still
some dirty pages left but where are they
they're they're very hard to find and
after a lot of pulling hair out of our
heads and stuff like that it turned out
that the G lipsy memory allocator was
partially to blame that's PT Malik too
because if you take a look at this C
code it allocates one kilobyte of memory
a real child process initially has a
private very RSS of 125 kilobytes but
after executing this code then the
private early RSS suddenly jumps up to 7
megabytes something is definitely wrong
here and we suspect that PT Malik to
makes a lot of internal bookkeeping
structures dirty and that's what causing
all these 30 pages so we research other
memory applicators that we could use we
tried net backlog first but we couldn't
get it to work then we tried J email
office is the memory allocator use by
FreeBSD and Firefox but this did not
seem to reduce the memory page at all so
eventually we settled for Google's TC
Malik and with this practical copyright
savings went up to thirty three percent
and
overall performance it went up with
about twenty percent so it's even faster
than normal Ruby and we concluded that
TC Malik is faster than PT Malik to for
our workloads the garbage collector is
actually still slower than normal Ruby
but because of the alligator we still
have an overall positive performance
difference and the thirty-three percent
memory savings in practice it's
confirmed by other parties as well for
example Shopify and e-commerce shopping
cart surface they testified that they
saved a lot of hardware resources with
this and we've also integrated this
copyin right technology with Fusion
passenger a ruby web application
deployment software if you use fusion
passenger with Ruby Enterprise Edition
then it will automatically use the
copyright savings so to sum up by using
a bit field as a mark table for the
garbage collector and by using TC malach
as memory allocator we were not only
able to save thirty-three percent of
memory on average in rails apps but we
were also able to make Ruby about twenty
percent faster and this concludes our
part about the garbage collector next up
is ning about threading improvement so
next up I'd like to talk to you guys a
little bit bit about improving threading
performance and this is actually a patch
that was contributed by joe d'amato
naman gupta where the office of event
machine which is a io library mainly
used in ruby and this is actually a good
example we believe from a patch or
contribution and that allows us to you
know optimize MRI for server
environments so first off let's go over
a bit on you know the threading model
ruby 18 so as you can see this is a
typical green thread setup where you
have any user space threats all mapping
to one kernel threat and this has some
advantages and as well as disadvantages
one of the biggest disadvantage of this
in his multi-core era is probably that
the you know the userspace threads can't
sim can't util
I symmetric multiprocessing will be one
line however uses kernel threads but
unfortunately you have a global
interpreter lock which still prevents
you from using multiple cores in you
know implementations such as Mac Ruby
however have been able to remove the
global interpreter lock allowing for you
know true utilization of the hardware
and if you want to know more about that
you should probably speak to the Hall
Cincinnati is sitting over there you can
tell you probably a lot more about this
than I can so as for the scheduling the
scheduling is is a pre-emptive scheduler
which basically means that every user
space threats threat gets a certain
amount of time to complete stats before
it gets preempted and another threat
will be scheduled in its place and this
is done either through an eye timer a
signal or through a timer threat it
depends on the platform that you're
probably using another important thing
about the Ruby 18 scheduler is that you
can apply explicit yuling so basically
you can do is if one user space spread
is executing you can actually explicitly
you your execution so you can force a
context switch by using threat but pass
so because we're using user space that's
how however if we do a blocking call in
one of these user space space threats we
are actually blocking the entire no
colonel threat as well and basically
blocking all the use of space threats
here as well and we need to solve this
in particular for i 0 but we'll get
there in a few seconds actually so to
illustrate this if you have a blocking
call for example on the left hand side
threat over there then all the other
user space threats will have to wait
until that blocking call finishes and
this may may or may not be forever so
basically you're blocking the whole
system so this is important for i 0 2 to
fix for i 0 actually because you know
you can have a certain you know read
system call which has to wait for
example for data to come in and Ruby
solve the salsas by using non-blocking
i/o and the way it does that is when it
detects a blocking operation for example
if you've you're reading from file
descriptor where there's no data to be
read from then basically we'll just
schedule another threat
execution in its place so like I said
there are some advantages over using
user space threats over colonel threats
it's a good way off that you need to
make and some of the advantage in this
scenario are actually that you know
they're fast and they're cheap to spawn
because everything happens in user space
so there's no colonel involvement at all
this also means that you can shut them
down as fast as you want and basically
you can you know you have a granular
control over the context which sure as
well because you have to implement this
thing yourself and well you can make it
as fast or as slow as you want basically
but this is not particularly the case
for Ruby unfortunately and this is what
the guys from event machine found out
when they worked on improving this
because event machine allocates a some
data on the stack to read and write from
and they noticed that you know the
larger the stack became the the harder
it became to contacts which actually the
conflicts which actually became slower
and you know as a analogous example I've
made I put it up here a/c function and
that basically should illustrate what
the problem was that they encountered so
first off you see here that we allocate
50 kilobytes of data on the stack and we
use memset here just 20 fill it so that
GCC doesn't perform any optimization
such as removing this piece of data
because it's unreferenced in later code
basically what this the c function does
is basically just allocating 50
kilobytes on the stack and invoking the
block that you give it as you can see
here so if we were to try to invoke the
sea extension function inside ruby in
this scenario with threads then we see
that here we allocate 50 kilobytes on
the stack first we do some silly cal
place for two hundred thousand times and
after each iteration we explicitly force
a context switch then joining all these
threats takes about 13 seconds now the
interesting part comes in in play when
we actually remove the invocation of the
sea extension method thus actually
removing the execution of or actually
the allocation of 50 kilobytes on the
stack then suddenly the execution time
drops down to 4.2 seconds and this is
actually a weird you know correlation
going on here
definitely something going on with
regards to allocating stuff on the stack
and what for and what consequences it
has on context switching so profiling
the results in an event machine scenario
with threats which is very similar to
the example that we just showed you with
Google proof tools you get something
like this and here we've ordered it in
order of you know frequency of
invocation so first off you see the Ruby
time slice handler nothing weird going
on there at some later point apparently
they're doing a hash table lookup then
Ruby scheduler and at some other later
point you get to Ruby AST interpreter
this is not weird at all however this
guy over here is because apparently at
some particular point it needs to invoke
mem copy and you know in order to
understand what it's using it for we
better take a look at you know what
happens when a context which takes place
so basically when a context which takes
place it needs to store the state of the
current threat basically so what it does
first is safe since cpu registers with a
set jump then apparently it saves the
stack frames to the heap so basically
it's taking the stack frames from the
sea stack and it's copying it actually
to the heap that's kind of a WTF but
we'll get to that a little later at some
later point it will save some vm
Global's and you know for restoring it's
pretty much doing the reverse of this so
it's restoring the vm Global's it's
restoring the stack frames using them
copy by the way and then it's restoring
the cpu registers and this has some
implications actually because it's using
them copy to copy the stack data from
from and to the heap and basically what
this means is if you have a larger sea
stack and a context which will take
longer so that's actually the symptom
that we saw over there when we had this
50 kilobytes stack then you saw that it
drastically increased the context which
time now Ruby 19 is unaffected by this
problem because it uses native threats
but you know Ruby 18 is still the most
widely deployed version at this time so
we wanted to solve this so in order to
understand how you know how this can be
solved you need to see how Ruby uses the
stack and an important element in that
is to know that the Ruby stack frame is
basically just see stack frame and
sharing this
and in particular you can determine what
the stack frame size of this thing is by
for example using gdb and subtracting
the base pointer from the stack pointer
that gives you the frame size every
function call as a result will put
roughly around one kilobytes on stack so
if you have some Ruby code on the left
hand side it will allocate one kilobyte
for each of these implications on the
stack and it's important to fix this we
believe because the typical real stack
trace looks kind of like this and it's
pretty much 65 levels deep basically so
if we can solve this problem then for
future versions that may use reading for
example in side rails and you know as
you can see realz uses the stack
extensively then context switching
should be much faster this has some
unexpected consequences during context
which is as well because basically what
this means because you know the stack
contents can change ad hoc during
runtime this means that it's very unsafe
actually to refer to a value on the
stack from a native threat because you
know at a context which the context can
or actually the the stack can be changed
so you can see that here for example if
we have a value on stack here in this
function and we work to pass it pass its
address on to a threat function and
spawn the threat then it would not be
very safe to refer to that value because
you know when a context which takes
place its contents could change
basically so the fix for this is of
course to instead of copying the current
stack from in 2d heat like so we can
just not copy that and just change the
current stack pointer so upon a context
which you just basically change the
stack pointer to the stack that you want
to use to the heat but this requires
some platform specific code and before
we dive into that it's good to do a
little refresher course here because on
x86 64 you have to remember that the
heap grows film grows upward so from low
address numbers to high address numbers
and the stack however grows from hiatus
numbers to low address numbers so as you
can see the heap close upwards Resta
stack grows downwards so if we were to
allocate some memory
the heat then actually the result using
other molecule memory map will give us a
pointer to that address and that address
is actually the stack top just a small
implementation detail so to solve this
for example for x86 64 you can use the
following inline assembly that GCC
supports so as you can see below so the
last two lines are actually the input
operant list and we're referring to the
input operat list in the assembly string
in a tokenized manner such as you're
familiar with using print out for
example so % 0 will refer to that value
and % 1 will refer to the alpha value so
what this code does basically is it's
moving the stack pointer to the value of
sdk base it's kind of them will be able
to discuss a little bit about that later
if you have some questions that and
eventually will invoke a it will invoke
the function actually RB threats start
at the address of RB let's start with
this new stack so there are some cabbage
do this actually because native stacks
grow automatically our operating system
usually takes care of this so you know
it grows as it needs to our stack
doesn't however because we're allocating
this on the heap and we need to manage
this ourselves so this raises a question
like how large must are threats tax be
also how do we allocate these threats X
do we use memory map or do you use
malloc and lastly how do we handle stack
overflow because it's very easy to fall
off your stack for example and you know
all about things could happen basically
so this led to some decisions and some
special cases first off it was decided
upon that the Ruby flex threat stack
size with default to one megabyte which
is about thousand function call bev deep
which we believe is adequate for most
use cases the size for this is
configurable during runtime for advanced
users if they should need to do that
also the decision has been made to use
memory map instead of malik so that we
don't have to you know don't have to
incur the Mallik overhead that only just
talked about and so that the memory is
guaranteed to be released back to the
operating system as well as for stack
overflows we fix this problem by putting
a guard page actually with
cotton long at the end of the stack to
catch potential overflow so if you try
to read or write from this guard page
then you know you will get a
segmentation fault or as a result of
that as well as actually that the signal
handler must actually run on a separate
stack because you know signal handler is
just a function and can also reference
the stack and it could also reference
that problem on page basically that you
know caused the signal handler to fire
in the first place so you need something
like sick old stack to prevent that so
in terms of benchmarks to see what the
results of this is we can use the 80th
threat ring test and instead of you know
trying to explain to you guys what this
boring code does it's probably better to
do this through a small animation I love
keynote by the way so what this
benchmark basically does it initializes
a number two for example 50 million it
then spawns 403 threats and it then runs
all these threats sequentially in such a
way that will subtract minus or actually
will subtract one from number and it
will continue to do this until at some
particular point number will be zero and
the next threat to be spun up will
notice this for example if this were to
be like for example threat number 13
then it will print out this number to
the console and it will exit so the
results of this benchmark er you know
pre self-explanatory as you can see Ruby
18 the original version takes about 1400
seconds to execute this reg Ruby 19
takes about half of this the patched
version however Ruby 18 is very similar
excellent to the Ruby 19 version and so
we can infer that there's a delta about
to time 2.3 times faster than going on
so the things are definitely looking to
shape up here but to truly understand
how this improves the threading
performance or actually the scheduling
we could spice things a little bit up by
introducing this function which just
recursively calls itself for about a
hundred times and yield the block that
you give to it and as we already
discussed earlier on every method
invocation that you do in Ruby and
locates about a kilobyte on the sea
stack so basically what this code does
it will grow the stack by about 100
kilobytes so we were when we rerun the
test sorry with this with this function
that we just introduced and then you can
see that Ruby
eight will now take two hours to execute
or actually to finish this code whereas
Ruby 19 will take 12 minutes and Ruby 18
the patch version will still similar to
Ruby 19 actually so it's 13 minutes as
you can see from from this example the
delta now is nine point four times so
you know it's it's definitely a cool
patch we believe so as for threading in
its current situation Ruby 18 in a
particular in Ruby Enterprise station
the patch that you know Joe and I'm on
have contributed is currently in Ruby
Enterprise Edition and it's available
for you now and it's currently only
available on x86 x86 64 but you know
other platforms could be supported
provided you give the assemblies and the
proper stack heap growth calculus thingy
so basically in conclusion Ruby
Enterprise Edition is not a fork but
it's a branch so regular merging does
occur with upstream we have contrary to
Ruby proper we have a more liberal patch
acceptance II policy so if you guys have
a cool patch that you know could
contribute to Ruby in a server
environment that we'd be more than happy
to take it into consideration and
eventually of course at one particular
point we do really hope that these
patches will find their way back to
upstream but until that point where
we're maintaining Ruby Enterprise
station so there are some other
interesting Ruby Enterprise Edition
patches as well by the community realz
bank for example and Jesus statistics
and M barb patches in particular which
improved the conservative garbage
collector of ruby ruby has a
conservative garbage collector which
scans the entire stack to determine
whether or not a value on that stack is
a pointer to something on the Ruby heap
the embargo patches actually optimize
finding correct pointers to do read and
lastly we have a nice one from a few
hungry GU colorful threats which will
definitely help you when debugging so
that's basically it and if you have any
questions we'd be more than happy to try
and answer them
what is pride nobody's going to ask if
we're going to have beers afterwards
because okay well that's basically it
thank you for listening and we hope you
enjoyed it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>