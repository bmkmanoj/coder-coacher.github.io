<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2016 - Day 1 Keynote | Coder Coacher - Coaching Coders</title><meta content="GTAC 2016 - Day 1 Keynote - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2016 - Day 1 Keynote</b></h2><h5 class="post__date">2016-12-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Vf7axkwtTOw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">&amp;gt;&amp;gt;Manasi Joshi: All right.
Thank you so much, Matt.
I'm Manasi Joshi.
A very good morning to all of you.
I want to talk about GTAC.
2016 is a very special year, as Matt pointed
out.
We are celebrating tenth anniversary of GTAC
conference.
So a very happy birthday to all of GTAC organizers.
Ten years ago when we first started this conference
in 2006, it was in London office, as Matt
mentioned, and at that time, the main motivation
behind this conference was, hey, back then,
there was no technical conference that focused
on software testing.
Test organization at Google, especially the
European regions of Google offices, was really
new.
So both in London and Zurich offices we had
just started hiring software engineers to
kind of start focusing on software testing
and verification, and we just wanted to establish
a dialogue with community outside of Google
that was also focused on those interesting
areas.
So that was a main motivation to kind of share
ideas and learn from each other and really
provide first-ever technical platform for
Google's testing, sort of -- for test automation
conference; right?
And since then, it was kind of, I think, attended
by about 20 to 30 different attendees, and
look at us now.
There are ten times more attendees now in
this room and some overflow rooms as well,
in person.
And this event is being live-streamed, so
we know there are even more number of people
looking at all the speeches that are going
to happen and learn from it.
There has been a phenomenal growth in terms
of the kinds of areas that we are exploring
in the last ten years.
So I believe, like, ten years ago, some of
the topics that they covered in the conference
were related to things like, hey, how do we
really do software testing in any business?
And how do we really use open source tools
that are out there for doing performance testing?
The WebDriver, the Selenium were new and shiny
things.
And now we are looking at a wide area of topics
in this conference.
So I really hope you will make full use of
today and tomorrow to kind of benefit from
all the topics that are being discussed here.
Just to give you a few examples of things
that will be talked about today and tomorrow,
we are going to deal with, like, big challenges
in finding bugs in C++ code using advance
techniques like fuzzing.
We're also going to sort of talk about how
to use machine learning to set up your mobile
test environment.
How to use statistics to really figure out
what tests are important for your project,
what test can be skipped.
We're going to talk about, like, in a big
setup of continuous integrations and kind
of continuously running environments, how
to deal with flaky tests, because guess what?
We all have to deal with flaky tests at some
point.
So there is going to be a wide variety of
very interesting topics.
And again, I encourage you guys to kind of
stick through and sort of take advantage of
all those topics.
I also want to make sure that I mention this.
That for the very first time, 2016 marks as
a special year for me personally because the
organizers of this conference have awarded
14 very special and deserving candidates diversity
scholarships.
So I want to extend my heartiest congratulations
to all the scholarship award winners.
And with that, I want to give a big shout-out
to all the GTAC 2016 organizers, all the presenters
in this conference, and all of you as attendees
as well.
So let's give a big round of applause for
everyone.
[ Applause ]
All right.
So now, again, I'm going to talk to you about
-- I'm Manasi Joshi and I'm going to talk
to you about evolution of business and engineering
productivity function at Google through my
experience at Google of course.
And what I present today may or may not be
directly relevant to all the areas that you
guys are working with; right?
But I really hope, and I sincerely hope that
you kind of walk away from this speech or
this conference in general and learn at least
a little bit that you can apply to your work.
If nothing else, just sit back, relax, and
let me share my experience with you all.
So before I go into the topic of today's discussion,
let me kind of give you a little bit of background
about myself.
I joined Google in 2005.
Back then, the idea behind hiring software
engineers not to do feature development but
actually develop tools and infrastructure
that focused on software testing and verification
was really new to Google, and brand-new to
me.
Google is my first job, right out of grad
school.
I remember in 2004, I was visiting my fiance,
now my husband, in California, and a dear
friend of mine invited me over for lunch.
We take food very seriously in Google, by
the way.
So I kind of gladly accepted his invitation
and came over in campus at the Mountain View
campus of Google, and what I saw there was
just amazing.
I mean beyond the delicious food that they
served, there were so many fun things happening.
There was a volleyball court and a swimming
pool and a whole lot of really awesome things
that I kind of started thinking, seriously,
like can people work here when there's so
much distraction?
But I want to be part of this; right?
Now, before my visit to Google campus, my
idea about working in an office in industry
was really, really different.
As a small girl, I remember going with my
dad into their factory setting where they
made boilers.
My dad is a mechanical engineer.
And what I saw there was there was a lot of
hot air and steam and people in good clothes,
clean clothes, but with dirty hands because
they were actually working inside the boiler
and around it; right?
And I knew that that's not what I wanted to
do.
Later on, my other experience working in an
industry setting was through my internship,
working at a medical devices company for about,
like, few, four, five months.
And even then my office was, like, full of
interns like me without any windows and people
with a really serious look on their face.
And I said, okay, this is what it means, like,
working.
But then compared to all of that, what I saw
at Google was just phenomenally different
and I immediately applied for the job.
Thank God I got it, and here I am almost 12
years after I came here and still very excited
and happy to come into work every single day,
and trying to explore new ways of testing
software and making my team grow and scale
and really -- I'm going to tell you a little
bit about my team before I kind of explain
what I do.
So I joined Google in 2005 in a team that
was kind of developing products for users,
and users were internal Googlers.
It was not an outward-facing product.
In 2007, I joined Google's ads business, and
since then, I'm part of this ads business.
Internally we call it display ads organization.
Externally, this is also known as AdSense.
So to just simply define what AdSense is,
when you serve the Internet and you see the
ads on the Internet where it says ads by Google,
that's called the AdSense business where Google
is responsible for literally funding the Internet
by showing those ads on different publisher
websites.
And internally, in display ads organization,
of which I'm part of, we kind of are responsible
for showing the right ad to the right user
at the right time.
So that's kind of a little bit about me.
And throughout this presentation, I'm going
to give you, like, information and anecdotal
kind of evidence based on my experience, generally
at Google but, more importantly, from the
ads world.
So here we go.
Basically, any business that grows has -- and
definitely this is applicable to Google, has
multiple dimensions of growth.
Now, what are all these dimensions?
If your business is -- relates to users, then
you can have tremendous amount of users that
grow.
Your revenue model changes.
The complexity added to your business changes
over time, too.
And all of these changes can happen in an
exponential fashion for which you're not really
prepared A at any given time.
So there's a lot of axes of growth, as I mentioned
here.
I want to draw an analogy with a model of
how a small town grows into a bigger city
and how that grows into a future city.
In a small-town setting of your business where
you have few houses and some roads that connect
the houses together, you have sort of very
minimal amount of sharing of information happening,
especially if the houses are far apart; right?
Like you really have to make a lot of effort
to go from one place to the other.
There might be other modes of, like, simple
buses or other transportation options out
there, but there is not a lot that's going
on.
For people to grow in such a town, sort of
business model, the kind of infrastructure
that's available is just not scalable.
And that's where, when people want to grow
and sort of start exploring and venturing
into new kinds of businesses, business models,
you have to also, like, have the evolution
of that small town into a big city.
In a big city setting, you have ton more businesses,
you have lots of construction going on, you
have sort of trains and airplanes and ships
and beyond roads.
And all of this basically amounts to the complexity
that comes into picture.
But it also amounts to various ways by which
people can share data with each other; right?
And then you kind of realize that, hey, but
in order to share that data I still am stuck
with the device on which I'm producing that
data, for example.
And that needs to change.
Wherever I go, my data should just follow
me.
In other words, my data should be universally
available and accessible to me without any
issues of connectivity.
And that's where you kind of start thinking
about, yes, I need to be in a world where
everything is connected and all the data is
just available at my fingertips, wherever
I go, without any issues.
And by the way, in the future city model,
when everybody is producing tons of information,
it's not just important that we process and
consume that information, but we need to use
advanced techniques like machine learning
or other kind of algorithms to derive even
more concepts from that data that everybody
is producing.
And then from the information that we derive,
we need to take informed decisions in order
to kind of venture into new possibilities
and tackle, like, new challenges.
So that's kind of the model I'm going to sort
of like follow throughout my slides moving
forward.
So what was it in Google in 2005-2007, around
the time when I joined?
Especially in the ads world -- again, my world
of ads -- the main things that we focused
on as a brand was test engineering.
Externally we were known as Google's test
engineering or Google's quality assurance.
At least that's what we hired for.
And internally, we kind of just said, like,
okay, what we'll focus on is unit tests and
maybe a little bit of system tests here and
there, but really speaking, we just wanted
to be very sure that all the features getting
built into our ad stack was -- were, like,
really fine and correct.
And that was the complete scope of our testing.
There were about 50 or so engineers focused
on testing and about 500 or so engineers focused
on feature development, and most of those
changes that were happening were around front-end.
So a lot of our testing in the world of ads,
at least, was really, really manual, and whatever
was automated was in the form of unit tests
and a little bit of system testing.
At the same time, Google was in the process
of acquiring a lot of, like, talented ads
businesses and growing quite a lot.
We were really feature hungry.
We wanted to develop more and more features
on a daily basis.
All the feature developers were.
And we knew this model of few tests and a
lot of manual testing was not going to scale.
We had to introduce automation, and we started
evolving our systems in order to kind of include
that automation.
Now, this is a cliche slide of test pyramid.
Some of you may think that, and yet I have
added it to my deck.
There is a reason behind that.
The main reason is maybe most of you already
know of a test pyramid.
By the way, for those who do not know, maybe
some of you, like me, like how I was, right,
like, 11 years ago, I didn't know what this
meant.
And maybe some of you who are new to the industry
who are is attending, like, sort of test conference
for the first time, I just wanted us to be
all well aligned and kind of have common language
and understanding moving forward.
So I added this slide which kind of says what
should be a typical model of test engagement
for a business.
And this again with a caveat, this may not
be the best model for your business.
But on an average, this generally works.
What this test pyramid says is that there
should be maximum investment done, and in
terms of sheer numbers, there should be lots
and lots of unit tests available for your
stack.
If you think of, like, your stack divided
into different components, then unit tests
are nothing but correctness testing of the
individual functions that you define inside
your component to make sure that that function
is behaving.
At the higher level than unit tests, you have
the component level test where your components
are exposing some sort of interface for external
components to talk to each other; right?
And that interface is essentially just a contract
that says that, hey, this is my request interface,
this is my response interface.
And you want to test the component at that
level which will take care of executing correct
number of functions, depending on what the
control flow is taken, and that becomes your
component-level tests.
Or, in other words, regression test to kind
of figure out that, like, hey, with new features
added to the component, new functions getting
adding, we are not breaking existing functionality
at all.
And one level higher than that is the integration,
where you can imagine a system where there
is not a single component that's out there.
There are multiple components that need to
now talk to each other in order to achieve
the greater good, right, that your product
is responsible for.
Depending on how complicated your production
setup is, your integration test can be extremely
challenging and resource-intensive.
To give you some idea and quote not direct
numbers, but to give you some idea from my
ads world again, we have an integration test
in display ads organization that has grown
just phenomenally in the last few years that
it's been around.
It -- Today, it runs thousands of tests, which
is more than 158% of number of tests that
it ran just two years ago.
It uses hundreds of gigabytes of RAM, which
talks about the footprint that this particular
test infrastructure has, which is more than
158% of the RAM that it used, like, two years
ago.
This test is used by thousands of developers
who are 30-day active users for this test.
This test is also invoked on every single
change that gets submitted into our repository
for all relevant code paths, which amounts
to about thousands of sort of what we call
presubmit runs, presubmit meaning before you
check in your change into the versioning system,
you want to make sure that that change works.
So that's kind of the presubmit model that
we have.
And this test gets invoked through, like,
thousands of such runs.
So it's a very fundamental, very, very critical
test.
And that, literally, reflects how complicated
our production system has become, with hundreds
of binaries that we test as part of this test.
But above all of this, what I really have
highlighted here is the top portion.
And the top portion is interesting, because
not many people think of it as important or
relevant.
Again, I have mentioned this might be a caveat,
this entire model might not even fit your
business.
But generally speaking, what those types of
tests are, are logs tests, which basically
means that, hey, if you're processing some
requests as part of your system, maybe the
requests coming from users or other components
of your system, then every system is going
to, like, sort of spew out some logs; right?
And that's going to basically tell you the
ground truth, what happened when a request
got processed.
And I'm telling you that there is lots and
lots of intelligence in these logs, depending
on what gets logged, obviously.
But you can leverage these logs and test the
ground truth about your system.
In other words, things like what we internally
call as system invariants, how a system should
behave, if you look at the system as a black
box, completely can be something that you
can apply into testing the logs of the system
and make sure that it works.
To give you an example, again, from ads world,
we do keep a track of how many ads were shown
on any given page; right?
And we do have -- track record of, hey, our
users are telling us that, hey, I'm interested
in sports and I'm interested in hiking and
sort of -- some sort of health issues and
whatever it is.
And we kind of keep track of that, and we
try to target ads related to users' interest.
So all of this information gets logged into
our logs, and we basically have lots of tests
around logs.
Privacy for us is really, really critical.
In the world of ads and other parts of Google
as well, we do a lot for the users, and we
keep a lot of the users that we know.
We have a lot of user information.
And we are stewards of data.
In other words, we have to handle that information
-- both users' privacy, as well as the data
privacy -- very, very seriously.
Internally, we have lots and lots of privacy
compliance rules that we abide by.
And that's what basically makes Google trustworthy
for all of you as users of Google.
So we have to also take into account privacy
in our test sort of strategy.
Definitely in the world of ads.
And then all of this kind of testing is happening
in development work flow.
But that's not sufficient, because as a feature
developer, I can't just, like, write code
for the feature, run all these fancy tests,
frameworks that my Eng Prod team is sort of
building and then assume that that feature
is going to reach my users.
No.
There is a big step of releasing that feature
into production; right?
And that release step basically happens and
should be sort of supplemented with what we
call internally as canary testing.
So canary testing means that during the release,
let's try to expose that feature to make sure
that the feature was not built only for audience
in the U.S., for example; right?
Especially if your product is global, such
as Google, such as the ads business of Google.
Now, from bottom to top, all of this kind
of tests requires higher amount of setup cost.
It requires tremendous amount of time and
investment as you start developing, like,
the stack of tests in this fashion.
It requires high number of machine resources
as well.
So you have to account for that.
And, by the way, it requires high level of
domain knowledge.
So don't forget that.
Now, by doing this, you have to achieve a
very fine balance.
And it's very hard.
This picture, as you can see, I'm very, very
inspired by my four-and-a-half-year old son.
And one day, while I was thinking about GTAC
and what I'm going to talk about, when he
was playing with his cars, he kind of stacked
them up.
And I thought, yes, that's it!
We need these four aspects about all of the
tools and frameworks as engineering productivity
engineers that we are to kind of deal with.
A, our tools have to be extremely useful.
We have to do something; right?
There has to be a utility to all the tools
and frameworks you're building.
B, our tools have to sustain, they have to
be robust enough in edge cases and extreme
conditions.
For example, if you are working in a database
area, your tool cannot break because of concurrency
issues.
C, you have to build the tool so that you
are aiding the feature develops in their velocity;
right?
You are making their life smooth in terms
of the feature development, the development
experience, the releases, so on and so forth.
So there's a velocity aspect there.
And D, all of this kind of testing, basically,
quality has to come out of all of this testing
only when you have quick and frequent and
smooth releases.
So there is a really fine balance that you
need to achieve across these four dimensions
for all the tools and frameworks that you
are building.
And that can be very challenging, because
all these four factors cannot work against
each other.
They have to work together.
So now, with all of that, I just wanted to
summarize, what was our test strategy 1.0
in, like, the first, like, sort of era, 2005,
2007?
We mostly kind of focused on before submit
and after submit, kind of defect ratios, for
some definition of defect.
And by the way, we wanted it to be 99% defect
free.
And most of our testing focus was still pretty
much like few unit tests and system-level
kind of most tests and whatnot.
Right?
But what was happening with that was, yes,
it was attractive to do it this way, because
we got done -- like, we basically solved the
problem at hand.
We didn't have any dependencies, because it
was very quick iterations of those unit tests
and smoke tests and whatnot.
But there was hardly any sharing.
There was hardly any concept of adaptability
of our tools in relevant, applicable, like,
sort of situations.
In other words, we ended up with a lot of
and a lot of, like, duplicated code throughout
the system.
As I mentioned, Google was -- the ads business,
at least, was kind of a hungry caterpillar,
where we wanted to do lots and lots of feature
development on a daily basis.
And so our entire investment model was, hey,
let's make sure that the features kind of
get tested well.
But who cares about, like, whether -- we did
not care, I should say.
We did not care so much about whether those
features were reaching our users on time or
not and how smooth that release process was
at all.
So there was a lot of problems with how we
released at that time.
In terms of our release strategy, then, for
1.0, what we ended up having was really, really
long cycles of releases.
We kind of released once or maybe twice a
month at best.
And a lot of those releases that we did, because
of the -- because of, like, sort of lack of
any kind of advanced techniques of tests during
development process, we had to rely on a lot
of testing during releases as a result, just
to make sure that things were working fine.
And what that did was, basically, we tested
in production most of the times.
Again, this is something I want to spend a
little bit of time, because some of you may
be of the camp that thinks that, why is this
such a bad idea to test in production?
Of course, it's attractive; right?
Especially when your systems are becoming
-- like, have become so complicated that you
just cannot replicate that setup in your test
environment.
You have to -- you have no other option but
just to test in production.
Or when, like, there is constraints on you
based on, like, machine resources or just
the expertise of how to mimic your production
work flows in testing.
There is no option but you have to test in
production.
But think of -- This is generally very dangerous.
Think of all the downsides that it can have.
In the world of ads, for example, what if
I'm a feature developer who has a buggy code
which was never tested with my unit and system-level,
like, smoke test, but it was a bug that dealt
with advertiser budget, and I exhausted all
the budget after I showed the ad, like, just
a few times.
And instead, I charged the advertiser an exorbitant
amount of money.
Or worse still, if I'm a developer who's dealing
with some user data, I introduced some sort
of bug that ended up deleting that user data
without the user's consent and sort of knowledge;
right?
These are really horrible things that can
happen if I just rely on testing in production.
And some of them are not kind of -- they can
be irreversible damages.
The biggest damage this can cause is getting
ready to be sued.
Your brand is affected.
Generally speaking, testing in production
is a very, very bad idea.
And you have to change the mindset to invest
a lot in testing upstream.
And that's what we started thinking about
as we made the transition from 1.0 to 2.0.
Another thing I want to kind of just mention
is, one of the heads of release engineering
at Google kind of was sharing a story with
me and gave me this example.
And I really liked it, so I thought I would
share it with you.
In terms of how the releases happened then
was, remember my town model?
So the business -- when there was a small
town kind of a thing, in small town setting?
So imagine your releases were nothing but,
like, bus service from point A to point B,
where those buses -- like, think of releases
as buses.
And those buses, let's say, carried a whole
bunch of passengers on them, but the buses
never left the station on time.
The probability that the buses will arrive
at the destination on time was very, very
low.
And oftentimes, the buses really encountered
some problem or (indiscernible), and they
had to be called back.
In other words, in reality, the releases that
we did were really long and bug give, and
we had to roll them back.
And we had to do more releases with bug fixes
and patches.
And that was generally not a very happy situation.
And because of, like, infrequency in releases,
we oftentimes in the bus model allowed late-arriving
passengers because who knew when the bus was
going to go; right?
And without really making sure whether that
was safe the bus could take that load or not.
In other words, we kind of entertained our
feature developers to say, hey, by the way,
if you are going to do the release, then can
you wait?
I'm just about to submit this change.
Literally, just give me two hours.
Right?
And we just kind of entertained a whole bunch
of these things happening.
This was really bad.
So we had to change it, and we changed it.
I'm very glad to say that.
Enter 2.0, where we transitioned from the
small town into the big city.
Where we are right now.
I want to sort of, like, call this out as
a period in Google and in ads world between
2012 and 2016 pretty much.
Some of the biggest changes that we did was
our brand.
Externally, we are no longer called as test
engineering.
We are called Engineering Productivity Group.
We are an organization who has invested a
lot of time and resources and the talent that
is now responsible for setting up systems
that are doing continuous testing, that are
deriving test coverage all the time.
And we as Engineering Productivity team are
sort of focused on writing tools that augment
these dedicated organizations who are improving
our releases or continuous deployment systems
and test clusters and so on and so forth.
And we are also very, very well aligned with
our products individually in different parts
of Google.
But with all of this, we have also made sure
to kind of focus a lot on metrics, which we
completely lacked in the first generation,
in the first kind of era period that I mentioned.
And I'll talk a little bit about that later
on.
Now, I want to quote a few numbers from my
world of display ads to kind of just give
you a flavor of what kind of growth we are
talking about here and what kind of evolution
was happening on display ad side from infrastructure
perspective.
I'm very proud to say that in this period,
we scaled a number of queries, the ad requests,
that we receive in our stack thousand times.
The number of developers who were developing
code sort of grew by ten times through hiring
as well as acquisitions, and they were responsible,
and they are, as of today, writing hundreds
of millions of lines of code.
That's the complexity we are dealing with.
In terms of our revenue, we are responsible
for a multi-billion business for Google, which
was a phenomenal thousand times growth, again,
since the beginning.
Now, again, with respect to ads products,
we are no longer catering to users that are
giving us query requests from desktop.
Our users have changed.
Our users have moved to different platforms.
So there are queries coming in from mobile
phones, and tablets, and Xboxes while playing
the games.
We show ads everywhere, and we show good-quality
ads.
Our feature set in terms of what kind of ad
formats we do has grown from what was just
contextually targeted ads, which was just
text ads, to more interactive ads which are
really, really useful.
And with all of this, we are taking into account
the first-party as well as third-party user
data.
In other words, the data the user is directly
sharing with us in terms of, hey, these are
my interests, what we kind of categorize them
as, like, user interest verticals internally.
And by the way, some of the information we
get as third party, from third-party websites,
from publishers with which we do business;
right?
So we take into account all of this information
and handle it in a privacy sensitive fashion.
And by the way, while we were growing in terms
of our business, engineering productivity
function for display ads also grew multi,
multi-folds.
What was an organization of, like, about 20
to 30 odd engineers is now an organization
more than 200 engineers.
We have grown in seniority and in our maturity
of how we look at systems and we are now tackling
information across that pyramid, across different
axes of the test strategy that we plan for
our ad products.
The other thing that I want to mention is,
in terms of, like, evolution of test strategy
that was happening, was essentially, as I
mentioned, going beyond unit tests and system
tests to tackle different parts of testing,
but also we are moving fast.
We are trying to be stable and achieve high
amount of test coverage, more than 70% on
an average, and releasing at a frequency of
about 90% of our releases in a week's time.
It is just phenomenal that we are aiding our
developers with the speed at which they can
go and do their development, along with keeping
the system sane and stable.
In the world of, like, ads, again, the user
trust -- I want to spend a little bit of time
there.
User trust matters a lot, and we take that
very, very seriously.
Instead of saying, hey, user data is too sensitive,
I'm not going to touch it in my test because,
yeah, I don't want to deal with user data;
right?
And basically just forget about disabling
code paths, and -- we used to be in the model,
I should say, early on, where we said user
data is too sensitive.
I don't want to be introducing it in my test
systems and, therefore, what we ended up doing
was turning off different code paths in production
systems while they were under test, because
those control paths really dealt with user
data.
But what ended up happening was what was responsible
for, like, a 5% revenue through that control
path now became responsible for more like
50% revenue, because we wanted to target users
with best-possible ads, and there had to be
user data used for that reason; right?
So in our tests, we also started introducing
concepts of user data in a privacy sensitive
way, and really invested a lot into writing
test infrastructures, developing tools and
frameworks where we could do a lot of analysis
of, hey, is there a data leakage happening
between Google and all the external parties
Google does business with?
Internally, are we classifying the data according
to what is considered as sort of privacy sensitive
sort of data classification rules, and so
on and so forth.
This is important because that's how we are
earning users' trust.
I'm very proud to say that Google is a partner
in Network Advertising Alliance and both Digital
Advertising Alliance as well.
Google is a partner where Google does give
users a lot of preference in terms of controls
as well as gives -- makes the user information
transparent.
So it matters, even in tests, that we kind
of make sure that things are working fine.
But with all of this, when we were growing
so much and had to develop lots of different
kinds of tools, what ended up happening was
too much duplication.
And I want to spend a little bit of time on
that topic as well, because some of you might
be going through this as well.
Now, we ended up having -- like, we kind of
built a lot of systems that achieved same
goal at high level but we used different mechanics
to kind of get there.
Generally speaking, duplication is bad.
Again, this is one of the points where it
can be considered as a con, but it can be
leveraged to your benefit.
Let's see how.
Duplication is generally bad.
Why?
Because it results into code debt.
You have a lot of, like, code that's doing
exactly or sort of the same thing, and it
also introduces different interfaces for the
users of those tools, making it extremely
hard, cognitive ads, a lot of cognitive load
to your users who are feature developers;
right?
Who want to use your tools to kind of effectively
verify that the software is working.
So for those two reasons, we can think of
this as something that should never happen,
but, on the other hand, duplication is really
good because it gives individual groups a
lot of flexibility and ability to kind of
provide customized solutions.
There is no dependency then on one team that's
responsible for a common tool to kind of cater
it your feature requests.
And you can just move really fast.
So there is a debate between where we can
entertain duplication, where we shouldn't
have duplication.
And sometimes it's important to kind of know
that this is a hard, sensitive topic.
And sometimes senior management has to take
in -- take some decisions which are very hard,
and they can say that, hey, we are going to
say we are going to stop use of this tool,
versus senior management can say that we are
forging a merge of multiple tools that are
doing the same thing.
Now, this can lead to a lot of unhappy feature
-- sort of tool developers as well as early
adopters of those tools, but my point is that
if the duplication is organized, it works
really well.
It can create a lot of competition, but it
will also create collaboration so that multiple
teams of good thought -- multiple teams can
come together and produce best-in-class tools.
But if it gets unnoticed, it will result into
code debt and confusion.
So something to kind of think about.
Metrics and measurement.
You cannot improve what you cannot measure.
That's very clear.
And we knew as we were evolving over time
that there was no kind of visibility into
how often we were releasing, how smooth we
were releasing, what kind of test metrics
we were applying to developer work flow, so
on and so forth, without really measuring
all of those things.
So we invested a lot into separate organizations,
and sometimes with help of engineering productivity
engineers themselves, to kind of gather a
lot of data around these axes.
Some of the kind of metrics that we are really
interested in, for example, are let's say
before you submit your change, how long is
it taking you for your change to kind of run
through required tests?
How long is it taking you for your change
to be available in production to your users?
And when you actually submit that change,
how often -- and you're building those tests
in a continuous fashion, et cetera, how green
are those builds?
How flaky are things in production?
Some sort of metrics around monitoring of
what's happening in production.
All of this matters a lot.
And we have invested a lot into building mechanisms
to make these metrics easily available to
feature developers.
One thing I want to mention here is it's not
sufficient to just say that, hey, build a
lot of metrics without really having executive
sponsorship from your leaders who also believe
in those metrics.
Your leaders start believing the metrics -- believing
in such metrics because it's really, really
important that we can stack rank different
projects on common footing when we can derive
data for those projects through -- via these
metrics.
That makes it extremely easy, then, to kind
of have a lot of different teams work with
each other in a friction-free fashion.
That's what the leaders want to achieve anyway.
So we in ads in Google have had tremendous
amount of support from our senior leadership
to kind of set quarterly goals, yearly objectives
to improve around some of the axes of either
presubmit or post submit of change kind of
like metrics that I talked about.
And all of this actually helps, because we
also wanted to change the developer mind set
of pushing testing to the last minute, and
instead doing a lot of testing upstream.
So with these metrics, it was really, really
easy for our developers to kind of know that,
yes, fine, this is how long it's taking for
me to kind of submit my change and get it
ready, available in production, so on and
so forth.
So I wanted to just summarize, like, how the
entire process at Google in 2.0 is working
out.
This is kind of a 360 view of how we work
today in Google in my ad space.
We have lots and lots of developer testing
happening, which is the upstream part of it,
and followed by a lot of continuous builds
and continuous deployments that run those
tests in a continuous fashion and give us
very early feedback whether things are working
or not.
We make internal decisions on which change
lists, which kind of sort of change -- like
sort of collection of changes is safe to pick
to start your release process based on these
continuous builds and test signals.
We have lots of frequent releases, as I mentioned.
Over 90% of our releases are within a week,
like on a weekly cycle, in world of ads.
And it's not sufficient to do, like, just
the feature-level testing or the developer
deployment -- like, sort of developer work-flow
testing.
We also do testing during releases, which
is the canary testing, followed by lots and
lots of production monitoring.
Product monitoring, why?
Because things can go really, really wrong
based on what kind of data was used during
testing versus what we actually see from users.
The traffic pattern changes completely over
time as well, as a lot of you might know who
are in the business of doing, sort of, exporting
features that are user facing.
So we have put in a lot of automation.
And along with that, we're also kind of reaping
benefits of that automation because now our
tests are more reliable, more repeatable,
more maintainable, and there is less toil
in terms of, like, what tests are run and
the records of it and blah, blah, blah.
We have systems which kind of tell us that
automatically.
So with that, we are now looking forward to
2017 and challenges in future.
We are at the doorstep of 2017, and some of
the key things that -- key challenges that
Google, as an organization, is going to have
are going to be related to some of the things
I've highlighted here, which is our brand.
We are no longer an ads and search kind of
company.
We have so many different products going on.
And there is a lot of integration between
those products that's already happening or
should happen.
We are going to be heavily, heavily relying
on abiding by the privacy principles of user
data and user information that we have with
Google in order to earn users' trust on a
continual fashion.
And we are going to have to sort of accept
the challenge of working with next generation
set of tools.
With all of this, by the way, one thing to
remember is machines do not come for free.
So machine resources and cost should be taken
into account while you're planning for your
test strategy, when you're dealing with all
of the next gen infrastructure.
And that's one of the challenges, too.
What's our test strategy going to be moving
forward?
The model that we're going to apply is really
focusing on deeper integrations.
By that I really mean that moving forward,
we are going to have users who are sitting
in their place and want to book a travel,
want to do shopping with just their voice
commands; right?
For that to happen, we have to have so many
systems that need to work together, whether
it's speech recognition, whether it's all
the query analysis of the keywords extracted
from users' speech and kind of connecting
that to the payment system back-end, so on
and so forth, in order to kind of do the flight
search in case of travel, itinerary planning,
et cetera.
There is a lot that needs to just talk to
each other.
Those are the deeper integrations I'm talking
about.
Google now is 70,000 people organization,
and engineering productivity function has
also tremendously grown.
It has grown not just in numbers but also
now seniority.
And the time has come where we have to start
talking to each other and achieve some standardization
and training against production engineers
across various parts of Google.
So there is a challenge around that as well.
How do we make multiple teams talk to each
other and work in a friction-free fashion?
And that's where the metrics are going to
really help us.
The other thing that I want to mention is,
all the metrics and sort of measurement that
I spent a little bit of time talking about,
why it matters is because the data that -- like,
we basically have tons and tons of data now
with our continuous build cycles and tests
and release and whatnot.
But we have spent very little time to kind
of analyze that data.
What metrics are working well?
What metrics need attention moving forward?
We have to start spending more time and do
kind of analysis there.
Things like machine learning models and machine
learning-based sort of software is coming
online more and more so moving forward.
There is Google Home that's going to gather
so many sort of queries from users.
And we need to analyze even that data and
make sense of it.
That's going to be a new challenge for sure.
And our users, when they use different kinds
of platforms, they're no longer going to be
just sticking around with desktops and mobile
phones.
There is going to be virtual reality out there
very, very soon.
We have to take that into fact -- take that
into account as well.
Google as a company was really successful
because we spent a lot of time scaling our
server architecture.
Our servers and distributed systems was backbone
of Google from the get-go.
But our ecosystem and our tools are not really
best-in-class when it comes to the new platforms,
especially, like, mobile app infrastructure.
And we realized that.
We realized that as a challenge.
And I want to spend a little bit of time talking
about why this is a challenge and why it really
matters.
This slide is from open signal that just gives
you a little bit of idea of how diverse the
fragmentation of all the Android devices there
is as of, like, August of 2015.
There is about 25,000 different Android devices
that are out there.
And for Google to make all of its products
work in such a diverse environment, it's a
very big challenge.
Among different challenges developing that
mobile app infrastructure experience are,
how do we do testing correctly?
How do we make the developer experience smooth
from engineering productivity perspective,
obviously.
And how do we do monitoring in production?
Especially for mobile apps, it matters how
much is the application package size?
What are the latency metrics looking like?
What is the crash rate for that particular
app?
What is, generally, the memory used by the
app?
All of these metrics are extremely important
for any mobile app.
And for engineering productivity solutions
in terms of, like, how do we test this, how
do developers really make sure that there
is device labs, there is emulators, there
is lots of unit tests.
And, by the way, all of this needs to work
with the two major players, Android and iOS.
So how do we even share those best practices
and make it really, really smooth for our
developers?
Think of so many different common things that
are out there that every single app needs
to go through, whether it's account management,
whether it's access management, the user identifiers,
the data associated with the user identifiers
and the users, that needs to travel from app
to app.
If all of those things are not made as common
utilities, common sort of infrastructure which
is non-flaky, we are going to fail big ways.
So we cannot.
And that's a big challenge, too.
And, finally, the kind of -- the kind of sort
of things that were working well before in
Google, which are, hey, we have, like, ads
area and we have communications area and we
have apps area, and everybody has really well-defined
sort of deeper integrations which are product
aligned, but in silos, is no longer going
to work.
We have to come together.
We have to share best practices and build
common platforms so that all of these different
areas can provide the best possible mobile
app infrastructure experience.
I encourage all of you to attend tomorrow's
keynote by Niranjan, who is going to go over
some of the interesting artifacts of how that
mobile app infrastructure experience is going
to change for Google and what evolutionary
and revolutionary things that we are already
starting to do.
So with that, I just want to leave you with
the note that there is a lot to optimize.
There is a lot that we can learn from the
data that we are gathering.
The challenges are enormous.
But the typical model of build, test, release,
and repeat is not going to be sufficient in
what is going to come in future.
I at least feel that we are very happy and
well-equipped to kind of take on these challenges.
And we have started using machine learning
algorithms, for example, to kind of optimize
developer work flow to kind of decide some
tests are completely useless and therefore
they should not be run when the user is trying
to submit their change and so on and so forth.
So we have started doing a little bit in that
area.
But we could do a lot more.
And that's going to be the next set of challenges.
But I'm extremely, extremely excited for engineering
productivity at Google, and especially in
my area of ads as well, to kind of tackle
these challenges.
So thank you, again.
Thank you, GTAC organizers, for giving me
this chance and opportunity to kind of talk
to all of you.
And you were a great audience.
So I'll be ready to take on questions.
Thank you.
[ Applause ]
&amp;gt;&amp;gt;Manasi Joshi: Thank you, Manasi.
Excellent keynote.
It's --
&amp;gt;&amp;gt;Matt Lowrie: Thank you, Manasi.
Excellent keynote.
I'm so glad we are not doing that at all anymore.
We're not.
[ Laughter ]
So, questions.
Would you like me to read the questions?
Or -- I'll -- I'll read off the questions,
and you can answer them.
Which tools or open source frameworks Google
uses for automation?
&amp;gt;&amp;gt;Manasi Joshi: Well, Google is not one product.
Google is multiple products.
And so this answer really depends on, like,
what product space you are talking about.
So I'm not, like, sort of sure where we kind
of keep a long list of open source tools that
we can use for automation or that we are using
for automation today.
But I think there is some bit of that happening
in our mobile sort of SDK-related development.
So whoever asked this question, I will be
glad to kind of get back to you with kind
of gathering more information on it.
But I don't have that list in my mind.
&amp;gt;&amp;gt;Matt Lowrie: Okay.
&amp;gt;&amp;gt;Manasi Joshi: So please talk to me afterward.
&amp;gt;&amp;gt;Matt Lowrie: I think the bask answer is,
&quot;Lots of it.&quot;
&amp;gt;&amp;gt;Manasi Joshi: Yes, yes.
&amp;gt;&amp;gt;Matt Lowrie: So how do you automate testing
a self-driving car?
That's a good question.
Something you do not want to do in production.
[ Laughter ]
&amp;gt;&amp;gt;Manasi Joshi: I agree with you.
I would love to know more.
I haven't explored that area as yet.
So I certainly do not have the best answer
here, how do we automate testing of a self-driving
car.
But, obviously, there is a lot of machinery
that's involved in terms of lots and lots
of data that's getting processed to detect,
like, sort of obstacles and whatnot.
And I sure hope that there is a lot of tests
out there.
I'm not familiar myself directly.
&amp;gt;&amp;gt;Matt Lowrie: Very good.
What is the percentage of automated test versus
manual tests?
&amp;gt;&amp;gt;Manasi Joshi: Okay.
So I can tell you for sure that, again, from
display ads infrastructure perspective, I
want to say probably close to 95% of our testing
is automated.
There is a 5% testing that is manual.
And things like we are now -- I don't know
how many of you have followed the news, but
Google has a new initiative to kind of produce
products for kids.
So make sure, like, there is YouTube available
for kids and whatnot, and how that experience
is going to change for a user when the user
is considered a child versus the user as an
adult; right?
And for that kind of testing, where there
is, like, a lot of mobile experiences for
kids, or tablet experiences, et cetera, we
do have some manual tests that I'm aware of.
But I want to say, generally, 95% is automated.
&amp;gt;&amp;gt;Matt Lowrie: That's --
&amp;gt;&amp;gt;Manasi Joshi: Again, there are a few teams
out there in Google outside of ads which are
more front-end centric, and this answer may
not apply as 95%.
It could be a little different.
But okay.
&amp;gt;&amp;gt;Matt Lowrie: Very good.
Can you tell us a bit more about what sort
of metrics are used heavily in Google.
&amp;gt;&amp;gt;Manasi Joshi: Okay.
So generally speaking, I think I alluded to
this a little bit during my presentation as
well.
But kind of metrics that we really care about
are how long are developers spending time
in running those tests that we really want
them to run before they can submit their change
list into our versioning system.
In other words, are we really slowing them
down at the cost of just saying, like, hey,
I don't want to do a lot of testing in releases,
so why don't you take care of all the testing
while you are doing feature development and
just get frustrated with it.
So we obviously want to squeeze that time
for developers how much time they're spending.
So we kind of typically refer to as, like,
a presubmit latency for our developers when
they're writing code.
So that's certainly one very major area that
we kind of literally think about every single
day.
The other areas, other metrics are in terms
of, like, how many bugs there are, bugs, open
bugs, that are there in the bug repository.
What are we doing to kind of address them
over time?
How many of them are considered, like, parity
0 bugs versus 1 and 2 and so on and so forth.
There is certainly a lot of emphasis given
to how healthy our systems are, in terms of
how green those continuous builds and continuous
deployment systems are, because the greenness
really matters, if your system, you are kind
of writing tests which are flaky all the time
and failing in production and whatnot after
you submit your change, your continuous systems
are going to also be very red.
And therefore there is a possibility that
you cannot even start your release if your
continuous deployments are red.
So we're going to focus a lot on, like, build
greenness in a continuous fashion as well.
And there is a lot more.
But I think we should get to more questions.
&amp;gt;&amp;gt;Matt Lowrie: Ah, yes.
One thing that I think is an interesting story
about Google, is that we like data so much,
we need people building dashboards to tell
us which dashboards we should look at.
[ Laughter ]
&amp;gt;&amp;gt;Matt Lowrie: We have that much data.
&amp;gt;&amp;gt;Manasi Joshi: Yes, I like that.
And by the way, one more area which I kind
of forgot to mention is, with so much of this
investment happening in engineering productivity
to build those metrics very easily and make
it available at fingertips and so on and so
forth, there is also emphasis and a lot of
interest that a lot of engineering productivity
engineers have to detect bad patterns in code
and to detect dead code and bad build dependencies,
because in -- in -- directly or indirectly,
this all amounts to the developer experience
and developers velocities anyway.
So we have very powerful techniques to kind
of detect, like, sort of bad dependencies
in our build system as well as, like, flakiness.
And, yeah, we kind of take that into account
as well, like focus on it.
&amp;gt;&amp;gt;Matt Lowrie: Okay.
And if anybody in the room had a question,
feel free to raise your hand as well.
I can get a microphone to you.
Otherwise, I'll continue with the online questions.
&amp;gt;&amp;gt;Manasi Joshi: Yes.
&amp;gt;&amp;gt;Matt Lowrie: Oh, here.
&amp;gt;&amp;gt;&amp;gt; Thank you, (saying name) from the University
of Maryland.
So this kind of related to both of your comments
on testing in production.
So you talked about the bad things in testing
in production.
But now that we are getting into more and
more learning-based software, it may actually
be impossible for a developer and tester to
predict what the output will be at development
time.
So are we going to enter a period where we
need to test in production?
And how do we get around that, the problems
there?
&amp;gt;&amp;gt;Manasi Joshi: Right.
So, again, as I mentioned, even before, the
model and the test pyramid, et cetera, it
kind of like works on an average, but not
in every single use case; right?
So for things which kind of include, like,
machine learning, where there is supervised
learning and whatnot, you obviously have to
test in production kind of or based on some
data sets, but also on the actual user samples,
right, in order to do the predictions and
whatnot.
So I think there is a mixture of, like, testing
in production versus up-front testing that
needs to happen.
But in terms of general behavior and correctness
for your software that's going to be responsible
for user data analysis, that, I would highly
recommend our, think, it will make sense to
kind of test before it hits production.
&amp;gt;&amp;gt;Matt Lowrie: Very good.
I think we have time --
&amp;gt;&amp;gt;Manasi Joshi: There's a question.
&amp;gt;&amp;gt;Matt Lowrie: -- we have time.
Let me take one more here, and then I'll get
to you.
But I think we have time for two more questions.
How much time does the presubmit phase take
to qualify new features?
Do you use test selection?
&amp;gt;&amp;gt;Manasi Joshi: So if I understand this question
right, how much time was presubmit phase take?
So typically, like, again, I can give you
an average answer.
It takes about ten days or so between the
time me, as a feature developer, has submitted
my change list into versioning system and
it is available in production.
It varies a lot depending on which part of
the world in Google I am.
But, essentially, in display ads organization,
again, we kind of spent -- I want to say a
good three to four days in, like, the presubmit
phase where it's not just about -- oh, sorry,
not three to four days in presubmit.
In about, like, two to three days in presubmit
phase where the developer is actually running
different tests or we are running different
kind of continuous tests on their behalf on
their change list in parallel, offline, and
kind of providing those signals.
&amp;gt;&amp;gt;&amp;gt; Hello.
(Saying name) from Oregon state University.
So, as an academic, I often find that software
tests in open source tools are somewhat -- sometimes
wrong outright wrong, or otherwise not sufficiently
(indiscernible).
So, my question is, does Google actually verify
that their tests are right?
And do you have any automatic means of doing
that?
How do you test your test?
&amp;gt;&amp;gt;Manasi Joshi: How do we test your test.
&amp;gt;&amp;gt;Matt Lowrie: Tough one for the last question.
&amp;gt;&amp;gt;Manasi Joshi: Okay.
So regarding the comment, the question was,
like, some of the open source tools are poorly
tested.
And before Google really uses some of those
tools in their sort of testing, does Google
verify that there are wrong tests.
Sorry, is that correct?
Some of the tests that are written are often
outright wrong.
And sometimes the assertions are not complete
or they don't really do anything.
&amp;gt;&amp;gt;Manasi Joshi: Are you specifically talking
open source tools or in general?
&amp;gt;&amp;gt;&amp;gt; In general.
&amp;gt;&amp;gt;Manasi Joshi: In general, okay.
So some tests are outright wrong.
And -- yes, and whatnot; right.
So, yes, that's the very reason why we do
not marry ourselves with 100% line coverage,
meaning your test is done, like, your testing
is complete.
Because, yes, tests could be wrong, and tests
could be wrong because of the way the binary
code is wrong and buggy.
Because if the test is written in a fashion
that it's too brittle, it's going to just,
like, you know, do, like, sort of -- reflect
exactly what the production code -- the binary
code is sort of doing, which could have bugs
in it.
How do we test the tests themselves?
So we actually -- I actually strongly think
that there is no good point where we can say
we are done with testing.
Like, the whole sort of -- So, again, engineering
productivity, as an engineering productivity
engineer, I'm going to do some sort of risk
assessment of what is the risk associated
with any particular launch and sort of suggest
the best possible test strategy and develop
the frameworks that are enabling my feature
developers to write tests in.
But I'm never going to be in a position and
comfortable to say that if you have written
tests here and there is a certain amount of
coverage associated with it, we are done and
we are in good shape, because things can,
like, change very well in production in ways
you haven't even dreamt about.
The way we do testing of tests, we don't actually
focus a lot on testing the tests themselves,
and therefore we kind of actually exclude
our test files when we do coverage calculations.
But how we -- We kind of believe strongly
in defense in depth.
And therefore we are not going to ever sort
of just rely on 80% unit test coverage and
calling that done.
We are going to also do, like, system-level
tests and some sort of log-level tests which
are testing the system invariants.
And that way, we are getting assurance from
different ways that your software is behaving
correctly.
All right.
&amp;gt;&amp;gt;Matt Lowrie: Fantastic.
I think we --
&amp;gt;&amp;gt;&amp;gt; (Off mic).
&amp;gt;&amp;gt;Manasi Joshi: Okay.
Go ahead.
&amp;gt;&amp;gt;&amp;gt; Somebody in Google also thought about
this issue, and we have something called mutation
testing where we deliberately change the code
and then test -- and then verify whether the
test is correct or not.
So we verify whether the test data and et
cetera is valid or not.
It's not used by many team here, teams here,
but, yeah, it's there.
&amp;gt;&amp;gt;Manasi Joshi: Thank you.
Thanks for adding that.
&amp;gt;&amp;gt;Matt Lowrie: Very good.
And we're out of time, but, yes, thanks, Manasi.
&amp;gt;&amp;gt;Manasi Joshi: Sure.
Thank you all.
&amp;gt;&amp;gt;Matt Lowrie: Lets give her a hand.
Let's give her a round of applause.
[ Applause ]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>