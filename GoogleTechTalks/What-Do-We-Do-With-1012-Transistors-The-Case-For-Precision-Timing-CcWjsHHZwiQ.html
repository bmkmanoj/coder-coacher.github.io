<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Do We Do With 10^12 Transistors?  The Case For Precision Timing | Coder Coacher - Coaching Coders</title><meta content="What Do We Do With 10^12 Transistors?  The Case For Precision Timing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Do We Do With 10^12 Transistors?  The Case For Precision Timing</b></h2><h5 class="post__date">2008-02-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CcWjsHHZwiQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today we are glad to have like a
professor stephen edward from columbia
university to give us a talk about
future hardware if you have a trillion
transistors will do with what we do with
it so let's welcome professor
so coming here actually there's a
workshop tomorrow at Stanford when I'm
presenting more or less the same thing
I'm going to be arguing with a bunch of
other people who have different opinions
about this but the premise of the
workshop is so we have billions of
transistors on a chip at the moment a
blink of an eye we're going to be up to
trillions you know what do we do with
all of them you know what you know what
are the chips going to look like so
first what I want to do is talk about
what we're not going to do with them
we're already at this point where a
single single threaded CPU just doesn't
make any sense anymore right you know
Intel and HP and all the rest of these
guys are just simply given up and said
oh you know we can't do this anymore we
can't make them any fat they basically
they can't figure out how to waste
enough transistors to make it worthwhile
so it's not going to be just a single
CPU we probably will have big memory
chips around but again that's not all
that interesting right one of the things
that I'm beginning to worry about you
notice this you know how long does it
take you to get the information out of a
trillion transistor memory chip so that
you can do something useful with it
right I mean you need a lot of pins and
so forth so still not very interesting
and if you start looking at it you know
gigahertz clock versus trillion bits
takes a while so another thing is to
just say well we're going to take
existing systems as we know them and
love them and so forth and sort of merge
them onto the chip that's already
happening to a certain degree but I
don't think that doing say Ethernet on a
chip or tcp on a chip or what have you
is is the right thing those are you know
very interesting protocols they work
very well but they're designed to attack
a different level a different assumption
that you can make about reliability
right in particular you don't really
have to worry about the fool with a
backhoe on a chip where is that's kind
of one of the main points of tcp/ip is
that you can deal with nonsense like
that so you know that's reasonable we're
not really super good at programming
systems consisting of a whole bunch of
computers connected well okay maybe you
guys are pretty good most of the rest of
the world isn't this is tricky so I
don't think just simply you know moving
an entire server room down to it a
single chip is quite the right thing to
do another thing we can consider doing
is say well
it's not memory but let's just make it a
big field programmable gate array right
you know choose all the all of the
things the problem with that is that to
come up with that much hardware an fpga
is already rather difficult furthermore
I don't know of any interesting system
that doesn't have some software running
on it anymore you know the last example
I could think of as you know like late
1970s early 1980s or whatever they still
built video games that have discrete TTL
parts right it's been a long time since
since that individual chips of course
might not have processors on them but no
interesting system doesn't have software
in it so these are the premises I have
so this is my hypothesis about what
these are going to look like and this is
a very pie-in-the-sky and there's a lot
of details that I'm leaving out here of
course and what I'm going to do is going
to try to argue what the CPU on these on
these things should look like but the
basic idea is this you know if you've
got a square flat thing what do you do
you put a grid on it because that's the
sort of the only arrangement that you
can you know think of it's easy to
replicate and all the rest of it this is
what we do with memories and all the
rest of you know even to the point we're
doing with this processors and there's
going to be have to be some fairly
clever communication system within that
mesh we know how to do clever routing
things onto BTW meshes the
supercomputing fellow people have done
this for a long time we actually
understand that pretty well and
furthermore just routing random wires on
a chip like this is going to be
ridiculous just because there's so many
of them so it's going to have to be some
fairly orderly arrangement like this now
okay so you've got a communication mesh
well you've got to do something
somewhere right you can't just be
passing bits around although that's
mostly what processors do these days
anyway you've got to you know stop and
actually do a little computing at some
point so what I'm going to claim and
this is definitely not to scale is that
at these grid points you're probably
going to want to have some sort of
central processing units something that
you know some sort of stored program
computer executing code of one form or
another you're going to want a lot of
memory no matter what because also no
interesting application doesn't need a
lot of memory or could not use a lot of
extra memory for some reason or what
have you you've got that
another argument is that you don't just
want a big software system because
you're not gonna be able to get the
performance you want now Intel can
afford to do this you know just give us
you know dual-core whatever's are or
what have you and they're fine they've
got a big enough market all to
themselves however there are people for
whom just plopping you know the next you
know 80 watt processor down is not going
to work right we can't do that for cell
phones can't do that for automotive
applications any of these things so
we're going to need some sort of
differentiation now the problem is is at
the moment it takes something like a
million dollars just to get a mask set
for a typical high-end asic high-end
chip or whatever and that price is just
going to keep going up and up because
the number of layers keeps going up
because the number of transistors
because of the precision and so forth
and we're seeing this also in the cost
of design for these chips it's just
going through the roof right I remember
reading at some point that Intel had
sunk roughly the same amount of money
into the Pentium as the US had had spent
putting a man on the moon all right that
was the first Pentium you know now we're
quite a bit beyond that that's just not
scaled right you know Intel can afford
to do that essentially nobody else can
design you know entirely custom chips
like this so my prediction the way that
the costs are going for manufacturing
these chips and so forth is that the
number of different chips that you can
fabricate in a year is going to drop
substantially this number has already
been dropping for quite a while I used
to work at synopsis just down the road
and one of the things they worried about
was the number of asic design starts
because of course that's when they could
sell their their products and the number
is is slowly decreasing and the problem
is just that it just costs too much to
design one of these a chip completely
from scratch and so instead what I see
is I see the number of chips that are
the number of fpgas say going into
consumer electronics and various other
things going up a lot because they're a
lot cheaper to manually or I'll tear
have done a lot of the engineering for
you it seems that that's an interesting
direction to go so I'm going to guess by
the time we get to a trillion chips
a trillion transistors on a chip no one
company maybe not even Intel is going to
be able to design more than a couple of
them a year and so instead the way
people are going to differentiate it the
cell phone folks or whatever is to be
able to program sir the FPGA components
of these chips somehow to get more
specialized behavior now that's all kind
of leading up to the main point of it
what I want to talk about most is what
the CPU ought to be like and my argument
is that it's not just sort of the the
same old stuff that we're fairly
familiar with 4 processor architecture
this is not going to be the Intel you
know core you know 64 dual quad whatever
it's going to be quite a bit more
specialized maybe a bit simpler in a
variety of ways and this is what I want
to try to convince you of today so
specifically what I'm going to try to
convince you of is it so called
precision time or prep processor this is
pret acronym which is rather an awful
acronym is one that my former adviser
and i came up with it at berkeley just
for precision time the basic idea behind
a prep processor is that you can predict
its performance you can predict a number
of clock cycles that a computation is
going to take as well as you can predict
the function as you can predict what
what's going to happen to the bits that
you feed into it and i'm going to try to
argue for the rest of this talk that's a
good thing that we're going to need this
we're going to want this for these sorts
of applications going forward this is
quite a change from what we're
accustomed to so I want to start this
argument by pointing out that embedded
systems is is where the action is at now
of course I'm going to make an argument
like that because this is where my
research is and this is how I convince
the NSF to give me money so this is this
is fairly obvious however if you start
looking at the numbers i think i can
make a reasonably compelling argument
for this so a few years ago 6.5 billion
processors you know this is counted both
individuals you know intel style things
plus all the way down to the really tiny
micro controllers or something and of
course if you look at the volumes the
microcontrollers have actually
long dominated the the unit numbers over
the Intel stuff of course until makes a
lot more money off of theirs because
they can choose a pre charge a premium
for it however of the 6.5 billion
processors ninety-seven percent of them
went into embedded applications right
the other three percent went into
desktops and things that you sort of
think of as computers but the vast
majority of don't get used that way and
this is kind of scary now I mean mind
you you know some reasonable chunk of
this is cell phones for example this is
rather tell this is rather terrifying
nearly three quarters of a million cell
phones sold in 2004 so each of those
have two processors so you can you can
figure out a reasonable chunk from there
you know game machines all sorts of
other applications something like 60 or
70 go into a typical automobile these
days it's rather unreal the numbers so
this is my argument that okay yeah you
know there are these server farms these
desktop machines and so forth but
they're a vanishing fraction of the
number of chips that are produced these
days the majority of more going into
places you know the the video projector
the video camera or the cell phones that
you all have like two or three of them
your pocket at least I do kind of kind
of scary now the thing about embedded
systems is the applications are
different than the traditional things
you find running on traditional
operating systems and so specifically
I'm going to argue that the challenges
are mostly of the form take the form of
hard real-time systems and so these
include avionics these are you know
controlling control surfaces in the air
aircraft there's all kinds of systems
like that automotive is a large and
growing application area for for
embedded processors like I say you know
80 or 100 in a typical car this this
number well they're actually trying to
get that number down because they don't
like that many parts but the number of
individual programs running in a car is
actually huge these days is looking at
growing if you go and talk to you know
BMW or whatever they say well you know
we've got this kind of interesting
computer it's it's got these weird
peripheral
you know rubber tires an internal
combustion engine and so forth but it's
really the software that lets us
differentiate our cars from you know how
dr whoever it's kind of amazing
multimedia right everybody has a couple
of DVD players at home at now
everybody's gonna have a blu-ray player
video game consoles things like that all
kinds of consumer electronics the
volumes here are just enormous oh and
all of these have the property that yeah
you've got to compute something but the
speed at which you compute it is often
just as important as the values that you
compute the analogy I like to give is if
you you think about a brake controller
in a car if you garbage collect while
you're pressing the brake and it you
know okay you get the answer but it's a
couple of seconds late gives new meaning
to the word system crash okay so what
we're familiar with in the computer
science world is that we don't when
we're thinking about the correctness of
a processor we don't think about how
fast it goes i mean we wanted to go as
fast as possible but you know nowhere in
the you know in the data book or
whatever does it exactly say okay this
is going to take this many cycles under
these conditions or whatever it this
used to be the case but no longer right
it'll tell you exactly what happens to
the registers and the status flag
stratus registers and all the rest of
that stuff and kind of vaguely what
happens to memory these days but that's
about it and this is so entrenched that
we're probably not even aware that it's
happening anymore and this gives us a
lot of advantages programming languages
virtual memory all of this stuff that
sort of standard computer science stuff
relies on this abstraction of I'm doing
computation in some order I'm doing it
as fast as I can and it's it's ordered
but I'm not setting the time exactly but
from time to time time does matter I
like this photo because it's the only
example I can find of engineering
notation being used in a headline in a
newspaper right you know this is a
couple year last year Kevin won the
Daytona 500 x 20 milliseconds I I like
this
great flick so time can matter so one
obvious question is well you know good
grief you know go get the the collection
of real-time scheduling books off of
your off of your bookshelf and get on
with it the problem with that is is that
all of the abstractions they use hinge
on knowing what the worst-case execution
time is for some for your collection of
tasks this is usually the Model T start
with now if you're programming an 8-bit
microcontroller where you can actually
count the cycles quite easily and
there's a long tradition of doing this
this is actually just fine the problem
is if you looked at a modern Pentium say
and say okay I have this chunk of code
how much time is going to take you
really can't tell the variability can be
huge because of caches and branch
predictors and pipelines and all the
rest of it it's just numbing and so as a
result being able to say what the
worst-case execution time especially for
a bunch of tasks that are all vying for
the processor and being scheduled and
all the rest of it it's really
unrealistic to be able to get very
precise estimates for this so if you
look at what can affect it so pipelines
are actually probably the worst because
they cause interactions very complicated
interactions but unfortunately usually
among fairly localized sets of
instructions or is if you look at branch
prediction it could be effective of an
instruction you know 100 instructions
away or 200 in such a way that can
affect this even worse their caches
where this can be affected by nearby
instructions by far instructions by how
the compiler lades things out in memory
how you know the memory manager put
things into memory how the other
processes have been playing with the
cash and so forth very very difficult to
predict the effect of this and there's
there's some work on it but it's it's
really difficult and this is really
interesting actually there's a paper
that was published a little while ago
pointing out that certain processes in a
modern processor this was looking at
instructions per cycle versus level two
cache miss x is actually chaotic in the
mathematical sense of the word so on the
left here we have this plot of level two
cache miss vs IPC for a particular
application in the center is a strange
attractor created by a nonlinear
equation being iterated and on the right
you have a herring so I it you know you
can actually put mathematical
quantifications on just how crazy these
things are these days to figure out
exactly what's going on in these
processors now crazy people have gone
and tried to very carefully evaluate
worst-case execution time on a fairly
modern processor and this is probably
the state of the art this was done back
in 2001 so they did it on a motorola
cold fire this was code that they got
from airbus designed to represent the
behavior of a typical task doing a
control surface stuff so this is you
know making sure the ailerons do the
right thing so they're really really
concerned that this runs at the right
speed and will always run at the right
speed and so forth so this is actually
very simple code no loops if I remember
that I think they're conditionals and
their memory accesses but that's it so
very you know very very simple code but
it's very complicated this on the right
here is the pipeline for this processor
it's particularly confusing because it
has a shared instruction and data cache
so to really figure out what's going on
with a cash you have to understand how
the program is interacting with the
memory that it's fetching at various
times you know this is really a dumb
idea they were able to do this but only
by taking these relatively small
snippets of code and expanding the
mounted to these huge integer linear
programming problems and solve it and
throwing it at some super expensive
solver and they could do it but my
feeling is is that this is they're
trying to solve a problem that it's just
much too hard right there's nothing
fundamental about this there's no rule
that says pipelines have to be that
complicated it's just a side effect of
this assumption that it doesn't matter
how fast things go just that they go
fast so the problem in a nutshell is we
have this incredibly precise timing and
digital hardware right we can you know
for a you can get a crystal oscillator
that's correct within 100 parts per
million even better if you want it
and then we put on all this algorithmic
complexity that throws it away
completely right you can you can get
incredible accuracy and then suddenly
throw it all away so why do that is the
central thesis here why not try to be
much more careful and make performance
and how predictable it is and the timing
of all of that as predictable as the
function of these chips right we
describe an incredible detail if you go
looking up in a process of reference
manual exactly what's happening to this
register and that register and the
status register and so forth when you
execute this instruction under these
versions and so forth why not make
provide models to the programmer and the
compiler and so forth that are that
precise but also for timing there's no
reason why we can't do that it was just
which show that we've chosen not to and
so the argument is that we need to take
a step back look at computer
architecture again and go after
precision timing as much as precision
function okay so let's talk about all of
the stuff that you have to rethink or
change or modify a little bit to try to
achieve that one obvious thing one
obvious source of unpredictability is
caches in the memory hierarchy you know
I have far too much experience with this
i just recently upgraded the memory on
my desktop machine until i did that i
had this case where you know these
programs would one reasonably fast and
then i would make the mistake of running
some other extra little thing that I
thought was not a big deal and suddenly
the machine would stop start swapping it
would go oh I don't know maybe 10,000
times the speed or one ten-thousandth
the speed if it was before you know and
basically I've got to pull the plug
because it's going so slowly it's just
it's just not useful just turning off
the caches and embedded systems and some
crazy people do this gives you a hundred
x performance hit compared it you know
going off of main dram or whatever so
this is just not practical we need to
keep around memory hierarchy this is a
good idea this is a way to make things
go fast I'm not saying go make things go
slow and saying make them go predictably
fast but instead of Cash's
you're sort of hiding in the background
I don't really tell you what's going on
and so forth instead what we need to do
is think about scratchpad memories which
are exposed to the programmers somehow
and by the programmer I mean either the
person actually writing the code or
perhaps the compiler or something like
that but the point is is that that
abstraction is exposed at a level where
you can model it you can understand it
you can predict it much better you're
still going to want to have a very large
dram connected up to a smaller SRAM
connected up to a very small SRAM with
large Mecca with mechanisms for fast
data transfers between them but the
point is is that we should be able to
know when the processor is is hitting
there or hitting or whatever more or
less because we tell it to and in fact
there's a whole bunch of existing
literature talking about how to deal
with with explicit memory hierarchies
like this most of the arguments have
been around power reduction they make
the argument quite rightly that caches
are very power hungry because they're
going fast and because they have all
this extra you know content addressable
memories and all the rest of that stuff
in them so most of these things actually
are about you know look at the power we
save but our view of this is well this
is very interesting but more to the
point look at how much more predictable
we can make the program look at the fact
that we could write a compiler that
would tell you exactly the section of
code is going to take you know 300
cycles to run exactly pipelines or
another huge source of unpredictability
mostly because we've pushed them so far
that there's a lot of interactions among
the instructions in a pipeline and if
you look at it the basic idea of a
pipeline is a very simple one but when
you start looking at what has to happen
with all the bypasses and under this
condition this gets connected over there
and so forth it gets very messy very
quickly now of course this is what the
folks up in an Oregon get paid to do and
figure all this stuff out but it makes
it very very difficult for a programmer
or compiler or whatever to understand
exactly how fast a little section of
code is going to run so a very heretic
illai dia is to say let's still use
pipelines I mean this is a good idea
it's a way to get more more performance
out of a given piece of hardware but
instead of trying to make them sing
threaded use so-called thread
interleaved pipelines and the idea is
simply this if you have an H stage
pipeline have eight different
instruction counters going
simultaneously and have them independent
have it so that each of them talk to a
different register files that you don't
have collisions so that you don't need
bypassing or what have you now of course
this chops the performance of an
individual processor down by a factor of
eight compared to what in theory you
might get if you had a you know
perfectly packed pipeline I'm going to
argue though that first of all we're on
these big chips we're going to have a
whole bunch of processors anyway so why
not increase the number of threads
available secondly there's a limit to
how much instruction level parallelism
you can ever get right you know it's a
factor of three or four or something
like that it's just it's nearly
impossible to squeeze much more of that
out of sort of average written code you
know in certain cases you get very lucky
but usually not so we're already started
doing this already right you know hyper
threading simultaneous multi-threading
or whatever is already sort of doing
this more or less realizing it well you
know there's just too many bypasses and
connections and so forth instead why
don't we run multiple threads on the
through the same ale you basically okay
another question is well what about
interrupts right this is a fundamental
way of peripherals talking with a with a
processor and certainly embedded systems
always have lots of interrupts and lots
of i/o and so forth here's the crazy
idea don't have an interrupt bit but
resort exclusively to polling to access
this this i owe stuff and you think well
jeez isn't this ridiculous you're
devoting an entire thread to doing
nothing but saying are you ready yet no
are you ready yet no or you ready no
well you know that that seems really
awful but again remember this vision
that I have we're going to have a you
know 64 x 64 grid of these CPUs each CPU
is going to have you know an eight deep
pipeline running eight threads each
you're going to have leftover threads to
play with and it makes a lot more sense
to do it this way you can also give very
precise you know essentially interrupt
latency things because if you have one
thread dedicated to each potential
interrupt source one each peripheral you
know exactly how much how much time it
will take you to respond to that and
you're no longer sharing resources let
me talk about this business of
inefficiency though this was the best
data I was able to drag up it would be
very interesting to do this on a more
advanced processor so if you look at a
modern very aggressive processor that
has all of these units all of these
pipeline stages or whatever an
interesting question is over a period of
time what fraction of the time are each
of these stages actually being used
right if it's a hundred percent this is
fantastic the computer architects have
done an amazing thing figuring out all
the you know done all this speculation
or whatever to cave with everything
occupied the thing is is that it doesn't
work that way it's very very difficult
to keep all of a modern processor
humming at once and that's probably just
as well anyway because if you manage to
actually get the whole chip working it
would probably melt so the sorts of
numbers you saw this is a rather old
paper now they were trying to argue for
this and ok the utilization of the the
energy units you know 12 to 50 percent
fifty percent is pretty impressive
utilization on the other hand if you
know like the shared some of the shared
buses and some of the address buses of
course if you've got a program that's
not using floating point the floating
point unit is just going to sit there
you know running nops essentially anyway
so this is happening already so my point
with all of this is that you know don't
be afraid to have you know computational
units not being used in a processor
because it's going to happen anyway and
so why not put them why don't why not do
things like use polling style io rather
than having an interrupt system and then
worrying about an operating system that
has to prioritize the interrupts and
interrupt handlers and all the rest of
this stuff why not just simply say okay
this thread handles that I oh you know
handles that peripheral end of story
how do you do communication in a print
environment the communication we're
accustomed to again is usually best
effort right if you think about Ethan
ettor even if you think about the buses
on on modern processors they tend to be
sort of best effort right you know
handshaking send the data hope that it
comes back as quickly as possible this
becomes unpredictable as well however
there are all our alternatives and the
real-time community is sort of figure
this out one way or the other the basic
idea is to use so-called time triggered
buses or time triggered communication in
sort of the systems world the ATM
networks were based along this line but
the idea is pretty simple it's you take
a relatively low speed heartbeat you
divide it up into some number of slots
and then you give each potential
communication communicating agent one of
those slots and so of court you're not
playing this game with the statistical
multiplexing or whatever and of course
the maximum bandwidth is limited by the
fact that you've pre-allocated all of
this on the other hand you get a
guaranteed amount of bandwidth from
point A to point B of course if you
change these schedules per application
or whatever you can sort of tune it
appropriately an example of this is a
bus called flexray that's being used in
cars now and the idea is is you've got a
whole bunch of nodes connected up to
multiple channels or something like that
divide time into multiple slots the
first large chunk of slots are
pre-allocated you know your turn your
turn your turn and then there's some
time at the end where ok you can fight
it out you know for random things or
whatever but these work really
reasonably well they're reasonably well
understood again it's quite a departure
from you know the the packet view of the
world that you get with ethernet and so
forth but if you want to get real time
this is how it's understood how do you
deal with shared resources in general
right a really good question i give this
this talk like this at alterra they said
ddr2 how do you deal with huge off-chip
memories right this this is going to be
the case for a long time that for
processing technology reasons it's going
to be difficult to put a whole bunch of
deer am on a chip and so
is going to be cheaper to put it on a
different chip where you can process it
or you can process that chip a little
bit differently so that means it's a
shared resource you've got a whole bunch
of processors trying to vie for that how
how do you deal with that well all of
these situations when I think of
something that's shared effectively
there needs to be some sort of
arbitration policy right you know there
might be multiple things trying to get
it at it at once only one of them is
going to win the question is how do you
divvy up that time now the obvious thing
is first come first serve a
prioritization or something like that
however if you go take the route of the
like the flexor a bus or something
another obvious thing to do is round
robin and again this gives you
guaranteed bandwidth it's not the
absolute highest bandwidth that you
might get out of it but again you can
predict the speed at which everything
goes much better here is a tiny example
of a chip that that embodies some of
these ideas we didn't design this this
comes from a company called parallax
that does mostly tiny microcontrollers
and it's it's really cute it's called a
propeller chip and the reason is is that
you can see the picture down here the
right hand side it's supposed to look
like an airplane propeller that goes
around and says okay it's your term to
have the for the main memory your turn
to have the main memory it's divided up
into eight so-called cogs each of them
are single 32-bit processors single
single thread within each but you've got
eight independent pcs running on this
they each have their own local memory
rather small but then also share a large
potentially off-chip memory and let's
see here are the statistics it's very
low power they're going off of four very
tiny applications you know less than a
third of a watt kind of slow 80
megahertz each of these each of these
things runs at 20 mins pretty modest by
these standards there's a one large main
memory like I said put a bunch of tiny
ones the number of cycles for each
instruction is carefully spelled out in
the manual it's very precise the one
thing that's somewhat unpredictable is
if you go and access main memory the
amount of time it takes is
basically you have to wait your turn and
it was just a simple round-robin
schedule that goes around it says okay
cog one you get the main memory cog to
you get main memory and so if you look
at the speck it says anyway from seven
to twenty two cycles depending on
whether it's just about to become your
turn or whether you just missed your
turn and this if you're an electrical
engineer this would be a great chip to
hack with in fact if you go on
thinkgeek.com they've got a board design
for programming video games in it this
is the only chip I know of that has a
masked programmed rom with a font in it
that has symbols for transistors and
resistors so this is a very nerdy chip
what about an operating system at a pret
environment well process scheduling with
any luck is not necessary you probably
want something that will it'll you know
operate sort of like a monitor where to
load all the process is various various
processors and so forth playing with
this resource allocation stuff you may
need some initial scheduling when the
application starts up okay what are the
schedules for these various
communication things but after that
point you just want the operating system
to get out of the way it should not be
doing scheduling it should just let this
thing run on the one nice thing that you
might want to keep around is the notion
of a hardware abstraction layer right if
you want to talk to you know a
generalized network interface you
probably want to have some translation
that will go from the generalized one to
a more specific one but that would
probably be the extent of it okay so
most of these have been ideas right I'm
sort of arguing for a particular view
now I want to talk to you about a very
beginning experiment where we actually
built something beginning to be a pret
processor to see whether you could pull
it off so this was I took a master's
student actually no let's see i took a
master's student told him do that failed
miserably took another master student
made a little bit more progress then
failed miserably but the third master's
student managed to pull it off and this
is the basic idea I said pick an ISA he
said MIPS I said ok
add one instruction the idea of this
instruction is it gives you cycle level
timing control and the the whole idea is
so most most embedded processors or
whatever or most processors in general
have some sort of timer that generally
can generate an interrupter maybe a
waveform on a pin or something like that
but it's a little awkward to use I said
let's put that in the ISA and call it
deadline call it dead for deadline and
the idea is you've got a collection of
timers and you can specify a deadline ok
I want the next block of code to be
executed in exactly one hundred cycles
now if it takes more than 100 cycles
then throw an exception or something
your program is buggy something is wrong
but if it takes less than 100 cycles
essentially Pat it automatically with
with no ops or something like that to
bring it out to 100 cycles so is it's
essentially an automated time waster and
so it's a very simple this is what it
looked like a bunch of general-purpose
registers the only thing we added with
these four timers all the instructions
are utterly utterly standard except the
extra deadline instructions the main
point of this was that this is not a
huge departure from what were accustomed
to so that compilers would work and so
forth this is the architecture so you
know you have a register file and you
throw up the counters off to the side
the counters are kind of like an
additional register file the only
difference is that when you write
something into them it starts losing its
value at a predictable rate and so
here's what happens if you have a
deadline instruction that gets executed
and the timer has not expired what
happens here on the right so I say
deadline with timer0 of eight and timer0
started three what will happen is it'll
wait until it counts down to zero and
then what it does it will immediately
reload it and then it'll start executing
the next block of code so the practical
upshot of this is that if you put
multiple deadlines instructions in a row
in straight line code the first block
execute in the number of cycles do you
ask for the second block executing that
set and so forth that's one thing even
more useful thing to do is to put a
single deadline instruction in a loop
and when you do that what happens is
you're guaranteed that the loop will
take at least that many cycles to
execute and as long as you haven't asked
to do too many things in that period of
time it'll take exactly that many cycles
to execute so the application we have to
demonstration applications here but the
compelling one was I wanted to do video
i wanted to generate a video signal you
know standard 640 by 480 vga but do it
entirely in software coded up so that in
software you generating the various sync
signals and then being able to dump the
pixels out of the right speed normally
you have to do this with hardware
normally have to code it in VHDL or
Verilog maybe run it through a logic
synthesis tool or something like that
that's generally how you do it or you
you buy somebody else a chip from
somebody else who did bet but if you
start looking at this so 640 by 480 it
takes a 25 megahertz pixel clock that's
still too fast for software and it's
actually too fast for this software for
this particular chip which we only got
running at 50 megahertz so to generate a
pixel every other cycle is a bit much
for the software to do however what so
what we did is the typical thing we put
a shift register in and so the processor
only then has to feed it another eight
pixels every eight cycles and in fact
you can do something useful in eight
cycles or 16 cycles I forget with what
clock rate we actually got this thing
running out at the end so this is a very
busy slide and don't try to understand
all of it but the main thing is down
here the main loop here under character
contains one deadline instruction and
this one loop in here basically it says
go fetch the character go fetch out the
font load it into the shift register and
then wait for the next time around and
by putting the deadline instruction in
there you're guaranteed that it's going
to write to that shift register you know
every eight cycles in this case and the
result is you type this stuff in you run
the program
yep sure enough you get a nice stable
video display coming up and previously I
there have been I've seen a few other
cases where people have said okay let's
do video and video generation and
software and again they they end up
doing something like this typically what
they do is they very carefully count the
number of cycles that each instruction
goes they're using some very simple
microcontroller where you actually can
count the things and they add a lot of
do nothing loops and no op instructions
to make the timing come out just right I
would argue that this is easier to
program one other interesting comparison
is the idea for this started with a
bunch of VHDL that I've written as a
demonstration from one of my classes and
this particular thing even though it's
coded an assembly and you should be it
should be able to get it even tighter by
coding in c say replaces something like
450 lines of VHDL so this was another
argument that you can code the software
you can called real-time software a lot
more efficiently than you can code
hardware now arguing that you've made
something smaller than VHDL lists like
shooting fish in a barrel if you've ever
coated in it it's it was clear it was
being it was created by people who were
paid by the keystroke to you know all
the identifier zar like this all the
keywords are long nevertheless I think
it's easier another application and to
me the speed at which my student was
able to put this together so I told him
go do the video stuff and what did the
video stuff I said oh yeah that's pretty
good here's another application go build
a serial receiver something it'll take
rs-232 decode it and then you know do
automatic baud rate detection and all
the rest of that stuff it came back to
me in like a day or two and had the
thing working it said oh you know you
know I'd send this data it picks it up
and this is kind of neat this actually
has a variable timing loop at the core
of it where you're handing it you know
depending on the baud rate that you've
guessed it will go and do the sampling
at that particular rate so you're
handing this code it goes and calculates
the baud rate turns that into something
to load in the timer registers and then
runs itself at a different speed
depending on what what that broad rate
runs so I that's a well-known trick
there's nothing to magical about that
but again
what impressed me was that the student
was able to take this idea coated up in
assembly language in this idiosyncratic
assembly language that we had in a day
or two and get this interesting
real-time software running and I don't
know of anybody who's done something
quite like this and so we have a room
full of these boards for a class I teach
runs at 50 megahertz pretty modest
unwind very sloppy processor designed
you know if it was a processor design
class he would have been kicked out for
it but you know this was not the point
of the Act the activity but it runs so
to summarize the vision for all of this
like I say is let's make performance as
predictable as function let's make it so
that you can so that there's a simple
model for what each instruction in a
processor does to the point where you
can you know very easily in a compiler
say compute the number of cycles that a
particular operation is going to take
and be able to predict exactly how fast
something is going to go and so that's
at the is a level and the question is
you know how do you fix all these other
things well you know instead of cash you
got to do scratch pads instead of
traditional pipelines you want to do
thread interleaf pipelines where there
there there no bypasses and collisions
and so forth and I've talked about all
these other things now I want to leave
you with one final provocative
hypothesis and I'm hoping we can start
an argument off of this so I've been
targeting real-time applications you
know hard real-time applications where
if you miss a deadline you get a you
know get a system crash with a capital C
our hypothesis is is that if you have
more predictable and in particular
repeatable timing in parallel systems
it'll make it a lot easier to develop
them right you might still have data
races but at least who wins the race
will be consistent each time right if
something goes wrong it will make sense
to be able to rerun the program and get
the same answer if you gave it the same
input
another argument is that if you're going
in and trying to tune the performance of
something having the having a much more
predictable architecture where the
performances is at least reproducible
will also help you a lot right I i do
compilers and one of the big challenges
is is so you do some optimization and
how do you know whether it actually
helped right you do some little people
you know twiddling around or whatever
and how do you know that you didn't just
shoot yourself in the foot by
inadvertently making it so that this
this particular instruction spilled over
and wait a cache line and collided and
made this pipeline not work and then
confuse the branch predictor and all the
rest of it it's difficult to understand
that if we had these predicates xers it
would be a lot more predictable so i'll
leave you with i'll leave it at that and
hope we can have some interesting
arguments on this thank you please
particular a lot of applications that
use threads for performance that's yeah
I would I would agree with you there now
it's the advantage is there would
certainly not be as strong as they are
in the in the real-time environment
certainly on the other hand perhaps the
reproducibility and the rest of that
would would help a little bit but
certainly not to the same extent but
you're right that's not a particular
application area on the thing didn't go
after with these I mean certainly you
should be able to code stuff like that
but you're right the advantages wouldn't
be huge please well so this the grid I
had at the beginning let's see if I can
bring that up
I guess I didn't explain that so the
idea here was that this this grid would
be a chip and so this node would be well
sort of what we're expecting on a chip
at this point but the whole chip would
consist of a whole bunch of these nodes
in some sort of communication mesh like
this so in essence the CPU yes yes
absolutely absolutely absolutely well
yes and no right I mean you go and buy
you know a high-end Intel at the moment
and it has a whole bunch of cache memory
now you're right that's it's nowhere
near as dense I don't know whether it's
as fast as what you would get with the
DRAM zand certainly not as dense as
flash or something like that and so
absolutely I agree on the other hand the
advantages of having slower well smaller
memory nearby the CPU is clear but
clearly for the huge amount of memory
that you're eventually going to want
it's going to have to be on ship off
chip somehow and so like I mentioned
there's this issue if you know what do
you know what happens when you connect
up a ddr2 to it or something like that
and the answer is you got to be careful
and and allocate the the behavior among
them however I think it's I think it's
clear that we're always going to have
some memory local to the CPU no matter
what and then the you know absolutely
this is not to scale and exactly how to
distribute all of that and allocate all
of that is a tricky thing certainly
you're there for ya now you're inspiring
them please proceed at your time or it's
something that's why the time or what so
practice the the vision I had was was
fairly conservative in fact so it would
probably be like the way FPGAs are used
at the moment which is basically at boot
time right most of the fpgas there's
actually typically there's like a little
serial flash memory hanging off to the
side that loads them rather slowly when
you power the system on but you don't
notice it after that and so the main
application of these things is not
reconfigurable computing or whatever but
basically so the manufacturers can
realize geez we screwed up let's send
out a new firmware patch or something
like that and so my vision more is that
what will happen is the customization
the FPGA will happen by whoever buys the
chip and that sells at the unconsumed
but probably you know the configuration
that FPGA is going to be a big part of
what they're selling to whoever buys
these things in the end so having that
change based on the application that's
running or whatever app probably not my
feeling is that more and more and and
certainly if you look at the fraction of
CPUs being used most of them are being
used for a single purpose write whether
there's one programmer one set of
programs that there they are built for
they run that for the whole of their
entire life and that's it and again very
different from the desktops they weren't
accustomed to and so I think this model
of you keep the the FPGA contents more
or less constant modulo oh geez we got
to fix it or what have you but it's
probably gonna be used like that
on your list programming languages
mm-hmm and we see the assembler right if
you have any thoughts about right right
a real a real programming language
definitely so the vision I want is have
basically GCC dash timing and you know
maybe pragmas or something like that
where you can insert timing constraints
within your code or something something
along those lines so at the very least
what I'd like to be able to do is have
something like the equivalent of G prof
or something like that that would stat
it well gee prof is dynamic but
something that would statically go in
and actually tell you ok this is how
long this chunk of code is going to run
or whatever and then you realize okay
you know this is not fast enough a
better go fiddle around weather or
something like that but I think for a
large to a large extent you should be
the base programming language ought to
look like see whatever now you know
going above that you know to me see
should be the next assembly language for
these things then there's a question
what generates the sea and how you do
that I have a few ideas about that
that's a whole nother talk it's not at
all clear how to do that and clearly
programming these things is going to be
a challenge but programming the next
generation of chips is always a
challenge that seems so that's the idea
well so at the moment if you miss if you
miss a deadline what happens is the
counter that timer counts down to zero
and that's and then stops at zero and so
when it hits the next deadline
instruction it just says oh it's already
at zero it just runs right through it
now that's probably not the behavior you
won what you might want is to throw an
exception when that happens so that when
you when you're testing her program it
immediately tell you you know bang
immediately you missed a deadline here's
the problem go fix it right you know
missed a deadline at line 57-year
program or what have you that's one
possibility another possibility would be
to you know keep it the way it is but
then you know have some other larger
counter that would tell you well okay
you know how much how many times did I
flip over other people have proposed
things like well if you miss this
deadline but then you can come in ahead
on this next one then maybe you're okay
for more soft real-time stuff there's a
variety of things and clear you clearly
you want some choices and all of that
the idea here is not to sort of argue
that I've got all the details about what
those instructions ought to do but more
just we ought to have this ability
somehow right right it's you know like
like most things you put in a processor
you want to be able to change its
behavior a little bit
no more arguments I guess it's too late
in the day</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>