<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Pipes: A Tool For Remixing the Web | Coder Coacher - Coaching Coders</title><meta content="Pipes: A Tool For Remixing the Web - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Pipes: A Tool For Remixing the Web</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qUINn5evcZM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today we have a tech talk by the team
that built yahoo pipes Pasha sadri
Jonathan Trevor Edward ho and Daniel
raffle and there's a wonderful quote by
Tim O'Reilly he says Yahoo's new pipe
service is a milestone in the history of
the internet it's a service that
generalizes the idea of the mashup
providing a drag-and-drop editor that
allows you to connect internet data
sources process them and redirect the
output so now we get to see whether Tim
O'Reilly was right here's Pasha blue
testing ok hi everyone have to say it's
a honor and a pleasure to be here giving
a google Tech Talk our talk today has
two parts the first part will be by me
it will be a high-level overview of
pipes as a product and the second part
will be by Jonathan Trevor who's going
to give you some of the details about
the implementation and it's a great to
see such a turnout at relatively early
on Monday morning all right so with that
I'm going to start my part so the story
started a few months ago when I was
looking for an apartment and not just
any apartment I was looking for an
apartment near a park now if you think
about how you would go about doing that
it would probably go something like this
you'd go to Craigslist apartment
listings and you'd go to say Palo Alto
apartments you go down to each listing
click on the map link and go to Google
or Yahoo Maps and check to see if that
apartment is close to a park or not as I
soon find out that's a very tedious
process because there are new apartments
constantly posted on craigslist and to
do probably go through 20 apartments
every day it's very time-consuming and
being an engineer I immediately started
thinking well how can i solve this
problem for myself technically it's
possible because craigslist has a RSS
apartment feed
for any of their searches and you can
use a service like yahoo local to search
for pipe parks certain types parks
within a certain geographic region and I
could actually write a little Perl
scripts to mash up the two data sets
together and solve this problem it took
me about an hour to to write and debug
that script and I had it going when of
course again being an engineer I started
thinking how can I further generalize
this so my script was basically taking
one data feed and mashing it up with two
web services that Yahoo geocode our web
service and the yahoo local web service
in other words this was a mash-up that I
had to write and as you know mashups
have been on the scene for the past
several years housing maps and Chicago
Crime are two very well known examples
of mashups that people have built
recently but now would it be nice if
there was a more generic way of creating
mashups of taking feeds and serve web
services and combining them to make more
useful information out of what's
available out there so this is where
pipes enters the picture pipes is a free
online service that lets you take data
feeds and services and mash them up
together and to create new data feeds
and hopefully that data is more useful
for the end user or the developer
whoever's putting the pipe together so
going back to the example that I started
with craigslist and yahoo local both of
them provide some kind of a API access
to their data and the only thing that we
really need is some kind of a glue that
allows you to make combine that data and
manipulated as we need so with that I'm
going to give you a little demo of the
apartment near something should we
didn't test this before we started but
so this is a pipe that i built and this
is one of our example applications built
in pipes it is apartment search
application that uses craigslist and
yahoo local and i'm telling you to
search for apartments in palo alto and
filter out the ones that are not at
least within two miles or let's say one
mile off at park should have hit the
wrong pipe a little bit earlier i love a
stocking so this is going to craigs list
and then geocoding all the craigslist
apartment entries and then doing it for
each apartment doing a search on yahoo
local to find nearby parks and doing a
filter on the output you can see i can
actually click the results are coming
from craigslist and hopefully this is
within a mile of that part so we can now
drill down into how this pipe was built
i'm actually going to create a simpler
version of it first
so first let's start with craigslist can
go to apartments peninsula search for
palo alto okay so if i scroll down a lot
of web pages have this RSS symbol down
there which is very handy
I can put in an RSS or atom or rdf URL
into the fetch feed module and it will
immediately go and retrieve those
results for me now these things don't
have a geolocation in them so we need to
the first thing we need to in order to
enable something like apartment near
something we need to geocode them we
have written a location extraction
module that tries like a different a
dozen different ways of trying to figure
out what's a location associated with an
RSS item so I connect these two together
and run it again
Oh
and if I drill down we see that each
item has been annotated with a wire
location which denotes the location of
that item coming through now we have a
looping construct can connect these ones
up now for each apartment that's coming
through I want to do a yahoo local
search and basically say find parks
within one mile off
well this is doing a lot of web services
right now in the back
see five months so now the output of
this here by Park to refresh so what
this is doing is for each apartment
doing a loop and doing a search on yahoo
local using the location of that
apartment and searching for parks within
five miles of that and annotating that
item with the first result that comes
back from yahoo local i can show you
where that is so the nearby park is not
an object that was returned from the
yahoo local search and it has this handy
distance element hopefully yes which I
can use to later filter out the
apartment on so the last thing I need in
this pipe is basically a filter
oh okay so that's what I was gonna say
this is a simplified version of the demo
in the full-blown version and the actual
yahoo local results that is wrapped in
another sort of filters that actually
make sure the results are parks and not
just more ballparks but this is a
simplified version for d you know that's
the like one or two hours I spent
debugging my perl script you know half a
hour of it was about the mobile home
parks so the last thing I just need to
do is the nearby park distance is say
less than one and that's the pipe output
finally I can now save this thing when
we save a pipe we actually executed in
the back end and annotated with all the
different data sources that it uses so
that we can offer a navigation on the
public pipe study yahoo com site so that
you can say which pipes are using say
craigslist apartment search and run to
the preview and this is going to
actually run that pipe for me and show
me the results so putting together pipe
is pretty much as simple as that I can
use this pipe for my own use I can take
the output as RSS or add it to various
RSS readers that are out there or if I
want to use it programmatically we also
offer a JSON output of the pipe output
now the next step after this becomes as
if I think that this pipe it might
actually be useful for other people as
well it would be great if I could
someone share it with other people right
now the values parks Palo Alto the
distance to the park are all hard-coded
in my pipe and that's not going to be
very useful for a lot of people
start looking exactly for the same thing
but there's a way to actually
parameterize things in pipes which makes
these more reusable by other people so
I'm just going to show you a quick
example here i'm searching for parks I
can actually parameter eyes that see
parks any place that I have a hard coded
value i can actually drag in a variable
and let's do so instead of parks i'm
actually i have a user input save that
again to see the result
so this time when I try to run the pipe
you see that we have auto-generated a
user interface for that pipe based on
all the user input elements and I can
send this URL which is publicly
accessible to my friends and they'll
hopefully be able to use this pipe to do
something useful so that's the demo
portion of pipes I'm going to jump back
to the presentation so now that you've
seen that the example that I gave just
used one feed and two web services but
in fact are probably millions of feeds
out there and thousands of web services
available so if you think about all the
combinations that are available to be
made from these services mashed up
together you kind of get a picture of
what's possible or why people are
excited about pipes certainly we are
excited about it and in fact there's
been all this great services that have
been offered over the past few years GBC
Bay Yahoo all that and what they have in
common is that they have all these api
is available and while they cater to the
their own audience and very well they
still leave a lot of unmet needs in the
form of the spaces between these
services and pipes aims to fill up these
spaces by creating mashups between the
different data and services available
out there so now you know what pipes is
all about I'm going to talk a little bit
about our design philosophy and putting
it together so the first thing we
borrowed heavily from unix pipes in fact
the name pipes is a play on unix pipes
as many of you know unix pipes basically
allows you to combine very basic
primitives and build up useful
applications this is a model that has
worked very well in the UNIX world for
the past 50 years or and we thought that
since we are basically flowing
processing streams of information it
could very well apply to feeds that are
available on the web our goal in general
was to build a minimum number of modules
that may enable the maximum number of
applications so
a lot of our built in modules in pipes
very closely follow the UNIX standard
and grab unique sort etcetera things
like that the other part of our design
philosophy was to work with as many
different input sources as possible so
I've shown some example services
therefore in terms of formats we support
RSS Adam JSON and just recently
arbitrary XML data structures and we
hope that you know people who have some
data that they want mashed up and will
offer it in a way that can be used
through pipes the other design
philosophies to support as many
different output formats as possible so
currently we support our SS Alfred we
support JSON output you can subscribe to
receive email alerts or SMS alerts or
you can put the output of a pipe in a
widget or a badge that you can put on
your website you can put it in a reader
you can put it on your personalized
homepage we have a hosted version of
pipes that you can come and execute a
pipe this is the rendering is similar to
web result set and if the feed the
output feed contains your information
you can actually plot it on a map so the
general attitude has been to try to
support as many different output formats
as possible and we hope that people are
going to start using the outfit from
pipes in their applications just as a
another element in their data processing
pipeline and finally we have this idea
that we want to have an open processing
module we have a number of built-in
processing modules it would be great if
you have a web service that does some
transformation on data you can plug it
into the yahoo pipes service and expose
it there so that other developers can be
aware of it and use it in their pipe
applications so overall by supporting
any data input any kind of processing or
any kind of use of the data output we
are aiming for a very open architecture
where we work with anybody's data and we
allow the data to be used by anybody
who's interested and the general idea
here is that by supporting the maximum
number of inputs processes and outputs
pipes will become more useful to people
the more of those that we support the
more a number of combinations that are
possible and some of those are bound to
be interesting so some of the examples
of interesting pipes that have been put
together recently that we are aware of
one of them is a data mashup between
last.fm and flickr so somebody took
actually Mr Speaker I don't know the
real name of that person took the data
feed from last.fm which is the recent
tracks played by a user put it through
pipes and looked up using content
analysis and flick your API use find
pictures related to each artist that was
being listened to by that user and took
the JSON output and parked it into a
greasemonkey script that inserts a badge
back into last.fm so as you are browsing
around on last.fm using this
greasemonkey script each user's profile
page gets augmented with this badge of
their Flickr images this is one of the
more interesting examples because it
uses so many different components happy
to see it another interesting example
was second life this is sort of i didn't
expect pipes to end up in second life
but max case built a translation service
because in second life you were likely
to run into people who don't speak your
language so he built an object that does
translation between different languages
and the actual translation is done
through a set of pipes and that's it for
my part and now jonathan is going to
talk a little bit about the
implementation thanks pressure
okay so when we were creating this
originally we actually had a significant
advantage over a normal web site
approaching this one that where you were
one of the assumptions we could make was
that we were targeting the top of the
the web two point O pyramid so you know
right at the top you've got the one
percent of developers which is probably
like ninety-nine percent here below that
you've got the remixes and the bloggers
another sort of eight or nine percent
and these are the people that we
actually wanted to target so because we
could we could do that we could assume a
lot of prior knowledge we could assume
they've sort of familiar with publishing
the idea of getting these things out
there we've got assume they're familiar
with loops and types and typecasting and
putting things together even the notion
of a mash-up is fairly familiar to these
types of people so while the developers
are the people we were targeting here
and there the people are going to
benefit directly is the people who are
going to consume the outputs of these
pipes whether it's in a badge whether
it's in a map they're going to get
indirect advantages of this so this is
the sort of fifty thousand foot view of
the architecture there's nothing very
surprising here obviously the sources at
the bottom are the most important things
that we have without the sources that
you know pipes doesn't do very much this
is where all the data comes from we have
an engine which is where all the heavy
lifting is done this is where the mashup
and the pipe definitions are run we have
the website which is where our community
this is where you can find pipe this is
where you can build pipes this is where
you can run your pipe this is the place
where most users actually interact with
the system and then finally on top of
that we have the clients whether these
are badges whether this is a web browser
whether it's part of another application
point is we don't we don't really mind
what consumes pipes we just want to make
it possible
so the engine itself getting the heart
of the system this is where it takes a
definition and it does all the hard work
this is where the I build the editor
this is where I just get to say oh
there's some problem with the back end
when it goes wrong it was my good answer
generally so pipes are defined by a very
very simple definition format it's
nothing very elaborate it's literally
what you see in the editor is it pretty
much goes down to the definition format
and the execution is paralyzed as much
as possible for performance clearly
caching is very very important too so in
the previous architecture I didn't show
they show the cache layers but we have
caching everywhere to avoid just
pummeling the services with too many
requests to avoid as going out to the
places basically to keep it going fast
so while we were pitching this very much
as an RSS service the engine itself
doesn't care about RSS it's really
processing XML or JSON or data sort of
strokes arrays and scalars so it's not
really limited to RSS and that's
something we will be exploring in the
future so the editor is where the pipe
definitions can get created it basically
just post them back to the engine so
while the editor is this nice great
fantastic thing that people can use to
create pipes very rapidly we could
actually we could actually write them by
hand in a text editor they're actually
not that complex the heavy lifting all
done by the engine the editors a very
thin view on top of all of that data
errors are all thrown by the engine
executions are all done by the engine
the editors are some type checking to
make users actually do the right to not
create bad pipes but for the most part
it realizing the engine one of the
things when we were starting out was one
of the things we observed is that now
browser technology has moved on so far
there's very little that's no longer
possible when we started off doing this
design we never said oh we can't do that
because it can't be done in this browser
or we can't do that we had a very open
mentality and I think there's something
we're going to see more and more
powerful applications like the editor
coming out so one other thing we were
very keen on doing was to make sure the
editor was easy to use we're targeting
the top
ten percent and the top 1% I'm sure
could probably make do with a basic tool
but the remixes the bloggers these other
people that are further down the web
point no to term it pyramid we really
want to support and make things easy so
they can just do these mashups as they
want to so we did this in a number of
ways one was to highlight target a
formant affordance so when you're
dragging the lines around the lovely
curvy lines that you're not just
randomly connecting them to different
terminals and there than just hoping it
will work it actually shows you what are
the valid terminal types that you can
connect to this was more of an
accidental discovery but is something
that once we realized we were doing we
were keen to keep going and that's our
the left-to-right readability so when
you're browsing and looking at a pipe
you may not understand at a glance what
it's doing but you can have a look at a
module and just read it to sort of get
some basic understanding of the actions
is performing so the example here is
fine pizza within certain magnum two
miles of a location Santa Clara one of
the other things we were very eager to
support was to avoid people going away
from the editor it's very if you're
going for this whole ease-of-use thing
the moment you have to leave the editor
change state open another web browser
it's really blown the whole experience
there's a lot more fiddly it's a lot
more painful so one of the ways we
avoided people having to leave the
editor was through inline searching so
this is a lot of inter search so you can
just type in the name of any keyword for
a feed or a pipe and it appears in the
top left of the editor and you can just
drag that on as a source module the
other bit that proved very very
important was what Pasha was
demonstrating is the inline debugger so
every single module that every single
stage of the pipe can just be clicked on
and inspected instantly there's no I
have to change page to debug it I can
just click and see what the output is
tweak the module refresh run it again
and I've seen what my effects have done
there are a few other people that are
trying to do similar sort of things in
the space create mashups or trying to do
data processing and data editing data
flow one of the things that
differentiates us is the instant on so
the moment you put a user to say when
you ask a user to download something and
install something you're going to lose
people you've already made that
experience a lot more painful than
people like nowadays again with the sort
of assumption that you can do everything
in a web browser we wanted to support
this instant on experience so with the
editor that you saw Pasha bring it up
you just clicked on create pipe and any
normal browser will just pop up the
editor and in your away there's no delay
you can just get it going data flow
applications are clearly very well
suited to visual editors so I'm sure
everybody here is familiar with sort of
linking of audio flows or video flows in
processing systems that was something
that we we capitalized on and finally
one of the big things that was it's
actually helping our success at the
moment is the view source approach so
HTML was when it first came out 15 years
ago 10 years ago was not many people
didn't know what it was they were very
very few tutorials but what you could do
is you could go to a page you can see
something that was sort of what you
wanted so yeah that's pretty good you
could right click view source copy it
paste it into your page and now you have
that and now you could tweak it a little
you didn't really understand what the
tags did but you could tweak it and it
looked nice and it did presented the
information you wanted to present so one
thing that we do is every time you can
see a pipe you can always visually
inspect it you can see how that pipe
runs you can then copy that pipe you can
add that to your own library you can
make the tweaks so pipes can propagate
that way and what this leaves us with is
an ID like editor that we hope many
people can use during our launch we
found that we were possibly pushing down
much further in the pyramid we were
getting people who'd never coded before
using the editor to put pipes together
which was a great win for us something
that was a very pleasant surprise now
I'm going to talk a little bit about the
why
is the one of the more technical parts
of this talk so Firefox Safari web kit
opera all support canvas natively canvas
is just a 2d drawing lake and draw
Bezier curves you can draw rectangles
you can you know you can draw any type
of 2d shape on there I x + 7 don't but
thanks to you guys we didn't have to
worry about that so thank you very much
and there's obviously some some docs on
the Mozilla site for how to get going
with that now because it's in a web
browser obviously we have this use you
know nothing ever works properly in a
web browser you always fiddling with
something so there are lots of cross
browser issues if you're just doing
static canvas tags there are very few
issues but the moment you start resizing
the canvas and I wanted to do those
things you hit a number of issues so in
Safari you actually need to completely
destroy the canvas and recreate it from
scratch every time you change its size
in IE the your wrapper doesn't actually
clear the canvas when you want to draw
different lines on it so you have to
manually clear it so it's a host of
little problems but they weren't they
weren't a big problem CSS is an
interesting problem so nowadays we're
used to separating out the the visual
language from the actual structure that
you're putting on the page and canvas
tags are just drawn using javascript
which is sort of code embedded in the
page is loaded so it's very hard to
tweak the look of something without
going into the code and just redoing it
so rapid evolution when you're trying to
create these things it's very difficult
so this is something to be aware of
performance is an issue with the canvas
tags we gain some advantages by assuming
that developers are going to be the
people using our tool because developers
actually tend to have higher end
machines and tend to upgrade their
browsers fast so that's a nice little
side effect that we got the biggest
problem with performance is the moment
you have any transparency and
overlapping canvas tags performance just
disappears it's really nasty so
something you have to avoid and the
biggest problem with the canvas tag is
that because it's you might be drawing a
nice simple thin line but if you it's
actually taking up the full bounding box
of a rectangle so the moment you
actually put these on top of anything
you've just swallowed all of your Dom
events and you can now no longer
interact with anything on there so this
is a one of the biggest problems that we
had and we reiterated several times I
just want to spend a few minutes on that
so first of all we side off with a
simple one which is the lines above this
is actually the design we wanted to go
with so you can exactly see where the
lines were hooking into the modules
unfortunately all your events get
swallowed you get no hover event to get
no click events everything swallowed
underneath these lines that are at the
top of your Dom so one thing we started
to do was to manually dispatch the
events into the browser using some
custom JavaScript code and I can tell
you that's a really bad idea it's just
doesn't work it gets gnarly across
browsers very very rapidly however
things are almost impossible to
calculate mouse in and out very very
difficult if we could somehow get the
w3c to implement a tag that's like
ignore events on this element that would
be really handy so we abandon that one
then we put the lines underneath and
obviously the lines underneath you have
the problem that you can't actually see
the lines going into the modules and
then if you drag the modules we wanted
to raise them up so you can at least see
where they were that worked okay but
again you just couldn't see where the
lines were in this other mode and when
you raise them up you lot you lost all
the interactivity again so we're still
the same problem as before the moment
they go to the top you couldn't drag a
line on to something and drop it if
there's another line that was obscuring
it with its bounding box then we we
played with a couple of ways of making
the modules transparence the idea here
is that as you look at the module you
can see a line going underneath and
entering it so this would solve our
problems you could see lines going
underneath modules you could see where
they connected visually it works
unfortunately the problem with
performance that I talked about earlier
is a huge problem the member we make
modules transparent and you start to
move them around performance drops to
single frames a second the solution that
we tried for a while was to make the
modules solid on drag so we leave them
they look we leave them sort of semi
loot semi-transparent to start with so
you can see all the lines but they look
a bit faded out and then when you start
dragging we just make the modules solid
and this actually turns out to be a
really really weird
fact it means that when you're in a
transient point in the user interface
everything gets solid and when you've
finished everything sort of fades away
to look a bit washed out so that we had
that one for a while but in the end we
decided that wasn't the way to go so the
solution we went with in the end was
this sort of a semi again lines on the
bottom and everything remains solid but
the moment you mouse over the bounding
box of the line so the like this lines
bounding box would be sort of this
square here and this lines bounding box
would sort of be like this square here
when you mouse over the lines the
modules are either end fade out so you
can at least see where the connections
are for the line that you're over so it
allows you to sort of see where they're
going into a module but we keep the
advantages of them being on the bottom
so I'm going to finish up by talking
about some of the the ongoing challenges
that we we have and I think a very
interesting every time we look at our
feature requests through a more and more
feature request for scripting support
for doing more and more things with
pipes and what we're fighting here is an
ease of use versus Power so many people
can come to pipes non-programmers we've
seen can come to pipes start using it
build their own data mashups put them on
their own site create their own content
but the more powerful we make it the
harder that gets to be and that's
something that we we have to be aware of
at every step of the way we have they
have this problem of scale at the momen
we're getting all on all of these
interesting pipes created but how do you
actually find the right pipe for you
how'd you dig down and find the one
there's a component that you can use in
your own pipe or uses the API that you
want to use so discovery bubbling things
up with sort of interestingness like
flickr does with photos it's something
we need to address api rate limits and
keys are just a small but significant
technical issue we execute everything
from this engine it only has a finite
number of IP addresses and most API ziip
rate limited so that's something we need
to think about and once users have their
own accounts and they want to do
processing on their own data we enter a
sort of an authentication problem so if
I want to do something with my email and
read it through say the Cascade API
yahoo has that's great but then how do i
give the credentials to the pipe how do
i make sure that if that surfaces as a
badge somewhere that somebody else can't
read my email so there's lots of joyous
issues with the authentication so last
week we added support for more data
formats not just RSS and now we can take
in xml and json so it's any type of xml
whether it's kml rest and so on so
there's a new fetch data module the
output the system at this moment is
still RSS or Jason but the input is now
pretty much anything coming soon
hopefully we'll get this extensible
module for doing arbitrary processing
the middle of a pipeline that Pasha
talked around and we're going to have
some just more easy to pick up renders
just to make it easier for the end users
that are coming to our site not the
developers which is who we were aiming
for but the end users who would just
found a pipe that they want to add to
their blog as a badge they can just pick
that up and use it so what pipes has
allowed many people to do is to solve
their own individual personal problems
you can you know the lots of people are
solving the big big problems but the
problems that I have like searching for
a sofa of a certain type within 10 miles
of my house so I can actually get at
home there's pretty much unique to me
nobody else probably cares about that
but it's really difficult problem to
build and solve but with pipes I can
build that instantly I can start
addressing these very high value
problems that I have that are very small
and focused we've started to see
something like user-generated features
appearing on websites so this is where
if somebody actually provides a mash-up
between various sources the application
provider can take that pipe doing that
mashup and insert it back into their own
site so you're starting to see that
people are building parts of
applications that have sort of been
pulled back in and we might be entering
this is a very / overused word at the
moment or overused phrase but disposable
applications it really is easy for me to
build something that I may only run one
time I might run it two times I might
share it with three people and then it's
done
but that's fine so we might be entering
a new era of disposable applications so
thank you and I'd have to take questions
so I think the real question that
everyone wants to know Pasha is did you
find an apartment and is it near a park
well it was somewhat of a hypothetical
example I I did actually find an
apartment I i live in app altar and
there are a lot of parks so he can't
help but be close to one but the apple
of the pipeworks you know so i have a
real question which is about caching and
i want to know do you run these pipes
just when the output is requested or do
you do some continuous processing in the
back end at different layers to keep the
data flowing now we tend to do it we do
it on request we don't do any continuous
processing at the back end but it
obviously goes through many caching
layers so when it's requested a few
times within a certain time frame you'll
learn to get a cached version of it I
think the reason for that is much of the
web actually works on their pool model
and we felt that just continuing with
that is there probably the simplest pass
on this problem to follow up on them
really curious about the performance
side back to you saw in general both
throughput and user latency like for
example for cold pipes that haven't been
requested recently you could possibly
need to go fetch 10-15 RSS RDF Adam
sources do you see poor end user latency
do people care or not that kind
so we try to address that problem as
much as possible by paralyzing the
execution of the pipes so if the pipe
relies on ten independent data sources
we try to exit fetch them simultaneously
in parallel and we do run into the cold
cash issue occasionally some of the more
popular feeds are obviously shared
amongst many pipes so the caching layer
is basically a transparent caching web
cache layer so we sometimes get hits on
popular sources the other thing the
problem is somewhat mitigated by the
fact that the output of the system is
RSS and it's often put into an RSS
reader so it is fetched asynchronously
on the user's behalf so the next time
that well if you wanted to use it in a
sort of a real-time application it would
become more on enough issue we can
probably do some things to optimize
further optimize the fetch sequence of
feeds or even one day be more proactive
that we know that you're requesting the
pipe every 10 minutes so we try to keep
up with the data sources as you're
updating more of a high-level question
how did how do you see this fitting in
with Yahoo's kind of overall API /
developer strategy well I think actually
pipes fits very well in there because of
you what pipes does is exposed a lot of
the api's that yahoo has developed in a
very easy-to-use format I think we've
seen an increased usage level of many
yahoo api's that are featured in pipes I
want to add that we didn't intend pipes
to be yahoo eccentric in any way and
we've gone we've made a point of
including a Google base module in our
default module list just to illustrate
that point hopefully over time you'll be
able to add more third-party modules in
there as i mentioned i think the power
of pipes comes from being able to work
with as many api's that are available
what kind of discovery capabilities to
have for you know finding feeds finding
existing processing and stuff like that
yeah I mean it's one weakness we have
now the the current website you can
basically browse by something like
popularity but that's not really a good
way of actually defining a useful pipe
for somebody or something to find the
right pipe that they want we do store a
lot of information about a pipe once
it's run which is why the save is a
little longer than it might be so we tag
what sources it was fetching so you
could you know find find me anything
that uses google com and pulls from that
source we do have a service at Yahoo for
searching for feeds so we do bring that
into the editor I talked about that
briefly so once you type in a few
keywords it'll try and bring in the link
to that feed and you can just add it to
that but discovery is a it's a problem
at the moment first it wasn't clear to
me but is there a way to join two
sources in your pipes yes it is oh I
mean you mean as in a sequel joint or
just merging the streams into each other
merging and sort of doing something on
on the union of the two streams so let's
see it's certainly possible to combine
two feeds into each other I can for
example there's a Union operator I saw
that but so does it let you get items
from two fields that have some common
element and then do processing which is
on data from both the feeds I see so you
mean take two items from different feeds
and then maybe merge them into each
other or create sort of one item out of
it and then do some processing on it so
me these two have a joint module and
that was too complicated to explain to
our initial target audience we may we
introduced that back in once we are able
once we've simplified a little bit that
we have an extensible system where we
can introduce new modules
so if you have a request actually for a
module it sounds like one please and
we'll build you'll be happy to get in
touch with you and try to implement it
but while you were giving the talk I was
just thinking of an example that look at
the news articles and get images
corresponding to keywords in the news
article so that would require a feed
from Flickr and some new source like BBC
or yahoo news yes that that simple case
is possible we have it for each annotate
where we loop over items in a feed and
then annotate each item in it with the
results from another service or pipe one
of the things that I didn't actually
show is the capability of using pipes to
extend itself so i'm logged in here and
my pipes are listed here i can actually
use a pipe that i've previously built i
can open this so this is this is
actually a pipe that does the Craig's a
more sophisticated version of the
craigslist apartment search and I can
actually use it as a data source in
another pipe so one of our aims is that
hopefully there will be a library of
these reusable components that people
can build more high-level pipes on top
of but in this case you can actually do
something like for each annotate and
drop sa flickr module or in kind of
source module and each item going into
the for each module will be annotated by
the results of the inner module sure
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>