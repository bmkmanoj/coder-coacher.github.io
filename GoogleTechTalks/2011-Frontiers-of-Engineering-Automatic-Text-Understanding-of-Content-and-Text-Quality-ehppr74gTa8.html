<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>2011 Frontiers of Engineering: Automatic Text Understanding of Content and Text Quality | Coder Coacher - Coaching Coders</title><meta content="2011 Frontiers of Engineering: Automatic Text Understanding of Content and Text Quality - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>2011 Frontiers of Engineering: Automatic Text Understanding of Content and Text Quality</b></h2><h5 class="post__date">2011-09-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ehppr74gTa8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're moving on to our second section on
semantic processing and I would like to
introduce and turn it over to our chairs
Alexander Kuzmanovic from Northwestern
University and our monarch subramanya we
sort of practice that a moment ago from
here at Google so a gentleman please oh
thanks Andy that was that was pretty
good so uh everyone my name is amar
subramanya I am a research scientist
here at Google so first off welcome to
google and welcome to the session on
semantic processing so we have a bunch
of really nice talks lined up so I don't
want to take too much time away from the
speakers so what I'm going to do now is
just give a very brief overview of what
you're going to hear today and tomorrow
so semantic processing has contributed
and continues to contribute to a lot of
the technologies that we use every day
so be it having the ability to search
through a large document corpus so if
you wanted to look for a research paper
or if there is an article in spanish
that you want to translate to english
having the ability to do that are having
the ability to search through a large
collection of images so it's it's it's
had an effect on a number of things that
we do every day in addition to all that
we at Google are deeply interested in
semantic processing in fact tomorrow at
the lightning talks you're going to hear
some of my colleagues talk about themes
that are similar to what the speakers
are going to talk about today and
tomorrow so so very briefly we're going
to start off by looking at what we mean
by semantic processing looking at some
of the difficulties associated with
semantic processing of textual data and
then that's the second talk is going to
look at how user-generated content has a
affected semantic processing and vice
versa and then we're all going to go
have some really nice dinner and then
tomorrow morning we're going to come
back and the first stock is going to
sort of focus on how semantic processing
can be used in in the context of images
and then the second talk is going to be
about how we can use semantic processing
for searching through a large collection
of plots and figures and diagrams and so
on so without further ado I'd like to
introduce the first speaker professor
Aune nan khoa who's a faculty member in
the Department of Computer Information
Sciences at UPenn over to Annie hello
see let's start here and see how I move
so one thing that may be kind of
striking difference from the previous
session is previous session the big
title was edited manufacturing and then
everyone had in their title editing
edited manufacturing and then something
else kind of like what specific sub area
if you look at our titles we have
semantic processing and then none of us
has semantic processing in our titles I
just want to take like 30 seconds to
kind of explain why this is the case and
kind of what do we mean by semantic
processing so really when we kind of the
people who will be talking to you when
you say semantic processing the question
that we are saying is we have a document
that's in digital form and we have a
machine what can the machine do to
represent the content of this document
and the purpose of the representation is
to compare documents I find similarities
met appropriate documents so these are
the problems that we will be looking at
and kind of will see that the documents
will vary inform and
of the representation will be
increasingly difficult and complicated
so today I'll be talking about
representations for text and hopefully
this would give you some good intuition
to kind of actually understand the talks
that will come after me so automatic
semantics what it does it mean for a
computer to understand a document and
again I like in my presentation I'm
focusing on tags but more and more
online there are different forms of
digital documents more and more it
becomes important to represent images
and then go go cross model and kind of
match images with text so we need the
automatic semantics because computers
help us navigate data so we want
computers to be pretty good at matching
what I'm asking for my query and the
documents they return this is the most
obvious one and I'll talk kind of in
more detail about other kind of finer
applications but even more and more um
we actually read more computer generated
text so we would want kind of computers
like doing machine translation like in
the application that amaru is talking
about or automatic summarization which
is kind of area of increasing interest
and we want the computer to be able to
understand and kind of match our needs
compare texts or generate a text that
has the right semantic properties and I
think this is the last time i use kind
of semantic processing and then i go to
like the more usual terminology that we
use in our fields so we want some sort
of representation and a way of meaning
of documents and what we want from this
representation is free to let us match
documents and compare documents and
there are two aspects when we talk about
interpreting texts and understanding
text one is what is this tech
text about what is the meaning what is
the topic and this is exactly the type
of kind of search related question that
people in our area have studied for like
I think it's not exaggeration to say 50
years so some of this I will overview
now but there is also other aspect of
understanding that is kind of the
frontier of kind of semantic processing
and language processing and speech
processing which is if I see a text how
how good is the quality of the text is
this a tags that are easily understood
is this text clear is it informative is
it well written and actually this sort
of question is something that
researchers have only recently started
to look at and I'll talk a little bit
about the applications and this emerging
area of research so a lot of the kind of
basic ideas that many of the semantic
processing applications are dealing with
say we employ a vector space model of
semantics what it means is that will
represent our objects our documents as
points in space and the way we will
match documents or find similarity
between documents is to say points that
are nearby each other are similar so
kind of the goal here would be we have
our text what we want is a vector of
numbers and then we want to compare the
number so we'll see this in all of our
talks you'll actually see some clever
way of doing this and for information
retrieval where we have what the user
typed the user query and the documents
that are available on the web kind of a
really basic idea is to represent the
text as a vector of terms so each term
each word that appears in the document
is one dimension
in this vector space and each vector
just represent to what extent the doctor
covers is the document covers this
dimension so for example the sentence
above which can be a document kind of
losing document the term document very
loosely is represented by this so this
is my vector that represents the key in
the front pocket of the backpack is in
the key is in the front pocket of the
backpack so it has three does one key
and one backpack and so on and this is
the representation of the backpacks in
the car and kind of what is like this is
really simple idea so a document what
will count is the the the words that
appear in it and these counts would be
our representation and the neat thing is
now that I have a vector of numbers I
can use any sort of distance metric to
figure out which are the similar
documents ordered the documents that are
close to each other and then basically
we will see that there is kind of you
can build on sophistication but really
the bottom line is this like we turned
our text into numbers now we are using a
metric to compare these numbers and one
similarity metric that kind of works
well for text application is the cosine
similarity so once you get the vectors
is just this component by component
similarity that often turns out to work
pretty well for comparing text so you
can say well I understand this this is
neat like we just counted and we got our
vector and we can now compute similarity
so you can find what is more similar to
my query and you can give me this
document but this doesn't seem like a
lot of understanding really and it turns
out the exact same approach can be used
for things where one would be surprised
really a computer can learn this so
similar approach has been used for
learning
learning word meaning condition
supervised fashion so what we represent
hear a word is a vector of context and
the context can be the other words that
appear around that word so imagine we
want to know what is red wine and what
is green tea so now instead of looking
at individual documents you say I go on
the web and I want to see what was
occurring around green tea and what was
occurring around red wine so now you
accumulate this context which are words
that appear near by the word that I am
interested in and this is the
representation that we get and suddenly
it turns out like by this method that
again is again on and have count
represent and compare is vectors you
suddenly start getting really neat
results for example if you ask if you do
this method and ask a computer tell me
according to your accounts what are the
words that are more similar to wine um
these are the results that you get so
white wine red wine beer Chardonnay
fruit are the words that are similar to
one and you can say well this looks more
like understanding and this is something
that can be useful so for example one
application of this sort of kind of
completely unsupervised semantic
learning is imagine I wanted to do a
search for additive manufacturing and
this is what I type but as the speakers
were kind of in the previous session
they were kind of going back and forth
between additive manufacturing and 3d
printing so it would be nice for you to
compute like when this matching happens
is to know that 3d printing is also a
term that I could have typed but I
didn't type and by having these
automatically connect collected kind of
families of similar word actually the
computer I issue my
search it's additive manufacturing and
the search engine when it Maps document
to my query actually amends the query so
this is the query expansion with these
other terms that are very similar and I
might as well have typed them but I
didn't in order to find a good match so
it's kind of again the very same simple
idea but it's already kind of growing
and sophistication of what you can do
with this and similarly you can ask ok
so can I find synonyms so for example if
I want to analyze people's reviews how
do I know what is positive language and
what is negative language can a computer
ever learn like what what does it mean
for a review to be positive and a review
to be negative and it turns out with the
exact same approach this work that was
recently done at Google and kind of
there have been smaller scale projects
before as well but the same approach you
can ask ok I know that excellent and
amazing are terms that can occur in a
positive review can you do this I just
go online read a lot of reviews and tell
me what are the words that are really
similar to excellent and amazing and
this is what the algorithm finds in
fights cute fabulous top of the line
melts in the mouth so now if we want to
do sentiment analysis and review this
automatically build dictionary can tell
you what are the phrases that you need
to look for and this is really powerful
because what is the alternative there
thur ative is that someone sits and
thinks of synonyms of how can I talk
about kind of good things and bad things
but it can never be as exhaustive as
what kind of the actual analysis of
reviews has shown and similarly with
this like what are the words that are
most similar to bed and horrible I'm the
algorithm fine subpar crappy out of
touch so sick to my stomach
so unlikely that and this dictionary is
huge it's thousands of entries it's
completely unsupervised just based on
this model and then instead of just kind
of saying oh I want to represent all
words what we want is tell me which are
the words that are similar to this kind
of polarity words and now I can do
better semantic analysis of reviews and
figure out whether people are happy with
something or not so happy and kind of
another level of taking this kind of
machine learning paradigm is to say okay
kenna machine actually learned that
mason and stone is similar to carpenter
and wood because both of them are
relations between the two nouns and it
expresses a profession and the material
that this profession works with most of
you actually probably have seen like
this is kind of the basis of some of the
kind of intelligence test definitely the
basis of language competency says that
you can get on a jury or s80 exam and it
turns out that the exact same
representation this vector
representation r that is accumulated
from vast amounts of text can be used to
train a system to do some Alera t's like
this so here what you're representing is
the pair of words so this is the item
that you want to represent and the kind
of features that you kind of the
dimensions are patterns that these words
occur in and where do you find the
patterns a human will not tell you the
patterns but you would go and and if the
system just reads off a lot of text and
finds water the syntactic structures in
which these two words appear in and then
by comparing the vectors of these
representations you can say how similar
is the semantic relations between these
two words and you can see immediately
this is actually building much more
understanding the
because now we can say in this manner
one can build a kind of formal
representation that says like scientists
can be academic or industry scientists
this sort of structure like the type of
or works with the sort of encoding can
now be represented in computer form
through this sort of analysis and it's
kind of fun actually people have tried
this algorithm on safety questions and
kind of the multiple questions so you
have an example relationship and then
you're asked of these five which one is
most similar to the one that I'm showing
you and the performance is above the
average of the nationwide SATs course so
it's not amazing but it's pretty good
and I think it's amazing just to think
about the fact that the sort of actual
understanding is based on a simple
counting and this is an intuition that
kind of is usually very surprising but
it works so some of the challenges that
you'll hear more about its kind of oh
this is too easy certainly it's not just
counting yes it is too easy the way I
represented it there are too many
documents and too many words so
sometimes these representations are huge
how do you actually kind of fit them in
memory and we'll hear a bit about this
tomorrow for the words and picture
representation type work then reducing
noise I said each pattern that I
represent is a dimension but truly these
representations that I was talking about
our more than necessary you have more
dimensions than you need so how do you
know which ones to drop and which are
just noise and which are kind of
throwing off your similarity measures so
this is a research question or and of
Richard context can you use concepts
rather than words or can you use
syntactic relations rather than just the
window of words that occur is nearby
ah for the remaining time I want to tell
you more about this text quality stuff
that is something that I have been
working on which is can a computer ever
actually get an accurate estimate of how
well-written a Texas and it turns out
that some of these kind of frequency
based features that one can derive from
large amounts of texts are actually
applicable for some of these tasks so
what I'll tell you briefly about is
three case studies that we have done in
my lab one is just asking people here
are some text can you give us readings
and can we figure out what correlated
with your rating so it's kind of getting
started on research the other one is can
we get a bunch of machine produced
summarise and figure out which are the
ones that are written well and which are
the ones that are written poorly and
then um also can a machine like see a
text and figure out how general or
specific is this text it's a really
important application if you think about
it when you're issuing a search in
Google very often you don't want
something that's too general kind of
saying like very little information
about what you are searching for so
having the right mix is really important
so how well-written is a text we asked a
bunch of graduate students at Penn to
raise some texts and then this is 30
wall street journal articles and then we
tried various things that we could
computer automatically to see what
correlates with their ratings and as I
was talking last evening and today a
little bit people and I talked about my
work in text quality people ask me but
surely and if people don't agree people
can have different ratings so this is
definitely true and we have looked into
this before this kind of study in order
to get some meaningful results we just
average across 10 judgment so
kind of what we are trying to see say is
what are the type of characteristics
that would be kind of predictive of the
average assessment from people and one
thing that does not correlate with
ratings at all are the type of things
that if people have ever read or and if
we're interested in readability metrics
the type of readability metrics that are
used in education and in newspapers to
make sure that kind of a text is
approachable kind of understandable for
high school level or kind of college
educated people is based on this sort of
features the average character per word
so longer words are supposed to be
harder and average number of words per
sentence these have no correlation at
all and this comes like this is actually
because we have a bias sample we have
pen graduate students so these are very
educated people it's kind of beyond the
readability metrics it's not like saying
eighth-grade education high school it's
we are all other things is that we found
are not at all related or for example
subordinate Court clauses very often in
psychology and education would say oh
kind of simple language you should use
simple language subordinate clauses or a
difficult construction you should not be
using it because people don't like it it
turns out that if you go through
graduate school you're actually pretty
comfortable and you tend to like tags
that have a lot of subordinate clauses
same with it's actually other groups
have had the same result highly educated
people this rule that you have probably
seen in prescriptive writing texts I
don't use our kind of indirect he he was
the pie was eaten by someone so it turns
out that these constructions highly
educated people absolutely no problem
with they they like them where they
don't matter at all so what did matter
for these highly educated people number
of words a shorter is better so this one
we did read
juice and it's kind of nice the other
one that is very related to this kind of
collecting probabilities of a large
amounts of text is that the text
probability where you have collected the
probabilities from a large sample we
used a word of New York Times News here
one year of new york times news so the
probability of the text under this model
turns out to be very predictive so
roughly does it look like things that
you have seen before and if it does the
the ratings tend to be higher the other
thing that turned out to be there
interesting is that the average number
of verb phrases in the document was very
predictive of how well-written it will
be perceived and at the time I was
surprised but since then I've been
reading a bunch of navigation
prescriptive right and this is exactly
what they say they don't choose a noun
if you can use a verb and so this would
be one thing we're kind of we're looking
at like finding nominalization so where
you have used the noun phrase so the
growth of the economy did blah blah we
can find this and say hey maybe you
should say the economy has grown by that
much and this would be clears its kind
of also enough pointing at directions
where we can actually give automatic
feedback on writing through identifying
this so the last one was the really
strong predictor of text quality and
these are the discourse relations that
appear in the text so probably many of
you have never heard of discourse
relations is pretty kind of narrowly
defined term to water it attics course
relations so very often in text to have
sentences like they these Mary likes to
cook her husband prefers to eat out and
any human reading this would say there
is comparison between some between the
statements
and can a computer actually find out
that there is the comparison between
these two statements there's also he's a
reliable person I would choose him there
is a kind of I contingency like this is
the reason why I'm choosing him again
it's not a vertically in the text is
something that's implicitly I figure it
out by the speaker by the reader and
their temporal relations so these type
of relations are either explicit so you
can kind of for example convey
comparisons with but then however and
then you explicitly know that you're
signaling a comparison can have a
because or sins and then you know that
there is an explicit contingency but
many times there are no of our signals
and it turns out that in order to
predict amar said I have five more
minutes I kind of in order to get these
correlations will how well-written is
the text you need both you can't deal
only with the explicit so you need both
in the mix so this means that if we want
to predict this sort of mix we want the
computer to be able to see the sentence
and say there is a kind of Reason
relationship there is a comparison
relationship that's inferred by reading
these two sentences and this is a really
hard semantic processing tasks like this
is something that a lot of people in my
field are now looking at and we also
have worked on doing this and kind of
since I was saying that the kind of mix
of these relations is very predictive of
text quality here's for those of you who
are curious what is the mix if you look
at a text then you see these implicit
relations that hold between adjacent
sentences and they're not overtly mark
you have about twenty percent comparison
about twenty percent contingency
about ten percent temporal so quite a
lot of the tent and about half of all
relations are implicit so about a half
of all the relations in the texture of
the hard pipe and it turns out to kind
of here I won't go much into detail but
it turns out that some of these kind of
Statistics gather from a large corpus
actually our features that are useful in
the representation and kind of being
able to figure out given two sentences
what is the relation that holds between
them for the automatic evaluation of
human of linguistic quality of summaries
I actually want to kind of go quickly
over it is but here is one thing that ah
we researchers in natural language
processing do a lot is okay we want to
develop automatic summarization systems
and we want to know which one is better
so we have these nice run evaluations
where Miss pace in your submit your
system and the next evaluators go and
read the summaries and rang them and a
usual kind of participation rate is load
is 40 60 teams so there are 40 60
summaries summarized errs that are being
evaluated on about 50 test inputs and
then people sit and rank like this is
summary that's good in content but it's
really very incoherent like it doesn't
read well so unlike what I was talking
about before in this setting we actually
have some annotated data so we have
samples of the summaries and we have
what a human NIST annotator have said in
terms of their linguistic quality and
one thing that we developed two years
ago was an algorithm that looks at these
summaries and kind of rains them and
what we want to know is how accurately
would we say that it once
some is better than the other where the
accuracy is determined by the what the
NIST annotator is that and we were
surprised by how good our accuracies
with were so we could say which is the
better system out of two with ninety
percent accuracy so fully automatic
methods of predicting the linguistic
quality and have only made ten percent
mistakes on where nastena taters were
working for it like it's extremely
expensive I can't give you the exact
numbers but they a lot of monies and the
really neat thing with having an
automatic method is I actually used it
can use it for development if I want to
have a summarizer that's better I can
actually you run these automatic metric
and see how good my summer is our and
then for a particular input the accuracy
was a bit worse so sentence specificity
was my next kind of text quality
application that I wanted to bring up
and here is an example and again this is
our predictions from our system um that
says the second sentence is general and
the first one is specific and you can
see how you actually want to kind of
have a mix of both because if you have a
lot of details about what book sellers
are saying and what the topics of a kind
of sales are you can say what why are
you telling me this and I'm telling you
this because I want to tell you that
this is a controversial price so general
sentence can set up the topic so that
you can bear with me and read the
details later in the specific sentences
so having the right mix is really
important and again we actually train
the classifier that is a pretty accurate
in predicting on the sentence level is
the sentence
we're general and it turns out that for
example evaluative words of this type
that I was showing you before so good
and bad so when you kind of express
opinions that these tend to occur in
general sentences so part of speech like
plural nouns like cats make good pets
general sentences this is being picked
up and weird specificity and also turned
out to be really good feature and again
like most of these you know features
were and have collected in a way that
was based on this kind of representing
compare vectors type of approach and the
classifier has ninety-four percent the
agreement when people agreed at a
sentence is general or specific and
again we did find that some sentences
are just very hard to judge in this kind
of binary scale so when people don't
agree the classifier is not good either
and I will take two more minutes to tell
you about one analysis that we did that
actually opens up a lot of different
theories of this kind of work where we
want a computer to generate text so we
looked at the summarization application
and what we wanted to know is when a
system makes a summary does the system
choose specific information to include
or general information and does it
matter in terms of the quality of the
text that they are producing so here is
the average specificity and the
specificity of the input so these are
the inputs urges news articles on a
given topic like think Google News like
a cluster of articles on a similar topic
and you want to have a summary that says
what was this article about this is the
average specificity and then if you ask
people to write a summary in their own
words these are the abstracts the
specificity glows slowed slightly down
if you ask people to
two sentences kind of say which are the
sentences that are most important like I
want to highlight them rather than
generating in your own words the
specificity goes way up and then if you
look at these hundreds and thousands of
you system summaries that were produced
in our annual evaluations they go even
more specific you can say okay does this
matter like there's obviously a
difference the human human summaries are
more general but does it matter and we
looked at it and it turns out in matters
it matters because especially free
summarization you can think about the
general sentences as ultra short
summaries so having something like
details of maxwell's that death are
sketchy it can summarize a lot of like
news writing on the topic or folks it
was an understatement so we actually
looked at these miss tainot teen years
and it turns out that systems that have
more general summaries scored better at
their content so people like what the
system picked out is good content but
there is a kind of the flip side was
also the summary the systems that
included more general information were
among the worst and linguistic quality
why because if you say a general
statements like this you certainly want
also give the details of why you cannot
just give it and if out of context so
for example if you see this sentence and
instead it sank like the Bismarck and
this is a sentence that a human actually
has included in their summary it's like
I don't want this like it has no content
it has no like it doesn't tell me
anything but if you read it in the
context of this previous sentence which
says this person has all the
qualifications for the job everyone
thought that he'll succeed and then you
say instead he sings like the Bismarck
perfect summary tells you quite a lot so
the problem is can a computer actually
figure out that these this is the proper
context to include the general
information so this is kind of opening
up a lot of kind of interesting new
areas in automatic summarization and
we've actually already been able to
improve the summarizer by and of looking
at some of these text quality issues
that need to be incorporated so my
conclusions are we have seen many
successful applications and content
understanding and this is the type of
stuff that you'll see in the following
presentations they'll talk more about
what are the current applications more
interesting but there is increasing
interest in this more subtle
understanding task that draw attention
to discourse relations and sentence
pacifista so they're opening tasks that
kind of people in the field have not
looked at before and it's leading to
progress in tax quality research which
actually probably will all kind of
benefit by so this is what I had sorry
my area chairs gave me conflicting
instructions on time uh Jamie hast in
University of Arkansas do you see it it
almost looks like you're getting to the
point where the computer could write the
paper you know if you if you give the
data you give papers in the past and
things like that do you see that
happening in the future I can just stick
my data and not have to worry about the
paper writing and working hard all day
realistically no I actually when I did
my PhD I I worked a lot on these kind of
generating methods for summarizing news
and it was impressive because we have a
big school of journalism and we would
occasionally ask journalists to look at
our summaries and help us evaluate them
and at some point they
no we won't help you because you'll put
us out of our jobs and like we were all
like what are you talking about we are
so far from its I don't think any
researcher actually kind of
realistically would think it would put
people out of jobs but I think it can
like the one thing that will make a big
difference is this sort of research will
actually help us write better so as
you're writing it will be able to point
out some places that you looking to
improve and hopefully we'll be able to
generate more and more specific like
what is it that you need to improve and
this I think is realistic and this is
something that I personally and have em
interested in working on so I have a
question about how do people react when
it makes the wrong recommendation so you
know people are very intolerant when it
comes to machines telling them that
something is incoherent when they
actually think it is coherent so how do
people react comments right so this is
why I have not so yes a kind of I think
that as systems are getting better it
has to be on demand so it cannot be
something like spell checker or grammar
checker that actually tells you it's
wrong kind of initially as we were
developing it has to be something where
the person can say tell me if you have
something to tell me so it has to be on
demand like we're not at a point where
we can say it's a feature it's on you
have to as you're saying like for the
exact same reason would be annoying for
many I would like to use something like
this I mean I have this problem that yes
I speak English most of my day but
English is not my first language
sometimes I want to kind of then it's
more free to beautiful i didn't get to
this but some of the kind of research is
also is it nicely said and it's very
nice very easy for me to kind of slip
into this very formulaic language but
saying it nicely and kind of more it
would more impact as something that I'm
interested in and I actually would want
the computer to help me with this
because it's hard for me
to do it in English hi my name is Audrey
Ellerbe I'm from Stanford University so
one of the problems that I find today is
that there are so much literature out
there that when you go to publish a
paper it's almost inevitable that you
can't do in there a literature search or
that you can guarantee that what you're
doing is unique and you know hasn't been
done before given that so much of the
writing that we do is suppose to cover
this body of literature that's already
out there do you see this as changing
the way papers need to be written so
that we don't need to include all of
this historical context and we can just
focus on the unis of what we've been
doing it is actually if you look at our
main conference the association of
computational linguistics in the past
three years there has been my growing
number of papers that say exactly kind
of how to generate related literature
search how to kind of summarize
scientific papers so that people can
navigate the literature better and kind
of more efficiently kind of making sure
that they don't miss anything kind of
what you're asking is will the computer
be able to generate the related work
section or kind of the positioning
section for us again I don't think we
are right there yet but I think we're
very close to the point where computer
will be able to tell you look at these
papers they're very similar zits kind of
the first model and it's also very good
like for example one thing is just you
don't need to just say like x and y did
blah blah blah what you need to stay is
and this is how it compares to my method
so you need this either explicit or
implicit sort of comparison and one
thing that I think it's visible to do is
to look at a kind of large corpus and
kind of help people and have see like in
your draft like do you have the sort of
comparison if you don't have the sort of
comparison suggest ways in which you can
phrase the comparison or access so
I think in terms of writing and
assessing um we're pretty close and
there is a lot of interest in this so i
actually do expect some kind of
interesting systems coming out and the
very like maybe in the next five years
which will impact the way we do research
because it will impact the way we reach
literature and probably even write
papers again I feel for myself I would
like the sort of tool and it's very
person like some people would say I know
how to write but I would want to use
this sort of tool Kathy via teaspoon
codec a nice talk I really enjoyed it a
question I have is a lot of content
nowadays that's generated is not written
well it deliberately or it's accepted
and I'm thinking things like texts or
Twitter feeds or things you know
comments to articles say the wall street
journal articles for for your type of
research can you take what you have and
map that to that that type of content or
do you really have to treat it as
another language that you then have to
train train on I haven't given it much
thought 11 application that I'm
interested in I I can like it's kind of
similar to what you're asking not
exactly but one thing that I have been
thinking a lot about is when I teach
undergraduate courses I like to give
students articles about kind of the
science concepts there say New York
Times articles to kind of get them
excited and it turns out it actually is
hard to find an exciting article even in
the New York Times there's a lot of
relevant stuff but some is like why did
they publish this as kind of not well
written so like even if like you're
saying New York Times is good but I'm
saying even within New York Times if
you're searching like there is average
there's pretty like oh like they needed
to print the issue they have to put
something type of stuff and then there's
stuff that's like this is exciting i
want people to read this and probably
they'll be more motivated to listen to
me in class so this is kind of the range
of things that i'm looking at for
information we feel and I
we believe that for science education
it's some sort of underutilized resource
honestly for my own work I haven't
looked at with her and kind of generated
user-generated content as much but I do
know that for example at Yahoo there was
work for example on user questions and
answers so kind of searching over it is
so finding good questions and good
answers and they have some pretty good
success with this so it's not impossible
to do is just I I haven't thought about
it Nick feemster from from Georgia Tech
your work on document summarization
seems like a synthetic exercise I'm
wondering if there's sort of the
converse problem it's of analytic
exercise what I'm interested in is
rather than taking a bunch of documents
and shortening it what I'd like to do is
have you trace the provenance or you
know phylogeny if you will a sequence of
articles for example you know you see
something syndicated or put out on a
news wire and then you see a bunch of
people copy it or inject their own
opinion or omit or excerpt types of
things and I wonder if is their work and
that type of thing particular motivation
might be for example detecting
propaganda or bias um there is a lot of
it very interesting to ask this because
just in the beginning of the semester a
PhD student from our political science
department approached me with very
similar requests kind of like can you
help me this do this sort of work so
would he was interested is he wants to
see how the way issues are discussed in
Santa kind of influence how they should
will be discussed in the popular media
there um there is a growing interest but
there isn't like a coherent body of
research in this sort of area there
there was something a couple of years
ago I haven't seen any follow-up work
I really liked it it was more of the
sword ok I have two concepts a kind of I
think they're running examples was the
kind of recent crisis and loans and you
can say I kind of didn't follow the news
wire these related so what they tries to
chain news articles to show what is the
kind of connection between two concepts
that a user has asked for so it was kind
of preliminary initial word but it does
exist i've read at least one paper in
this sort of kind of space of questions
all right thank you everyone for
questions and let's thank ani once again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>