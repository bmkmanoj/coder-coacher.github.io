<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2014: Impact of Community Structure on SAT Solver Performance | Coder Coacher - Coaching Coders</title><meta content="GTAC 2014: Impact of Community Structure on SAT Solver Performance - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2014: Impact of Community Structure on SAT Solver Performance</b></h2><h5 class="post__date">2014-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AY2b54d-HSo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">&amp;gt;&amp;gt;Zack Newsham: So have you heard, my name
is Zack Newsham, and I'm presenting a work
specifically on the impact of community structure
on SAT solver performance.
This form was part of my master's thesis and
was done in collaboration with my supervisors,
Vijay Ganesh and Sebastian Fischmeister, as
well as Gilles Audemard and Laurent Simon.
So there's been a number of different approaches
proposed to software engineering such as formal
methods, program analysis, automated testing
and program synthesis.
What all these methods have in common is they
can take the program under analysis, convert
them to a set of constraints, and then pass
those constraints to a SAT solver.
One reason you might want to do this is it
allows a separation of concerns so your developer
can focus on their core competencies, such
as developing a program reasoning tool.
In this scenario, the tool would take as input
a program and a specification, convert it
to a logical formula, invert the formula,
give it to a SAT solver which would then either
give a proof of correctness or generate a
counter example that can be converted into
a fault revealing test.
Some concrete examples of using SAT in software
engineering are things like verification.
SAT solvers have been used to verify the correct
behavior of hardware designs prior to production.
They've also been used to verify cryptographic
primitives such as AES.
In addition to this they have been used for
malware analysis, reverse engineering, and
automated exploit constructions.
More specific to testing, SAT solvers have
enabled the development of concolic testers.
These are programs that can provide a guarantee
of 100% code coverage with a minimal test
set or automatically find dead code.
They also provide isolation testing and event
tracing.
More formally, a SAT solver takes as input
a logical formula and will return either satisfiable
or unsatisfiable.
We say that a formula is satisfiable if there
is an assignment to the variables in the formula
such that it evaluates to true.
The Boolean satisfiability problem is NP-complete,
but we have noticed in some situations it
can be incredibly efficient.
This has enabled some really novel software
reliability approaches to be developed.
In the last decade there have been enormous
improvements in SAT solver performance, and
every time there's a 10x performance in improvement,
there are new applications that are developed
that relies on that performance improvement.
So for example, there's bounded model checking,
concolic testing, and more recently, constraint-based
programming languages.
One of the scenarios we've noticed that SAT
solvers are particularly efficient at dealing
with is in industrial instances, examples
of which are code that has been converted
into SAT formulas.
We found that this is true for instances taken
from a diverse set of applications, and in
some cases they have tens of millions of variables
and clauses, so we don't think this is purely
based on the size of the formula.
We're not entirely sure why it is that these
instances are so efficient when they're are
so huge.
The question has stumped theoreticians and
practitioners alike for a number of years,
and answering it may lead to better solvers
which in turn will lead to better solver-based
applications.
There have been some previous works on defining
the structure of a SAT instance.
In the early 2000s there is a work that showed
that random instances with a clause-to-variable
ratio of approximately 4.25 were particularly
hard for SAT solvers to solve.
Unfortunately, this was only true for random
instances and did not explain the power of
CDCL solvers for industrial instances.
In addition, the results show the industrial
instances have a community structure, and
I'll go on to define what a community structure
is later on.
And finally, there was a large successful
predictive model that became the basis for
machine learning base predicter.
So part of my research is to answer the question
why are SAT solvers efficient for solving
industrial instances?
We know that the SAT solvers exploit some
kind of hidden structure.
We don't know what it is.
It's lead to these three questions.
What is the structure of an industrial instance
that the solvers exploit?
What evidence is there that connects the structure
to the performance?
And how do the solvers go about exploiting
the structure?
So this talk we're going to focus on answers
to questions one and two.
The take-home message from this talk is that
the community structure of a SAT instance
strongly affects the solver performance.
The community structure is a metric we call
Q which defines how easy it is to separate
the graph into clusters.
We're currently working towards a small predictive
set of features that forms the basis for a
complete explanation.
Unfortunately, we don't have this yet.
Our take-home message is supported by three
concrete results.
The first is that hard random instances typically
have a low Q value of between 0.05 and 0.13.
The second is that the number of communities
and the Q value of SAT instances is more predictive
of SAT server performance than other metrics
than, such as number of clauses and number
of variables.
And we've also seen there's a strong correlation
between the community structure and the literal
block distance in the glucose solver.
So I've covered the reason why it is that
SAT solvers are important in the software
engineering domain and why we want to capture
the structure that's inherent in these industrial
instances.
I'm going to go on to define what the community
structure is in the graph, and then we'll
look at other results.
So when we talk about the community structure
of a SAT formula, what we mean is we take
the SAT formula and we convert it to the variable
instance graph where a variable in the formula
becomes a node in the graph and we create
an edge between two nodes if those variables
appear in a clause together.
A community is a subgraph that has more internal
edges than outgoing ones.
The community structure characterizes how
well clustered a graph is.
Community structure's been used to study all
kinds of complex networks, such as social
networks, for example, Facebook, the Internet,
and more recently, the graph of logical formulas,
as well as an enormous number of other domains.
So this is an example of a non-random formula
taken from the SAT 2013 competition.
As you can see, the graph is highly separable
into the communities, which are represented
with colored edges, and have relatively few
intercommunity edges, which are colored in
black.
If we compare this to the graph of a random
formula, you can see it is much less separable
and there are for more intercommunity edges.
We refer to this as a giant hairy mess.
So how do we go about computing the community
structure for a graph?
The algorithm -- The decision version of the
algorithm is MP complete, but there are many
approximate algorithms that have been proposed.
For example, the Clauset-Newman-Moore Method
and the Online Community Detection Algorithm.
We use both of these algorithms in our experiments
and get similar results with both of them,
which has increased our confidence in our
results.
The modularity or Q factor of a graph lies
between 0 and 1 and measures the quality of
the communities, for example, by which we
mean how separable are the communities in
the graph.
The higher Q implies there's a good community
structure, with highly separable communities,
whereas lower Q implies a bad community structure,
with the giant hairy mess we saw.
We tested three hypotheses for these results,
the first of which was, we looked at seeing
if there's a range of Q values for randomly
generated formulas that are hard for SAT solvers
regardless of the number of clauses or variables.
We also looked to see if the randomly generated
instances outside of this range were uniformly
easy.
For this experiment, we generated 550,000
SAT instances, in which we varied the number
of variables between 502,000 in increments
of 100, the number of clauses between 2,000
and 10,000 in increments of 1,000, and the
target Q between 0 and 1 in increments of
0.01.
We also varied the number of target communities
between 20 and 400.
We then ran each of these instances multiple
times in a random order on the MiniSAT solver,
with a timer of 900 seconds per run.
900 seconds was the time I used for the SAT
2012 competition and it was our initial baseline.
When we got these results we plotted Q against
the time and noticed that there was a significant
increase in execution time when the Q would
range between 0.05 and 0.13.
Unfortunately, our random generation technique
did produce more instances within this range
than outside of it.
So to ensure that there wasn't a bias in our
result, we also considered a stratified random
sample, taking 250 instances from each 0.1
range of Q, and saw almost an identical result.
So this is a graph of the results that we
had.
And as you can see, in both the average and
the stratified examples, there's this enormous
peak range for the Q.
The graph that shows all of the instances
actually shows a double peak.
This is separated by whether the formula was
satisfiable or unsatisfiable, which was something
that was quite interesting to us.
The second experiment we looked at tested
the hypothesis that the number of communities
and the modularity of a formula is better
correlated with the running time of a solver
than traditional metrics, such as the number
of clauses and the number of variables.
We also wanted to know if the correlation
was better for industrial instances or randomly
generated or handcrafted ones.
For this experiment, we used approximately
800 instances from the SAT 2013 competition.
For the remaining instances, it wasn't possible
to compute the community structure due to
memory resource constraints.
We used the Online Community Detection Algorithm
because it's faster and more scalable than
the Clauset-Newman-Moore Method, and we took
the timing data from the SAT 2013 competition
itself for the mini pure solver and used a
statistical tool called R to perform a standard
linear regression.
We performed this regression twice, once including
the community structure metrics as well as
the variables on clauses and once without,
and compared the adjusted R squared value
from both experiments.
The R squared value is a measure of how good
the model is based on how much of the variability
in the data is accounted for by the model.
A higher R squared means that it's a better
model, generally speaking.
Unfortunately, a large number of the instances
from the SAT competition timed out, approximately
60%, which led to the question of do we include
these instances and have possibly a biased
result or exclude them and not have enough
data to render a result at all?
Obviously, we decided to include them rather
than not have a result at all.
But to try and defend against this, we used
a log of time rather than the standard time.
This also helped protect against the fact
that the instances which finished had a far
lower solve time than those that timed out.
There was a large gap between them.
And we also standardized the data to have
a mean of 0 and a standard mean of 1.
We do this as standard practice because the
regression can sometimes assign a significance
to a variable that isn't really there if the
scales are too different.
So the two models we considered, the first
model included the community structure metrics
and accounted for approximately half of the
variability in the data.
While this isn't a particularly great result,
it is significantly better than any model
that has existed in the past, such as the
number of clauses, number of variables in
the clause-variable ratio, which only accounts
for a third of the variability.
In addition to looking at the R squared results,
we looked at the significant value for each
of the terms in the model and noted that every
one of the most significant terms either contained
a number of communities or the Q value.
The third and final result tested the hypothesis
that the number of communities in a conflict
clause is strongly correlated with this literal
block distance measure.
The LBD measure was first introduced in the
glucose solver, and it's a measure of the
distinct number of decision variables that
a learnt clause contains.
The lower the number of decision variables,
the better the rank of the clause.
The LBD is a powerful measure for the utility
of a conflict clause.
And the reason we care about this is because
modern SAT solvers create an enormous number
of conflict clauses while they're solving,
and without clause deletion, these clauses
would quickly consume all the available memory.
This led to the question of which clauses
to get deleted, for which LBD is an answer.
The glucose solver periodically deletes clauses
that have a bad LBD rank.
The number of communities in the conflict
clause is defined as the distinct number of
communities that the variables in the clause
belong to, so similar to the LBD.
The intuition behind our hypothesis is that
high-quality conflict causes typically span
very few communities.
In addition to this, high-quality conflict
clauses normally cause more propagation per
decision variable and are hence likely to
have a lower LBD.
And we already new the LBD picks out high-quality
conflict clauses.
We considered 189 industrial instances from
the SAT 2013 competition out of a total of
300 for the remaining instances we saw these
memory errors.
And for each of these 189 instances, we computed
the community structure, the number of communities,
and the LBD for each of the first 20,000 learned
clauses.
Beyond that, unfortunately, we had resource
constraints.
This led to an interesting question, as there
were hundreds of thousands of data points
to be considered across these 189 instances
and we had to decide how to analyze these
results.
We chose to use heat maps, with a single heat
map for an instance, where the heat map shows
the correlation between the number of communities
and the LBD value of the learned clauses within
that instance.
This is an example of one of the heat maps.
The LBD measure is along the X axis and the
number of communities is along the Y axis.
As you can see, there's a very strong correlation
between the two of them.
So we are very happy with our results so far,
but there is a large scope for improvement
that will help us improve the confidence in
our results.
The first is that we want to use a larger
set of data.
This is going to enable us to exclude the
timeout results and improve our result overall.
Unfortunately, there aren't very many larger
sources of data than the SAT competition results,
and these are not comparable with each other.
Every year that the SAT competition changes,
the computers used to run the solvers changes,
the solvers themselves change, the instances
change, and the timeout values change.
So they aren't really comparable at all.
We are in the process of running these instances
on our own development machines, but this
takes quite some time as the timer is set
usually between two and three hours and there's
a lot of instances.
We're also going to focus on individual categories
within the SAT competition, such as either
industrial or random or the hard combinatorial
instances.
The reason for this is we noticed during our
regression that the overall R-squared for
the entire data set was sometimes lower than
each of the individual categories.
This suggests that the instances really are
very different and a different model should
be used for each.
We're looking at considering different regression
techniques rather than a standard normal regression.
This is because the norm normality of the
data we are considering mean that it was almost
impossible to estimate confidence intervals
for our predictions.
We're going to use more solvers as well.
We've looked at the glucose solver, the mini
pure solver, and the MiniSAT solver and seen
they all have similar results.
But these all stem from the same code base,
so it makes sense they would share some of
the results.
We're going to look next at Lingling (phonetic)
and possibly some of the parallel SAT solvers.
And finally we wanted to compare some different
random generation techniques to ensure that
ours is valid.
This has led to some interesting directions
for future work.
We're going to consider in the future the
graph representations -- different graph representations
such as the cause incidence graph rather than
the variable incidence graph, and we want
to check whether it's possible to determine
a highly predictive model.
After this, we're going to look at comparing
the community structure model that we have
with graph with base models.
And we're going to look at other solver measures.
For example, the memory usage and the conflict
clause generation rather than the time.
Finally, we've built a visualization tool
called SATGraf which generated the graph which
we saw earlier, but it also allows us to track
the evolution of the community structure of
a formula while it's being solved.
So to conclude, the take-home message from
this talk is that the community structure
of a SAT instance strongly affects the solver
performance.
And we've seen this with three results.
That the hardware random instances have a
queue between 0.05 and 0.13.
The number of communities in the queue of
a SAT instance is more predictive of mini
pure performance than other measures, and
there is a strong correlation between the
number of communities in the literal LOC distance
and the glucose solver.
That's the end of my talk.
Thank you very much.
[ Applause ]
&amp;gt;&amp;gt;Sonal Shah: Any questions for Zack?
Awesome.
Thank you for the talk.
Ah, please come up.
&amp;gt;&amp;gt;&amp;gt; So most of the applications that you mentioned,
like bounded model checking or concolic execution,
what they do is they build up lots and lots
of related formulas; right?
They solve one, and you want it to be unsatisfiable
to move on to the next one; right?
And so what you get is this set of incremental
SAT formulas.
How can you use this work in that space?
&amp;gt;&amp;gt;Zack Newsham: One way you could use this
work is to look at how the structure of the
formulas change.
Every time you run the formula once and find
something that didn't quite work, would you
add a new set of constraints to it and rerun
the formula a second time, and you'd keep
going, of course.
And you'd look at how the structure of it
evolves.
We're more focused, though, on how you could
convert an existing structured formula that
is slow and turn it into one that a SAT solver
can solve more efficiently.
So rather than the ongoing picture, we're
looking at how to make each execution run
faster.
Okay.
&amp;gt;&amp;gt;Sonal Shah: Any questions on the moderator
link?
Okay.
Thank you, Zack.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>