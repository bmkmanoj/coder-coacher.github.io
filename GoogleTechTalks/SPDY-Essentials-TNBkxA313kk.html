<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>SPDY Essentials | Coder Coacher - Coaching Coders</title><meta content="SPDY Essentials - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>SPDY Essentials</b></h2><h5 class="post__date">2011-12-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TNBkxA313kk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi there I'm Roberto and this is and I'm
will chan yeah so today we're going to
cover some basic topics I'm gonna cover
why we're doing speedy what is speedy
how does speedy make things faster
what's the current status of speedy and
also where we're going with speedy
there's a lot of future work but a lot
of things we're doing right now and a
lot of things we opened fire online
alrighty so first thing I will talk
about is you know what a web page looked
like around May 2010 we'd already done
some work on speedy by then but you know
this is still basically informative it's
actually gotten worse than this in many
ways so the average web page has a whole
bunch of resources contacts a whole
bunch of hosts it is in the hundreds of
kilobytes and it is nowhere near as
compressed as it should be so when we
looked at this we realized that as it
says here the incrementer of
improvements to http didn't really
change things a whole lot and we'd
actually tried to make some changes as
Google we'd we'd tried a different
compression technology and what we found
was that ended up being rejected at
least temporarily by our clients and
actually by the internet simply we could
work and the reason it didn't work
despite the fact that both the client
and the server supported it was that
there were transparent proxies out there
on the Internet and these transparent
proxies would muck with the headers or
they would muck with the data and they
would make this communications
impossible so as an example there you
know the stripped accept-encoding header
'he's made it so that we couldn't
actually advertise properly what
compression protocol from the server to
the client that we were speaking
connection use in 2010 is shown here on
this graph so we see that the you know
things got worse since the previous
thing so there's there's a trend towards
using
or connections well sometimes it's for
good reasons for instance you want to
split off user user developed and user
generated content from content which you
believe should be secured because it's
difficult to secure user generated
content but for the most part this seems
to be done so that you shard your
domains and you allow increased
parallelism which is of course one of
the big problems with HTTP which is that
it's head-of-line blocking and you
essentially only get unless you've
turned pipelining on which most haven't
one request for a connection at a time
that generally doesn't work so well so
yeah so HTTP had all these problems and
we were thinking since one of Google's
major initiatives is to make the web
faster we wanted to rethink it see what
we could do to make things faster our
goals are to make sure that we can
deploy speedy without requiring web
content owners to radically change their
websites obviously we want to make the
web faster and this shouldn't require
people to drastically reading deploy the
websites it should be a drop-in
replacement and it should make things
faster so that's what we're going for
so basically speedy it's not really that
complicated we see problems like
headline blocking concurrency issues
with regards to only six or however many
connections per host so basically we
decided to deploy some basic features
use a smoke connection stop charting
across multiple host names in order to
get more parallelism and just use motive
taxi instead ad request extreme
prioritizations you can tag which
requests are more important for the
server to respond to so basically you
should just have the client send the
request to the server as soon as
possible and the server will decide when
to send it
send the responses in whatever order it
thinks makes sense so another area that
we've seen incredible redundancy is
headers basically on every single HTTP
request you're singing for the most part
the same has over and over again vici
can you please mute your mic so we made
header compression mandatory
this results in big benefits especially
on the uplink so yeah that was awesome
another big feature is quoting server
push this is a fairly advanced feature
and it has pros and cons but we hope to
see large benefits from server push so
yeah it's not really rocket scientist
science it's pretty basic but we hope
that you know you know show and we have
data that shows that it does increase
the speed of web pages so yeah what is
speedy speedy is an application layer
protocol it's built on the current way
that is deployed is on top of SSL TCP
and IP so it's the at the highest layer
of the stack we use mandatory header
compression to reduce the redundancy in
our HTTP headers
I guess all stuff I covered in the
previous slide one important thing is
that we are to point over SSL we think
this is really important for the web
it's really lame that when you're in an
internet cafe that anyone can just sniff
your cookies we think that everything
should be secure so that's one of the
major reasons that were deploying over
SSL yes and another major feature is
sort of push this should albeit the need
for techniques such as in lining with
data your eyes
yeah so the data that we see here are
from lab tests that we did some time ago
but unless we say lab results we believe
these are generally still true so we saw
a significant reduction in upload bytes
from speedy and this is basically due to
the compression of headers that we were
just mentioning before so compression of
headers you may think oh it's mainly the
cookie well the truth of the matter is
it's not just the cookie there's a whole
lot of data actually you know there's a
lot of text that's there that happens
every time for instance your browser
saying what user agent it is that string
is actually pretty darn long and
generally your browser isn't changing
with the connection every connection is
going to be using the same browser
almost all the time
at least if you're guaranteed that the
connection is from client to server
which in this case we can guarantee a VP
please remember to mute a you in the
conference room that means you mute
please all right so download as we can
see there was nowhere near as much
impact here and that is because the
majority of the download is actually
unsurprisingly what you want to see it
is data so we are not reducing by a
whole lot the amount of data that is
downloaded at least not today
and we're not reducing it too much today
because much of the data at least in our
tests was compressed now as we mentioned
in the previous side that isn't always
the case there are cases where we can't
do that there are transparent proxies
that mess with it
there are transparent proxies that
disallow us from trying to deploy new
protocols a rat sorry a new from new
compression techniques those shouldn't
be a problem
with speedy although speedy itself won't
lend a whole lot to decreasing the
amount of your download all right the
story with total packets you'll notice
that the reduction in total packets is
significantly more than the reduction in
at the payload size there's a couple
reasons for this the first one is well
you know the payload size is actually a
little bit smaller so that it has some
effect but there's another big reason
which is that if you're multiplexing
over 44 connections you're much less
likely to have full packets right if you
put it all over one connection you can
almost guarantee that every packets
going to be a full packet except for the
last one of any particular non idle
session so in that case you're simply
going to reduce by quite a bit the
number of packets and as many TCP
implementations look at their congestion
window and packets this can have a
surprisingly positive effect increasing
parallelism hopefully this will be
obvious given the increased give it
given the fact that you know speedy does
multiplexing and it doesn't have to have
a new connection for each thing so you
can look at the slides if you want to
examine this in more detail in the
future so multiplexing and how does it
compare to pipelining well on pipelining
side you know it exists it's been part
of the specification
since the specification was a
specification however it was actually
really hard to implement and one of the
reasons it's really hard to implement is
as just just as we've had with
compression there are these transparent
proxies out there that mess with what
you can do and it's kind of hard to
detect them because of the word
transparent sure and so I've had
experience with this in the Chrome
browser side another problem with
pipelining is that it's really hard to
detect when it's broken what is a clear
signal that HTTP pipelining isn't
working you get these weird errors and
it's hard to tell that the result of the
HTTP eor connection errors that you're
seeing is because of pipelining and
that's because of all these transparent
proxies who muck with it and do whatever
they're doing and it makes it really
hard to see whether or not you can
enable pipe on you for this origin
server or that
server and also when you move to a new
network and you have new intimate
intermediaries in between then it fails
again so and then of course there's one
other humongous problem which is future
looking for pipelining which is that Val
shall not pipeline for 9:00 on it for
non idempotent requests and if you've
been following what people have been
doing with web pages ie Ajax and
increase in amount of interactivity a
lot more of what the web is doing is
done over a put or a post ie
non idempotent requests so those can't
be pipelined so on the speedy side we
don't have to worry about do we detect
properly the fact that this is broken
it's not we're guaranteed that given our
deployment strategy which we'll talk
about more we are guaranteed that we can
do however much multiplexing that we
want well okay I mean yeah we're limited
to four billion simultaneous sessions
but I think that should be enough for
anybody at the moment and if it's not we
can put a new opcode and a new frame out
there and change it we can do better
compression as we mentioned before
server push and of course really
important one there is no head-of-line
blocking we can interleave data frames
that we send back for multiple requests
simultaneously based on their priority
and nothing gets blocked that shouldn't
be blocked
one other thing to go with an advantage
of speedy over pipelining is that when
the connection dies and your pipeline is
two three five however many requests
deep you don't know how many of those
were actually handled by the server
whereas for speedy when the connection
closes it sends a go away frame which
says I process up to like this many
since streams so even if it dies in the
middle you should that you generally
have a better failure mode yeah so
basically what this graph is telling you
is that our teach he is important we've
big increases in bandwidth capabilities
but the speed of light is not getting
any faster and our teach he matters so
even though we have more bandwidth
available the actionable actual usable
bandwidth by webpages is not improving
so yes round-trip times matter basically
more data to show that RTD matters so in
particular in the previous slide note
that we have a a log relationship with
an increase in size basically it started
with an increase in the size of the pipe
right so it looks like we're approaching
some limit as the bandwidth increases of
course when you look at our DT this is a
completely different function it's a
line and of course linear relationships
are far more powerful than logarithmic
relationships at least when we're trying
to use it in this fashion so you know
the the intent of these slides is to
ensure that we know that a reduction in
RTT matters a lot more than an increase
in bandwidth so when we were doing
speedy we're talking about where she
make the changes should we make the
changes in the transport layer should we
do do it at the application layer we
know that there problems in the
transport but currently in transport we
have tons of separate connections they
don't have shared state shared
congestion control state shared you know
all the other connection state there's
you're not allowed to send data out with
your TCP syn actually you are but it
doesn't really work on the internet yet
there are lots of problems with it
there's slow start
there's delayed acts like these are all
problems for but these are all things
that cause latency for webpages but the
problems in the application layer are
way worse
the the connections are single-threaded
you can only use one request per
connection unless you have pipelining
but we've already talked about
pipelining there are tons of tons of
redundancy across requests due to HTTP
request headers so and there's no
compression going on there there's not
enough compression going on even in the
response bodies basically the benefits
that we could get in the application
layer are so huge and it's pretty
difficult to make changes in the
transport layer so we're going for the
low-hanging fruit first we need to solve
the application layer and that's what
speedy does today so when we're talking
about how to actually deploy speedy yes
we could try and change the lower-level
transport as discussed that's pretty
hard
you would have to deploy kernel changes
or bohdanka but UDP and then we
implement lots of the congestion control
algorithms that is already given to you
in TCP and yes there are problems with
Nats so trying to deploy over another
protocol that's not TCP is probably
difficult yeah so back UDP back to TCP
for most of the web that's right most of
the Internet there are only really two
ports that you have available there port
at this port 80 and port 443 firewalls
often will block all other ports lots of
people debate this and we have data to
show how successful this is support 80
everyone always asks us why don't you
going over port 80 why aren't you using
the HTTP upgrade mechanism yes so the
upgrade header
that would require a round trip like on
top of the TCP handshake you would have
to do an ATP upgrade in order to
negotiate to another protocol so that
was not to up to good weren't very
pleased by that so luckily while we were
talking about our deployment strategy
the WebSocket team was trying to test
out how best to deploy WebSockets and
they ran this experiment within Chrome
where they tried to connect to port 80
port 6 195 and port 443 and speak
WebSockets and as you can see here the
success rates are pretty horrible for
port 80
it gets better for port 6 195 but really
if you're actually trying to deploy it
widely on the Internet
poor 443 is the clear answer and so to
reiterate right
why does port 80 fail well it doesn't
fail because the packets fail to get
there it fails because of these
wonderful horrible things called
transparent proxies that were supposed
to save us but in a van in actuality
stifled any innovation because any
innovation is not recognized and it
basically fails so this is what we found
yes question yes there was some you mean
HTTPS there was some amount of failure
that was common amongst all of them so
there were times when the browser's was
simply not able to connect to the
internet for instance and we did not
remove that from the results of the
study so the the reasons that HTTP over
port 80 and HTTP over port 6 1 985 were
different it wasn't actually easy to be
but anyway the reason over port 80 is
because of transparent proxies and the
reason over 6 1 985 we assumed
is because the packets were never
delivered so that in other words we
actually have less mucking about by
firewalls over random ports than we have
transparent proxies mucking with your
content stream which is a very
interesting finding
so that informed the decision what do we
use well we're gonna use port 443
because we act we want some chance of
getting this protocol out in our
lifetime port port 443 is so reliable of
course because there are no transparent
proxies mucking with it because it does
traverse firewalls perfect right so how
do we actually allow us to upgrade well
we could have done HTTP upgrade right
that we could have done that equally
over port 443 however it would have
introduced yet another round-trip and
let me tell you SSL already adds a
round-trip which we will talk about in
another slide so we added an extension
to SSL which we are trying to push
through any standards that we can at the
moment called next protocol negotiation
and as far as I know we have submitted
patches for open SSL which will
hopefully make it into the next release
but we'll see it's up to their
maintainer x' and NS s of course yes so
we're hoping that that will not just be
for speedy that code that would be for
upgrading to whatever protocol people
decide to use on that port but the nice
thing about the port is of course to
reiterate nobody can mess with that data
that is excellent for the evolution of
the internet because it means that we
can continue to make new protocols for
instance if somebody thinks up something
better than speedy which hopefully will
happen they can deploy it yeah
and it's part of the SSL handshake so
there's no additional roundtrip so one
of the questions that we've got a whole
lot when we when we announce this
decision to various people within the
company when it was an experiment is
whoa geez but isn't necessarily well you
know actually not so much our CPUs are a
lot faster than they were when it when
SSL originally got this reputation for
being so slow CPUs are significantly
the implementations have improved a bit
TLS does increase bandwidth by a hair
but of course speedy does header
compression and that ends up being a
larger factor than the TLS overhead does
but yes
SSL does require some additional round
trips in the beginning for the handshake
so you'll notice a thing that says can
we reduce to zero artis well we can but
and we did run an experiment but the
complexity of that implementation was
such that we were not confident that it
could be successfully deployed to the
Internet at large so we're not going to
propose that as a whole but we will note
that I hope that that we hope that
future work occurs in potentially
revisiting SSL as well to allow for all
of the innovations that we're doing with
us to sell today but also to allow for a
zero RTT handshake because as you recall
our tt is the dominant factor in any web
page load all right
so here we see a diagram of multiplexing
for speed ease so we've tried to color
things in different colors just to make
it a little bit obvious so we have a
stream at a low priority of three we
have another stream of a low priority of
three and at some point in time we have
a stream with a higher priority which
would be zero and what we see here is
first of all that things are interleaved
but second of all when we realize that
there is more important data we are free
to insert it into the middle of whatever
stream and thus delay delivery of the
other streams so that we can service
that stream first so what is speedy
here's what it looks like basically on
the wire note that in reality these
headers are compressed as we've said
multiple times but basically if it was
uncompressed this is what it would look
like so this is a syn frame its stream
ID is one it doesn't have any Associated
streams so it's not a reply to anything
yeah
the priority is one which is a
relatively high priority although not
the highest it has four headers it has
these and the headers are key value
pairs where
he is the first element and the and the
value is the second so the method it was
a post URL is not something we use as a
key these days but basically and I'll
explain briefly why in a moment
user agent etc etc okay so why is it not
URL anymore well we thought in the very
beginning that URL would be a wonderful
thing to have there because hopefully
it's what people type into the into the
browser line into the box there
however HTTP actually has some
interesting redundancy here which we
found that we had to emulate in order to
be compatible or as compatible as
possible basically you can have a URL in
the first line of your HTTP request
while simultaneously having a different
host header great so we actually have to
have two separate fields for this so in
modern speedy implementations as its
indicated with the draft if you go to
the speedy website which will tell you
how to find later you will find that
there is actually two men two different
header fields there so does it work so
initially when we're working on speedy
we wanted to see how it works so we ran
a bunch of lab tests you can read what
we've done here basically we tweaked all
the major variables packet loss
bandwidth RTT and we examine the top 25
web pages this is just lab results so
later on we'll present some real-world
findings but this is from the lab yeah
so with real-world packet loss set to 1%
speedy oh sorry so the slide says flip
that was the internal code name for
speedy before we settled on speedy so
whenever we see flip we mean speedy so
we did do some internal lab tests of
speedy not over SSL so you can see that
that was also pretty promising it was
very exciting 41 to 47 percent faster
for packet loss between one and two
percent another big question is what
kind of websites is speedy actually good
for is it good for the the top websites
which are theoretically highly optimized
content so here we randomly picked sites
from the top Alexa top 1000 overall page
load time speed up was 40% which is
pretty good sorry for the communication
in this graph at the bottom that does
not start from zero
it's from 1200 but as you can see that
speedy not over SSL still has huge
improvements over HTTP and I'll want to
note one thing about these lab results
they really didn't wait mobile very much
and with the advent of mobile of course
our TTS unfortunately have gone up which
means that our TTS matter a whole lot
more so things that can take off one
hour tt don't show anywhere near as much
improvement as you in these slides as
you'd actually have in and a study
redone today and by things that would
remove when RT t Roberto means things
like serverpush yeah so speedy does
wonders for web page loads uses as far
fewer connections which is awesome since
you don't need to open up new TCP
connections do new DNS resolutions have
much more popular blow on the internet
using fewer connections is awesome
if you're not familiar with buffer bloat
please put it into your favorite search
engine and look it up because we need to
kill it
all right so what is buffer bloat sorry
alright buffer bloat basically is that
for various reasons we have increased
the amount of buffering we have on
intermediaries between you and the
server and between the server and you
and note for what various reasons these
may not always be the same path but
often are so your cable modem or at
least cable modems very recently would
have a fairly sizable buffer in them of
certainly a megabyte or more and the
interesting thing about that is of
course it takes some time to drain a
megabyte maybe you're maybe you're one
of the lucky few and and your your
upload speeds from your cable modem are
significantly faster than a megabyte per
second but for a lot of people these
days it's still not true and I'm just
using this as an example and this
actually occurs in routing devices out
in the Internet as well so it's not just
limited to your endpoint but the larger
the buffer the longer it takes to drain
that is the essential thing that people
need to understand about buffer float so
you have this wonderful voice chat that
you want to do so you send some packets
and it goes into that packet queue on
your cable modem which takes one whole
second to drain that is buffer bloat we
don't want that we want appropriately
sized buffers so that we can get packets
through with reasonable latency and with
dependable latency so that the amount of
latency doesn't change all the time by
any great factor so the more connections
you have the more likely it is that
you're going to cause some buffer bloat
because the amount of space that you're
gonna use in each of the buffers will
will actually be potentially more not
that we've spoken too much about Buffalo
already but basically my suspicion is
routers out there just wanted to say
that they have this packet loss so like
how do we prevent packet loss well
that's increased buffer size and it just
increases latency really if there is
congestion
and the buffers are getting overloaded
you should just drop packets that sends
the signal back to the peers that yes
there's congestion you should back off
so real-world numbers for speedy this is
chrome 12 speedy compared to HTTP that
we will have numbers we'll have an
announcement verse HTTP later but within
chrome we have an experiment where we in
our NPN and extension we advertise
speedy for 95% of the cases and for 5%
of the cases we do not advertise speedy
so we just do in vanilla HTTP and for
sites that do support speedy we compare
this the page load time for speedy
versus HTTP and you can see that we have
a fifteen point four percent improvement
out in the wild and one thing I'll point
out about these numbers is of course in
April 5 there weren't all that many
external web sites that were using
speedy and unfortunately that's still
true although it seems to be rapidly
changing which is very exciting
so this data is also colored a bit by
the fact that unfortunately the major
consumer of speedy was Google and Google
has traditionally done a pretty good job
of optimizing web pages so the fact that
we're seeing this much improvement for
data which is dominated by Google is
pretty interesting more qualifications
modifications on a number if you look
previously many of the major web
properties at Google that supported HTTP
like most of the traffic would be stuff
like Gmail or whatever where page load
time is not a world if not a really good
metric for actually whether or not
you're speeding up to the web page the
browsing experience yes we use other
metrics
um so there should be taken with a grain
of salt but well good as any other but
it's as good as any other metric all
right so here's the wonderful thing we
wanted to say so as hopefully many know
Google recently launched a search over
HTTPS well it was really great to see
that for for the Ajax search
we were actually faster over SSL because
we were using speedy than HTTP which was
kind of a WoW moment you know that this
was okay maybe some of us expected it
but you know for the majority of people
this was a somewhat unexpected result so
we just want to say with us that look
SSL can be faster than HTTP for your
website right so you think it's gonna
add latency it may not
at least not if you use speedy and we
will be presenting numbers but someone
from Google will be presenting numbers
for this at the next velocity conference
so stay tuned one question the chart
that I showed before
so these are aggregate sticks as
measured by chrome chrome does not
record any information about which sites
it went to so we can't actually know
that this is Google but as far as we
know from looking at our server numbers
mature the majority of this data should
be coming from google.com by weight of
volume basically so chrome randomly
disables speedy today for 5% of its
instantiations ok and that's a nice
thing basically on startup
chrome will use a random generator and
in 95% of the cases
it'll activate speedy and 5% of the
cases that we're not
responses for speedy capables oversight
this ii ii craps on that thank you
until they they are actually speedy
capable so it it goes through NPN and
ignore speedy and it's not all HTTP
right correct
we are only comparing numbers for
servers that use the NPN extension
already so yes so the question was will
it be the case that on mobile that HTTP
using speedy is faster than HTTP I would
suspect it could be significantly faster
on mobile for various reasons but it
does depend on what exact implementation
we use so the again the interesting
thing about mobile is for many mobile
technologies that are deployed today the
RTT is significantly higher than what
you would have on your desktop so
anything where you can reduce the number
of our TDS is significant now speedy
reduces number of our TTS in a number of
ways
the first one is multiplexing right the
second one which is a bit of a surprise
is that for mobile since you are so
bandwidth constrained header compression
ends up being a significant reduction in
the amount of bytes you have to send and
this allows the server to respond
significantly faster and then the thing
that's on the timeline although we
haven't played around with it too much
for lack of time is server push so we
also know that there are things that we
can do with speedy that we really
haven't turned on that we suspect at
least for mobile can make a significant
another significant difference by the
way all these numbers we've presented
previously are for websites there are
not speedy optimized so they are not
leveraging things like speedy server
push they're not undoing
hostname shardene so basically there are
more gains to be had
this is a drop-in replacement so we're
like speedy is already improving things
as is we can save our G T's by undoing
her theme charting so you don't have to
do the extra DNS lookup the extra TCP
connections so and those are huge for
mobile another interesting thing for
mobile is that the there are two things
that seem to consume battery most on
mobile phones and they are display and
radio and the interesting thing is the
fewer bytes you ship and the fat and the
faster you get your webpage the the
faster the faster you will get the radio
turned off and so you can actually see
an increase in battery life which is
surprising you know the CPU isn't the
main consumer it's these other battery
drains that are the main consumer of
battery so anyway my question
so the question was well there was a
question of two parts really yes one was
talking about the fact that there's a
significant cost to turning the radio
back on and so that was an observation
the other one is that another
observation that there's significantly
more packet loss over 3G than the one to
two percent that we had here so I can
honestly say that we still don't really
know we don't have enough
implementations out there in the wild
that we can tell you what actually
occurs and I think that we're gonna
require that before we get you any real
conclusions I can say that speedy does
have a mechanism in it which is
interesting that allows you to not have
to use TCP keepalive which is the ping
frame so basically you can test for the
connection for liveness using the speedy
protocol layer alone without having to
actually call any handlers on any server
so this allows us to test that it allows
us to assume that a connection is open
and thus we have it open without having
the cost of waking up the radio randomly
for packets that have no utility
whatsoever other than keeping the
connection alive all right okay
sure but will any other time so yeah go
ahead okay Speedy's in ableton home and
that we use speedy for all the secure
traffic at Google car when I'm using the
Google services I get HTTP speedy when
you use HTTPS in Chrome and it talks to
a speedy capable server it would
negotiate speedy
so speedy is not a new scheme so you
will not see
PD : / / it's still HTTP colon slash
slash oh I see thank you okay if you
want to find out whether or not you're
actually speaking speedy we can talk to
you later there's some Chrome URLs that
you can type in that will show you what
sessions are speedy etc I imagine that
Firefox has something similar and so
we'll find out from Firefox of what that
kind of thing looks like when they've
given us more information
so yes current deployment status is
speedy speedy has been on by default in
Chrome since chrome 6 and as we
discussed previously we are running a 5%
hold back so we can gather numbers it's
on so if you're using Chrome or another
speedy capable browser like Firefox 11
then it's on for all Google as a cell
traffic we have a number of other speedy
implementations out there we have an
open-source speedy proxy is it can serve
as a for as a for proxy or reverse proxy
there's also out in the wild Amazon has
deployed the Kindle Fire which uses a
speedy proxy Android browser has
supported speedy since honeycomb Firefox
will support it in Firefox 11 we have
ongoing work to support it in Apache it
actually there actually is an existing
mod speedy it sort of sucks because it
doesn't do what I Plex seen it's correct
but it's very suboptimal we're gonna fix
that there are a ton of other speedy
implementations out there I get all
these alerts to say hey someone has
written a new C implication of speedy or
today I got an alert telling me that
someone wrote a new Erlang
implementation of speedy
it's always very surprising and very
interesting to hear about this but yes
so basically speedy works is out there
in the wild you can use it to speak to
Google there are other servers out there
strangely deploys it for certain
websites and yes there are other ones
which we're not allowed to talk about
external people will have to announce it
themselves question hold anymore
questions to the end because we have
enough yeah we only have five minutes to
go through the rest all right so one of
the things we like to point out is sea
wind which is the congestion window for
TCP which is how we do rate limiting for
four bytes sent on a TCP connection is
still a problem
there are various kernel changes that
can be done but of course deploying
kernel changes isn't exactly the easiest
thing in the world especially when you
know the equivalent changes for Windows
can take a very very long time to go out
there even on Linux there are various
problems just talking about the TCP
layer things as well and ignoring the
implementations the back off for a
single connection versus many
connections is well it's unfair right
you end up if you compare a speedy
connection as it is today without any
TCP modifications which we may talk
about later if we get time is likely to
have a significant increase in bandwidth
reduction as compared to the many HTTP
connections if there is randomized
packet loss so we are actually talking
with people to fix that so what are the
next steps basically there's an
incentive for web apps to open more and
more connections if they want to have
more Seawind available so and that's not
really what we want to instead of either
people to do
so what are we doing going forward so
our number one thing that we do right
now is we're trying to help the
community so we have been working on
updating our draft respec we are a
beginning work on a speedy compliance
test suite so hopefully that'll make it
easier for external implementations to
find out that they are indeed doing
things wrong or in hopefully other cases
that they're not doing it right
we're going to split out the speedy dhcp
gateway we talked about briefly before
so that you don't have to have the whole
chromium code base to compile it we've
heard complaints that that's a bit of a
barrier and of course as we mentioned
before we've been working on making mod
speedy in apache a real first-class
citizen in the speedy world all right so
one of the things we'd really like to
talk about and one of the reasons I said
no to questions before
what are speedy best practices these are
the things we believe you should do if
you want your web server to perform well
for speedy so number one it's really
optional you don't have to shard your
domains so if you were starting your
domain so you could get more concurrency
in the past you don't have to
depending on how your search structure
is done you may actually end up with
multiple speedy connections if you don't
shard them you won't and again there's
no concurrency problem with speedy watch
your search size so do some basic
blocking and tackling for your SSL make
sure that you know the search chaining
is done properly so that you don't have
to add a whole bunch of round trips when
you're doing your SSL handshake use
wildcard certs if possible that way your
cert will match multiple domains and so
you can do connection pooling and speedy
which can can end up being a significant
latency reduction use reasonable SSL
write buffer sizes try and use server
push instead of inlining inlining we'll
talk about in a second if we have time
use appropriate selection of priority
for instance HTML is most important and
JavaScript and CSS is pretty darn
important and the rest of it is probably
not anywhere near as important pick an
appropriate frame size so that you can
do in through leaving with reasonable
latency our implementations use a 4k
frame size which seems to be just fine
that doesn't add too much overhead
leverage persistent settings so one of
the things we didn't talk about very
much earlier is the fact that speedy
allows us to have
some settings about your TCP connections
saved on the client machine and they are
connection settings like measured RTT
and measured Seawind and what this
allows you to do is this allows you to
very quickly resumed the the Seawind
that you had with your previous session
on a future session another thing that
you should do is turn off tcp slow-start
after idle which is frankly kind of evil
set in it Seawind to 10 and that's
basically becoming a standard as far as
I understand so you should probably do
it anyway and whenever possible use a
single connection because otherwise
you're contributing to various problems
Oh server push vs. inlining please
server push vs. inlining why why why do
people do inlining well first of all
people do inlining because they need to
reduce the number of round trips total
makes sense right so what they do is
they encode the data in the web page
itself and the base64 format hopefully
they get to compress that page so it
doesn't blow up too much but there it is
now the biggest disadvantage there is it
will never be in the cache addressable
as its object so that can be a problem
for future page loads so people tend to
you know it's it's very difficult
because sometimes people don't do it
multiple times and sometimes they do it
so another interesting thing about
server push is because we don't have to
rien you are never increasing the size
of the resource all right so we're
hoping to get people to use server push
we note that it may have more
interesting and again people on the VC
mute there are people who are not muted
make sure you are muted thank you Raman
at Google you need to mute okay
all right another thing that we're doing
is researching TCP level improvements we
hope that this will have an impact on
things that are not just speedy but we
definitely know that there are things
that it will help for with speedy you
know in particular we know that there
are some instances where speedy doesn't
perform as well as HTTP on high latency
high loss links this is this is due to
the fact that the amount of loss for a
speedy connection is more what I mean by
that is the amount of loss of bandwidth
for any particular packet loss is more
and we need to fix that because we
should not be incenting people to use
many connections standardisation this is
a big one we're going to IETF in spring
so we're beginning the standards process
we've done our experimentation we
believe it works we believe it actually
makes sense to make this a standard
we're going to IETF with it and
thankfully there will be other
contributors there externally for to
Google who will Mozilla for instance who
will be there with us areas for research
all right okay questions now the
question was about better performance in
terms of packet loss so basically when
you have multiple connections you have
multiple Siemens so when you have a
single connection when you have a class
the Seawind dropped in half right if
you're six connections now only one of
them gets it dropped in half so that's
effectively a 1/12 Lawson bandwidth as
opposed to 50% loss in bandwidth right
and and in cases where there isn't quite
so much packet loss yes speedy can
actually do better which was I'm not
sure which sense your question was but
it can do better exactly because as you
suggested it it triggers the fast
retransmit fast retransmit path because
there's more likely to be a subsequent
packet on the same connection
yes yes it depends on how much packet
loss you have and what the latency is
honestly we don't have a great sense and
the main reason for that is that it's
changing all the time
so even if we analyzed it then I can
tell you that the results today are very
likely different but we are going to be
presenting some interesting numbers
about exactly about what happened with
web search at the very least at velocity
very soon so the question was is speedy
a drop-in replacement on server side so
long as whatever the server framework is
speaks the speedy protocol the answer
should be yes but that is for current
web pages if you want to do something
speedy specific obviously you need to do
something speedy specific so the
question what the question was is there
a transition path for I guess
application writers so that they can see
when the client is coming to them via
speedy and react to it I would imagine
that will depend on the exact server
implementation I can say at least for
Google servers what we do is we will
simply advertise that the connection
came in via speedy and so they can react
appropriately so just to be clear the
server will know whether or not it's
over speedy over normal versus no more
HTTP because that information is in the
TLS extension exporter con ago she ation
so servers always have an firm ation
so the question is what do we expect the
role of proxies to be in the future and
is this killing all proxies so no it
isn't killing all proxies but at least
my hope and since we're going to
standardization we will not be the only
ones saying anything about this but my
hope is that it will change us to a
world where by the only proxies are
authorized proxies so they are the ones
that you agree to use as opposed to
today where we have these transparent
proxies that you don't agree to use they
may do things you don't want them to and
you have no control over it does it
answer your question it would be one
that you have configured in your browser
the question was what is an authorized
proxy' the answer is it's one you've
configured to be so today in chrome you
can configure a speedy proxy so you can
talk directly to a proxy with speedy any
other questions so the question is you
mentioned some concern over high loss
high latency links and the question is
is that where the mobile web is going I
don't know as I said before we need to
gather some more numbers on the mobile
web now the interesting thing there is
that it may very well be that bandwidth
isn't the dominating factor in many
cases or at least the recovery is fast
enough that you can deal with it
it may not be I don't know yet we are
running a battery of experimentations
sorry of experiments to try and see what
we can gather right now but we're also
going to do some real-world tests as
soon as more browsers are available on
the mobile handsets that and devices
sorry that can actually speak speedy
without giving away any numbers
internally at Google we have run some
lab tests and it shows that most of the
cost in those Hylian see high packet
loss
connections is because of see when
getting throttled so multiple
connections does went out in that case
and for now and that's why we're looking
at trying to improve the state of TCP
implementations at Google and also out
in the wild question about you mentioned
using the Android browser in honeycomb
and yeah we can't say anything else but
the if you write an Android application
no that is unless is using webview it is
not going to be able to speak speedy
yeah I I can't say any other questions
all right yeah so since there are no
other questions at the moment we'll talk
about the other two things that we
didn't mention before so we'd like to
allow the server to authenticate for
other host names so right now when you
make an ssl connection a cert is served
that is the only cert that has served
over the ssl connection normally unless
you renegotiate but without going
through the pains of renegotiation what
we should allow is for the server to
announce that it has this other cert and
thus it should I be able to authenticate
that it is actually valid to push down
requests for this different hostname
down the same connection and this
because it's a cert will hopefully be at
least as secure as any other cert based
authentication mechanism can be another
thing that we would like to to research
is possibly pushing or republishing DNS
data down the same pipe so that you
don't have to do additional DNS queries
in order to figure
that you should use the same connection
or maybe use a different connection
there's another aspect to this which is
also interesting which is that if the
server can publish DNS information for
its own hostname it can actually steer
traffic to a different IP for the same
hostname that's actually really
interesting for load balancing and and
potentially it would actually end up
with reduction of latency because right
now most localization for domain names
is done unsurprisingly by your local
resolver and unfortunately your local
resolver man may not actually be local
we've seen indication we've seen cases
where for instance people in Africa have
a domain name server in Europe and I
promise you that if that domain name
server is handing out things that are
close to Europe they are not close to
Africa so a mechanism like this would
allow the server to realize that a
problem had happened in the DNS
resolution or at least a sub optimality
and to fix that and I believe that's it
do you have any more comments yeah so
we're out of time thank you for allowing
us to present</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>