<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DocEng 2011: Citation Pattern Matching Algorithms for Citation-based Plagiarism Detection | Coder Coacher - Coaching Coders</title><meta content="DocEng 2011: Citation Pattern Matching Algorithms for Citation-based Plagiarism Detection - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DocEng 2011: Citation Pattern Matching Algorithms for Citation-based Plagiarism Detection</b></h2><h5 class="post__date">2011-10-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cNzMFvY_FXw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah hello welcome and first I'd like to
mention it I'm here with my colleague
and co-author norman moisture so he's
over there um I start with the outline
and so the paper is about a new pager
isn't detection system and first I want
to talk about the motivation why we have
developed this new approach and then I
will talk about the approach and how we
try to identify plagiarism and then I
will talk about the algorithms we have
developed to identify them and I will
give a short overview about our
prototype it's called side pluck and of
course if you build such system you want
to evaluate it and show whether it's
actually better than the existing
systems so that's what I will talk about
that after that and then I will give an
outlook so the currently available
plagiarism systems they have the problem
that they are solely text-based
and Professor Dr Weber wolf she runs
evaluations on the existing systems and
her conclusion is that plagiarism
detection systems find copies but not
plagiarism and the reason for that is
that they compare the text of the
documents and try to find identical
patterns so like Engram so words that
follow in the same in the same sequence
and similar stuff but if you paraphrase
something or if you translate something
then the existing systems hardly find
anything and I mean usually if his
scientist plagiarized something then he
will make a little bit of an effort and
maybe try to change some words so for
that reason it's very very difficult to
identify and plagiarize documents and of
course the human examiner he will if
he's from the field he might find it but
there are plenty of studies that have
shown that even in databases like
archive and or canned and even yeah
well-known journals that
the people that peer review the articles
before the publication have not
identified these plagiarized cases later
we'll talk about the doctor'll Caesars
of the former German defense minister
and he was caught some people already
laugh yeah he stepped down and but it
took ya several years before people
actually found out that his book I mean
it wasn't just to see this but he even
published it as a book but still it took
a long time before and he was identified
as someone who plagiarized that so now
I'd like to give a brief overview about
existing systems and so in general you
can distinguish these available systems
in two M local similarity assessment and
a global similarity assessment
sorry Surya of course for the for the
recording so M the existing systems are
based on text comparison on string
comparison and on keyword analysis for
example one way and these similarities
are calculated is just by via performing
in statistic analysis of frequencies and
occurrences of certain keywords or
another very popular method is to create
fingerprints on documents and another
method that is used but the performance
is not the best is to analyze the use of
certain words and grammatical constructs
punctuation and so on so that you can
identify whether the style is consistent
within one document or whether maybe
different people have worked on that
although this is not really helpful to
identify plagiarism for in the context
of scientific articles and I'd like to
talk about fingerprinting a little bit
more because that's the most commonly
used technique so um you see on the
right side I hope that the font size is
large enough you see an example and it's
this is a sentence so you can create a
fingerprint from this sentence and then
you store or created hash value stored
in a database and then you just compare
these hash values in a large database
and this way you can see whether maybe a
sentence or a paragraph and has been
yeah used before somewhere else and if
you find the collusion so if you find
two identical sentences in such a hash
table in a bucket then it might be worse
to look a bit closer and to see whether
this is a identical sentence or
paragraph but the shows again the
problem that if you create such a
fingerprint even if you just alter a
single word then the hash value would be
completely different and you wouldn't
recognize the sentences maybe that it
has been used to never
before em I'd like to talk about the new
approach we want to present we call it
citation-based plagiarism detection
might sound strange in the beginning
because usually if you plagiarize
something then you think that maybe you
don't really use citations but we have
observed that if people plagiarize and
they use citations because if you want
to publish something you need citations
and usually people power phrase their
work but they use a lot of the citations
that also have been in the original work
and maybe alter them a little bit so in
this figure you can see that in document
a M there's a citation on document C D
and E and in document B there are also
references or citations to document C D
and E and they are in a very similar
order that document B there are two
additional citations but otherwise
pretty similar so if you just compare
the order of citations you recognize
pattern and the idea of rotation based
plagiarism detection is to look for
these patterns in scientific documents
and to check them for yeah whether this
might be something wrong
maybe there's something plagiarized so
the suitability for plagiarism detection
depends on certain characteristics for
example the order of citations or the
textual proximity of citations but the
citations are in the same paragraph or
maybe even the same sentence another
factor we take into account is the
probability of co-occurrence for example
if there are M two papers and both
papers are very popular and have been
cited in thousands of times of course
it's more likely that their Co cited and
other documents as well as if two
documents that have hardly been decided
that they are Co sided another factor
that we consider is the citation style
so depending on citation style the order
of citation
can alter so you can have citation five
three and four in one bracket or you can
just have three to five so in this case
in citation five and four would not be
right next to each other or very often
in scientific documents you have tables
that contain multiple citations maybe
even taking over from another from
another source and then we have some
challenges in order to identify them so
we have transpositions so that the order
of citations and changes or scaling so
that maybe some other citations are in
between the citations or that the whole
alignment is completely different so
that may be in document a a part of a
whole paragraph from the conclusion and
is used in another document in the
introduction and so on so that it's just
mixed in order to identify these
patterns we have developed some
algorithms they're called longest common
citation sequence greedy citation tiling
and citation chunking vibra graphic and
bibliographic coupling is an approach
that is already being been used not for
the purpose of citation and plagiarism
detection but for via citation analysis
purposes in this table and they
distinguish whether they look for global
similarity or local similarity and
whether the order is preserved or not
preserved and now we'll talk a little
bit about these algorithms so on the
Left we see by the graphic bibliographic
coupling and bibliographic coupling just
and describes or measures the amount of
shared citations between two documents
so in this case we see the document a
and document B that say both side
document C and D so the coupling
strengths would be two in our analysis
with database with PubMed we have found
documents that have that share more than
twenty citations but of course this
alone doesn't indicate
or so or improve that this might be
plagiarized it's quite normal data via
publication from a certain field that
they're share and references on the
right there's a yeah in the description
of the longest common citation sequence
so this just examines whether is there
two documents whether they share
citations in the same order so all the
red numbers are citations and the red
numbers are the citations that are
shared by those documents and in this
case three four and five would be the
longest sequence then we have the
algorithm greedy citation tiling and
review citation tiling it aims to
identify all matching substrings and I
think it's yeah pretty easy to
understand so we talk too much about it
bit more complex is citation chunking
with citation chunking we don't have a
fixed tanks length for the string
citation string we we analyze but the
size of the string depends on the
citations that occur before the the
substring of citations or after and if
there are similarities then there will
be included in the in the citation trunk
and this way we have an algorithm that
works quite well too for yeah for local
similarity not so much to compare
similarity similarities between two
documents in general but on a very local
basis for example was in one paragraph
or one section I think the time is not
enough that I will talk about the
algorithm detail but maybe if there's
some questions later I will come back to
that so if you develop a new page within
the detection system and it's very
difficult to evaluate that because if
you um yeah use an artificially created
test set then
it might not be really realistic because
if you try to disguise plagiarism then
there are very different methods you can
the symbols form is just that you copy
and paste or that you maybe paraphrased
everything a little bit or that you
paraphrase it a lot so by creating an
artificial test set the results might
not be that realistic and another
possibility would be to use actually
plagiarized documents but there you have
the problem that you usually don't know
what where is the plagiarism in this
document you don't have the ground truth
you don't know how many sections were
plagiarized and what were the sources
and if you use available systems then
you only find what you can find with the
available systems anyway so you will not
find more than that and obviously it
would be better best to have a real
document a real-world example and there
is not artificially created but it
should also be well examined so that you
really know as many parts of the
document that have been plagiarized so
um I will talk a little bit about our
evaluation of a real um plagiarism case
of the former German defense minister
and mr. Guttenberg whose was his
doctoral thesis I think around 350 or
400 pages and I will talk briefly about
our evaluation of the PubMed database
then we use open access part of that and
yeah here you see a little summary of
the different test collections that we
considered using and you see the
advantages and disadvantages and the pun
corpus is an artificial created corpus
which we would have loved to use because
it was created for conference to measure
the performance of different different M
plagiarism detection tools so it sounds
perfect the only problem is that all
existing systems only analyze the text
of documents so in this artificially
created corpus no citation information
is available that's why we couldn't use
it so now I come to the thesis of mr.
Guttenberg what you can see on the left
is the seasons with all the pages and
all the colored parts so plagiarized
parts so you see it's a lot in there and
from different sources and this graph
this figure was derived from the
findings of the Gutenberg Project it was
a crowdsourcing projects where hundreds
of volunteers tried to find as many yeah
all the the pieces that have been stolen
so we believe that it's quite
comprehensive and includes probably not
all the plagiarized parts but at least
it should be very good data set to
evaluate different algorithms what you
see on the right is the figure that
shows how long it took to identify all
these different various plagiarized
sections and what is quite obvious that
in the beginning it might be quite easy
to find plagiarized and section but it
gets harder and harder the fire is the
first 5% of plagiarism it's pretty easy
you just need to digitize may be making
in this case it was only a book
available but you can make OCR and then
you use the existing systems and then
you might find around 5% that's what we
found was was the software tool called
if always which is supposed to be one of
the best packages so with this
commercial tool we found yeah five
percent of of the translated plagiarism
I have to say or even less than five
percent of the translated plagiarism
overall 63 percent of the lines in this
document are plagiarized and 94 percent
of the pages contain plagiarism and this
slide shows em on the left the original
source of a document and underneath you
see Gutenberg so that's the doctoral
thesis and same colors them symbolize
same sources or same citations and for
this evaluation we used the sixteen
sections in this fields that have been
identified as plagiarized but they were
all translation and translated from
other sources so it's not just German or
English but translated from for example
from English to German and the existing
tools couldn't identify any of these and
translated and sections so we wanted to
know how well with the citation based
approach work to identify these sections
and as you see even with applying any
algorithms you see that there are
certain pattern you can recognize so the
big graph on the on the bottom on the
right you see so there are only two
pages but they contain I don't know how
many citations that were maybe 15
something like that and you see that
there's some similarities if you apply
these algorithms I just presented and if
you clean this up the whole sequence of
citations then the result is with what
you see at the very bottom on the right
and you see that the order of citations
is quite similar and that's on the
coincidence obviously so and if you use
these algorithms and see that there are
more than three or four cite a
in the same order that might be a strong
hint that the section has been
plagiarized the table above and shows
some figures so the copy and paste
plagiarism that was identified in the
thesis and if you use the existing
software you have pretty good results so
you find like 70% or more of the copy
and paste plagiarism sections and the
citation based approach is not suitable
to identify copy and paste plagiarism
because obviously if a section is very
short it only contains maybe one or two
or maybe not even a single citation so
it doesn't deliver very good results for
the sections of disguised plagiarism and
the tools identify less than 10% so it
depends how strong it is disguised and
in our case yeah it was around 30
percent then idea plagiarism is of
course very very hard to find because if
you compare text you might not find any
words that's similar or if you find them
then maybe there are hundred words in
between so the existing systems are not
really suitable to identify idea
plagiarism with citation-based you can
find them sometimes it really depends on
the way someone how someone yeah
plagiarize and even for a human examiner
it's very hard to identify idea
plagiarism just the nature of it and but
where the citation based approach
delivered pretty good results was with
the translated plagiarism so in these 16
examples you see that some of them for
example on page 224 there were only two
citations so you would be able to
identify such a section as page Erised
if you use a citation approach because
if you would look at all these sections
you would just have way too many false
positives but if you put the threshold
maybe on three or four citations in the
same order then
you get pretty useful results add any
questions to this figure okay um this
graph shows the results when we ran the
analysis on PubMed I won't go too much
into detail I just want to show some
figures so what you see here is on the
Left it's not a logarithmic local
rhythmic scale but on the right it is so
you see that on the ya X on the y axis
you see the document path amount and
then on the x axis you see the coupling
strengths I talked about this so the
amount of shared references within
between two documents and you see for
example that yeah that if you have if
you look for coupling strengths of 8
that you still find quite a lot of
documents so this alone it's not really
useful to identify plagiarism because
you just have way too many false
positives that's why it's important to
combine it with all the other algorithms
where you consider the order of
citations and and patterns and so on um
the strengths and weaknesses of the
algorithms are displayed in this table
so what you see is that the citation
chunking algorithm delivers pretty good
results for most cases and most forms of
plagiarism and the longest yes
greedy citation tiling has some
advantages in some cases and the same is
true for the longest common citation
sequence
um in the following I'd like to talk
about our software system that we have
developed to identify this plagiarism
and the steps that are involved so in
the first step we just convert the PDF
document to an XML document and
identifies the citations we use a an
open-source software for that called
pass it with some minor modification for
example in our case it's important that
we know where and which sentences and
the citation is and where exactly it
stands so we're not just interested in
extracting the bibliography of document
but we want to know where in the
full-text the corresponding citations
are located and then we match them with
our database we do some article deed
application and author name
disambiguation then we run the citation
based plagiarism detection tools
algorithms and compare the entries in
the database see whether we find any
matching entries and then we generate a
report using the citation based approach
and we also use text-based and
plagiarism detection software because as
I said for example short sentences and
so on they can be better detected with
text based approaches yes and then in
the very end they're combined and you
get the results so I think my time is
nearly over
so um to conclude um the existing system
they deliver a pretty good job for copy
and paste plagiarism but they struggle
to identify plagiarism that has been
disguised or like if it has been
paraphrased phrased translated or idea
plagiarism and the we see the strengths
of the citation based approach in cases
where the document has been altered a
lot like in translations or strong
paraphrasing then the citation based
approach should not be seen as a
substitute but rather an extension of
existing approaches and
yes the results at least we were pretty
happy with them and we hope to implement
it on a larger scale and establish a
larger database with citation
information and see where we can apply
it in the future there's also some yeah
maybe talk about the outlook a little
bit and something else we want to
include is a consideration of the
argumentative structure so that we
analyze keywords for example if it said
Hamilton argued something and whereas
someone else said something else that
you includes these words and that you
compare not just the pattern of
citations but also how these citations
stand to each other and what kind of
context and last but not least we don't
only want to look for plagiarism because
in many cases it can be interesting to
see and if you read a paper
what other papers has have been read by
the author that have not been cited so
that does not mean that it is that it's
plagiarism but with putting the
threshold a bit lower you can identify
documents that have been read and maybe
for example some sources from this
document have been used and for his own
work and so it can give you maybe some
ideas for further reading that might be
interesting and this reading was at
least of interest for the awesome yes
that's it thank you
yeah so at the moment we for example on
PubMed with several hundred thousand
articles and depending on the algorithm
they have different running times but
around a week but if you have computed
these n times then obviously it's way
faster you if you just have a new
document and it's a matter of a few
seconds yeah so if I understood the
question correctly and the question is
how can you and whether you could use
this technique to identify and what's
worse reading in a paper you got or
whether it's just something that has
been published before maybe even by the
same author yeah so I think that that is
really good question because if you use
this technique you could for example
highlight the parts of documents that
are really new so you would save a lot
of time for the reader and he could
focus on yeah what's really relevant and
what's worse reading and not just
reading all the related work before that
you have read maybe somewhere else
before so you can use it for
recommendation systems that's definitely
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>