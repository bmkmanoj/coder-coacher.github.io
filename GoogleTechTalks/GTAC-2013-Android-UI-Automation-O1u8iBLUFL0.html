<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2013: Android UI Automation | Coder Coacher - Coaching Coders</title><meta content="GTAC 2013: Android UI Automation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2013: Android UI Automation</b></h2><h5 class="post__date">2013-04-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/O1u8iBLUFL0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay welcome back hopefully you had a
good break we still do have the
conference rooms out in the lobby there
some people were asking hey where can i
plug in my you know my device in the
middle of the conference room tables
there's a bunch of power strip so you
can plug in there if you need to take a
call or something you're free to go out
there I ask if you can squish into the
middle to leave the aisle seats open for
those that are sort of popping in we had
a bunch of people standing in the back
for a bit and just a reminder please
don't sit on the staircase we're not
allowed to do that but if you could just
grab a seat that'd be perfect okay so
with that we're gonna pick up our next
set of speakers we're gonna have Kwang
soo and Adam Montez from Google come and
talk to us about Android UI automation
and with that welcome guys thank you
thank you funny hi guys my name is Kwang
soo so we are software engineering tests
working at Google on Android test
engineering today we're going to talk
about Android UI automation so step
aside or not you go ahead and and and
finish the overview of the technologies
and I'll step back in and I'll show you
a little bit about the API
thank you so before before I start I
want to give a shout out to I know there
are teams both externally and internally
have been trying this out already and
some of them have been trying before
even we've finalized API and even after
we final roughly finalize it I know we
have some bugs or inconveniences here
and there so thanks for living with us
through the somewhat painful process
hopefully there's like more gain at the
end so let me just jump into the
presentation first of all what problem
we are trying to solve we thought about
this before we started because I think
this helped us narrow down what kind of
functionalities
we want to provide through the
uiautomation library and what things we
want to exclude or what things are
better excluded as a separate kind of
like a helper type of thing so the way
we see it your automation is access
things back traverse and manipulate UI
widgets of the application on our test
we see it as a control feedback loop
basically the control piece is when you
do input event injection and the
feedback loop is when you do the UI
inspection and see if the application is
hitting the state that you wanted or
where the application is to decide
what's the next step of your test so the
API is that we are providing is mostly
centering around these two key pieces
and also before we start the another
thing that we did is we did a survey on
Android automation just look at the
landscape what solutions are already out
there and we kind of like looking at the
underlying technology and decide what
can we learn from it and how they can
how we can benefit from their like
creative pre-existing experiences
so with that survey we noticed that
there are three major trends one is
based on instrumentation framework the
second one is based on image recognition
for UI inspection the third one is
something using is using something
called Android hierarchy viewer I'll
cover all these three pieces in
following slides so first one map is the
instrumentation based hopefully you guys
already have some notion of what
instrumentation works how
instrumentation works on Android
basically it's a special thread it's a
special way of starting the application
under test with imitation thread so
essentially you live in the same process
as the application on your tests so two
key pieces the UI inspection is done
through programmatic a traversal of the
view classes or
view hierarchies and the event injection
is down through the API is provided
through instrumentation object and
because as the diagram diagram shows the
instrumentation lives in the same dalvik
VM same process as the application on it
has you have you can have direct access
and it's pasting the same process
there's no IPC going on and what you get
is fast UI traversal or and you can if
done properly you can go pretty fast and
also it supports a large range of
platform versions that's the API levels
that sort of lis briefly covered in the
previous presentation because
instrumentation has existed for a long
time going back to I think the first
version of Android there are some
limitations to this approach namely you
cannot cross application boundary or I
should say more precisely you cannot get
out of the application on it has because
key injection only works inside the
current application process the
instrumentation will not have
permissions to inject events if you wish
to test a kind of end to end functional
and integrated level kind of test so the
next the next approach that we looked at
is the image recognition based approach
key technologies includes UI inspection
is down through image recognition we
take a screen shot of the device on a
test and we have pre recorded test cases
that says look for this particular piece
may be of text or of a pre captured like
region of button you want to click and
input event injection is mostly done
through a double shell commands there's
a adb shell input command or you can do
it through emulator I've seen approaches
like you cannot sorry you can also hit
on the keys of the the emulator to
simulate input events so characteristics
wise
you don't need explicit knowledge of
what's the URI hierarchy honored in the
application on a test and you don't even
need to sort of have a kind of like
rough knowledge of how the UI framework
works because your test is sort of just
text-based or based on I want to click
this button I take a syringe I take a
crop off this button and I say I want to
click it during the test so also this
works for framework standard widget or
customized UI what that mean is some of
the applications has pretty structured
application UI we use buttons we use
like some layout components on top of
them to arrange there are relative
positions and but some may choose to do
some for a nicer user experience do some
fancier stuff like I have like Cover
Flow type of thing or Carol's all type
of thing those those hierarchies are not
directly exposed to the outside
framework and with other tools normally
you just see a blank piece of view now
with this approach you can basically say
that based on image comparison you can
say I want to target this particular
region so there are some shortcomings
with this approach namely it may be
respectful to theme differences let's
say I have a blueish button background
instead of you know blackish
that's also depends on how good the the
image recognition algorithm is also
there's a slower execution speed because
taking a screen shot is kind of slow on
devices and especially if the device has
higher resolution so the third approach
is hierarchy viewer based basically
there is a tool called hierarchy viewer
where you can use it to inspect your UI
hierarchy well if you have you tried
that you will see that it will actually
create eight
a nation of your UI on your host site
and you know if you are tempted to do UI
automation you will think wow this looks
great from your mission standpoint
because you get to see everything inside
the app and if an injection is down
through similarly to the image
recognition approach you do adb shell
event injections the benefit of that is
works across apps but it's kind of
slowing execution if you've used the
tool you know it's kind of slow this is
not blaming on the tool is because the
the amount of information exposed
through the pipeline is just quite large
another shortcoming is you may see
invisible widgets like for example if
you try to see it off use it to see
launcher you may see all home screens
and all apps already included in the
hierarchy which is bad because from a UI
automation point of view whatever it's
not just whatever it's not displayed on
screen you cannot interact with it also
it may not work on customized views
based on what I just explained and
actually there's another shortcoming is
that you can only use emulator or a
device that's rooted or with engineering
builds so sorry I'm going backwards with
that I would like to introduce our new
tool called Android UI Automator so the
key technologies are we use excessively
service for your inspection we use we
talk to input manager for event
injections some some of the character
characteristics includes we can work
across apps because it's a complete
completely external approach the apps
the test list lives outside the same
process of the app it's a complete
external entity and it's meant to be as
a sort of a black box approach also it
can be faster than manual execution
speed
basically if you think about it we are
not living inside the same process as
the app on their tests
obviously we cannot directly poke at the
states of the app everything is done
through IPC you know the benefit of that
we get to work across processes and the
downside of that is you know when you
interact with apps
you can't go as fast as you want it but
still it's faster than manual execution
also you know human beings get tired
right so the third one is the customized
view it's subject to the same problem as
the other approaches but we do have a
traversal interface that for these views
that they can implement and by
implementing that you actually get two
benefits in one goal first is your
absolute section more accessible because
that's what we are keying on the
accessibility service the other is
basically you get to interact with the
what's inside the view that's originally
not available the fourth one is we
support API level 16 or higher
I just want to expand explain more on
the API level thing because that was
asked earlier in the Q&amp;amp;A section
basically API level is what we saw
android framework provides a set of
api's and over over platform revisions
we evolved API by providing more API
calls more functionalities a key promise
that we are making is it should be
always backwards compatible basically if
you compile an app targeted at a lower
API level let's say 10 it should be able
to run automatically on API level 10 and
higher every time we make a a P I will
make an API change we will increase API
level but that's not that's not saying
that we added a new function we increase
one if you 11 is not how it works
basically we have release cycles every
time we release a new platform we
have a new platform release you know
like gingerbread Gingerbread mr1 mmm mr2
I think we have for mr that stands for
maintenance release API level 16 maps to
I think four point two point zero it's
more of a product release thing I don't
really remember basically jellybean
equals API level 16 or higher this is
because we have underlying framework
dependencies I also get asked about this
a lot like this is a great tool can you
guys back pour the changes the answer is
not really because there's no way for
like all the deploy devices we go back
and update their framework to support
the UI automation the last point is I
want to make is we have CTS which stands
for Android compatibility test suite
that checks that the device supports the
underlying accessibility layer so that
we guarantee that the the well we want
to make the current II that the the
framework will work API level 16 or
higher you know if a device regardless
if it's a Nexus device from Google or om
devices say from Samsung LG a SUSE and
so on and so forth they should always
support this functionality going next
mmm-hmm trying to yours force here but
that doesn't seem to work does it go
back this is not a slide I have
although them sorry right
all right thank you not sure if this is
the next light okay so that should be
the next one thank you
so Android UI Automator unload hood how
does that work
you see there are roughly four pieces
the blue one is the test case that we
have is the test case that you were
rolled if you're interested in
developing a UI Automator test case the
purple one is the application on a test
and sort of orange yellowish one is the
UI Automator framework and it has gained
self alright so the one on the lower
right corner is the android framework so
the way it works is when you do a test
invocation that's coming from a DB it
stands for Android debug bridge it goes
into UI Automator it's a test runner it
loads the test cases and it the test
cases then has a bunch of API calls
which goes into the you are on the
framework and we channeled the calls
through to over to accessibility manager
to do UI inspection and we talked to
input manager for in the event injection
both of these lives inside the system
server which is part of the android
framework and this in turn talks to the
application to do you know to check
what's on your screen what's on your
screen or you know click here do a swipe
or press this button something like that
so now as a package that we are
presenting to the end-user this is what
we are providing first it's the core
framework which is roughly outlined in
the previous light then we have the test
runner this works in a sort of Jo a3
style and we have a your UI Automator
viewer tool which you should see next
it's a whole site tool to let you
inspect the application under test UI
you will see what the test will be see
seeing at the wrong time so you can
decide what widgets that you can look up
and what widgets you can test against
and the next piece is SDK support we
have a end paid based bill we have some
ant script for you to create a MT UI
test project and for you to build a test
and you can the outcome of the test is a
jar file you push it into the device and
you load it with the test runner to do
the test execution so with that I'll
hand it over to Adam you go over with
details of the thank you so how does the
workflow look like when I want to write
an automated test where do I begin and
how do I write this test this is a very
simple example we're gonna show here how
we go through the UI Automator viewer so
that we can identify various widgets on
the screen and write a test against
those witches so we can automate them in
this example all I want to do is launch
an application but the steps to launch
the applications go through launching
the all apps first which is that button
you can't see any text on it it's just
an image then once I land on the all
apps I want to ensure that I have apps
selected in the tabs not widgets so I'm
gonna go ahead and click that then I'm
gonna look for my application that I
want to launch in this case settings and
it doesn't look like it's available on
this screen so what I want is identify
an area that I want my Automator to
swipe for me to find settings and launch
it for me so I identified here these
three different controls how do I
proceed from here since I have these
controls there is this tool that one put
together for us it helps us identify
various codes on the screen what they
where they are in the layout and what
sort of properties they have on them
whether the widget is has content
description it is scrollable it is long
clickable is it clickable you can see
all those properties for every widget
that you hover your mouse over so let me
see here we can probably do an example
can we switch to the left oh please
so thank you so we have the UI Automator
viewer on the screen we have a tablet
connected we're gonna go ahead and sync
an image from the device and we're going
to get the current layout hierarchy in
the UI Automator viewer obviously
pulling images from the device is a
little slow but here we have it so what
we do now if we for if I move the mouse
over any of those objects on the screen
you'll see that it's highlighting the
layout for me telling me what I'm
looking at if I go to this all apps
button I'll see that it has content
description apps so the developers have
labeled it for us and this is good all
apps should have their content
description for all their widgets
already labeled for accessibility
purposes regardless if the app is a
camera video whatever users who are
partially visually impaired or
permanently or temporarily need to be
able to use all apps so we insist that
our developers add these content
descriptions everywhere we miss few but
we're working on it so back to the
slides please
so using this tool we are going to
identify these three controls now how do
we proceed with writing the tests first
I need to find information about the
widget that I want to automate and I
need to create a selector the selectors
job is to find it for me once I find it
I want to do some actions on it I want
to click it maybe swipe it pinch it or
read some properties off of it that's
the job of the UI object every UI object
requires a selector so it can do its job
then we have a couple of extra classes
the UI collection in UI scrollable
they're basically objects that can
enumerate other objects on the screen
and I'll show you how we use one
next first is a good idea that we
declare all of our objects ahead of time
and it's better idea if we were to
create sort of a an intelligent class a
specialized class for every application
if I have gmail I may have a specialized
class that knows how to return to me the
various widgets on the screen for Gmail
this way I can keep my tests separate
from the declarations of those widgets
so if the UI changes I don't have to
chase down 600 different test cases and
change the layouts in those for sake of
an example here I just put everything
into one function we have the four
objects that I talked about all apps
button
we don't have text on it so we have to
use the description apps that we found
in the UI Automator viewer when we click
we're going to land on the all apps view
and we want to click the Apps tab that
has text on it so we tell the selector
just find anything that has apps for now
and click on it we can actually narrow
the search even further we can say text
apps dot some class name if you want to
get further into that and you can
specify what you really looking for and
then we use a UI scrollable for the
container that has all the various
applications and we tell that container
you are a horizontal list meaning that
if you have to scroll do it horizontally
back and forth until you find the object
I'm asking you to find last thing I need
an object that I can use an assert on to
make sure that settings actually
launched in this case I'm going to tell
the selector find me anything that's on
the screen that it's coming from this
package and I will assert on that later
to make sure that my application
actually launched that's that's as
simple as it gets
the first thing I'm going to do I'm
going to make sure I start from a common
place I'm gonna grab a get UI device
press home that's going to set me at the
home page I'm going to grab the
all apps button say click followed by
apps tab click followed by the list of
apps find me a child that has the text
settings that is a textview
from this class notice we're not putting
any sleeps or anything between these
calls the system is the framework is
automatically synchronizing it waits
until the object is visible then it
proceeds to click it once I have the
Settings icon or the position for it
scrolled and visible I'm going to go
ahead and click it and then I'm going to
assert that it actually launched that's
my test it's done so you notice we have
few objects UI object that can take the
form of any widget on the screen
whatever its selector you give it it
becomes that widget on the screen so and
then we have a collection and a
scrollable UI collection your scroll are
the same thing one Scrolls one doesn't
that's it that's as simple as the API
gets you can quickly without really
knowing anything about the application
totally black box approach begin to
automate leveraging all the
accessibility information that should be
already in the application so with that
can we switch to the wolffish in here
please
thank you so we have a tablet here and
the use case we are trying to
demonstrate is somewhat artificial well
as in all test cases but basically it's
uh it's it's bit about let's say you
really like a restaurant or a food place
and you sort of find it on Google Maps
save it to the contacts later you pull
it out and maybe you want to have a meal
there and then you just try to find
directions and sorry the screen goes so
that's that's the case we're trying to
cover here we are driving it using you
our meter
and here on my hands and not on the
under device so I'm just gonna kick off
the test and off it goes so first it's
gonna open up the apps menu launch maps
and then find out what our current
location is and then goes and search a
like we kind of code it at search term
that says pizza near Times Square and
there it goes and Wi-Fi works great
we're lucky and or maybe the Wi-Fi
squeak and it clicks the one that we are
trying to find John's Peoria save it to
contacts down go back home launch apps
again and pick up the contacts and then
pick out the address click on it and it
will launch Google Maps again focus on
it and trying to get directions and
things we are in New York City it's
trying to get transit directions and
there it is we have a couple of options
here so basically everything is written
through your meter and we are driving it
with this command line that we are
seeing it's not a lot of lines of code
fortunately unfortunately I can't really
show it here so can we have the slides
back please thank you all right so
what's the demo let's go on to the next
one life of you art has so you've seen a
life you art has in demonstration how
did we come up with it what do you
typically do you want when you want to
write a UI test
so we've already established that
basically most of the UI test cases that
we found are a slightly modified version
of existing manual test cases modifying
as in you know we try to add more
deterministic deterministic into it try
to make it more deterministic and less
you know requires kind of like human
decision and we try to try to break down
large complex cases into smaller
sections with verification steps you
know just to make sure that you know
because the longer the longer it is the
flaky that the more chances for
flakiness that you will have the second
one is inspect key phases of the a bone
that has with the viewer tool where we
just stand mode typically you go through
the different screens that this apps are
showing at different stages of your test
case and decide you know I want to pick
out these text boxes click on the
buttons and all that and maybe in some
cases you you have to go back to the
developer say that this widget of yours
cannot be identified because it's
missing text text a textual description
or a Content description or in a more
complex case you are using a custom
rendered widget and we see nothing
excited so there might be some work
coding work involved to to add into the
app to make it more friendly to your
animation and also I want to just stress
this again this will also make you a
more more accessible and the third step
is set up a test project and we have
existing SDK tools to support that and
we have instructions up on the developer
site which I will mention at the end you
set it up and you line up the IDE and
you write compile and debug your tests
and also we support the Android debug
err
basically you can start it with a debug
flag it will wave for a debugger to
attach first and you can step through
your test cases and when you are doing
test development to figure out if
anything went wrong so just to avoid
using printf I guess next step profit
basically you have your test line up and
you should have increased automation
power and hopefully the discount
streamlined your development test cycle
so you
cases I've been talking about all these
and you might wonder what is Google or
what it's Android doing with this tool
we've actually been running this for
quite a while we started off with Ice
Cream Sandwich and and it and we sort of
run through a lot of internal vetting
basically you know we have this new tool
we have since new approach works great
but is it stable can it get what we want
it and does it is it friendly for the
end user can can can this be down on any
production devices meaning you buy a
phone that is ginger jelly bean or late
later and can you can you run tests on
it and that sort of thing so bomb is
we've done a lot of testing internal
trials with this already so we find it
useful in three main categories first is
targeted for app developers it's useful
for application functional testing you
as I said you have your menu cases you
can try to automate it with the you
animator 12 and to basically get rid of
some of the highly repetitive but
important cases that you want to cover
or you know you can with based on how
how accessible your apps is you can
deploy more complex cases at this point
it's really your choice to like how how
how much you want to automate because
there's a cost benefit there that's a
different topic but basically that's
what we are trying to do with
application functional testing the
second point that we are trying to do is
more targeted for platform bring up
system smoke test I have a new build
after I have a new device filled out is
this safe to put it onto my device for
me to try it out how do i how do I have
a some preliminary level of confidence
that this is good enough this is what we
do with URL major tool basically
we go through the app drawer and do a
basically do a round-robin off all the
apps launch I'll want one of them at a
time and make sure they don't crash
launch them play with them a little bit
and make sure there are no crashes or
and there are no bad things going on and
you know move on to the next app by that
we can establish some minimal level and
quick assessment of what's the state of
this bill yes the third one is also kind
of four well it's the third one it's
both for app development and and system
level testing bring up a platform bring
out level testing it's for stability
basically you can do some sort of random
randomized UI events testing you but I
guess some of you might be familiar with
monkey monkey sends out random events
without verification what we are trying
to do here is trying to be smarter than
the monkey talk
we make informed decisions on which
widgets to click on and we and on a more
advanced level may choose to remember
what steps we have gone through and you
know what may be which screens we have
visited and to basically flush out some
of the stability issues because one of
the the top complaints we get for a
monkey is these steps are looks like
humanly impossible or not a real use
case something like that well with you
automated tool guess what you know we
are doing things at close to manual
speed and you are apt to not behave
badly and the other thing that we used
is also during device spring up we may
run into graphical and on the line
system level issues and you can use
simple repetitive steps to bring out but
I no one wants to sit there for let's
say two hours repeating the same steps
until you see an artifact or you see
something bad happening on the device
that's also what we used and it's being
proven pretty useful
and this is not really a role map more
like upcoming features I've mentioned a
couple of these kind of hinted of these
yesterday I launched a launch roundtable
discussion first is we want to do an
instrumentation integration with
instrumentation framework so currently
it's running as a shell user that's how
we get our security guarantee basically
your when you run it has you assumed
that entity of a shell user you can poke
at the apps and as you want it so
instrumentation framework as I mentioned
earlier is targeted for apps really
however because it's also launched from
shell we can basically use that angle to
sort of say that since you are launching
from share we have this trust in you and
you will be able to instantly get a sort
of a privilege token for you to do your
inspection and key injection and this
also has a benefit of your apps will
become a your test will become a real
app and you can sort of also mix and
match with the real instrumentation
approach this is basically up to your
decision and also a lot of people have
been asking this I won't can I get a
contacts object from your test right now
not really but once you are running a
search instrumentation you are a
legitimate app inside the system so you
will have access to all that with behind
that there's a plethora of system
services in you can reach into so two is
faster iteration as unbounded test
library basically we are using we will
be using all public API soon and this
means we can actually sort of unbundled
ourselves off the platform and the test
library itself sort of becomes like a
real library yeah that has specified
stated API level dependency so the third
one we are doing is improved UI widget
lookup basically we're trying to exclude
optionally exclude some of the
components they are not interesting for
for your automation perspective like
layout components I'm not really
interested in that
interesting things with tax on it or
with content description on it so by
simplifying the the UI chi that we are
exposing we should be able to do faster
a widget lookups so that actually kind
of I don't know why I put the force
between you know it's basically that the
same thing that we are covering here
alright so more information we have a
kind of like a landing page for UI
testing this is on developer.android.com
and if you're interested feel free to
search or just search Android UI
Automator I think the first hit will be
the landing page and was that yes thank
you gone I have to say when I was
watching the demo I got thinking of this
area of computer science it's called
pessimal algorithms has anybody ever
heard of this this is where computer
science is try to find the most
inefficient way of doing something
without duplicating work they figured
out how to do like an Ncube sort and so
in this one as I was watching I was like
well just click the directions button
now we went back through the contacts
and all the way back to the direction so
you may have a career in Paestum
algorithms on a more serious note though
one thing you gave us all a reminder on
was accessibility I'm you mentioned this
in the middle of the talk this is a
really important topic for all of us is
how do we make all of our applications
and devices very accessible to everybody
so thank you for that reminder with that
we'll take probably two questions and
then I'll give you a rundown of what's
gonna happen in the afternoon and then I
will let everybody go have lunch because
man that food smells really delicious so
the catering department has outdone
themselves this time with that first
question please limitations what can you
not do with it
so like I explained at the beginning so
the thing we are so the problem we are
to solve trying to solve is really UI
interaction and sorry UI inspection and
event injection that's
that's basically the api's that are
centered around anything outside it we
see it as you know not a problem that we
are trying to address you know I don't
know if that sort of answers your
question or if you have anything
specifically well that would be my fault
I didn't talk too much about the rest of
the api's on the get UI device the UI
device object has all those api's on it
you could rotate the device in any
direction during testing you can query
to see which current position the device
is in you can also do multi-touch you
could do multi pointer to multi-touch
you know just five second fingers or two
or swipe out and pinch we have all these
api was already built in to make it
possible to do everything that the user
could do i just want to add one more
those are upcoming futures that's
actually the one out list by I forgot
yeah there is some multi-touch already
in the previous I think API 17 but we've
enhanced additional API is in 18 going
forward and the rotation is already
there I guess right yeah okay thank you
we have time for one more question and
then one thing I'll mention is we have a
ton of questions that are up on
moderator we're gonna ask all the
speakers after the conference is over
and they get a break for day as the rest
of the questions had come in a lot
moderator they'll go answer the
questions there too so don't feel like
we we were gonna skip it
and though the speakers will also be
here afterwards so one more question
please so Joe Kozlov from big bar so we
actually create fans of your frameworks
so you have been able to run UI
Automator tests on the real devices on
test Red Cloud since December already so
and we see people actually doing it so
one question is that
will you guys support web views because
that's that's kind of the modern apps
you use a lot of those and also before
you answer I have a demo of a visual
recorder that outputs UI Automator code
including all the swipes and drags and
and all that and I'm demoing in the
corner here here so anyone who wants to
take a look just very nice come over
here thank you so specifically what were
your falls into the category of
customized views that I just talked
about unfortunately we don't have any
accessibility provider to specifically
tackle that problem yet I think there
are there are things in the work but I
don't think I can say when or any more
specifics about that because it's not
within my team I don't want to you know
I know about it but I don't want to say
provide like more specific about yeah
all right thank you guys so there's some
quick announcements I'm gonna move into
so you guys thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>