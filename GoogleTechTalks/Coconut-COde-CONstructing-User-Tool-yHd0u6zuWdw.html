<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Coconut:  COde CONstructing User Tool | Coder Coacher - Coaching Coders</title><meta content="Coconut:  COde CONstructing User Tool - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Coconut:  COde CONstructing User Tool</b></h2><h5 class="post__date">2008-03-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yHd0u6zuWdw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody welcome i'd like to
welcome Christopher and an end here he's
a professor from mcmaster I know I'm
back from the University of Waterloo
he's now working on code generation
multi-core work he's been doing some
stuff with Haskell so I think he's got a
lot of things that will be interesting
to Google I'm he's also formed a
start-up to commercialize some of this
high-performance multi-core software so
with that I'd like to welcome
Christopher and am thank you can
everyone hear me yes okay so coconut is
not a compiler it's does some things
that a compiler does but it does other
things that a programmer usually does
and the idea is to be able to optimize
code better by crossing the boundary
between what normally is done by a
programmer and normally done by the
compiler so I think we now can write
safe software and at least some of us
can write fast software sometimes we
need to write both for example for
real-time medical imaging applications
both time and safety are critical so our
view is that performance now and
increasingly in the future is about
parallelization so some of you may be a
bit bored by this slide if you look at
one chip the salle de in order to fully
utilize the computational resources on
that chip you need to put 384 ways of
parallel ilysm into your code this is if
you're doing 32 bit if 32 bits contain
your fundamental data type so floats or
ins okay where does this come from
there's four-way simdi at that
sighs there are eight cores on this chip
you need to unroll six times to hide
latency in the instructions and you have
to at least double buffer your
computations because you need one in
flight while you're doing your
computation okay and that's just on one
chip this is a roadmap for coconut
organized by levels of parallel ism so
at the top is simdi parallelism the chap
check marks mean that this is oops so
the check mark are four things that are
done and the half is for things that are
in progress and the things that we're
targeting now are marked with arrows ok
so for simdi we have a domain-specific
language that captures efficient
patterns of code generation force MD and
we've generated a library that's
distributed with the cell SDK three
point 0 because of the increased
performance that we have we can verify
some of the code that we can generate
that's an ongoing problem i'll talk
about why later multi-core is our most
active area of work now we are modeling
multi-core on instruction level
parallelism because it's something that
people have been able to leverage and
its really worked well for decades of
performance improvements we have a
linear time verification strategy for
the code that uses our communication
primitives which we think is important
if you want to do rapid development we
have we can generate some code that
we've generated basically to test our
linear time verification but that's
really what our next work is generating
that code and debugging the runtime
which is written but not yet test
distant parallelisms is something that
we're not actively working on now
because we have enough to do on the side
we also have a scheduling algorithm and
I put it there on the side because
currently it targets our sim decode but
our plan is to use the same scheduling
mechanisms for actual machine
instructions and communication
primitives in our multi core layer and
eventually communication primitives in
our distant parallelism layer this is
this shows that the structure of how
we're actually developing this all
embedded in Haskell house closed a
functional programming language and it
takes away a lot of the hassle when
you're creating your own language so we
can just concentrate on what's
interesting we've organized our code in
layers so in the in the kernel of the
code are the actual are the actual
assembly instructions that are pure
operations they don't involve control
flow then those can be wrapped in
patterns that are pure patterns then we
can include control flow we don't have
any distributed patterns but one day we
will and eventually user code can call
into any of those layers the scheduler
explicitly stage software pipelining is
also written in haskell so it's very
easy for us to to change the interfaces
when we have better ideas about how to
do control flow which is the real reason
that we didn't want to take conventional
compiler approach because in a
conventional compiler approach if you
have a new idea about control flow you
have a big headache you have to change
your language that is very expensive
so I'm starting here in the middle in
the background you see the nested nested
language diagram so our control flow we
so far the control flow that we support
are things that functional programmers
think of as functions that map an
argument over a list zip things like
that okay there are lots of things that
come from linear algebra and other
mathematics like matrix multiplication
that we also support so here's map if
you don't know map can take a lot of
round pegs and put them in square holes
and it can do it in parallel okay if you
give your compiler your favorite
compiler a loop like this to implement
map then it has to put in a lot of
overhead in order to support that so
they're optimizations possible but
basically it has to increment input
pointer up a pointer the counter compare
the counter to the limit and branch okay
the advantage of requiring our our
coders to write in terms of these
higher-order functions is that we can
then we don't have to try to recognize
this pattern when this gets complicated
on the inside or funny things happen we
know it's there and we can concentrate
on how to efficiently implement it okay
so this is these are our instructions in
our target cell SPU instructions had
architecture and all of that overhead is
handled by a single arithmetic
instruction in our implementation of
this pattern this doesn't always make
sense but it almost always makes sense
because the code that we're mapping is
bound by computation bound by arithmetic
computation so all these other
instructions are free they fit into
empty dispatch thoughts okay so this is
the kind of thing that we do and if you
want to know exactly how this works with
a single arithmetic construct
I'll explain it to you after okay at the
low level are the sim d patterns for
pure functions and the actual
instructions so when I say pure
functions I mean things that we can
implement as pure functions without any
control flow no branching it doesn't
mean that other people might not
implement it that everybody would
implement it that way so there are many
things that people do with branches when
they take computer science 101 that can
be done with predication and we handle
all those things in our pure layer as a
result of encoding those patterns we can
write small code so if any of you have
tried to in your spare time write a
really fast implementation of hyperbolic
tangent you might know that there are a
lot of details to getting a fast
implementation well this here is all of
our code that implements that it
actually implements a version that's
unrolled twice with partial sharing in
order to improve throughput for unrolled
loops which is really what we're
targeting now this is not what our code
usually looks like because our code is
embedded in literate Haskell and we at
least try to document all of our code to
this kind of level where we can use the
fact that it's it's lay tech all the
comments are lay tech to explain
mathematically the reasons why we did
certain optimizations we can put some
warnings in there's actually compile
time assertion checking to make sure
that nobody changed a certain pattern
for integer division that would make
this code unsafe since i upgraded to
leopard i'm having some fault font
issues so these should be nice equations
and they're just some of them are gobbly
gook ok for people who are beginning
work with our tool something that is
really simple but very helpful is
we can use the formatting that's already
built into literate Haskell to format
their code a certain way in one font
format the machine operations which they
may not remember in bold so that they
can recognize them and the patterns that
we consider to be in our language we can
underline them so it makes it easy to
understand where the performance comes
from and if you were going to model
another function on this one what you
need to do ok so this is implemented as
a class in Haskell the domain-specific
language and there are two instances of
that class the first instance is used
for rapid prototyping and unit testing
and it actually use it while either in a
compiled Haskell function or in the
Haskell interactive interpreter you can
test little bits of code down to a
single instruction to understand what
your pattern is is actually producing in
specific cases ok so this is really
important for rapid prototyping it's not
very useful if you want to actually run
the code at high performance
interpreting it in haskell so there's
another instance that produces a code
graph it produces a code graph with
maximal sharing so we do that
optimization at this point then if if
you think you have a better scheduler
you can pretty print that to see with
inline assembly and you can schedule it
or you can pass it to our scheduler and
generate assembly code or if things are
not really working out the way you like
you can use a bunch of visualization
tools that we have which we've found
kind of useful so this is old-style
scheduled assembly code it's kind of
hard to tell what's going or going on
even if you remember what all these
instructions are this is one of the
visualizations of the scheduled loop
that are possible so when our version of
turning on debugging is to spit out
10 different graphs like this which show
different stages of the scheduling and
pattern evaluation in this case the
colors represent different stages in
this software pipeline loop and I know
you can't see that level of detail
instructions are hyper edges represented
here in the form of a bipartite graph as
solid object and values are these other
non boxed edges and they're in addition
to values data values there are also
state values and I probably can't find
one so here's a here's a branch it takes
a state and somewhere else there's a
branch hint and it takes another state
and produces another state so when we do
our scheduling all the information is in
our graph okay not necessarily graph on
your screen but the graph that's used
internally contains all the information
there's no information hidden in the
context you never have to there's at no
point do we worry about reordering loads
and stores because all of the state
required to make things safe is
explicitly represented in the graph so
what is the result of this well we this
is we've generated a whole library of
these special functions this is our main
benchmark and using our patterns our
code is four times faster than code that
was and optimized in C using inline
assembly so this is a logarithmic graph
and if you've forgotten what logarithmic
graph means it means that up here these
slower functions written in C take 96
cycles to compute a float value and our
equivalent halfway down the graph
requires about five cycles okay there's
a big variation in the amount of
performance improvement it really
corresponds to how complicated the
patterns are so we can
by multiple patterns in a single
function and get really substantial
performance improvements you'll you have
access to the slide so if you want to
know exactly how we did this comparison
that is here if you actually do sell
programming you might want to know that
all of the code is available and
information about that is on this
website in fact if you have a cell and
you've updated to SDK three you already
have the code so the only thing you'd
want to know is some details about how
to use it okay so at this point what we
have is really i think the ultimate
assembler it gives you all the control
that you have writing hand-tuned
assembler you have access to machine
instructions and even the machine
instructions that involve control flow
which you can never access via inline
assembly code in well without really
writing the whole function in assembly
but you you don't have to handle the
annoying details of register allocation
things like that and you don't have to
you don't have to implement patterns of
efficient execution on your own you can
write these patterns in haskell so if
you have a pattern and you can't write
it in haskell then I think you need some
you need to talk to a haskell programmer
okay any any pattern that's useful can
be implemented this way we can do rapid
prototyping because we can do unit
testing of assembly code where as before
you would have had to write little task
programs for special cases it really
saves a lot of time where do we really
get this performance improvement from
it's because we're not targeting simdi
which I didn't define single instruction
multiple data we're targeting what
people sell today as simdi which is
really a single
complex instruction operating on a
merged chunk of data so most commonly
128 bit vector now I'll talk a little
bit about how we do the verification
internally we are representations are
all in terms of these graphs and we do
complicated things with simdi sometimes
we want to know what they're really
doing because they're not quite working
so we can we can translate this is this
graph represents a single floating
multiply simdi instruction it can easily
be translated into a parallel set of
floating multiply instructions yes so we
are we're assuming that there is a
simple specification without sim sim
dies a shin or any parallelization and
it can be compared to that so for the
codes that we're looking at where this
has really been needed big linear
algebra things like that it's easy to
write a naive specification it's hard to
tell whether the transformations you
made to optimize it are faithful to that
specification okay so this makes it easy
that map the map overhead at some point
we would like to actually verify the
whole code and we can verify parts of
that now I don't think I've run the
verification on the whole of that but if
you are doing things which involve
swapping parts of bit fields from
multiple register values and so on it's
very hard to to figure out what it
actually does on your own it would be
nice to have a translation into simple
types that we understand floats in sand
so on boolean values and see an
expression that presumably matches is
best for
education okay so this gets more
complicated when your instructions are
not pure simdi instructions so this is
an instruction which rotates the bytes
in 128 bit register when you do the
translation the instruction actually
disappears so the instruction was here
it's now gone because in terms of the
underlying data type 8-bit ends there is
no instruction we're just re we're just
shuffling them okay so these things can
all of the rules to make these
transformations can be encoded in a
bunch of rule generators okay not we can
do it makes big ugly graphs so it gets
harder to see what's going on the
problem is when oops the problem is when
you do things that really break any kind
of sensible typing doing things with bit
fields within floats for example we
these patterns have to really be done by
hand and that's ongoing work that's why
I put a half there okay so what do we
have we have for cogeneration rapid
prototyping and we're reaching peak
performance for verification we can do
what's equivalent to symbolic execution
so even if you don't actually have a
specification to give it you can look at
this symbolic execution and decide for
yourself whether it's what you wanted
and this has already been useful for
debugging especially this linear algebra
we still need more transformation rules
for those complicated operations okay I
think I'm going a bit slow try to speed
up a bit this is the part that I think
should interest people at Google how
we're approaching multi-core so we see
this we see an opportunity to relive the
glory of instruction level parallelism
out of order execution course at the
multi-core level ok so this this table
gives you a translation
that between what we know how to do and
what we think we should do okay so
instead of a cpu we're looking at a chip
instead of multiple execution units it
has multiple cores instead of registers
it has data buffers in loking local
working memory or cache memory it also
has signals really registers were
heterogeneous before but there's a
bigger split between buffers and signals
in this model than different kinds of
registers instead of loads and stores
you have DMAs either explicit or
implicit in terms of streaming and
instead of arithmetic instructions you
have computational kernels which are
purely local and they're pure functions
okay the catch is that we don't have the
soundness that you have on a cpu so
except for some specialized embedded
CPUs whenever you have out of order
execution it's the CPU hardware that
maintains the illusion that you're
executing sequential code okay so that
you can program it without having to
worry about that all of that is hidden
from you and the hardware ensures that
you have order independence okay there
there are some cases now with multi-core
CPUs where they're breaking some of that
with memory access but that doesn't fit
our analogy so that's not what we're
thinking about here with multi-core our
assumption is that the compiler needs to
insert the synchronization when it does
the parallelization and the hardware
cannot efficiently verify that this is
done soundly so soundly soundness is up
to software in our model we use only
asynchronous communication why because
we don't want locks locking is a
multi-way operation that involves
sending at least a signal some kind of
signal from one chord to another and
back to say yes
that lock is free possibly sending a
signal afterwards to do an atomic update
or something like that this encouraged
long unpredictable delays and that's bad
for scheduling so we want them out of
the model so you use asynchronous
messages that matches what sufficient in
hardware and if you want to do something
else you create patterns that synthesize
it on top of this I think I'll skip this
this shows a 22 patterns of memory
access in a multi-core architecture and
one of them use it takes better
advantage of asynchronous communication
here's an example that fits the the
second picture where you're sending data
from one core from core 12 core two you
may can operate do an operation on that
data you send it to the third core and
so on by doing this you've introduced
synchronization between core number 4
and core number one although there's no
direct communication in between those
cores okay if you want to reduce traffic
on memory and communication buses and
it's it's buses that are having more
trouble scaling than the pure
computation so I think that's always the
thing that we want to do you want to be
able to take advantage of the
synchronization that's employed even
though it's not explicit okay so here's
a simpler example what what you need to
do if you want to send some data from
one core to another core okay first of
all you need to know that wherever this
data is going is not being used by
somebody else so you have to start with
a send signal to send the signal that
means okay to send the data then you
send the data on the core that's sending
you need some way of knowing that the
data transmission is complete so that
that buffer that what we consider
analogous to a register is free to use
for another operation okay so on an
out-of-order cpu all of this extra
synchronization is done by the hardware
so if you have a load and the load
doesn't hit in l
to cash the load has to go to main
memory it will cause a stall and any
other instructions past this point would
just wait until it's safe to proceed
okay on a multi-core architecture there
is no hardware or our assumption is
there's no hardware that can efficiently
know what's going on on all those cores
okay so we have to build that in
ourselves now there's an advantage to
doing this asynchronous communication
and that is that you get these reorder
windows where you can do other
computations that don't depend on this
particular bit of communication okay and
you get a reorder window on both the
receive side and the sin side the
problem is that there are all kinds of
hazards data being rewritten while it's
being read by read for another DMA or
being used by a pure computation locally
okay so the question is how can we avoid
these things happening because on a
really on a lightweight multi-core
architecture it's going to be very
difficult to detect these kind of
failures so to do this we need to keep
this language simple probably language
is too big a word to put on a slide like
this these are all the communication
primitives and there's one computation
primitives so the only thing that we ask
is that all computations only use local
data okay and these are the primitives
that I talked about before a way to
think about what we're doing is that
we're trying to do control concurrent
control flow in a way that allows the
compiler to come along later and
schedule it efficiently okay so what's
the purpose of scheduling the purpose of
scheduling is to hide the latency in
them in the operations in the
communication operations so that you can
eliminate any stalls that would occur
but these all of this communication is
somewhat in predict
vic table even on a tightly coupled
single core okay so it's up to software
to make sure that the stalls do occur
when they need to occur okay even though
a good schedule would require no stalls
so the second step is to have these
weights in your code so that if the
computation if the communication is not
keeping up with your computation that
you do stall rather than just do
something incorrect okay so that's a way
of thinking about our model the key to
being able to verify a particular
program using these primitives is
correct is what we call a locally
sequential presentation it just means
that you present all of the instructions
that execute on your multi-core chip as
a list okay so they have an order and in
that order the send signal proceeds the
wait signal that it's supposed to target
okay that we call this a local okely
sequential program without this any
algorithm to check the soundness of the
parallelization has to do backtracking
and forward-looking and there's no way
to get reasonable computation time okay
locally sequential does not mean
sequential okay not sequential so you
can see the indices can be reordered
they can be reordered in such a way that
the computation is still correct okay
that's good because it allows you to
overcome jitters in the communication by
having extra buffering of computation
okay it is not good when the when the
order changes and the computation is not
order independent anymore okay so here's
an example you have to actually do some
computation the computation is now just
in this box if you have any computation
using
data and the signals are going to the
wrong weights then this is likely to be
using the wrong data to do the
computation okay so the key again is to
have this presentation so what do we
need what do we want to show that the
results are independent of the actual
execution order and that there are no
deadlocks okay to do this we need to
keep track of all the possible states of
execution of this multi core program we
can only do this in linear time if we
have a one-pass algorithm actually we
could do it with two passes or three
passes but I know no way of doing it in
more than one pass that would weaken the
conditions that we require so one pass
it is the algorithm is constant it has
constant space requirements the actual
amount of space required depends on how
many cores you have and how many
resources they have okay just like any
scheduling algorithm the impact of this
is that you don't have to do any
parallel debugging you never have to
look for deadlock states or race
conditions and this is our hope that
every optimization trick that has worked
for instruction level parallelism on out
of order cores can now be adapted to
multi-core okay and there are a lot of
tricks that are quite effective in
getting increasing performance out of
these cores we're now ready to implement
all the algorithms that are implemented
in so-called skeleton form if you know
what that means things like map and
reduce which you may have heard of and
it enables us to do some new things like
for example doing power reduction where
what we're trying to do is minimize the
amount of duplication of data in
multiple memory locations by replacing
caching by in flight data
so there may be data that is is nowhere
stored in memory that's globally
accessible in any way it's just passed
around without without ever being
available okay and we think that in the
future as the memory component of the
power equation gets larger this will be
something interesting to try okay now
I'm going to go quickly through this how
many of you know about software
pipelining of loops yes okay so let me
okay software pipelining loose let me
let me be really quick if you have a
loop with different stages and there's a
lot of latency you can chop the loop up
and schedule it in parallel okay there
are lots of algorithms for doing this we
have a new algorithm our algorithm is to
explicitly cut the dependent to
explicitly cut that code graph in two
stages and then scheduled them in
parallel it's in fact the simpler way of
approaching this problem how are we
going to do the cut well when you make a
cut what's the implication of that
you're cutting your cutting edges those
correspond to data values and in a nice
schedule those will all be register
values so when you cut your creating
register pressure at the bottom of the
loop and at the top of the loop
generally we try to reduce register
pressure so we can try using min cut on
this the problem is that there are if
you just do this naively there are bad
cuts their cuts where you use a value in
stage one that you don't compute and
still take until stage two and well
there are actually ways to make this
make sense generally we don't want to do
that so how do we stop that from
happening so ignore those infinities
I tell you they are really there so we
translate our code graph our code graph
involves values and operations we make
both of those into oops sorry first we
collapse what we know something that is
already assigned to a stage either above
or below the active section of the code
graph then we make all of the nodes and
edges of the previous graph into nodes
we add edges with wait one for consuming
a value we add infinite weight edges ok
now some of those infinities are really
there and the infinite edges can never
be cut ok and we add backwards edges so
that we can never make a cut that puts
something that's consumed in a stage
above something that's used ok so that's
the algorithm and we tried it again on
this same benchmark and we found that
first of all we can beat what xlc does
using swing modulo scheduling which at
least in some papers is benchmarked to
be the best performing software
pipelining algorithm and from our point
of view independent of that other
algorithm where we're very close to
optimal so we're within measurement
error for most of the functions this is
a bit this graph is a bit older and we
actually do even better than that now so
the importance of doing this is not just
that we have a faster scheduling
algorithm which is good but we have an
algorithm that for high assurance
applications we believe is more will be
more reliable because everything is
based on principle graph graph
transformations there's no additional
state that you need to keep track of to
understand or implement this algorithm
and we can use this for not just in
scheduling instructions we can use this
for scheduling our communication
perimeter
that I talked about in the multi core
layer and we can now do novel control
flow that you couldn't do with swing
modulo scheduling okay so we do this
this is technical we use nested control
flow graphs and I'll just give you one
example so if you have a loop and the
loop contains control flow so simple
example you have a switch statement
instead of producing one loop body one
scheduled loop body we produce one loop
body for each possibility in the control
flow and then within the loop we add
computation to figure out which version
which way the switch statement is going
to go in the next iteration of the loop
and we replace the branch at the end of
the loop by by a computed branch and if
you have an architecture that has branch
hinting we can now completely get rid of
all misprediction penalties okay and we
have benchmarks where the branching is
data dependent so there's there's no
prediction possible there's no
prediction algorithm that will ever do
anything better than fifty percent and
there's a way so there's there are three
nested if statements here so we would
have a lot of brat of we would have a
lot of mrs. mispredictions if we didn't
have this hinted branch okay so this is
the kind of thing that we want to do and
I think we'll be able to do similar
things on the multi-core level okay so
that's coconut I think it's a fun
combination of functional programming
and assembly language programming for
people who find both of those things fun
it really works well for simdi it's a it
motivated us to develop this so far
unbeaten scheduler it gives us this
interesting approach to multi-core which
we're now going to try to exploit
okay some of the names used in this
slide are trademarks of other companies
and I'd like to thank all the students a
lot of them undergraduates who have
tried to understand simdi and functional
programming and contribute to this
project in different ways and Robert an
ankle at IBM for support and for
research support IBM cfi OIT and Zurich
and Apple Canada any questions
how does this compare to the work on
streaming languages which allow you to
specify kernels and the graph of how the
Eternals communicate and then have the
compiler automatically do the software
pipelining and Cindy ization and so on
for example stream it from MIT um so we
could if you specify your code if you
specify your algorithm in that way we
can easily translate it to this
framework if you do that you wouldn't
have well you'd really only have to test
correctness of the translation once ok
so the interest in being able to do the
verification in linear time is a lot
less okay it might still be interesting
once we're really targeting things that
you won't be able to specify in that way
and really there are what the reason to
go to all this trouble is because
eventually I think everything becomes
memory bound as we go forward unless
some magical technology arrives and when
you do very complicated things in order
to get around that memory bottleneck you
start breaking all of those kind of
paradigms so if you have this sort of
safety net underneath your distribution
pattern that just checks ok it still
makes sense then you can develop new
patterns more quickly ok so we our
approach is to give more control to the
end user let them figure out better
patterns for their application specific
code rather than tell them you know use
streaming streaming is extremely
effective if you know if you haven't
parallelized your code
I think it's a very good way of doing it
but whether it is going to be enough for
any given application I think that's
hard to say now so our approach is don't
tie yourself to one way of specifying
concurrent control flow screaming or
something else and give yourself this
rapid prototyping environment so that
you can make things up okay maybe that's
just because we like making up new
algorithms but I really think that this
the fundamental problem of memory not
memory bandwidth not growing fast enough
we'll keep breaking any models people
come up with so this is going to happen
every couple of years for people who
really need to run at the highest level
of performance so if you had presumably
every time you implement new
sophisticated transformations on your
graph you need to update your
verification tool to take into account
those transformations so there are two
kinds of there are two cases so the
answer is yes a note a lot of patterns
don't break simdi in a big way and so
the patterns that we have the
transformations we have work but if you
come up with some new idea for using bit
fields in a floating point number
they're not in our list of
transformation so what will happen is
you get so we take the the code graph
that has it's a sim decode graph and we
just lift it into a bigger set of graphs
with mixed types and then we try to
translate all the elements of the graph
into the simpler type part of the graph
language okay and if you do something
really interesting it's likely that
you'll get stuck and it won't know how
to do that translation so then there are
two options either you try to build with
your improver
that will find the patterns that find
the transformations to complete that
process or you build them by hand and if
you build them by hand then you can do
fairly simple model checking that the
transformations are actually valid okay
the the reason why we're not devoting a
lot of energy into trying to make that
complete is that the semantics of
floating-point computations are not
really simple okay so we do things that
in the strict sense are incorrect but
are acceptable trade-off yeah so we're
not going to try to encode that in a way
that a model checker could check where I
think it's enough that the pattern is
there that having the pattern their
documents that you're accepting this
kind of transformation as being valid so
you do transformations that are not
legal with respect to normal rules of
floating-point ID do things to constrain
the locks of accuracy no so the user
figures out what the right trade off is
because really very rarely do people
will people who know what they're
talking about say we will only accept
this amount of loss of accuracy there
are always ways of getting accuracy back
if you can make something 10 times
faster you can then wrap it in some kind
of iterative correction right so we want
to let the user do that and we haven't
figured out how to make all this
transparent yet but we can really the
demonstration is that we could rapidly
develop this tool with undergraduates
participating and it works so we could
do or develop that library okay and we
have other code that we use internally
but so the two levels of it sounds like
you roughly have two two levels of
patterns one is the low level 70 and
Russian patterns and the other is these
slightly higher level kernel level
patterns that are communicating with
blocks of data is that we don't have we
have very few patterns at the second
level right I'm the talk is most mostly
focused on this md yeah well I just I
spent too much time I wanted to expand
the other part more but what we have we
have proved we feel we've proved the
concept by doing simdi well and we were
blocked for a while we didn't have a
good algorithm to do that verification
because I didn't realize we needed to
make some others make some put some
constraints on what we allow people to
present to us so we started out with
programs presented as a couple of core
programs to run on each core right and I
didn't find any reasonable way of
verifying that okay and and I tried so I
don't know it's not possible but it
certainly wasn't obvious but when you
require this locally sequential
presentation then it well it becomes
easy to say it getting all the details
right is a little bit of work but it's
it's simple enough that if a couple
people look at it you know you know it's
right and you you can do a few test
cases it's not something horribly
complicated that requires a model
checker have you tried your model on
algorithms that are something other than
dense array like sparse array or graph
algorithms or no so the the proof the
verification mechanism was only
completed last week okay
we know that other people have had
success writing simdi code for other
kinds of algorithms so we know there's a
lot of performance there but we well we
have stuck to things that either I
needed for my other research in medical
imaging and NMR or that somebody was
excited enough about to distribute and
and maybe pay us for okay how do you so
how do you choose what your sort of
basic data set size is mean I get the
impression that you divide data to
fairly small pieces so you can you know
shuffle them back and forth between the
cars but what happens when you have an
algorithm which can run efficiently on a
single car but becomes much less
efficient if you have to distribute it
across course so we are not trying to do
anything automatically ok so our our
philosophy is that smart people are
going to be needed to develop efficient
parallelization of non-trivial
algorithms ok so we're not trying to
automatically figure this out what we
have ourselves done is look at these
things from the bottom up and figure out
what size of chunks do we need to do X
and still be computationally bound on
the cell architecture ok and then we
just use the smallest size that allows
us to be computationally bound because
eventually that's the goal it's not
possible for all algorithms and
architectures but if you can get your
algorithm to be computationally bound
then you can say ok I'm done there's
nothing more for me to do I don't have
to do a better scheduling job than this
because it won't help so we've
approached these things the way that you
know you're taught not to approach
computer science problems we approach it
by looking at the bottom figuring out
how the Leafs of the algorithm leads can
be done efficiently and then we figure
out how to structure it how to structure
those leaves so in what proportion of
the common algorithms can you reach this
nirvana of being computationally bounds
using small chunks oh I think I'm not
going to give any answer to that
question we that the algorithms that I
really know are these scientific
computation algorithms so they're either
pure just funk map or they are dense
matrix operations or they're structured
sparse operations and you can work out
the sequencing at compile time to make
them as efficient or more efficient than
dense operations like if there's
symmetry for example that you can use it
to reduce your memory overhead you're ok
for general sparse I've talked to some
people who do optimization of sparse
matrices for different kinds of
architectures some extinct and some not
extinct and I believe that we can do
some interesting things but we we
haven't done them yet okay and I know
that other people have looked at
discrete algorithms and that often
they're not able to make them
computationally bound I mean a lot of
those algorithms there's no possible way
of making them computationally bound on
any kind of architecture that you can
imagine somebody building today but in
that case you can never really say
you're done because you can always get a
little bit better performance by
reducing the problem with the memory
bottleneck ok that's why I really like
these algorithms because we can
say we're done it's it's optimal now so
you had cell processor as your hardware
target what if you try to produce FPGA
code instead I mean if you try to
program an FPGA with your algorithms
will it work much better so it depends
what what you're doing for the things
that if you're doing something with
floating point I don't believe that no
matter how smart you are you'll never
get better performance if you're doing
something that involves a lot of say
bite swapping around I don't think you
can beat the cell for that it's good at
that there are lots of algorithms where
the f ph FPGA will do better and we
could target those we could target FPGAs
but the a lot of what we've done is
really about actual programming
efficiency rapid prototyping and those
things are not really going to port
they'd have to be redone for fpgas there
are different issues there and there are
other people looking at those and
somebody whose opinion I respect told me
that well fpgas are basically memory and
so there are going to scale like memory
and memories not going to scale as fast
as computation so why white I that stone
around your neck so that's really why
we're looking at the cell the cell made
a lot of things hard for conventional
programming models okay and it exposed
things that we wanted to have exposed
and which were not properly exposed
before a lot of things which nobody will
ever expose to us no matter how many
nda's we saw in like how branch
prediction works on architecture X I
don't believe anybody's going to tell us
that so we can't reverse engineer
those it's a lot easier with an
architecture where all those things are
exposed it causes most people a lot of
trouble but it gives us these levers
that we can pull and then we can say
this is optimal whereas on an
architecture where the hardware is doing
a lot of these things for you you know
you never will get that optimal
performance because they're always
slightly random things that are throwing
off your model of how it is guessing
what you're going to do right so thank
you thank you for letting me speak</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>