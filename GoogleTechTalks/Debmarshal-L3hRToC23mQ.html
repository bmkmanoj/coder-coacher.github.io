<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Debmarshal | Coder Coacher - Coaching Coders</title><meta content="Debmarshal - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Debmarshal</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/L3hRToC23mQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Deb Marshall is a project that was
written last summer by an intern wan-chi
Lin I was supervising him and it's it
was intended to be released as an open
source project it has been now it's out
on code google com and it is designed to
solve a problem that we had which is to
produce linux distributions for servers
owners there is a basic conflict that's
that service owners have while they are
putting together their service and
getting ready to deploy it they want
changes made to the distributions they
want their fixes put in they want their
software added they want all these
things and as soon as they deploy they
want no more changes instantly and
that's basically two different
distributions one of them that's
changing and one that's not and since
service owners are continually rolling
out new services we continually need to
have these frozen and developing
distributions the software that exists
to release distributions is fairly
cumbersome Debian for instance releases
every few years Ubuntu derives the the
cycle faster and releases every six
months we wanted to be able to release
at least every six months in probably
every couple of months because there are
new services all the time here this is a
public project but if you have questions
about confidential information go ahead
and ask me afterwards so some of the
features that we added two dead marshal
to support this one the top thing was
simply to maintain a debian ubuntu
software repository that was a mirror of
what was going on in the upstream and be
able to stage it out across the
enterprise the other thing we wanted to
do was to be able to add packages to it
so process an incoming queue and build
up another release of packages that were
added on to that and stage them out we
don't want to make a change and
instantly roll it out to every machine
in google and find out that we broke lib
c6 again
we also wanted to be able to preserve
all of the snapshots that we had taken
of this release continually being able
to roll back to test whether something
was working and have multiple stages
where all this was going on and we
wanted to automate consistency checks
yes preserving both of them dead marshal
manages both the source and the binaries
and so we just stick all of them on
their disc is cheap it's much cheaper to
save everything then wish you had saved
one thing and we wanted to automate
distribution wide consistency checks
things like does every binary that's in
a distribution have a library that it
needs in that distribution one of the
common problems the Debian has is that
you know something will get uploaded
that's linked against unstable but it's
uploaded and propagated through to
stable without having everything make it
through there's some complicated logic
that goes on in making sure that happens
and every once in awhile it falls over
for good bunt to it falls over a bit
more often
what we mean by a distribution is just a
list of packages the way at Debian
repository is structured is that you
have lists of packages for all of the
different releases that you might have
they're all referencing one pool of
packages so if if you upload version 6
of a particular release even though you
have multiple distributions version 6 of
that package is only in the pool once
all of them reference it through the
packages lists they have indexes in the
packages list themselves have check sums
of the the binaries and the sources that
are in them there's a release file that
lists all of the different indexes and
their check sums and that release file
is signed with a gpg key ensuring that
you really are getting what was
published here's a look of what a
typical Debian archive looks like yes
what is a package the checksum is there
to ensure that you're really getting
what you are supposed to get the
versions and version numbers and file
names are also in there as well yeah but
it means you're saying it's not
sufficient to know that you have
for that for a 2 dot P or whatever you
need to actually you know you're trying
to put miracle I guess that you have the
exact same package or yeah you're not
just having a package that claims to be
version 4.2 you're having the version
4.2 that Debian published or that you
published if you're running your local
pool so for the typical for a debian
release you have the disc directory
underneath debian lists etch which
happens to be one of their code names
for a release it's about to become their
new stable release and that has a
release file which has the check sums of
the indexes in it and the release gpg is
the signature on that you have the
public key you verify that that's
correct and then you go down there are
sections so the release file references
multiple sections and architectures main
all the different binary architectures
each one of those has a packages gz
violin it and the sources as well and
multiple lots of different versions of
these then Debian also says well X is
still in testing it's not released so
they have a symlink from testing to etch
this is what their normal distribution
architecture looks like and the pool
just has a hash of the section again and
then a directory for each source package
with all the binaries that that source
package builds and all the versions that
have been built for it that's what a
normal Debian repository looks like with
Deb Marshall we throw in some extra
stuff we use exactly the same pool
structure so I won't repeat that part
but instead of using just a distribution
code name we use a distribution in a
track so we use a subdirectory as well
it's the edge distribution but there's
lots of different releases of that as we
add packages fix things get it ready for
release or in maintenance put security
fixes into it so there's a subdirectory
version 23 which would be the 23rd
snapshot and a symlink to that that's
automatically maintained latest that's
just pointing back to 23 so you're
testing machines can be running edge /
latest
and they will continually get the latest
thing that's been updated you can have
multiple other siblings pointing to
versions that you have tested that you
trust more that you're ready to stage
out to more people and they put in their
sources that list file which one they
want so down at the bottom line is the
sources list that you would typically
see for a client that's being updated
from a Deb Marshall repository instead
of just saying etch its edge / latest
other than that it's pretty similar to
your normal Debian sources that list
file and here's what that would look
like in a tree diagram you see the X the
23 version number and then underneath
that exactly the same layout of release
main binary architecture sources and
symlinks that are paralleled underneath
etch rather than at the etch layer so
it's adding one extra word into your
sources list and now you have version
control and version management on all of
your architectures so I'll get into how
some of the programs are used they're
not user friendly I think they could
describe it as user hostile they are
they were written very quickly to do the
job because we had a job to do and
they're to be run from scripts and cron
jobs so I want to apologize for the user
interface on these one of the things
that we ran into is is an actual full
size repository is very large and you
don't want to every time you generate
any minor change to have to go through
and open up every single file again
uncompress it find all the metadata load
it all into your program and then try to
do something with it with the with deck
and other repository management systems
they've gone with a full-on SQL database
to store this we found that it wasn't
actually that useful to do all we needed
to do was store these things without
having to reparse them so we put them in
a Berkeley DB file from and we access it
from Python just using us you know
dictionary lookups it turns out to be
fast enough we can process what we need
to do in a few seconds
upload we can do the consistency checks
I was talking about in about five
minutes and it's all cpu-bound it's not
disk bound whereas reloading an entire
distribution takes two and a half hours
on to what some of these programs do
there's index pool which is the basic
thing that gets you started before you
begun using it it goes you take a
distribution that you already have you
run index pool and it goes and opens up
every Deb and every source package and
every changes file and loads it all into
the database all at once this is the
step that takes two and a half hours to
get started populates everything you
then run it again if you are if you're
using something like Deb mirror rsync to
pull down your local repository from
debian or bun to run it again afterwards
it goes through and notices the files
that it hasn't noticed before loads them
into the database you still don't have a
list of which packages you want in which
release but you have the packages
themselves indexed and you don't have to
open those files ever again make
releases what does most of the work in
Deb Marshall this is how you generate a
new release after every upload so in
this particular invocation we use it to
take an upstream release file in an
upstream distribution we say the others
been a new sink from upstream we've had
a new mirror pulse let's generate a new
snapshot that represents that new change
pass it in the disk and the code name it
digs into the repository finds all the
packages files and generates a new local
image of that and output and Rio puts
those packages inside the subdirectory
and it resigns them
you can name the track the same as the
upstream which would be a fairly
rational thing to do or you can give it
your own name either one works on we
we've rolled out one for Debian that
uses the Debian code names and then
internally for a bun to we've done some
that use our own names the latest
symlink always points to whatever the
last snapshot uploaded is so it's much
easier to locate that and just leave
your test machines always pointing at it
the other thing is that this particular
when you're tracking an upstream Jeb
Marshall has two different modes that it
can be used in one of those modes is
tracking where it just says everything
that came from upstream I want to make
that a snapshot there's also a managed
mode where that isn't the case we want
to apply all sorts of logic we want to
control version numbers and and do
consistency checks but tracking mode is
the simple one it just follows along
with what ups upstream has done and your
incoming is is the second third program
here it's doing the opposite of what the
make release is doing it's just taking
some packages that you've just uploaded
right now and it's adding those to the
distribution so it's it's the it's what
you do when you're not tracking when you
are managing your own package pool it
checks the signatures on that on the
incoming pool which make release isn't
doing make release trust that if you
pull down the whole archive you made
sure you pull down the right thing dead
mirror does that for you with inner
incoming it's the thing doing these
signature checks if the everything
checks out move those files out of the
incoming directory straight into the
package pool and index everything stick
it in there you can run it every minute
it finishes in just a few seconds and
every minute you can just keep running
this over and over again with with enter
incoming running on an incoming pool you
can use standard Debian pools like tools
like D build Deb sign and upload which
debian uses internally a bunch of uses
and that way you don't have to relearn
anything you can use what the upstream
is you are using in order to upload
packages the make release when you're
when you're managing it under under
enter incoming you still haven't
generated a new a new distribution every
time what so what you do is you run make
release right after inner incoming and
say I want the thing that was just
uploaded to now be a new snapshot you
can also control what gets uploaded so
when you're in development you might
want to not control anything anything
new uploaded goes into the blue your
distribute your development track
where's your maintenance tracks where
you've got machines running these
they're in production you don't want
anything that just gets uploaded
immediately in those tracks so you have
a release specification file we're going
to change the name on that we're not
really red hat people it used to have
the name dot spec and that's going to go
away because that was pretty unfortunate
but make release generates a new
snapshot according to you know there's a
number of flags on here this this
particular one says says to generate a
new release with exactly what's newest
you can also put this release special
specification file the controls I don't
want in this package name at all or I
want this particular version of this
package name and then you can freeze a
release and have more control over what
goes into it when you typically run this
immediately after running it or incoming
here's another one one utility that's
that's rather handy is to simply know
what the difference is between two
different releases so you can send out a
message every time a new upload happens
what was actually uploaded and added
into the distribution so this particular
one just computes the diff between
whatever the release track is version 89
and version 88 you can build these right
into your cron job that's running every
minute and send out a message whenever
something changes
yeah yeah it throws out the old one
debian distribution can only have one
package of a particular name so if you
want to have multiple versions of the
same package installed at the same time
you actually have to give them different
package names so you'll get things like
nscd 235 and nscd 232 to have two
different versions of that package ah
yeah the question was can you can you do
packages automatically get bumped out
when when they become obsolete and the
answers they do so the handle alias
command is a very very light layer on
top of link it's just maintaining
symbolic links but it's storing the
history of what was going on when you
change them so you could avoid this but
we put it in there anyway because we
like to be able to look back and see
what what version any particular symlink
is pointing to the update command is
what changes it and the log command is
what lets you see what's going on in
that log
now this is something that took the
second half of the summer that was how
we got started was to handle releases
next thing we wanted to do was find out
whether releases were good if there were
actually problems with them so wan-chi
did a lot of work trying to figure out
whether or not two different packages
had all of their dependencies met and
dependencies in Debian can't be fairly
complicated you can depend on something
that's provided by several other
packages you can conflict with something
that's provided by other pet multiple
packages you can have version
dependencies and version conflicts I
took a while to get all of that working
to the best of our knowledge what he
would write it and then we would find
that there were hundreds of flaws in the
upstream distributions and we'd look at
them and find out that some of them
weren't really flaws but there was
something we hadn't even considered as
possible that was that was allowing it
to work but going through we went
through and there's still hundreds of
flaws left there are packages that
looking at what Deb Marshall spits out
they can't be installed or at least they
can't work they might work if you
upgrade from an older distribution but
if you just installed the current one
you're never going to get that
particular package working there's
another problem that also regularly
happens with distributions and isn't
well detected is that you can have two
perfectly good dead packages that
provide a set of files or a set of
features and each one of them by
themselves is perfectly fine but they're
both providing the same file and neither
one of them knows about the other one
well you can't have both of them on the
system at the same time the D package
system won't allow the second one to be
installed it will start and then it will
complain and once it complains apt-get
which is Debbie ins main package
management you know Overlord doesn't
really know what to do when D package
complains and so the whole thing falls
over you stop getting updates so it's
very important not to have a
distribution that has these problems in
it with debian ubuntu the way they do
that is they have a long development
cycle and they simply find they get bug
reports from all of the million testers
and find out that yes there are
conflicts and somebody reports it and
then we go
fix it with Deb Marshall we can detect
those and it and it can be detected in
about five minutes on the upload so
every new package that gets uploaded we
can run this verification command and
find out whether or not it's going to
conflict in a way that isn't declared
with any other packages that are already
in the distribution yeah issue doesn't
sound very satisfying solution to that
problem so I'm moving around thing at
the same thing certainly if two packages
need to be simultaneously installed and
they hope refer to some common file it
needs to be in different states that
that's a logical inconsistency so I
don't expect that to work but what you
offer rent or something where maybe the
stock package from debian installs maybe
of wine on etsy and then my service
wants to change one of those files and
that's still the same problem that
you're talking about right because both
packages will sort of own the same file
you're saying the package management
only permits 1045 right so it seems like
it would still be nice though if there
were some way to prevent packages it's a
dependent on another package to take
ownership of individual files and still
have safe I'll level consistency checker
yes well okay the question was how
paraphrases whether or not there is a
way to allow multiple packages to
provide the same file and there there
are ways to do this you can have to pack
two packages can provide the same file
and they are perfectly legal to do that
as long as they declare a conflict with
each other only one of them is allowed
to be installed on any particular
machine the distribution can have both
packages but any one machine can only
have one of those two packages installed
so that's that's the simplest solution
is to just have one of those packages
conflict with the other and then the
package management system knows
otherwise well that's that's the
simplest one now the more complicated
thing to do is to have the second
package rather than conflict with the
first package it diverts that particular
file this is a feature that as far as I
know only exists in Debian which is that
you can run a command D package divert
and tell it I want you to take that file
sent rename it to something else get it
out of the way and then I'll control it
from now on whenever the original
package tries to refer to the file by
what it thinks the name is it gets
automatically at a lower level renamed
and the original packages install
scripts and post inst and manage scripts
are all hitting the diverted file which
is not the live one so only one of them
has the name they don't both have two
different files at the same file name
because UNIX file systems don't allow
that but they can both be installed at
once and only one of them has the files
you might have your default
configuration files that ship with the
package and then you wanted to ship
another package that replaces some of
those files well you divert the ones
you're replacing send them off somewhere
else the first package doesn't know and
you just provide your files there there
are some complexities involved in that
as it turns out not all Debian systems
obey the diversions Deb cot for instance
doesn't because it's all custom scripts
and none of them bothered to check
whether there's a diversion they just
write directly to a file
ok
so one of the things that that you can
also do with one of these derivative
distributions you can say the tiny
distribution where you might have a
dozen packages is not going to have all
of its dependencies met within that it
has a base distribution that all of this
came from so we specify what the
underlying distribution is you can have
multiple layers of these so for instance
Debian security depends on an actual
Debian base and your local repository
depends on at least your Debian base and
probably your security packages as well
and then it goes through it it will
automatically load in when it's doing
these dependencies checks into its local
repository all of the metadata to find
out what conflicts without having to go
and reload them continually and that's
what the underlying line in the
repositories configuration files about
yeah
just going to
well it's storing their any changes in
the are the changes stored in the as
dependencies change for packages they
are stored every every package version
that's uploaded all of the metadata for
that package is stored so every time a
new version is uploaded a whole new set
of metadata is stored for that and it
does grow but because these are you know
nice compact binary hashes of just the
things we needed it turns out to be
about a tenth the size of the original
packages files because we just aren't
storing everything we're only storing
the things that are of interest to us
yeah
because I'm still trying to figure out
motivation I I thought that if you
wanted to create a system up with a
snapshot of a particular set required
versions that the normal debian would do
that would be to create a virtual
package that has dependency links on
specific version numbers for all
packages which version you wanted to
lock even if that's everything in the
system is that is there some I I guess
I'm trying to understand how this is
different than better than just doing it
that
couldn't I get the question was couldn't
you just create a single virtual package
that depended on exact versions of
everything that you wanted in a
particular snapshot if you did that what
you'd end up with was your only choice
to be on that particular managed release
stream would be to have every single one
of those packages installed snapshot
well this nap shot incision own state
yeah well isn't that what a snapshot is
well snapshot is what's available it's
not necessarily what you've installed so
each snapshot has 15 to 18 thousand
packages in it most systems have a few
thousand packages installed snapshot of
an entire distribution right not a
snapshot of what is actually in
yeah it's it's a snapshot of the entire
distribution that's what each one of
these snapshots are ya play out of space
vicious one package changes between Asha
23 snapshot 24 how much this space or
spending on the system
how much disk space get spent every time
a new snapshot is generated well it's
generating about the same amount that
every new packages file generates so if
it's a small local archive then you're
talking about tens of kilobytes if it's
the full distribution if you know Debian
pushes from release one up to release
two then you get a full set of packages
files which are a few megabytes in size
so you can plot out that it's growing
and with modern disk sizes you can go a
couple of years before you exhaust you
know if you if you left double the space
you need on your disk storage then you
can probably go a couple of years before
that becomes significant and and what
what all of this verification gets you
is basically unit testing on your
snapshots you can build a snapshot you
can stage it out to a few people staged
it out to more see if it all works and
it's one way to test the snapshot itself
you produce it and now you can start
testing it
so the typical workflow that we would
imagine using this with would be
software developers produce a package
and they upload it into the pool as soon
as they like their package they have
installed it on their desktop they like
it they sign it they uploaded it goes
into the pool now the distribution
maintain errs don't have to choose this
package one of them might whoever's
doing you know the the distribution of
everything latest anew but all the
others might decide they want to wait a
little while so each so you can have
multiple distribution maintainer Zahl
digging out of the same pool and some of
them can be very rigorous about what
gets in if you have servers doing
something very important you probably
don't want them updating just about ever
that's that's what our experience has
been with with service owners is once
the once they have stuff working they
don't want us messing with it at all so
you just put these different symlinks
well the symlinks are very lightweight
you can have as many of those as you
like too many of them is probably too
much work but the system allows you to
have as many as you think you need and
then you change the labels find out what
the difference between them alert people
that you're making this change and it
automatically gets updated by the
systems that they're running tron apt
every night you take away the minus D
flag and it automatically installs the
packages each night out of the
distribution it's probably safe to track
the ones that are going through rigorous
controls before they get updated
so there are some related pieces of
software that I thought I would mention
snapshot Debian net which is run by
another Googler has full snapshots of
all Debian releases ever and it's
several terabytes in size there's no
local upload capability you don't do
local distributions it doesn't do but it
does do verbatim indices which is
something that Deb Marshall doesn't do
just yet so when we we generate a
packages list and a release file riri
sign it with our key rather than the
Debian archive key which has some some
consequences but it gives us more
control and it was easier to do Rep rep
row is a nice little repository control
system that that we've used a bit it's
um it does every it's designed strictly
for doing a local repository it doesn't
do the multitrack releases like we were
talking about here and it's very
aggressive about cleaning up the pool
every time you're no longer using a
package it gets deleted right now and
we'd rather not delete them but it is a
good piece of software it also uses a
berkeley DB to keep track of of the
packages and metadata deck is what both
debian and ubuntu use it does not do a
local derivative repository particularly
well and it doesn't do integrity
checking to any great degree it requires
an SQL database but it is the thing that
scales up to the size of the two big
distributions that are running Debian
packages it's also pretty lightly
documented the Ubuntu archive maintainer
and the Debian archive maintainer are
both James troop and he knows how to
make it work because he wrote it and de
package scan packages which is the
original it just goes into a directory
scans for all the Deb's extracts their
metadata and writes a packages file and
that's all it does it doesn't scale very
well to multiple tracks but it's
something that you can use for a very
lightweight piece of repository
maintenance
so we have some things we want to do
further with Deb Marshall we'd like to
publish some good user interface
wrappers around the scripts the scripts
themselves do the work we want them to
do and they're completely scriptable but
nobody really wants to look at those
command lines I had in my titles there
and published some better docs on using
it and setting it up we're working on it
this talk is part of that process we'd
also like to publish a manual of the
suggested workflow when we have more
experience running Deb Marshall we've
already revised what we do a few times
with this changed around from using
different tracks to using multiple
labels in the same track and switching
back and forth so part of it is just
getting operational experience and then
saying what we want to do with it
publish the integrity checks of the
upstream distributions we think it's we
think most of the problems we're seeing
right now are pretty much real problems
so when we're a little more confident
we'll start telling Debian people about
these problems publishing lists and
letting letting welt them and us work on
it the release files which were
resigning with our own signature it
would be handier if when they haven't
changed when it's it's it's exactly the
same as what upstream gave us to keep
their signature so that's just a little
tweak to the program if nothing has
changed go ahead and use theirs because
then the installers which have the
public keys hard-coded into them can use
the archive the Deb Marshall is
maintaining yeah
the integrity checks isolating and
separating them and sending them to
Debian as it as a recommended
integration
the actual package producing software
it's rather than publishing the results
of the integrity checks if you could
take the automated test and build it in
action systems than these would cease to
be a problem in the future yeah could we
could we isolate out the integrity
checks and put it into the Debian build
and process and everything it would be
somewhat difficult to do the place we'd
have to build it in would be something
like dak because it has to be something
that can see the entire archive most
Debian maintain errs have what they need
to build their package but they don't
actually have the entire archive sitting
on their disk today it's a reasonable
thing to do the whole archive is still
only a few hundred gigabytes but it's
not something that most developers have
sitting on their disk that they could
scan in and it takes a couple of hours
to build that index the first time so
the place to build it in would be DAC
and we haven't really worked too much
with making that happen because deck
looks hard yes
if the goal is to mind my partner's wife
in the check what
THX be run against something lightly
data-based a nap while this is instead
of getting the entire
view into the entire archive why not
just the coolest violence involved
why couldn't lint Ian use the existing
database to just grab the list of files
for each packages and each package and
detect whether or not there are
conflicts well part of we can't detect a
lot of conflicts but we get a lot of
false positives that way too because the
the list of what conflicts and what
replaces and what overrides and is
allowed to replaces is a fairly complex
to calculate so we did it this way there
are probably other ways to have done it
but I don't know that that information
is all out there and parsing it locally
we were able to get everything we needed
to determine whether or not there was a
conflict
and the availability Deb Marshall is you
can check internally for the internal
operational documentation on how we're
using Deb Marshall and it's also
published out on code.google.com / p /
deb marshall under the GPL version 2 so
thanks for your time if you have any
more questions let me know yes
but
I'm wondering about recompilation of
things like the extreme cases will be
getting engines to
you need some changes to header files
for Lucy
what sir
what's the normal what's the usual
practice at this part as
finding other packages that would end on
when lexi is couple of
when there's not any specific version
what'swhat's the Debian policy on
rebuilding packages that depend on a
library that's been updated the policy
is basically not to do that pack
libraries that are uploaded should not
break backwards compatibility if they
have the same name if they're going to
break backwards compatibility debian
policy is that they should have a new
esso name and a new package name yep so
is there are ways to wash for packages
maybe the example season say so you've
got some package and it was built of her
bro support and how do you know that
some birds future version will not build
it correctly and left out the purpose
that make sense yeah how do you how do
you ensure that some future version of a
package doesn't drop some important
compile flag that you needed well if
it's a released distribution so if
debian or a bunch you have released it
they should only be doing security
updates and if they determined that
Kerberos is insecure and they drop that
whole flag well I guess they can make
that argument but in general debian
doesn't go and drop flags they they make
the minimal change necessary once it's
released if it's in development all bets
are off yeah yes because I don't like
your answer you said that if I want to
patch a header file ypsi that forces you
know binary changes to boast of the
system say is everything links to lipsy
the answer is I can't that I have to
publish lipsy two or something like that
have my package that links to lipsy too
and those guys can coexist nicely but
all the previous stuff that maybe also
wants to take that long fix it maybe I
fixed a bug in mouth there must be some
way that debian has it if I actually do
say propagate the version of a basic
library and I do need to recompile
everything is there some easy way to
recompile everything for source like I
like gin to magento you can just go and
change one line
header and type ever go and rebuild
everything is there something like gin
to where you can force a rebuild for
everything if if a library has a fix
that was in a header file so it really
does need to be recompiled and Debian
doesn't really have that capability it
was basically the the thinking is that
you should be able to come up with a way
not to have to do that you should be
able to make it backwards compatible
without having to recompile if the bug
is in lib see you should be able to fix
lipsy without recompiling the
applications the applications are using
an API and if the bug is not in the API
if the bug is in you know the Mallik
code itself then fix the mallet code
don't change the ABI if the bug is in
the ABI debian still has gets I think I
think Gentry probably still has gets to
get string so it's the get it get a
string into a buffer and you don't tell
it how long the buffer is and so if your
if your entry input line is longer than
whatever you guess the buffer would be
then it over rights and you have a
buffer overrun that API is published
it's part of the C standard library and
Debian hasn't gotten rid of it it's
still there because it was a busted API
but it's it's kept for backwards
compatibility sure yeah suppose I had
that neat yeah I needed at the library
and rebuilt my system it doesn't
as a functionality you replace the
library
Americans are shared up
cheers here's a concrete example
the yellow cake
Oh
the library for people
six orbits so now
this
but if you change a publish
we've got to make
next iteration the further because
that's what publishing
11 vapi is published the whole world
depending on it the chances don't change
so it's
temple
reference variables
your carpet here I sorry pthread
interface very public spec so hard to
change and there were some issues with
user filled in structured passed into
the potions pile so in the start website
shifted
but it
if the flaws in the API then make a new
version
there's the flaws in the implementation
implementation
in this case then come taste
key programs are sensitive to
implementation not just a TI
yeah yeah basically there are examples
of cases where it would be nice to
recompile everything because you know
even though the API you want to you know
keep it the same that word changes in
places that do slip into the compile of
your client through the include files
where you might want to do that and
Debian doesn't have the capability to do
that they went with you must come up
with a way to do it without changing the
API or just bite the bullet and change
the API go from Lib C version 52 Lib C
version 6 and make it a new package name
no more questions well that was quick
okay well thanks for your time and i'll
be around later</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>