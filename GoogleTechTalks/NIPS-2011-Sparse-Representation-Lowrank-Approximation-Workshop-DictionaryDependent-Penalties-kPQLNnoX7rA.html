<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Dictionary-Dependent Penalties... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Dictionary-Dependent Penalties... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Dictionary-Dependent Penalties...</b></h2><h5 class="post__date">2012-02-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kPQLNnoX7rA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so this work is kind of ideas from
the paper so if you have this is based
on that and then pushing a little bit
further some of this also extends to
rank minimization but I don't know time
to talk about that today but a lot of
the techniques and ideas that tried out
for demise and rank and we can do some
interesting things there as well
okay so then also please interrupt
anywhere if you have a question okay so
the basic problem I'm looking at is just
sparse estimation or small are familiar
with this just a linear model in
dimensional vector of observations why
when you have a dictionary of basis
vectors I think sometimes I use the word
dictionary just in a very generic sense
in compressive sensing this could
include the measurement matrix and the
sparsity transform and everything is
lumped in there I'm just using it as
just generic matrix of basis vectors the
key is that it's over complete so
there's many more colors than rows and
then you have another coefficient vector
actually then obviously you can add
noise this one as well it's just for
simplicity noise this case okay so the
dimension of ax
we assume as much greater than the
dimension of life service that infinite
number of solutions
so most elements have actually like to
be equal to zero exactly so this is a
combinatorial optimization problem
minimizing the l0 norm the non zero
elements so it's np-hard discontinuous
non convex so the most common thing
people do in this situation is to find
the closest convex approximation
approaches minimizing the l1 norm so
here we just replace 2 l0 with the l1
and trying to solve that and there's
been a huge amount of research as I'm
sure all of you know in the past 10-15
years
quantifying exactly what conditions
guarantee that the l1 no solution equals
the l0 norm of solution exactly and just
to summarize in a very hand wavy fashion
those results basically the dictionary
phi is sufficiently unstructured then I
want me to succeed otherwise it will
fail so just explain that a little more
what you mean by structure basically if
you take the dictionary and you look at
every pairwise correlation between if
most of the energy is along the diagonal
I'm saying that's construction so
there's not a lot of correlation there's
some because the dictionary is
everything so there's always going to be
some correlation but it's as small as
possible so examples are a few about
unstructured dictionaries
whereas if you just
Gaussian ID elements and make a
dictionary or a hue randomly several
rows of discrete Fourier transform so if
you do that you get a big / complete
dictionary you take all these inner
products and most of the energy is on
the attack
so the structure case is more general
now you have lots of correlations in
possibly an arbitrary structure so
basically if you pre and post multiply
an unstructured dictionary by matrix a
arbitrary invertible matrix or say
something like a block diagonal matrix
you are adding correlations in the
column of the dictionary so this is what
I'm allowing the structure case so
here's a block diagonal example just to
kind of show how this can cause a
problem for regular a1 minimization here
you just take a say a Gaussian int
dictionary and multiply it by a block
diagonal matrix and now when you look at
these inner products you have these food
plots of correlations along the data and
what typically happens in this these
sorts of examples if you want to solve
for the maximum u star solution
typically that l1 solution will select a
0 either 0 or at most one basis vector
from within each of these blocks so it
will from all these correlated basis
vectors that form like a little cluster
it won't pick one of them maybe it will
pick the right well maybe not that it
will usually pick one and the cluster
support might be mostly correct meaning
it will mostly pick
clusters that at least have one nonzero
in it but that chosen basis vectors
within the cluster will turn completely
wrong so this is just I guess
symptomatic of a bigger problem like
this is what you want to solve you want
to find
our solution X naught so you're
replacing with something tractable a 1
or minimization problem and the problem
really to guarantee that X 1 equals x 0
with arbitrarily structured dictionaries
is that the only way that can be true is
that the signal here observing is equal
to or proportional to one of the
dictionary columns which is a trivial
case so and the motivation I guess for
this is that most Theory essentially all
theory applies to the unstructured case
but many and maybe even most
dictionaries have some feather so for
example people have been doing recently
face recognition where the basis vectors
are stacked images a lot of people use
image patches for classification they
form dictionaries they learn basis
functions that can be hive informative
so there's many examples where the
dictionary had a lot of structure
possibly unknown structure and these are
the ones the dictionaries people are
using but a theory of class the
unstructured case and actually I would
argue even in compressive sensing many
of the dictionaries are not actually
unstructured because if many hardware
constraints mean that you cannot sample
random int gas flows for example for an
MRI it's very difficult to make your MRI
machine
exactly compute Gaussian voice in space
so typically that's not done so I I
would make the case that there's
somewhat of a mismatch here and not the
theory of class time structure case but
many people are actually using structure
dictionaries so one of the things that
people have done to improve our
minimization
and is used for example in MRI heavily
isre waited a lot so this is a way of
refining the whole one solution to
hopefully improve the recovered signal
so the question I'm drinking erase here
is is reweighed in l1 in its present
form really addressing the problem with
structured dictionary so what we would
leave weighted along minimization is and
you you basically take an arbitrary
computation of the function first a
concave non decreasing function of the
9x use and this is a closer
approximation to the LZ Ramon and how
one mark so possibly you can get sparser
solutions that are closer to that zero
no solution so here's the what a penalty
function looks like and obviously this
is closer to the hell 0 because of this
concavity and the way it works is you
just do these simple iterations and
they're guaranteed to find the local
minimum of this cost which so the
question is are these iterations is
really helping dealing with structured
dictionaries and no these literature's
are really helping with solve a
different problem example of this so
here's the most common multifunction
back canoes forever be weighted our one
minimization and there's a primary you
can choose you can either set it with a
continuation nothing or just pick a
fixed value and basically the object
rules become a now one problem with some
ways here these whence start equal to
one so the initial iteration is just the
regular basis pursuit the information
and then at each iteration you compute
them waste via this formula and this
formula is just based on a first order
Taylor series approximation to this so
know just computing the gradient based
and you just iterate things until you
get to a local minimum but the problem
is if the initial one iteration is bad
because of Dikshit dictionary structure
there's no compensation for a dictionary
structure in this weight update and you
will just be stuck so like some
simulations I try if I use a structure
dictionary with this update basically
this we should never changes from the
regular one normal solution it just gets
stuff but what these updates really help
with is dealing with learning magnitudes
there are very different scales so the
regular l1r solution is invariant to
coefficient magnitudes basically whether
or not you achieve the maximum our
solution or not is only dependent on the
sine pattern and the same way by using
these updates you can actually there was
a paper recently proving that you can
get slightly tighter Brown set at a
slightly better pace transition to
recovery not that I was you're annoying
yes basically because these iterations
if you have a few coefficients that are
really large this way becomes zero the
next update is free to use those
coefficients and it's kind of like
solving a smaller subproblem and then
later can get the smaller magnitudes so
intuitively this weight update only
depends on the magnitudes and it's only
really designed to help deal with
recovery problems so so the new style is
like looking at is somehow getting to
weight updates that depend on the
dictionary and one way to get weight
updates that depend on the dictionary is
using a dictionary dependant sparsity
penalty so the way I do is there's
already a lot of sparse penalties but
they're always applied directly on the
coefficients X so why not first project
the coefficients to a new space that
maybe has some better properties and
then apply the standard sparsity
penalties that have already been used so
here is just some abstract
projection PFI and now we just apply the
same sort of sparsity penalty some G of
Z normally there will be the X here
right now it's disease so it's very
simple and the key is that this
projection it has to somehow compensate
for the dictionary structure and the
very key property is that has to
preserve sparsity so it's busiest bars
that must translate into the X being
sparse otherwise you haven't
accomplished anything so this is the
admin area you would like this to be
true in all cases obviously that's
impossible but this is the real the goal
is how close to this can we get if you
start with some unstructured dictionary
and maybe you multiplied by some sort of
structure introducing matrices a and B
and you get some five structure you
would like some sort of correspondence
stuff set if you're minimizing the
sparsity penalty with respect to Z in
this projection it's somehow results in
performance that's like minimizing this
G with the original unstructured
dictionary something like that obviously
you cannot do this in all cases this is
a hard problem overall but perhaps
there's many situations you can get this
sort of correspondence so a simple
pipeline for this method or a recipe is
just choose some sparsity penalty
function maybe there's some criteria
that some
than others and also choose a dictionary
projection and then compute the updates
and look for properties that are useful
for dealing with structured extremes so
I say you know I serve before these
realated are one updates but you could
also do we wait about two updates I
won't talk about that you can get good
results from that as well so the one
thing is that all of this can be done in
what I call either our primal space or a
dual space so I used a slightly
non-standard definition of these but
it's useful for adult developing
different classes of these penalty
functions so any sparsity penalty you
can represent in this primal space which
is the most familiar definition there
also a dual space so the primal spaces
means you have the sum function of X and
it's dependent on a concave non
decreasing function H of the magnitudes
that ensures that every local minima of
the complimentary will be a sparse
solution but any function that you
expressed in this form you can express
as a minimization over these hyper
parameters gamma and so these are like
variational parameters or something like
that and if this H function is concave
non decreasing then this H star folks
will be concave and non decreasing so
you can actually derive sparsity
penalties in either space you can either
choose this function and then work out
what G is or you can just choose this H
directly and when it comes to applying
these dictionary dependant projections
it makes a big difference whether you do
it in this space or in this space so
I'll give some water
but the idea is in the primal space you
just take the regular sparsity penalty
and with the projection coefficients in
there if you could also do the same
thing to this a star penalty do a
projection of the piece latent variables
and they'll face them their projection Z
in here and and then create some you
know this one is less transparent but
the concepts are basically the same and
this one there are sometimes the
performance isn't quite as good so I'll
give one quick example of this so I
carry the sparsity penalty is just the
log of the magnitudes of X you can also
put a then regularizer in there if you
want and the dual here is just looks
like this usually the function here is
not the same but in this case it is and
so a candidate projection operator is
the dictionary times the diagonal of the
magnitudes of the x's times the
dictionary transpose so at first it
might not be intuitive why this is a
good projection operator but what's
gonna drive up tables I will try to
motivate why this might be a good
projection operator especially so here
are the aggregate penalty functions you
get when you when you plug this
production operator here but the primal
space which is a little
what's going on this is the function you
got the projection operator of these
guys
so there's summing up the logs at the
eigen dice which is equivalent the
determinant is the product of the
eigenvalues so you move it inside the
log and this is the penalty question we
get and this is like a measure of volume
so you can kind of see where this might
be a spicy penalty because the
determinant is the volume of the
parallelogram formed by the eigenvectors
of this matrix here so the way you
minimize the volume is by collapsing
individual dimensions as opposed to
reducing all dimensions equally you you
would you you can this thing goofs the
volume to zero if you do that so you can
kind of see how this is a sparsely
product it's also concave in the
magnitudes of X do the same thing in the
dual space and now you get a similar
sort of penalty function in the dual
space but there's no closed form
solution for what this is but you can
still do all the minimization all the
every weighted l1r we learn about to
minimization in either space then one
interesting thing is that if the matter
goes to zero which I just added a random
regulator if the lambda goes to zero
would either this penalty function or
this penalty function
they both have the same minimum as the
hauser amaury and if the lambda goes to
infinity they both converge to like this
gilbert
so they're pretty well motivated
sparsity but that the real thing is what
what are the update rules look like and
do the updated when you do this we
waited a one minimisation do they help
you compensate for a dictionary
structure and I believe they do so here
the promise basically was just the
easiest one to analyze if you look at
exhilarations the first one is just
minimizing that I wonder why so the
first iteration just has weights equal
to one and you solve a regular basis for
soup and solution so what's more
interesting once you have a solution you
plug it into the so the weight updates
are just obtained by taking the
gradients of these two functions so I'm
doing the problem space one now so if
you take the gradient of the primal
space function with respect to each of
the magnitudes this is the weight update
the gap so what is this actually do it
so inside here this term is basically
the eigenvalues of this comes from is
the same as this like that the
eigenvalues of that are where this
parcel penalty and what and what happens
is that if you have two basis vectors
that are currently their projection over
this matrix are going to be basically
the same so the weights WI and maybe
some other WJ are basically going to be
the same if you have two corn
and the question is which ones will be
there and which ones will be small
because those will determine which basis
vectors are eliminated in the next round
so what happens is if you have if you
return to that clustered dictionary when
you multiply an unstructured dictionary
by a block sparse matrix you create all
these clusters so what happens is if the
clusters are active in here the Elwha
motivation will typically pick one basis
vector inside of each cluster and so the
subspace of this will represent all of
the active clusters so the weights here
will basically be the same if you're
inside of an active cluster and it will
either be really small inside an active
cluster or they'll be really big a house
has an active cluster so the first
update will basically choose all of the
appropriate clusters and then later
iterations will pin down the support
inside that cluster so just to say that
worse the initial iterations locate
appropriate groups of correlated based
suspected factors and prune the
irrelevant ones and yeah once the
support is narrowed down once you've
chosen the appropriate sub structure
then later our one iterations can narrow
down the support further so the review
aal11 iterations naturally handle this
transition and the dual space penalty
also does something similar but it has
some additional theoretical properties
which are useful so I'm just a notation
to show some of it here
tied to this idea this side of you and B
is the set of our spectres it's not with
soft support pattern you and Sun pattern
B so X is this vector here this die of
UV would just look like this so it's
just quantify are describing than a
support and sign pattern so here's just
a first result when you use the death
penalty under some very model conditions
basically these are conditions like to
prevent you from having a matrix with
all all one's you're obviously not going
to be able to find the sparse inverse so
the main condition is that every M by n
block of dimensions has to be
independent so the conditions are pretty
model then there we waited I want an
implementation of this dual penalty it
will never be worse than the regular l1
norm solution and for any dictionary no
matter how poorly the columns or
whatever structure it has any sign of
support pattern you'll always be cases
where it does better and maybe the most
interesting part is that this is not
true of any possible projection free we
waited a 100 so that minimizes the cost
function of this form so really this is
saying if you want to compensate for the
dictionary structure you have to do you
have to do something like this there's
standard penalty functions and we waited
on one operators are not sufficient so
an alternative approach though is I
before I started with penalty functions
and then worked out what the update
rules are and and they seemed possible
but less once you've done that why not
just about the weight
it was initially just directly and then
try to learn what properties you had and
can you improve performance any so
here's the basic we waited a 1
optimization procedure
you saw waited on one steps and then you
compute some weighting function but this
path you can choose however you want you
don't need to start with the penalty
function you can just start with what we
think is a possible F and perhaps the
compensate for the dictionary structure
and you can yeah you can choose it
without regard to a specific penalty
function but if this F is a non
decreasing function then there will
always exist an Associated sparsity
penalty that is being minimized so what
are like a general candidate that I've
used kind of based on an intuition and
kind of to get some theoretical results
the hole looks like this form again you
have this term in here where the
magnitudes of X play a crucial role
there basically choosing the subspace at
each iteration the sparsity pattern of
this chooses the subspace that the
dictionary columns that are active that
have important properties for this bar
solution and then there's an exponent
here of P and then there's another
exponent Q and then again you have the
projections of each columns over this
matrix in the middle and for some p and
q bigger than 0 so the implicit penalty
function from this you cannot write out
in closed form it's an integral that you
construct but for the right choice of P
and Q you have some guaranteed recovery
results for structured dictionaries so
here's a cluster dictionary model again
I didn't design this totally folks
or cluster dictionaries but cluster
dictionaries are easier to analyze so
this is just showing that this
particular choice is very useful for
dealing with structure a particular type
of dictionary strong so I defined by
unstructured K to be any dictionary such
that the l1 minimization succeeds for
any sparse vector with sparsity less
than K so then a structured version of
this kind of derived from this edge
structure dictionary is any dictionary
where you take every column of this one
and replace it by a little bouquet on
clustered basis vectors so you have some
radius e every column here you know you
add a bunch of columns that are within
some radius and form a little bouquet or
cluster and then finally the cluster
support is just which of these little
clusters of the case have at least one
nonzero basis vector or coefficient
inside so here's the result if exon is
some sparse signal under optimal
conditions we waited out minimization
using this weighting function I proposed
with any clustered dictionary model will
recover accent as long as the clusters
report is less than K and the sum of the
number of basis vectors within each
cluster is less than and single diamond
look for P equals two cubed some cube
bigger than one and then epsilon
sufficiently small what this is saying
is that it's we've significantly
extended the classic of dictionaries
where you can provably recover the
Maximus bar solution this means within
each of these bouquets you're recovering
the exact support of the nonzero
elements so regular l1 minimization it
can handle unstructured dictionaries
that's well known and it can also handle
dictionaries with a few clusters and
style structure but if you do subsequent
related iterations with these dictionary
independent penalties I'm discussing
then you can extend these results to
many type clusters and in really I have
used this method I tried it out on a lot
of different types of structured
dictionaries it works very well it's
just the clustered ones are somewhat
easier to analyze so though but it's the
open questions what other structures you
can truly recover so here's just a
similar simulation example to show that
this performance works you generate some
sparse vector generating a Gaussian ie
dictionary and then a block diagonal
matrix with uniform elements that are
four by four so so this is introducing
correlations between groups of four
columns and then you compute two
observation vectors one with the
original unstructured dictionary and one
we've included to be here so now you
have two observation vectors why
unstructured and structured in one
structure so you run all these different
methods and you try and estimate X
naught that the two different
observations and ideally the performance
would be insane ideally it would have
not our niche observations you have
because in practice you don't have the
choice of choosing between these two
offering one is just given to you so and
then you can repeat
repeat with different amounts of our
cigars and with this protection method
basically the green and the blue nothing
changes like the performance is
basically identical this is the sparsity
of the bacteria recovering this is the
probability of success if you use the
Kenda's remaining weighting method which
is the most popular and is very
effective their performance drastically
degrades when you use the structure
dictionary and you also might ask why is
the this method inferior even in the
unstructured days and I leave that this
is because the the dictionary size that
used is pretty small it's only 250 500
so there's still a lot of structure even
in a random sample of the Gaussian at
that size if you increase the size of
the dictionary a lot further this line
will get closer to these two so sorry
yeah so I'll just I'll skip a couple of
things and all this summarizes so the
the main point here is that in practical
situations really dictionaries are
highly structured it's almost an
idealized scenario where you get to use
a dictionary that's purely random and
purely on the structure but essentially
mostly to the standard sparse estimation
algorithms are really in doubt but
they're not designed to handle this
situation and so where we've suggested a
general framework to help compensate for
this with dictionary independence
sparsity elements and really it could
lead to a whole new family of sparse
estimation algorithms so in quick future
directions what is the best penalty
function of rejection
these are this ones I've experimented
with and but there can certainly be
better ones and can you also take any of
these projection operators and swap them
into existence fast algorithms and
improve them for the structure
dictionary case maybe there's some very
classic design in this job and then also
at the snips there was two papers also
developing interesting algorithms for
basically adjusting the same problem
with sparse restoration structural
engineers which normally additionally so
there could be some interesting
connections between this work I think so
so one step is very good overnight so
but for the block sparse cases of like a
idealized scenario right so for the
block sparse case one iteration is
pretty good but for other things I've
used it on you you need for their
iterations but they waited out one
converges very quickly so the most
durations I ever run is on the order of
five five or ten after that most of the
time you just kiss any all of them
related one algorithms converts very
quickly so they're just dimension
marginal returns beyond ten generations
for sure but but I tried a lot of
different sources where in practice you
could I normally just keep it fixed but
you could I mean the lambda has
something to say about at what level of
coordinated basis vectors you want to
have to save weight so the bigger you
make the lambda the more correlated
vectors will need to correlated weights
movie cocktail or so
it's that you can see like the subspace
of these active clusters will and how
big it expands out of that to nearby
basis vectors will depend on that vendor
so but for small</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>