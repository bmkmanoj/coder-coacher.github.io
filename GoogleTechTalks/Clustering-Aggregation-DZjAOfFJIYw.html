<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Clustering Aggregation | Coder Coacher - Coaching Coders</title><meta content="Clustering Aggregation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Clustering Aggregation</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DZjAOfFJIYw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I
okay good afternoon everyone I'm very
happy to be here and see many familiar
faces so I'll start with a brief
introduction of myself so I originally
came from grace and i got my bachelor in
athens greece then i moved to stand
forth in this area where i got my master
in PSD with other the supervision of
professor at gmod funny and since then i
move in finland and i work in right now
I'm very certain basic research unit of
Helsinki easy dude for information
technology and again what sounds a bit
weird going from to Finland from gleason
California my perspective change in many
different ways so I found that it's
possible to work on frozen sea and
skiing on flag and they enjoy very much
doing so now and things like that so I'd
like also to give you some information
about the place I am and what I'm doing
and how the place looked like so I'm in
basic research unit and so our unit
comprises of three areas so this is data
analysis new informatics things like I
CA etc and context-aware we are
computing which has a lot to do with
mobiles and making mobile phones more
clever etc the directors of our group
group is professor as Coquelin professor
Haiti Manila we are about 50 researchers
and the working environment there is
very good we have good funding we can
work on problems we like I can host
visitors you can travel so it's a nice
place my duties there is to do research
supervise students and teach courses and
in particular a member of the data
analysis group so the research identify
our group is to do data analysis for
areas that they have mostly areas in
science that they have data and they
have need for to analytic tools
to do to make sense out of the data and
extract some no lids and in particular
we collaborate with people in biology
genetics and ecology atmospheric
sciences so what we are trying to do is
to go and talk to these people that have
the data and discuss about problems are
abstract the problems and thinking of of
developing methods for that problems and
if possible taking the results of our
findings back back in practice and apply
to these methods ok so I'm proceeding
with a talk so the main part of my talk
would be on clustering aggregation which
is some work I did last year with shaky
Manila and panayotis about us and I will
also try to get some time to describe
some other of the projects I have been
working and I give some overview of them
very small summary of what I have been
working me if I get time and feel free
to ask questions at any point to
interrupt okay so so our work in
clustering aggregation is motivated as
follows so clustering is one of the
well-studied problems in data analysis
and machine learning and you have many
there are many different algorithms have
been proposed with different
characteristics and our theses are like
motivation was that it might be able to
combine many different algorithms and
improve the clustering quality so there
is that if I have 10 favorite clustering
algorithms that all of them work okay
but some of them might make some small
mistakes on some part of the data then
combining all of them together might
improve the quality of clustering so our
task is that we start with some black
book clusterings that have applied on on
some data set and then we try to find
clustering that disagrees as little as
possible with the input clusterings and
as you can imagine our work is motivated
by this a lot of work in aggregating
results in machine learning for example
it has been a very useful paradigm there
are methods like boosting bugging etc
and also in web search so one of the
ideas all the ideas has been that
instead of trusting any single search
engine just performers sets in five of
them and then try to aggregate the
results and hopefully get a better a
better quality of results ok so I
started with a small example here very
Tory example just to demonstrate the
basic ideas so I have in that case six
objects x 1 up to X X and I don't know
anything about them so if they can be
like points in some dimensional space or
documents images so the only thing it
has happened here is that I have three
clusterings c1 c2 c3 that they have run
on these objects and they have produced
some clusterings so the labels also here
0 1 2 3 etc are not so important so the
only thing I care I see is that for
example the first clustering clusters
the first two points together because it
gives the same label ok and the second
third and fourth points together fifth
and six and so on and then the same for
the other clusterings so if i have a new
clustering see then i can count the
disagreement with all of those input
clusterings for example you see that on
the first two points and on the first
clustering see you on my new clustering
see agrees because both of these
clusterings play place the points on the
same cluster ok on the other hand see
disagrees with c2 on the first two
points because see to say splits the two
points put them in different class this
one into while si si place them in the
same
so this is the the measure of the notion
that we would like to minimize so this I
will be more precise and I will give
more formal definition but this is the
measure and so this is so this is
another clustering c-prime that has
smaller number of disagreements and I
only want to point out here that in that
case e Prime and C double prime are
identical so it's just real labeling of
the cluster so it's one two two two two
three and three two one if you see so
the labels don't mean anything just the
decision of which points go together to
do with clusters okay so one of the
vision that we made with this framework
of clustering aggregation is that one
can cluster categorical data so this is
one way to class the categorical data it
has been some research on that and
proposed specialized algorithms but for
example you can see that in that case I
have a database table I have some
attributes and now the idea is that view
is a tribute as inducing a clustering on
the data for example the third attribute
job title induces a class thing so I
have all programs together all project
manager sales manager together and so on
okay the same for all for all attributes
and you see some of the issues here for
example the first attribute induces a
clustering that all the points are more
or less single atoms because it's person
has i can say unique name in that
company say okay also the second
attribute in it's a numerical so again
it would be all points be a single tone
but I can also have I can apply a
clustering on one dimensional cluster
all of these points to some range so i
can say the young people some ages I
don't know 15 to 25 and and and then so
on so forth so again the idea is that I
it's
attribute it uses one class three and
then by aggregating this clusterings
gives me one way to do clustering of
categorical data okay one of the issues
in clustering it's that often is
difficult to decide the number of
clusters so for example we have four min
formulations like k-means where the
objective function is minimizing the sum
of square of distances of each point we
represent ative and of course this
objective function can be trivially
minimized by taking k equal to n so it
can become zero or if I k equal to 1
then it gets some very large value so
how i go to select k in between so
usually there is no like a
straightforward way to do it there are
some ways usual case considered to be a
parameter so one of the advantages of
our method is that it doesn't require to
specify k so we just going to minimize
the number of disagreement so then k
comes naturally so there is no trivial
minimizer k equal to N or k equal to 21
for example yes you have tried that
actually one way to select some natural
way of the number of clusters is take
one dataset try different case and then
aggregate them and then you have done
some experiments showing that you get a
good a good idea about k using this
method of course it doesn't work so well
if you have if you try very diverse
values of k if you try one ten ten
thousand ten million then it's going to
get biased towards the larger values of
K but if you have a rough idea what is K
and then you tried different clustering
around that k then it tends to give a
pretty good answer okay
so these are some of the benefits of
this framework so it improves the
robustness we can use this framework for
clustering categorical data the
framework does not need specifying a que
you can do outlier detection with this
relating issue so if we have many
clusterings and I have one point that
all of these clusterings decide to place
that point in a different cluster that
it turns out by our objective function
that it is better to have that point as
an outlier okay and also some other
benefits you can discuss that more okay
so I'll start now becoming a little bit
more formal about the definitions and
and how what kind of algorithm do you
have proposed so I have a data set x 1
up to xn and then I have a clustering
seaweed which is a design partition of
this data set and a clustering is a
function that gives a label to its data
point and this is the label of the of
the clustering and now the label is the
same if and only if two points are on
the same cluster and now based on this
notation I'm going define the notion of
disagreement distance so for that
consider two points you envy in my data
set and to clustering c1 and c2 so I
first define what is this agreement
distance on the on that particular pair
of points U and V ok so I'm saying that
the two clustering c1 and c2 disagree
and the distance is one if they take
different decisions about this pair so
one clustering beside say it decides to
place them in the same cluster and the
other clustering decides to split okay
now and doesn't matter which clustering
does what if they disagree or if they
both take the same decision if they both
place them in the same cluster of or if
they both us split the pair then they
don't disagree so the distance is 0 and
now I define this agreement distance
between c1 and c2 ought to be
the sum of this distance on the pair's
over all possible pairs in the data set
okay and you can see an example here
again I have a disagreement for example
on the last pair of points 5 x5 and x6
because one cluster says they go
together the other splits them so given
these definitions the problem statement
is the following we are given the data
set and we are given M input clusterings
which at this point for this problem
definition we don't know how they came
from and the problem is to find a new
class during see the disagrees as little
as possible with the input clusterings
okay and now we capture that by taking
the sum of disagreements of that ah new
clustering with each one of them okay so
this is the objective function and we
call this problem clustering aggregation
there is an alternative way to view that
problem and this is as follows so
consider the data set X and consider
again M clusterings now I view the data
as a graph complete graph and the edge
disk of distances and if I make the
distances to be the fraction of the
points the fraction of the of the
clusterings which the two points
disagree okay then my clustering problem
becomes finding a multi like a partition
of this of this graph in such a way that
the objective function is the following
for its a point so for its pair of
points that I place in the same cluster
I pay for the distance between these two
points because these are the fraction of
clustering that disagree and I decide
them to put them together so I disagree
with all of those clusterings while if i
split an edge i pay 1 minus that
distance okay so if I have some input
clusterings and I make this graph now
this is
and I want to to solve this partition
problem this becomes an equivalent
problem but of course one might creates
at the graph without starting from input
clustering just have a graph and have
some other way to get weights on the
edges okay so we call this problem
correlation clustering the problem on
starting with the graph and and this as
I just hinted it's a generalization of
the clustering aggregation problem
because you might create a graph without
starting from from some input class
drinks and the reason that that for in
that framework we don't mean to specify
key is that we penalize for both edges
inside the clusters and as it's outside
the clusters okay so k comes it's
optimized somehow without needing to
specify what is a correct number of
clusters okay so here i have another
example but so this maps this instance
of clustering aggregation to the
correlation clustering and this is how i
would form these partitions and i want
to cut only light at this and preserve
this heavy edges ok so these are the two
problems we consider and then what we
did about this problem first there is a
very simple observation that for the
clustering aggregation problem I can
obtain a factor two approximation
algorithm ok this is a very trivial way
to obtain this this algorithm we also
have an algorithm that works well I mean
it gives approximation factor 3 for the
correlation clustering problem this more
general problem that we also consider
and we also consider more
practical algorithms like conglomerate
of algorithm etc and to give also
experimental evaluation of the framework
of clustering aggregation okay so there
is related work on this topic so people
in machine learning and data mining have
also consider other class are clustering
and symbol scheme with different
objective functions like information
theoretic ways and other ways but not
with this objective function this the
notion of the problem of correlation
clustering has been introduced by been
solid at all for graph with edges plus 1
and minus 1 and okay in our case we
consider the correlation clustering
where the edges had special special
values so for example they were between
0 and 1 if you allow arbitrary weights
then it has been so that the problem is
equivalent to multi weight cut and thus
it has a log and tight bound okay and
actually as I said at the beginning our
work is from previous year this problem
i'm presenting and meanwhile it has been
improved by I'll on that and at all that
have a very nice algorithm for both of
this problem so they improve both of the
result so for the clustering aggregation
they have an algorithm 11 / 7 and for
the correlation clustering an algorithm
that gives a recruiting factor too ok so
this algorithm work on expectation our
algorithms work works in the worst case
but still these are very nice this is
very nice work ok so I'll start first by
giving the very simple algorithm I
mentioned at the beginning for for
clustering aggregation so the
observation here is that if I fix the
point the data points and I view
all possible clusterings on those are
data points the disagreement distance is
a metric distance on this space of all
clusterings okay and the reason is the
following so the disagreement distance
composes of some of distances
disagreement distances on on pairs okay
so before going to give you to tell you
that I by medic here of course I mean
that this distance is symmetric
non-negative and obey the triangle
inequality so the reason that this
symmetric is that this sum of distances
over the pairs and if the sum of this
duv satisfies triangular quality then
this the other day satisfied so the
dragon equality and this is very easy to
see why this is so because this is a
triangle inequality so the only way that
I don't get this inequality to be
satisfied because my distances are 0 and
1 is if the left-hand side is one and
the other two are zero okay but what
does this mean it means that if I have
two points U and V and 23 clustering c1
c2 c3 and the right hand side is 0 it
means that c1 and c2 agrees on the
points c2 and c3 agrees so it has to be
that c1 and c3 also agrees on the points
so this is the reason that but I get
this this property get satisfied and now
i have a very simple and folklore
argument for one medium so what I going
to find is a clustering that it can be
any clustering right but instead of
considering any class any what if i
consider one of the input clusterings so
i have a general metric space and
they're going to find one median in that
space so if i take one of my
input points this gives me a two
approximation factor okay so this is a
very simple observation and to have
shown that for that case so we call this
algorithm based class let me just try
all the input class dreams and take the
one that minimize their the objective
function and you have so that there are
instances that this is tight so that
this situation the solution can be two
times far from the optimal so this
algorithm is it has this nice guarantee
but we found that it's not very
intuitive in many case for example if
you think of the case that we apply the
algorithm on the unclass during the
categorical data then we don't find it
very intuitive just to pick one of the
attributes and cluster on that so you'd
expect that something an algorithm would
be able to do something smarter on this
categorical data so for getting
something like that we are considered
also we work on the problem of
correlation plus 10 which is
partitioning that graph okay so what I
would like to do here so i remember i
have this graph and i want to to find
groups of two to partition the points in
groups so that i minimize the sum of
distances within and one mind minus the
distances for the points i split okay so
here to rescue comes again the
triangular quality that holds for if you
assume that it holds for for the
distances okay so the idea is that if we
take if we consider some center point Z
and we take other points x and y that
are close to c then by triangle
inequality x and y have also to be close
to each other okay so the algorithm
leverage leverage is this observation
and this
following so take the vertices in any uh
any order any arbitrary order so
consider one vertex you then take a bowl
of all other vertices around that vertex
you so take all vertices that are in
distance less than a half from that
vertex you and make a bowl ok and now
look at the average distance of this
ball with respect to that point you if
the average distance is smaller than
some parameter alpha that we fix later
then so all of these points are parked
nicely within together so we make that a
cluster ok otherwise if the average
distance is far off is greater than
alpha it means that that point is
somehow far away from from from even for
from points within its ball ok so if you
that us outlier so make that point a
singlet own cluster ok so in either case
you have removed some points from your
data set either the point and it's whole
ball or just the point itself so take
that points that you have already
cluster take them out and repeat the
process until you left with no points so
we could show that if we choose alpha to
be one-fourth then this algorithm has
its three times away from the optimal ok
and I'm not going to the details call
this is proven ok so this is our result
we also experimented with other
algorithms that we found also intuitive
and practical so for example
agglomerative algorithm just stutter
bottom-up algorithm mayor's close class
nearby clusters until the
cost function does not improve and also
we could so that in that case if we
start with M clusterings then get
approximation factor of M minus 1 which
is not very good but for some small
values of families is it's reasonable
and then we in our implementation we
experimented with a local search idea so
assume that you have some clustering
already may be obtained with one of the
previous methods now start making local
moves that might improve the cost so
take one point you and then see if you
can improve the cost by putting it in
some other cluster or if by making
making it a singlet on okay and if the
cost improves that make this move
otherwise don't make pick another point
and continue until you make a fixed
number of moves or until your cost does
not improve okay and I found that this
also helps a lot and also in practice or
even though that alpha equal to
one-fourth gives three approximation so
that it's better to come to to consider
larger values like a point for for
example but again this was empirical and
it might work for some data set but not
for some other okay so one unfortunate
thing about the algorithms is that they
are not so scalable so they are
quadratic so for example for correlation
clustering quadratic complex disinherit
because you start with this graph of all
pairwise distances so for the case that
we start with the input clusterings and
the complaint quadratic complex is not
inherent we consider how to make that
more scalable so our idea was to how to
use some sampling okay so just take a
sample of the points cluster them and
then for all of the other points
now you assign them to one of the
classes found in the sample but in that
way if you have really small clusters
that are some far away and your sample
and you don't sample from that clusters
all of the points would be said to be
outliers so you have a second step that
we take all points consider as outliers
and we cluster them separately so this
was the idea of how to make this
algorithm more scalable so for our
experimental evaluation I'll just give
you some examples of results so we did
the results on synthetic data sets in
real data says so for for real data said
to him mostly apply the framework to
clustering categorical data the way I
explained it at the beginning so for to
start with some very simple like some
toy examples some illustration of how
the algorithm works so here is a little
example that I created myself by
clicking on some on some matlab screen
and then i applied default matlab
functions to class at the point so he
end of course I I try to make this data
set so that it has difficulties for all
the algorithm so for example a human
would cluster the right most clusters
are separate but I created the breeds in
between so that I fooled a single look
at algorithm use I makes a spanning tree
and and stops at some point and i also
had imbalance imbalanced clusterings non
concave something like one close to
another and things like that okay so
this is not a challenging but it's just
an illustration so what's the idea so if
you apply to this data set single
linkage then ah there's some mistake on
the
right most clusters and also this other
two both get together and things like
that and then if you get another
algorithm from MATLAB like completely
nuts and you apply to this data set
again you get some things that you
consider mistake like splitting the
upper left cluster and splitting also
this big cluster on the bottom left side
and things like that and average linkers
again that's a mistake words clusterings
a mistake k-means some other mistakes so
you can see now the the lines of the
Voronoi diagram on that large cluster on
the on the bottom left side and the idea
is that if you aggregate all of them you
get some answer that what you would
expect okay but as I said this is just
for illustration purposes for real
datasets we did experiments on late
assess we downloaded from UC Irvine
repository of machine learning and so
use votes mushrooms and census data one
thing here that it's a bit debatable is
the how do we evaluate clustering so so
what are the what is the ground truth
clustering so what we did it was that in
all of this data set you have one class
that is usually used for for learning so
for letting purpose for example in the
host data says you have a members of US
Congress that vote on 16 different
issues but you also have the information
that it's but if it's person is democrat
or republican okay so what you see is
that we don't use this information for
the clustering but we compare the
clustering with that so it's like using
the clustering to do the classification
test so we measure for its cluster we
found
these are the purity of the cluster like
how many of the take what is the
percentage of the majority class this is
the how much you classify correctly so
this is the purity and the rest we call
classification error and this is how we
measure the quality okay so the first
result on the votes data set so I
present different algorithms so we
compare again two algorithms existing
algorithms for for doing the specialized
algorithms for doing categorical data
clustering so like rockin limbo so my
point so what I going to mention here is
that so our best algorithm like local
search does has a classification error
of twelve percent while both of the
other both of the two other algorithms
have eleven percent but we had the
advantage that are k was found it was
discovered automatically from from that
data set while for the other algorithms
they are not so parameter free so you
have to set k equal to two in both of
them and some parameters fee and theta
that we asked from the authors of that
paper or we get from their papers what
they would use okay so this is the
resultant votes and for example on yes
so the rock is an algorithm it's old
algorithm by Sudipto googa and cusick
Seemandhra zebra snuggie and they use
some notions of the card coefficient too
so they say too so they somehow measure
the distance between two values by
common neighbors so using the jacquard
coefficient and they devise some metric
like that limbo is by undred sauce and
supporters who is also my co-author on
this paper and this I think information
theoretic clustering on categorical data
so limbo is doing really well for
example is in that data set so this is
the mushrooms okay so we do our like ten
percent classification error or 11 for
the agglomerated with seven clusters
while limbo does also 11 for two
clusters yes
so yeah so in that case we don't so you
use this idea to do clustering of
categorical data so we take its
attribute induce a clustering and
combine those so just use the data yeah
and okay so in that case we do better
than rock we do worse than limbo but the
idea is that it's not so specialized
method and its parameter free okay and
this is an example on how pure the
clusters are found by agglomerative so
ugly relative algorithm find seven
clusters and here ok so the class here
so that the two classes that we consider
a real our classification on poisonous
and edible and all of most of the
classes five of five of out of seven of
these classes are pure they contain only
poisonous or edible mushrooms except so
the most classification error is caused
by the first clustering okay this is the
most unpure cluster ok so now another
interesting point for example for says
census data set so this is some data set
on US population and attributes like
income and marital status education and
things like that so when we applied to
that our framework gives very many
clusters like 50 or 60 okay while
actually the classes are only two like
in that case the classes they say if a
person earns more than fifty thousand
dollars or less so it's not so fair to
compare with with other clusterings that
or we say it's not very reasonable
meaning
full anymore to consider the
classification algorithm so we get this
somehow artifact that to get 50 or 60
clusters but after inspecting closer the
results we find that many of these
clusters are meaningful for example you
have things like Eskimos fishermen and
you know very small data set that are
everything is different from from the
from the rest of the data set so you
have minorities of attributes that are
that are there okay so this is again
okay so I don't go to scalability
because I i would like to okay it scales
okay the algorithm this is the point so
I would like also to get to some other
projects that i have been working and
give an overview so in particular so so
how much time I have because I started
like 10 minutes after one so they
stopped at two or should I continue
until 10 minutes okay thank you yes
yes
yes so yes so the question was that the
objective function has it was somehow so
if you say that two points agree if so
to class drinks are green to pones if
the two clusterings both split or both
put them in the same cluster so this is
symmetric while it might it is
meaningful to also consider not
symmetric so to give the different
weights so that's a very good point so I
think many of these algorithms can be
adopted in that cases we didn't have a
good idea of what might this way its
come from so we just stay on the higher
level of the problem but yes I agree in
practice you can play with with these
parameters ok so I would I had prepared
to talk about three projects i think i
will go only for the last one so network
immunization because this is some work
that have been done recently and i would
like to continue on that on that general
topic at least so and this is work I did
with Georgia copies of maria tellez in
Vonnegut's operas so this is in the
setting of a complex network social
networks etc so nowadays you have many
data man-made or systems or that
organized us as networks so for example
have internet computers to computer web
web pages point to other web pages you
have social networks people have
interaction with other people protein
networks like protein protein to protein
reactions and stuff like that and often
the operation of such a network is
threatened by the propagation of a
harmful additive throughout the network
for exam
ball in social networks in networks of
people in communities we have diseases
that gets played in social networks we
have some gossip or panic behavior that
gets spread and so on can have computer
viruses on the internet so the question
is called can you restrict the spread of
the virus on the network and this is a
topic that he has studied a lot in like
many in that sand in our sensory by
because it's essential in epidemiology
and so there are mathematical models to
study that problem and so on so what we
consider this is what ok I will try to
point out where is our contribution and
our work but in general the way we view
the problem is that we have a network g
of so we view it as a graph with
vertices and edges and known that
network we have a model that a virus
propagates so and this model can be
probabilistic or deterministic or
something and we also have an adversary
so this is somebody some like
adversarial entity that places some
virus on the network so that versary can
be blind like does know anything or can
be adaptive so that versuri is aware of
what is our strategy of restraining the
spread of the network and try to place
the virus in a place to do as much harm
as possible ok and our problem all our
tasks the following is to device
immunization algorithm that is given the
network is given some budget which is a
number of nodes and say we're like have
a model of how the virus propagates on
the network and the task is to find K
nodes on that network so that the spread
is minimized and you can think of some
very reasonable immunization study this
for example if I pick the node with a
max degree that might intimidate node
then I might avoid spread of the virus
in the network
ok I'll peeking at Adam might be a good
strategy if I don't know anything about
that network so we consider studied two
cases of so the algorithm of course the
immunization strategies should depend on
the virus propagation model how the
virus spreads ok so you consider two
cases one is what is called a dependent
cascade so that is that we have an old
you that at some point becomes infected
at first time and then at that point it
becomes affected it has some neighbors
so the node tries to infect the
neighbors and succeeds with some
probability P okay so if the node u
succeeds then the neighbor becomes
infected if it doesn't succeed if it
fails then it never tries again to
infect it so this guy I cannot infect
human ty I stop bothering okay so this
is the so it's one shot probabilistic
model with infecting of one node to
another so for example you can see that
if i if i have p equal to one if I
always in fact all my neighbors then in
effect if I place the virus in one place
of the graph then i infect the whole
connected component that I start if P
equals one but in general p is not equal
to one so in that case if I would like
to immunize the network I would select
the node to immunize that minimizes ah
when I take out minimizes the size of
the connected component okay so in the
blind case when the adversaries blind I
will try I think
I would try to minimize the expected
size of the connected component if the
adversary was adoptive I would try to
minimize the size of the largest
connected component okay so so for that
we propose a greedy algorithm that tries
to minimize the expected size of the
largest connected component okay there
are many details here I'm not telling
you how this is done it's not so
straightforward but we found that this
heuristic works better than immunizing
the max degree node in so it always
works better than immunizing the max the
green old max degree in many cases is
equally effective so if the graph is rad
okay if the graph is rather everything
works about the same even Adam started
this work the same if the graph is power
law then max degree works equally well
as this greedy heuristic but if the
graph is small world in the sense that
it has small average distance a large
clustering coefficient so this what is
called small world graft then this this
heuristic the strategy is much better
than selecting the max degree node okay
so this was 11 virus propagation model
another one is a more dynamic case where
the virus is death with a birth-death
process that evolves over time and in
that case the model is the following so
if if if I if an old has that has the
virus then it it's it infects enable
with probability better okay and I call
that propagation probability at the same
point also if a node might heal itself
with some other probability beta Delta
so the
ratio of the two probabilities better
over Delta is well what is called the
infection rate of the virus in the
network and it is known from studies in
epidemiology mathematical models that
there is a threshold phenomenal and
transition phase in the graph so i have
this quantity 1 minus 1 over lambda 1
where lambda 1 is the maximum eigen
value of the vector of the of the system
and if the infection rate is smaller
than this threshold in the network then
the virus dies out exponentially fast
okay so our task our immunization task
would be to select a node to immunize
that minimizes the first eigen value of
the system as much as possible okay
because this will bring the infection
rate below to the to that threshold of
the network ok thank you ok so for for
that particular virus propagation model
the strategy proposed is a following so
you have the that is it there the system
matrix you compute the eigenvector ok
now it's of the coordinates of the eigen
vector corresponds to a node and now you
pick to immunize the coordinate that is
it has the maximum value in the eigen
vector and the idea for that is that
since you want to minimize the
eigenvalue and the eigenvector is
somehow is a vector and the and the
length of this vector corresponds to
again value and you want to minimize the
gain value you better choose one
coordinate that it's aligned the best
with that eigenvector ok
so and you iterate this process until
the infection rate goes goes below the
threshold of the network and again so we
don't have a theoretical proof that this
method is the one that brings the
eigenvalue as much as possible in fact
it is not there are examples of networks
in which this doesn't work it's not the
best note to that brings the eigenvalue
as low as possible okay but what we
found that this is this heuristic is it
works very well so it works always as
good as immunizing the max degree node
and force again for the same class of
graph the small world graphs it is much
better than immunizing the max degree
node in many cases max agrees very good
like scale-free graphs or other kind of
graphs max degrees as much effective
okay and so this is an area i would i
would also like to consider as i study
more like I social networks information
networks how information spread on
networks and these kind of things so
I'll summarize my talk so I'm generally
interested in problems in data mining
and I have studied problems in
clustering sequence analysis zero and
data and social networks and I would
like to work in problems that have
applications to real data okay thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>