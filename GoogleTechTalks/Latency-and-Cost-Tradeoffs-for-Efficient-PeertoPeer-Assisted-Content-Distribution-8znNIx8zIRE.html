<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Latency and Cost Tradeoffs for Efficient Peer-to-Peer Assisted Content Distribution | Coder Coacher - Coaching Coders</title><meta content="Latency and Cost Tradeoffs for Efficient Peer-to-Peer Assisted Content Distribution - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Latency and Cost Tradeoffs for Efficient Peer-to-Peer Assisted Content Distribution</b></h2><h5 class="post__date">2009-11-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8znNIx8zIRE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you all for being here and thanks
Andre for having me over my name is
Wayne oh gosh and I teach at Texas A&amp;amp;M
University the kind of stuff have been
doing so I got my PhD at Illinois
urbana-champaign in 2007 then I was
opposed talk at Stanford for about a
year and I've been teaching at Texas A&amp;amp;M
since early two thousand eight so I've
been there about a year about two years
now so the stuff I'm going to talk about
today has to do with latency and cost
trade-offs in particular for peer to
peer assisted content distribution but
there's a much more general problem here
on queries and how you kind of
distribute them you know based on the
latency that they suffer so before i get
into the talk i just want to kind of
give a little overview of the kind of
stuff i've been doing and where it fits
in with networking in general so this is
a picture of the internet created by
brad Huffaker at kada so kayla is the
place where I interned a couple of times
I've been wandering a fair bit and I
spent two or three months in a whole
bunch of places now the picture here
basically shows the internet at what is
called the AAS level the autonomous
system level so basically what it
indicates here is that you know or the
the location the radial location of the
of the particular autonomous system is
based on the kind of number of customers
it has the amount of traffic it carries
kind of stuff and the the location on
the ring is based on geography so this
is North America that is the Atlantic
Ocean Europe Asia Pacific Ocean so the
reason why I put up this picture quite
often is that when you look at this you
see that there's a tremendous amount of
interconnectivity you know between
multiple different locations and very
often we don't really exploit this
diversity you know what I need might not
be just in one location but it by be in
multiple locations so really the kind of
stuff I've been doing kind of in in the
space of network control and network
algorithms I'm going to talk about
content distribution today but in
general I've been working for example on
protocols which take advantage of the
multiple routes between a source and
destination I've been also looking at
how to provision qos on an end-to-end
basis I've been looking at access
methods for example if I'm sitting
around with my laptop I see multiple
different access point can I somehow
split my traffic and take advantage of
the fact that I'm
have diversity in terms of number of
channels available to be multicast takes
advantage of the idea that there might
be multiple people interested in the
same thing so why should i transfer
multiple unicast to somebody when lots
of people wanted together in particular
the application we were looking at there
was kind of like in a football stadium
where people have smartphones and people
want to look at pretty much the same
thing scores perhaps replays and that
kind of thing so this is all has to do
in the space of network optimization and
control also been working on network
economics how do I price this thing each
of these kind of boxes is what is an
autonomous system google for example is
an autonomous system the guy in the
middle over there is verizon business
which is a s number 701 it's had a long
history it used to be uunet than BBM
planet and finally we are verizon bought
it now there's more there's a good deal
of competition between ISPs which
operate at different levels so good
pricing models which are stable is a
very interesting question this plays
right into net neutrality because the
the economics has to do with DD with the
QRS as well because you cannot have TOS
unless somebody is going to pay for it
and you can't have anybody to pay for it
unless you give them two eyes so
economics and technology really ties up
in this space of the internet economics
because you need good pricing schemes
you need to know what isp competition is
likely to lead to and that kind of thing
finally and this is where Andre comes in
I've been working a fair bit with kada
on network topology why this thing looks
this way how does it evolve with time
understanding the performance of
protocols in the real world and that
kind of thing so i can if people are
interested in this space i can of course
spend some time i'm here till the end of
next week so the thing i started with
just a little while ago was if i want
something why should i go to the other
end of the world to fetch it this here
is Houston which is close to college
station which I'm at and that is Bombay
in India now if I want a piece of
content there is no reason to ship it
from Bombay you have to be you can get
it from multiple different locations
indeed if you have queries to be
processed you don't need to process it
at one particular server farm or one
particular cluster you might have
multiple different locations you might
be able to process it so the question
really is how do we take advantage of
the diversity present in where my query
or my content
can be serviced from so this is a little
picture which which is generated by
pando networks which is a start up with
whom we have had some contact so they
basically illustrate the fact that you
know when the internet was created like
this was a little earlier but roughly
speaking the internet really took off in
the early 90s the big applications there
were FTP and email right so the idea was
because the internet was created to ship
large data sets between you know big
labs like cyclotron data which is huge
massive data sets you ship them from
slack in Stanford perhaps to Illinois
where they have a supercomputer center
so you notice ftp was enormous an email
has was was in existence and then people
realize that hey we can use this
internet for many other things in
particular the web started growing up
and so web traffic really took off in
the mid to late 90s but more recently
the understanding that queries can be
processed from multiple locations so if
I have a file on my machine I can give
it to somebody else if somebody else has
it they can give it to more people so in
other words using end hosts as servers
has really really taken off in the past
couple of years and you notice that in
2006 for example a good deal about
seventy percent of the bytes as these
guys measured where is different
measurement traces seem to indicate
anywhere between about 60 to 70 odd
percent or 75 percent of the bites on
the internet seemed to appear for a
peer-to-peer traffic now further this
curve stops it 2006 more recently there
was a report in nanog this year on on
the fact that there is a good deal of
video traffic growing up on the internet
and so the p2p in North America for file
sharing seems to be going down but
peer-to-peer for video distribution is
really popular in East Asia in China
there are these things called PP live TV
and QQ live and so on all of which we
are using now peer-to-peer ideas for
video distribution so so the question
high level again is how do we use kind
of a distributed resource like
peer-to-peer servers or multiple
clusters in a server farm to service my
request rather than servicing it in just
one location so I'm going to talk about
these two i'll talk about in fair detail
and the third one
less so the first question I'm going to
deal with has to do with latency and
costs in so-called ISP friendly
peer-to-peer so the idea here is that
you know I can service my query at
multiple locations there is latency at
multiple locations and there's a cost to
get there this might be a transit fee it
might be round trip time it might be a
bandwidth costs so question is how am I
going to do load balancing in such a
system in a very simple fast and
distributed manner so I need to do this
in a parallel fashion now the second
question I'm going to talk about has to
do with file-sharing so very often when
things get popular they get popular very
quickly this is called a flash crowd
Allah give a little more an idea of what
it means in a little light but really
we're going to understand what
constitutes a flash crowd and how do I
service such a crowd do I use a server
alone or a fixed capacity content
distribution network do I use peer to
peer or do I kind of join them somehow
together and finally if I do get time I
will put up a slide or two on using
peer-to-peer for video delivery as well
so the first problem has to do with
latency versus costs in so-called is be
friendly peer-to-peer the motivation is
the following supposing I have multiple
different internet service providers say
comcast is one of them suddenlink is
another Road Runner is one more today
peer-to-peer when I asked for a list of
peers to communicate with what it gives
me is a list of peers from all over the
place so in particular if I belong to
comcast I might get some peers from
there some peers from there some from
here now is that good or is it bad now
it is good in the sense that it gives me
the best peers I get the peers who are
most likely to be able to give me
something worthwhile it is bad in the
sense that Internet service providers
have a cost of transit traffic between
each other so if I have to move traffic
from here to here it costs something if
I have to move from there to there it
costs something and so on so Internet
service providers have been kind of
trying to kind of saw tamp down on
peer-to-peer traffic simply because it
increases between ISP costs but if you
just look at costs alone you don't
realize that the latency of users can
suffer if you just force them to live in
one particular isp so to give a clear a
description of what the problem is we
would like a system which takes into
account both
transit fees and latency now this is
very important to this the distributed
query processing as well which I will
just mention in just second so I can you
know reduce transit fees by simply
localizing traffic I just keep it local
no transit fee I can include reduce
latency by choosing the best guy who can
serve me the question is how do I handle
both of these together how do we achieve
this optimal operating point so this
tussle between fees and latency is what
I want to talk about and there's a bit
of work in this space very often it's
kind of heuristic it doesn't quite
capture the essence of the problem in my
opinion but nevertheless there are some
solutions which work fairly well and
what I'm going to talk about is a little
bit about the analytical solution to the
problem and how we implemented a very
nice distributed solution for this so
here is my problem so supposing for
example say I'm thinking of a query
processing a location so basically these
guys are all front-end clusters or
basically when my query shows up it is
handed off to some particular fronted
these requests which are coming down
here or all the queries is okay so for
example this is a particular location
there is a fair number of requests which
are coming to this fronted now this
front-end usually what you would think
is it is connected or is associated with
some back-end processing engine it could
be a cluster of some kind so all these
nodes could be some kind of processors
and this this front ends job perhaps is
to assign these queries to that back-end
cluster fair enough this is happens if
there is just one front in and one back
end available to me what happens if I
have multiple locations so here what the
objective is is I have lots of requests
coming in here and a fairly limited
processing capability here here I have
fewer requests but a larger amount of
processing capability because I have
more nodes then so really I should i
would think that hey I should send some
of these requests over there so that
they can be handled quicker however the
quickness or the latency of handling is
not quite everything because there could
potentially be either a round trip time
latency between these locations or a
bandwidth costs or in my isp case there
could be a transit fee between so really
it's not purely about latency it is both
about latency and fees now
these the other was just are saying
exactly the same ideas apply to load
balancing of queries I have queries
woodshop they have to be handed off
somewhere for processing do I hand them
off to my local cluster here do I hand
them off somewhere else what do I do
with them so that is really the
requirement this business of steady
state and transient refers to the fact
that there could be clusters for whom
the request rate is less than the
available capacity well there could be
other clusters who for whom the request
rate is bigger than the available
capacity so clearly I should hand off
some part of this over here but what
fraction how do I do it in a dynamic
fashion how do I measure the latency in
a feedback fashion how do I do it in a
simple fashion so that it converges
quickly and efficiently so that is my
first problem here so they have a little
bit of notation here so all I'm saying
is that this superscript refers to the D
back-end cluster while a subscript will
refer to the front end which does the
assignment so really what I'm saying is
any cluster we assume the cluster in our
name I is assumed to have C soup I
queries per unit time capacity so for
example this cluster out here would have
fee soup 1 C 1 amount of capacity for
processing queries the requests arrive
at the front end server which I am
calling a multitrack em tracker or
whatever a front end server is as some
process it could be a Poisson process it
could be anything really with certain
parameter as X sub J queries per unit I
so to go back to my picture I might have
requests coming in here at X sub one
request per unit type so the question is
what do I do with this X sub one request
per unit time do I send them local do I
split them up what do I do with it so
the important thing is I actually don't
need to know either of these for my
algorithm to work program properly I can
work with measured feedback latency
signals as I'll illustrate when i get to
the algorithms so anyway so we have a
kind of timescale separation argument
here we say at the large time scale the
load changes so in my p2p system where
i've taken several measurements it's the
the things which yet popular get popular
together so of the order of like 22 half
an hour 20 minutes to half an hour
nothing much changes in terms of the
number of requests per second so I can
think of each of the
is kind of half an hour time slots now
the capacity of the systems each
peer-to-peer cluster or if you are
thinking of back-end clusters the
clusters themselves could be dynamic in
that some things could go online some
things go offline and so on so I assume
that in the order of minutes the cluster
capacity or the peer-to-peer cloud
capacity itself can change so so this
was like half an hour intervals this
could be something like 10 minute
intervals finally I have to do my
traffic assignment where am I going to
round my traffic to this I do of the
order of seconds so really I am first
going to just deal with this looking at
capacity is constant load as constant
then I am going to go up here and say
how do i change the capacity finally I'm
going to talk about how does the load
change so the notation which we had it's
exactly the same thing that I had
earlier so we had this X sub J the
request rate at the front end server J
in number of queries per unit time and
an assignment matrix X is the way I
assign this so what does this mean so I
have this front end server J and it has
multiple options say he has three
options so then he has to decide how
much of the traffic he is going to send
to option one which will go to the
cluster one how much to send to to how
much does into three and so on so
basically this is the way the front end
server J partitions is his load now
there are multiple front end servers so
each one is doing this partitioning we
call the whole matrix of all the
partitions the assignment matrix as
capital X so is there any question on
what this means basically to give
another illustration of what's going on
I have an X up J coming in here I have
an x11 going in here and X 12 going
there and x 13 going here potentially
similarly this guy can send traffic to
each one so this the assignment matrix
would be a nine by nine matrix because
each guy has potentially three options
options and so so you can have some of
them could be 0 of course but
potentially there is a nine by nine
assignment matrix here oh sorry three by
three a total of time yes sorry so there
we all have our assignment matrix the
question is what am I going to do with
this so again to just recollect the
notation the origin is always the
subscript this is the front end where
the query is I
chilly received and the superscript is
the destination to which the query is
routed so that is the notation all over
the place so X sub J soup to means that
the jet origin will route this much
amount of load to that particular
cluster and obviously I cannot drop any
queries so the sum of the partition or
the sum of the assignments must add up
to the load at that particular
particular front end so the question is
how am I going to do this assignment now
in order to understand how to do the
assignment I need to have some idea of
what latency is likely to look like so
what is latency so really I send you a
query and after a little while you
respond that little while or a long time
depends on how many others are also
wearing the same thing so so this is a
very simple latency function it comes
from queueing theory and it's really
straightforward to understand what's
going on here so basically the idea is I
have a engine which can respond to
queries at r8c soup I queries per unit
time and I I feel in a load at a rate Z
soup I number of queries per unit time
these things go and sit up and queue up
and this query engine keeps processing
them and throws them out question is
somebody comes in here he sits there he
kind of goes across gets certain get
help the question is how long is he
going to sit there what kind of shape of
the function is this going to look like
and the answer it it always looks convex
increasing because the idea is in fact
that if you look at that expression as Z
is much smaller than see you have very
low load as Z comes close to see the
denominator goes to zero there is
therefore the whole thing goes towards
infinity so the idea is as I increase
the load on a particular server the
latency the time between arrival and
departure is going to increase in a
convex increasing patch and this is true
not only of such functions it sure
pretty much any system you choose to
measure like this is true for
deterministic are for a deterministic
job size it is true for example in
peer-to-peer systems which have got some
measurements on it's pretty much always
happens that as you get closer and
closer to capacity the the number of the
latency involved in fact if you look at
the 101 at six o'clock you'll see this
in action so you know as you get closer
and closer to capacity you have huge
latency
so the total latency which is my
unhappiness of my users so the per user
latency was that and the number of users
per unit time or number of queries per
unit time is ISA pie so the total
latency suffered by all the users
getting in here is this so how does that
play into the picture of this
distributed cache distributed query
processing so that was my picture right
I had you know traffic coming in or our
requests coming in from multiple
locations to a particular affronted and
that in turn went to the back in to be
processed so how much traffic is coming
in so we have basically traffic from
there there and there to be summed up so
so basically we have traffic from front
and are coming to cluster I so add the
cluster I the total load is just the sum
of traffic which comes from the various
different locations so in that
particular diagram the the cluster
number one is handling x.x from two to
one from three to one and one from 1 to
1 itself so that's why I have to sum all
the traffic which comes in and of course
the total latency has some expression I
am just using the MN 1 expression here
for illustration so I just stick in Z is
equal to the total load coming in and
this is my total latency experienced at
some particular cluster so that is my
latency great what else do I have I have
this transit fee as I said transit fee
can be round trip time on the link which
is a potentially a propagation delay
plus queuing delay it could be the
bandwidth costs because i have to pay
somebody for bandwidth costs in my guess
what i'm saying is it's a transit fee
between ISPs so whatever it is there is
a certain fee which which which is
charged or which is the cost incurred
for transferring users from one location
to another and that is charged on a per
query basis so if i transfer one user
from our to cluster I r is the front end
where the user actually showed up and I
is the cluster where it finally gets
processed per user it cost me that much
so again we have to find out how much is
the total transit fee to get to a
particular cluster and that we do by
simply summing up the total load so in
other words I have traffic coming in
from two to one from three to one and
from one directly to 11 to one costs me
nothing
two to one cost me per use or something
three two one cost me per user something
and that is really what is being summed
up here so this is the total transit fee
or the total cost in getting to the
cluster right so much for that so what
is my system cost my system cost that I
look at is the total latency plus the
total transit so we developed an
expression for the latency which was
basically load divided by c minus load
so if you get rid of this this basically
says for any particular cluster I what
we have is the latency on the cluster
plus the transit fee to get to that
cluster and then I have to add up over
all my clusters because that is my home
system so if I have multiple locations
each location experiences in latency a
total latency each location there is a
cost to get there that is the total cost
of a particular cluster over all
clusters is the sum all right so we need
to design a controller that somehow
minimizes this this really is the
unhappiness of the world as far as I'm
concerned so how do I minimize this
unhappiness right it costs me and
latency cost my users because they get
unhappy transit fees cost me because i
have to pay it out finally so the
question is how do i design such a
controller that is both simple
distributed and fast so these are the
three things distributed in particular
is very common among network
optimization folks simply because you
know there are millions and millions of
flows on the internet there is no way on
earth you can do an optimization with
all those flows three in one particular
you know central computer so distributed
processing of such optimization problems
it has been a big question and there's
lots of ways of handling this now so the
objective really is you know this is my
unhappiness there is a lowest
unhappiness that somehow I want to
attain and I want a distributed way of
getting there so this makes me as little
unhappy as I as feasible now there is a
whole bunch of controllers which can do
this oops so the question is how do I
solve it and there are lots and lots of
optimization techniques and i'm going to
pick one which is particularly useful
for this problem so usually there are
some which are fairly complex in terms
of the computation you can do
centralized computation you know in one
big machine
and these methods although they give you
very accurate solutions require a lot of
processing in a particular machine so
the other methods which have put down
here these are some of them are
basically distributed methods so there's
Newton methods there's log-linear
learning best response which means do
the best thing given the state gradient
descent and so on so what I'm going to
describe is actually a coupling of these
two which is particularly suited for
this thing but really what I wanted to
say here is there is a whole bunch of
tools which have been developed in the
kind of our optimization the the
operations research literature which are
useful for such problems and so very
often when we solve network optimization
problems you can go and you know look in
this bag full of controllers and pull
out one which is most suited and then
show that it's suitable for your
particular application so so I will tell
you what my controller is going to be in
a second but before doing that I need to
know what the kind of per unit cost is
going to be so what does this mean I
told you that the total cost of the
system is this the total latency the
story transit fee now the question is if
I take a decision on how much load to
send to a particular cluster how does
this cost change so for example I have
two clusters and I have a arrival rate
of you know requests coming it now if I
move sum the sum to one cluster and some
to the other maybe it's good maybe it's
bad question is how good or how bad is
it so the question is if I change this
if i change my assignment on a
particular dimension so i move a little
from one to the other how does the cost
change is it going to be a simple
function is it going to be messy how do
i do this so the answer is usually in
fact in this particular example it's
actually very simple so all i need to do
is basically i need to understand the
impact on the cost of changing one
dimension so to recollect X so sub J
soup I was the amount of traffic which j
the the front end J was sending to
cluster I so the question is what impact
does that one dimension change have on
the overall cost of the system an answer
is obtained by partially differentiating
one with the other and the answer has a
really nice form here what it says is I
call this the marginal cost marginal
call
off of a cluster the front MJ on the
cluster I and this marginal cost has
three terms one is the marginal latency
I send people there they become unhappy
so how unhappy to they become I have a
per-user unhappiness what the next thing
is the transit fee I send one guy from
one place to the other it costs you p ji
because it costs you per user that much
so in other words these two are very
intuitive I send one person from A to B
it he gets unhappy based on the amount
of latency experiences at be if I send
one guy from A to B the amount it costs
in terms of transit fee is pj great
there is a third one which is not quite
so obvious which is my impact on the
others unhappiness so in other words i
send a guy from A to B they're already
guys being served out there so the more
i sent their the the other guys who are
being served also get more unhappy
because my impact on their latency has
to be considered as well that is the
third term which i call a congestion
costs so these three together is really
the impact on the total cost of the
system so there is a marginal latency
which is the user experiences a marginal
transit fee which I have to pay and a
marginal congestion which is experienced
by everybody so this really is how the
unhappiness of the world is going to
change as I change my particular
dimension now these three mind you are
all measurable in fact in the system
which I am going to consider we don't
even know our sea or the XOR we know the
p but we don't know anything else you
can measure this you can measure that
and you know this value because it is a
cost given to you so the question is now
now that I know what my marginal impact
on the system is how am I going to kind
of change my assignment so that is where
this the dynamics comes in so to repeat
I have currently an assignment matrix
who is getting sent where I want to
change this matrix I change it on each
dimension separately so I have to change
really the amount that i am assigning
from j two s so j is the front end to
which the request came s is the cluster
where it was served how do i change the
number of users i am sending to each one
and the answer in this case turns out to
be very nice
and this is one called replicator
dynamics this comes from evolutionary
thought the idea is things which are
doing value should do more of it things
which are doing badly do less of them so
that's pretty much all this is saying so
suppose if I were sitting here and I
have three options available to me
option one costs of say ten dollars
option through costs I don't know ten
twenty dollars option three cost me five
dollars so clearly I am going to send my
guys to the five dollar option I keep
doing that until the five dollar becomes
ten dollars now the 10 and the can I
have two of them which have exactly the
same cost then perhaps I'll do equally
until they all hit 20 and then you know
the system is essentially stable so
really what I'm looking for is a
comparison between the cost per unit for
a particular option from of sending
users from j2s with the average cost
over all the options available to so
again in the three option example I
would calculate my average cost over the
three options and I would know the
individual costs because I calculated if
the particular option is doing better
than average which means it has lower
costs I should do more of it so if this
has lower cost than the average do more
of it so this minus this will become
positive because this has the lower cost
than the average cost so it goes up if
the particular options cost is higher
than the average which means this is
smaller than this because this this cost
is lower than this this thing turns out
to be negative I'll do less of it so do
more of things which work well do lots
of things which work badly and the we
can prove actually this is actually
stable it actually attains this point
out here that if we do this over and
over again as a dynamic system it goes
to that lowest cost point which we were
looking at if we use the fee in this
fashion so again we have to consider per
user latency per user transit and the
impact on others the per user increase
in congestion if you add up these three
and push it in to this into this kind of
do more of whatever works for you it
actually can be proved to be optimal and
how well does it work now this is on
something called network simulator so
basically we we modified the bittorrent
client there so that you could actually
move traffic from one location to the
other and so the three curve
which you're looking at is the following
so single tracker means that we don't
allow you to split you have those
multiple locations I say I will not
allow you to move your traffic from one
location to the other now that is of
course going to do very well in terms of
transit fee because I'm not moving
anything it does not cost me it but it
is horrible in terms of latency because
there are some clusters which have extra
load some clusters which have less load
so the the cluster which have extra load
more than their capacity they do really
badly so the overall latency goes up
significantly now the others which I am
comparing here at the phone line so this
green one assumes that there is no price
ah no transit fee that is so basically
it says okay shall we move your traffic
wherever you feel like it's your
business of course it does very well in
terms of latency it reduces the latency
to bare minimum but it does cost you
some a fair bit in terms of transit fee
the third one which takes into account
both of these and uses this replicator
dynamics kind of stuff what happens to
it it has a fairly low latency and a
fairly no transit fee we add up the two
in the next diagram out here what we
have is the lowest possible latency plus
transit fee that can be achieved with
this particular our load so in other
words the assignment matrix here is the
optimal assignment matrix and the nice
thing about this is it converges fairly
fast so basically in our peer-to-peer
system which we which we implement it we
basically took this assignment decisions
every eight seconds so we took a
decision measured for eight seconds took
a new decision measured for eight
seconds and so on so we measured
basically the per user latency which can
be done in the cluster and fed back we
measured this increase in congestion
which is the change in latency for a
change in load read that back and of
course the price was known the transit
fee was known so we we took these
decisions every eight seconds and we
found in about 4 1 2 3 4 5 so roughly
five or so you know splits or four or
five the number of fast assignment
decisions it converges in fact this has
very good convergence properties so and
as I said earlier the really nice thing
about this is it is
clearly distributed each front end can
take its own decision based on the cost
that it sees so you don't have to solve
the you know centralized optimization
problem each one each front end simply
looks at the latencies which exceeds in
various things it sees the cause and it
knows the congestion because it's fed
that back as well and it takes a
decision at each time where is wrong
which what query shows up now are there
any questions on this kind of assignment
problem here yes okay this is actually a
network simulator simulation on a on a
on a system but it's not actually
deployed we're building a test bed but
that we don't yet have answers one so
what this is this network simulator
simulate the whole protocol stack and
you basically take multiple BitTorrent
clients which are riding on that so each
guy comes he starts up multiple TCP
connections to various different peers
tries to get it back and we be on top of
that we built this so-called tracker
system which allows people to be
assigned to various different domains
and we artificially assign costs of
going to each of these domains so under
that is is where our simulation is so
this is basically we had of the order of
something like I don't know something
like 5,000 clients or so running
simultaneously and the the the the
number of times we ran it in order to
get these plots we ran the whole thing
about 40 times and averaged out and
these are the standard deviation parts
for right
that it does but the thing is each one
has only so many options available so
the question is how many if you give
each front end the option of choosing
everything on the planet of course it
will have lower convergence time if each
front end has a set which is allowed to
choose from italy 0 decrease your
convergence time significantly so that's
the thing so our idea was we don't want
to fetch for example going back to my
bombay vs houston example I don't want
to interact with the peer-to-peer
network in Bombay I would like to
interact with things other things in
Texas and possibly California so the
number of options should not be that
high because in any case my transit fee
will get really large if I interact with
multiple far away options so that was
that was the object was the addition
function that you use when use some of
the latency and the transit fee we are
just using plain addition you can pick
any convex combination it will still
work so we basically thought latency we
measure in dollars transit fee we
measured in dollars and just add them up
together so it's just simple addition
but you can change the constants in
front it will change nothing in the crux
of the matter it could be if the transit
fee itself is a kind of point-to-point
link latency it could also be in there
so it's it's yeah in which case it's all
Layton sees so the next question we
wanted to answer in this space has to do
with admission control the idea is
supposing my clusters are of a certain
fixed capacity whatever it is so and
supposing the load is much higher than
that then there is no assignment on the
planet which will reduce my latency to
our acceptable level so at that point I
need to know what to drop how much can I
admit how much do I you know drop so the
the kind of idea expressed here is that
given a particular load vector I know
how to minimize the total and happiness
of the world but then this total
unhappiness of the world which is
minimized might itself be unacceptable
because they are simply too much load in
which case I better start dropping some
of them and how do i drop so the idea is
that each user has a happiness
associated with himself that can be
thought of supposing for example each
user brings one dollars worth of
happiness so each user pays me a dollar
to be serviced
in which case the rate at which users
arrived as x j the rate at which money i
mean each guy brings one dollar in which
case this log won't exist i will just
have dollars times x so each user brings
one dollar and i try to minimize the
happiness minus unhappiness in a kind of
generalized setting in network
optimization we often consider concave
increasing functions like log the idea
is that the marginal impact of each user
becomes lower and lower like for example
supposing i have a query which consists
of say a request for search results with
images with ads and so on so each each
marginal part of this so i guess the
search results are really important this
big text wha you know a bunch of texts
which I need the images are perhaps
slightly less important then perhaps the
the ads I don't know how important that
I presume they are important but the
question is how so at some point you
have to decide which is more important
than the other so this idea here is that
the log the guy tries to indicate that
you know the marginal impact of the the
increasing number of queries somehow it
is it is important it does increase your
happiness but it doesn't increase it as
much as you know in a linear fashion so
if i have three queries my first query
must be handled the second query perhaps
not so good the third query perhaps even
less so you know that kind of thing so
this will work of course for linear log
and a whole bunch of concave functions
so in general the high-level idea here
is you want to maximize the sum total
happiness or minus the sum total
unhappiness so just to put this in the
right framework the load assignment
problems job is to just minimize total
unhappiness whereas the the utility
maximization problem is to maximize the
total happiness which is the things
which make me feel good and for the
things which make me feel bad so the
question is how am I going to do this
admission control in a simple
distributed fashion so essentially what
this means is each of these front-end
the servers has to decide whether or not
to admit a query and it needs to do so
understanding what the impact of that
query is going to be on the happiness of
the system so really if this is the
total happiness of the system I need to
know
what the impact of changing that X which
is the load which is coming in on the
system in this case it turns out it is a
very simple function we just have to
differentiate with respect to exchange
and what we can do is use something
called a gradient ascent type controller
which essentially says if something is
you know increasing the cost then cut
down on it if something is decreasing
the cost go up and we implemented this
as well on our ns200 thing oops i meant
backwards right and so these three
colors now are completely different
basically there's just one system with
three front ends so there's a blue front
end a red front end or green front end
and they have to take admission
decisions and the objective here is to
show that although they take their
admission this to be decisions in a
distributed fashion it does converge and
the total system happiness also
converges so the idea is that somehow in
a distributed fashion I need to minimize
costs and maximize happiness so that is
what is being done here this
interestingly is about seven percent
from the optimal solution this is again
being done over a huge number of plow of
bittorrent clients with actually in this
case it was three fronted so now so the
key insight from this part of the talk
is basically that it is possible to
align incentives in terms of end-user
latency and cost objectives and it is
possible to do it in a distributed
fashion in a fairly simple way because
everything that you need is measurable
so I don't even need to know this the
capital see the capacity I don't need to
know the load all I need to know is this
marginal impact which can be measured
because I can measure late and see I no
cost I can measure congestion so ongoing
work in the p2p space capacity changes
in a weird fashion because people come
people leave and the question is how do
I incentivize people to stay and serve
other guys and also we're just working
on a very simple test bed which we want
we just want three servers right now in
which which are connected by real
Network links with a certain costs and
want to see if this performs as it
should so that's that's the current
state of that any further questions on
this part of the intopia
the prices daily when the price has
changed over time okay so if the price
is going to change over time it's so
then okay you question oh ok over over
that time scale nothing will change
because the this the convergence of the
innermost loop is of the order of
actually 40 seconds to a minute so if
your prices change of the order of days
no change what's in it later converge
long before you can keep convention so a
further things so what happens when you
actually drop it I mean did in a real
system wouldn't the client didn't just
try again and then like paying that
transit costs and everything if you drop
it well you will drop it again so
basically once you've got this value of
here this is really the supportable
traffic so basically this says once it
converge that if I want to maximize the
happiness of the world this is the
amount of load I can support so any
further load I just have to keep
dropping aren't you go paying that as if
we every time the request is made
because you're not making that decision
to accept it until it arrives at the
server right so you are okay the the the
query itself as far as p2p goes the
query itself is just this tiny thing
saying give me a piece or give me part
of the fine the the transit we actually
has is in the service itself so once
have been accepted shipping the traffic
across was not the we say the cost is a
periphery basis that p IJ is really only
the response yes hey service itself yes
if I decide to accept it otherwise I
just dropped it yeah although in the in
the admission control problem the
admission control is done at the front
end where it originated so you won't
even be queer you won't go across to the
other side if I'm not going to accept it
in the first place so what I'm trying to
get at here is I've got the oops ah this
is a good picture so here is where I
take my admission decision so I won't
actually send the query there if I think
that you know it's going to have a bad
impact on the system so once your drop
here in your rock so you might come back
you might be dropped again
so it's only if I think that you know
the system performance is not going to
be massively impacted by you that I
accept you and in the inner loop I
decide where you are going to be shipped
so the inner loop decides where to move
users around the outer loop decides
whether or not to admit them in the
first place if if they're yeah yeah from
the search problem where there are no
transit causes the additional latency
there you're you know that's being paid
on the drop it takes you that much extra
longer to decide that your queries not
going to be serviced Oh actually no
because it's dropped at source so if
anything it'll add to the latency of the
the query processing as in the front end
this fronted not that okay so in other
words you are dropped where your or your
request originated so there's so when
you when you talk about congestion do
you also consider the decrease in
congestion let's say I go from one to do
so you talk about the back end to
increase in position you would take also
take the decrease in congestion at 10
you sum it up or all the it that's where
the stuff about comparing with the
average happens because I compare so I
the query comes here and I compared the
the latency out here with the average
over all the options available to me so
I am inherently comparing what would
happen if i sent it locally because it's
doing better there is why I send it
there if it were to do better locally I
wouldn't even bother so that so yes
there is a comparison with it it's
implicit there's no explicit comparison
between all right I'm actually running
out of time but rather nevertheless let
me talk about this stuff about Flash
crowns my eyes yes that's out here right
so the idea here is has to do with how
do I share files when they are very very
popular so so the question here is the
following this is a cute little graphic
from Pandu again so panda basically says
if I have a CDN only you can think of it
as big server as I have more and more
people who request the content from the
server there's worse and worse
performance because the latency is bound
to increase in exactly the same convicts
fashion and if I want to service them at
a given quality I either have to
provision a lot which is higher costs or
those guys will you know be unhappy with
me and leave which again means a
decrease revenue so the point is that
you know if you can somehow utilize end
user resources in the drawer you can
think of these end users one possibility
is to think of them as caches which are
closer to the end homes perhaps at the
the pop level or perhaps even at the
cable head ends or you can think of them
as end users like you and me so
basically the point is if I can somehow
utilize the resources lower down in the
hierarchy then more and more users or
more and more viewers of this content
might be better and better performance
simply because there are so many more
copies of it available and you can lower
your cost because you do not have to
unicast all the way from your server so
the question is how are we going to
combine these two methods you know the
server based and the peer-to-peer based
so before we ask how do we combine it we
need to know what kind of load we are
likely to see so this is an analytical
model for describing such things what it
describes really is a viral distribution
viral propagation of interest so I of T
is the number of people who are
interested in a particular piece of
content at some time so this is the
total population of guys who say I want
it now how does that change with time so
this is called the bass diffusion this
is comes from Frank basses work in 1960s
where he was looking at the propagation
of interest in microwave ovens so
there's this new gadget this microwave
oven how do people are requested what
does the rate of change of interest in
such things now this has been used much
earlier in population dynamics in viral
models and so on the idea is the
following so there are so many
interested people right now right and n
is the total number of people so n minus
I are the people who aren't interested
right now if I advertise it reaches a
constant fraction of the people who
aren't interested so k is a constant
which describes my advertising budget
maybe I do television billboards
whatever and it reaches some constant
fraction of the population on the other
hand people tell each other things so
each guy who's not interested might tell
the guy hey this micro oven is so cool
so people who are interested pick
somebody at random and say this is
interesting the probability you find
some
who is not interested is the number not
interested by M so really this captures
to impact of X an advertising effect and
a wire Olympic now very often in things
like YouTube videos for example there
isn't very much of an advertising effect
it's by and large viral so for a file
distribution of this kind we're going to
drop the advertising and look is
essentially only at the viral
propagation although the results which
I'm going to talk about essentially it
hold for that is what the advertising is
better so we can solve the differential
equation explicitly you see this all
over the place this is the number of
interested people and it follows this so
called logistic function the idea is
initially nobody much is interested
people start telling each other at some
point suddenly you have this growth this
is a exponential growth at that point
and then it levels off this is
characteristic of what is called a flash
crowd essentially the idea is at some
point you see sudden increases in
interest of something question is how am
I going to service this interest okay
this is fairly common I work with a guy
called Mike Friedman who runs a content
distribution network on Quora called
coral Syrian it's based on Planet lab
which is this university Network and he
hosted a whole bunch of files on it in
particular tsunami videos you know
basically then the Asian tsunami struck
there were lots of home videos and this
is one example of such you know the
interest growth you know you have this
sudden spike of interest all right so
what am I going to do I can use the CDN
which I call a bank of servers I can use
peer to peer or I can use both and since
I've giving this talk I'm going to
basically show that hybrid has some
phenomenal performance improvements so a
little bit of notation again I of T is
the number of interested people in this
file it is a cumulative demand p of t is
the number of people who possess the
file it's the cumulative service so i
really need to just work with these two
and i will say why this total latency is
the area between those in just a second
so i have a server it has a capacity see
how am i going to kind of understand the
latency experienced so what happens is
this blue curve is the demand I of T and
the green one is the service curve so
what happens initially very few people
ask so the rate of requests is much
lower than the capacity I can serve it
pretty much instantaneous so it goes
along there at some point
the rate of requesting is more than the
capacity available at that point I see
this divergence so the idea here is so
if somebody shows up here so the blue
curve is demand so somebody showed up at
this time he got served at this time so
there is a little delay so as time
progresses since the the arrival rate is
so large somebody who shows up here got
served here shows up here got 17 so this
area between the curves is the latency
or the DNA experience between the the
demand and service so the slope of this
curve is the capacity because the server
is running at capacity at this point it
simply cannot keep up at some point when
the demand slackens off again it catches
up so great so basically we have this
knee when my server just gives up and
eventually it will catch up and the area
between these curves is what I am
interested now I can actually
analytically characterize these exactly
and the area scaling looks like theta N
squared over C don't bother to remember
this for now I will do a comparison at
the end but it's just intuitive to
realize that this looks kind of like a
triangle because the the interest looks
almost like a staircase and the service
looks almost like a straight line so the
area of the triangle roughly gives you
the intuition although we can do an
exact calculation n is the total number
of interested users yeah finally the
service model for peer-to-peer is
essentially the same kind of thing it's
basically if I'm interested but I don't
have it I ask somebody at random and if
they have it i can get it often so
really the thing about this is the
interest and the possession coupled into
each other because only if I'm
interested when I ask only if I have can
I serve so that is the the point this
too can be solved explicitly it gives
you a green curve which looks like that
the interesting thing here is initially
nothing much happens all the way I ask
people do you have it well they don't
have it so nobody has it nothing happens
but once a small number of people have
it it just shoots up exponentially
because each one can serve a whole
number of other guys so basically until
about login time nothing happens and by
2 log in time everything is over so
again we can characterize this area and
the most interesting thing is the CDN
does well initially until the server is
overloaded p2p does well once the
installed user basis
which one some number of people have it
so how do i combine them I first use the
server then I use p2p and we can show
that it gives you a very good
improvement so I'll give you an example
here supposing I used the server
capacity n by log n n remain recollect
is the total number of users who are
eventually interested the server alone
or the CDN can give you a latency of the
order log n P to be alone can give you a
log in the hybrid dog can give you a log
log in which for fairly large values is
essentially constant further I use this
server only for a very short time only
for login time in the hybrid case so the
thesis here is that you must serve for a
short time using your server at which
point your p2p has enough capacity to
really ramp up so these are simulations
in the space so the first one is the
server based again demand-supply this is
the peer-to-peer one you can see there's
significant areas between these curves
essentially the per user average delay
was of the order of 10 seconds here here
it's are of the order of one second
roughly because the area is much lower I
do server first then peer-to-peer and
well then the question others what
happens if I do both simultaneously
answer is not very much because if I
first asked the pier and then ask the
server initially everybody gets served
by the server and so this pink line is
the amount of server usage so initially
everybody is getting from the server but
once you hit a certain value nobody much
you know it just levels off and most
people are getting it using the peers so
again this just reinforces the idea that
an initial boost using a server is what
a peer-to-peer system needs you do that
boost and the peer-to-peer can take over
after that and this is true of Syrians
like Akamai as well where you might want
to do unicast stew the caches at an
initial amount of time after which you
know the D casuals themselves can handle
so I think I should stop here on an take
questions if you are interested in
peer-to-peer streaming I'm around as i
said at sanford till the end of next
week so please do send me an email and
go we can discuss some of this also I've
given Andre the papers on the in this
space and so I will just stop here and
say thank you and take questions thank
you
further here yes I always run the risk
that that user randomly decided so okay
I'm not interested anymore now I'm
walking away alright and now I get even
if just for a short time I get a timeout
so my service is worse as well then if
it would be connected directly alright
so the way we are implementing this or
away we are looking at how it should be
kind of implemented is as a feedback
system where I keep track of my you know
my average rate and the system as a
whole has a target average rate if your
your rate falls below threshold the
server should boost you if your rate is
above the threshold you are fine so you
might get timeouts but this is on a per
chunk basis so in other words there
might be a few chunks which you might
you know have a lower rate for but the
the the server will quickly boost you if
your if your service rate falls below
threshold so essentially that was what i
was talking about here in the case where
people are leaving I've done simulations
on men people lead even there you get
significant improved hear what I was
saying was if people don't leave the
server is not used after a while it just
remains constant so this is the
cumulative so essentially if I looked at
the derivative this would go to you know
zero after point but so the server is
essentially used only initially for this
now if people start leaving again the
number of people who possess which is
this red line would kind of go down in
which case the server will be used again
in the end we did some simulations on
that for various different departure
rates even there you do get significant
improvements in the amount of server
usage so so the server can be
multiplexed over multiple pieces of
content rather than you know having to
serve everybody through the same unicast
so so yes as long as there are peers in
the system you use them if at any point
it falls below that your service is bad
your feedback loop will take care of it
by boosting using the server so that's
how you'd want to combine the two to
ensure quality of service guarantees
further have you looked at the social
aspect of p2p is like the p4p work that
you mentioned right so
like you connect to your social network
and the whole whole fact is that you
incentivize your social network so like
BitTorrent by default as the initial
upload rate which is same for all the
beers it is connected to whereas I i had
done some work in this and people would
be work was more extension of that and
you've seen like up to twenty to thirty
percent decrease in the total download
time when you use this inspector have
you looked at this what we are looking
at actually it is kind of like an
exchange kind of situation like
essentially I have content I have
bandwidth I can sell it to somebody
essentially so we are looking at it as a
distributed market system now we looked
at the prices there as the best thing I
can get now if you're my friend I might
want to subsidize you that is an aspect
which have not considered but I've been
looking at pair up to appear as an
exchange economy where you know you
essentially can exchange your resources
for somebody else's resource so yes some
work but not in the context of friends
giving to other friends were cheaper so
no she'd stayed assistant have you heard
of Big direct yes so yeah there's
there's a lot on how to incentivize
users to stay one way which bitter and
automatically does is you know it just
kind of slows you down at the end so
that you have a little more so i think
these incentives have to be considered
so if you look at it as i said as an
exchange economy many of these
incentives are automatic because i am
staying because i make some currency and
i can use that later on for something
else which i want to download so so
answer is there is a lot of space in
this in this regime but the one question
which automatically arises those who
owns the content so if i am making money
out of the content through kind of
uploading it to somebody else am i
somehow you know kind of drawing or
detracting from the amount which would
be made by the owner of the content so i
don't know some kind of scheme like the
way the google does it there are each
publisher can will get a small amount of
money based on you know visits to their
web page and clicking on ads might be a
feasible solution so it's it's unclear
how to create such exchanges I mean as
in the how the currency mechanism
work but we can show you get significant
improvements if there were such a
currency</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>