<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2010: Lessons Learned from Testability Failures | Coder Coacher - Coaching Coders</title><meta content="GTAC 2010: Lessons Learned from Testability Failures - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2010: Lessons Learned from Testability Failures</b></h2><h5 class="post__date">2010-12-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4CFj5thxYQA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm the first speaker they actually
has these really fine and nice shirt
shame on you previous speakers and the
next figures you should wear it so hmm i
am the the first began opening the
disability session which is really cool
and i'm going to open a with some lesser
some stories that i'm going to tell
about disability famous and disability
challenges that we had at opera and some
lessons that we learned from from those
stories and the the experience of those
things that I'm not to tell so I'm going
to start with a small introduction a
definition of disability then I'm going
to tell you three stories about those
disability failures and challenges that
we had a server side projects tappara
then I'm going to make a kind of main
claim like a main conclusion of of those
experiences and then I'm going to back
up that main claim with three arguments
and those three arguments are going to
refer to the stories that I'm going to
tell after we're just some random advice
and some closing thoughts and then you
can ask questions or share your own
stories or talk about whatever you want
so we finally testability more than
really defining it and going to give you
a couple of characteristics that i think
that that are important for testability
and that i use to get a feeling of how
good a disability in a given project and
the reason i'm going to do that is
because my probably my favorite quote
ever especially when talking about
technical things is this one from
ethanol which is I don't like how rules
at all i think they are all so
i don't really believe in definition so
processes that rigid or strict or
anything like
I like that things are adaptable so
that's why I'm going to give you a sort
of vague notion of what I am thinking
when I when I say testability when I
have disability in mind so those three
things are these funds so you can see
that the first one I have if before I
talk about this we were joking Russell
Tracy where are you low we were joking
earlier that we were having several
talks on Tess ability and probably
everyone would give a definition and
probably all of those definitions will
be completely different it's probably
going to be true but I think my source
of definition is kind of compatible with
everything that is going to be said and
everything that is there has been said
so far so anyway the first
characteristic that I think is important
for project to be testable is having uni
testable code having cold a house with
api's and doesn't have doesn't give you
problems when you want to write unit
tests for it the second characteristic
is I call multiple deployments it's just
that the project makes it easy to make
multiple deployments for multiple
installations of it and all of them work
have the same reliability so the point
of this is that you can recreate the
environments that you need to run your
test cases and you know that when you
run your test cases you are going to
have exactly the same behavior when you
have the production code like everything
live and the third characteristic is
introspection which is the ability of a
system to give you information about
itself about how it's behaving and ways
behaving in ways behaving like internal
Statham so when we started that project
over a link we had a number of
challenges the first one was
coordinating different teams and here
are emphasizing they were different
because not only they were different in
the sense that they were not on one but
there were actually very different teams
this size was different the technology
that they
using was completely different the
mentality was different the way of
solving problems the way of seeing the
project everything all that was
different and that sometimes would give
some problem because sometimes when we
were discussing bird so how to fix
something people who have completely
different opinions about where things
should be fixed or how they should be
fixed we started with three teams we
have the server team the over desktop
team and the opera mini team of rimini
is one of the mobile versions of opera
so of course what happen is that
typically the server team would say this
bug is obviously a client problem and
the client has to stand like proper data
and eastaboga make up to fix it and of
course the client would say now
obviously there has to be stalled on the
server because the client can be
worrying about that kind of thing so
because of those discussions another
challenge that we had was deciding who
had to find a word so when we can't
agree we could go forward this wasn't
completely trivial when we started
because of different teams were
completely independent so we had to find
a way to to make someone kind of the
owner of those decisions and that people
in different teams would actually listen
to a person also we had some problems
with the first specifications that we
had we thought that they had all the
information that we needed but obviously
they were missing a lot of details and
when we were missing those details
different people would make different
interpretations of those things and that
gave some problems we hadn't actually a
really big issue the the first
iterations between minion understood
also because of these details that we
were missing the spec sometimes it
wouldn't be clear how much the attesa
school tests like how much detail they
could test so to speak so that led to
some sometimes having anything the
changes like
people would fix bugs or refactor code
or do whatever change and those changes
would make unintended changes in the
behavior but we would miss some of them
because we didn't have all the detail
that we should have had in other specs
hmm I'm probably the most important
challenge that we had when we thought it
was finding the source of each bug
typically we would have these three
components I had to work together and
when we had a bird it wasn't completely
clear where the bug was so finding that
sauce actually took initially a lot of
time and that was kind of a problem in
the project so what do we do faced with
these challenges and these initial
problems to to kind of fix them so the
first thing we did was documenting the
obvious details and I'm using pulsed air
because it was kind of funny that a
bunch of people thought that it was
obvious and they thought it would it
should obviously be a some of the people
would think that it was obvious but it
was obviously should be B and they
wouldn't even talk about and they would
make components that didn't work well
together and so on so those details that
seemed obvious for a lot of people that
were not that obvious so documenting
them was actually a really important
thing that that helped us a lot in the
long run writing a good server test
suite was also very useful because had
allowed us to when we had a bug and we
were not sure what was the source will
write a test for the server so if the
test failed we will know that it was her
server bird so we we could do TDD
because we had already a test written
for Aiden and so on and if the test
passed then we could rule out the server
from the equation from the problem so we
would know that we had to look for
debugging in the clients and finally
another thing that we did that was
really helpful was adding
a sort of hidden option that only
internal testers at Oprah had access to
that load the traffic on the client so
if you could reproduce a bug you could
enable log traffic that that monitoring
so you would go through those tapes I
would reproduce the bug then you could
go back and see what had happened and X
myung that output similarly we also log
traffic on the server which was useful
when we had say a bird that involved
both desktop and mini so it was much
easier to just log on the server certain
account and regulates of what product of
what insulation you were using we would
have all the information in the same
place the second story that I want to
tell is related to a different project
the my opera project my opera in case
you don't know it's the Opera browser
user community portal or user community
website so these project started is
quite old and it started as simply
support forums so we initially only had
those support forums and then we started
adding some features and modifications
and changes and whatnot and it ended up
becoming more of a kind of social
network so from the initial support
forms that we had we end up having a lot
more features and a much more complex
site that had blogs and albums and the
initial support forums and user groups
and whatnot so as I was saying this is a
quite old product and around four years
ago they were there was this kind of
tipping point where my opera became more
important for the company and we wanted
to to spin like more resources on it but
up to that point and it was already
quite all project four years ago up to
that point the myopia project hadn't had
any testers and that was so that was
really bad because the testing was done
by the developers or the front-end
developers or or whatever but the
problem was after all those years
without having testers the main problem
wasn't that it had this other bird or be
so that thing was reliable that this
other thing wasn't scalable or anything
like that that was a live explain that
the site had the biggest problem of the
side had was that it was really hostile
for testing it was really time consuming
to test and he wasn't reliable to test
one example of that there will be some
features that for whatever random
reasons they wouldn't work on internal
installations so if you were testing of
those features and of course in an
intern insulation and they wouldn't work
you would never be completely sure if
they didn't work because they are not
supposed to work in internal systems or
they didn't work because they call was
actually broken and they were going to
be broken when you went live so I exist
around four years ago when we have this
change what did we do to counter this
situation and make my opera become a
much more maintainable and much more
reliable much better project we did a
bunch of things the most important thing
was removing the testing roblox this
removal of testing roblox of course it
wasn't an easy task that you can just go
to the BTS and say you know file a bug
remove testing roadblocks and assigned
to someone I get fixed in a wake of
course it was much more complicated than
that we we had to go through the side
see the pet peeves of the testers and
see the problems that we had when
testing the site and file a bunch of
relatively small birds that things that
we had to fix and kind of lobby and
cease to make sure that those things got
fixed so when we fixed all the important
testing roblox that allowed a lot of
very good things to happen to the
project we which wouldn't have happened
if we
hadn't removed those roadblocks first so
the first thing that happened was that
we could write automated functional
tests that were reliable that was a very
important thing for us so we reached a
point where we had enough functional
tests that if you broke fairly big
things we would start catching
regressions and at that point we added
continuous integration to the mix so
every time people committed you they
will know to may will be sent and
whatever and people will know if they
have broken something so we started
gaining momentum on that trust on
automated tests and the team being more
like automated tests oriented and so on
so that point what we did was writing
even more functional tests and when we
had enough of these functional tests and
most of the functionality of my Oprah
was somehow covered by tests so we were
quite confident that if you were
breaking something the continuous
intuition system will tell you at this
point we had in a trust in the system
that could start doing heavy refactoring
we didn't really like the code that much
and we wanted to make a lot of changes
but of course it was too dangerous to do
all that before having continuous
integration automated tests functional
tests and so on so once we had removed
components and add in in the place new
more modern component so we rewrote the
code so we had better pizen and
everything was better at that point we
were ready to write unit tests which was
another really important milestone in
the sort of quality management of the
project of course those unit tests were
also in contains integration so most
things that you could break we that way
important we would catch and the first
story but I wanted to tell us again
about Oprah link and it's about how the
opera mini team had
who I big technology technological
change and they had to rewrite the link
support that they had so when a mini
team were implemented the the Lync
client support at that point the open
link project was relatively mature he
was around two years old and we had had
a quite stable API and we didn't have
any like big problems everything was by
reliable and was working really well so
there was this assumption that when they
were implemented just testing from the
end user point of view that would be
enough to have a good implementation
amini but that was actually a big
mistake because all the interesting bugs
were actually at the protocol level at
the low level so a bunch of those bugs
that were they the really interesting
ones would make it look like everything
was working correctly you will get all
the data replicated and you will go to
the second installation you will get the
changes are you were doing from the
first one and so on but those bugs could
add say rest condition so make corner
cases fail or make the service a bit
less reliable or they could simply work
perfectly from the functional point of
view except that they would add more low
to the server we actually had one one
such problem so of course when you are
testing internally you only have like
dozens of clients or something like that
you wouldn't notice those things but
when you suddenly went everything live
and you had millions of people using the
service then you would start saying that
that you have too much salt in the
service and you have problems that you
have to do something to come to it so in
this case will do what do we do what
would kind of solutions or what kind of
things did we go for the first thing was
actually convincing people that it was
important convincing people that that
you I centric testing wasn't really
going to cut it so we needed like
better tools and more low-level tests
and so on and one after go missing
people one of the changes that we could
make was adding some like mobile test
tools for example that some small tool
that would give some internal
information about the antenna status of
of the Lync client or a whatever
Alistair was useful when tracking bugs
another thing that we did was adding the
ability to switch the link server what
the client things that the links of is
basically the URL to the server lots of
people didn't see that much value in
this didn't understand why it was so
important for us but if you think about
it it's really useful to have something
like the ability to change the idea of
the client has that the client has of
what the server is because say that
you're working on the server you have
referred to call you come you have made
whatever changes and you have the
akamaru lease of the server say that
you're going to release in in one or two
weights or something in some staging
server if you can't make mean in this
case connect to that staging server and
do some integration tests you won't be
completely certain that when you go live
when that upcoming version or the server
goes live you are not going to have
problems all the uses that you can that
you can have for this ability to swing
the linked server was related to another
thing that we did that was by useful
that was writing a fake server only for
client testing so this server would more
or less behave like the real link server
only that it wouldn't really storing the
data you wouldn't really implement the
functionality or villain server it would
just check the incoming requests from
clients that they will check that the
requests were correct and they were the
correctly formed and they had all the
elements that he should have and he
didn't have any extra elements that
shouldn't be there and so on and then it
would
canned responses to the client to see
how the clan behave and what happened
with it so after all these experiences
in this and other projects be like main
conclusion that that I got from them
were so the main lesson was that you
have to fight for testability hence the
military themed pictures that have been
showing hmm so in case you're wondering
why I think that you are lucky because I
have some arguments to back up my claim
the first argument is that disability is
the only way to scale scalar and talking
about the size of the project of the
size of the team or having different
teams working together so these refers
these maps to the first story that I
told there was about how the upper link
project started so why do I think that
when you have good taste ability that
allows you to have good tests and this
is always important but it's even more
important when you have when you start
distributing into different components
and different teams and so on that is
even more critical that you have
information about how each component
works so you know that you can tell that
that integration will work correctly it
also allows for tests were needed even
if you haven't had time to write all the
tests say you would like to you can
always when you are closer release and
you don't have time to write all the
tests you like to you can always verify
bug fixes or or do quick checks and so
on if you have a good enough testability
good disability also shows how systems
work which again is something that you
want anyway but it's even more important
when you grow in size of their team and
the components that they have to
interoperate and it also Louis bear be
barrier to entry in two senses in the
person
means you can developers or change
developers and the teams that you have
and in a second sense you can add more
components or more teams working on
other products that have to be part of
that ecosystem that you already have and
it will be much easier to tell for those
new developers of those new teams how
everything is supposed to work and
perhaps most importantly related to the
story that I told having good disability
helps isolating root causes that that
was one of the biggest problem that we
had when when we started so having good
disability allowed us to separate where
the the bugs were so we could tell where
to fix stuff or who to blame which is
more fun so the second argument I have
is that the the more testable very high
quality and these maps to the my opera
story that I told the motifs ability you
have the more tests you are going to
have because the the less energy and
time that you need for each test that
the more you're going to end up having
the more tests you have the less time
that you spend on bugs the less time you
spend tracking them figuring out where
the bugs are the less time you spend
fixing them the less time you spend very
fine that the fixes are correct etc and
the less time you are wasting if you
wish on birds the more time you have for
features and the more time you have for
refactoring and of course refactoring
also leads to better code so in the end
more features maura factoring better
code you have more quality and that
final argument that I have is that
testability doesn't come for free and
these maps to the second overlaying
story that I told about how Oprah meany
the opera mini team had to rewrite the
link support
I have seen a lot of people that either
really believe or they behave as if they
believed that if you can use a product
you can test it that's wrong not
necessarily you can test a piece of
software effectively because just
because you can use it doesn't mean that
you have repeatability that there you
can easily set up test environments and
test cases and and be sure that if that
works everything is going to work in
production for example just because you
can use it it doesn't mean that it's
going to be easy to alternate and even
though automation you may not consider
it a requirement for good testing the
bigger your project ace and the bigger
your team is and the more complexity
that their your project has of course in
more important than automation the more
important automation becomes again just
because you can use a product or a
project doesn't mean that it's easy to
make multiple deployments of it and that
those deployments are reliable and just
because you can use it doesn't mean that
the project will give you information
about itself so you will have all the
information you need to track down bugs
to to verify how things work etc so
those were my arguments and now some
closing thoughts so I'm going to start
with some advice and then some final
conclusions and then you can ask
whatever you want so probably the most
important advice that I can give you is
thing before you cross especially here
in India that's really important think
I'll look before you cross don't forget
that and some less important advice that
I have for you think about what takes
time if you have processes or task or
things that you have to do in your your
project particularly in testing that
take too much time and then Here I am
defining too much time as enough time
that people
do them as often as they should be doing
them then you probably have a problem
and you should look into it and you
should try to make those things quick
similarly think about what you don't
know if there is some information that
you need in order to debug something to
work on something to fix something to
figure out whatever that you need if you
don't have access to information try to
create a tool or an option or whatever
is needed to get access to that
information so you have everything you
need to do your work third think of the
children and think of the developers
because developers are also affected by
these things that take time or but those
things that they don't know so
developers these things are not only for
testers and developers have probably
also good source of these pet peeves of
these problems that you should be fixing
when you're talking about disability
done like in your project when if you
want to improve your disability don't
just talk about it like very vaguely
like we have we have to have twenty
percent more testability or something
like that because that doesn't solve any
problem does not contrast constructive
and people are just not going to listen
to you because that doesn't mean
anything for anyone so try to come up
with concrete things that we improve
testability in your project and if you
can extra points if you can implement
those changes because you won't be like
annoying people and you will be able to
show how usually relatively small
changes can have big impacts in the long
run and especially for those people that
might be skeptical about the whole
testability thing and why these things
are so important stuff show them the
results make sure that when you fix on
testability problems and surf show them
that now this and that thing is much
easier always much faster and evenly
might be
too much much faster for them when they
are trying to fix bugs I'm some final
conclusions you should think of
anticipate a as a roadblock is something
that it's there in the middle and it's
not going to allow you to go as fast as
you could be going so that is something
that you should try to get rid of so you
can get sort of maximum speed and here
I'm talking about big project I'm not
talking about testing I'm talking about
the whole thing testing and development
so don't think of it as testing is going
to be slower think of it as my project
is going slower tester heroics in the
sense when something that should take
ten minutes takes an hour instead so
testers work six times as much because
things are just low and stuff that is
bad unless you don't have any other
choice because you have a really close
release or something like that avoid
their behavior both those kind of
heroics and try to you know bitch and
moan about the proper tools that you
have and and whatever and try to change
that thing because it's going to have a
big impact in the long run and if you
have changes that you can make in your
project hand in your code that helps
your testers or helps your testing and
your debugging and stuff don't be scared
of adding those things into your project
it's not a hack it's not bad it's not
like to be frowned upon or anything like
that it's good it's going to help your
project so just at it don't clutter the
end you see interface or something like
that you are those things but it's
completely ok to add parameters for
example command line options or hidden
button so hidden whatever that your
testers will find useful or anyone that
is testing including you your developers
will find useful and kind of wrapping up
kind of final conclusion opacities the
enemy the less transparent your system
is the
more problems you are going to have with
it so if you want to ask anything I'll
make some comments your own stories
don't ask these questions please I don't
know them and don't know the answer I
meanwhile you can see the credits of the
awesome pictures i use my presentation
so anyone
hello I just don't understand are there
any established metrics you know by
which you know we could have lip it
again okay are there Lee established
metrics okay matrix matrix okay which
could help us you know to have an idea
or with regard to the effectiveness or
efficiency of a particular testability
of a project right I know like with a
set of parameters okay you know which
could help us to deliver the testability
of a project are in a product right
would be resources and moments and
number of those things but can we think
of any metrics okay which could actually
help us to have an assessment enough
arif as an assessment to see how good
this the stability of this project yeah
I think something as abstract
accessibility is really hard to to have
like good metrics full so i guess the
James Whittaker was talking about making
up the number and make people challenge
you that you're wrong and and so on I
guess it is so hard to to measure that
you have to to kind of agree like you
know have the opinions of the team and
stuff but i'm not sure if there is
anything that that is subjective that
you can measure that it's easy to its
really i don't use anything like that if
hi so with those examples it seems like
these testability measures have been
sort of retrofitted afterwards I wonder
whether the the success of these
testability improvements have helped
people realize that these should be
added earlier on whether you now see
people adding these sorts of hooks and
mocks and things earlier in the sort of
implementation phase so you asking sorry
with a question so after these examples
where the testability has been sort of
retrofitted in as this encourage people
to add testability earlier or you still
retrofitting testability to new products
I think so I think so in my upper case
in particular I think there was a really
big shift after we added all these tools
because people realize her that they
were really helpful and we also had a
the team changed also so the the sort of
the new team was also more receptive to
those things but I think that showing
people when people see how useful those
things are I think yeah I think that
encourages them for example we used to
have some people that were still kind of
skeptical about the whole automated
testing because you know sometimes thing
will break if you change some link text
or something like that I like we had
some salacious like that but but over
time those also those people when when
they broke things in concerns
integration of stuff they started paying
attention and they started thinking that
it was like a really big deal on
something important and something that
that we had to work on in the link
project I would say yes but I'm not so
sure we had a moral unit tests and
things written by developers after after
like the initial efforts of good server
test suite and so on so I would say that
that idea that we maybe not as much as i
would have liked but yeah we
think people believed more in this thing
and incorrectly we're encouraged to to
write more testable code or pay more
attention to those things I think so
esteban have you found it difficult to
sell these ideas to product or project
management in the sense of how much time
are you spending in a sprint relative to
feature development and if you have
found this difficulty what would you say
has been the way that you've gotten
around it or how you've sold these ideas
I think the best way to sell probably
any idea but in particular something as
abstract as this thing that is sort of
long-term and people have to wait a bit
to in order to see results it's one of
the things that I mentioned in one of
the slides that was showing the results
usually people if they are no use to
these things and they are probably not
used to these things that they haven't
thought of them themselves they're going
to be skeptical too like where's that
they don't understand or or something
that requires a lot of work and then
they are promised that in the long run
things are going to be okay so that was
one of the reasons why in my Oprah for
example in particular I started writing
that functional test suite those
automated functional tests and I started
writing them just by myself and you know
I didn't involve like anyone else in the
team anything like that because I wanted
to work on them myself until I thought
that they were useful then showing to
people so people could see you know that
now I'm going to start with this
testability thing and maybe in two
months they will see something but I
wanted them to see directly some results
something that they could sort of relate
to and they could see a concrete good
things that that they could get out of
it okay
hello okay got it it's working I don't
know so now I'm actually curious how do
you introduce a test ability for her
legacy code and you know that's built
with no idea that someone in the future
will come and talk about what
suggestibility into their company so the
question is how do you go about
disability with legacy code yeah what do
you do my opera was a perfect completely
perfect example of that because as I
said like the fishiest I wasn't such an
important project it wasn't that big and
it wasn't so so like critical for the
company's that back then absolutely
nothing of like disability or testing or
all those kind of things well like photo
for design for or anything like that so
i think that the approach that were the
best was what we did that was we can
really change the code to make it
testable than ungood and whatever
because we will break a lot of stuff and
we want to know that we're breaking a
lot of stuff so that's why I wanted to
start with a functional side so we could
by changing very little code to make it
more to make it work more like the
production environments that was like
enough very small changes and I guess I
writing those functional tests so we
have we started building this kind of
safety net so once we have that we could
start like really rewriting code and
remove the legacy parts that we didn't
light and and make like put new modern
modules hmm as part of the code base so
I think in general that that's the best
thing that you can do if you legacy call
you don't trust it don't cheat a code or
change as little as you can start with
functional testing then you'll see when
when you feel comfortable enough to
start changing stuff anyone else
thank you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>