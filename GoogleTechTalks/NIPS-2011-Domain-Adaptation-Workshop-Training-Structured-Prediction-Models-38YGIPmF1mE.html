<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Domain Adaptation Workshop: Training Structured Prediction Models | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Domain Adaptation Workshop: Training Structured Prediction Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Domain Adaptation Workshop: Training Structured Prediction Models</b></h2><h5 class="post__date">2012-02-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/38YGIPmF1mE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">but yeah it's our exhibit so we'll look
at practical setting and natural
language processing and look at
different structure prediction paths and
already kinda staged very good job just
do put it in terms of supervised
learning one is a mismatch domain where
our target distribution is different and
as an example parsing our training day
that is typically news water data and
they're very few questions but when you
try to apply parsers to web data there
are a lot of questions and the Purser's
don't know how to do with us
and as Mary said meditating is expensive
so for example for is syntactic parsing
it's about the dollar for worked to
annotate with these structures because
you need English that have been
specifically trained
so it's unexpected that's labeled
training data for all possible mains and
those change all the time is love but it
is realistic to get partial annotations
so even though you might not have much
diversity background if I ask you what
is the main worry about this question
you probably will be able to point out
that
and so we want to leverage those type of
partial imitations and in the other
learning is that the objective function
that we optimize it's oftentimes not
exactly what we share so this might be
reduced weak objective function that we
composed according to the model
structure or because we're using a model
in a pipeline where we don't really care
about the exact performance on a
thirty-two instead but be careful
downstream application specific
performance and so and that's well
actually this work the rest from we're
working with machine translation team
and they take sentences parse them and
then change the word order before
translating them so this is a very
simple way of incorporating syntax and
finishing translation and typically
results in better final translations and
so it doesn't really matter exactly what
the parse tree month as long as we get
the right the ordering we're we'll be
fine and so we would like to feedback
and either from the translation or from
just murdering into the parsed
information about what the structure was
that what helped us with the task
so
these extrinsic loss functions that we
will capture either of those two two
problems and we'll present a framework
that that can handle multiple lost
consciousness we will just iterate over
those in Rome from in fashion and
there's a little bit of theory but I'll
focus on yelling most internet very cool
effects so we'll use structure for step
front as our learner because it's so
simple right familiar with it
just the one thing to emphasize it is
we're looking at structure based on
position here this heart match will
involve dynamic programming and
searching over a larger dimensions
patient so it might actually be quite
involved and so we iterate over examples
and we have where our intrinsic data our
label data will have the target output
as they will fight an englis and so will
iterate over those and adapt our
parameters but then we'll consider
scenario where we have additional
experience that has no overlap
necessarily with
with intrinsic data and where the output
space Y prime is different so it can
over these partial partially anything on
trees or over this reordering data or
machine translation of data before these
sentences and we will just assume that
we have a loss function that given the
model prediction can't compute the loss
relative to the target y prime that we
had for that today
and so he's to taste different but we
can still iterate over the intrinsic
Reena in get a warm start except to a
good point
and then we'll start mixing in the
extrinsic data and according to some
scheme switch between the two and we can
include multiple loss functions in the
trees so these certain functions as I
said will be targeted towards the task
so to make this a little bit more
explicit when we go over the extrinsic
data we were so if you assume a parsing
task what we'll do is we'll produce a
list of possible candidates and this
list can be sampled just from the
carcass base or in practice what tends
to work better
it's a rank of K best list of outputs
and then we can compute the loss
relative to Y prime is partially
annotated there are a specific notation
and then if the loss for something
further down the list is smaller than
the loss for the top ranked item we'll
take a perceptron step in that direction
it's a very simple recipe style approach
that works works well in practice and we
can show that as long as it's useful one
of the items in this K best
approximation forces us to do an update
step and the data is separable
we will converge to a separate
so
to make basically because we were
already best in common output our model
we don't need to post any restrictions
on the last function and we can use this
for commercialization amazing precision
C parsers
that measure every specific whatever it
lasts you you might care about and it's
not miss the last function it's easy to
evaluate this would be happy to talk
more about the related work session
sorry no no okay I can think about it
one thumb and difference especially as I
just work our customization it's also
somewhat be there but we don't make
assumptions
so let's go into the critical part the
first task is to may not official and as
I said our training data is newswire and
actually in the 40,000 sentences that we
have a string in it they're less than
one percent questions and so the cards
will test on a question feedback I've
learned not only evaluating any
questions and we'll consider the
scenario where we have the main part but
the question has been given to us and
because we had for that bill feedback we
actually have to fool trees also
compared to using the polarization so
the only thing that will get is
percentages regime in participation
we're just a standard in domain
performance is about 96 percent when we
go to this question domain we dropped
below ninety percent and this is in part
due to lots of unknown words that we
haven't seen before but also because the
word are changed significantly from
our input demand and when we get this
partial supervision that want to purpose
and iterate doing one step one percent
of maple changing it up in one percent
one percent step on done partially on
the theta beta we can bump this up to 92
percent roughly closing halfway to get
you to if we had before notation in
parsing the numbers are actually more
striking so how well are we able to
determine what the main work of
synthesis and it drops down to less than
50% basically the person has no idea how
to handle questions at the beginning but
if we give it about two thousand
examples of sentences where we have
their local remember how we improve the
way that one to 83 percent 91 was
like fornications but we not only
improve the rooted one because we go
down the list of our given trees we
actually end up picking very coherent
trees and improve on all other metric
services unlabeled accuracy or for each
dependency art and this is a label of
Hypersport livery so we got very
substantially through this no the second
task is going to be task adaptation
where we want to do the socialization
example we won't look at the final
output because that seems to be a little
bit too far linked from the parser well
look at there are these reorderings and
those are fairly easy to generate for
profiling those speakers and we have a
data set with about 5,000 of them that
we use for what is task and so yeah we
asked people to rank them
for that hospital for taking initiative
Japanese English is a subject for object
language that Japanese is a subject
object word languages of the birth
typically needs to move again and
there's some milk of remains that we
join in Islam and who uses a lost
country something based on this
reordering sport which looks at how many
chunks are aligned so looking at how
well we can match that and also this
song for any score we see that our
baseline model blue one includes quite
significantly and we show how doing more
and more than these updates in that
direction
improve at some point it starts dropping
or changing and then you might say well
how much do we care what you're in
scores only tell evaluation sensation
output and we have experiments with blue
scores and human relations why we show
that
there's more than a few point inclusions
and they're strong so even though and
looking at the parse trees and
evaluating them on a parsing benchmark
they tend to either stay flat or
actually get worse on a personal network
you can smash them a very different
domain for this class so or I finish one
one truck and so basically and they'll
be see down together and we've created a
new ahartry Bank which is not big enough
for training but it should be interest
and interesting data set for testing so
we have five points for each of those
domains get to tell the label simple
systems and then more than a hundred
thousand combinations that exists that
can be used to do unsupervised the
patient and we're organizing a shared
task so if there are
participating good we talked about
amended Lhasa structured perception it's
a simple way to utilize partially
annotated data and adapt for specific
tasks or so this is kind of I guess it's
you have to spot a little bit but one
thing that you know hearing the morning
officer than doing this one and it seems
like you know you also mention so every
like balance like the line with a very
showy like these these basically say
well either your loss is the same or in
the case where the distributions will
share support like I have a new word
they don't really know kind of where
that word should attach in English
sentence what
you know we kind of don't really know
what to do right you might get lucky
with some of these heuristics but in
general like we don't have any great
ideas and then I mean I guess like do
you have any have any feeling about
whether the authentic laws were these
kind of different laws of function and
style feedback how that applies to
particular types of features or rather
certain types of clauses that you feel
like oh I can I can correct this mistake
but you know these other mistake these
other types of things I already knew
well from my source domain that I don't
need to focus on these these particular
I guess imagine emissions in cars
yeah well I mean I think it's it's hard
to don't for me at least to speak on a
general kind of annoy minutes depends a
lot of occupation anything that kind of
one of the difference the main
difference that I see here this is a
much easier to set up because we have to
eat like we don't have to just talk
about distributions we have explicit
feedback but this is you didn't belong
here and we don't know what the best
output is that we're getting like
reinforcement feedback okay this one is
better we should try to predict more
like these and so if she's annotation
targets exactly the Harris that matter
you're a guy I kind of here pending a
problem off to the human to solve for
without event to the machine
and that makes it very makes the task
easier and in a practical setting
especially school where you can get user
feedback it seems good way to kind of
site stuff
this is really nice I wonder about your
experimental results or network of
questions yeah it's possible to you wish
to take sentences converted we will take
question from the other two questions
then goddess yeah what would you get
who's the based on the meat so I don't
have thanks lack numbers but that's the
first approach we took before we knew
about the specialty - whatever
declarative sentences and try to
tournament questions having proper
questions that's better because the
heuristics that you write to convert
them the question see don't always
result in perfect questions I guess we
should go back and compare the like we
have and will have approach we
okay understand sky</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>