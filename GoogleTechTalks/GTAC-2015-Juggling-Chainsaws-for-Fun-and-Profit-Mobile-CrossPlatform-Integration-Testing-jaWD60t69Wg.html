<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2015: Juggling Chainsaws for Fun and Profit: Mobile Cross-Platform Integration Testing | Coder Coacher - Coaching Coders</title><meta content="GTAC 2015: Juggling Chainsaws for Fun and Profit: Mobile Cross-Platform Integration Testing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2015: Juggling Chainsaws for Fun and Profit: Mobile Cross-Platform Integration Testing</b></h2><h5 class="post__date">2015-11-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jaWD60t69Wg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">So our next few talks are going to be pretty
quick.
So to get us started with our first lightning
talk of the day, it's Dan Giovannelli from
Google to talk about juggling chainsaws.
&amp;gt;&amp;gt;Dan Giovannelli: Hey.
So a robot is a tough act to follow, but I
will see what I can do.
Someone had to.
[ Laughter ]
My name is Dan.
I work at Google in New York as a software
engineer in tools and infrastructure.
Talk is called &quot;Juggling Chainsaws for Fun
and Profit,&quot; and I'm here to tell you some
of the mistakes my team made building cross-platform
mobile test systems.
Like I said, my name is Dan Giovannelli.
I primarily work on mobile, and my experience
is specifically in iOS.
On a team called the mobile display ads testing
infrastructure team, which from the name you
might guess means we build testing infrastructure
for mobile display ads.
A little more specifically, our customers
are other engineers who are working on a whole
bunch of different ad products -- display
ad products specifically all serving to mobile
devices.
So as a rule, if it has a screen and no keyboard,
that's us, phone, table, watch, all that kind
of stuff.
This, basically, means we have to build testing
infrastructure for a bunch of different ad
products serving through several different
native SDKs on both Android and iOS.
So when our team first got started about two
years ago, we were sort of facing this really
thorny problem, which is we had these engineers
building very, very complicated back-end stacks
with a lot of different moving parts.
And they needed to be able to test their changes
to those stacks on mobile and make sure that
they weren't breaking anything on mobile devices.
Unfortunately, they weren't mobile devs.
They were usually back-end devs so, Java,
C++.
They're not mobile experts, and they don't
really have the time or bandwidth to become
mobile experts.
That was our job.
And we had to avoid getting caught up in what
we called the matrix, which Juergen talked
about a little bit in the keynote, which is
basically this idea that you have multiple
operating systems, multiple SDK versions,
multiple device types.
And you have this sort of a combinatoric explosion
of things you have to test that gets really
out of hand really, really quickly.
So what is our plan?
We decided pretty early on that we were going
to let our users write tests in this sort
of non-mobile domain-specific language.
It looked a lot like JSON.
And you can see an example of one of those
tests up there.
Basically, they're data-driven tests.
They lay out the most important parts of a
test, what ad to load, what sizes to load
it on, what to do with that ad.
And then we under the hood ran those on mobile.
We, basically, had our own pipeline to convert
those to something that worked on mobile and
run them so our customers didn't have to worry
about that.
We decided also pretty early on to run on
simulator.
Just that makes the whole matrix thing a lot
easier to deal with because you don't have
to worry about having all these different
devices.
And devices are hard.
They crash.
They break.
They catch on fire sometimes.
That actually happened once.
So we figured simulators was sort of the easier
way to go.
We wanted to make our results just really
easy to understand and have clear action so
rather than saying, &quot;Something's broken,&quot;
make it more &quot;This is broken.
This is how you should fix it.&quot;
And as sort of a parallel request, our customers
wanted a manual testing app.
So, basically, an app that they could just
in front of them as part of their development
cycle load up an ad, interact with it, see
what happens in addition to the automated
framework.
So how do we actually build this thing?
Like I said, we had custom Python scripts
to take those DSL tests and both what we refer
to as unfolding them which was taking one
test and turning it into a bunch of tests
across operating systems and all that and
convert it into intermediate formats that
played nicely on mobile.
So JSON for Android and plist which is a binary
XML format that Apple and Apple alone uses.
Surprise, surprise.
[ Laughter ]
We had custom Android and iOS apps that allowed
for manual testing, like I said before, the
apps that could open an ad and run through
them.
And then those same apps were, basically,
driven by those tests for automation testing.
Those ran on simulator and they were driven
by Espresso on Android and then internal iOS
functional testing framework for iOS.
We had an AppEngine back-end that we used
for storing golden images so for testing rendering
you could compare an ad to a known good image.
And it also stored some additional data for
tests.
We had internal Google systems for actually
spinning up the simulators and interacting
with them and getting data from them.
But there was really nothing there that you
couldn't build at home from open source parts.
We just were lazy and used some preexisting
stuff for it, internal preexisting stuff,
not open source preexisting stuff.
And we integrated with the existing Google
tooling for building binaries and ruing tests.
So, basically, we could integrate very nicely
into our users' development workflow.
The same systems that they were used to for
running unit tests and integration tests,
they could just kick off these tests and get
results back the way they were used to.
There's a little bit of a diagram there that
I won't spend too much time on.
But, basically, the stuff that's in the clouds
is existing Google infrastructure.
So we were able to kick off our system from
that and then sort of hook back into it.
So the braggy part, what went right?
We put a lot of thought into how to avoid
the matrix, and it pretty much worked between
running on simulator and using that domain-specific
language.
We, our team, had to worry about the matrix
a lot, all the time.
But it was completely opaque to our users.
They did not have to care about how they were
going to run across all these different devices
which was a major goal of ours.
And on a similar note, our back-end developer
customers really appreciated that they didn't
have to know anything about mobile.
They could write these JSON-y tests, kick
them off in the systems they knew, get back
results in the systems they knew.
They really liked that.
And it was a useful system.
We had great adoption for both the manual
and the automation components.
We had a lot of metrics, and they said nice
things.
Now the fun part.
What did we mess up?
We thought a lot about these back-end developers
who were going to be using our system.
What we didn't think about was the developers
who were also working -- who were actually
working on the native SDKs that we were building
into our apps would also want these kinds
of tests.
They wound up using the system, too, but though
wanted less abstraction.
They wanted to be able to write tests natively,
and they wanted to be able kick them off from
their IDEs.
Unfortunately, because of how our system was
architected, we couldn't really give them
that.
We couldn't integrate it with their development
cycle, the native development cycle.
And that became more of a problem as time
went on.
We also had a whole bunch of systems which
should have been really lightweight that were
sort of stuck together because as we're developing,
we had, you know, the manual versus the automation.
We had all these different SDKs.
And we sort of kept stapling them onto the
system until it became this kind of Frankensystem
where whenever one thing broke, totally unrelated
things would also break because they were
closely tethered.
And that caused unnecessary flakiness.
We also didn't think through platform differences.
We sort of thought from the beginning we'd
build things for Android, we'd build things
for iOS, we'd provide them both.
It didn't really work like that because they're
very different platforms.
And we wound up with things that were a little
harder to implement on one platform or things
that were completely impossible to implement
on another platform.
And that sort of led to this problem where
we had inconsistent feature sets, and you
had a test that could only run on Android
or only run on iOS.
And that added cognitive load for our users,
who now had to worry about where their tests
could run, which sort of defeated the purpose
of part of the entire system.
So what do I wish I'd known from the get-go?
What did we learn from this?
Subdivide your systems.
It's really easy to think, oh, I'm building
something that's sort of built on top of something
else, so I'll just put it in there.
And that gets you velocity in the short term,
but it causes all sorts of problems in the
long term.
If you think two things should be in different
codebases, you're probably right.
Ideally, you know, you want to have sort of
logically divided systems sharing common infrastructure.
If any of you are thinking microservices,
that's -- yeah.
The whole idea of the God object anti-pattern
from object-oriented programming, where you
have one object that does everything and nothing,
that applies to systems, too.
You know, it's generally just good software
engineering practice.
But it is important to keep in mind.
You know, the more things change, the more
they stay the same.
Know who your customers are and have a plan
to give them what they want.
And notice I did not say know who you think
your customers are and what you think they
want, because you will be wrong.
Take some time to go out there, talk to people
who might be using your systems, get a sense
of who they are and how to build what they
want.
And the answer may be, there are going to
be people who want to use a given system where
it's really not the right fit.
And that's a great thing to know at the beginning,
because you can say, this is not the system
for you, we'll build you something next, rather
than them trying to use it and not being able
to do what they want and getting frustrated.
Don't ignore platform details.
For any given feature, have at least a pretty
clear idea of how you're going to implement
it on whatever platforms you're using.
And if it's going to be much harder on one
than the other, have an idea of how to mitigate
that.
Maybe that means putting more development
resources behind the harder one or delaying
release of one even after it's finished until
the other is finished.
Just have some idea of how you're going to
address the fact that the platforms are different.
Simulators are great in automation.
They let you do all sorts of great stuff with
sharding and parallelism.
You don't have to worry about them catching
on fire.
You're always going to need some amount of
real device testing.
There are bugs that will only pop up on real
devices.
There are features that you can only test
on real devices.
So things like NFC or camera.
But save that for manual testing.
Anything that you can run on simulator for
automation, you should run on simulator, in
our experience.
So that's it.
Got four minutes left.
If there are any questions, I'll take those.
&amp;gt;&amp;gt;Yvette Nameth: Can we switch to top view.
iOS automation is painful for everyone.
Are you guys going to open source any of these
internal tools?
&amp;gt;&amp;gt;Dan Giovannelli: Some of them have already
been open sourced.
There's a great tool called Martian that we
use for proxying requests for verifying network
traffic and mocking out network responses.
The people who built that and open sourced
it are actually here at GTAC.
So if you see Brahma Ghosh, say hi.
Some of the tools we want to open source but
can't for various reasons.
And some we want to open source but it's a
slow process.
And it's in process, but when that will happen,
there are a lot of variables as to when they'll
come out.
&amp;gt;&amp;gt;Yvette Nameth: So what simulator and test
tool set do you use to avoid a big lab of
matrix of devices, OS, browser combos?
&amp;gt;&amp;gt;Dan Giovannelli: For the simulator, we're
just using, like I said, it's mostly open
source.
So on Android, we're using the Android emulator.
There are a few sort of tweaks to it that
we've made.
But it's the same emulator that you all know
and probably hate.
Same on iOS.
It's the standard Xcode iOS simulator.
We're using the just SIM control tools and
the latest Xcode versions to actually drive
those.
Before those were available, there were certain
open source tools, libimobiledevice that you
could use for similar effect.
And then it's just some internal tooling to
sort of bring those together nicely.
About the like I said, it's nothing that you
couldn't build at home.
&amp;gt;&amp;gt;Yvette Nameth: I hope you can understand
this question.
How do you choose the devices to test for
preventing being sucked into the matrix?
&amp;gt;&amp;gt;Dan Giovannelli: Usage metrics are really
important.
If you know that .012% of your users are running,
I don't know, Jellybean on a rock somewhere,
then that's probably not worth testing.
[ Laughter ]
Really, it's -- this is a -- it's a really
good question.
And it sort of falls into the same bucket
as don't assume that your customers are who
you think they are.
Don't assume the devices are what you think
they are, because you will be wrong.
Maybe 60% of your customers are using Jellybean
on a rock.
Really, just having good metrics around what
devices your code is running on is -- and
operating systems and all that, is really
important for just knowing what to test on.
And just as importantly, what not to test
on.
&amp;gt;&amp;gt;Yvette Nameth: And, Dan, since you talk
really fast, you get one more.
Why did you choose to implement your own language
rather than adding a library for something
like Python?
&amp;gt;&amp;gt;Dan Giovannelli: So it actually is Python.
[ Laughter ]
Those are Python dictionaries.
And we use Python scripts to turn them into
non-Python.
But, basically, it's not so much that we declared
our own language as much as you can sort of
think of it as we declared our own schema.
We said these are the elements of data that
a test needs.
These are the elements of a data that a test
can have but doesn't necessarily need.
You structure that as a Python dictionary,
and then we do the rest.
&amp;gt;&amp;gt;Yvette Nameth: Thank you, Dan.
&amp;gt;&amp;gt;Dan Giovannelli: Thanks.
[ Applause ]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>