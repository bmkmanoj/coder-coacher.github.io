<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MySQL Community Meetup: MySQL Proxies | Coder Coacher - Coaching Coders</title><meta content="MySQL Community Meetup: MySQL Proxies - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MySQL Community Meetup: MySQL Proxies</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/74FZOP8QucY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi I'm Alan I just started my school
this week yes no i've been using mysql
since 99 similar to 323 version it
worked I've built large medical
installations at livejournal I'm going
to connect by online com and now six
apart so why journal again I've been
recently involved with some of this
prophecy stop selling another give a
pretty I'm going to try to give a quick
introduction on this to say what it is
and so forth if we have any questions we
can go from there and then after that
I'm going to defer to Eric for the Q&amp;amp;A
Yeah right so this supposed to work now
of how far should I hold it that's good
okay I'll try to not move so who here is
using a proxy for their database are you
using my scope Roxy what are you using
internal okay there's lots of internal
proxies that people have made over the
years and even more so recently who here
is heard of minus QL proxy do you
understand what it is and we flavors of
it all seeing it not so much
so my school proxies it's pretty cool
it's a young Connecticut just I guess
hacked this thing together over a
weekend in February where he he uses the
language the scripting language Lua for
other projects lady lighttpd and decided
to write a product right another proxy
to do I guess the same the same lowest
off for MySQL it uses a the same same
generic architecture as lady if you ever
used that or you're familiar with that
so what this allows you to do now is you
can now do any kind of crazy magic trick
you've ever dreamed of or had a
nightmare of that you could ever
possibly want to do with MySQL but
couldn't ever get your head around the
source code the sits in between like
this the nice tequila operator here it
sits in between your MySQL client and
your mysql server and it speaks mysql so
you can use any client with it you're
ready pc client your your java connector
client your c client or COI client
wherever you want it works and and then
the proxy talks back to your mysql
server pretending to be a client that's
about where the simplicity stops and
starts getting kind of interesting so
i'm going to show just to illustrate
that what this is so basically it's it's
the proxies is written see the base
protocol handler all that wonderful
stuff is written in c and then this is a
scripting language which is luis very
simple syntax is kind of similar to
python if use that before it's a very
very fast very small very low memory
usage scripting language and honestly i
think it's a lot of fun so i'm going to
show a quick demo if i can make this
work there will be a small miracle
so I'm fired up here I don't know if you
can see this very well the official
mysql proxy with a tutorial louis script
now this script which I cat it above
this thing like an idiot I don't need to
see that far it's pretty small you can
see just some basics and here function
read query packet if packet bite looks
like a command quarry then crap
something out to the print just standard
out and then down at the bottom you see
the required result so when the quiet
where the query is coming in to my to
the MySQL proxy it'll fire off both of
these functions at some point during the
execution when the car is red and when
the query results red and then a little
print a nice little time out if you ever
use MySQL slow log presently you'll
notice that you only have a resolution
about one second so if you have a query
that takes 300 milliseconds that still
sucks but you'll never know something
like a proxy in its most basic usage is
very useful if you want to say put this
in front of your development environment
and say I guess track down and yell at
any developer writes a query that takes
longer than 30 30 milliseconds 100
millisecond 300 milliseconds you can
profile large query small queries in the
more advanced usages you can run other
commands around this like if you wanted
to look at the state of server before
the quarry runs and then the state of
the server after the quarry ones you can
add this all in very simple Lua
scripting
I'm gonna connect to the proxy here and
you can see down on the corner printed
out some crap query time show databases
when you connect with the the MySQL CLI
client it'll run a few queries just so
it can bring you to this nice prompt you
see a show databases or show tables and
I select version comment you can see
those took an amount of time if my
laptop weren't underclocked right now
it'd probably be a little quicker so I
could run queries are all day but
something a little more interesting is
this window here I'm not talking to the
proxy but not to the minus QL server so
the proxy itself has an important port
in min protocol you can run
administrative commands you can change
the configuration let's see if I can
remember the query here
so this proxy config table does not
exist in my mysql server that's running
here in fact the the server version that
popped up is not my mysql server that's
that's 50 45 this one is reporting 5 120
agent to enterprise minus QL something
that's that's not me so with this you
see that the proxy is something you can
you can modify you can play with you can
write administrative tools with you
could have a database without ever
having the database if you were were so
inclined and I don't know any other I
didn't have them time to prepare any
other cool command from there or any
other any other demo so I'm just the
other thing I'm Michelle real quick is I
wrote my own proxy this is a long story
which I won't actually tell but when the
my scope Roxy was first developed it was
not free was not open source it was not
available you couldn't download it
unless you were to enterprise customer
so I talked with Yan and he kind of
encouraged me to actually give a shot at
writing my own so I tried that
now you're utilizing
well I want to I want to use both I mean
mine is actually when when the official
minus no proxy data eventually come out
with the source code there are some very
I guess subtle architectural differences
mine mine actually scale better so if
you wanted to say if you wanted to put a
proxy in front of your giant massive
production database and then say run
sending pull your connections down so
you only have as many back in
connections as you need but you could
have 10,000 clients connected to Mize go
but the only back in coin back in
connections that are ever used or ones
that are actually running queries so if
you have say a huge amount of clients or
you want to restrict what curries can
run in your back-end based on how big a
query is how long the quarry 8 is a
historic profile of a query or if it's
an administrative connection you always
want to let something get through or a
heartbeat or anything at all you could
do whatever you want to restrict that so
I'm going to show up my fire up my
default demo here creates a back-end
talks to the server okay I'm great I'm
fine I'm happy
and I'm going to connect to it connect
to my proxy with a little shell wrapper
so this is connected to my proxy it
claims to be 50 37 my server is 50 45 i
think i just i used to have a clever
clever string in there but I took it out
so here's some here's a fun little demo
the default demo from my proxy is a
connection pooler very very basic
implementation of what I just described
if we show here we have one connection
shandra show process list and I won't do
that yet over here we have another
connection to the proxy is is handling
the other connection that comes in it
says hello to the back end it runs the
queries against the back end and I can't
spell now over here we still have one
connection when you do a show process
list go back over here one connection is
it both connected to the same mysql
database just going through the proxy if
i use my test database over here and
show tables over here
my you can see that the session is
shared I wanted to have a much cooler
demo ready but I decided to play video
games this weekend instead of finishing
up the code oh and so this demo doesn't
at all what I was thinking for the more
fleshed out demo if you're running a
real real connection pooler you would
want to in the my school protocol you
can detective if a transaction is
started so at that point in time you can
say this back in connection has a
transaction running on it just ignore it
until the transactions dawn to pull it
out of the pool don't let anybody else
use it as far as session session
variables there's different ways you can
you can try to parse out as somebody's
using a session variable or you can just
tell them to not use it if you're if
you're designing for use with the
connect with a connection pooler you
have to kind of keep that in mind there
are ways around it and it'll probably
get easier as time goes on people write
libraries for these things to detect if
you need to recycle a connection a lot
right
which is the IP address for ya and you
can localize the variables and any way
you want you could do crazy tricks as
sharing variables all kinds of crazy
nastiness that you probably don't
actually want to do in most cases what
what the you'll want what you want to do
with if your dress just trying to
implement a basic connection pooling is
if somebody does something that dirties
your connection you can detect that you
probably just want to reconnect later or
drop the connection or market is dirty
or lock it to that one connection you
can but you can certainly run
transactions through this you can run
and any kind of query through it at all
and interact with it I guess I have one
more thing actually
so I that's obviously not real SQL the
other part of my demo that I had working
is just very very simple query rewriting
when i get the hello command I just
rewrote to select one plus one so you
could take you could write basically
anything into your SQL and have it be
command for your proxy you can write
comments into your SQL and parse that
out and then have that be something
significant like I could say one one
example that I would use with with
connection pooling is uh I'm looking if
I'm using connection pooling I obviously
want to keep pretty good control of
what's going to my back end so if I want
if by default i could have queries that
take longer than a second automatically
be killed that if there's selects so the
proxy would say would have an internal
timer that says okay you've been going
for a second die but I might have this
other query that might take a couple
seconds occasionally that a cron runs
and so I'd have a comment in there that
would say the max in the next time that
this query can run is 15 seconds and so
the proxy would use that instead and
then kill the quarry after 15 seconds
uh I guess so yeah yeah
um it never gets there so I'm the first
one to look at the comment and you
usually strip it out or you can rewrite
I've actually played this too much I
just did the straight-up comment you
know / star whatever so there's no
version number in there it's just a
comment but you could you could prefix
it with something that gets rewritten
before you pass it back to the server
such its most of what i had for the fun
little demo completely separate
completely separate it's just a separate
small c application that uses live event
and lua embedded the question was is it
a is it a separate process is it built
or is it built into the server one of
the things i wanted to try doing in the
future was actually slowly move it
towards being embedded in the server so
one thing you can do now is if you
wanted to run the proxy to control your
connections but don't want to add you
know you obviously have to do tcp on
either end of it which adds a little bit
of latency but you could say run my
school proxy on you're on the same
machine with mysql and then connect over
unix domain sockets and later on
development we could play around with
adding it as a plug in the mysql that
just listens on a different port and
talks mysql and speaks is on protocol
but proxy stuff internally and as its
own thread in mysql
that's it said earlier on
so my school proxy is actually usable
mine is a hairy Harry mess I'll just
give you an idea so well it's it's
actually the same the same architecture
we have we both use live event we both
use little and one thread one process
but the way I do in one of the best ways
to show this is just this is my default
here this is my demo you can see it's
doing all kinds of stuff fires off a
back end it's got the listener listed it
has a greater new back end and then up
there you can see a function server
handshake you're actually getting a
handshake packet and performing part of
the authentication inside Lua so what
mine does what the official one does is
it wraps up everything into nice fancy
little C libraries that make it make it
easy for you to write queries from from
the proxy makes it easy for you to
connect something you just say I want to
connect to the proxy I want to connect
to my back and server I'm going to get a
connection I'm gonna do this I'm going
to do that and just works with mine
every single packet is wrapped up into a
lua object and sent up so in in the
minus school protocol when you connect
to a server the server hand sends you a
handshake packet and you respond to that
with an authentication packet and then
it responds to that with an ok packet
and the official proxy you don't see any
of that in mind you actually get those
packets as as somewhat easy to use
little objects and then you can rewrite
any of the fields inside that pad inside
that packet now obviously I'm going to
make it a little easier to use by
actually writing little libraries that
wrap this stuff so you don't have to
play with it but it means that on a for
my proxy you have a much lower level
access to to all this junk you can
pre-allocate package you can pre store
packets and a wire format
and we can implement a lot of the
features at my school the official my
school proxy is implemented in C in pure
lua with mine and it's still the same
speed or faster any questions no I'm not
I do I have no this demo is going to
work with it right now
so one of the things I did
is I wrote a very done exactly one
optimization with the architecture so
far to speed it up a little bit and this
is of course just running on my laptop
here which is on its battery and is
underclocked so this table the select
star from blah this tables about 9
million rows 8 million rows and every
single row that gets sent from my SQL
server to your client comes as a
separate packet and these each have to
be handled independently in the proxy so
with mine I got this this uncashed query
back in about 10 seconds the cash one
should come back in about six seconds I
think before my before my optimization
this took in order about 20 seconds so
the big pretty significant overhead you
know about six and a half without the
proxy in here at all this quarry takes
about five seconds 56 seconds so you can
see it's adding even on the same machine
and I'm not not binding anything two
different CPUs or doing any other
optimization exciting very very little
overhead even with at even with going
tcp/ip to the client and then to the
proxy from the client and from the proxy
to the back end I'm expecting I designed
it as a streaming architecture it's a
little stream result sets back and forth
you don't have to buffer an entire query
up the official proxy if you're
modifying a query as far as i can tell
you have to buffer the whole thing so if
i if i were to run this query against
the official medical proxy and then
change something at all and then send it
back it would have to have to wait for
it to buffer up entirely and then wait
for it to send from the proxy to your
client before you can do anything so
that adds a lot of latency I've done a
little other work about being able to
stream actual packets so if you have a
in complete packet that's traversing to
the server and you know you don't want
to do anything with it you can just go
straight on through add almost no
latency at all so the kind of the goal
of mine is to be able to get it to scale
to the point where you can run it in
production I'm not sure if that's really
the goal for the official proxy yet
we hope so it'll probably get there
eventually I'm sure young could do it in
20 minutes it'll take me three weeks to
get anything of the sort but this is
pretty cool so far I mean this is
basically trash with whatever
so the streaming works every single
packet that comes into the server for
every single packet that comes into the
server I'm not going to bother bringing
out the code what happens is as a packet
comes in and then internally in the
protocol there are different states of
what your connection can be in you can
be waiting for a new command packet you
can be expecting a field packet
describing your results you could be
writing a row packet or any of the low
level stuff so there's there's a state
transition and then there's a data
packet and then when you're writing your
little code and for my proxy you
register callbacks based on the based on
the transition so if your weight if you
only want to look at command packets
you've set a callback on new command
packet and then as the packets come in
the proxy will check to see if you're
waiting for a new command packet and if
it's if you're not waiting for it will
just blindly pass it on if you're all
waiting for word it calls up in the lua
and then in wraps the object and then
perhaps the packet and a thin object
then gives that to you to do something
with so the idea there is if you're not
if you only want to listen on one part
or if you're only listening to certain
parts or you can you can enable or
disable certain callbacks if you don't
have a call back at all in it is it
might as well have not ever touched the
proxy to let's go straight through
anyone else architecture questions
anything really confusing why would you
ever use this this is weird not so far
not so much I don't know about
production I know some I know jaan is
using it obviously for consult
consulting users of MySQL the official
one is very helpful for profiling
queries or plugging into an environment
which is somewhat low low usage but is
having a performance problem it'll just
pick out your bad Corey just like that
so I know some consultants who are using
it that way and but i'm not using a
production anywhere yet six apart is
actually pretty interested in using mine
in production so we'll see how that goes
once there are multiple billions of
queries going through it every day but
no not yet
any other questions comments not so much
okay that's all i have i guess i'll
defer to eric for the rest the QA
test no definitely not clapping because
I came up here we have about I don't
know 45 minutes or so to kill does
anyone have any general questions
problems that you have I know Jeremy is
much better this than I am problems
you're having with minus QL yes in the
back my sequel server is already running
shut it down you can stop
you have the slow query log enabled
the problem areas is essential
what this is
if I could find something more I think
you can see details yeah and um rocky
from your server so so yeah the question
was how do you get a basically a stream
of queries running out of a mysql server
that's already going in production these
the answer is that you really can't you
can't actually use ether ill or T thrill
and actually parse the queries out of it
teather will spit out syntax it's very
easy to parse and buries the log I would
necessarily turn that on on a production
server and just let it stream to a file
because it will fill up really really
fast but yeah mysql itself doesn't have
anything built in for that you have to
actually hook into it by hand so yeah
the general log is one suggestion but
you can't enable that without restarting
until 51 something I believe so that's a
good question
what's that how is it the question is
how is 5.1 not ready yet for production
it's getting really close they're still
having some issues with partitioning and
other things but it's gotten a lot
Stabler over the past few months if
you're starting development now on
something that's going to be released
into production eight months to a year
from now 51 is a pretty good option for
that I wouldn't plan on putting it in
production right now with five
partitioning inside of MySQL or like
Jeremy usually talks about partitioning
above MySQL okay there's two different
types of partitioning there's
partitioning within one single instance
of MySQL so if you're if you have
something like logs coming in and say
every month you want those logs to
rotate into a new table so that two
years from now you can drop a month's
worth of logs that's actually built into
51 and that's mysql is official
partitioning support the other kind of
partitioning and the one that usually
comes up is partitioning above multiple
instances of mysql so say you have a
user database with a million users in it
but you can't get one instance of mysql
to run fast enough to support all those
users so you might put you know users A
through H on one mysql server and you
know so on throughout the alphabet the
high level above mysql partition
partitioning isn't supported by mysql
it's something you have to do in an
external library or project such as hive
DB which which kind
ok
I like database and
we've been using dell 29 50s with eight
15k SAS discs that's with 16 gigs of ram
that seems to be about the best
configuration of price versus
performance those are about 6 grand
apiece so once you scale beyond what one
machine can do you partition across
multiple machines and then always have a
backup in there so one partition would
consist of two different machines
not really and I can explain why do I
have any mysql cluster experience and
that's referring to the NDB cluster
storage engine to MySQL I really don't
because I don't have any customers that
have a workload that really requires
that clusters built for small relatively
small databases that can live in memory
on four day machines and update really
really fast it was originally designed
for working in cell phone switching
towers so if you picture a cell phone an
area of like 30 miles or so all of the
cars and people coming in and out and
updating every time they do so there's
not a lot of people that have that kind
of workload there's basically two
methods for that right now if you look
on MySQL com you'll hear a lot about
drbd which is a replicated block device
between two machines I don't really
recommend that and I can explain why the
method that I use is the dual master so
you basically have two masters set up
your applications writing to one if that
one dies you fail over to the second one
so somebody has too many
i recommend manual failover there are
some people that are doing it with
heartbeat right now the problem with
heartbeat is it likes to fail over when
it's not supposed to so it'll flap a lot
it causes all kinds of problems so
typically if you're big enough to have
been operations team or someone
listening on the pager and they can be
in within five to ten minutes look at
the system see what's wrong and make the
decision to failover it works a lot
better so back okay
business some okay so anybody hear what
my signals for long-term plan is to work
around
falcon falcon is a storage engine
written by Jim Starkey it's supposed to
replace nodb as the acid compliant oltp
storage engine there's also another
company called solid DB that's local to
this area and I don't think anyone's
here they've built an embedded storage
engine for years and years and they're
now turning that into a storage engine
into mysql so they'll basically be
falcon solid and nodb fighting it out to
see who's going to be the best as far as
the Oracle acquisition goes Oracle's
still pushing a lot of really great
updates into nodb they don't seem to
have any plans to stop anytime soon so
you know I still recommend that to my
customers because it's going to be a
year or two before Falcons close to
being stable enough to run into
production Oh through deploying
something now within six months or a
year stick with nodb
not too much Jeremy would be better for
that question so the question was do I
have any experience with data
warehousing Jeremy would be the person
asked I do more monitoring operations
and capacity planning for oltp sides so
the last also the question was how many
times have I seen 50 crash the last
three times I saw it crash it's because
it was running on top of the rbd so
wasn't actually my schools fault so it
was customers that had a pair of master
set up drbd was replicating the block
device between both of them the first
instance it cried the master crashed
drbd sent something across that caused
the secondary to crash the last two
instances drbd replicated file system
corruption across so
sorry what's the latest in terms of
consistency checking technology when I
was using my civilized your studies and
I Sam and you have very very large
gigabyte faces and you have my simple
crackers that files or forcing those
crash my RM check is counting got your
passengers well is the only way to get
back right if you're using mysql and
production or in a situation where if it
crashes and it has to come back up with
a few minutes don't use Maya same use
nodb so yeah it keeps everything
consistent on disk always if the Machine
crashes unless the file system was
corrupted it'll come back up it's
optional I usually use o direct on Linux
yeah you can walk up to a machine with
nodb as long as the the right caching is
configured correctly pull the cord on it
it'll come right back up yeah so
actually the backup strategy that I use
involves checkpointing the file system
underneath nodb it's been solid for
years so using using lvm taking an lvm
snapshot of the file system and then
just backing that up so which is
essentially 10 0 DB is the same thing as
just shutting the power off so
that's Lucy
it depends so the question was in terms
of speed which is better Maya say
marando DB that gets down to the
internals of each engine and the type of
tables that you're actually building
with them quick example is as a nodb can
actually be faster for primary key
lookups so where you're selecting where
something equals a primary key value the
reason for that is nodb stores the row
data with the primary key so as soon as
it walks down the tree there's one seek
to grab all the road data- am typically
faster for secondary key lookups but it
depends you know DB as row level locking
so it'll commit transactions and handle
concurrent updates a lot better so
that's the 5 minute summary there's
really a lot more involved in it so it
depends again if you're running
something in production where if the box
dies it has to come back up just run in
0 DB take the performance hit by an
extra machine it'll save you in the long
run so questions more questions
the old
end of June April beginning of June
anime yeah it was fun it was we get we
did kind of an extension of the same
session that we did the year before so
this is more about partitioning versus
the actual failover stuff so I think we
have that tutorial session blocked out
for the next ten years so we'll be there
again next year
if eligible
the question is if you're replicating
MySQL from say the east coast of the
west coast how to make it faster the
easy answer is to speed up the speed of
light the master once it receives a
query and it's processing it it tries to
push those updates out to the slave as
fast as possible so the latency between
to Matt to master master and a slave is
as fast as the network and write it so
the bottleneck in that system is usually
how fast the slave can pick up those
events and apply them to its own tables
so I don't latency across the countries
what like 80 milliseconds something like
that so I don't know unless you're on a
really really slow connection it's
usually not an issue so the slave will
usually fall behind applying those
updates before the master will fall
behind sending them so
and chicago or dallas yeah yeah it's
really how fast the network can actually
shuttle all those packets across so
okay what is the best way to prevent tax
that can be in that door
to implement tags hmm that's a tough
question tag
I don't know Jeremy is going to have a
better answer for whatever I come up
with so the basic problem behind that is
that tags are a graph structure and
mysql is all implemented using trees so
you're basically translating one data
structure into another which is never
that fast so there's a big hole in the
industry right now for how to implement
that really efficiently as far as I know
right now people just build the tags
table in a relations table and then cash
the crap out of it with memcache so
brick might have some better ideas
that's that's what i've seen so
hopefully soon someone will come out
with a storage engine or a patch that
will handle graphs and social networks a
lot better so
questions anyone else
sobbing of like
20 clubs
can we
hmm so the question was why is summing
and group by so slow that's actually
partly due to how mysql handles group by
and handles temporary tables as its
reading through and resolving a group by
query it's actually unpacking and
packing that row data multiple times
both out of the table that you're
reading it from and into the temporary
table that mysql is using to store
results watch Jeremy's blog because I
think he's going to patch that within
the next week or so so hopefully we
fixed that for min and Max but not
really for some because it has to read
all the rows anyways so min and Max
group i will read either end of the
index yeah that the major overhead is
reading in and out of the temporary
table and repacking those rows so it's
going to build a hash table which
essentially acts as a cache in front of
the heap storage engine so instead of
repacking those rows every time he's
going to keep an LRU of 128 or so
entries of the most commonly used ones
so instead of repacking those rows you
just update the hash table it doesn't it
should
questions the time is it
god you're doing this a lot it's over
historic data keep summary tables
Rose
through the current data the last his
own
property
yeah cash it
the experience of cashing the table in
the memory to make the reader
yes mysql will do that internally for
nodb tables in the buffer pool depending
on your workload you can probably
benefit by sticking memcache or a
memcache cluster on a different set of
servers and cashing it there memcache
will be a lot more efficient than MySQL
at caching just simple key value lookups
you can you can actually cluster
memcache so it'll do yeah you have to do
the invalidation in memcache yourself
but as far as it can broadcast updates
and it'll partition across most most of
the client api's will partition across
multiple memcache servers yeah it's it's
really fast
questions
I don't do what
ma
ok
so the question was how to change a
table form is aimed nodb the actual
changing is really easy just alter table
your application has to be smart enough
to deal with transactions that were
rolled back due to deadlock and what
else what am I forgetting Rick I think
that's the main one yeah I think the
main one is that transactions can be
rolled back due to deadlock internally
nodb keeps locks on rose so if you have
a connection a that locks euro a on a
table connection be locks roby and then
they try to lock each other's rose it'll
deadlock nodb will detect that
automatically and roll back what it
thinks is the smallest transaction so
instead of getting ok back in your
application you'll get transaction was
rolled back due to deadlock and then you
just have to retry so and that can
appear what looks like randomly but so
you do with Maya same you don't within
ODB you do myisam only locks tables and
it locks them in a predefined order so
you know two threads can race and lock a
bunch of tables but they can't lock a
table that someone else is already
locked so there's no deadlox with Maya
Sam so
it's a good one
and ODB you're at this table space there
only grow it never shrinks and you take
the table out of there well an empty
space in there but your disk file
alright
it won't shrink the only way it'll
shrink is if you're using a 0 DB file
file / table in which case it'll newly
created tables will get created in their
own tablespace file and if you drop the
table that'll go away but if you're
using just the regular global tablespace
you know DB you'll never shrink that
file you have to dump everything out
shut the server down delete those files
start it back up and import everything
again yeah yeah well it'll be reclaimed
depending on what happens that can take
a long time so also a conforming yeah
one thing to note about that is an alter
table since alter table always creates
the new table as a temporary table of
the correct storage engine type if you
have one table in NL DB and you alter it
your table space will grow you know a
little bit less than twice as big so
which can be surprising so
more questions
hmm
collection I've
I do isn't
basically what advocates I won't be
consistent
so the question was how to get a
consistent snapshot out of my a.m. when
inserts are running are using concurrent
inserts in my Sam the point of
concurrent inserts and Maya same is that
that inserts can be happening and not
interrupt other selects because there's
no real concept of row locking the only
way they can do that is to always append
the new data on the end of the file and
then not let the new rows be visible to
existing connections and tell all of the
index updates and everything are done so
I guess the answer is you really can't
yeah yeah you can block everyone else
out you can turn off concurrent inserts
if you want to be able to have that kind
of control over consistency and views i
would say use nodb because you can leave
everyone as as you know read committed
or repeatable read and then if you have
a connection that you want to use to spy
on someone else's transaction you can
set it to read uncommitted and see what
they're doing there's not a whole lot of
control there except for locking tables
you blocked shot blocking very very true
yeah
yeah I mean you mean like consistent for
backups are consistent for other
connections yeah changing the the the
insert concur I just lost my trance
there's actually there's actually a
really great page of the manual on the
speed of insert queries the main thing
with nodb is to buffer them as much as
possible so if you're using auto commit
or you're committing after every single
insert it's going to be really slow so
you have to to batch you know up to
10,000 rows or so in a single commit and
you'll be able to do insert as probably
almost as fast as my asst am but not
quiet so it shouldn't be an order of
magnitude difference like you're seeing
though
doing multiple rows
yeah yeah both of them have
optimizations for inserting both those
optimizations deal with inserting
multiple rows at the same time in ODB
buffers secondary index updates my SM
weights to flush the indexes until the
inserts are done or the tables unlocked
so it depends come talk to me afterwards
of all well we can get more into it
depends on how big the inserts are I
would say if your bulk loading data safe
to stay around 10,000 rows or so
internally when you do alter table
that's the number that the MySQL picked
for alter table it spins yeah but it's
still not that fast yeah right so
actually yeah I found a bug in that the
other day so minus cue ball dump when
you create a dump file that reads max a
lot of packet and then tries to create
the dump file for that max allowed
packet it accepts that as a parameter on
the command line so if you want to dump
like one gig rows like if you're storing
huge blobs and Rose you say max allowed
packet two gigs and it'll write that out
to the file but it never writes the max
allowed packet perimeter in the dump so
when you try to import it at pukes Oh
so yeah I'll hopefully fix that this
week or next week takin fine yeah
exactly so yeah the defaults are only
built to handle like medium blobs more
questions
on what I don't know what that is
del storage oh the external raid arrays
so question was do I have any experience
with the the md three thousands or the I
forget what you are used to have
that's another great Jeremy question
grade level typically raid 10 stripe
sighs I don't know jeremy has that on
his blog he does most of the system
level benchmarking stuff so I don't yes
yeah yeah it's going to be somewhere
within an order of magnitude of the nodb
page file or page size which is 16 k so
a 64k perfect
questions anyone else want to take
questions
that's a great yeah the memcache is a
great topic that I haven't dealt with
that much so actually what does anyone
know yeah we've got some time yeah so I
actually wrote this all into the wiki on
if you go to danga calm / memcache d and
click the wiki thing in the corner you
can see me rambling on for about 10-15
pages i wrote a in architectural
overview so basically memcache is the
simplest key-value hash table as a
server you can you can design it doesn't
get much simpler than that what it is
conceptually is a two layer hash table
so if you if you visualize how a hash
table normally works I'm probably not
going to bother diagramming this you
have your list of you have your list of
keys and then your list of values and
when you look up when you want to look
up a value you specify the key does a
hash operation on the key to find out
where in your status structure your
value is and then give that back to you
now memcache d the server isn't that
clever internally it's just a network
Damon that listens on a port and you say
get value and it gives you the value say
set value it sets the value of increment
decrement add replace delete you can set
expire time that's not is not that
interesting but what gets interesting is
the client layer of memcache d is
actually a second layer of hash table so
you have your client which takes your
key and hashes it out over your list of
memcache servers so you can have one
memcache dean it works ok you can have
30 mm cash days and it still works ok
because your key will just pick one
random guy in the cluster in little work
and memcache dias since it's fast enough
it you don't really need to worry about
load balancing in most conditions there
are a couple very extreme cases where
usually Facebook
where they end up with a couple memcache
DS which are getting 30,000 60,000
queries per second and everybody else is
only getting 20 28,000 queries per
second they get very frustrated at that
are basically what it does is allows you
to to take your results of anything you
have in your application either your
query results from your your post object
or results your user object that you
need old up all nice and all nice and
pretty or your your actual HTML result
you can stick that in memcache and get
it back later so the first time the user
views your profile you pull the profile
data out of their database you pull
their birthday you pull their of their
bio page you pull their their friends
tags there i am info all this junk out
of six different databases that you have
wherever and then you compile it all
together in a nice fancy HTML using
their template which you pulled from is
another database and then did a whole
bunch of processing on it you pull out
their bad text and in place to BB code
and have done a lot of work at this
point and at that point you can take it
and just shove it in memcache and say
this expires after 15 minutes or maybe
an hour maybe a day or whenever the user
updates their bio at all next so the
next time user comes in they would go in
and say oh well their bio change did I
we use this cash here this cache key
here so we're just going to tell
memcache to update at a blow away so
then the next time the user comes in
views are you their profile it just
loads immediately instantly just pull it
on memcache comes up it's great and
happy so it's really it's it's a
wonderful tool it does complicate things
a little bit you do need to mind your
data you can probably think it's
somewhat similar to the my school curry
cash which will just basically cash your
the results of a rose on a table but the
query cache gets for an entire table
gets completely emptied every single
time the tables updated is meant because
I'm cash because MySQL has a hard time
finding out if you're if the the cash
queries were affected by an update so it
just blows the whole thing of light with
memcache you can make that your decision
you can say well this user data if
their comment text it will need to see
it for 30 seconds so it doesn't matter
cashew for 30 seconds or when you you
know is in the application developer but
i've just updated this data so then you
at that point send it to memcache so the
next time the user when the user updates
their comment and then they click it
click reload to go view it you never
have to hit the database you just have
to hit memcache and you can really do
you can build memcache servers that are
very cheap you can put memcache d
wherever you have free ram cpu usage is
usually pretty low it's it has a
threaded mode now thanks to facebook
where you can run it on multiple cpus if
you're really hitting it that hard has a
few interesting features some
interesting upcoming developments but
otherwise it's pretty pretty stable
pretty small pretty compact product that
usually does what you wanted to
throw the client side so typically right
now if you go and grab a client random
client and set it up and do that yes
your entire cache will become invalid
and you will have a some factor of some
mathematical algorithm I forget of
having a chance of having your day to be
in the place that you think it would
think it will be which it isn't isn't so
bad you won't run on a memory on it is
memcache ISM isn't a lure you at least
recently used cash so all items that
aren't used will be pushed out in
preference of new items so you will
temporarily lose most your cash there
has been developments on the clients to
use a technique called consistent
hashing which I can explain it in more
detail if anyone's curious but it means
that you I guess it adds an extra
abstract layer of hashing so that you
have a third layer of hashing where you
hash your server configs out into into a
hash table and then you look up this you
hash your key onto this hash which then
points to your server and then you can
find your memcache d and find it
basically what this means is if you take
you add a new memory to your list and
you have your secondary hash index
you're basically just adding it to this
other index you're not shifting
everything around so you have a still a
small chance of missing some some of
your your caches but when you add
servers remove servers using it consists
a client that supports consistent
hashing there's almost no problem at all
as far as I know none of the men khaki
libraries plan chambers none memcache
decline language offers support dynamic
reconfiguration
and certainly not a comment so the
question is do they support the dynamic
reconfiguration of the pool and what do
you mean by not especially that's not
usually a big deal if you're if you're
using consistent hashing and you're
adding a server would absolutely is
necessary because where you do a right
let's say that you a teacher you're
right and at t1 the full configuration
changes and at t2 you do a read in that
case for RIT
you read right and you've got to serves
your pool that happen ever if you're
using if you're using consistent hashing
you're going to get the same server back
for that key anyway if you're using
consistent hashing and you're adding a
server it doesn't just doesn't disrupt
your current list you might you have a
very small percentage chance of doing a
cache miss in some cases but with with
consistent hashing if you add a server
it doesn't it doesn't shift anything
else around so you can add servers and
even remove servers and if you remove a
server it will cause cache misses
obviously if you add a server it'll
become you'll start getting getting
rights new rights going to a different
place you probably won't there might be
stole case or two where that where that
does happen I can't figure that out in
my head right now I guess I'm concerned
about anytime soon perfect bashing
because there one there is no such thing
and two you know if you add more service
of a fool than they have different
configurations for example I want to get
twice the number
I want to assign twice number the hash
table our buckets to a server that has
twice the network added twice easy
answer is that the load balancing across
multiple know through district capacity
load-balancing all the algorithms
i'ma treat them all the same and
ok
sometimes it sounds possible due to
figuration
we basically had a situation where
everywhere experiencing a traffic spike
to new servers for handling client but
since there was around
single-issue got
great so the solution was actually just
drop the lowest tuition
what happened was the political part of
it I dropped with restore ones out in
we're out of past put
sometimes just better
and it definitely depends on how big the
difference is if they're 1 for 2 speed
yeah as far as load balancing if you're
trying to load balance memcache is
usually something wrong if you have a
server that has twice as much RAM in it
and the CPU usage is not what you're
concerned about you can add the server
twice so you can you can add it twice to
the consistent hashing this let's get
hit twice as often probably not a good
idea if you can avoid it for obvious
reasons that Eric listed if you're
trying to load balance it in any other
way trying to put an elf or in front of
it you're kind of defeating the purpose
right no I'm just saying that for
completeness I guess well the clients
aren't bundled with the server the the
clients are not bundled with the the
server so you can get if you're using
what language you're using Python know
PHP yes PHP has two or three different
clients there's lib kodama which you use
in front of any other client lib kodama
was written by last.fm and implements i
think it was actually the first
non-parole implementation of consistent
hashing from ice for memcache well the
it started as pearl so the example is
one bride fits Patrick wrote the cpn
module set consistent hash so it's just
a generic consistent hash module that
you can use with the pearl client to
implement consistent hashing it's a lot
simpler than it sounds saying it out and
once you actually go look at the perl
module there's also another PHP classes
uses it and i think a PR memcache the c
library has an implementation of it if
not it's on the way
oh yeah you can do whatever the hell you
want to balance your lists the poll for
one example with the pro client if you
have say 70 memcache service or 70
memcache instances like I have before
and you have user data that you all want
to fetch so you have user data the users
birthday and their their name and some
of their URLs and some other information
and then 10 unique data entries like the
10th blog post by a user and if you're
just normally using memcache and you're
storing each of those 10 blog entries in
they could end up on different mem cache
servers now when you're trying to read
your data back you have to go get from
one get from the other get from the
other get from the other there are two
there two things you can do to deal with
that one is a client's which use
non-blocking i/o so they actually send
all the requests out and then read them
slowly the other thing is at least in
the pearl client you can define your own
hash value for a key that you're giving
so you say if this user is user ID 10
I'm going to say this hash value for
this for this keys for all these keys
that are storing for this user our user
colon 10 so that I'll make sure that all
the all the all the data for that one
user will tend to end up on one single
server and then you can use a get multi
you send one request down to memcache
with saying I need 10 keys and I'll to
stream it all back it's really fast so
that makes that a lot easier
I have invites for my
so what's the overhead of using it less
than crying my school it's it's a very
very very small usually it's just the
latency of your network a couple hundred
bytes for so the value when you're
storing a value in memcache you asking
for the memory overhead so then the way
memcache stores your actual data is
using a slab kasher so it breaks up its
you might have a gigabyte of memory in
memcache but that's actually broken up
into a power of n different classes of
slabs the largest being a megabyte the
smallest being thing one hundred
something bites your average is going to
be about 100 something bites or to kill
a lesson to kilobytes so what happens is
as you allocate memory you end up it
allocates these slabs and the slabs are
created like maybe 100 bytes 125 bytes
150 bytes 200 bytes 250 400 700-900 a
kilobyte in that way up so when you
store a value it'll pick the one that's
it'll pick the slab class that's closest
to the size of your data so you will
have a little bit of overhead when
stephen grim from facebook got to that
and and redid the slobber a little bit
he really lowered the amount of overhead
you have there but you do lose a little
bit of memory but i guess the idea is
that ram is cheap and you can actually
tune that value so if you if you know
that your data is always going to be
less than two Gila bites you can set
your max at your max down you can adjust
the the power of the slabs and say I
know all my data or I'm going to have
four million objects that are exactly
Andhra bites so I'm going to make sure
that most of my values are in that range
and you'll get very little red any other
questions
how does how does what hey guys gave us
has this big guy 23 cash um I'm not sure
off hand any other
the same is it eh cash I might have seen
this obviously once you get anything
more complex in memcache it's going to
be a little more work also when you're
when you're storing java objects you
have to implement serializable and in
all these cases with with memcache you
have to be able to serialize your object
if you're really careful about it
they're probably pretty similar but
memcache is is very much I guess tuned
for batch processing single connection
can handle a stream of results or
require ease it comes in stream out
results so it's very tuned very very
quick tiny little accesses and you don't
really have to worry about storing trees
or you have to worry about storing
storing access-list and this and that
there's some development for adding more
complex data types into memcache but
it's done it's being done very carefully
so that we continue to scale 0 n 0 1
with whatever you're putting in and most
of the cases if you want to store some
kind of complex data object into
memcache d you can you can do a number
of things actually on the wiki as
examples for storing in it's the best
example I don't have a good example off
the top my head you could store in you
have like a tertiary key that describes
list items that are other that are keys
for other objects that you want so if
you want to store a list you store your
list in one key and then you get that
research that so you do one back and
forth and then you get the rescue keys
and you pull it out and you just update
that tertiary key when you're when
you're updating your list so there's
different ways you can you can store any
kind of data even though memcache is
perfectly flat and anything else
our bust is the UDP facebook says it
works okay anything above the the MTU of
your box of 14 anything about 1,400
bytes and well usually if you're using
memcache you might as well use
persistent connections you can have
twenty thirty forty fifty sixty thousand
connections again femme cash and you
only waste a little bit of memory it
won't actually slowed down at all
because the memcache is only going to
respond to connections that are active
with data so if you have a connection
that's sitting there idling it's just
wasted memory right so in the in the
patchy PHP case and you can't use
persistent connections facebook has they
say they get they get a really big
performance boost by using UDP for for
certain objects so if it doesn't matter
as much if you lose it you're probably
not going to lose it if you're on a lan
but if it doesn't matter as much you
will get a pretty market perform an
increase I haven't I wish they would
have benchmarked it I haven't tested it
myself yet but they say it's pretty
quick
Facebook wrote one they said it was
embarrassingly slow so they never
released it through is just a generic
proxy and you can connect to it through
through a eunuch socket you connect to
it through TCP I don't know so the
question is using shared memory and map
files usually if youÃ­re if you're in a
situation where you can use a map to
talk to your cash you have a much faster
local cache already
you could do that I don't know I don't
know of anything that does that I've of
course something like the minus bill
proxy or my proxy you could pretty
easily plug in a PR memcache into that
and use I think for most languages
persistent connections to my SQL work
okay so you could have special
connection special commands like
memcache get that passes through the
proxy that actually just does toss just
talks to memcache through a persisting
connection anything else anything clever
roxbury just sort of shooting the ball
whether you're just shooting the
problems from one layer and that's right
because you're here instead of having
all clients connect directly to memcache
d now they're clicking the approximate
stay we have not remembered to move the
front yeah she remembers her box broncos
game
anything else a little bored how we're
doing on time huh
Oh
the Lightning 830 about there you go to
danga calm da NGA calm and it'll be
linked on the front page anything else
last call ok</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>