<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oracle Coherence Data Grid | Coder Coacher - Coaching Coders</title><meta content="Oracle Coherence Data Grid - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oracle Coherence Data Grid</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4Sq45B8wAXc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today I'm pleased to introduce
Cameron Purdy cameras is the founder and
CEO of tango Sol which was acquired by
Oracle earlier this year he has over 12
years of experience with java and java
related technology he's currently the
specification lead for Jas r107
otherwise known as Jay cash he's here
today to talk to us about oracle
coherence datagrid this talk will be
going out on google videos so please
hold any conference confidential
questions until the end Kamerion thank
you so this is a presentation I'm going
to do today is a little bit old I just
had it updated by someone to the Oracle
format so I know the slides pretty well
but it doesn't cover all the the new
stuff we've been working on for the past
year it touches on a little bit of it
and I'll probably dive into some of it
without the assistance of slides but if
you do have questions as I go through
certainly asked them a lot of the things
that I'll cover in this are not
specifically product ID and then there
are a lot of things that are obviously I
mean everything I'm talking about is
somehow related to what we do in our
product but a lot of it's more just
general technology and then of course
there's number of things I'll cover that
are specific specific to what we do this
this introductory section is fairly high
level so I'm going to run through this
pretty quickly
basically when we explain what it is
that we do and why it is that we do it
you know there there are a couple
different ways to explain it usually
including graphs and bubbles and
pictures and lines but in general you
know if you look over the past 10 15
years the amount of information that
were collecting accessing managing
archiving auditing storing searching
through analyzing rolling up aggregating
and backing up etc has exploded a
typical system you know 10 15 years ago
if you look at just the complexity of
the schema
you know you you have potentially an
order-of-magnitude
maybe tour or magnitude increases the
complexity of a schema under an
application just in the past decade or
so you know you used to get a database
and it would come with seven tables in
the pubs example database you could run
an entire business on it now a typical
invoice object is represented by in 170
tables so it's a you know you just look
at the explosion of complexity of what
we deal with in part of it too is that
you know when the internet happened you
know systems that used to be call
centers with one hundred hundred and
fifty people operating them all of a
sudden became globally accessible we
still had the same craft sitting behind
it you know this same mainframe system
you know tied into the to the shipping
department and the billing department
and everything else so we took in green
screen scraped applications that we had
converted the client server for our call
centers and we converted them now we you
know put another wrapper around it and
made them available to the web but the
problem was we didn't have a hundred 150
people using it on sudden we had you
know potentially thousands I don't know
if you look at the statistics of usages
on various transactional sites it's a
lot lower than you would imagine from
what you read in magazines but it's a
lot higher than most people who are
dealt with so some of the travel sites
that we work with for example run about
40,000 concurrent users which doesn't
sound ridiculous it's probably far less
than than the numbers that they publish
but it actually puts some pretty obscene
loads in one case one of the three big
ones you know they host just the hotel's
portion of their business on I think 300
servers which you know if you consider
the amount of work that that that 300
servers in aggregate can do you know
each server having probably four to
eight cores running at around three
gigahertz I mean that's a let me change
my French there that's a lot of that's a
lot of CPU cycles that are that are
getting burnt to do relatively simple
stuff I mean all we're doing is sticking
text on on someone's screen over a TCP
socket how hard can that be
you know you look at the know no offense
to Google you look at some of the trends
as well I mean everything we do as as
engineers as we're as we're inventing
new complexity so we can rewrite what we
just wrote a couple years ago and you
talk about virtualization the ability to
move apps around and spread them out
across multiple machines or pretend that
one server is actually a whole bunch of
servers
okay SOA where we basically redo the
whole green screen transition except now
instead of a green screen you know some
collection of code that we have and we
wrap it up and make it you know the
wrong granularity so we have to call it
too many times in a loop and you know
send back more information than actually
is needed and things like that so all
we're doing if you will you know
generation after generation of software
is finding less and less efficient ways
to do what we've been doing for a long
time you know you look at SOA as well it
introduces another kind of analog to the
Internet in terms of the users of
applications when when the internet
happened you know if you call the quick
call center it said well you know you
need to tick it to Alaska and they said
well you know my computer's slow right
you know hang on for a minute
you don't know they could be down for
all you know they could be like writing
on paper but when you go to you know
Expedia or Hotwire Orbitz or any of
these Deltic on any of the travel sites
etc and you start you know booking a
ticket if it's a 404 that you get back
you know there's no person saying oh my
computer slow I mean it's down and you
go somewhere else so you know you you're
your failures become public similarly in
an SOA system you know if you're
building a composite set of services and
you're relying on other services you
find out that that uptime isn't isn't
predicted by the uptime of the service
so for example if you if you build a
composite service that uses six other
services you now have assuming they all
have 99% availability you have best-case
scenario 93 you have seven services that
have to be running all running so it's
99 and 99 yen
so you end up having to basically make
each individual component a hundred
percent available to even approach you
know the numbers that that that you
would expect any any business system to
provide and ninety-three percent
availability means you're down for what
half an hour during an eight-hour window
business day so I mean it's uh it's
pretty pathetic or depending on your
business it might actually be pretty
good compared to some systems
event-driven architectures I mean this
is this is death by a thousand cuts or
million cuts or a billion cuts being
able to take information and have have
changes to information drive in a ton of
attano comic fashion to drive other
transactions to change more data that
produce more events that so on and so
forth so you know you've probably read
something about like staged event driven
architectures in what I'm referring to
here is systems that basically take
information that's being managed in you
know transactional information and as it
changes those changes drive other
business processes which which create
you know feedback loops within within
within the data sense so basically all
these things you know lump together if
you look at you know the history of you
know what we've done as an industry over
the past ten fifteen years we've
basically massively increased our need
for information for the applications
we've massively increased how much
information we create how much
information we consume audit everything
else and at the same time a traditional
infrastructure for managing information
hasn't kept up I mean we've doubled the
speed or overall throughput of CPUs
every I don't know roughly twenty four
months twenty six months now it used to
be eighteen but it's a not quite as good
anymore you know we've increased the
number of CPUs we can put in a box
theoretically but at the same time those
those those increases have not kept up
with with the other growth that we've
looked at so it's what we call a
scalability chasm you know systems that
have to be able to handle hundreds of
thousands of you know relatively
computationally expensive transact
per second just as an example you know
things that just aren't conceivable on
traditional infrastructure in the way
that we address the challenges is by
lashing together a bunch of cheap
computers and letting them work together
and so fundamentally we're gonna look at
how how we do then so what's a data grid
basically it's it's something that
manages information in a grid
environment lots of servers working
together but it's a little different
from kind of a traditional grid where
the servers run independently and don't
really rely on each other it's not a
stateless infrastructure it's actually
an infrastructure explicitly designed to
manage state and as such you know it has
to be able to manage it for example
transactionally it has to be able to
provide actual correct information not
just best guess or relatively recent it
has to be able to guarantee once and
only once operations even if server
failure occurs before during after you
know whenever it occurs so fairly you
know fairly high standards for what it
has to be able to accomplish and it also
you know when you add servers it's
really nice if you can actually do more
with them it's one of the concepts I
guess behind the scale-out term which is
you add more servers you can manage more
data you add more servers you can handle
more transactions per second and so
we're gonna look at how that works as
well I'm just gonna skip this cuz it's
marketing same thing here I mean this is
basically what I've already said I mean
this this is what this is what has
really driven the need for our software
and if you think about it just like step
back from all the technology for a
second we started a company in 2000
which was you know dead center of the
beginning of the open-source revolution
and we actually sold software and we
sold enough software that we more than
doubled revenues for seven years in a
row and we're still selling a lot of
software and so like something that we
did must have been valuable to someone
or those previous marketing slides that
I skipped must have been really good so
there must have been some massive needs
some massive pent up demand
for solving this type of problem and if
you look at you know the number of
companies trying to imitate what we've
done I think it speaks for itself that
there's a real market there and a lot of
interesting stuff going on so talking
about data as a service I mean it's kind
of a summary what we look at what we do
in terms of our software as we provide
these capabilities of managing
information across a grid of
infrastructure we look at that as a
service it's something that we provide
to an application or to a series of
applications we do it in what we refer
to as the middle tier meaning it's not
the database it's not a GUI it's
somewhere in between you don't consider
inside an app server or our soft resolve
our implementation is all Java so it's
it's typically you know in the same type
of location as the app server would
might even run inside an app server it
also allows all sorts of things that
haven't been previously possible
including the fact since it's working
with a domain model you know things like
integration can occur at the domain
model level as opposed from you know
hacking directly against you know the
hundred and seventy tables you know
representing one of those objects so it
allows concurrency control and
operational control at the domain model
level so what is the datagrid as I said
it combines data management data
processing so it's not just being able
to access information it's actually also
about being able to push processing to
where the information is being managed
in this environment so a couple key
points I think I cover in some of these
slides we talked about I'm just gonna
skip around a little bit
see if I can find what I'm looking for
yeah I think I lost some slides in here
somewhere we talk about a single single
system image which is it doesn't matter
if I have one server or a thousand
servers I need to see conceptually when
the application asks a question it needs
to see the entire set of information so
for example you know if I if I'm adding
servers dynamically I don't want to see
information like disappearing
temporarily and
somewhere else I don't want to know what
server it's on I don't to have to like
go to a particular server to get
information no matter how the
information is organized within a
distributed environment I want to see a
single system image I want to I want the
software to lie to me and pretend that
all the information is local it doesn't
matter if it's terabytes of information
I wanted to pretend it's all local so if
I have to ask inefficient questions
they'll all be answered without having
to have a knowledge of the
infrastructure
what servers there are what addresses
there are you know so on and so forth so
that's what's called a single system
image at the same time it is a lie right
it's a lie that it's all in one place
it's a lie that it's all local any any
sufficiently scaled system is gonna have
to spread that information out across
lots of servers so having done so you
know you don't want to fulfill the lie
you don't have to fulfill the promise
the lie because what it means is you
know if you have a single-threaded
process it says how many items are there
you know I'm gonna go into a for loop
I'm gonna grab each one grabbing means
dragging across the network asking how
many there are means you know working
with all the servers to figure out a sum
and by the way you've got 100 threads
running on a hundred servers that's ten
thousand threads what's the likelihood
that that number is not gonna change in
fact what's the likelihood that it's
ever you know how many stars are there
in the universe there's there's no
answer only God knows because you know
the concept of time in a distributed
system doesn't exist there's there's
only relative time among servers so all
these concepts that you think about
within a single threaded system which is
how most of us on our have been
fortunate enough to to code
none of those concepts actually hold
true even in a multi-threaded system let
alone a distributed environment so what
we want to do if we have all these
servers managing information in in
concert we want to be able to push
processing to those servers we don't
wanna have to drag information locally
oh wait let's create a transactional
context first drag it locally work with
it locally put it back and hope you know
we don't have to roll back and repeat
the transaction right why don't we just
send what's inside the beginning commit
let's just send it over to where the end
nation is if we have 100 servers and
we're processing information that's
spread across 100 let's just send it all
hundred at the same time push it out to
those servers let them process in
parallel so in other words once you can
achieve the guarantees in terms of
information reliability and once and
only once processing it gives you the
ability to much more blindly push
information around and push processing
around because you know it's actually
gonna get taken care of you don't have
to worry about what happens in the case
of server failure and things like that
so fundamentally when you build
distributed systems no matter what
acronyms you make up for your internal
projects no matter what you read in some
book written by a guy who's actually
never built a working distributed system
I haven't read in a book yet so don't
look at me no matter what you call it
there's really only two things you can
move conceptually
you can move state and you can move
behavior and that's it like any any I
mean fundamentally you can only move
bytes across the wire but conceptually
you can only move state and behavior so
when when you build a distributed system
most distributed systems today have been
built around the concept of moving well
messages so basically moving behavior
disguised a state if you will
in other words telling another server to
do something we call an RPC I'm you know
in the bathroom here there's something
nice on the wall telling me how to
simplify RPC I thought oh my god I'm
going back to core bidets now it's a I
assume this is public knowledge that you
hang things in your bathroom telling
telling people how to build build
software so but I mean RPC is is I mean
this is the way that we built
distributed systems 20 years ago and to
outside a very specific context I would
I would suggest that the RPC should have
stayed 20 years ago and so what I want
to show is well today is that that a lot
of what we what we attempt to accomplish
with things like RPC is actually much
more naturally considered
as for example state transition so it's
state management not so much just
messaging RPC etc but the ability to
actually represent the things that we're
doing much more naturally and
efficiently and less error-prone
by managing state so what I'm saying
earlier I mean managing information
you're either going to move the
information to where your codes running
are you gonna move the processing to
where the information is sitting the
benefits of moving the processing well
there's a whole bunch of benefits some
of them I've already described like the
parallelization of work if the
information is spread out across servers
I can spread out the processing
naturally as well there's a whole bunch
of other neat little things like
distributed transactions all of a sudden
become much less relevant distributed
transactions are a nightmare because
they fundamentally can't work I mean the
theory of distributed transactions if
you actually get down to the theory
you'll find that at the end of the
theory where it tells you about how it
works the last little fine print on the
theory is by the way this doesn't
actually work there's no way to make it
work you always end up in a situation
where the distributed transactions that
you have to make one of two choices
doesn't matter if it's two-phase commit
three phase commit four phase commit at
the end of it that else that lasts else
the you know otherwise on the case you
know it drops down the defaults on the
case that last that last gotcha you have
to make a choice you have two choices
you halt
let it be manually recovered for example
or you make a blind may be educated so
maybe not necessarily blind but you make
some sort of heuristic decision you make
I'm going to pretend that it worked oh
wait no I'll pretend it rolled you know
pick one pick one you either halt or you
guess so being able to eliminate
two-phase commit for example is one of
the keys one of the very essence is for
producing continuously available
software for transactional systems in
other words if you cannot
eliminate two-phase commit by definition
you cannot create a continuously
available system unless all the
components are continuously available
which in reality doesn't exist even a
mainframe has a parity error every three
hundred and something years and user
error much more often let's see oh I
should have also mentioned the second
one this is also extremely important
based on the granularity of information
concurrency control can become a major
factor in large-scale systems so if I'm
moving information around whether it's
transaction control or whether it's
explicit pessimistic concurrency control
locking service if you will if I have to
move information around to manage it so
I have a transaction a local transaction
running potentially multiple places at
the same time either optimistically or
pessimistically or I have code there's
actually locking information changing it
and putting it back in all these cases
I'm coordinating across multiple servers
if I can push the logic to where the
data is being managed within a
distributed environment all the
concurrency control can be localized
it's a it's a fundamentally different
way of looking at a problem that most
people don't see until you get well past
you know unit testing phase and and I
have one example I can't name any
customer names etc but this this ended
up this this problem of concurrency
control and distributed transactions bit
one of our competitors very badly
because they have because because they
don't have a consensus based clustering
mechanism underneath their product they
ended up having to use explicit
concurrency control through transactions
and the their their their throughput
turned out to be about two transactions
per second on the same hardware by
pushing by delegating the logic to where
the information was being managed we
ended up driving somewhere between 200
and 250 times the number of transactions
per second on the same hardware with
actually higher levels of available
so I mean it ended up because it's a
much more natural way to do it to push
all the responsibilities related to the
information to where it sits within a
distributed architecture and I want to
get actually to talking about that and
this is the goal for pushing logic to
information which is locality just one
more example before you dive into some
pictures one of our bank customers had
50 servers running a compute grid doing
a big big risk calculation will take 50
days so 50 days if you're a bank and you
have a month end means that your
calculation ends after you next month
end it's fairly useless
that's with 50 servers on the same
hardware so no additional hardware being
utilized we dropped the calculation to
under an hour so we didn't have any
extra CPU cycles so where'd the
difference come from it's very simple
you know go back to your office after
this and reboot your computer go into
the BIOS and turn off your l0 l1 l2 and
l3 caches actually do it on your SMP
systems that are running Google comm see
what happens when you reboot the
computer instead of being you know 40
seconds it'll be 12 minutes or maybe 20
minutes hey why is that well main memory
on an SMP system there's like a database
for an application 5 lots of servers
sitting in front of that that database
it becomes a shared bus of information
and the information that the application
needs isn't where the application is is
only in main memory there's no there's
no cache if you will in between so by
having the ability to move the
information closer and closer to where
the actual processing logic is occurring
in the CPU l0 on l2 cache etcetera the
cache built into the CPU the latency is
on those access accesses drop to an
immeasurable level and on the latest
Pentium chips it's actually I believe
less than a clock cycle for information
access from from the closest to cache so
it's literally instantaneous it's almost
almost register speed if you oh I think
it's just slightly lacking in the
parallelization side but very very fast
and so as you go back through those you
end up
adding more and more latency to the
point where actually becomes measurable
in terms of nanoseconds and nanoseconds
don't sound like much but they add up in
a hurry as you would see if you did your
BIOS tweak as I mentioned so all we did
was eliminate io8 in other words those
50 servers by having the information
already on those servers ready to go we
had 50 times for eight cores each eight
times roughly three billion operations
per second now normally 2.9 or 2.99
billion of those three billion clock
cycles would be sitting there in a sleep
state for the CPU because it's waiting
on an IR cue to be triggered by a
network card sitting on the PCI X bus
you know coming because there's
something got received from an Oracle
database or whatever is sitting behind
it just sitting there doing nothing
three billion clock cycles why do you
think you run more threads than you have
cores because if you don't if you run
one thread per core and you go to a
database to get something or to a
message service or whatever it is you're
just gonna be sitting there waiting the
CPU is doing nothing it's it's just
cooling down from the last person to
work it did so you have to run more and
more threads just to keep your CPU busy
because you're always the thread is
typically spending 90 99% of its time
waiting on i/o similarly for these
calculations by moving the information
into memory all of a sudden those three
billion clock cycles became usable not
all of them because in this case now
we're in an SMP system we had to wait on
the the main memory of the box but much
much lower latency so let's look at some
pictures
I'm gonna just start with where we
started back in 2001 where we started
with full replication the customers that
we were working with you know we'd be
talking to architects developers CTOs
whatever and they'd say well if we could
just keep the information in memory keep
it in sync across these app servers we
wouldn't have this problem we wouldn't
have to go to the database all the time
so he said okay we can do that so we
built a product and people bought it and
this is where we started we would just
replicate information across all the
servers so this picture is almost as old
as the product if you can't tell by the
beautiful antiquated choice of colors
and basically what it shows is an
inference of information in this case a
unique key of a B C and D just for the
fun of it
accessible by all the servers so here we
have four servers each one is able to
access that information locally in fact
this scales beautifully right if I can
do a thousand Reed accesses per server
and I have a thousand servers I'll still
get a thousand Reed accesses per server
the only problem is on the right side it
scales abysmally because now I have to
update 999 other servers which it
doesn't matter how you do it doesn't
matter if you use deterministic
broadcast doesn't matter if you use
multicast doesn't matter how you do it
conceptually it's expensive best it's n
log n I mean it's AD pass deterministic
a good deterministic broadcasting
algorithm so that's actually not the
major point though the major point is
going from two servers to 2,000 servers
my capacity for information stays the
same if I have this much information and
I fill my heap with one server that much
information is gonna fill my heap no
matter how many servers i have so so we
haven't solved the scalability problem
here except for very small sets of
extremely high read information so what
we did notice the new pastel modern
modern Millenia color is being used here
so he did is we partitioned the
information so same single system image
same API same everything but now behind
the scenes whatever information there is
is being spread out load-balanced if you
were let's pretend just for sake of
argument even though this is purely
pretending that there's some magical
system we built called a data load
balancer right so instead of load
balancing HTTP requests we
data at it and spreads it out magically
across the service in fact it's really
good because not only does it spread it
out across the servers but somehow it
magically keeps track of where it's
spreading it out to so if we go back for
it it'll send us to that server and not
only that if it decides that too much
it's going to one server it will
actually somehow load-balanced it and
remember all that - so there's some
magical pie-in-the-sky thing that
doesn't actually exist that's doing all
that and it's somehow doing it across
all these service and no central
directory no master server nothing like
that it's got completely at the network
level it's completely peer-to-peer
no no hierarchy or master server but
basically what it does is it provides
the same logical view of information it
looks like all the information is local
this is the lie right oh yeah sure I
have four pieces of data now to say I
have four pieces of data I had to ask
three other servers because I don't know
what they have or don't have and in fact
by the time I get the answer back it
could have changed okay but I can I can
answer the question yes I have four
pieces of data and if you ask me to
iterate through them all iterate through
them and you'll see them as if they were
local but there's a cost to doing that
so what we've done is we've gone from
free access locally from the frontal
replicated case and here we have a fixed
access because we have to go to another
server drag the data back deserialize
you can give it to the application fixed
why do I say fixed because it's one hop
there one hop back unless it's currently
being load balanced with server died
it's it's one hop there one out back so
in terms of you know Gigabit Ethernet
off the shell if you're talking 76
micros to the switch 76 micros to the
server 76 micros 76 micros back couple
thread switches you know 700
milliseconds top end for for simple
access then of course it's deserializing
using Java serialization so I had
another 45 seconds and you get your
object
I meant 4.5 milliseconds sorry it's
actually a little faster now in the
latest JT case updating information well
it sends it to the server that owns the
data so that magical load balancer that
doesn't exist it knows who owns the data
it's an unambiguous ownership at all
points in time there's exactly one
server one an only one server that owns
that information and that server because
it owns it has to make sure that there's
a synchronous backup or however many
backups we've been to
to manage for this information so it
until it exists on both that owner and
whatever backups there are it doesn't
exist and at that point that it does
exist this operation is complete and
even a failure occurs in the middle of
it which I think I have a slide for
somewhere no guess not they killed that
slide so even if it dies in the middle
of it this operation actually still
finishes and still finish this correctly
partitioned because it's partitioned if
I go from two servers to two thousand
servers but the amount of information
that actually can be managed goes up
linearly notice I said from two servers
to two thousand not from 1 to 1000 so
it's a because if I only have one server
and I go to two servers I actually don't
increase the amount of information I can
manage because I now have a full backup
so one of them owns 50% and backs up 50%
on the other etc also because of this
you know magical non-existent load
balancer I know I can go directly from
any server to any other server so all
the communication on the wire is
completely point-to-point which for us
is is pretty efficient we use UDP which
is just one skin-graft greater than IGMP
itself so so very very efficient
ownership is also extremely handy that's
non-ambiguous ownership because it also
means there's someone responsible not
just for managing the informations but
when we start looking at pushing logic
out to the information that's the owner
that's responsible for doing that work
so it's as if every piece of information
whether or not it even exists right
because I can also do processes against
information that's not even there like E
there's no e here but it it would land
right here
I don't know I'm pretending I know but
if it would land right here it's as if
there's a queue there for e with all the
operations that some wants to do against
E so all the concurrency controls done
locally etc so everyone knows that if
there's any operation against a there
would go to that particular server and
as you can see from these backups
there's always a plan in place what to
do if a server dies so everyone already
knows that if that if this server died
any e operations would always go here
or wherever some some other server so
achieving this before I go on to the
next site achieving this non-ambiguous
'no siz actually the probably the
hardest part of what we do I mean the
actual information management is
relatively easy relatively easy means
just you know on the order of hundreds
of thousands lines of code as opposed to
zillions but knowing unambiguously
without having some sort of central
service etc knowing unambiguously what
servers are there when they die who's
responsible for what you know being able
to do those things non ambiguously so
it's a being able to know that an answer
is correct being able to know what to go
and failover things like that that's
actually a the difficult part so what
sits under this is a clustering protocol
that we developed called T CMP which for
those of you who have worked with
distributed well I know you have the
distributed systems for years it's it's
actually quite inverse to the concepts
behind traditional distributed computing
traditional distributed computing each
server basically fens for itself what we
built is something where we consider
each server to be completely disposable
and we don't actually care if it dies
and it's what I refer to at least from
from a marketing perspective as a
organic clustering meaning it's it's
actually an organism it's a
multicellular organism it's not a whole
bunch of single-celled organism it's not
a whole bunch of amoebas trying to eat
each other it's actually one living
organism where each of the cells plays
some role within it and are completely
disposable so you know servers are not
trying to protect themselves from
failure they're actually trying to
protect the organism and so it's it's a
what it means is that the services that
we mount on top of it are designed
explicitly what they're communicating is
the necessary information to make sure
that they stay disposable so at every
point any server can be killed there's
no central server there's no master
server there's no whatever I mean it's
all servers are equally disposable at
any point in time
near topology basically is if you think
of the partition topology as being like
an l2 cache your topology is like an l1
cache right this is something that sits
closer it does two things first of all
if I go and get this information and
bring it back and store it locally I'm
sorry so this is what we had before the
logical view of all the partitioned
information I'll store it locally so if
I ask for it again I get it locally so
now I've returned to the latency as I
had with replicated caching think of
this is partially replicated caching
demand-based replicated caching now how
do I get rid of this information well if
it changes I may not want to get rid of
it I may want to keep around and I don't
care that it's stale that's what we call
ex-priest it's like Jack keep it for
five seconds one of our online gambling
customers they have a near cache that
times out in like a second and a half or
two seconds and the reason they do that
is because the amount of communication
event based communication to keep it
invalidated at peak loads on the site is
actually greater than what Cisco can
handle just the number of packets is
overwhelming at least prior to our our
bundling our packet bundling and
implementation and so what happens in
that case is you know it's actually
counterproductive to cache it so they
they catch it for a second and a half
which gives you good enough visibility
and then on a user by user basis this
session will actually overlay so if the
user does something that affects the
information before the transaction
completes it actually puts it into the
session hiding the cache data so the
user sees their own activity until the
update actually comes back through yes
so the question is what's the latency
for access here so remember this is all
when I say zero latency access I mean no
network activity no IO no
deserialization no so it could be it
could actually be 40 seconds because you
could accidentally just ask for the
information then hit a GC so you know
zero latency access in this case could
be just by act of God you know 40
seconds or whatever generally speaking
it's you know roughly analogous to
accessing some local data structure you
know the API looks like a hash table you
know it makes everything look like a
hash table so you know it's roughly
analogous order magnitude similarity in
terms of performance characteristics so
zero latency of course is not zero from
the point of view of the CPU designer
it's zero from the point of view of the
application designer so do you design
CPUs err no okay so yes it's not exactly
zero latency it's just relatively zero
latency
events so because you have an owner if
information changes it knows who's
interested in that change and it sends
out events to say it change so you can
find out what it was what it changed
from to everything else you can listen
to all the events happening to a
particular data set or you can listen to
particular identities you can filter
them you can say I only want to see when
things get inserted I only want to see
when a particular item gets deleted
whatever it is the filtering of course
occurs on the backend eliminating the
network traffic and you can even listen
to a query which is really cool because
we do parallel queries so if you issue a
query it spreads it out across all those
servers they're all doing the local
portion of that query coalescing the
results on the issuing server and then
the end result if you combine that with
the events on the queries it creates a
continuous query meaning you basically
issue a query you get back a data
structure a local data structure a
materialized view of what you've queried
and that materialized you stays up to
date and by the way it's the same API is
everything else we already looked at so
you can actually sign up for events on
on the local on the continuous query you
can query yet you can actually put a
continuous query on a continuous query
so concurrency control this is just
explicit concurrency control in the
first case the ability to you know if
you want to boil it down to the simplest
possible way of understanding it we
manage in this picture we manage a key
that's a we manage information that's a
red looking blob thing but there's also
other information the system manages
with it such as who's listening to
particular events that's how we did the
previous slide we also manage who owns
it from a point of view of pessimistic
concurrency control so all the
information related to particular
information in the system is managed
almost in the same way meaning it's all
synchronously backed up so you lose a
server you don't lose events you lose a
server you don't lose locks you lose a
server you don't lose your data but
basically it means that we can very
efficiently and scalable implement
concurrency control transactions roll
that up to you know units of work larger
than individual items so optimistic and
pessimistic transaction
etc read through and write through
basically what this means is you have
some underlying system that this
information is being pulled from so if
you don't find it in your near cash in
your l1 and you don't find it in your l2
we just call the application back and
say hey do you have a for me the
application sits on top of hibernate
calls hibernate hibernate goes off to
hibernate toppling your own day or
whatever code you want to stick in there
it goes off the database gets it Maps it
into an object we don't want to deal
with any of that stuff so once it's an
object though we'll take it oh thank you
for the object we'll store it here we'll
return it back the nice thing about this
is if I have a whole bunch of threads
here asking at the same time for a only
one request goes across the network if I
have a whole bunch of servers that come
here as asking at the same time for a
only one request goes to the database
and actually in the latest release it's
even less than that because it actually
automatically Tunes itself based on
based on access patterns it'll actually
coalesce multiple reads into one so you
know the higher the load the less number
of database reads per cache miss occur I
think that's correct
basically it starts coalescing them
together into larger and larger queries
right through basically does the same
thing in Reverse it writes it first to
the database once it's in the database
it commits it to the cache it's not a
two-phase commit though remember
two-phase commit means that we have to
get to a point on the on the transaction
control where we have to either give up
and halt the system or choose the wrong
answer half the time right so in this
case what we do is we require item
potency and this is I should probably
have a slide I may have a slide in here
on it idempotency is actually the Silver
Bullet of distributed systems
idempotency means in software terms it
means you can do the same thing ten
times a hundred times or one time and
have the same result you can think of it
in terms of like an update where from a
database point of view oh no I have a
version two I'm changing it to a version
three so on update that information do
all this work where key is this and
version is two
right so it's if I run that update a
million times it's only going to change
the information once so I assumed it
would be once just based on in fact I'm
not going to explain the entire example
but in other words item potency is as a
pattern of development where you build
chunks of functionality that can be run
an arbitrary number of times why is it
important here because this server could
fail before or after the database
commits the transaction and nobody knows
well God knows anyone here God okay so
you have to actually build a system that
has to keep running with you not knowing
what happened to that server did it
commit it did it not commit it you just
don't know the responsibility fails over
to the other server so it now has to
write data out but it doesn't know if it
already happened or not so the only
thing it can do if you want to eliminate
the two-phase commit conundrum is repeat
the same operation so it can only do
that if the operation can be repeated
blindly because it doesn't know if the
operation already went through or not so
idempotency becomes a silver bullet for
scaling reliable systems so in this case
it fails over it writes out the data
again again meaning item potently
non-destructively so if the other one
committed already then this does nothing
if it didn't commit already
then it does the update and so the end
result is that we can eliminate
two-phase commit in this scenario now
right behind caching is actually slower
than no caching I'm sorry right through
caching is actually slower than no
caching because it not only has to write
to the database but it has to manage the
cache so why would you do it well easy
because if you're using read through and
write through together the cache becomes
a veneer so if the writes are low
frequency enough the cache always will
stay up-to-date because the writes are
going through the cache so you're paying
just a very little bit of extra to
manage the writes and then the reads
stay free so this is as opposed to the
cache aside architecture where you would
check the cache if it's not there you go
to the database you pull from the
database you stick it on the cache
yourself that's that's what we that's a
traditional
approach this actually interposes the
cash between the application and the
database and as such it actually can
manage the consistency across all these
servers of that information right behind
basically inverts what we just looked at
was right through it writes it to the
cache and doesn't care if it's in the
database so you change it once it's in
the cache change it 100 times latest
versions in the cache an hour later it
rates the latest version out to the
database so if I'm changing a hundred
times I only have to do one write to the
database not only that if I change 100
items a hundred times I can batch it all
up write it all out to the database and
one write you know think about session
management in fact the original customer
we built this for was Geico this feature
was driven by some of the requirements
on session management their sessions are
not HTTP sessions their user sessions
for the lifetime of a customer and so
they would load the information of a
database store it in memory manage it
there and then write it down to the
database later and they had a session
server that they built to do this well
it only had to crash a couple times as a
single point of failure before they
figured out that was not a good idea and
in basically their system evolved to the
point where every HTTP request would
load it from the database parse it
because it was XML of course change it
and then write it back out to the
database database under load you know it
would read a structure that size would
take about 200 milliseconds let's write
it back out would take about 800
milliseconds actually over 200 over 800
so you had a second per HTTP request for
nothing
absolutely nothing well other than
making sure it was in the database and
so the other thing was I mean their
database was running then at a hundred
percent utilization during the day you
know when people are actually using the
system which gets really expensive so
what what what we did since we can
manage the information resilient lis
across multiple servers eliminating the
single point of failure we would take
those sessions store them in memory and
then an hour later we'd write it out to
disk now how many customers would
actually come along and do that many
changes within you know would stay on
the site more than an hour so basically
we would read at one time and write at
one time regardless of what they were
doing and the end result was you know
something like 90 something percent
reduction and load on the database and
the the latency I mean we just
eliminated that that that thousand plus
millisecond latency because the access
coming out of the near cache was zero
close to zero latency and the the
updates were just the cost of the XML
serialization itself which was about 23
to 25 milliseconds so the end result was
you know extremely well extremely
scalable through the partitioning and
extremely efficient you know from a
performance point of view and of course
I should point out the reason that we
keep two copies is so in one server
fails the other takes over the
responsibility to write it out to the
database and once again you have the
same problem was it already written or
not you can't be 100% certain so again
the same item potency requirement for
the same reason so what I want to talk
about now is moving behavior so the
first two examples on this slide are
what I'll call old school the first one
is interesting you know the formatting
on this is all right yes five minutes
all right this is the last interesting
slide I have basically the the first one
shows how to basically send it to a
particular server I want to send you
know the server of their send processing
to that server it's basically useless
because everything else we do deals with
State not with servers so the only thing
that we use is for internally is JMX
because you manage servers not not state
with JMX
the second one is you know work
management so you create tasks you throw
them and it does it we do this in our
product that gets used it's really handy
but you know there are a million things
that do this as well
JMS for the MDB is you know in a lot of
cases just as good the last one though
is this pushing information I'm sorry
pushing logic to where the information
is and this is this is what gets really
cool so let's go back to our one Oh
example of how to lock information and
update it here's a sequential counter
implemented in a grid I'm going to lock
it I'm gonna get it I'm gonna change it
I'm gonna put it back out and I'm gonna
unlock it if you know the Java memory
model this is hardly different from
synchronized you
access the field add one to it put it
back out leave the synchronize block
monitor enter on the way in monitor
accident on the way out there you go
what happens though is this lock
remember that lock is information that
we have to store on at least two other
servers so it's hop to the primary hop
to the backup hop back to the primary to
verify it hop back to the requester now
we're gonna get it to the primary and
back now we're gonna update it to the
primary to the backup to the primary to
the requester now we're gonna unlock it
to the primary to the backup to the
primary to the requester that's fourteen
hops that's a lot of hopping so what if
instead we just sent the whole thing
inside the Curly's where did you send
that over ship it over so we don't have
to lock it because it's lining up in
that virtual queue sitting on that
server for that identity it's just sits
there in a queue waiting to be run so I
could have 10,000 servers doing this at
the same time it shows up there it gets
run and all the running happens because
it's coming out of a queue virtually
speaking it gets run in order so so I
don't have to lock it globally across
all the servers it's it's locked by
nature of the data structure feeding
feeding the logic so now it's one two
three four it's to the primary to the
backup to the primary and back to the
requester so for hops instead of 14 72
percent reduction in hops very
importantly those this in the first
example from the time I lock on the
primary to the time somewhere during
this unlock I don't know which steps so
somewhere around 12 Network hops out of
the 14 no one else can do anything with
that identity hey it's like synchronized
right
I said no one else do anything with this
is pessimistic so I've if those twelve
hops took say two milliseconds total I
just cut my transaction rate on this
identity to 500 a second doesn't matter
how fast I make my computer doesn't
matter how many computers I have 500
that's it that's it if I can eliminate
that critical section which is what we
do with the bytes by sending the
processing into those queues all of a
sudden I can just line them up and start
just chewing through them so this is one
of the ways that we achieve some of the
action volumes you know 100,000 plus per
second sustained I mean because we're
spreading the transactions out there all
cued there's no network critical section
time and it's just amazing
not only that but if I'm processing
multiple things multiple pieces
information it's doing it in parallel so
here you know just a set of identities
to do it to or a query to do it to so
automatic parallelization still with the
once and only once guarantees which is
pretty fun so those are all scalar
functions so thinking in terms of you
know say database Theory incrementing a
value is the scalar function we also do
aggregate functions so this is a
parallel aggregation capability you know
I want to go out and you know things
like distinct some average etc these are
all things that occur just like the
parallel query they occur as much as
possible locally with the information
partial results come back and are
tabulated by the requesting server and
all these capabilities the processors
and the aggregation etc these are all
quotable you know that is you don't have
to just use the ones that we provide so
you can basically think of what we do is
as example code so I have used cases and
stuff but I'd like to switch and see if
there's any questions at this point
because I think we're basically out of
time that's all the interesting stuff
anyway
any questions yes
so how do you coordinating updates to
conflicting conflicting updates two
pieces of data that live in different
places when they're happening
concurrently well the answer is they
don't
well the answer is the data doesn't live
in multiple places there's one master
copy of that information and it is
managed by the grid as a whole I mean
specifically at some point in time at
each point in time there's one server
that owns it and one or more servers
that have a synchronous backup of it but
all the updates that information occur
in serial on the owner of that
information how do you ensure what how
do you ensure that the two rights belong
to the same owner the rights don't
belong to the same owner the data is
owned by a particular server so the
rights simply go to the server that owns
the information
is that the same question you're asking
so how do you update two pieces of
information or chronically that are
spread over two servers well
fundamentally it's impossible so it's
it's a by partitioning it across
multiple servers that level of
theoretical to atomicity cannot exist
now you can simulate it so that the
application thinks it's entirely atomic
you know there's the all I can think of
is Gordian not know it's a Lampert clock
there's a Lampert clock approach
multivalued concurrency control etc but
in reality there is no such thing as an
atomic operation across multiple servers
so we don't we don't even attempt to
pretend that there is what we do is we
actually because we created the
clustering protocol itself we got to
define the laws of our physical universe
and in the laws in our case are based on
inorder reliable messaging from each
point to each point so it's a fully
peer-to-peer system so if I have
endpoints from every point to all N
minus 1 points
I have inorder message delivery and they
are reliable well guaranteed for the
lifetime of the as a member
well the way that we would do the Train
the way that we do the transactions if
it's an optimistic transaction one of
them or potentially both of them
rollback both of them would roll back
only if there's a deadlock if you were a
potential deadlock and in the
pessimistic locking that occurs as part
of the transaction two-phase commit in
the pessimistic case one of them would
either wait or rollback
so I mean we do support transactions
it's just as a higher-level construct to
achieve the transactional guarantees did
I come close to answering it okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>