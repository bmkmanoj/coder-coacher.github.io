<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Performance Tuning Best Practices for MySQL | Coder Coacher - Coaching Coders</title><meta content="Performance Tuning Best Practices for MySQL - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Performance Tuning Best Practices for MySQL</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/u70mkgDnDdU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hi guys so this is Jay pipes he is
a my sequel employee he's in charge of
North American Community Relations I met
him this week at the my sequel user
conference which is down in Santa Clara
now the deal is that we're going to host
this on Google Video so if you have
confidential questions if you could
leave them till the cameras are turned
off that'd be cool with it no more ado
Jay pipes all right um
nice to meet all of you I'm going to be
giving a talk that I gave on link
Tuesday Wednesday on performance tuning
best practices for MySQL some of the
things are version specific but for the
most part it's pretty general we're
going to be covering profiling and
benchmarking core concepts sources of
problems that you might encounter index
guidelines schema guidelines coding
techniques and lastly we'll go over some
general things about tuning server
parameters
since the audiences I think sort of
mixed levels of experience will just
just shout out questions or tell me if
you want some more advanced stuff
all right benchmarking concepts the
general idea behind benchmarking MySQL
is to obviously have a track record you
know when you change things in your code
in your application or the configuration
files with MySQL or your schema design
or your indexing you want to make sure
you have a track record of what you're
doing because a lot of times if you if
you don't keep a track record and don't
do benchmarking you won't really know
whether a specific change that you made
had a detrimental impact or whether it
actually increase performance so the
baseline in you always want to have your
baseline for your starting point and
always give yourself a target don't
don't just do it so it's a moving start
moving target you always want to have an
end goal you know you either want to get
you know 20% increase in performance
that's your goal but just don't have it
you know just kind of end with nothing
because then you'll just spend years and
years and years trying to
you know tune milliseconds out of your
out of your application when there's
really no benefit to it so change only
one thing at a time pretty basic concept
but it really it goes a long way in
benchmarking if you change two things
and you rerun a test how do you know
which one of those two things actually
affected the the difference between the
two tests so always change one thing at
a time and rerun the benchmarks and
record everything configuration files
information on the hardware information
on obviously the operating system the
the version of MySQL and everything to
do with the benchmarks keep it in a
separate folder and keep everything for
for a historical record a lot of times
you'll think that you won't need
information when you're running
benchmarks and then six months later you
realize that Wow it would be really
useful if I had oh I threw it away
so always keep information from your
benchmark so you have a historical
record of what you did
and specifically with MySQL you want to
disable the query cache whenever you're
running benchmarks the reason is it it
will skew the results that you get from
read intensive queries and write in
write intensive queries as well so
disable the query cache by setting the
query cache size equal to zero when you
when you run benchmarks so proof yeah
good
problem with calories
right yeah the the question was what you
were experiencing that the operating
system caching was was skewing the
results of the benchmark generally the
the idea is either either you can
disable caching if it's actually getting
in the way or if you bump up the actual
number of runs in the benchmark test you
can minimize this the skew effect so
that's usually what we recommend but
when you get into specific caches
there's very specific things to do so it
would kind of depend on on the exact
scenario that you're that you're doing
so but yeah bumping out the number of
runs in the benchmark generally tends to
mitigate a lot of that so does that
answer your question
okay profiling concepts profiling is a
little different than benchmarking
generally you're profiling a production
or a running test system that you're
trying to figure out you know diagnosing
what's going on while it while it's
running so instead of doing you know
finding the the stress or the load that
the system can take you're actually
trying to pinpoint where the bottlenecks
are and in the code while it's running
with MySQL the best tool that you have
is the explained command so regardless
of what version of MySQL you're running
you really want to get familiar with
everything in the explain command
understand what the access types does
everyone know what the explain command
is everyone use it okay does everyone
know what every single type of access
type is okay it's sort of a poorly named
column in the act in the explained
results but it actually describes the
path or the optimization pattern at
which the optimizer is saying okay well
I'm going to you know access you know
this result set and join it to this
result set and it then each of those
things the ref the refferal the equi ref
range all the different access types
mean different things and and the
performance you get from each each of
them you can go into MySQL manual
depending on your version you can see
which ones have a better performance for
different types of queries it's not
always the same so you want to make sure
you understand those
understanding obviously that if you see
all in the access type it means it's
doing a full table scan you know just
little things like that be aware of what
what they all mean and what the
performance impact of the different ones
are so using the slow query log and
MySQL dump slow to parse the the query
log very easy to setup log slow queries
and then you give it a long query time
anything above that it'll log to the
slow query log there's also I think in
for one we we put in an option that you
can anything that any table any select
statement that does not you use an index
on the table can get log to the slow
query log regardless of how fast it runs
and that's and that's useful for when
you're doing an indexing strategy and
you're trying to determine ok my
patterns of access do I do I have
indexes on those fields that I'm
actually accessing you know in my where
clause and on clausal and that kind of
thing so low-hanging fruit law of
diminishing returns tackle stuff that's
going to get you the absolute best
return on investment for your time don't
concentrate on you know I know you guys
are Google so a millisecond definitely
does count but if you've got if you've
got a choice of you know going from five
milliseconds to four milliseconds or
going from a minute to you know a second
it's a no-brainer you know concentrate
on the stuff that that's going to give
you the biggest impact using my top
jeromy's rodney wrote a nice little tool
mostly probably familiar with the top
utility in unix it's almost identical
but it's just for MySQL and it gives you
a an ability to on a delay on a
repeating delay you know show you the
process list of what's going on in MySQL
and show show varial show commands the
status status variables and gives you a
nice little interface to to profile into
the system while it's running and the
one nice thing about my top that I
really like is that you can you can use
it for remote hosts as well so you can
you can run it on your local machine and
access a remote host so that it is a
nice little tool okay
sources of problems this is kind of my
personal opinion of where I rank the law
of diminishing returns like what I
actually like go for immediate
léa the performance team tends to think
that you know sometimes you can you can
tune a server parameter and that's like
the biggest bang for the least you know
amount of time but I think really where
performance problems come in are the are
the definitely the top three here poor
indexing choices will kill an
application faster than really anything
else if you're querying on a field and
you don't have an index on it and you're
repeatedly hitting that it's going to
kill the application faster than you
know whether sort buffer size can be
adjusted so same thing with the
inefficient or bloated schema design you
know I talked in the in the performance
dog I've seen a number of cases where
people designing new applications will
actually denormalize from the beginning
because they think it's like going to
give them better performance don't do
that you know there's there specific
cases where you can denormalize a schema
and you can get some performance
benefits but don't do it right away you
know just because you think it's going
to get you know a performance benefit
use good efficient normalized schemas
and we'll talk we'll go into detail on
this later
bad coding practices MySQL has some
inefficiency problems with sub queries
but in general using joins is a lot more
efficient than using sub queries we're
going to go show some examples of
specific where you where you can really
get a performance increase by just
simply changing the way that you the way
that you code SQL server variables not
tuned properly add to source of problems
but in my opinion very specific
situations can be can be helped by by
tuning server parameters it's not some
catch-all you know it totally depends on
you know how much memory you have in the
machine what the architecture is what
operating system you're running on what
version of MySQL what storage engines
you're using your you know the type of
application that you have whether it's
OLTP or heavy reads you know all these
things yes
tuning server parameters can help but
it's very specific to certain situations
and it also requires retesting and
rerunning but you know you tweak one
thing you have
you know run the benchmark test to see
if see if you got the performance gain
out of it it can be time-consuming to
tweak server variables whereas it's
pretty easy to you know change some some
seek some SQL statements and immediately
we run it and see if you've got a huge
performance gain from doing that
and finally hardware and and what n
network bottlenecks io CPU bound network
license latency I there is a good book
that that I brought along that I
recommend for that and when we get to
the last slide I'll I'll show you that
so all right indexing guidelines poor or
missing index definitely the fastest way
to kill an application like I said look
for covering index opportunities we're
going to go into some slides here too to
kind of explain what that is but a
covering index is especially for who's
using my ice and versus nodb nodb
raise hands my I Sam raise hands nobody
is using my I Sam for a single thing
okay say again ah gotcha
okay well we'll explain some things
about covering index opportunities that
are very specific to nodb
but covering indexes basically what it
means is that the mysql can get all the
information from the index records which
are by definition slimmer than the data
records because they don't contain all
the fields in the table and use that
information to complete whatever the
query was without having to go into the
data records one of the the key concepts
is the slimmer that your index records
are the more that you can fit into a
single index block and the more that you
can fit into an index block the fewer
reads you're going to do the faster your
application is going to be so covering
index opportunities definitely look
around for those when you're deciding on
which fields you actually want to index
make sure you take a look at what the
selectivity of the of the field is
selectivity is the cardinality or the
number of unique values that are
contained in that field divided by the
total
number of values in the table so
obviously a unique index would be a
cardinality of one because it's you know
unique it over the total count that's
obviously the best-case scenario but you
can get situations where you've got
awful selectivity if you're querying
just on that index or just on that field
you may get awful performance from that
because or it may just just decide not
to use the index because it ceases to be
useful I think generally like team work
could talk a little bit more about this
but I think the the numbers about 30% if
the optimizer sees that okay you do it
you do a query select you know start
from whatever where column a equals two
and it'll take a look and see you not
and an estimate from what the
information that has on the on the index
will this particular query return you
know around 30% or more of the rows in
the table if it does it's actually much
more efficient to just sequentially scan
the table and it is to go into the index
and randomly seek and grab a bunch of
records and then do lookups back to the
data records so be aware of the
uniqueness of the values in your index
for lower selective fields where you
don't have very very much selectivity
you can sometimes tack them on you know
to a multi column index so that
especially with group-by queries and
we'll show an example of this you get
some efficiencies there but just be
aware of the selectivity so on
multi-column indexes like it was kind of
explained just now the order of the
fields is very important you have to
look at the queries that you're running
against a database you know if if you
have the the columns mixed up or they're
not optimal the optimized you just won't
use the index and I'll show you a
perfect example of something that
happens all the time with this on group
by queries and as the database grows you
want to ensure that the distribution is
good you know if you set up an
application writing new application and
you know you go through a couple months
you got a couple hundred thousand
records in there okay great everything
looks great the index strategy that you
design for it is wonderful but that
doesn't necessarily mean that in nine
months you know you've got millions of
record
in there that the index strategy that
you initially picked out is going to be
optimal you know make sure that you
understand how your data changes over
time
you know if business practices can
change and so if you've got values let's
say you got seven values in a field
right and you've got an index on that
seven distinct values that you're
putting into like a status field okay
and over time ninety percent of the
field values contain one specific thing
that index ceases to be as useful as it
was when you initially designed the
application because 90 percent of the
the rows contain the same value so just
be aware that as your database grows and
your application changes that you need
to re-examine you know what you
initially thought was a good index haiji
okay remove redundant indexes for faster
write performance obviously every index
that you put on a field you get a you
know slight performance impact on writes
because it's got a right and index
record as well as the data record every
time you do an insert and every time you
update if you you know update the key
value it's got a right to the index so
unfortunately - quelle allows you to
create redundant indexes on the exact
same things so I've seen cases where you
get 40 40 indexes on a table and you
know ten of them are are redundant
you know objectives over time teams of
people work on it and no one wants to
change other people's stuff you know so
they just add their own indexes and with
different names and all that but they're
redundant they're on the same fields so
you know as time goes by make sure you
check remove redundant indexes to get
some write performance okay common index
problem here kind of a web 2.0 example
you got it you got a tags table which
got tag ID and text for the tag and a
products table both of these tables have
an auto incrementing primary key and
then you've got a relationship table
that stores the many to many
relationship between products and tags
and if you notice that the primary key
on this products - tags table is product
ID tag ID make sense pretty basic
okay here's two examples of pretty
common queries run on this products -
tags table only the top one is going to
use in the index on it and the reason is
because of the order of the columns in
the index on the top one your grouping
by the what's essentially the product ID
and so it's going to be able to take
that left side of the index and use it
in order to count the right side value
which is the tag ID now on the bottom
you're reversing that you're saying for
each distinct tag give me the give me
the count of products it can't use the
index can't use that primary key index
so it won't use it yet no read read
performance from it
the remedy would be to create an index
that has tag on the left side okay now
there's two examples here where it says
create index the top example create an
Josiah X tag on just the tag ID for all
you guys use an n o DB that's exactly
what you'd want to do for my eyes am
you'd want to do the bottom one to
create a covering index opportunity does
anyone know why the top one should be
used for nodb and not the bottom one
what's the difference between the
secondary indexes in in ODB versus my I
Sam exactly for it because it's a
clustered index organization in nodb
every time you add an index on a field
that's not the primary key every non
primary key index the primary key is
actually appended to every single index
record so if you've got a primary key
product ID tag ID and you add an index
on tag ID product ID you're being
redundant because you've already got the
product ID actually in the index record
in nodb so it's important to understand
what the difference between the clusters
organization and the my I sin which is
none on clustered where it's just a
record number that's it that's attached
to each index tracker instead of the
actual clustering key so be aware those
differences
so when I was talking about redundant in
it indexes that one on the bottom will
be redundant for nodb did the mic just
change okay okay sorry schema guidelines
inefficient schema another great way to
kill your application use the smallest
data types possible that you really need
if if you're if you don't have 4 billion
rows in a table don't use big int I mean
it's just the basics you know keep them
as trim as possible you know if you have
maximum of 100 values don't use int use
tiny int you can fit you know 255 unique
values in it
don't use kr200 you know when a ver care
or you know a smaller care will do
little things like this make a huge
impact in performance and the reason is
remember that every index record has got
to contain you know that width so if
you've got an index on a care for tea
every single index records got 40 bytes
in it and well a little bit different
because there's some index prefix
compression that's going on but you know
the general concept stays if you have an
index on first name just index the first
10 characters of it or the first 15
characters of it you fit much much many
more index records into a single key
block and therefore you get fewer reads
faster performance when you've got many
columns in a table especially many
nullable columns consider splitting the
tables into two tables like a main and a
detail table or something like that if
you've got many nullable columns or
columns that are rarely even accessed or
rarely searched on but those in a
separate table and join them on a
one-to-one in a relationship okay
and the reason for that is you can pull
many more of the main records that
you're actually searching on into memory
you get especially if you don't need the
detail records are those fields you can
have a lot more opportunities to to get
more performance from that consider
vertically splitting many Road tables
using partitioning or the myosin large
tables which since you guys are using an
ODB doesn't matter
partitioning is in 5:1 so guys are
little ways away from that alright you
can also do you know your own
application level vertical you know
partitioning where you're naming
different sets of tables and then using
views on the one person who's using 500
you can use views in order to aggregate
many tables into a single view and
always remember the you know fewer reads
those faster results and and this is
this is my the limit of my graphical
ability right here that's about it it's
just you know a little flow smaller data
types leads to narrower index records
more records per block fewer reads
faster performance it's just the general
thing how to get better performance from
your indexes
there's schema in ODB choose the
smallest possible clustering key since
like I just explained it's appended to
every single secondary index record so
if you've got a 12 byte primary key on
your index in on your energy be table
that's 12 bytes per index record that
are being appended every single time you
insert into the table so be aware that
don't use surrogate Keys when a
naturally occurring one exists we see
this all the time in the forms and I
often wonder what what the use of it is
and why people do it
you've got a naturally occurring Product
ID tag ID key it's a naturally occurring
primary key yet a lot of people will
just kind of put an auto increment in
there just for the heck of it you know
but you're never really going to be
looking up by record ID you're always
going to be looking up based on product
ID or tag ID so some people call it a
surrogate key or you know a false key
get rid of them it's just wasting space
coding guidelines I tend to call it
chunky coding habits cutting everything
down into the smallest possible units
you can you know thinking of SQL in
terms of procedural programming is a bad
idea
it leads to well correlated sub-queries
and other things I hate you want to you
want to be an SQL programmer that thinks
in sets don't think in terms of for
loops okay
think in terms of merging and
intersecting sets of data
and you'll you'll you'll be able to cut
it up into chunks and you'll be much
more efficient the way when the way you
could SQL the five-o guy use store
procedures if you're not using store
procedures you're missing out on a
performance increase in 5.0 in ODB how
many of using select count star on in
ODB tables I know there's song you okay
don't nodb cannot optimize select count
star queries okay so your table and your
schema will will not scale well okay the
more records you get in there it
requires actually reading through and
counting all the records
whereas with my I same it can actually
get the count from the index within ODB
it cannot it actually has to count all
the values and the reason for that is is
the way the nodb implements what's
called multi-version concurrency control
and there's a version so that the row
level locking can occur in in ODB
there's a version for each row so
there's complexities involved with
actually counting you know the number of
values because well which version are
you are you comparing and so use counter
tables so instead doing you know select
count star products have a separate
table that stores the number of products
and when you insert into the product
table increment it when you delete
decrement it okay that's how to scale
nodb for performance we've seen this
byte a number of big form applications
that you know they always do you know
counter the number of threads and the
count of some how many users are logged
in this is a big problem when you know
if people use switch from my I Sam to
nodb
without looking at the code just simply
switch the engines you know it was a
yeah yeah well also consider you you
don't actually necessarily want to know
the exact number of rows do you care if
you've got a thousand threads or a
living under thread it really doesn't
matter you want a ballpark figure so
even if your number is not fully up to
date it doesn't matter if you want to
run a cron job that does the inner DB
count once an hour then put that value
somewhere you can put it in another
table you could put in a file that you
include
into whatever page is displayed that
works fine then you reduce the problem
to once an hour rather than every time
you run that query all right um
isolating indexes or index fields on one
side of the equation you know your basic
your basic math class you want to put
the single on the one side and have your
equation on the other you know well
we'll show an example of how to do this
in a second and using calculated fields
when they're necessary you know let's go
onto the example of isolating index
fields on one side of the equation okay
here we've got whenever you apply a
function to a field that's indexed the
optimizer can't use the index on it okay
so you want to look for opportunities
where you can rewrite SQL statements
like like on the top here into at the
exact same SQL statement or the exact
same result but you've you've isolated
the index field without any function
operating on it on you know the left or
whatever right and just converted the
actual statement into a just a different
way of representing it okay now in this
case you're just taking the two days and
converting it into current date minus
seven day you know just make sure you
understand the greater than and equals
and all that you know
so you're including it now what's wrong
with the second part well how can we
improve that with our performance anyone
know it's again
yes well yeah basically that the current
date function is a non-deterministic
function and the problem with that is
that the query cache won't cache this
query well the thing about current data
is it only returns a different value
once every what 24 hours so if you're
running this you know let's say a
hundred thousand times in a day instead
of running this once it's going to run a
hundred thousand times you know with the
query cache it actually puts the query
into the cache stores the results and
every time you issue that exact same
query it just takes it right other query
cache never even has to hit the storage
engine the might scales whatever it just
takes it right other query cache so in
your Perl script or your PHP script or
whatever you're using to write just
insert the date instead of using current
date okay a lot of people don't realize
this but very good way to get
performance out of your query cache okay
using calculated fields when you got
where email address like percent com
index can't be used the reason is
because you can't have a wild card on
the on the left side of a constant
comparing it with like if you put the
wild card on the the right on the on the
end and you have a constant string and
then wild card the index can be used
because it can look at the left side of
the the values okay so the way to do
this is to do a little trickery and put
a reversed email address actually in
your table okay with 500 you can use a
trigger that actually inserts the
reverse of the email address that you're
inserting directly into that new reverse
email address field okay and then on the
bottom you can just do the like reverse
concat and now you can act you now you
actually can use an index on that fuel
so that's a way of using calculated
fields or you know fields that you know
store different values based on other
fields in the table to get performance
out of something that you'd never be
able to use an index on you know
yeah I mean this is something you know
if you have an application that's
looking up in all the email addresses in
your database that end in calm or
aol.com or whatever it is you know
that's you don't want to be running that
top one every single time someone
requests that you want to be operating
on a reversed email address and using
the index on okay
learn to use joins like I said earlier
eliminating correlated sub-queries is a
great way to get performance out of your
application currently like team or who's
going to be speaking in a little bit is
on the optimizer team and he's working
on sub query optimizations but there's
you know there's just a better way of
doing things with joins okay it's it's
easier and it follows the set-based
programming principles of SQL instead of
the procedural language principles that
most of us are very familiar with you
know the for loop the operating on you
know everything one at a time okay so
now let's go into some examples here oh
don't try and out think the optimizer
team team works pretty smart you'll meet
him in a little bit but he's really
smart uh you can give the SQL server the
MySQL server and a joint hint using like
straight join but the problem with that
is while while you may get some
performance benefit if the optimizer
currently can't figure out the right
path and I've only seen this in like one
or two instances but in two months your
data set changes and the optimizer that
you've now overridden with your straight
joint hint it now you may have the
suboptimal plan for it and you forgot
that you put straight joint in there and
now you've got a suboptimal plan so if
you are going to use a joint hint make
sure you thoroughly document where you
put it in the code and don't forget
about it because as your data set
changes the optimizer could much more
easily pick an optimal path for the
query execution okay eliminating
correlated sub-queries with for one and
it looked like most of you are on for
one you can use sub queries I tend to
avoid them like the plague
we see this a lot on the forms this kind
of thing if most of you like advanced
SQL programmers would probably not do
anything like this but we see it every
day you just I mean the thing is to try
and get the the maximum sold price from
an order items table for each product
and the procedural programmer you know
who's a Perl coder doesn't necessarily
you know isn't familiar with SQL hasn't
been trade-in set based you know
thinking this is a very natural way of
doing something you're saying okay I
want my product stable I'm selecting for
my product stable okay and I want each
of those products and for each one I
want to get the the maximum price that
it was sold in the order items table you
know correlating on the product ID it's
a very natural way for a procedural
programmer to think of things you know
but remember it's for each product I
mean that's the way that people think
about it that's not the set-based way of
doing things on the right is the set
base way you're joining you're saying
okay I've got two sets of information
I've got my product and I got my order
items okay and I want to transform these
these order items by grouping them on a
product ID and then finding the maximum
price so that's my set the maximum
prices for each product ID and then I
have my product set and the maximum
price is set and I join them that that
actually is two operations right two
conceptual operations grouping and
joining to the products on the left
you're actually doing a query for every
single product so it doesn't scale with
a linear model so you have a thousand
products you're doing a thousand sub
queries to get the maximum price from
the order items table you have a million
you're doing a million queries you're
still doing two queries on the on the
right so think about how you can get rid
of correlated subqueries this is one of
the best ways to get better performance
out of your SQL code using derive tables
this might seem like a big mess but it's
actually more efficient on the right and
it goes back to the same principle of
how many queries that you're actually
issuing on on the Left you've got a
from customers to orders and in in the
where clause you have a correlated
sub-query with a group by in it okay not
a group I with an aggregate so you're
saying the customers join to the orders
but I want to group by the company of
the customer not the customer ID and in
order date equals the maximum order or
the last order day in the orders table
where the customer is the same is
correlated to your customers table see
and then group by the company well
you're going to be doing a sub-query
for this select match for each customer
in the orders table now on the right we
say okay I have this customer set and
we're inner join on what's called a
derived table a sub-query in the from
clause that's one set we're taking the
the maximum or the last order date for
each customer a distinct customer ID in
our orders table that's a set and then
we're joining back to the orders table
so there's three three operations three
conceptual sets to get that result
versus the non scaling method of a
correlated sub-query in the where clause
where which has to be executed for every
single customer in the orders table okay
alright and then this is my last slide
and I tend to kind of skim over server
parameters because it's I'm very happy
to talk specifics with with any one of
you but to generalize about server
parameters okay but they're very
specific situations where they can help
or you can get some definite performance
boosts be aware of which server
parameters are global versus what's per
thread per connection made to the
database okay for instance key buffer
size so it's a global variable sort
buffer size it's per thread so be aware
of that
don't you know assume that sort buffer
sizes for the whole database and bump it
up because it's actually per thread so
make small changes preferably a single
change to your configuration file retest
see if you get a performance benefit
from that
cific you know change often provide a
quick solution but they can be temporary
really really the the performance
benefit is is optimizing your
application your indexes your schema
your coding query cache it's off by
default so the size is zero for your
query cache so if you haven't changed it
it's not on it you can do shows to show
status and it'll say query cache type I
think it's clear cache type on but
there's no size to it so it actually
isn't used by default so unless you
turned it on you're not using a query
cache if you're doing read intensive
applications like how many you're doing
like lots and lots of rights versus lots
and lots of Rees who has 90 percent
reads or more okay do you have your
query cache just turned on
who has the query cache turned on that
they know they've added size to one guy
in the back all right uh-huh
- all right excellent okay if you're
doing read intensive applications turn
the query cache on why because it
doesn't have to have to execute the
query if it's the exact same query and
your data set isn't changing much it
just takes it directly from the query
cache the query cache actually stores
the results set it doesn't just like
compile the query and store that the
query so that can be reacts acute
actually stores the rows so that it
doesn't actually have to do any of the
parsing or anything in the optimization
it just sends the rows right back to the
client so if you're doing repeated
queries like that turn the query cache
on give it some size the in the
configuration file it's the query
underscore cache underscore size set it
to whatever 32 Meg's 64 Meg it can you
can play around with it you can also do
it online
I think it's set global query cache size
equal something or other it's in the
menu so you can do it online or you can
do it in your configuration file be
aware of since you don't use my SM
tables how many of you think that you're
not using my SM at all I hope not ok the
MySQL internal database is in my icing
ok so that's why if you do show status
you'll see actually that you have my I
Sam opera
patience going on so even if all your
tables are in ODB the the MySQL internal
database is in my I Sam so you still
need to put you know something in there
so leave the the defaults for key buffer
size there so the my so the MySQL
database can you know live happily
but the nodb buffer pool size that's
that's your main one for nodb
that determines how many how much memory
is allocated for storing data and index
pages in ODB I think Hecky recommends
about 50 to 80 percent of your total
memory on the box should be allocated to
in a DB buffer size okay other important
entity be stuff is the inner DB log file
size and log buffer size I think he
recommends that the log I think the log
buffer size maxes out at 4 gig but you
can adjust it depending on you know you
know what your situation is and I can
take some questions oh yeah memory is
the cheapest fastest easiest way to get
better performance out of MySQL don't
even ask your boss just stick another
piece of RAM in there it really is MySQL
just loves Ram you know the more ram you
give it it's like pac-man just eat the
stuff up it loves it so that's the
easiest and cheapest way to get better
performance from a hardware situation
yeah
okay the question was and if you've got
a logical key in a in a character field
right say for instance I don't know
social security number something like
that is there a benefit to making a what
you're calling a surrogate key out of an
auto incrementing ID there is a benefit
to that a big benefit to that depending
on the especially like if you have a you
know a care for versus an int you're
probably not going to see much of
performance difference but you've got a
you know care care aid care 16 that
that's you know your product ID or your
record ID or whatever it is
versus an auto incrementing key you're
going to get a lot better performance
from the numeric data type in in 4 bytes
it's optimized for that so
um okay there's that string data type
that we need just you mean versus like
Vera care 4000 or something versus text
or whatever there's some maybe our can
or you can talk to that I don't know if
there's a specific you know how much
performance benefit you get from doing
very covert text I think in a DB there
may be something but yeah in yeah well
so the question is Easter benefit to
using char rather navarch art in certain
cases in my 800 or text versus varchar'
in my item text fields are actually
stored separately texts and blobs the
actual row contains a pointer to where
the text view lives in case of char
varchar it's part of the row in a new DB
same up to a point in certain cases you
would just find everything in the row
depends on which version and so on in it
B itself doesn't care whether things are
charge of are charge it always
internally users are charge basically so
in any DB you don't need to make that
distinction
okay I want to optimize for fixed length
fields that kind of stuff
it doesn't care about that my I some
would care so that's the basic story
that's that answer your question
okay all right well thanks guys thanks
for letting us come here
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>