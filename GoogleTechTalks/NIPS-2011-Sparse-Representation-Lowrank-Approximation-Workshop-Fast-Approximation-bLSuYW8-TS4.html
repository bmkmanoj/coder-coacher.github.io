<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Fast Approximation... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Fast Approximation... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Fast Approximation...</b></h2><h5 class="post__date">2012-02-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bLSuYW8-TS4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's see so like them so matrix
coherence is something that's got
attention slightly different rates
different people are we talking about
how to compute equipment in fact you can
get an awful lot more using a base
technique including a whole suite of
things i'm going to call statistical
leper so let's let a justification a B
or n by D matrix I might have a slight
later where I slipped a man in but
anybody matrix and it's tall and its
large d is small so do it with tall
matrix all right the ideas what stem to
fab matrices its industry mean Galactus
look standard to l1 based penalties into
the results of 800 prefer the moment
i'll be talking about it in this
emotional content right the statistical
leverage scores are the diagonal
elements the projection matrix on to the
left singular vectors of a so a is tall
it's less singular vectors at home all
right give a tall matrix the columns are
unit length on the bottom of the rows
are not but this could be a bunch of
college and identity this could be a
bunch of palms of Adam are so these row
legs could be very uniformed very
non-uniform at anything in between and
the coherence of the Rose is the largest
subscore all right sometimes coherence
means the largest off diagonal moment of
that projection matrix which is equal to
not the largest you putting on the road
the largest got park sangoku for all the
balls so the basic idea of statistical
effort is going to measure the
correlation between singular vectors no
mazes matrix in the canonical basis
intuitively gives you how much influence
of leverage each row has on the best
links where it spit or food will be
where in the high dimensional space is
singular value information is being sent
not with that singular value
you know so relatedly the extent which
of data point is an outlier all right
how it's always good to ask who cares so
you might none you know given the slide
you may or may not know that you
probably do care actually all right so
this is probably something that touches
were cheated so density in statistical
data analysis and machine learning these
are the diagonals and so-called hat
matrix using regression Diagnostics for
measuring how much data point is now
alive if you kick the machine with the
date of generativity just have a total
outlier or not recently you know the
nicer met the matrix reconstruction
holes who these things typically assume
that these scores are flattening the
parameterize in such a way that you need
to sample k 1 k if u of D or something
lots of something times the coherence
right now if the coherence is small
that's not bad right and a questions in
a curricle questions to what the
appearance is why i vish an empirical
fact if you look at any of a wide wide
range of data don't mean while it is
that they're wildly YZ not all right so
that's a little disconcerting in that
the theory assumes if left so if you do
this you should care numerical linear
algebra has been a huge range of work on
randomized algorithms for matrices
didn't you heard but some of the server
here this understanding the role of
leverage here is the key structural
bottleneck for making those algorithms
impractical all right roughly if you
want to do random sampling algorithms
you use these as an important sampling
distribution to sample respective and if
you want to do random projection albums
you rotate to a base for those things
all right I can work with your science
before these things are useful and
practitioners and data
numerically Albert did not pick up on
this was only when Renee's might be
convoluted the randomization from
structural linear algebraic properties
then we'll chase these stores pop out
with these algorithms were implemented
America loyalists and users in machine
learning and data applications
theoretical computer sign this was a key
structural non-uniformity to deal with
the worst case analysis so these are the
various statistical things these are the
key non-employment it came to deal with
you could find in sample or wash away
the projection so how do you compute
these things do SVD or QR six high
dimension x lo dimension squared time n
times d spread time so do QR or STD it's
going to make its q and evaluate the
euclidean norm to that in june they say
n d square flow dimension x so high
dimension x lo dimension square we're
going to want it faster so little older
than me square even faster theory faster
clock time or a little over orthonormal
basis if you have a skirt with no
assumptions look so everyone so how do
you do this so the main idea is
extremely simple so if you like to root
of the complicated results you'll be
disappointed if you want an extremely
simple result this will be it now that's
upstate was easy it took us five years
behind but once once I found it
extremely sales so the main thing I'm
going to be given the matrix set up
ahead we can return all and diagonal
elements this projection matrix and we
can get all to relative error and you
can get all large off-diagonal elements
to address the problem in all these
applications is when things are large is
you can replace in this distribution
that you get one half this distribution
someone have uniform so this is actually
relative error these things are very not
in the one we can find all the large
amounts and we're going to run a little
om d squared time what that means it's
ND log.d time alright and for very
modest size matrices you have a couple
hundred by couple thousand faster if
you've got a bit of everything numerical
computation lifetime I'm not going to
try to go we actually have
implementation you can put the stuff on
Amazon ec2 paralyzed over 8 16 32 64 its
own processes all right so let me take
an aside and if you understand this you
understand ninety-five percent of work
and randomized basics algorithms
including how to compute these for us so
change the problems like that and
promise you will do the lab prep scores
different slightly different problem
over constrained least squares again a
is still tall minimize ax minus be the
solution is Duke you are or chill ask
you whatever that's you know SVD to
compute this X opt right anytime d
square on how do you do it faster here's
an elegant it's not faster but if you
understand what's going on you'll see
how to prove so for I equals 1 to N
compute the leverage scores exactly or
approximately randomly sample a small
number of constraints using that as an
importance of sampling distributions and
solve the subproblem to get X have opted
if you solve that some problem you're
golden so don't know about camera is a
fudge factor you get ax minus P is less
than 1 plus epsilon times the exact
answer your relative our own objective
and the difference between X locked and
X head out is absolutely times a
condition number its own fat or a
relative error both sense it's very
strong senses unfortunately the naive
algorithm with your face
d squared times of Union elements no
faster might fail with some probability
Delta and epsilon horse so it was just a
lot for itself but it can be improved
I'll tell you how to do that and this is
a key bottleneck and not only with
squares implementation put a whole scope
low-ranked implementations ran the
projection Allison's you heard about so
how do you make it fast you either do a
random projection when you do a random
projection you rotate to a basis with
the leverage scores that uniform along
with net frustrations then you don't
need to compute them the same uniform or
approximate the leopard sports all these
things are robust you don't need these
expert let me give you two approximation
1 plus epsilon approximation that's how
I'll tell you how to do that so under
constraint i described over constrain
similar results to go through the under
constraint least squares problem perhaps
not surprising thing so back to
approximating leverage scores view the
computation of the leopard scores in
terms of some different under constraint
least squares problem so recall if a is
n by D and it is tall consider this
problem take every column of a and get
the best least squares fit to the icon
for a clearly there's a solution for
that right you return the iphone that's
not the best solution and you put your
nonsense the best solution x transpose
is given by this e I times a plus all
right but P I which is the leverage for
the Wanda
but VI is equal to e I basis dive into
you it's the concluding on a bed roll
you can add that without changing
anything so it's a I got into this but
this is a projection matrix I want to
make my life difficult i can write that
projection matrix this way so the
leverage score is equal to the euclidean
norm of that of this country or connect
and related to the subject constraint
least squares problem so what I do I say
P I is equal to the i put the I element
of a and Plus that government now this
is expensive to compute but if I wedge a
random projection in there fast hat on
her face Brandon projection and a winter
random projection in there then I get
relative error and why is that it's
because you can think of this is a QR
decomposition right I protest asking you
but it's really QR decomposition if i
take the exact matrix a and I Duke you
are on a if I post multiply that by our
inverse I get Q and I'm done I think you
are not on a but on the projected
version of a and I post multiply a way r
inverse results for the projected
problem and i'm almost done i get an
almost orthogonal matrix in fact i need
to put a second projection then when
they get any fast but you do that then
you're stuck there yeah if you do that
what you get is you get your 1 plus
epsilon in fact you have a skips it's an
almost orthogonal matrix you can
consider the the dog products between
different roles of the sketch to get all
the options you can extend some fat
matrices streaming environments and so
on so you can actually check or test the
hypothesis that your coherence is small
that you leverage for the small and
little ol orthonormal basis time that
applies to arbitrary matrices to give
poor Elena dictionary</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>