<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Provenance for System Troubleshooting | Coder Coacher - Coaching Coders</title><meta content="Provenance for System Troubleshooting - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Provenance for System Troubleshooting</b></h2><h5 class="post__date">2012-10-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Q7HPPEPxpxs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hi I'm so I'm Jeff million i am a
Googler and Mark went to grad school
with me at Tufts he was Alba couches PhD
student and he is doing a postdoc with
Margot Seltzer at Harvard right now and
he is going to talk about provenance for
system troubleshooting thanks Eddie so
like Eddie said I'm at Harvard right now
and I am finishing up to your computing
innovation fellowship I've been working
with Margot Seltzer on issues
surrounding provenance and I'll talk
about what provenance is if you haven't
heard of it yet and a little bit of back
more background I spent a long time in
the IT management industry as this is
administrator a UNIX tool Smith etc etc
and one of my goals is to basically help
that community to troubleshoot systems
to not get woken up at three o'clock in
the morning by a relentless database
that's saying it's out of disk space etc
etc so this is our lovely maxwell
dworkin hall at Harvard which doesn't
really enter into the presentation
except that it looked nice to me so we
have historically had a problem in IT
management assisted ministration which
is that we try as this administrators to
develop a mental model of the systems
that we're looking at that were in
control of or nominally in control of
and what we want to do is troubleshoot
from symptoms back to root causes and we
rarely have the opportunity to do this
because there's always something going
on putting out fires so we're always
responding to the latest trick tragic
accident with you know the netapp filer
or whatever it happens to be and so we
rarely have the chance to sit down and
actually get to the root cause of
problems maybe re architect the
infrastructure if we're if we're capable
mostly were attacking symptoms and not
real root causes of problems so what end
up what ends up happening is that that
mental model that we've been developing
over time is constantly getting beaten
back by all of these little fires we
have to pull out put out now we're
actually fixing things but usually
they're just sort of temporary patches
so what is the solution well one
solution that we think may very well
work is to not hold it all in your head
which a lot of system administrators end
up doing they gained a tremendous amount
of experience it's not easy to transfer
that knowledge on to junior admins and
to keep it as institutional memory for
your organization and frequently you may
get you know somebody can get hit by a
bus or you know get sick or something
and that knowledge that's contained in
that person's head is basically no
longer there or useless what we really
want to do when we're troubleshooting is
we want to expose hidden dependencies
among components in the system because
oftentimes when we're building our
mental model we get this really really
incomplete picture right we look at some
some part of the system and we say all
right from my experience I know that
this this configuration file effects
this web server but I also didn't know
that the configuration file is you know
being written to on a regular basis by
some kind of a automated process and
this especially happens when you walk
into a site as a consultant for example
and you you need to understand how
everything is working together so what
we want to do is we want to build these
models of these dependencies at a
certain level and we're not claiming
that we're able to do it for every
single abstraction that exists but at
the system level we think we have a we
have a plan
we want to build models of these
component interactions that you can
actually query so you want to say you
know show me what Apache depends on and
I'll get to a definition of our
definition of depend in a moment so how
do you do this you sorry you collect and
maintain the history of digital objects
which is what we call provenance and
traditionally in the physical world we
we all probably know about provenance in
one way shape or form so for example you
have provenance of one of Raphael's
paintings right and these are these are
really important if pieces of
information especially for people who
collect or people who want to buy or
people who need to know you know where
the latest genome sequence came from for
example you've got wine you've got a
dress from a pic from a movie picture
you got something that I wished was my
car at some point so what what are we
calling that's physical problem is what
are we calling system level digital
province Providence well the short of it
is that we record this history of
digital objects and right now we're
doing we actually have we have a
prototype and I'm building a different
additional prototype but we collect
information about processes files and
pipes and a few other things by
intercepting system calls that the
current inside the colonel and this is
the Linux kernel we're talking about and
this project was started about six years
ago and its continuing to evolve and I
want to point out that all of this
collection of when these system calls
happen is time-stamped right so we have
at least on a single system we have a
sort of total ordering of what these
events are so what do we what do we
collect we collect process creation and
process destruction
so when you exec something you fork
something we're going to collect the
fact that it was forked and we're also
going to collect information about the
environment variables what was on the
what was on the command line the file
name the time that it was executed and
the parent process and we may be we are
certainly able to collect more than that
but for right now this seems to suffice
to do some of the things that we want to
do or it has been it may not be
sufficient entirely for troubleshooting
but I'll get to that in a little bit
okay so now I want to talk a little bit
about dependency we're going to build a
graph a model essentially of these
dependencies and what do I mean by
dependency so if a in this case for
example this is a process and this is a
file and we want some way of sort of
connecting making a connection between
the file and the process saying at some
point in the history of this of this
file it was written to by this process
or opened for writing for example by
this process now does that create a
dependency of the file on the process it
depends on what your definition is and
how you look at it in the context of
troubleshooting what I want to get at is
the likelihood that the contents of this
file depended upon this process in other
words it wasn't this it wasn't it
wouldn't be the case that maybe this
file would exist or was even written to
if this process didn't exist so in some
ways i'm getting at a a kind of weak
form of dependency and in fact without
actually studying the content and
knowing the semantics of the program we
cannot be sure of any dependency between
any two objects in the case of a process
so in the case of a process we want to
say that the process
may dependent be dependent on input
right so previously we were saying the
file which is an output is dependent on
its on the process that created it and
now we're saying that if a file is input
it may affect the behavior of the
process again we don't know we may know
a priori that this is the case but we
don't know engine in the general case
that this file affects the behavior of
this process now I'd like to go back a
little bit on dependency and behavior
for troubleshooting purposes I want to
define behavior as expected behavior as
as the behavior that's expected by the
person that's actually using these tools
assists administrator knows roughly the
sort of parameters and and outputs
inputs and outputs that go into
something like Apache so we want to know
when something may have changed the
behavior of the program does it make it
work differently or does it make it not
work at all and we're really interested
in does it make it work sort of
incorrectly from the standpoint of
behavior right if Apache starts serving
the same file over and over again when
it's being requested to serve different
files then we know something is wrong so
that's the expected behavior is to serve
the correct file right now that's a
little bit fuzzy because obviously we
can't know the behavior of every single
program on every single input right but
sis administrators do build up this
knowledge and we want to be able to
contain that in some way so we also
collect the fact that when two processes
communicate via a pipe and really it is
this just happens in the file system in
Linux right in other environments it may
not but it's probably a sure bet that
inter process communication happens in
some way that we can collect especially
if we're inside the kernel and we also
collect the fact that that a process is
axing the accessing a file through
memory mapped i/o now we're not
recording the the actual page inputs and
outputs just the fact that that if
something is written to an area in
memory after a memory map that it's
going to be it's going to go to that
file we're going to make an association
between between those okay so what is
our method for organizing provenance
well what we want to do is essentially
build an a cyclic graph a directed
acyclic graph the nodes are going to be
these digital objects of interest
processes that were instantiated at some
point and files and pipes and sockets
and I'm actually working on other other
channels of communication sockets and
semaphores and things like that and I I
will point out again that the the edges
in whatever graph we develop really
represent potential dependencies right
we can't really say anything about
causality with the provenance the
provenance is actually a trace of what
happened in time right so we can say
that the provenance actually depends on
something you know if the if a process
is running and out and output something
that's actually what happened and at
that point that thing depended on the
process that object depending on the
profits process excuse me process for
actually existing or being in whatever
state it is but in the general case we
don't necessarily know that that file is
always going to be written by that
process or dependent on our process so
why is it that that it's also potential
because we don't analyze the content
that's passed between objects we don't
have any semantic knowledge of the
process itself and it's too expensive we
we don't want to actually do that if we
if we want to come to any conclusions in
our graph in any reasonable amount of
time
alright so this is a truncated
provenance graph of of the binary
compile of wire test which is which
happens to be and I forget which suite
this is in but essentially the target
isn't executable and we we are just
compiling it and so we've trimmed down
this this fairly large provenance graph
to show the kinds of inputs that we're
getting two different processes and it's
a little bit harder to see on here but
we have processes in gray and we have
input files in this case in just the
white boxes so this is interesting um at
least to me we have header files all
right this is kind of a standard compile
thing you know we've got our source
source files right here is why our tests
see we've got our cc1 compiler we we
create an assembly a file and we link in
with a dynamic live with a shared object
and eventually we get another object
style and we link we linked in with
other libraries and eventually we get
our target something that's interesting
here is that this since this is
truncated I'm showing you the sort of
pertinent information around the
compilation process but this is not how
the graph looks right we we want to be
able to actually figure out that these
are the important things for this
particular executable right here and
you'll notice that here there's tar and
the reason tar is there we left that in
is because white SC and a bunch of
include files from the from the am utils
that's the sweet we're untoward so we
recorded the provenance of that as well
but this may not be germane to to the
whole process right and we're going to
need some way of saying you know I don't
care about that now you know just one
tar is not too hard to look at you know
when you're assisted minister ater but
if it's all kinds of
previous ancestors it's going to get
very difficult okay so let me turn to I
should say that that up until now there
hasn't been a real burning use case for
provenance right a lot of people have
worked on it in in the context of
databases a lot of people have worked on
it in the context of scientific
workflows helping to differentiate
between two different workflows either
different when they should be the same
or set very similar and my my
troubleshooting context comes obviously
for my background in IT and I think that
this is a very very poignant example or
use case for for Providence and i think
that there's a lot of things surrounding
this that can be expanded to help
administrators and even security
administrators and as we all know
sometimes your your projects get beyond
you and people will use it for all kinds
of things that hopefully are
constructive and helpful to them so so
the troubleshooting example that is
slightly embellished and a trimmed-down
for deep for details so that i can
clarify things for you but it's
something that i actually ran into maybe
a year and a half ago of to figure out
why my name resolution wasn't working so
I see that my name resolution isn't
working and I look at I know as a sis
administrator that there's a likely
starting point is the resolver now the
resolver neither neither really here nor
there but there was a library that's
linked into most applications that are
using the network excuse me so I go to
the resolver I know that I know that
resolv.conf traditionally is used to
specify the name servers and the search
domains and things like that now
traditionally this file is also managed
by human beings or edited you know via
scripts that change things
well in today's debian-based desktops
knots much basically that file if you
look at it it says something like it has
a comment in it that says don't touch
this file whatever you do will be
overwritten it's being controlled by
network manager okay well that kind of
makes network manager on the desktop is
you know in control of the network
pretty much but so what are the inputs
to network manager what you know what
configures or effects network manager
well this is probably pretty easy to
look up in a man page and I don't really
need a dependency graph for it but I i
can see that network manager amongst its
other inputs which include its
configuration file which i looked at and
saw that there was nothing about name
resolution in there it also has it
communicates with a socket endpoint a
unix the main socket right so i'm
thinking to myself well okay so it's
communicating with some other process
some other component in the system then
i trace this back to the d-bus oh so the
d-bus for people who don't know is is a
shared interprocess communication bus
essentially they register you register
services on this bus and then somebody
requests it and you make basically a
two-way socket connection through the
d-bus it's a shared shared message bus
uh so i'm going to do a little bit of
hand waving here and say that i was
successful using various sort of band
tools and time stamps to look to look
and find what this is communicating with
the there are many many many inputs and
many many outputs to the d-bus and a
normal debian-based desktop now this
this may not be the case at all on
server systems right it may be much more
traditional or you may be using a
different kind of shared bus but the
point is that this is a very opaque
object and we'd have to do some some
finagling to to get out the information
we want but let's assume that we can
and we see that this this dhcp client is
connected to the d-bus amongst many
other inputs and I said well Wow you
know I didn't realize that this goes all
the way up there in the chain and the
network manager is actually taking some
of its configuration from the dhcp
client it makes sense because the HTTP
is what's configuring your host right in
a lot of cases so we see the we take a
look at the D H client process and once
again we see that we have a lot of other
inputs and we we know that the D H
client com from from documentation or
from experience is where its
configuration lies so we're going to
look at that config file we want to see
you know maybe there's something wrong
in there that is causing problems with
name resolution oh well okay now i see
this is this is its config file and it
requests all these things of the dhcp
server and somehow I must have hit the
wrong keys and put in a pound sign in
front of domain search and hostname so
this thing is not even getting what
domains to search and what hosts to act
what what its host name should actually
be I probably would have discovered this
pretty quickly because other services
would have been complaining that i have
no hostname you know what's going on I
need a need network access and if
there's any questions don't feel free to
interrupt and if there's any suggestions
please talk to me afterward okay so but
wait so we cheated a little bit you know
we did some hand waving when it came to
the d-bus really really what that's an
example of is that even when you have
something sort of simple in front of you
you know something's going into the
d-bus you know something is has to come
out of the d-bus it's not really that
easy especially manually to be able to
trace trace back through that graph
and then what happens when your graph
comprises thousands or hundreds of
thousands or even millions of nodes
right you get something like this and
this is like a very simple thing of a
high interconnectedness between
components or this which is taken from a
real a real run so about seven years ago
I I was investigating this problem not
in the context of provenance but just
trying to see what I could explore with
which processes open which files and so
this was a force directed graph that was
created in a in a visualization system
called profuse and you can see something
like this where we have lips ESO 6a
everything connects the lipsy well
that's that's that's expected and
actually we'll see later on that that
this makes lipsy not very interesting
right in terms of possible causes for a
problem and you've got some other things
that I think cups d is it's not showing
up on the screen right now but the cups
t is up here and there's a bunch of
stuff that is pointing toward cups T now
in this case so this isn't really a
provenance graph this is pointing the
direction of the arrow means that cups d
depends on all the other things that are
going into it whereas in our Providence
graph it's the it's the opposite so cups
T would point to something else as
depending on that thing so it's the
opposite dependency is really potential
dependencies the opposite of of
information flow essentially right if
you're if you're a process and you're
pointing toward a file it means you're
reading the file right so the arrow goes
toward the file but the information
flows toward the process and that maybe
I mean that's just sort of a canonical
wave that we do things in provenance but
it's it's not written in stone
necessarily all right so we've got this
prominence graph we got a whole
Providence that we have no idea how to
parse visually or manually what we want
to be able to do is traverse that graph
from one or more processes that are
exhibiting this unexpected behavior as
I've explained back to the root causes
of that behavior now this is inherently
a very very difficult problem in the
general case in fact it's NP hard but
sis administrators have been doing it
for years and they do you know
approximate things and they do pattern
matching in their heads and they try to
figure out what's actually going on but
since we can't be sure that inputs
affect a processes behavior you know
where we're at a pretty significant
disadvantage and we need so we need some
way of talking about the edges such that
we can rank rank then say okay this
process depends on this file we want we
want to have a rank of sort of
dependency that says in order for this
process to work correctly or exhibit
expected behavior I this file is
necessary right in every indication of
the program or maybe it's only necessary
when these options are given to the
program right so Lib C is a good example
it's pretty much linked into everything
so flipsie was gone or corrupted
basically none of your programs with
work right but other programs are not
like this I mean sorry other files all
right so we need some some way to rank
the edges so this is a an extremely
naive approach and so a lot of this is
speculative because I'm still building
the prototype but one intuition that
seems to make sense is that if a program
reads from the same input at least once
on every invocation right then there's
probably a higher likelihood that the
program depends on that input right if
every time I run that program it reads
this file
now this may not be this this may not be
true but I think that it's going to turn
out to be pretty pretty accurate so
certainly in terms of libraries and
things like that this is almost a truism
but again we'll see later on how they're
there are there are definitely
exceptions so I i define the pro a
program as all the processes in the
graph thus far that have loaded the same
executable and we're going to talk about
executables having different versions
and and so forth so the into it the
other intuition is that as time passes
we can refine this rank now this is a
base rank so this is something that
we're going to you know if we have no
information whatsoever about particular
edge we're going to just say okay
according to some rules we're going to
assign a rank this this base rank and it
could and it could end up being
something completely different than this
but this is what I'm going to look at
first and then we we will tweak the
ranks and actually make the rank sort of
a multi-dimensional characteristic of
the edge so a lot of different things
will go into assigning a sort of a
probability to the to the edges real
dependence or non dependence so for
example here we have apache the process
apache and we happen to know a priority
that apache opens httt be calm okay and
so maybe we assign a base rank of 12
that it's that's that's pretty much
pretty well known but you can always
give a configuration option to Apache
that doesn't open HTTP I'll comp it
opens some other comp file right so you
have to take that into account and I'll
talk about that whereas you know here's
Emacs and I'm just using the editor to
modify Mopti which is a message of the
day right and maybe I do that you know
once every six months or something so
everything that's been collected in the
graph is you know it's two percent of
the opens that Emacs has a--'s has and
this is this is unrealistic even I mean
it's
probably be much much much lower than
that because II max is opening its its
libraries every time it's run its
opening lots and lots of things so the
caveats are like I said the inputs of a
lot of programs can be can be can be
changed by options on the command line
or shell redirection or other ways of
doing it and that might skew our ratio
right of opens to I'm sorry of opens on
every invocation versus opens not on
every vacation so but we do record the
environment recall and the arguments of
each process so we we should be able to
treat implications in which those differ
as separate programs so we'll have you
know 11 base rank for Apache with the
option to open at sea HDTV calm and
we'll have another version essentially
of the program with the option to open
you know in this directory in my
directory whatever it is and it's a
similar thing for new versions of
executables right if you install a new
version of an executable you don't
necessarily know a priority that certain
things haven't changed that are going to
affect the graph so you treat it as a
new program so you reset its base rank
now I was talking about sort of this
multi-dimensional ranking before and
when I talk about tweaking ranks I this
is constructed in a way that that makes
it seem like we set the base rank and
then we're adding these things or
subtracting these things from the rank
to according to some heuristics and
stuff but what I really would like to do
is is integrate figure out if I can
integrate this tweaking into this
multi-dimensional ranking right so it's
not just like base rank and ended up
it's it's an equation that essentially
gives you a rank with a high likelihood
of being right so one thing we can
observe is that observe is that a lot of
first order dependencies are known right
every single if you run ldd for example
right you're going to know all of the
statically
libraries or or shared objects that are
loaded at at load at load time right so
we may assign a rank of 12 something
like SSH and Lib crypto now again these
things can be changed but they're not
often changed so we'll have to figure
out if if we we may have to tweak this
somehow and say that this is not always
going to be a 1 point 0 tweaking its it
may be you know some other combination
now similarly edges to files that reside
in well known configuration directories
like Etsy we may want to tweak the rank
up we may want to say well because to do
is reading Pam calm well it's not sooo
dude calm but but we see that it reads
fairly frequently from this and we know
that it's in etsy so this sort of adds
to the the stability of this rate that's
ranking sir so we're going to increase
from let's say it was 0 point 6 we're
going to increase it two point eight
right and I don't exactly know how much
that's going to be sweet but I imagine
that it will be done via via
experimentation and what kind and what
kinds of patterns are appearing in the
graph similarly we may reduce the rank
of edges to files that reside in well
known configuration directories all
right sorry well known log directories
right so maybe you know user been last
accesses VAR log W temp but to see the
last users that logged in but to
actually run correctly and give you
expected behavior last doesn't
necessarily need the W temp file right
it'll tell you nobody logged in if it
doesn't find the W temp file right now
that may be unexpected in terms of I'm
expecting to see people have logged in I
know people have logged in right but
it's not going to make for example
last crash it's not going to it's not
going to make last behave incorrectly in
fact the correct behavior for last is if
it has no file then it just tells you
there's nobody logged it that nobody
that logged it in the past whatever
number of minutes so that's that's a
little fuzzy too I mean again we're
dealing with sort of the expected
behavior right so it depends on the
system administrator it depends on a lot
of things but it's not easily quantified
then there's other programs like you max
that right temporary files right when
you're editing a file it'll right a an
example of the an example here is Lisa
11 present right so it puts a little
hash mark in front of that that file and
it keeps it as a temporary file and then
it writes what's in memory once you've
edited it to to that file and it keeps
it as a backup file on a temporary basis
so we might be able to say something
about the ranks of this that they should
be decreased because they are they're
not they're non-essential there there
there might be essential to you when
you're writing a script and you you make
a mistake and you didn't you know but
they're not essential to the running of
Emacs itself right and this is a user
level application obviously if we're
talking about demons and things like
that it gets it gets more complicated
certainly applications like Oracle and
things like that right temporary files
for good reasons that that the entire
application depends upon so so we'll
we'll have to look at that and tweak
that as well now this is a relatively
controversial statement my colleague at
Harvard Daniel Margo has been working on
something called prov rank which he
gratuitously uh based upon page rank and
the idea is that if we're going to query
this graph
and find pads and sub graphs that are
that are likely to have to contain
offending object we need some way to
limit under specified queries so if I
just issue you know give me all the
ancestors of Apache I'm going to get the
entire graph you know going up and out
from Apache all the way up to you know
and then mark installed linux on this
machine right so we do want some way to
sort of limit to an interesting sphere
of potential influence and it may make
sense that as you get farther away from
particular node you know the influence
of of those those other nodes gets
weaker and weaker right but there could
always be a really long path that that
led to this thing and you do have to go
back up you know several several parts
of the chain to find out that that one
item that was changed at some point that
ultimately caused this sort of chain
effect so the observation like I said
before with lipsy is that popular
objects are probably less likely to be
the singular cause of a problem when
only one or maybe a few processes are
misbehaving because if it's a very
popular object it's likely that a lot of
stuff will be misbehaving and not just
one thing right so we may want to reduce
the the rank of those edges and sub
graphs that include the popular objects
and then we also want to give the users
of the system ways to explore you know
changing the ranks and especially if you
you have domain knowledge you want to
perhaps say you know I know that that
file is only read and on this occasion
and it's not a real dependency it's it's
just fluff and then we can apply we can
sort of apply this logic to compute
ranks for other combinations of the
graph nodes so we can we can calculate
the rank of any path in the graph
this is again a naive statement to take
the arithmetic average of all the edges
back to a particular node right so for
example I don't know why this is still
here but here we have the previous wire
test and we have a few other a few other
programs that were that were compiled
down here by GCC and so we may have a
path ranked by taking an average of
these these ranks as observed and come
up with it's actually closer to like
point seven one or point seven two right
but we see that it you know these were
all the ultimate inputs to creating why
your test now we have a linker here and
these may be two separate indications so
maybe we we we linked and loaded wire
test previously when we compiled it and
then later on we we ran we linked and
loaded it again right through LD but in
this case may be you know the
environment variable the LD preload
environment variable was set to
somewhere that contain lipsy 226 right
and you should get a version mismatch
and maybe if you do you will look back
in here and say why it's why this
occurring up maybe you won't be able to
find the fact that our preload variable
is pointing to different to making a
point to a different lipsy right this
this actually happens all the time I
don't want to really confuse it with
having you know this is a process that
ultimately makes the same binary but do
we want to say that wire test definitely
depends on Livesey to force right maybe
it's just GCC that's depending on lib
c24 it's compiling wire test now this is
an unlikely scenario admittedly because
you you're going to get a lot of
information during the compiler or doing
the load of this library namely this guy
won't won't even start right he's going
to complain that his
is it is in his API is wrong and then
this basically the same basic approach
may be able to be used to rank sub
graphs etc and like I say after we
experiment more we will probably find
that using other functions such as a
weighted average of some kind or or
other or maybe even machine learning
techniques to actually give us a sort of
a confidence interval for the path or
for the entire sub graph I'm also
thinking about turning sorry I'm also
thinking about turning these edges into
nodes and a Bayes net and talking about
them as if they are priors and being
able to to navigate the network by using
Bayes theorem and using Bayesian
probability I think it might it might
work pretty well but again a lot of it
is dependent on you know baez's bayes's
pretty robust to mistakes in the
probabilities but a lot of it will
depend on what system administrators for
example are capable of I don't know if
they if they will be wanting to first of
all right queries all the time and I'm
going to talk a little bit about that
too so at Harvard we developed a
provenance graph query language called
pickle that's based mostly on SQL and
sparkle and it allows us to extract a
lot of this information programmatically
from the graph so just to show you two
examples sort of for the flavor let's
say I want to fulfill this query after
every process is executable is or was
send mail retrieve its direct dependence
from the last 24 hours right so I want
to see which files essentially did send
mail right in the last 24 hours you can
do this with regular UNIX tools
sort of it's so for things in the past
you don't necessarily know that sendmail
wrote it but you can find all the files
in the common places where send mail
rights files in the last 24 hours but
you weren't you're not necessarily
getting all of them I mean I'm sure you
could probably figure out a pipeline to
do it but so here we select P&amp;amp;F as
unbound variables from the entire
provenance graph as P and we're peas
executable is bound to X and the input
at time e is bound to F and the type of
P is a process and the path is is a
string send mail and now minus the any
of those times when the when the process
was run are the file was created sorry
is less than 24 hours and that the type
is a file so I'm not looking for a pipe
or socket or whatever I'm looking
specifically for a file now you can you
can see this this can return a set of
objects upon which you can do further
query or you can specify you know
further restrictions within the query
itself yeah restore it yeah we store the
provenance graph in a database right now
it's just stored in a in a key value
store which is based on Berkeley DB
because Margot Seltzer is a very big
proponent of Berkeley DB since she
invented it and that is in so our
province where storage system right now
uses a what we call a stackable file
system and so underneath the file system
is something like X 3 and then at the
the stackable file system is something
through which virtual file system
requests go through we collect the
provenance and then we pass it on down
to X 3 and we also put the provenance
into a
into a database at the x3 level that's
hidden from the user on I'm working on
doing this in in the Xen hypervisor and
I'll talk about that tiny bit later on
but the but the essential thing is that
yes you're going to get start to get
larger and larger graph databases and
especially that should say graph
database we're also working on a graph
database that's particular to prominence
that will be extremely fast and do a lot
of things in memory that you know once
you put this stuff into the database you
got to figure out you know how much
history for example do I keep you know
if I if this is a continual online
process it's tweaking ranks and stuff
like that or are there situations in
which I can do batch processing right
where do I keep the do I keep all of the
provenance how do I compress the
provenance these are all questions that
people are working on in different
different areas not just at Harvard no
no this this is not taking into account
any of the stuff that I showed you in
terms of of rank and stuff like that but
we would have to modify this language to
to take those kinds of things into
account so you could say you know where
input is so-and-so and you know the
probability is that of a dependency is
like point 9 or something like that
right or order by ranks right if you
order by rank you naturally if the
ranking is good you naturally give you
know since administrators sort of you
know top top possibilities for being the
cause of your problem or something or
along this in this graph in this part of
the graph or along this path or
something like that here's another one
retrieve the name of every direct or
indirect process ancestor of httpd calm
so anything that wrote essentially to
ATT be calm um but also anything that
wrote to a file that was read by another
process that rotate
to be calm so we're talking about
ancestral chain all the way back to the
beginning of time the epic or whatever
that also has a direct file ancestor
that's owned by me so maybe I'm asking
sorry so maybe I'm asking you know did I
modify a file that was input by a
process than that then wrote to http com
so we can select the stinky name from
from all the Providence graph and you
probably wouldn't want to make this
query because this is this this right
here the input plus basically says go go
back to all the process ancestors so
you're going to get a set of you know a
gigantic set of processes and there's
probably lots of ways to optimize those
kinds of queries you know when we when
we do the popularity ranking type thing
we might be able to say you know plus
automatically acts with the locations of
the popularity ranking so that it stops
at a certain point when it thinks that
it's found most of the nodes of interest
as opposed to going all the way back up
the chain all right so I've talked a
little bit about digital prominence and
this being the origins and history of an
object and I should say that it's clear
that at this level of abstraction we
don't have a lot of knowledge about
semantics we don't have almost any
knowledge about semantics of
applications etc etc this is very sort
of agnostic in those terms but we have
at Harvard developed something called
the disclosed provenance API which
allows you to use this core provenance
library to build build in providence to
your applications so that you can pass
down through the layers more information
about a particular application so for
ample let's say you know you get a virus
in a file okay you you're probably able
to find out at the system level that
this file came from you know your web
browser right but you don't know
anything about the web browser you just
saw that you know the web browser wrote
59,000 files into the cache one of them
contains a virus so you would like to be
able to have the notion that this file
is associated with a session inside the
browser and that session is you know
contains all of the attendant
information like which which site you
were browsing and what time and things
like that so we can do that it requires
as of now requires linking to you know a
new library and writing in some disclose
provenance routines but you could
definitely see in virtualized
environments being able to use binary
writing or technique similar to that to
actually inject this kind of disclosed
provenance from outside of this from
outside of the actual program right now
I want you can do that in in a small way
using things like pin from Intel and
system tap on linux on unix systems
where you can get more information so
really the key to all this is this
comprehensive model we're really
building a model that that is going to
give us much more insight into the kinds
of operations and the patterns that
applications use in you know in in your
everyday workload that that your servers
or your desktops are using there's a lot
more work to be done we want to look at
potentially applying structural
algorithms to assist in narrowing these
queries so if we if we see sort of the
same patterns over and over again we may
be able to sort of put a bubble around
certain applications and say you know
it's it's never ever read from this
particular directory
so if you see that maybe there's an
anomaly there another thing we want to
it's very unlikely that this
administrators will be sitting there
writing pickle queries all day long
right so what you want is to ultimately
have a tool that allows them to put
together things either visually or you
know programmatically that they give you
blocks right so suppose you have a
tickle the ticket trouble system which
which sysadmin have been using for a
long time a lot of times you know they
document some of what they do and some
of their command line stuff is put into
the air about how they trouble shocked
the problem here you'll be able to put
inquiries that say okay well I see these
symptoms and this these this is the kind
of thing that I did and this worked well
when I did these these three queries so
maybe you want to do those first when
you see these similar symptoms in the
future and it may you know it may your
mileage may vary on that but at least
you starting to build up sort of this
this concept of a storehouse also so I
mentioned that I so how how we have the
system right now linux using this
virtual file system abstraction and
storing and collecting the provenance
from inside the actual BFS is very
fragile so every time a new kernel comes
out we have to do regression testing we
have to modify a bunch of different
things I had to recently I think we
moved from like 2-6 one of the lower two
sixes to like to 635 or one of the
higher ones and I had to actually go in
and change a bunch of things that
shouldn't change you know like that I
was just thinking why why did they
change this this just breaks a lot of
stuff and just making sure it's correct
so my colleague and I thought that it
would be a good idea if we could move
the Providence collection into the
hypervisor and doesn't have to be sent
obviously but we're just doing approve
concept where we can make it more
platform agnostic so linux changes far
more frequently than hypervisor and its
file system things and things like that
but the actual like things that we need
to look at inside of a linux guest like
the task struct doesn't tend to change
that frequently and so we can we can
have a slower development cycle by
trying to move this into Zen trapping
system calls inside of inside of a
hypervisor with without regard for
expense right now I know that this is
this could be a very costly move but
anyway so so that's the concept and then
we will have something like a demon
running in dom0 that will asynchronously
pull provenance from the hypervisor
buffers and do the analysis may be on a
separate core for example and so so then
you can start to see the evolution of
sort of a continuous online system
that's able to do this and as you
probably know there's a lot of cloud
providers that currently use Zen amazon
being the largest of them and I thought
that third-party kernels are very very
unlikely to get adopted by anybody on
mass some people say well why do you
think that the Xen hypervisor would be
able to I think that we have a better
chance of getting this into the Xen
hypervisor trunk the actual upstream
than we do in something like Linux eras
a dock at Stony Brook invented the
crimped FS filesystem the e crypt
equipped FS file system which is now in
the linux trunk and it's it's good you
know they're constantly having to to
modify it now and it's a commitment
especially coming out of you know
university or something like that where
we don't have a full-time
working on this we're trying to so that
is the spiel and we see our little
trusty system administrator here
thinking all these these thoughts I
don't know when he had found time to
actually sit down and do something yeah
so right now in the linux kernel the
links colonel implementation we only
collect about 10 system calls parameters
from 10 system calls and things like
file names you know file descriptors and
and we hook into like the inode
structure the DN tree structures that
are in the linux kernel so at past v2
which is the the system that they built
actually before I got there exhibits
depending on workload and especially aya
workload anywhere from three percent to
i think twenty one percent degradation
performance degradation which may be
acceptable in some circumstances and may
not be and probably most of the time
wouldn't be acceptable you know it's a
it's a university prototype so not
expecting miracles there are definitely
ways of improving this compressing the
data at the source right so for example
instead of collecting every read and
write if a process opened a file and
read from it once you can say well maybe
there's a potential dependency there we
don't have to we can ignore all the
other reads right so you'll just
basically return out of the hypervisor
once you've once the process has read
something once all subsequent reads will
be ignored so maybe you cut down a
little bit of time there or you do
compression on you may be able to do
actively do compression on the structure
of the graph saying that we already have
another candidate
for example for this node and we have
you know a thousand we have a thousand
previous examples of that we don't need
another example of that so so even cut
it right out of the path and say don't
collect it you know all of this logic
does have to ultimately be essentially
in the hypervisor because that's where
you're going to experience the most
performance degradation so you have to
work on on that direct path you know out
of the high of out of the guests and
into the hypervisor and then right
they've done some interesting stuff even
with the Linux kernel that we now have
but I don't think that they have done
you know extremely large workloads of
any kind most of it was to help
scientists discover you know what's
what's the difference between these two
these two provenance workloads to work
loads for which we have provenance
things like that oh one more thing there
may also be an opportunity to to find
differences by doing analysis of the
graph itself and things like you know
graph isomorphism it's expensive but
there's there is probably a possibility
I mean whenever you're talking about
graphs comparing small small parts of
them to gain more insight as to the
differences between parts of the graph
is is probably within tolerant tolerable
limits but that remains to be seen it's
because especially depending on the
density of the graph what we've seen so
far though is that a lot of the now not
on the system sorry not at like
collecting root volume information about
you know servers and everything running
from boot time but from other provenance
volumes we've discovered that actually
the graphs are kind of sparse there tend
to be long chains of things right but
not as much like that graph that I
showed you with the with all the pretty
colors
anybody else are we a little bit over
okay so yeah yeah so i'm working on a
relatively comprehensive idea of network
provenance there have been a couple of
sort of approaches to defining network
provenance from my point of view system
collecting the system level provenance
across the network is a very patchwork
guess II kind of thing because you don't
really have a lot of information if your
if your other machine is running you
know provenance some kind of province
collector then then it's easy right you
can correlate between these packets were
sent at this time from here and there
from this application etc etc but its
heart becomes harder and harder when you
don't have collection on the other end
and you're likely to start out not
having collection on the best majority
of other of other systems now for a site
and for troubleshooting it would be
great because you could you could make
your site you know provenance aware
essentially now you have connections
between province collection and
connections between all your machines
and you have dependencies on services
stuff like that when you start going out
to you know other devices on the
internet or servers on the internet
right none of them are going to have
anything so we may get this notion of
probabilistic provenance you know now
you want to make a judgment as to you
know how many times have I communicated
with the server and what kind what kind
of packets have come in okay well
they're all tcp/ip they're all UDP or
whatever I can maybe make some guesses
about that I can you know I don't want
to do me maybe I don't want to I want to
do some kind of a stateless packet
inspection or something that's like
raising ranks and lowering ranks and
things like that that's a very it's a
really tough question but I think I
think it could be well implemented in in
in a particular site
or cross you know anything under your
control clusters to in the cloud right I
mean if you have this hypervisor thing
working then it's then it's a fairly
simple matter of of using the same
hypervisor on all all the machines in
the computation right so the biggest
question just remains to be seen as far
as performance you know whether whether
anybody would even think of adopting it
if you know if it cuts 5% out of your
performance or whatever yeah I I think
that we could definitely that's actually
a really good point we could definitely
do some static analysis right certainly
the information that you gather from
something like LBD is essentially a form
of static analysis right so you know
that this library is definitely a de
penser of a particular process or
program so there's no reason why we
couldn't we couldn't do that it would
ever be actually very cheap right
because none of this would be happening
at runtime so any other thoughts
questions comments well thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>