<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Long Road from Text to Meaning | Coder Coacher - Coaching Coders</title><meta content="The Long Road from Text to Meaning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Long Road from Text to Meaning</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sbLK0EdZoWI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm very pleased today to have a kill
Gary to visit us here in tech talk Adam
is a leading expert in Ward sense he got
a PhD cici's many years ago on this
topic and has been working with a lot of
companies that combine like words up as
dictionaries like university of oxford
university press and so on and so I'm
very pleased Adam here because a lot of
a lot of us sort of worried about before
sense as well remaining anoka mostly we
do now it is keyboard suit all right
thank you very much and done in fact I'm
so much for inviting me and this is a
some English sweets to go round good
company your high quality in the in your
canteen this how do we switch the oh
sorry
Thanks oh the long road from text to
meaning a rather a grand title and first
I'll talk about the research program
that is trying to find our way from text
towards meaning and by way of examples
i'll use corpus lexicographer that's
making dictionaries from corpora and
word sketching and thesauruses and if
there's time quick word on something
we're calling collocation ality and and
brief word about the data and then
coming back to the title do do pass
these on round pizza and let's start
from something nice and straight
forwards and obvious what's language
well there's one answer and there's
another answer and the good thing about
it is that we know what to do with them
that we know since sorcerer writing over
a hundred years ago that we need both of
those to have something that's
recognizably language but the issue
rolls on and it rolls on no we know both
of those are necessary for language but
it remains as a methodological issue how
are we going to study language are we
going to study the sorts of stuff that's
in our heads or are we going to study
the stuff we find in texts around us and
the first approach is explicitly
supported by you know approaches the the
competence approach as was advocated by
Chomsky and that's in the rationalist
tradition and Chomsky makes his debts to
leiden it's clear um
which is fine two things to say about
firstly that's it's you know this is a
method for finding out about language
that involves linguist saying what do I
know asking themselves what do i know
which is an unusual method project
objective science but are quite apart
from the theoretical objection says the
practical practical problems that the
approach to linguistics you get with
that approach people choose one topic
that they're interested in and study
that but it's very hard to talk about
all the different phenomena that happen
in language because linguists don't
happen to think of them so that's the
approach based on studying language in
their heads the other approach is to
study text and isn't it nice to think of
language is something we can study in
the same way that physicists study
forces and chemists study chemicals will
be able to study language because we got
lots and lots of data in speech signals
and in and in text and that's the
empiricist tradition in the line with
the british philosophers john knocking
David whom a kind of grey title I was
tempted by for this talk was
rationalists and empiricists in the age
of Google but decided that will be over
egging it and something important to say
about this approach is that it's when
we're you know everyday language uses
what's important about a sentence with
its meaning we're mainly interested in
what people say and what we're going to
say in terms of what in terms of the
meaning of what we say there's something
about the corpus methodology an
empiricist methodology of treating bits
of language in the same way that
chemists street chemicals there isn't
any room for the meaning in that
approach we're going to throw away the
individual meanings of sentences and
just look at patterns so it's always
worth kind of remembering how odd a
thing to do with language that is and of
course with its with the advent of
computers that this approach has sort of
got renewed vigor and renewed strength
and now we're getting much better at
finding patterns in large data sets from
machine learning we've got much larger
data sets with corpora in the web and
we've also got no with 15 with with the
advent of computer
tional linguistics or the no progress in
computational linguistics over the last
20 or 30 years we've got better and
better tools which help us use the large
data sets like lemma Tyson part of
speech Tigers are our terms like limit
icers and Potter speech tag listeners
everyone here sort of knows what I mean
and well if you've got an English word
like inviting then one thing you quite
like to know is that is a form of the
verb invite and that's what a lemma
tizer does and so this sort of way of
looking at language has had 15 years of
it's all so new and my my son is now a
teenager my oldest son and at 14 he's by
my estimation slightly older than the
web which by my reckoning I 1994 was the
year in which I it started sort of
hitting the newspapers and being talked
about so and so it's a teenager I'm like
the analogy between the web which is
sort of growing at an untrammeled and
weird way and nobody knows quite how the
idea that it's now a teenager they sort
of seems to fit her and so the model for
this approach to language research looks
a bit like this and but we can we can
input lots and lots of data we can find
patterns in that data we can then use
those patterns to put into a lexicon and
then there's quite a lot in
computational linguistics work in
lexical acquisition from corpora and
that means that we've got we had talked
to start with but we can improve the
tools we've got four part of speech
tagging and parsing and limit izing and
so and then we can always add in more
data and get more add in more data and
then get more out of the data by
improving the annotation on it and by
extracting more more patterns from it
and this is to my mind the way in which
we can sort of make more progress in
understanding the structures in language
and as sort of stuff that languages I'll
illustrate it now with a few cases first
the corpus lexicography making
dictionaries from corporate this
splendid fellow is James Murray who was
the editor of the Oxford English
Dictionary between eighteen sixty and
1927 when the first edition was
published he died a bit before that in
fact she goes to show that you really
need a very long beard to write a
dictionary the the stuff behind him when
I talk about corporal that just means a
collection of texts many of you who
aren't linguist and not used to that so
corpus lexicographer is making
dictionaries based on lots of evidence
found in text and that's his corpus
behind him it's the pre-computer age so
it's not on a computer it's on a large
set of index cards and there's a there's
a book about a book called sleeve and
thinking the states it was called the
professor in the madman and which was
about with that was about him yep I mean
or he was he was a professor he wasn't
the madman and the the process of
developing that corpus people all over
the world read books newspapers whatever
they came across an interesting what
they considered an interesting use of a
word they wrote the sentence containing
the word on an index card underlining
the word that was the one of interest
sent it to Oxford and then he got filed
in that filing system and alphabetically
under the underlined word and there are
about 20 million of those and they're
all but there's lots of them in the
basement at Oxford University Press
where I do quite lot of work now and I
gave versions of this talk in two places
recently where I was able to one of them
was lower bear in Paris and the other in
the Netherlands to places where i was
able stay yes and that's their corpus
like that ones that like that's yours up
there on the shelves as it was in as it
was in the real bear so that's corpus
lexicographer in the pre-computer age it
means when James Murray is going to
write a dictionary entry he usually got
one of his children to go running along
the corridors to find the right box file
with the right with you for the word
or organized alphabetically of course
and then they'd bring that back and that
would be the evidence on which he'd
based the definition for that word which
involves lots of running up downstairs
and running around to get the boxes so
round about nineteen eighty the
observation was made that the computer
could do that very well of course the
1980s kind of wasn't you didn't have
computers on your desk in 1980 the
computer was a large thing involving
several rooms downstairs in the basement
of the building and sending a request to
it involved sending it in paper and
getting would jizz of paper back but
what the computer could do would produce
a key word in context concordance
something that look like that that's a
concordance for the english word party
or english lemma party and so what
happened in the co build project which
was the innovator in this work in the UK
was that the lexicographers were
allocated work were allocated these 30
words to write definitions for and what
they were given along with the
allocation of the words was that much
paper with all the key wording context
concordance lines on it and what they
then then did was this and to mark it up
to do color coding for the different
meanings if we look at that concordance
the first one is which will be used to
take a party of underprivileged children
well that's party meaning a group of
people will call that one the turquoise
one you are invited to a party and after
couple of drinks well that's the social
event party will call that the green one
we believe politicians of all parties
will listen to our views so that's the
red one so we can go through this
concordance marking up different
meanings by giving them different colors
and then when we come around to writing
the definition the dictionary definition
of the party we first do the red bits
and then we first look at all the red
evidence for the red meaning and then
all the green evidence with the green
meaning and so forth
tennis just the coding and and the
important thing to say about that his
use of concordances in that way has
revolutionized lexicography and it's
kind of changed how the people writing
dictionaries think about what they're
doing previously you know what was it
what was the source of the knowledge
that you were going to distill and put
on the put in the dictionary entry well
it was in your head but with corpus
evidence it's become more and more
viable to say with what I'm going to
distill is the concordance lines is the
cook is he is the is the sort of good
evidence i get for thee for what the
word means and so that's been the
biggest change in X acog relief from the
last century and now I'll just mention a
couple of the limitations of it let's
call / get bigger it's obviously nice to
have more data and more evidence and
there's one or two chairs right over
here if you want to squeeze through it's
not very good viewing lines but
we're on we're on making dictionaries
with with corpus evidences if there's 50
examples for a word you can just read
all of them and based your analysis on
that if there's 500 500 lines well it
will take quite a long time to read all
of them so maybe you don't want to and
if there's 5,000 lines you're not going
to be able to so then we want to use the
we want the use of a computer not only
for delivering us the data but also for
summarizing it in some way and the basic
solution is simply this you can make a
list of the words occurring the Nate in
the neighborhood of the headword with
their frequencies and then you can sort
by salience in somewhere other to find
the words that occur most often almost
interestingly and most often in the
neighborhood this is and that's the sort
of list you get this was actually in the
first paper proposing this approach in
1989 bike in church and patrick hanks
and looked at the looked at all the
words to the right of the word savor the
english words save in a corpus of i
think 40 million words and between one
and five words to the right and added
them and saw how often each word
occurred there and then applied the
mutual information statistic to indeed
whether these were the top-scoring items
and if I sit down for it I'll just give
you a moment to take a look at that
listen if you're writing a dictionary
entry for the word save what's good
about it and what's bad about it just
take a moment to put yourself in the
feet of a lexicographer
and am
these different things bigger a lot of
pieces oh that's the other words these
in colocation don't tell you anything
like the moon was saying like my
thousands yeah so thousands isn't very
good and one can we go oh yeah any more
yeah yeah the face lights up olders
meaning you haven't bought it yeah yeah
it's not so good to hear any other thing
so we like we like face we don't like
thousands we don't like enormous is this
more normalized for like the marginal
frequency of each of these words yeah
although thousands costs dollars
enormous dollar sign up there all have
monetary hace act to the men soon yeah
so it'll be nice to do some sort of
custard reason that we're going out
there yeah you lose rear needs this is
bremen is rear meetings if you initially
the top of a pile oh yeah this is for
purposes of fitting on the slide and any
other any other comments are sort of no
way to discern it from its opposite like
waste might have the same yeah yeah
that's true those often people might be
able to come back to that yet so so we
we've found some that we like I mean
other ones that we like we like face and
the other ones the other ones that we
like
so yeah we might also see that life and
lives could be put together and that
would that's this limit ization issue
it'd be nice to limit eyes these ones
weren't and which other ones do we like
other than will do each other once it
look like good evidence phase and money
yeah lives and yeah so all the ones that
we like are basically the direct objects
and no yours definitely already good
it's kind of quite surprising that
you're got in there at all these ones
what's going on with these ones is
enormous and and estimated ah then the
adjectives governing the direct object
so they it's kind of interesting that
they occur a lot but they're not
directly related to save their only
related via the via the the object so we
might so we start wanting to think well
if we if we added a bit of grammar in
here we could do something more useful
we'd like to get rid of some rubbish and
we'd like to do some limit izing and
we'd like to add in some grammar so we
can distinct so we can't because we know
that with verbs is pretty obvious
because pretty often the direct object
that's the most interesting thing and
thanks for nice thanks for the input and
so that's what we've done with it so the
word sketch is like that previous slide
except it's it brings in the grammar and
bringing in the grammar tends to get rid
of a lot of the noise as well and and to
make good word sketches you need a large
Jake's modifiers different grammatical
relations and then the statistics
haven't changed much from the previous
slide but we can sort them too but we
can have a different list for each
grammatical relations so we can sort of
say what different classes they fall in
the first version of word sketches went
went into making that beast the
Macmillan English dictionary for
advanced learners and and was Michael
Randle the editor and I took the talked
about how we'd used word sketches at the
the lexicographer II conference it's in
Copenhagen in 2002 and people people
sort of it they got a good response but
various people came up afterwards and
and said to me can I had them for my
language please and so far had only
really thought about doing the for
English and hadn't given it any thought
so I said no no it depends on having
this large resource for English and this
one after after three or four people ask
me that not given three or four were
these very downbeat negative responses I
said well maybe we can do better and the
upshot of that was a product called the
sketch engine where the input is any
corpus of any language which we like to
have limited and Porter speech tagged if
possible and then a specification of
what all the interesting grammatical
relations are for that language for like
objects and subjects and modifies and
heads and then the system was is
integrated with a corpus query system
which means you can do concordances and
sort Berman's and find other patterns in
a range of ways developer was Pavel
rickly from Renault oh and we've now got
quite a few dictionary publishers using
it Oxford University Press and collins
and chambers and macmillan and quite a
few universities my main reason for
being in california is actually because
framenet that project at Berkeley is
using it so I'm talking to them about
having and how we make the best use of
it there and that's the URL but let's
show it now
so that's a word sketch for the English
verb engaged based on the British
national corpus so we had four thousand
examples and but by the time we've
passed to identify what the object is
and then done some sorting bearing in
mind the emotional frequencies for the
word that's what it looks like any any
observations about the the first three
words on it in the in this first column
in relation to the verb engage
consultants not one rite aid might this
British data so there might be finishing
this different British American
differences yeah exactly so as we were
saying with save face what you want what
one of the important reasons for using
corpora for addiction we making is so
that you don't miss any meanings because
it's pretty hard to think of them all
off you know without any external
assistance so it's kind of rather nice
this example because it immediately says
the very first three words it shows you
are three completely different meanings
of engage engage attention that's a sort
of mental meaning engage gears is the
straightforward thing you do on cars if
they're not automatic and engage your
consultant is employer consultant
identify usage exists in the state so
three completely different meanings that
need separate treatment in the
dictionary so we see engage almost
exclusively being about
p on the side Oh get greeting to get
married is this a British personally of
bricks American English distinction or
I've seen right especially I was used
there is my signature there's been a
woman in a second okay it's set in
column subject let's see and it's always
you I think that sense in this always
passive which creates a communication
and it means these are probably missed
targets tricky so you're not staring and
this would be engaged versus engaged
well they should be created in it now
even those men engages and not the in it
there's probably a part-of-speech
tagging issue here that if we don't know
it would take a bit of I mean we can do
a bit of debugging to see why why
nothing about that meaning they were
engaged to be married is pretty common
in Britain and well it's kind of this
sort of is a verb it's just a verb that
has to be in the passive in that in that
meaning so it's surprising that it
doesn't feature there it might be that
there aren't any obvious collocations in
that meaning and if we and cuz if we
just got yeah there's a pretty weak one
here if we look for specifically they
were the word form engaged I'm the nice
thing about having that they train at
all as its you can easily enough should
have onion no I sorry I didn't delete
this one
yes we haven't got we haven't got many
engaged to be married here it could be a
bit more systematic and take a sample
but yes it seems like maybe it's more
maybe it is a video british-american
different well it's a pretty wide
general Curtis corpus so if that's true
then it would be more about it's quite
surprising hasn't come up there yet
intense this one if you I can we could
looking at the corporate American ones
here too we could also I this we can
also do an analysis and the proposed
well if we yeah most words do when you
start looking them so if we look at up
to a window and between two and five
words to the between one and five words
to the writer engaged we can do that too
and so we can sort of we suspect that
married might be there yes well it will
quit see what we get turn so it looks
it'll look through all those examples
and find a call yesterday usually
quicker than this but as I was saying to
their kind earlier I better not blame
the internet connection when I'm at
Google
yeah there's not much discussion of
engaged to be married in the score or oh
yeah mr. so it is up there with 37
examples not over not a dramatically
high number the word sketches are
looking for pre specified grammatical
structures and engaged to be married is
sort of it's kind of kind of double
passive isn't it the first Clause is
passive and the second Clause is passive
too so that so that's not a pattern i
would eventually look for the seventh 1
the sixth one is opposite of marrying we
hope it's the opposite this is the tool
gut do play it play with it to be like
we've got some we've got lots of
languages loaded in it now so we've got
we've got full of most most of the
biggest languages in the world chinese
english french german irish no no irish
that's not one of the biggest italian
japanese and portuguese and spanish and
they're mostly web corpora that we've
collected so we've gathered other
corpora as and when we can and for most
of those I've worked with collaborators
who are experts in those language who've
done things like know what part of
speech Tigers our best and and set up a
posi okay
back to the PowerPoint so we're going
this first three words consultant is
your corresponding different meanings is
that also part of the algorithm to like
find like the most common things
associated with it in like five second
most common given that the first one is
not there we did go sort of get onto
that a bit because I do we do something
pretty much like that and but that sir
yeah it's good question for leading on
to the next part of the next part of the
talk and pressure so it's the idea that
you sort of premium their subjective
knowledge so engaged is a work and then
you discover semantic knowledge words or
deal also just opposite
and it's mainly using using a ship using
what we know about syntax to bootstrap
both finding about out about semantics
but also finding out more about syntax
because you know one would always hope
to be able to improve the grammars that
are used to pass the corpus with the
output of the of the process and you
know one of the best one of the sort of
leading pauses for English thats around
at the moment is is one that uses
detailed syntactic knowledge from the
long and dictionaries the the rasp aza
and so if the dictionaries get you know
people use disorder systems make the
dictionaries better that's going to
improve the pauses or the systems might
be more tightly integrated than that oh
so I shown you word sketches I was also
just going to show you not very you also
thought well now we've got these words
sketches we could turn it illustration
courtesy of google images of course so
this is just a bit of decoration stick
the terms in google image search and and
take the first one is i think we've got
an online version of this where they
change every so often but we want to be
sure about the copyright status of that
so it's not in public display ER than
it's quite nice that a culture gets you
a sort of splendid image like the
posthumous
and it so where are we on the overview
will I talked a bit about corpus
lexicography and work sketching skill
mrs. the sauruses the source is a
resource the group's worse according to
similarity and to cut two types of
thesaurus manual like Roger a picture of
Roger and wordnet is pretty much a
manual tisouris and all the publishers
Authority Oxford has its thesaurus and
so does Collins and so did almond so
does merriam-webster and all the other
publishers they're all made by people
making judgments about which words
belong together but then there's also
automatic thesauruses and a spot Jones
who died just last week was instrumental
in proposing the ideas back in the 1960s
and her PhD thesis and then hang later
it was was sent with central to my
understanding of what was possible with
automatic thesauruses also called
distributional thesauruses and the basic
idea is two words of similar if they
occur in the same contexts and something
to kind of puzzle about about the
sources is to what extent of these ones
similar to those ones and because the
sources are extremely useful for
language technology or natural language
processing wherever you've got you know
the reason is really so simple whose
house Bell is spaced out anyway is
because the basic problem with languages
you've got sparse data there are so many
words that you don't find most of them
in most of the context so when you come
across a new sentence and want to
understand it you're pretty likely not
to have found those words in those
contexts those words with those other
words before and so call that the spots
data problem and and
I skipped over one there on it and the
basic question that you often want to
ask is you know do two words go together
or two constructions go together does X
go with y and you don't and looking at
existing data you don't know because
you've never seen them before so let's
say well we can't answer that question
so let's fall back on a on a variant of
that question is does X and its friends
go with Y in its friends and if they do
then that will provide indirect evidence
that X goes with Y and the thesaurus
tells us who the friends are so and it's
kind of a strategy that looks like this
sometimes goes by the name of backing
off or is similar to other strategies
that by the name of backing off and and
here's an example and and that sort of
business they're trying to find out what
goes with what and it being very helpful
if you've got a decent backing off
strategy is kind of applicable in all
sorts of areas of natural language
processing and from speech understanding
and spelling correction and and in
various pausing problems one big part in
problem that I'll illustrate it with is
conjunction scope
what you reckon the shoes old are the
apples old no now the apples aren't old
the shoes might be able to get those
aren't old so there's a hypothesis there
that so this is a pausing problem
because we want to know whether we put
brackets around there before we add old
or whether we put brackets around here
before we add apples and and the
hypothesis is that you'll only get the
scope that puts boots and shoes together
before you and has them both modified by
old when the words are similar in
meaning to each other and that's an
example of that and this is some work of
some colleague Francis chantry's thesis
does quite a lot of work on this yeah
yes there are lots lots of scope
problems in pausing there and but this
is one where we think the thesaurus is
potentially going to be quite helpful
and quite directly helpful and first
experiments on it by Francis were
promising if not they didn't immediately
solve all the problems so that's one of
so that so that's kind of an example of
this very approach which can be applied
to lots of problems of using a thesaurus
to find out which words go with which in
order to find which patterns and most
widely applicable of course there's
other sorts of essaouira see it's the
story you can use and I was very excited
when I discovered google sets a couple
of years ago because that's also giving
us the source information about which
things go with which and would also be
pretty lickable for these sorts of
problems
and here's an e so thinking going to get
about these to cut types of the source
automatic ones and manual ones and what
things do they put do they say a similar
to each other well in the case of
automatic thesauruses it's pretty
straightforward they're talking about
words manuals authorities probably
people who start innocently in naively
putting words together into into
different categories of similar meanings
think they're putting together words and
then they and think they're putting
words in a hierarchy and that's
appealing idea but then they come across
crane and they see that crane either
belongs with tractors and and other
machinery or it belongs with geese and
herons and other birds and they think oh
no I can't have it can't be words the
time putting into this hierarchy because
some words have different meanings that
belong in different places homonyms so
it can't be words it must be word senses
their problems with word senses their
theoretical problems and practical
buttons there's Plato and Aristotle
trying to work out what meanings are and
two and a half thousand years ago it
started a long argument it's still going
on with that bunch of philosophers in
the 16th century and going on all over
the world it's going on in China as well
and it still hasn't reached any sort of
conclusion you know what what
philosophers have to say about word
meanings is bewildering to put it mildly
if they you know we hopes there were
these simple objects that we might be
able to manipulate and work with and for
lots of assembler just just serve to
show us that they're very complicated
difficult and not things we should trust
at all and most useful thing to my mind
said about word meanings or said about
meaning by philosophers was Victini
Stein's comment don't ask for the
meaning ask for the use
that's the theoretical difficulties
there's a quick summary of the practical
difficulties and the so we want to use
it to draw us because it's a practical
tool to help us with other tasks like
words like like finding the correct
power structure or speech recognition
and and lots of and lots of the sources
say we'll yes use me on a helpful
resource but before you can use me you
have to do word sense disambiguation
because the things that I organized into
structures are worth sensors not words
which would be okay except there's a
huge cost to doing word sense
disambiguation before you can use those
tools and and well there's been involved
in lots of work on evaluating the sense
disambiguation but I won't get
distracted now but I think it would be a
fair comment to say that under optimal
conditions the best sort of approaches
might get something approaching eighty
percent which isn't very good it means
something like if you want to use this
tool you use this tool first replace
one-fifth of your input with junk which
isn't kind of very promising humans you
get all represent well yeah kind of it's
not obvious what a hundred percent means
because word senses owing to us
philosophical difficulties are such
awkward things that and that in
dictionaries typically gives different
sets of meanings for Edison others so
people don't get a hundred percent if
you view one hundred percent as if you
measure it as in to tag agreement to get
a table but most of the eurozone argues
too human yeah mostly they're not area
they're not errors that anyone wants to
argue about very much thick here because
there's an area of gray between the one
meaning in linux and but what that what
that little diatribe aura side is meant
to slee the conclusion from it is that
we should really avoid word senses and
because getting distracted into them
means that we were introducing a whole
set of other problems that are harder
than the problem we wanted to solve in
the first place
so we don't really want to say this word
has three meetings or three senses but
in village dinning mode we want to say
this word has three kinds of use because
if we've done that then we got something
fairly well founded an empirical that we
can build on and how does that fit into
the overall so I'm a fan of the
automatic de sources which manipulate
words so much more than many ones which
manipulates mid-foot say something why
you even want to say that it has three
users I mean that might be causing as
well can you just get all we're saying
all this uses similar to this one when I
in all three or four now and quite
likely depending on the particular tasks
so I mean that the the the tickle the
sort of typical reason why people have
wanted to and people wanted to use the
sources in artificial intelligence a lot
with artificial reef with with automatic
reasoning and and so to put to put
meanings into hierarchies and then to be
able to use his or relations and that
sort of approach in order to draw
inferences and so I kind of think that
from us fish intelligence is pretty
desirable that and that I'm an
artificial intelligence point of view
then saying that then talk about the
meaning talk about the use isn't very
helpful because they want things that
they can deter in hierarchies that they
need raw inference is over so there's a
set of precious to to to to be talking
about word meanings Robin rather than
word usage which is which hadn't really
said much about here that's the kind of
counter argument that's the that's the
thesis which i'm offering a position
against in a way and to come back to
this overall picture and i hope the
things like the old boots and shoes
example have given you given an
indication of how the source is a
particularly useful tool in this sort of
looping around because we can use it to
find more patterns and it's kind of
lexical information that we can add in
to improve our pausing and also other
processes to get more structure out and
and so that we can sort of loop around
them in a work because all the other
bigger popular question is what we want
to divide thing we want to sort of make
progress in in getting towards a richer
and richer understanding language and
we're going to have to do things about
meaning in somewhere or other or we will
kind of mr. point i'll come back to that
right at the end actually because
addison it's there
om this is just another example of this
sort of looping process the sort of
thing we can find out when we've got a
big corpus and in the right sort of
database which were the most collocation
all it's a question which words have a
strongest tendency to occur occur in Col
occasions which is maybe for this
audience it won't be obvious why that
might be an interesting question but
it's sort of a characteristic of some
some words do have very strong
tendencies to occur with fixed patterns
of words and others are much more much
more willing to go with whatever other
words their national promiscuous they
just go with whatever other words there
are around and wreak havoc exactly yeah
I'm all good examples a bit like that
and so this came up because the
dictionary publishers they'd like to
warn their readers you know they'd like
to tell their readers which words
particularly inclined know which was
beware of the word wreak because you
shouldn't use it with anything much
except havoc they want to be able to say
and that's useful information for
language learners and so I was asked by
Macmillan how can you can you help us
work out where we should give
information to to our dictionary readers
about words that have particularly
strong call occasional patterns and
thought about the bits and realize that
this collocation was an awful lot like
entropy and and we could take every verb
and look at the objects it had and we
knew its frequency from the database
with you know from the corpus we knew
it's over that's just a probabilistic
version of that colonizing and then this
one's and we can calculate its entropy
in that column net which was over a
simple sons and it meant we could work
out which verbs hadn't had highest
entropy for their for their direct
object slot and produce a graph like
that which maps frequency against
entropy and so we know so it's actually
the so it's
the ones but and there's a bit of
normalizing to do with the overall
frequency because it's a tendency for it
to go up and then we can say that the
ones the ones with the lowest score here
are the ones which works most worth
considering telling the language users
something about the something about the
fact they've got strong tendency to go
just with a smallish number of words
here are the words from the British
national corpus which had I never end
with its highest or lowest entropy
highest collocation ality before the
issue like that most of them you can
sort of quickly see what what it is they
tend to go
door was quite a nice one because there
aren't actually that many things you do
with doors you open them and you close
them and you shut them and you lock them
but but they weren't no but there aren't
many other verbs that door is object of
place is most dominated by take place
most of them are dominated by one or
maybe two colleges any questions about
anything from you so yeah yeah yeahs pay
tribute to they kind of misty fairly
obvious play tennis and oh we have got
yes we have got your example then say
set a Miss pause is there it's really
there's always passing problems so one
thing that you get in that the one thing
that you get forever in journalistic
texts said the spokesman and because it
often occurs the wrong way around the
long way around delfina and it was
impossible said the spokesman because
we've got this in English usually has
the subject in front of the verb but
where you've got quoted speech it will
often have you the wrong way around so
that's why it's firstly miss paused it
and then that's dominated by said your
stated
three nuns so are they in
so these didn't have the same partner
like take yeah excitation or something I
mean is there is that weird shouldn't
really be something else yeah could you
normalize these for like the frequency
of the work yeah that's that would be a
further sophistication that I've thought
about but haven't haven't done yet i'll
like wreak havoc know how expresso is
really yeah yeah so this year this
doesn't bear those frequencies in mind
tonight i agree it would be interesting
to see what how that would change things
and whereas the next bit i was going to
say tiny bit about in the talk was about
the data and so all i've been proposing
is you know having also data is
important that's simple summary of how
data sizes have changed over the last 30
or 40 years nice straight line providing
improve logarithmic scale up this side
though maybe now we're going up steeper
than that with a whip and and that's
great because the bigger the data set
the better results you get the more you
can do and for you and one of the one of
my main sort of research agendas is
getting people to realize the potential
and benefits of using the web for
corporate until not so long ago and you
know people would it's quite the way it
was so new the British national corpus
doesn't have words like blogging because
they didn't exist when it was built and
it's always worth remembering that and
people carry on using the British
national corpus which was very long and
painful process of building you can
build lots of writing to publishers to
get electronic texts but that was done
in the late 1980s and early 1990s in the
world is transformed as far as those
things are concerned
oh I i put these lives in mainly because
i wrote a paper just recently cool
google ologies bad science so I thought
I'd better explain what I meant by that
in case people had ready to object and
if we want to use the web as a corpus
then and there's sort of two ways of
doing it either by Google or as I was
something I wasn't meeting with the
other day said who didn't say Google he
said search engines with 2 o's in them
so we knew search engines with two tho
in them or not um and it's very tempting
to use the ones with 20 z because there
are no setup costs so you can stop
queering today and you can immediately
get some sort of evidence to test a
theory and the methods of using them
could be just using the hit counts or
using using the snippets that are in the
in the search results page or you can
find the pic or which is like what some
metasearch engines do this one whip corp
which has been around the world or you
can find the pages and download them and
there's been quite a lot of NLP research
which uses google or yahoo and then and
example of it and and and two together
lots of evidence we can't believe in two
pieces at work that use the hit count
there's one by keller in the part of
which which made 36 queries to estimate
the frequency of fulfil obligation and
and because they wanted to check various
different forms of the noun verb hair
and birds do wiggly what the fort worth
at formals and they had to try them all
separately and on his another quotation
from some similar work d there's been as
there's a small community of researchers
doing this sort of work there's a
wonderful thing called the corpora
mailing list which is the main
discussion forum and the group has
intense interest in query syntax
and care greatly if there's one very
marginal and minor change to the Google
query syntax or the yahoos query syntax
so that was at that at the time they did
the queries that was a legitimate query
glisten with some wild carding and it's
not it's not a very good way to do
science because you'll only give us a
thousand hits for query and you're only
allow us two thousand queries for day
through the API and I think that's that
service is being terminated or as these
shows signs of not being trustworthy for
the medium or long term future and the
10 words snippet around each search turn
that's not enough you've got this
ridiculous sort order we want we want a
random sample of all the instances of
the world untrustworthy hit handsome I'm
there's lots of iron they're not
trustworthy I'm sure you know much more
about that than me limited search syntax
and no regular expressions and of course
it's there linguistically dumb you don't
do limit izing and part of speech
tagging and party so there are reasons
for not doing linguistics not doing
linguistic research using google as an
interface to the web and external
interface to google really my my my
agenda that this the agenda of the of
the paper i wrote is is exactly not you
it's to actually to say we want everyone
else to have as good access to the web
as you have which that's that's exactly
the agenda one technique that i suggest
switch the problems were they care
deeply and most of our users wiggly yeah
is and helicopter Cincinnati to the
query yeah come on yo the DNA sample you
don't get you do get tricky it's running
right it's the wrong amount it's yeah
you can and some of those methods have
and I've done that sort of thing self
and but you don't really know where the
helicopter and Cincinnati or arbitrator
randomly something then there's
definitely not and so how many noise
words and have you choose the noise
words and what how did concept yes so so
this is kind of the more of this story
that using google looks very appealing
because it's zero cost entry you just
start googling but the reality is that
if you want to do dependable solid
scientific research on it it's it isn't
really much a shortcut because you hit
about you hit all the other issues so
things aren't replicable and of course
commercial companies don't tell you the
innards of how your choose how you're
choosing which things to put where and
so that's the justification for my
comment there the so what I'm really
involved in is the the not answer here
which is and I've been involved in
things like we've set up a special
interest in a Association of
computational linguistics is a CL
special interest group on the web as
corpus which wants to use open source
methods and some of you might be
interested in our next workshop coming
up in September as I said the corporate
in the sketch engine or quite a lot of
them a large web corpora Oh at this
point I have to say to Google's thank
you very much on behalf of the academic
community for web 1t the you know that
the trillion were all the engrams coming
out of a trillion words of data is kind
of wonderful and so too so add us all
this week together and well back to
rationalist and empiricists we've got
one way of thinking about languages will
there's two kinds of people who want to
study language in one set of the
rationalists and they're most interested
in the meanings and they want to work
out things in the formal semantics
tradition of how meanings of words go
together to make millions of sentences
and how an answer to a question hasn't
been you know how you can present a
mostly how many the words want to go to
meaningful sentences and how many's of
sentences will combine to give new
meanings and that's so that's the logic
tradition if you like with yeah yeah and
the other end we've got empiricists who
are mainly interested in the raw data
and what tends to happen is that the
empiricists starting down here and the
ration is going down there and both of
them think they're talking about the
same thing as each other in fact this is
greatly gap in between and and the sort
of method with the spiral that I'm I've
been advocating here is consult be
represented a bit like that that well
when it was raw text we were just
dealing with characters with you know a
string of characters we weren't dealing
with anything that we could talk about
linguistically then once we've tokenized
and lemma tised it well then we can
start talking about words which is
better than talking about strings so
down here we're talking about strings
here we're talking about words well by
the time we've part of speech tagged it
we're talking about where we've got a
richer idea of what the word is and then
we can start talking about grammatical
structure as well once we passed it and
as i said i'm visiting framelit night
each of these stages makes it a bit more
linguistic a bit more like we're talking
about meaning which is in a way a
response to your question earlier we
want to we really want to meet up with
those guys we'd like to be able to talk
about the different relation or the
logical relations between related
sentences but it's an awful long way and
my kind of I'm keen to work the frame
net because the approach there with
frames the structures for conveying the
meaning for holding the meanings of
words is kind of the next step up on
this on this agenda so this is the this
is the long road from texture meaning I
think we're still an awful long way away
there are many other
to do between times but join me on the
journey thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>