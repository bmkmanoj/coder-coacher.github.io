<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Parallel Revolution Has Started: Are You Part of the Solution or Part of... | Coder Coacher - Coaching Coders</title><meta content="The Parallel Revolution Has Started: Are You Part of the Solution or Part of... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Parallel Revolution Has Started: Are You Part of the Solution or Part of...</b></h2><h5 class="post__date">2009-01-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/A2H_SrpAPZU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I will emphasize one thing though is
something that I gained an appreciation
for when we got involved by funding the
rad lab would dave is how effective
David and his team have been in engaging
with industry in general it's sort of
been the very productive relationship
that they had and the way they interact
with them and with us and people like
ourselves and I encourage you guys if
you're interested in with these to reach
out and look for opportunities for
collaboration as well so without further
ado I'm going to let they've talked
they've just I just have to mission one
last thing that your slide the title and
the abstract sounds a little bit like a
Stephen Colbert sort of ran to some
extent so i'm going to i'm wondering if
you're actually claiming that cmps or
multicores are destroying america and
when way or the other david all right
I like to thank Google despite its
Stanford origins to starting on Berkeley
time which is 10 minutes after the hour
it's make me feel welcome here so this
is our logo and kind of as we as you'll
see a cup for several years we've been
talking about this internally and we
decided that we could think of the
problem is the parallel hardware is one
of the towers and the parallel software
is the parallel applications another
tower meet the software span to connect
them together the more we thought of it
that looked a lot like the Golden Gate
Bridge that's the view from Berkeley and
then there's the users in Marin and then
the valley down here and it's got the
industry so we've got to connect the
users to the to the industry via
applications and software so i'm going
to talk quickly about how do we got here
and that's a bunch of the points here
especially for the students that spend
more time about the power lab and then
I'm despite the starting time Luis
promised I could talk about this brand
new performance model which will be more
technical in-depth thing for people
interested in the lower layers the
software layer and architecture of a
pretty simple way to get some insights
and whether your program and your
architectures running as well as it
hoped brand new slides here so how did
we get here we ran out of power it was
amazing to me that every generation we
could use more power nobody said
anything with micro processors and now
they say you can't you don't get any
more power so there was nothing they
could do and this was a surprise to the
computer industry and will this
illustrates this surprise is something
called the international technology
roadmap for semiconductors which
includes all the semiconductors
companies in the world may get together
every two years and say what's going to
happen in next several years so this is
a plot of clock rate over time and by
now everybody's course using 10
gigahertz processors and next year will
be 15 gigahertz and so that was 2005
just two years later romantic slam down
so this was a surprise apparently the
people just had blind faith that we're
going to overcome these obstacles we've
always had and the clock rates to keep
going up so that's slamming down it
means that we've we've jumped to
multi-core not because of any
breakthroughs in computer science it's
just it's the only way we could imagine
getting more performance given the power
limits and which kind of stepped on the
third rail
for computer architecture was well I've
got this great idea for a new computer
you've got trained all the programmers
in the world to do this new thing well
that's actually what industry debt is
and so you know maybe evolution and
global warming's controversial in some
parts of scientific computing but I
don't know any computer architects who
aren't saying that the future is
parallel and by the way all the
companies that built micro processors
have their next four or five years of
products under way and they're all
parallel so it's it's it's run away and
those of us who know our history know
that boy that this is a pretty sad
history in parallel computing occasional
advances but a lot of failures and it's
a whole lot of companies some of whom
have led to wonderful buildings in the
southern belle area that's at a time
parallel computing looked like a very
promising thing to do so what's going to
happen if the PCs of the world the
laptops of the world if everybody who's
got say 32 core laptop or PC and your
friends ask you should i buy the 64 core
laptop or PC and because should i do
that and and if everybody in the world
says no there's no software that you
care about that will run on 64 cores any
fashion 32 cores so the two interesting
thing about that statements it seems
like I would bet a lot of money that
that's true right I'd bet there's no way
a 64 car lap cops don't run anything
that even he cares about faster almost
nobody that would be true at the same
time if that's not true for the industry
the way that we're get Moore's Law
faster processors is to only to use more
cars that's going to be a bad thing for
the industry and maybe the actual blue
part of the line is the actual growth of
PCs would we see this falling down of
sales of PCs if computers didn't get any
faster anymore that's been kind of
practically Moore's law for most people
has been higher clock rates which they
associated higher performance there's no
more higher clock rates will there be
actually higher performance you know
that's to be determined so is this
really interesting problem are we just
because we're friends with industry
working on doing their dirty work for
them so let me quote three different
sources our good friend Jim Gray a
completely
opposite of the spectrum the Gartner
group recently said what would be big
problems to solve and then John
Hennessy's quotes so Jim who was lost
good friend of Mavis lost at sea almost
two years ago when he won the touring
award 10 years earlier his number one
problem he looked at all the touring or
wore dresses and put a lot of things up
there that are AI complete he thought
the number one problem was massive
parallelism the Gartner group they talk
about things like automated speech
translation pretty interesting problems
like that number two on their list was
parallel programming of things that
would really change the world and you
know John Hennessy who is supposed to be
he's you know on your board he's
supposed to be circumspect is the
president of Stanford University he says
look this easy easy use pale computers
this is a hard problem this is about as
hard as anything the computer sciences
face in its history and that's what
we're proposing to solve this time so
when we think about projecting the
future we think about the context of
what things would be like we're not
talking about two years from now but 7
or 10 years from now well our friend
lease I believe coin the term the data
center is a computer and that's our
perspective to the really interesting
things the server's of the future our
warehouse size buildings and then the
client is the it's the iphone or the
android at the other end so those two
extremes are computing is what we think
is the new client-server computing and
that's what we want to be thinking about
applications and those technologies not
more interested in extremes that in the
middle so the technology what's going to
be happening will certainly flash
memories exciting technology obviously
in cell phones but I think we'll see in
this next year so flash memory popular
in servers and our actual new level of
the memory technology happens pretty
rarely I think they're all these things
you read about what things actually
happened in my career in my 30s years
here well you know core memory magnetic
core memory got replaced by
semiconductor memory fiber-optic cables
you know that that that was a new change
a lot of the other stuff you read
holographic memory all these other
things ccd remember they never made it
so there's maybe we're going to see
flash will actually be another level the
memory hierarchy for
lots of computers we're pretty excited
especially in the rad lab about software
as a service and this cloud utility
computing the sea this smells like a new
thing that's going on instantaneous
elasticity of computing available for
anybody with a credit card what we call
cost associativity using a thousand cpus
for one hour cost the same as a one cpu
four thousand hours we've never had that
before and scaling not only scaling up
which we've kind of had but scaling down
I've never had scaling down before what
if you're going to build a big data
center why in the world would you make
it smaller well to save money and and
say power so these these are going to be
forces that will be happening this next
decade as well since a lot of companies
gone out of business with that bet why
don't we go out of business this time
and we're kind of and I'm not you know
basically knowing the history I'm not
saying we're going to going to solve
this one for sure I just feel as
researchers we're being called upon this
time of need to stand up and really take
a hard crack at this problem so I think
researchers especially those of us who
get government funding well I guess used
to get government funding and I guess we
feel an allegiance to the industry at
this time of need and we need to stand
up and help we need to try that's what I
think I want to say so why do we have a
shot this is the first time there's been
no killer microprocessor out there
there's nobody is building the 15
gigahertz under wats microprocessor
that's going to make all the programmers
just ignore all the work you did if you
want to go faster it's got to be
parallel there's no question about that
so that's never been true before the
other way this may work is which you
know wouldn't break your hearts would be
the utility computing cloud computing
services approach that that's the only
way we're going to figure out how to use
these things with millions of users
using them but that could that seems
like that's got a good shot the past
that's been startup companies working on
it this is the whole industry the
holders room there's no company in the
computer industry that's so we're going
to pass on parallel computing this time
they're all trying to figure out so
there's more people working on it I
think the open source movement is more
important commercially now than it was
the past perhaps they will respond to
the ideas and embrace them perhaps you
know this will be a good test for open
source whether they'll take advantage of
this technically a thing that we haven't
had before is if we think about
single-chip system single socket systems
there's a lot of stuff you could do
inside a chip that wouldn't be
economical if we're talking about
hundreds of thousands of chips so maybe
that'll help we're pretty excited as
i'll i'll say is for those of us who
want to build prototypes of systems of
the future including novel architecture
and a bunch of us used to try and buy
build ships which was very painful and
very very slow and it take you four or
five years and three months after you
got it working the programmers to tell
you what was wrong with it so then you
go back four or five years later it's a
slow iteration cycle field programmable
gate arrays allow us to build credible
but slow but credible computers that he
could have interesting characteristics i
think are convincing enough that you
could influence the real products of the
future and this last thing is you know i
was involved in the risk ideas and the
big thing we're trying to convince
people is the people we willing to
recompile their programs no they won't
yes they will okay well listen this time
industry is pretty open to new ideas
right they bet their future that
someone's going to solve the parallelism
problem so more cores equals more
performance they're pretty receptive
right now to people who claim they have
solutions Busan's to that so it makes a
little bit easier to technology transfer
all right so that's the context what
about what we're doing so you know three
and a half years ago a bunch of us
started sitting in a room from all kinds
of parts of the Berkeley campus and
Berkeley environment from embedded
computing down the hill at the Berkeley
wireless research center to
supercomputing up the hill at the
Lawrence Berkeley National Labs and we
all got together and started talking
about wow this is coming what's the
implications of this I think we spent
the first year just agreeing what
everybody meant by the same terms
because every community had their own
variation of these things that led to
this berkeley view report which for
technical part was pretty rightly read
and kind of obvious i guess obvious now
it seems surprising then that what's the
goal is to make it easy that is
productive to write efficient that is
power efficient and
asked programs that are at least as
correct is sequential programs they got
to be portable not unique to one
computer but move between parallel
computers and you know and we think the
most cost-efficient point power
efficient point is lots of cores per
chip and it's got as these things got to
get better as you add more cores and
they're going to get add every two years
so that's when you put all that together
that's pretty amazing goal but I think
that's the goal that is the research
community we have to shoot for and wow
that's that's some goal so what's the
big idea of the power lab well no we
haven't got a big proceeding idea I
think what we're going to say what we're
doing is we're driven by the
applications and if you look at a lot of
the projects of the past often people
like me will say hey I know how to solve
this problem I've got this idea of
hardware design and we're going to make
the whole world rethink how they do saw
four to match my hardware that and we'll
spend almost all the money the project
on the hardware because it takes a long
time to get working and stuff like that
so or it might be a language thing I've
got this language in this language is
the right way this is this is something
about computer science that we have
faith that if somebody just invents the
right language any problem can be solved
and we haven't solved all the world's
problems because we just haven't
invented the right language yet so we
got kind of so somebody come in language
then we'll build everything about it so
we're not doing it that way what we're
going to do instead of oh the hardware's
finally working let's tiresome
undergraduate to build a you know Global
Climate Change Program in the last three
months the project will actually be
driven by real applications people who
are real experts and so instead of us
doing the old wrong problem the way it
was done 15 years ago will be at the
state of the art by somebody who knows
what they're doing and so we're going to
let these applications and application
experts figure out what really helps
them and they'll tell us which of our
ideas are really big and so i'll tell
more about the process and who people
involved rather than where's the big
idea that you know is right some other
parallel centers already know what the
big idea is and they're betting a lot of
it you know on their hardware mechanism
or language mechanism well that's not
the way we're taking it but I'm really
glad there's lots of centers working on
it because it's a really important
problem
so I'll show these five themes of
applications some design patterns are
two layers of software and opposite
operating systems architecture and then
diagnosing what's going on and turning
it into colorful chart with our
applications at the top in some purplish
color we've got a two layer approach to
software what we call a Productivity
layer and efficiency layer that's the
green and pinkish things at the
operating system and architecture at the
bottom is to support those goals
correctness fans lots of layers on the
right and diagnosing on the left so I'll
kind of highlight that as I go through
the tunnel so applications it's kind of
it's pretty depressing this won't happen
this audience but in other audiences the
conference i'll go and say talk about
the futures multi-core and they'll say
who needs a hundred cores to run
Microsoft Word and you hear a question
like that and you kind of wonder as an
educator whether we failed in computer
science so the outer limit of all of the
best thing we can do with computers is
Microsoft Word that's that's a very I
think there's a you know you think about
historically I think the technology we
built was so cool it was ok for us not
to know anything about applications
because it would help everybody right
and so now that we actually perhaps
better get more in touch with
applications to be able to innovate we
don't a bunch of us don't know what they
are if they also be note we use word
maybe that's the application that's the
killer app for multi-core so so we think
there's a lot more exciting software
that's been written yet and so we wanted
to find these experts the way we did
that is we went on the campus lots of
smart people in the bay area and find
somebody who was really enthusiastic
really needed a lot more horsepower than
they could get today knew what they were
doing and we want to get a variety of
one so these will give you examples
these are once we started with thus
probably still the biggest surprise to
me is there's people in our music
department at Berkeley who are need more
cycles they are desperate for more
cycles I thought they were music
professors didn't even know how to
barely know how to use a computer these
are a lot of them technically savvy
David Wessel not only do they need
factor of 10 or 100 more
Michaels and they have today real time
is serious for them our friends and
garrix need a lot of horsepower but
actually if it doesn't look that good
our Isle kind of hide that if a couple
of slides that way and not our ears are
yours can hear imperfections pretty
clearly where he used to work he said
three clicks and a electronic
performance and you were fired all right
so it's a high high standard there he
has this idea of a 3d sound system
that's this example they've built with
speaker company has a hundred and twenty
tweeters on it seems like the type of
thing ought to be around here somewhere
recreate the symphonic experience right
right in front of you because of spot
hearing he actually works with hearing
aid companies swell to help you hear
better in general and hear music
particularly and I think most the music
professors are performers as well and
he's tired of being bound to keyboard
interfaces he wants much more
interesting novel interfaces to compose
much more interesting music so it's a
surprising and interestingly demanding
application plausibly especially at the
at the client facing thing what
performance means in the future is more
responsiveness rather than you know some
other kind of measure of performance so
how long does it take me to do something
that's the way we should be judging
which is closer to kind of a real-time
evaluation of a system we also working
with people in health according to
American doctors the problem the United
States isn't the doctors not the
hospital is not the problem is we don't
do what the doctors tell us to do so
suppose what happened you had your body
scan you sit there and you meet with
your doctor and he says this is what
your heart looks like on the left there
right now but the way things are going
if you don't take this medicine or you
don't change your behavior it's going to
look like the one on the right and as
you might suspect that little red
constricted thing that's really bad so
so perhaps if you saw your body and your
doctor told this to you you would do
what the doctor said so how big is
coronary artery disease you know it's
gigantic just in the United safe 72
million have high blood pressure 16
million symptomatic half a million
deaths a year this takes a lot of cycles
lots of interesting things that runs on
really big computers now could we
get it to run so that's something you
and your doctor could take a look at in
privacy and feel comfortable those of us
who have cell phones with a lot of flash
memory in it probably had a lot of
images labeling all those would be a
pain why not search based on images so
this idea that was started both first by
Intel was you give a cup examples those
two images of sisters came he's trying
to photograph with those together and it
uses support vector machines machine
learning to try and train and find the
images so that was a content-based image
retrieval we've got speech experts in
Berkeley one of them is really good at
meetings in a meeting room that this
kind of noise can you understand what's
going on wouldn't it be nice if you went
to a meeting and get a diary that's the
meeting diarist where it would list
approximately what everybody said but
identified by speaker so he said Oh
Lewis said something last time what
would be the what was it he said I think
it had this and you'd try and find it
that way for those of us who are on
large telephone conferences wouldn't be
nice as there was kind of a closed
captioned version identifying the
speaker's going along so we think
advances there and finally a lot of
those were mediated we wanted something
more like an application or set of
drivers was different from that and
closer to home so we think of how can we
how could a browser take advantage of
multi-core Jimmy make the browser
experience whatever the computation
piece of that much more interactive and
then since a lot of the web to point out
our apps are being downloaded already in
it is you guys well know what would be a
parallel scripting language so that all
the people will develop web to an Athens
would be riding parallel programs
whether they read or not is there a way
whether to do that so if you can't abide
those applications kind of down the road
well but I like to have well since you
actually take your cell phone with you
everywhere and what you could do is take
a picture of your plate before you ate
and afterwards and if they had a sensor
you might be able to weigh it before and
afterwards so it really know how many
calories you consume not what you said
you consumed and it says you carry it
with you when you exercise it would
really know how much you exercise rather
than how much you said you exercise so
now it was time to eat you know it might
say mom Dave you might want to order
that salad instead of the free pizza
here at the Google cafeteria and well
because this is what you look like if
you keep eating that way but the real
killer application for me would be is
you put your iPhone in your in your
shirt pocket and when this guy comes up
to shake your hand that looks oddly
familiar but you're not quite sure is it
oh that's Luis Barroso you worked
together you had a couple of papers
together and a conference several times
and he support your bike ride don't
remember remember to mention that and at
whispers a year who is and you just
sound so smart hands for me I would for
if there was a name whisper application
and it would get better every year with
more course i would buy the new one
every single year so but the idea here
is that if you think about the pc some
people like to bought the pc back in the
early days the apple apple one was
because they wanted to write visual
basic programs our basic programs right
so that probably wasn't a lot of those
so that what the killer app was
spreadsheets right somebody came up with
the invented the idea of a spreadsheet
and everybody that's sold a whole lot of
pc so the parallel computing debt isn't
that all you have to make all the
software in the world will run well that
somebody will come up some killer apps
that'll be need a lot more cycles than
we can get today and that that'll
attract people to want to buy newer
machines with faster and faster or I'll
way to think of it if that works if we
can do that if it's a field we can I can
figure out a way that apps need as many
cycles need and we can figure out a way
you just add more cars everything runs
faster and that'll if that'll last for
the next 20-30 years we're in Fat City
right that would be a wonderful world to
be living in alright so that's kind of
that's the apps that are gonna drive
what we do we did this thing in the
Berkeley viewing port has said and this
is a relatively controversial claim when
we talk to pee friends and high
performance computing they said look
there's about seven things that are
going to dominate high-performance
computing and so that because there were
seven they got called the Seven Dwarfs
so we started off from that idea I
looked at a whole bunch of other areas
embedded computing desktop computing
databases games AI machine learning
computer aided design said do these
dwarf supply and a
shivam did and but there were others
that that weren't there we needed to add
so the first seven columns there in this
temperature cart red beans it's really
important for these applications yellow
and green are milder and blue they don't
really matter very much and see this
checkerboard pattern that which led us
to these dozen things s we talk look at
these five applications do these things
apply and basically they did so the kind
of a fairly controversial claim is
although the data types of differ and
they'll be you know lots of different
data structures as well in some of these
applications these things will find
themselves that kind of the bottom layer
of a lot of software so if we could
figure out how to make these types of
things run well in your programming
language or in your machine that would
bode well for the future so that's our
our themes that way now what about the
software which is really the heart of
what we're doing and I'll just have to
give you a quick overview there so since
lots of types of programmers can use
things they use computers today not just
the people in this room we're splitting
it into two pieces an efficiency layer
the ten percent of the days programmers
are building the operating systems and
libraries that are really after quote
unquote bare metal efficiency they don't
want to give anything up and then for
the rest of us for domain experts they
have to be able to program these
computers and will provide those
frameworks for them but they're morons
and productivity and not getting the
ultimate and performance there they'd
like to get good performance primarily
what we're saying is productivity people
need to scale up as you add kors it gets
faster but the efficiency procore
doesn't have to be spectacular the
efficiency layer people probably want an
efficiency procore to be good as well as
scale up and then the big challenge is
composition how do you how do we compose
things together so that they're useful
so as I said this is kind of what we're
trying to do rather than we've done
Koscheck sin who's a younger faculty
interested got done interesting things
with correctness and testing he is
thinking about interpretive productivity
letter or making it hard to express
problems that will have non determinism
so they can still get their work done
but not get them into holes but
we're going to have to put a lot of
things into a lot of efficiency layer
all those nasty things that can happen
and but maybe we can do things with
testing and verification to try and help
that so an idea that he we had good luck
on sequential code is verify what you
can to what you can't automatically
generate test cases to exercise those
what about compilers and cogeneration
well these days getting performance note
I mean he talked to the architect they
don't know how fast stuff runs nobody
knows how fast stuff run so we're in a
very weird era where how the world can
compilers figure things out when you
know no human being can figure it out so
the response to that is what's been
called auto tuners they actually search
the space they try things and see which
is faster this particular example it was
when you would have blocked something
how many rows and columns and the little
red square at two rows and 4 columns
turned out to be by far the highest
performance in that temperature chart
how could a compiler have ever figured
out there's nothing obvious about that
so this idea is you generate a lot of
variations and you try it on that
particular machine to see what runs well
so it's experimental code generation and
there have been many successful attempts
at that it's particularly for sequential
computers and we're starting to see
something for parallel computers so
here's an example of for multi-core
systems from you know a variety of
makers these are very different the
cells you know the club returned and
opteron are very similar from intel and
AMD x86 out of order execution
prefetching kind of the conventional
wisdom designs cell is very different
designed for the playstation the niagara
from sun was designed for very thin
servers and the sea they have much lower
theoretical performance in terms of
floating-point operations at that bottom
line with intel at the top and much
lower clock rates for intel for example
for the sun example but what happens
when we ran naive versions of these
programs on those computers first of all
for cell it's so unusual there's no
naive version that runs on cell
but look how little performance we got
from that any versions on the Cloverton
is you know 175th of the potential
performance by a lot of tuning we could
perform we could get things better
exploring the space trying things out we
could get the things learning a lot
better and you know much bigger speedups
you know factors of 1.5 to 3 then you'd
never expect that from a compiler but
auto tuning it knows more about what's
going on doing more experimental things
so we think that's going to be a
promising vehicle here now auto tuning
takes pretty smart people in
architecture and applications we've got
recent results using machine learning
kind of from our my rat lab hat what
it's trying to do is learn relationships
between a set of optimization parameters
and the performance so we try these here
a bunch of parameters here's the
performance we end up we see these two
spaces can we find these correlations
and figure out what's going on and we
were able to do that for one of these
kernels a stencil kernel and get within
one percent of what the expert was able
to do and a lot less time because we're
using the machine learning techniques so
this is promising because it's pretty
hard to build these auto tuners by hand
can we use machine learning technology
to help us automate the creation of auto
tuners and so that this could be a more
general technique so we're doing auto
tuning not compiling we got these two
layers rather than one layer and
libraries go ahead what about closer to
where i live at the bottom of the stack
well you know small is beautiful is
going to guide our designs we think the
kind of the most power efficient and
cost efficient point is modestly
pipeline CPUs with you know not a lot of
dynamic stuff in it because that's
wasteful than energy we're thinking
about something more reconfigurable in
the memory hierarchy sometimes it's
local store sometimes it's a cache and
really fast partitions you can do
technically they operating systems
deconstructing them going back to
virtual machines and if we're going to
have this ability to partition the chip
very efficiently and hardware can we
provide that partitioning via thin
hypervisors to applications so they get
full software accesses bare metal
efficiency how are we going to prototype
these things as I previewed before we're
part of this ramp effort
see what rap stand for is all the our
words blend together in perinton LC
research accelerates research
accelerator for multiprocessors and this
is f PGA's have get get bigger by
Moore's law so you can now if you're
trying to do multi-core you put a lot of
cores in one of the PGA which makes it a
much simpler problem to build than if
you were trying to build one processor
of a lot of fpgas so we put this product
you know to demonstrate the idea this is
a thousand processor system in one rack
of the equipment and since we did this
prototype Chuck Thakur of Microsoft
built helped us design the next boards
and there's a third party company
selling them and there's a consortium
building software and Gateway around it
from a half a dozen universities so we
think being able to emulate ucf pjs will
let us more explore the space in a more
interesting way even it runs slower how
about in terms of the performance
evaluation so we think we're going to
have to do measurement to be able to
help that out and then the questions are
what do we tell the efficiency level
programmer well we can try and explain
what fraction machine he's doing and
what's why he's not getting all the
bandwidth but for the productivity level
programmer it's got to be the higher
level of abstraction so an idea we're
kicking around is maybe we need to come
up with the equivalent of the moral
equivalent of I Triple E floating point
standard we need to have a performance
counter standard that everybody has to
has to run so that we can have portable
debugging performance tuning devices
across machines that a maybe and if we
had it then the operating system could
use it to more efficiently schedule
these machines and our argument is in of
course whether you know it or not the
performance counter thing is this thing
that hardware architects put in to see
how it's working in they don't have to
work because nobody supposed to care
about them so we should change that
culture and look if we're going to get
twenty percent more performance as a
result of measuring if that's true why
can't you get spend ten percent of the
resources on that so that's been the
justification in the past to put in some
wild and crazy branch predictors and
stuff like that well we get more
performance than the resources a good
idea well I think in this new era where
we don't know how to program these
things better measurements going to get
a much better return than
you know then transactional memory and
all these other crazy ideas that people
are trying who knows that these are
gonna work measuring what's going on
accurate things is going to make things
run faster so this is the thumb thing
we're going to pitch and we're going to
build it all right this ramp will let us
build it will be able to have the
world's first implementation of this new
standard or will prove it on this this
implementation that runs real software
and demonstrated the value of it
hopefully the companies will embrace it
so kind of putting the picture year why
are we getting these experts to work
with this well we're going to help them
get their software running on the latest
computers and we're going to be able to
afford to get a lot of them and we'll
even because we have close to Silicon
Valley we can get stuff before it's a
real product to be able to use and in
return they let us use their apps to
innovate in the architectural operating
system and they're going to you know
kind of interestingly ramp might be the
best place to run computers in front of
your parallel program because you're
going to have as many courses you want
and it's going to have as much
instrumentation what you'll know more
about your program and ramp then you'll
know in any computer in the world it
just runs slower but since you want to
port your things a lot of platforms if
you run it to all the Intel and AMD ones
and from Nvidia and hours you get kind
of the best of both worlds you can run
in lots of real things and then you'll
get these insights from a ramp and we'll
be able to build better computers
demonstrate them for the future all
right so I'm going to pause this is the
summary of the kind of the power lab you
know how do we get here what are we
working on and then I'm going to get
myself since I spoke plenty fast enough
despite the 10 minute delay to do the
performance model stuff after this but
I'll take questions now is this an easy
thing where people just raise your hand
or is it okay yeah
yeah okay the question given
software-as-a-service aren't we done i
think i think i believe i can tell you
right now in the city of berkeley i
don't get cell phone coverage in parts
of berkeley in fact you know there's
this big city council debate whether
they'll put up more cell phone tower so
i think if we had ubiquitous
communication anywhere i went there'd be
an argument that i'll just i'll have a
thin thing cloud but you know there's
going to be hundreds of gigabytes of
flash in this thing i think there'll be
times where i'm going to want to be our
there'll be low latency things like
speech recognition stuff like that i
believe there will be a case for local
communication at all times as well as
the future I think the application mode
of the future is I'm just things that
you guys do here right it's in the cloud
and it's on the client it's in the data
center and it's on the cell phone so I
think there are interesting challenge
will be can we figure out how to make
clients stuff run fast and take
advantage of more things in the future
as well as in the cloud and I think you
know that's at least as researchers if
we can do that the it's great right but
you know kind of leaving researching
side is from a commercial perspective
the inability to take advantage of
multi-core many core maybe an
accelerator towards putting things into
the cloud right if we can't figure out
how to you know if if companies like you
can get much more cost performance out
of multi-core than people can for
laptops maybe that will be shoving
things into the club question I'll to go
back and forth okay
you look like in a few years of skating
in the same rate as a number of course
ok the question about memory hierarchy
and memory bandwidth so basically if you
look at the 25 second summary of
computer architecture less 25 years
basically the computer guy who the
people who call themselves Peter
architects got to pick the packaging
technology and all these startups in
this area would do very innovative
packaging technologies invariant memory
faces and then the person making the
decision said do I want to bet my baby
on whether that packaging technology
works and they'll we're going to know
that if it's in the field for a few
years so Marlys everybody said I think
I'll just go build a bigger chip okay
and put a lot of caches around it so it
doesn't need innovative memory
interfaces so I think going forward
we're probably not going to be able we
will be able to get away with that so we
have to hit we're going to do you have a
lot more cause we're going to need more
memory bandwidth off the chip and I
think well there's I think 3d packaging
more innovative ways to interface with
memory has a lot more performance in
there we've just never explored them I
think we're going to have to do that so
I think the good news even though it's
hard to find the evidence that there's
the ability to get a lot more bandwidth
to memory I believe it's there as I said
you know I think the flash memory is
going to be very exciting so kind of a
related question is what about IO but if
we get a new member a new level of
memory Harkey that's not mechanical you
know this you know just like I still
have colleagues use the word core when
they referred to memory you know it's a
possibility you use the word seek time
in rotational delays and people won't
know what you're talking about in you
know in five five years from now so I
think there's some reasons flashes and
phase change memory is a real reason for
optimism for i/o and I think there's
innovative ways to go to memory right
now I think another kind of on the
memory Harkey what's going to look like
the Cell processor reuse did not use
caches at local store and the it had a
veal as a VLIW instruction set really a
pain to program for but the funny place
we are today from efficiency level
programmers perspective as architects
put caches in to try and make it easier
for programmers the young have to worry
about the mini hockey
today at the efficiency level people are
trying to reverse engineer all this
hardware mechanism prefetching and stuff
like that so it's a very weird state
putting hardware in supposedly to make
it easier but it's not easier they're
trying to reverse it if you just put a
local store in that's easier I know what
it does i can put in dma that's
predictable things like that so we're
looking at a memory on our key that's
flexible but sometimes it's caches maybe
for productivity level programmers and
sometimes it's local store for the
people know what they're doing because
local store is kind of easier for them
to reason about and use that's along
another long interview yeah
you
the questions about interconnects so
there's on the chip in between ships so
on the chip you know power is the big
it's where are we today power is what
architects worry about not transistors
you can put my transitions you can turn
on so we could be you know this is a
route I mean what a strange world we can
be thinking about structures that you
put on ship that use up transistors and
gates and all that stuff that are turned
off a lot of the time that you only turn
on when you need it so we're thinking of
having that the power efficient way on
the chip will have many dedicated
networks on it that that'll be more
power efficient and more bandwidth per
you know / effective wat as a way to
connect on the chip off the chip the
serial line technology is very exciting
I have colleagues who are very
interested in optical interconnects so
optical neck on the chip in particular
so when you go off the chip you don't
have to have to have that electrical
optical connection so that there's
arguments in there so that that's
promising and of course you all here
today because andy bechtolsheim gave
Larry and Sergey some money to start and
e vector chime is working on
interconnects in the data center
thinking there's a opportunity to do
data center interconnects much more
efficiently than the Cisco's of the
world where he you know it used to work
so I think there's a lot of you know
pretty you know pretty interesting sets
of challenges that have to do with power
on the chip and probably power and some
of the data set of things off the chip
in the data center in interconnects yeah
okay all right yeah I'm not going away
but I going to get to this performance
model I've given this top three times
and every time I never got to this so
we're going to go at this time we're
gonna do it okay so why do you need a
performance model for multi-core well
like I just said there's no consensus on
a few years ago everybody built more or
less the same microarchitecture
everybody was out of order branch
prediction
all the stuff you didn't matter the
company you knew what was inside boy the
world is different today and right now
to get a lot of performance hopefully
they don't always be true you got to be
really good at the application and
really good at architecture to
understand what's going well not very
many programmers know both architecture
in the applications and pretty much
performance is the reason many people
are worried about multi-core so they
gave me an idea the diversity here are
the four machines I talked about before
kind of graphically these are all dual
socket system so all of them have two
things there the the Cloverton is
actually done with two independent ships
in the same package that's the socket
and it's a shared bus interface the
opteron the more modern design with the
bust going to DRAM they have separate
DRAM interfaces The Sun has this very
thin kors very low clock rate so 16th in
kors across these two sockets each 8-way
multi-threaded that's a pretty radical
design and I mentioned about the cell
that has local stores lots of the simdi
thin design so all over the space is
where things are today who you know I
don't see any reason that this will
converge shortly because we don't know
what the best thing to do is so this new
model was focusing on these Dwarfs I
mentioned it before I'm going to present
the floating-point version although we
thought about others today and so this
new performance model is not trying to
accurately predict how fast it's going
to run its what's called a bound or
bottleneck analysis so we're trying to
figure out where the bottlenecks are the
idea is from either an architect's
perspective or programmers perspective
what they're interested is why can't
this program run faster we want to know
what the next bottleneck is you have to
get through the go faster so that's what
we're doing and right now these examples
are going to assume the parallels of and
load balancing on your shoes we've
talked about that we've thought about
that other but we're not going to show
that in the slides today so what's the
bottom line for floating-point programs
well peak floating-point performance is
a bottleneck and a lot of the
applications day everything doesn't fit
in the caches where the applications
we're talking about so what one of the
key figures of Merit that we're going to
use that's going to drive this bottle as
operational intensity and this is the
average for this set of examples how
many floating point operations do you
get per byte that goes to dram this is
after going through all the caches so it
actually gets all the way up to the
memory system for everybody how many
operations do you have to do so that's
we think is an interesting figure of
Merit for these Dwarfs so now we're
going to this is not only a performance
model it's a visual performance model so
the y-axis is going to be gigaflops for
floating-point programs that make sense
and it's on a log scale so you can look
that up actually just look that up for a
computer in this case the red line is
that 16 this means that's the peak
floating-point performs that Hardware 16
gigaflops okay now what we're going to
make the Y scale the X scale is going to
be operational intensity so this is the
number of floating-point operations
everybody goes to DRAM well if you think
about that it's a number of floyd point
operations per beer am well we can plot
another line that is the memory
bandwidth since gigaflops per second
divided by flops per byte is gigabytes
per second that's that 45 degree
diagonal line so because its operational
intensity at the bottom right it's how
many flops per byte and the memory speed
is bytes per second so you'll turn in
that into a 45 degree line that connects
intersects with the peak floating-point
performance so this is the model right
this is the model report saying right
here is that slanted piece and the flat
piece is what we call the roofline
that's where the name came from so
depending on the operational intensity
you can't go any faster than those two
lines you can't go faster the peak
gigaflops per chip and memory bandwidth
in the area where you have few
operations for bike going to DRAM that's
it's another limit ok so far so good so
what we call the point where those two
things meet on the roof line is the
ridge line and the ridge line is the
minimum operational intensity where you
get the peak performance right if you to
the left of that your or if you do the
right of the bridge point that means you
have enough operations per byte that
your computation bound so that
mine would intersect run it do you think
that is a poll that runs into the roof
well if that's your operation intensity
you could theoretically get the 16 gig
of flops performance if your memory
bound if you're left to the rich point
then the memory performance is going to
limit what you can do and you won't be
able to go higher than that slanted part
of the roof line wherever you intersect
okay so it's a simple model that we're
looking for bonds of Bob Knight
paralysis gigaflops on the Left
operation comes into right and we get
this roof line with ridge point so now
the question is well that's interesting
to you what are the roof lines look like
for these multi-core computers I told
you about and where is the real
operational intensity is it way over to
the right where that should be easy
everything but you compute bounders our
way over to the left for everything's
memory better okay next slide you use
all so those are the four machines we
showed the slides before with the Intel
Cloverton being that green line pretty
steep slant Barcelona being the red line
the cell blade from IBM is the blue line
and the Victoria Falls to the purple
line there and so look at that clover
clover didn't has almost no flat spot
there right that's almost the whole most
of the region that I've plotted here in
terms of the flops per byte is memory
bound and that's true right at the rich
point is 6.7 double this isn't floating
point operations for every bite that
goes to dram or you could fit was an
8-byte double piston floating point
operating that's like 56 floating point
operations for every double word that
you read from from drm so that's that's
a lot of operations the partial is alone
is only a little bit better at 4.4 flops
per diagram on the other hand you get to
the cell blade it's down to kind of two
thirds of a flop for bite and it son's
the amazingly one third point per bunt
here so so now where are these where of
operational intensities for these
kernels show up so these are real ones
sparse matrix the stencil a structured
grid code and FFT and those are along
the left its operational is a quarter or
a half about one and one and two thirds
and you can see that these lines more
than
up so even though the peak
floating-point performance for the two
leftmost lines are much lower than
Cloverton in barcelona because their
memory bound but the cell and Victoria
Falls have higher memory performance in
practice you get more performance of so
so that's so architects if we if you see
a roof line for a computer you can get
an idea what you need to do to get the
performance out of it right we're how
hard is it going to be and I'd say a lot
of things we look at our list the one
flop per byte that goes to the D ramp so
that's the peak performance what happens
if you're below it well it turns out
what you can do is you can try and
figure out what the optimizations are
that are commonly done for that type of
program and plot them as ceilings below
the roofline and then the operational
intensity is going to tell you which
ones do so let me show you that so if
you were cared about floating point
performance one of the things you want
to do is at the top is have an equal
number multiplies and adds there tends
to be an equal number of floating-point
others and floating-point multipliers so
you want to have an equal number of ads
and multiplies if you have that option
to get the peak performance so that's
the first ceiling there at the top if
there's an imbalance you can't get all
the way to the top now on these x86
architectures but this is a bot about
they have the single instruction
multiple data operations that you can do
to double-precision floating-point
operations in one instruction if you
aren't using those instructions you
can't go above that ceiling and then
these will execute up to four simdi
instructions per clock cycle if you're
not executing for instructions per clock
cycle you're going to be at the next
lower ceiling so these are examples of
computational ceilings what wood
ceilings look like for memory well those
will be slanted ceilings so prefetching
is very important to get the peak
performance out of the memory system so
if you're not prefetching it comes down
a level and then on these dual socket
systems if you're connected to DRAM if
you going through a chip to somebody
else's DRAM that's slower than going to
your own DRAM so there's a NUMA effect
as well so if you get a colorful version
of this kind of the yellow memory thing
and the blue computational thing in the
intersecting area which is green yellow
+ blue is where you have to optimize
both computation and memory system so
what the idea is those there's those
four lines we saw before for the four
different kernels if you're way over on
the left don't even have to worry about
the computation stuff you're limited by
memory so just worried about prefetching
in nuuma if you're in the middle you
don't really have to worry about Cindy
or multan even worry about multiply add
you're never going to get there just try
and get the instruction level
parallelism there so depending what your
operational intensity is you look up and
see what you should be working on and
there's it tells you what you don't have
to worry about and more importantly
what's the best you can do is if you if
you're in the second one and you've got
seven gigaflops per second that's pretty
good but even though the peak gigaflops
is 128 still seven is about the best you
can do okay so this is performance model
so where are we on this somebody besides
us actually used it that's always
unusual test for academic research is a
number of users exceed the number of
authors the paper this person did a
summer intern and he used two different
kernels there were financial kernels PD
solvers on for very different multicores
GPUs well standard Intel processor the
upcoming Larrabee and two NVIDIA GPUs
and the first version of this PDE he
used he looked at the bandwidth that fit
in the l1 cache so he didn't have to do
any optimizations for it the second one
didn't fit so he could use the help
roofline to help figure out what
blocking he needed to get the peak
performance and what it could get we're
looking at non floating-point kernels
presumably a lot of you are interesting
other things like sort and graph
traversal and there's a whole you know
we just scratched the surface it's a lot
of architectures a lot of algorithms
particularly we think this will work for
i/o which we haven't you know worked on
yet but so we're pretty excited right
now so let me wrap up in just perfect
timing the talk and see if it's time for
more questions so why are we doing this
if nobody can build a 15 gigahertz 100
watt uniprocessor if you can there's a
big market
for you up there and you know the
industry's best future on this we're
used to getting more performance over
time maybe we can keep selling computers
even though computers don't good faster
but we don't have a lot of experience
with that I think from a researcher
perspective you can argue this is the
hardest problem in 50 years that's both
this tremendous upside opportunity and
the trend its downside if we don't and
not only that from a research
perspective here's the chance to
reinvent the whole hardware software
stack there I don't know when the last
time you had a chance to reinvent the
whole hardware stack but right now if
you could solve these problems you could
change a whole lot of things if you
really deliver on that so in terms of
are you part of the problem or not so
it's not failing this time it's it's not
trying so are you going to be part of
the problem or part of the solution okay
I'm ready for applause
I'm so surprised and this our sponsors
and if you wanted to learn more about
the roof line there's Sam filed his
dissertation yesterday okay I'm ready
for questions right on but let's try
okay first so I think in terms of
questions about how do you quantify
composability I think for right now at
this early stages is can we we have a
bunch of interesting component
technologies like auto-tune errors and
these kernels and also can we put this
together in a nap and have the
application you still have pretty
reasonable performance will that work at
all and then can some of our
applications experts like productivity
level people can they use those to
compose things together and get
pre-performance so for right now it's
going to be what kind of the measurement
will will it work at all and how fast
does it run down the road it will be
well how much effort is it and as we
said in the Berkeley view thing there's
a whole bunch of parts of the police the
Berkeley campus and probably hear that
does human subject experiments computer
scientists pretend like you can't do
experiments with human beings possible
because there's too much variability
well you know human beings very all over
the campus yet they do experiment so
down the road it will be if we can get
things that work at all you know
performance rise and correctness rise
can we do these experiments to see how
easy it is for these tools to work can
can more domain experts use them and how
long does it take them and stuff like
that that'll be you know down the road
measuring both how hard you know
performance sides as well as
productivity is how long does it take
how many make mistakes to make this
stuff like that
yeah he talked about reinventing
software stack this wounds etc etc
change their way
the question is reinventing software
stack Lisp Lissa nobody uses list is
that the list was I let's be smart
better yeah right always pick the
moistures okay so this will get me i'll
try and restrain myself here let's see
the reason we could invent the whole
software stack is the I characterizes
industry if those of you knows football
industries thrown this Hail Mary pass
right and he looks like the game
football game you're behind what do you
do you throw the ball it's farsh can so
it turns out it's pretty easy to throw
the ball it's the catching that's the
hard part right so the industry is a
industry is it is hoping somebody will
catch it right and if you come to them
say well we can catch it here's the
changes you got to do I don't if you can
really deliver on that goal of
correctness and efficient and scale up
and stuff like that I think they'll
consider all kinds of ideas right now
well so I think we could really invent
things now in terms of you know things
have never worked with my rad lab cap on
we've been teaching courses at Berkeley
about Ruby and ruby-on-rails ruby is a
pretty cool language there are people in
you know they're getting real
productivity games you know in terms of
we we can do this in class we have a
bunch of undergraduate students who've
never seen Ruby never seen rails and
they're building apps that they're
excited about deploying them on ec2 of
three and even getting some users in in
a semester and how long how big of the
programs i think this must be 1000 to
2000 lines of code I never thought I
would see 1,000 or 2,000 lines of code
in my lifetime you know it looks like
something you'd say well it was thirty
thousand lines of code or something like
that and doesn't really work they seem
to work they look they have a pretty
nice user interface the one part one
person was trying to do medical records
for boarding schools in third world
countries they had user
in Senegal during the class they had
reached out and they were giving
evaluation so it's the this is a very
productive environment ruby-on-rails has
its own set of advocates it's not just
you know the academic community so I
think there's reasons to believe of very
innovative software systems are out
there kind of just like Lisp many people
replicates list I think they're out
there and there's I think there's a
chance for us to create the technology
to make these mainstream it's good ruby
on rails or a ruby like language
something like that the productivity
language for multi-core what we need to
do is figure out how to scale up the
number of course so maybe Ruby is
inefficient on one core but it can use a
hundred course well if that were true
that would be a pretty powerful argument
for Ruby because it's I think
unquestionably it's more productive but
everybody argues about scale or
efficiency so I think I'll cut myself
off their lease do you have a question
okay yeah
yeah so I think in the book leave you
thing we is this argument for
heterogeneity and it's just found by law
right if there's some pieces that are
more sequential and some pieces that are
more parallel you could be this bound in
bottleneck analysis is that unlawful
think you could be limited in
performance by this less parallel piece
so I think there's an argument for
having some faster course what's
happened I think commercially I think
it's unacceptable for an AMD or intel to
bring out with a processor that would
genuinely run some programs slower that
may be just too hard a business thing
for them son on the other hand is this
radical thing in and somewhat a.m. to a
very skinny slow course that would run
some program slower now I think I'm less
worried about the business side of that
then technically right if that's the
most efficient if we can figure out a
way to use that and that's the most
efficient point and MIPS per watt and
things like that my guess is going to be
I you know and can operating systems
handle that kind of scheduling know the
schedule the pieces that are more
sequential on the fatter cores and the
ones that are more parallel with
intercourse you know I I'd say kind of
technically we could do relatively thin
I you know I think this five stage risk
pipeline you know that's we built that
our version was like 30,000 transistor
so that's a very efficient point in
technology maybe vector is pretty
efficient so I think of its power you
know what's very efficient in power and
transistors points I think historically
those have been so I think that I don't
see a reason to go less than that and
there's an argument for getting to that
point but what's the programming
challenge is historically it's always
been it's better to have fewer
processors and fatter cars because of
all the problems government parallelism
if we break no cut that Gordian knot so
we can really embrace a lot of
parallelism that I think it will head
towards a lot of the skinny cores in
maybe some fat course because they'll
most often go ahead
Yeah right yo happens to legacy code
right and people a sec Adem excellent ah
yeah so I think it would be a mistake to
come up it may be possible it'd be hard
to come up with an architecture idea
that could not run that code it may be
possible to do that i would not
recommend that i think just like I think
you need to be able to run words of the
world there's a bunch of software I use
that I don't think it has to get any
faster but I want to run it so I think
you have to keep running it I think in
the you know if it was a start-up model
business model is there not there's an
opportunity in the next few years to
automatically paralyzed so that you can
get some value out of eight cores from
legacy code that's just not you know and
if I thought that was the past of the
future that we could automatically
paralyze any code including legacy code
and we'll get up two hundred cores if I
believe that you know maybe working that
but I don't believe that all I think
we've tried that awful lot I think you
may be able to get modest parallelism at
a lot of cars you know for a while but
once we get a lot more I don't see how
that could possibly i get amazed if that
could work course that'd be a huge
breakthrough but i don't think that and
so I'm not as worried about that
although it's commercially really
important it's hard enough without the
legacy cold problem go ahead
so you know I'm right now wearing two
really interesting hats and one's the
rad lab which is machine learning to
make data centers easier run easier data
centers so we're really that part of us
like the movie i'm really interested in
distributed applications and how to
design them to run well work well with
machine learning into operation all that
stuff so that's not so much that's more
of the focus of the rad lab the power
lab is more focused on the client stuff
i think the technology we invent for the
clients will carry over so i think the
auto tuning work I just can't imagine a
future that auto tuning be amazed if you
can solve this problem without auto
tuning that there's so many things there
so I think there's a in some of the
programming layered stuff I think that
stuff will carry over but kind of at
Berkeley both of those things are going
on there just you know not there one
floor apart so I think but I think you
know the apps of the future will be our
cost both of those things so they will
be distributed computing going on
research at Berkeley in the client stuff
but not in the single lab that answer
your question yeah okay
yeah the question is what about
predictability small changes into
unexpected consequences I think there's
this version of what the future might
look like the reason it's more promising
is that will redefine what performances
so I mentioned like responsiveness is
another definition but I think part of
responsiveness expression thing about
music is predictability as well so
suppose what you could do is if you got
a lot more cores is you get them more a
kind of stable more predictable system
it wouldn't sometimes sometimes it runs
great and sometimes it it runs poorly so
and I think there's a you know borrowing
kind of from the other lab there the
machine learning stuff the machine
learning is an interesting way to
explore a much larger space than we
would normally explore with real
computers and it are already in the in
the auto tuning there's a lot of
combinations if we're going to run it on
things we have to come to use heuristics
machine learning has this interesting
ability to kind of separate the wheat
from the chaff and try and see what some
of the fundamental principles if that
actually works we've already thought
about because we could search a bigger
area we've already thought about
performance and power could we get a
kind of predictable performance or more
stable performance as you vary the input
data sets or something it doesn't fall
off a cliff so I think that might be a
niche you know you know be great to look
10 years in the future and see whatever
happened but I could imagine that's the
way we define success sure maybe it
really didn't get this linear speed-up
they expected for more more cores back
in two thousand eight but what you got
is much more stable performance or you
were are a lot of the time it was the
responsiveness most of the time you're
not using the cores but when somebody
wanted to use it the distribution of
response times was much later so that
this responsiveness predictability I
think is a really interesting way to
wave at potentially evaluate that we're
really seriously going to look at both
of those things response to this
productivity it that might be the way
that's going because you know kind of my
friends who've done this for decades by
good friend John osterhout who's now at
Stanford he just he did this for a
situation he doesn't think it's going to
work he thinks you know fail there's no
way its course and
so you know and he's a smart guy and I
think there's a reason to bet against it
so but maybe if we redefine if we look
at computer systems of ways we haven't
looked in the past there shouldn't we
plenty of things we could do better than
we do today and maybe by redefining what
people really want in new ways that'll
give us the flexibility to actually be
successful this time and like the past
okay thanks
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>