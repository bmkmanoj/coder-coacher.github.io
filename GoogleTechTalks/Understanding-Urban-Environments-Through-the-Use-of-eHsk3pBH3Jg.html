<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Understanding Urban Environments Through the Use of... | Coder Coacher - Coaching Coders</title><meta content="Understanding Urban Environments Through the Use of... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Understanding Urban Environments Through the Use of...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eHsk3pBH3Jg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">ranko was previously working at Boeing
and he got his degree from brown
university and he will tell us about how
to simplify dance models with the three
buildings while still preferred
preserving legibility thanks a meal
thank you guys for coming here it's a
great privilege to be a cool so I'm here
today to talk about kind of a broader
research question that I've been working
on and the idea is try to understanding
urban environments see the use of a
thing called urban legibility and I'll
talk about that a little bit so this
talk is actually going to get broken
down into about two parts the first half
is going to be on simplification of the
urban models while maintaining urban
legibility and this is a talk that I
gave at siggraph this summer so I
apologize to people that were there will
be repeat almost almost verbatim repeat
and the second half will be on
discussion and future work where I'll be
talking about some of the things that
we've been working on as well as some of
the things that we would like to be
working on the future and before I start
I'm just I just like to talk a little
bit about what we're doing at the
visualization Center at Charlotte and
specifically one thing that we've been
really interested in is in this idea of
knowledge visualization and to give you
an example of what we mean by knowledge
visualization you can imagine like you
have a screen of labels just lots of
text and they're kind of overlapping
each other now if somehow you can take
the screen and extract some sort of
higher knowledge either from the scene
or from the user then it is
theoretically possible that you can some
you can focus your resources on the
things that you want the user to focus
on so in other words we think of it as
you want to minimize the resources that
you're using but you want to maximize
the information that you're giving to
the user and resource could really be
anything I mean it can be a cpu time it
could be a number of polygons and in
this particular case it really is just a
number of pixels that you're using on
the screen
give you an idea what what what's what
we consider has a great knowledge
visualization paper here's something
done by agrawala installed who are both
at Stanford on there with the patents
group and this paper is on rendering
effective route maps and here's an
example of directions that would be
given by mapquest and you see that this
direction is physically accurate it
shows you exactly how to get from point
A to point B but that's that's that's
that's all it is you don't really get a
lot of information out of this whereas
typically speaking if you ask somebody
to give you directions this will be
something that people would draw you so
there's hand sketch thing it's not at
all physically accurate I mean this
showing the 10 highway 101 into a very
small amount where if emphasis more how
you get onto highway 101 or 110 and how
to get off so of course in their paper
that was published at siggraph 2001 they
were able to mimic what humans typically
do and in this case really showing off
the important information in this task
of giving people directions and maps so
we want to take this idea of knowledge
visualization and apply it to urban
models so here's a here's a model of a
city in China and the question is what
is the knowledge in this scene what is
it that we want to be able to preserve
interest and to highlight to answer that
we turn to this idea of urban legibility
and urban legibility is a term that was
made famous by Kevin Lynch in his 1960
book called the image of the city so
what he did for this book was that he
went around the city of Boston and he
just asked local residents and asked him
to sketch out just kind of use a pen and
sketch out their immediate surroundings
so what he actually got was a stack of
these images that you see on the right
here when people just simply sketched
out you know this is where I lived this
is a big road around me and so on and he
took the stack of sketched images and he
categorized the important things into
five groups he kind of rise in the past
which our highways railroads roads
canals edges shorelines or boundaries
districts industrial residential
district notes which are which you can
think of as areas where lots of
activities converge so as an example
times square in new york city and then
landmarks and limericks can really be
anything it can be a tree it can be a
post sign it can be a big building it's
whatever people use to navigate
themselves in the urban environment so
Kevin Lynch defined this idea of urban
legibility as the ease with which the
city's parts may be recognized and can
be organized into a coherent pattern so
that's kind of a mouthful but to me what
that really says is if you can somehow
deconstruct a city into these urban
urban legibility elements we can still
be able to organize a city in that
coherent pattern the use of urban
legibility in computer science really
goes back a little ways Ruth Dalton in
our 2002 paper just chronicles the
history of what people have done in
computer science in the use of urban
legibility and it kind of broke down
into two groups there's one group that
tries to justify whether or not the idea
of urban legibility actually makes sense
so what they did was they try to figure
out if these elements are actually
important to human navigation and what
they found out interesting enough is
that past edges and districts are very
important to navigation but landmarks
are kind of questionable there's some
groups I think that is very useful
there's some groups that say that it's
wholly useless and the one L the one
element that's missing here is the
element of nodes and people have not
really been able to successfully
quantify what really a note is so there
hasn't been as much research done on
trying to figure out nodes are helpful
at all and the other group of
researchers just use urban legibility
and in particularly in graphics and
visualization most notably ingram and
Benford has a whole series of papers
where they try to use urban legibility
and navigating abstract data spaces
so the question is why do we decide to
use urban legibility and to give you an
idea here we take an original model
these are a bunch of buildings in our
Atlanta data set looked at from a
top-down top-down view this is what you
would get if you use a traditional
simplification methods such as cute slim
and I'm assuming people know what Q slim
is but what you see is that a lot of the
buildings get decimated to a point where
it doesn't really look like a building
anymore whereas our approach is a little
bit different we take an aggregated
approach and this is why you will get
and if we apply a texture map onto our
model this is what you end up at the end
so it's actually really interesting that
when we take these four models and we
put in the flight through scene just
kind of a test scenario and we measure
how many pixels are different from the
original model and this is a graph that
we get so you don't have to look at it
carefully but the important thing here
that I'm picking out is that basically
using all these models they end up with
very very similar differences in terms
of pixel errors and what that says to us
is that even though you look at these
four models and you say well they look
very different to me but in effect if
you measure purely quantitatively using
pixel errors they actually come out to
be very similar so what that really says
to us is we can't really just use pixel
errors as the driving force behind
simplification of urban models would
have to use something a little bit
different we have to use a higher level
information to hear and to simplify this
let me just state our goal for this
project is to create simplified urban
models that retain the image of the city
from any view angles and distances and
as an example of what we get you see the
original model on the left the middle
image shows the model having been
reduced to 45 percent of the polygons
and the last one is eighteen percent and
you kind of see a little bit of
dimming effect across when it goes from
original to less polygons but the
important thing here to notice that when
you're doing this the important features
in the city are retained so for example
top the road here is still kept the city
city square area is kept and you pretty
much don't get a sense that this is the
same city that you're looking at even
though there's only eighteen percent of
the polygons in the scene right and I'm
just going to run the application really
quickly and hopefully nothing goes wrong
let's see
okay so this is a this is using the
Chinese city data set and this is
running live so as you can see I can
just kind of look around move to
different places and here well hold on
one second this is where demo goes wrong
okay so I'm just going to start zooming
out from this view can you imagine how
you gotta jump in the jump this textures
are totally fake the geometry is
actually real so what we got was we got
the original footprint information and
we got a approximate height information
in terms of number of stories or a
number of flights per building and we
estimated that each story is about three
meters so so the geometry is kind of the
extrusion of footprints so it's not real
in terms of those three true 3d models
but the footprints and positions are
actually absolutely correct you'll ever
in fact that maybe graphics passing
there's definitely some but i'll talk
about that in a little bit yes sir sorry
specifications
as it turns out I'll get into a little
bit later too this is oh okay sorry so
the question was what is what is it what
kind of hardware I'm running this on and
the honest truth is I have no idea but
what I do know is that this is kind of
state-of-the-art laptop from Dale but as
it turns out explain this in a little
bit but the the bottlenecks actually not
in the graphics art is actually in my
crappy code where I'm not transferring
data fast enough this is the pipeline
that's actually the bottom one program
but that's just my fault I wrote some
crappy code so here I'm just I'm just
zooming out from that from that
particular viewpoint and to demonstrate
my point which is going to keep zooming
out and keep zooming out keep so we go
and at this point I'm going to freeze
the level of detail simulation I'm
taking away the ground plane I'm taking
away the textures and when i zoom back
in you see that this is actually what
was rendered when you're that far away
and you can see the number of polygons
at the bottom right here in terms of
what how many polygons was actually seen
when you're really that far away so the
important thing here to note is that
even though we are doing some pretty
drastic simplification we still try to
retain the important features in the
city and just to give another quick
example of this I'm just going to run
this without the texture we also take
into consideration of high preservation
so what that means is ah ah
so I can be looking at a city from kind
of an oblique view and again if I freeze
this process i zoom out you see that
there there's a lot more detail in the
round where the viewpoint is and I think
go out further the models are greatly
simplified but the interesting here to
notice even for objects are far away the
tall buildings are still rendered
separately so you kind of get that
skyline even when you're looking at from
different view angles even close the
ground plane okay
okay cool just to give you an idea about
what work has been done in terms of
urban flight or even walk through and
people have tried different things you
know visibility and occlusion is a
popular one and Peter Wonka has a great
paper called instant visibility in two
thousand and shuffler has another great
paper in two thousand as well and those
in siggraph and in here the idea is that
if you use occlusion or visibility you
can reduce the number of polygons that
you're rendering so that you can
actually see a bigger scene without
actually seeing the bigger scene and
there are people using postures or
billboards with textures started with a
marshal and surely in 95 and then silly
on in 97 extended it to kind of blend
between imposters as well as real
geometries and in 98 Shalaby and his PhD
thesis extended surveillance work but to
add it in some elements of legibility in
there as well they're procedurally
generated buildings I think this was
really started by Peter Wonka in 2003's
paper called instant architecture and
this year's cigarette Pascal Mueller has
a great paper on procedurally generated
buildings and then lastly we just have
popping like you know Google Earth and
Microsoft life so there are about five
steps to our algorithm in order to
preserve legibility and in in order the
first thing we do is we try to identify
and preserve the path and the edges and
we do not see this hierarchical
clustering that I'll be talking about
the second step is in creating logical
districts and nodes and when do that is
through cluster merging then the third
step is simplifying the model while
trying to preserve past edges districts
and nodes and that's done through a
simplification process then we
hierarchically apply the appropriate
amount of texture and that's the
texturing process and these four steps
combine to become the pre-processing
step and at the end of this process
pre-processing step you end up with a
hierarchy of meshes as well as texture
of or hierarchy of textures then we feed
all this stuff into the runtime process
where we try to highlight the landmarks
and choose the appropriate model to
render based on the based on the
viewpoint and that
the lod with let mark preservation
process so far as I talked about how we
do a preservation of past and edges
through hierarchical clustering here you
see the result of two clustering methods
the one on the left is more of a
traditional thing like k-means and
whatnot and the right is what we use and
you can see that this is cool animation
you can see that the where the two
clusters were the two clusters meet in
the first example it doesn't really
follow any sort of logical paths whereas
in our search been in our implementation
we really try to make sure that the two
clusters are separated on a logical road
and the way we do that is by using
something called single single-link
clustering and it's a pretty simple idea
it's basically internally groups the two
closest clusters together based on in
our situation Euclidean distance so as
an example let's say that you have six
buildings to start with ABCD and E a b c
d e and f and you just can you just
start grouping them two at a time and
eventually what you get back is a binary
tree or sometimes called a dendrogram
the thing that's interesting here to
note is that in this particular scenario
the dendrogram is actually very
unbalanced on one side you have no day
on the other side you have b c d and e
and f and this is this doesn't work well
at all for our purposes so we had to do
a little bit of balancing of the tree by
penalizing larger clusters and the idea
here is we want to create a more
balanced tree here's some simple images
of the first few steps of these
hierarchical clustering process and you
can see red is one cluster greens the
other and we just follow one particular
path down down this process
you can see that throughout the
simplification process the divide
between the two clusters mostly follow
the road pretty well okay so once we
have to simply once we have to
clustering then the next thing to do is
to merge the clusters together to create
logical districts and nodes here's an
example of what we mean by creating a
logical district you have one cluster of
buildings in red the other cluster of
building and yellow and the blue the
blue outline shows what what what the
merger will be and this is what it
should look like where the blue outlines
should just encompass all of them
together including the black space in
the middle so the way we do that is
pretty simple we first find the
footprints of each building ordered in
the counter clockwise manner then you
find the two shortest edges that will
combine these two things then you just
start tracing it starting with one of
the vertices now when we start with the
magenta vertex what we end up with is
actually what we're looking for which is
the resulting merged Hall now it's
interesting to know that when we start
with the different vertex and again the
counter clock clockwise fashion what we
get back is what we call the negative
space or you can think of it as the air
that's introduced by merging these two
halls together and this is actually very
important because the negative space is
what we end up using in determining what
model to render later down the lay down
the process so once we have that then
the next step is to simplify it and
here's an example what i mean by
simplification you see on the image on
the left it's the merged Hall of the
Atlanta data set and here you have about
6,000 edges and the idea here is we want
to simplify it without really killing
too much of the features of the city and
in this case we diminish it to about a
thousand edges or so and I'm just going
to show you real quick how this works
okay right so this is a this is just a
small data set that I'm showing and
right now you see the blue outline
that's the that's emerged Hall of these
all of these buildings together and I so
I start sliding the slider down I'm
actually doing simplification as I speak
so you can start so you can start to see
that little features are starting to be
filled in and I just keep going
so what's interesting about this
algorithm that we developed is that
eventually if you just keep going at it
you get back to convex hull so it's not
at all the most efficient way of finding
convex all but you do find the convex
all right so once it simplify or once
the polylines have been simplified or
once the merge Hall has been simplified
we create what we call cluster Messrs
and these are nothing more than
protrusion of the footprints and where
we determine the height of this cluster
mesh to just be the median height of all
the buildings in the cluster and this is
what we mean by what a cluster mesh is
once we have that then we apply texture
on till these cluster models and we do
in the hierarchical fashion so first of
all we give each cluster mesh six
different textures we give it a side
texture we give it a top-down view of
the roof texture when we kind of do an
imposter ish kind of thing where we give
for rough textures from four different
angles and then we put these custom
messages into bins based on how visually
important they are and the idea here is
that if you have a particular cluster
mesh that you know won't show up until
it's like really far away then what you
can do is you don't have to give it as
much texture real estate so in such a
case what you would do is put it in the
earlier bins and I'll explain that in a
little bit each of these bins that you
see and over 2 and Row 4 integrate each
each bin will contain the same amount of
texture texture resolution so what that
means is if you have a particular
cluster mesh that's put in the first bin
we need to end over tube in the amount
of texture resolution that you will
receive will be a lot smaller because
there'll be more people competing for it
whereas further down this pipeline the
more visually important clusters will
actually get a lot more textural
resolution so this is a way for us to
actually control the texture a little
bit because texture in in a lot of these
urban models as you mentioned earlier is
is a pretty big problem
so once we're done with all this then we
put this the resulting cluster meshes
and the hierarchy of measures in the
hierarchy of textures into our runtime
system where we try to preserve
landmarks and choose the appropriate
models to render so first we talked
about how we choose the right model to
render and we start with a real note of
our dendrogram in this case ABCD and E
enough and then we take the negative
space of that particular cluster mesh
and that's shown here as a approximated
3d box that's the shown in red and we
project that box onto screen space and
if the number of pixels that docking
price exceeds some user-defined
tolerance then what we do is we reject
that node and we recursively check for
its two children and it just keeps going
until you find the appropriate cut in
the LOD tree next thing I talk about is
the landmark Preservation process here
you see the original Scylla on the left
the middle image shows without the
landmark Preservation and the last image
shows our method with the landmark
Preservation and all you really need to
do is you add a few buildings that's
visually important and you really give
back the sense that the last image is
very similar to the original and the way
that's done is basically projecting a
user-defined tolerance alpha onto each
cluster mesh and if there's any building
in that cluster that's taller than alpha
height then it will be drawn on top of
the cluster mesh in other words here's a
scene here's a scene of coaster message
and you see here on the lower right hand
corner is a little green line and that's
the projected alpha height and these
buildings are drawn separately because
these buildings are told in alpha height
so those will be drawn separately
now I talked a little bit about the
results of what we get out of the system
here you see this little line blue line
that runs across the bottom of the scene
and that's that's that's rendering using
the on simplifying measures and it's
just a constant frame rate and we see
that in this particular case we have the
flies flies through scene where the
camera starts out really far away zooms
into the model and flies back out again
so what you end up seeing is when the
camera is far away on the top you have
really really great frame rate but once
you started to really really zoom in to
the point where you almost touching the
ground then the problem is the overhead
of traversing your lld tree really
catches up with you in which case the
frame rate is actually worse than if you
just render the entire scene in the
independently and correspondingly in
terms of the number of polygons that's
rendered you see the line on the top and
that's the number of polygons in the
static or the unemployed model and you
see that the two curves are or the two
graphs kind of our inverse of each other
because the number of polygons is
inversely related to the frame rate so
in conclusion there are a few things
that we found by working on this project
the first one being that when you just
use per pixel error it is actually not
all indicative of the visual quality of
your simplified urban models so what
that means is you really have to go
towards some higher level higher level
knowledge in our case from city planning
to help extract and retain visually
salient features in the model
in that particular case using urban
legibility we find that allows efficient
and intuitive simplification of the
models there's some great limitation to
what we're doing here the first one is
this rendering engine that I implemented
is horrendous I cut all kinds of corners
so we're not using display this where it
takes a raise any of the newer ideas
with graphics card programming so that
needs to be worked on the second thing
is the pre-processing step takes
absolutely forever because all the
algorithms are basically NQ processes we
can think of ways to simple it down a
little bit but it's something that we
need to work on and something that's
inherent to this algorithm is that we
end up with binary trees as our
hierarchy tree and binary trees are just
inherently deeper than quadtrees or
arteries so traversing a binary trees
traversing binary trees just takes
longer and the last thing is we really
want to improve upon this by allowing
user interactions and what we mean by
that is you can imagine that districts
in a logical sense don't always follow
roads or geometric geometric distances
for example if you have a historical
residential area downtown chances are
that's not bounded by roads I mean
that's probably just right immersed into
everything else but to the local
residents that district is may be
particularly important so we need to we
need to have the kind of interaction
where we allow the user to be manually
put in going there and say no this area
is a little bit different this is a
historical region for example
and that pretty much is it so I want to
thank the co-authors on this Tom
Caroline Zack bill from Charlotte and my
advisor from brown Lindsey pollen who's
at Carnegie Mellon at this point then
this project is funded by the army Miri
project and some of the people that I
work with in the architecture department
so this is part one and I'll just take
some quick questions if you want the
second half will be talking a little bit
about what we want to do in the future
and all
so far the questions of these I remember
that this talk will be available on
Google video so reserve all the
questions with a confidential content
for after yes sir so you mentioned that
you know what we honestly have no idea I
think oh sorry the question was a if per
pixel is not a good indicator of the
visual quality of the simplification how
do we measure our results and short
answer to that is we have no idea i
think it's interesting similar to the to
the paper by the stanford group we did
the efficient route maps it's kind of
the same thing because how do you
justify that that kind of direction
giving is good well unless you really do
a full blown user study you won't really
know and talk to some people about that
particular project and people agreed
that probably ninety nine point nine
percent of people in the world would
believe that that was a good good good
sketch or drawing of the directions
however they mentioned some people
living in the islands of the Pacific's
that really believe that the earth
rotate around them in which case chances
are that drawing will be absolutely
useless to them so it is very subjective
we thought we thought long and hard
about it and I think at the end of the
day to be able to justify this is useful
you just have to do a user study
yes sir
slightly the spirit having emerged the
footprints of two neighboring buildings
as I was in the two dimensional
so how do you actually blend the Jiangsu
buildings inside so the question is the
cluster merging process is on 2d how
does that extend to 3d geometry short
answer for that is it doesn't really
extend to 3d geometry we basically
assume that all the buildings are two
and a half D so you can only all you
have to do is play in the 2d realm and
do protrusion from that and I'll talk a
little bit about future work about the
3d geometries yes sir so it was working
nicely because you have
how what's the difference oh well yeah
okay right so the question is how would
this work for for for true 3d models I
guess and again the short answer is I
don't know but I'll talk a little about
what we plan on doing in the follow-up
yes sir sorry n is the number of the
number of buildings actually let me take
that back in some cases it could be the
number of vertices it's a terrible
terrible algorithm there's a n cubed is
actually the absolute maximum right so
in reality doesn't run a tank you but
regardless we never really tested a
theoretical limit of where it really is
running at but but that's something that
we're definitely you know thinking long
and hard about yes sir
for this demo is actually pretty quick
for in this particular demo there's
about 30 some 40,000 models and that was
probably i would say half an hour to 45
minutes but i will be honest with you
when we push it up about 15 to 60,000 I
let it run over a weekend and never
quite finished so that n kim really
catches up with you ok i'm going to
quickly go over the second part of this
where i just want to pick your brains a
little bit about what we plan on doing
the future and you know so you guys can
definitely help me out in terms of
finding a good direction for this oh
this doesn't show up very well this is
roughly my research tree and is
separated into three different
categories on the left hand side we have
mostly more more core graphics title
problems the middle side is more on
visualization of urban forms and the
right-hand side is a blend between
architecture specifically urban
morphology and what we've been working
on so far so I start with talking about
the architecture and urban morphology
the idea here is that I just I just
talked all this thing about urban
legibility and how we use urban
legibility for simplification but the
fact of the matter is I never actually
got back to saying well what are the
legibility elements in the city you know
we have no idea where is the real Road
whereas a park or anything like that we
just use the idea to do the
simplification now you'll be really cool
if we can somehow go back and find out
what are they what are the elements I
mean what are the important features to
a city so this we consider is kind of a
feature extraction and you can imagine
that if we overlay a model using a using
very very strong simplification or a lot
of simplification we might end up with
an image like this and we overlay that
the map now we can start to see that
here's a road and on the GIS data said
they might say that that road is Main
Avenue or whatever in this area is
something or rather now we can start
extracting really what are the things
what are the elements that remain after
our simplification and we can do this oh
sorry yeah just pointing out some of the
features here and we can do this in an
iterative manner where we start to we
start to do more and more simplification
and by doing that more and more elements
will start to appear and eventually
you'll get all the way from the very
very important features down to the very
very fine grain level of detail and one
that gets us is now we now we can have a
ranking or a hierarchy of urban
legibility elements starting with the
most important to the least important
and this allows us to be able to
quantify an identifying urban model and
in other words this allows us to
actually have some semantic
understanding of the model that we're
looking at and this is important because
if you can start to do this then we can
start comparing between cities so let's
say currently if you ask any architects
are there any architects here okay good
ah if you ask architects what is the
difference between New York City
Washington DC and Charlotte they will
tell you that in a very abstract and
very subjective way that Oh New York
City is made up of a grid Washington DC
is more more of a ray like structure
where all the roads are emanating from
the Congress in the White House and then
there's Charlotte that just has terrible
urban planning so the roads are just
absolutely everywhere but these are
things that people sorry these are the
things that people can tell you in a
very subjective way right but this is
not quantifiable this is not repeatable
and our goal is that if we can somehow
start to quantify a city then we can
start really comparing between these
different cities in a more objective
manner and along the same line we can
start to track how a city changes over
time again in a quantifiable manner
here I just show two images again this
is Charlotte in 1919 and then this is
Charlotte and today and this is roughly
the same region but you can see that a
lot of expansion has occurred and nobody
has can really tell you what
quantifiably speaking what are the
things that occurred over this 100 years
but what people can tell you is that in
1919 Charlotte is a pretty perfect grid
oh then show very well but in 1919
Charlotte is a very good grid our
downtown there is you know but a first
street second street 3rd Street and so
on but once they start expanding that is
the roads just go over the place now we
presented this idea to to a workshop and
we found out that this whole idea of
being able to track the city changing
over time it's still a fundamental
challenge in the GIS community people
have not had a good grasp on how to be
able to do this in terms of a city
changing over time along the same line
if we can start to quantify a city the
other thing that we have to start doing
is a smart labeling or in other words
position-based intelligent labeling so
for example if I give you a model again
this is of China somebody says I'm here
and then asked about what one of the
important things around me now we can
start to say well you know here's a
canal inform you here's E Street this is
main I've and so on interesting thing
that I want to point out is now we can
start to show the important features
based on their relationship to where you
are so II street for example might not
be very important in a global sense of
the world or in the global sense but
locally to you it might be very
important so we'll be able to highlight
that because it's because it's close to
where you are similarly main avenue even
though it's not related to you but
because it's important we will show it
and then you can start to show greater
regions such as downtown and just group
all the elements within it into a into a
district
the second half of this is the more of
an academic question now if we can
actually do the thing that we just said
can we reverse this process back to its
beginning in other words can we start to
sketch out a mental map similar to the
ones that were drawn in Kevin Lynch's
1960 book so we believe that there is a
direct correlation between being able to
do this point based intelligent labeling
and being able to retrieve a sketch of a
mental map so this will be a similar
idea to what the Stanford group was
doing with the intelligent route maps
but instead we'll be doing mental maps
instead the second half this research
direction is on visualizing urban forms
and this is a very interesting problem
for the architects because they
mentioned that the study of urban form
really hasn't changed at all since the
early 19th century people today are
still looking at urban forms are either
2d or 3d maps so for example this is
something taken from Mark's GIS which is
a product by ESRI and this is what
people normally look at when the urban
planner is looking at a urban model now
if you look at this and you even though
you overlay additional layers of GIS
information in this case you have some
student formation you have some
electricity information the bottom line
is these maps do not actively they do
not actively help the viewer understand
changes or trends or creating the city
in other words it really doesn't help an
urban planner to be able to do is job
any better so what we'd like to do is to
start to apply some things that we do
know as visualization people we want to
apply some of these visual analytics
techniques to urban form so let's say
that we start to integrate a lot of
these more abstract visualization
elements
alongside this urban form idea now we
can start to really see how a city
changes just in a more data kind of
perspective and by the way this is not
actually built the the background image
this whole system was actually built for
bank of america on international wire
transfer fraud detection but it looks
good together so but and we show this
whole idea to the architects and they
immediately found things that they can
start using for example the down to the
bottom image of the bottom windows shows
changes over time and they can start to
see how they can put in things are
changing the urban model into such
format so we believe that this is very
promising and the last part is more of a
graphics core graphics kind of question
and this I'm definitely doing a lot of
head waving their lot of people here who
know a lot more about this and I do but
the first thing that we start realizing
is using our method we can really do
some model compression and on top of
that to be able to progressively stream
the changes from course to find this is
really well built into the system that
we have so this should be a very simple
step but I think it's an important step
we're not all the data has to be
transferred all the time you can just
transfer incremental changes and the
second thing is we want to be here the
extenders to 3 models now this is a much
bigger problem because this includes
true 3d models and buildings and this
also includes what we consider as trees
and forests trees and forests is a very
interesting topic in terms of GIS
modeling and such because people really
haven't had a good grasp on this most
people still do trees as billboards or a
combination thereof and realistically
speaking people haven't really done a
great job in terms of seeing a whole lot
of trees so to us the problem is very
similar
on one set on one side we show that we
can do simplification of lots of very
simple 3d two and a half the urban
models and cheese and fourth of the same
way I mean we people have been able to
do l-system trees and whatnot where you
can procedurally generated but viewing
and doing the number of viewing trees in
a large quantity is still a very
difficult problem so we think that we
might be able to use some of the ideas
that we gain from doing 3d or two and a
half day building models and apply it to
trees and be able to take a stab at
visualizing large quantity of trees if
not forest so we took a quick stab at
this and what you're seeing here is
basically a 3d model simplified using
our method and eventually get back the
3d convex hull it's very preliminary and
the truth is going from 2d to 3d there's
a lot of technical difficulties that we
haven't completely figured out yet but
we're taking some stabs at it so you see
where it goes and that pretty much is
the talk is there any questions comments
yes sir
starting song if you try it out with
fashion accessory medicine to the city
where most of the building skyscrapers
yeah sorry so the question is in terms
of landmark detection have we done this
in a scenario where all the buildings or
skyscrapers the answer is yes and the
the idea there is that because the
system doesn't really care about whether
or not their skyscrapers or not the idea
is you basically have a base cluster
mesh anything taller than that based on
your current view position would be
shown on top of it or overlaying it it
doesn't really matter if they're all
skyscrapers are not there will still be
rendered correctly in themselves that's
basically based on where your ID point
is right so that's that's a very good
question though the big I can talk to
you about a little about this afterwards
but the idea of projecting your negative
space and the reason why the negative
spaces is depicted as a 3d box is
because you do want to know about the
height so when you're looking at a tall
cluster mesh from the side the
projection of the negative space was
still projected something really large
in which case it will be broken down
into smaller sub pieces
anything else actually just to follow up
a little bit about a little bit about
that there's some interesting question
about what is considered as a landmark
and that's a very open question you know
if you ask a person who lives in the
city and say well what is it landmark
around where you live they might pick
out the local grocery store you know
they might pick up the the the watering
hole the pub that you usually go to
maybe because the pub has been there for
four years everybody knows about it but
from a purely graphics and visualization
perspective and that information is not
really as relevant because they're not
visually important but if we're trying
to do this system for for example for
the purpose of a mental map then what
constitutes as a landmark is not just
purely geometry then you start having
you really have to start incorporating
other GIS data set to be able to finger
some of those things out any questions
any other questions well thank you very
much we really appreciate it
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>