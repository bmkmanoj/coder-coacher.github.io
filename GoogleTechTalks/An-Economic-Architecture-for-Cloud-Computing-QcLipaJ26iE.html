<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>An Economic Architecture for Cloud Computing | Coder Coacher - Coaching Coders</title><meta content="An Economic Architecture for Cloud Computing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>An Economic Architecture for Cloud Computing</b></h2><h5 class="post__date">2009-05-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QcLipaJ26iE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everyone good afternoon today
I'm here hosting an external speaker
this is kevin lie he's a research
scientist from HP Labs and he's also the
lead developer on the open serous
project cloud computing project and so
today he's going to talk about his work
on applying economic methods to resource
management in large-scale computer
systems so Thank You Sergio I just want
to thank Sergio Marty and John Wilkes
for inviting me it's really a privilege
to come here and be able to present this
work to you and get your feedback so if
you have any questions please feel free
to go ahead and ask I'm usually used to
taking questions in the in the middle of
the talk so you don't to save your
questions the end especially if they
relate to the slides that i'm about to
show so i used to do work in networking
especially wireless networking and there
was this question we had about you know
should we do link level retransmissions
someone turns on a microwave and this
lose wireless packets and if your base
station can retransmitting packets then
you can get really good performance but
on the other hand there's a question of
you know should you implement this in
all of your switches so you know we
thought about the the seven-layer you
know I so model which is nice it divides
things up it simplifies things and
there's this design principle that they
have for networking called intend
argument which basically basically says
that if you have some kind of
functionality like link level
retransmissions what you should do is
you should push that functionality as
high into the stack as you possibly can
why is this because for one thing you
need to have question yes okay sorry
very close okay I'll do my MC impression
here um alright so yes so
retransmissions so the intent argument
basically says that if there's a
functionality you should try and push it
as high into the stack as possible
because you need to have for one thing
for correctness reasons okay for example
if you have something going on in the
application like a web server it needs
to re transmit anyways because you can
have all kinds of failures that happen
outside of the the network link so if
you're going to read transmit this high
layer the only reason you'd want to re
transmitted at a lower layer is for
performance reasons make it faster
that's what the intent argument
basically says it also says that if you
do this you can have it's easier to
innovate it's easier to innovate because
you don't have the functionality
embedded down in the low layers of your
network and your network switches and so
it's instead that that functionality is
that your endpoints which are easier to
change so the argument is basically one
of simplicity and also one of innovation
okay you know this was when I was a grad
student and Plato has pointed out that
actually the ISO seven layer model is
maybe a little bit simplistic a little
bit naive because in the real world
right you you want to take care of the
the financial considerations right you
care about the cost of things you care
about the willingness to pay for things
and so really it's not a seven layer
model it's really an eight layer model
with the financial information dominant
and so I you know I someone I think
mentioned this is more of a like a joke
on a t-shirt and I thought about this in
more detail I thought well you know
financial information really isn't
something that dominates your
application isn't something that tells
your application what to do rather
financial and economic information is
something that pervades the entire stack
it's information that you want to pass
down from layer to layer be both the
pricing of resources and the willingness
to pay for those resources and it's
something that that goes
from the application through all the
other layers this information is prices
how much does a resource cost it's the
willingness to pay which is for an
application how important it is that
some resource gets devoted to it and
it's also the actual payment of payment
information go back a second and it
turns out networking this this principle
is basically embedded in all of networks
today you know if you you can see this
in the way that cellular providers price
their networks you can see it in today
the way that companies are moving to her
per bit payment for networking services
and things like that okay so what's this
have to do with cloud computing well in
cloud computing you can imagine a
similar kind of stack right it's a
little bit different in that instead of
having these the seven layers instead we
can think of it in four layers so you
have the application which sits on some
type of distribution or parallelization
layer whether they send thinking of
something like mapreduce which is sits
on some kind of OS a virtualization
networking layer this is Linux Zen
VMware also your switches if that you
have the physical layer the actual
servers your disks memory and your power
considerations that these physical
resources consumed and so if if we apply
the intent argument and also the
layering ideas that we learn from
networking to this it basically tells us
two things number one like in the ISO
layer layering model you should have
strict layering which is to say you
shouldn't have things that the
application layer that are directly
talking to things that say the low low
low level air they shouldn't be you
shouldn't have something in your in your
application that is say controlling
or directly instead maybe you should
talk to the distribution and
paralyzation layer which will talk to
the next layer which will talk to the
next leg that's strickler second
consideration is that from the intent
argument that if you have some
functionality you should try and push it
as high into the stack as possible okay
so if you have some functionality let's
say whether you're going to turn on
whether you're going to allocate more
resources or not you shouldn't let
something in the lower layers decide
that are you going to turn on a server
are going to create a virtual machine
instead you should push that information
up into the application does it need
more resources does it not need more
resources and the application can convey
that information down to the lower
layers
so so this basically summarizes why I
said okay you're going to have strict
layering higher layers are going to
request resources from the lower layers
you can have the intent argument which
basically means that the application
decides what it needs and what its
willingness to pay is and then you're
going to have these financial interfaces
that attach to price teach externality
by externality that just means any
resource the consumption of which is
going to affect someone else okay so for
example if you have a limited amount of
resources in Europe of any kind of
resources in your system there's going
to be demand for it from other
applications then that's an externality
and then you have to put a price on that
okay so here's this is you know one way
of looking at things go what happens if
you look at this in a different way what
are the consequences about so here's
here's a concrete example of some of the
consequences of different architectures
from the one that I present so it's what
I call the mini allocators problem so
you think of a data center cloud
computing data center and you have many
different applications okay have a here
and that be you have a destroyer layer
below it distributed layer decides how
many mappers you have and how many
reducers you have it decides issues like
data placement and then beneath value of
the virtualization OS layer which
decides how many virtual machines you
have how the disk out the CPU the memory
the disk bandwidth and network bandwidth
is allocated and finally you have the
physical layer and the power layer which
decides how weather machines are turned
on and off and how much power
consumption they have now the many
alligators problem is this at each of
these layers you have to make some kind
of allocation decision for resources
that i just mentioned okay now the
question is how you're going to make
these decisions and basic and if you
look at the the architectures that are
out there there are sort of two main
camps so one camp respects strict
layering so each of these let each of
the controller is that each of these
layers makes a decision about how to
allocate and it does it independently of
every other allocator
okay and the problem that you get into
here is that the the alligator skin to
conflict right so some virtualization
system like VMware's vSphere besides
that it's going to allocate more it
needs more virtual machines but some
controller at the physical or power
layer decides that it needs to shut off
machines because you're consuming too
much power and so these things start to
become in conflict and you get poor
allocation now the second approach is
well instead of respecting the strict
layering what we're going to do is we're
going to respect the end-to-end argument
so here what you have is a controller
configured as a dictator of the entire
system which gets information from each
of these layers and decides how it's
been allocate on some kind of global
level so it tries to optimize for all
the applications but it does it without
respecting the layering so you have this
global controller it knows it
understands your virtualization system
your how power is consumed and it
reaches its tentacles into all the
layers of your system okay so the
problem with the second solution is is
the complexity all right because now you
because you don't have strict layering
this global controller now needs to know
how each of these resources allocated
and also what is the consequence of the
allocation of each of those resources on
each of the applications performance and
because you because of this complexity
you end up having things like
one-size-fits-all approach where
basically this global allocator since it
can't know the the needs of all the
different applications in your cloud
computing system basically decides that
there are a few templates for
applications and it makes allocations
according to those templates and if your
application doesn't really fit that
template then the performance will
suffer in some way
so what I'm proposing is that we respect
both things we respect the strict
layering and the end-to-end argument and
we do this by attaching these financial
interfaces to all the layers we charge
for each of these externalities and then
we can get we can a we can get
simplicity because we don't have any
kind of global controller that needs to
understand what's happening in all the
different layers we can get we can have
the ability to have innovation in our
system because each application now gets
to decide which which resources it needs
or wants and will fit its particular
application requirements and and
everything will be fine okay
so no questions at all about this it's
kind of a stunned silence question
if you go back to your previous slide no
I think oh sorry next yeah so you're
saying you're you're combining both lay
you know the layering and the the
multi-layer approach but you're not
really because its end-to-end principle
which doesn't really break the layering
approach maybe I misunderstood what you
or say you saying you're combining both
of these other you know these two
separate camps but these these two
bullets are the same right this
end-to-end principle listicle applies
when you want the strict layering
approach ah yes I see what you're saying
so that first bullet should say under
strict layering higher layers request
resources only from its directly lower
layer so for example you know your
application doesn't go off and decide
how many physical servers it needs
rather it says to the Hadoop layer I
have a Hadoop job or MapReduce job and
here's ten dollars go and run it so as I
clarified so the consequence of that is
now your application doesn't have to
suffer through understanding okay you
know I have this physical server how
does it map to my application
performance rather it just has to
understand Hadoop how much money I
MapReduce I'll give it this much money
what does that mean for the performance
of my application which simplifies the
whole problem yes so I think some
applications are admissible to your
suggestion that you know you say I have
this many dollars do it the best way you
can with this amount of money other
applications may have deadlines or
firmer constraints on what they can do
so I'm not convinced yet that those
applications are admissible to the
suggestion you just made maybe possibly
you can draw an analogy to what's
happened in networking when we try to
have you know straight better guarantees
of
free of service did that continue to
obey the principles you've set forth
here do they break those principles and
networking now that's that's really good
question and I think that is the one of
the things that you sort of sacrifice in
this approach okay with the with a
dictator approach where you have the
central controller that controls
everything you can get a great
predictability right because you know
everything that's happening in the
system and you can if you have some
application that absolutely must run you
can make sure that that controller knows
that and guarantees that in this kind of
a system that I'm going to talk about
today it's slightly different because
now each application makes its decisions
independently say how much money they
want to spend on each resource and so
there's what's called the price of
anarchy okay you have a little bit less
order and you have a little bit more
chaos and question is how much more
chaos right how much more anarchy is do
you have in the system and I'm going to
talk about that a little bit how do you
provide some degree of predictability in
this kind of a system because of course
you know you don't need absolute
predictability if it's off the deadline
by a millisecond or two you know for a
lot of applications it's not going to
make a huge difference okay that's a
great question any other questions okay
so we built the system called tycoon and
the tycoon is it's basically a resource
allocation mechanism that we apply at
each of the different layers so
depending on the layer it applies to a
different kind of resource at the
virtualization layer my apply to the
number of virtual machines or the amount
of CPU allocate each of those virtual
machines physical layer it applies to
how many servers there are then at the
MapReduce layer applies to how many
nappers or reducers you might have and
the basic idea is that the allocation is
going to be the bid which is something
like a dollar per day divided by the sum
of all those bids from all the
applications so if I have one
application that's bidding a dollar per
day another one that's bidding two
dollars per day then the first one will
get one-third of the resources and the
second get two-thirds of the resources
so it's extremely simple the mechanisms
required to support this are embedded in
most operating systems and switches it's
very agile because you can calculate
this out very quickly it's fair in the
sense that the the allocations are
directly proportional to the willingness
to pay for the different applications
and it's what's called work conserving
which basically means that if one of the
applications doesn't use its allocation
that allocation can be allocated to the
other applications that have been in the
system but the big problem is that it's
unpredictable which I mentioned before I
can put in a dollar per day someone else
can put in two dollars per day and I'm
happy with that one third then I'm
getting but now what if the other guy
puts in four dollars per day now i'm
only getting one fifth of the resources
and that might not be enough to satisfy
my requirements so we did this sort of
extension of this proportional share
allocation algorithm with something we
call a contingency allocation here the
application or the layer provides a some
additional information provides what's
the minimum amount of resource that it
wants and it provides some extra
information which we call a contingency
fund which provides some degree of
predictability it provides some degree
of guarantee that that nilam allocation
will be met and it retains these nice
features of the proportional share them
it's agile it's ferrets were conserving
and it provides some assurance against
unpredictability the assurance of course
is not absolute guarantee it's
contingent maybe that's not the best
word but it's proportional to the amount
that's put into the contingency fund so
let me just give you a short example of
this okay proportional share Alice puts
in two dollars per day a Bob puts in a
dollar per day and so Alice gets two
thirds of the resource and Bob gets one
third of the resource and let's say that
Bob is happy with this this is exactly
what Bob wanted in terms of resources
but now as I mentioned before Alice now
puts in four dollars per day bob is no
longer happy with this of course Bob
always has the opportunity to to observe
this and if Bob's application can
tolerate this temporary lowering then as
soon as he sees that this is the case
then he can increase his his bid to go
back to the amount of resources that he
wants but that's not always possible so
you know Bob could be away his
application may not be designed to react
to this it might be a situation where
even a tiny millisecond deviation from
that one third resources causes Bob's
application to crash so this is the this
is the problem the contingency allocator
deals with so Bob in addition to saying
one dollar per day for the resource also
says I need as a minimum one-third of
these resources and furthermore I'm
willing to spend ten dollars to go into
my contingency fund to ensure that I get
that one-third spit okay so this is used
as follows now when Alice says ok four
dollars per day now boss didja see fund
which was ten dollars gets drained by
one additional credit so that Bob's
effective bid is actually two dollars
per day and so his his resource
allocation goes back to one-third and
this all happens before Bob's allocation
has ever been set to anything less than
one-third so it's never the case that
his allocation is below one-third so
this is somewhat more complicated
calculate there are a lot of corner
cases which I won't go into then the
proportional share algorithm but
basically it still retains the same
algorithmic complexity as the
proportional share algorithm and it
takes care of this this case and provide
some degree of predictability in fact it
provides a wide range of predictability
because now you can if if Bob wants to
get a high degree of predictability
almost near a perfect reservation you
can put a lot of money into that
contingency fund if he wants just a
little bit more predictability than the
proportional share then you can put less
enough so it's provides this nice knob
where the users can control the degree
of predictability that they want
in their allocation our our pubic so one
question i have is typically application
so Bob and Alice don't ask for a portion
of the share because they have no idea
how big the cloud is right they ask for
a specific amount of resources so how
does that work in that context ok um so
yes the idea is that the resource
provider exports the total amount of
resources in the system and so given
that you can compute you know let's say
I want 33 servers or one let's say 33
servers and i know that i have 100
servers in the system then I know that
ok I need to put in a amount of money
sufficient such that i can get one-third
of those things as resources right
yes yes yes and so the contingency
allocator takes care of this when you
when you enter that information you the
minimum you can enter it as you know
one-third or you can enter it as thirty
three servers you know calculates the
appropriate could use the mic let's go
but then you broke the end-to-end
principle cause you're not supposed to
know that you need anything oh of course
right so it's it's not it's not
necessarily the case that it's the
that's the application that does this
when I talk about I I shouldn't have
said application before it's the the
next higher layer of this particular
resource that actually asks for that
resource so in the in the particular
physical server case it's let's say the
the virtual machine management system
that asks for those physical servers my
impression is in the business world when
people have I guess variably price
resources they tend to resort to long
term contracts and then occasionally
have a human radio she ate those
contracts can you compare that type of
approach to yours and your automatic
contingency fund approach and maybe say
if there are any business situations or
people use sort of automatic contingency
funds okay um so these the allocation
that I'm talking about here today is
this is sort of meant to do the low
level allocation system that the actual
thing that's implemented in the hardware
and on the hardware but close to the
hardware that to do the allocation at
how you would actually sell this is a
sort of a different issue alright so a
higher layer you can take these kinds of
mechanisms and package them up in many
different ways you could package them up
as a completely fixed price type of
thing you could package it up as it
actually is which is a highly variable
price thing or you can package it up as
you know saas with a penalty in case
they get violated right
automatically so someone beauty order
the customer has to figure out where
seven tops that's right- ah ok so i
think that's that's that's an open point
that's good question i think that's an
open point i will talk about later like
we implemented this for some different
kinds of applications and and i'll show
you the results of those those results
so first question is from New York
question from New York go ahead yes so
the way you define this extra fund I
don't see why Bob would like beat
anything per day so what you can do we
can I mean I'm wondering what type of
equilibrium will happen if you define
the roots of the game like this so what
happens is that we can the Bob can beat
zero dollars per day and have a large
better contingency fund or something and
this makes sure that I then he has some
minimum requirement so if there are
other competitors in the system he like
you have he will end up paying more
otherwise he will gets whatever he wants
without paying that myself I don't see
why I mean Bob would pay any dollar per
day maybe I didn't understand the roots
of the game exactly okay I missed most
of that question but um what is it
probably it was very simple similar to
my question what is the steady state of
your system yeah so I I'm interpreting
that the question to be about the
equilibrium situation like you know what
happens if we think of this as a game
and now it's like everyone has this
mechanism to apply you know what what
happens in that case okay so it's
important to remember that some
preferences for resources are not
admissible basically if you have two
applications and they want more
resources than are available then
there's just no way you're going to
satisfy both of them in no resource
allocation scheme is going to going to
handle that so
so we have to take those situations off
the table but now there are some
situations where Alice and Bob they want
an admissible allocation that they're
both of their preferences can be can be
handled but it's such that that there's
a bidding war goes on so you know Bob
specifies a contingency fund and Alice
specifies contingency fund and then now
they sort of escalate upwards to some
very high price before one of them
decides to back off and so the the idea
of the contingency fund is that that can
sort of be calculated in one pass as
opposed to before without the digital
fun now Alice puts Bob puts in a bit
then Alice puts in a bit then then they
go back and forth and then it you know
like an ebay auction that goes higher
and higher and higher and then finally
one of those slides ok I'm going to back
off that's it but with a contingency
fund it basically because you put in
this explicit information about your
minimum resource and you explicitly say
your willingness to pay that all gets
taken care of in one pass so that in
that particular situation the only
advantage of the contingency was sort of
efficiency latency issue does that sort
of answer your question also the New
York question so I mean ok for example
so like you haven't defined to half the
rules of the game completely because
what I see in your example is that Bob
would not pay like any dollar per day he
will put all his fans in the country in
them in his extra fun and he put the
minimum and depending on the competitor
he will use part of the diff'ent so why
does he pay extra I mean declare
something like one dollar per day if
it's if he knows that he has a fund that
he can use so at the end of the day all
players in this game should pay zero
dollar per day and indicate some fund
and then
they have a minimum requirements as well
so then I don't know how you complete
like these funds with each other so
there is a chance that all these also
use the same font oh I say um yeah so
the reason that you won't put in a 0 for
your base amount and just put everything
in the contingency fund is basically
because well ok it's a corner case that
it has to do with how much do you want
in the the average case but I think
maybe we should go on and take some more
questions I'm going to be here this
afternoon so you can we can talk about
this in more detail offline I think this
is an important case I just want to make
sure we get to some of the other issues
so the other issue is basically if you
if you don't think of the the actual
payment the day payment it's the
contingency funds that are going through
a market clearing house and that's
that's basically what what is happening
it's your auction like all the other
stuff is to like deal with deal with
limitations and things like that if you
get rid of them if you look at the
simple model you may as well get rid of
everything in terms of like daily
payment and work on the market model
where you contingency funds are actually
like the contingency funds are actually
active you know doing the
market-clearing was it was your question
yeah it was as comment I mean it was a
suggestion to like you know these all
these issues related to like how much
you're paying and how much we need to
pay on all the stuff or if you get rid
of them yes then you end up with a
market auction and the DoD depending on
like who's who's in it you basically
start getting payments there there so
all these things on the left side are
probably like you know digressing from
the the problem the solution which is on
the right side
oh yes yes yeah but you don't have to
refill it on you know micro second level
you can refill it on a once-a-week level
or something like that but the graph
that you show that to serve versus one
sir seems to imply some kind of uniform
pricing of the Union pricing is
available and that price is visible to
the different actors Alice and Bob and
how Ellison Bob value these units are in
a uniform fashion whereas to them but
maybe poor so he's one dollar is you
know he he holds on to his one dollar
very dearly whereas Ellis is rich enough
she doesn't care you know she can throw
money around and in that case I rather
see Bob's contingency fund gain some in
Troy somewhere if it's not spending it
right away otherwise you know there is
no sir you know well no circuit well
okay so I think there are two questions
there so one question is suppose Alice
and Bob value their money differently i
think that that gets incorporated into
their willingness to pay so it and how
much of the resources they expect if if
it's the case that Bob values the
resources less than Alice does and also
has then he'll put in less money right
so how this marginal cost coming to play
market cost no marginal cost of just a
little bit more extra resource you get
you can get so there will be nonlinear I
mean yes yes it will be that's true I
mean that this is a you know variable
pricing model and so it's the marginal
cost is definitely variable even for a
single person it's going to be here the
second question is about the interest
issues I'm going to sort of defer the
macroeconomic types of stuff to the end
account its question
occurs to me that having a continued
funding you're really doing serve an
emulation of a second price auction here
which is more stable in terms of lakhs
of bidding I know okay so the question
is is this condition find emulation of
the second price options that you can
think of it that way in some especially
for the case in which the users the the
preferences are sort of naught naught X
cannot be expressed the initial
preferences could not be expressed in
the initial proportional share of
bidding method it's it's like second you
can think of it like second price sealed
bid in the sense that they put it today
that he's actually trying to mock my
right right yeah I think that's a true
am i doing for time here
my mom is okay so the the actual
architecture which we we implemented
this so this this gives the architecture
that we used to implement the tycoon of
the virtualization layer we also
implement it at the MapReduce layer and
also at the physical server layer but
this was the initial implementation so
the basic idea is that you have a
service location service it's called SLS
here which keeps track of the resources
the virtualization resources at this
layer there's a bank it's just database
that keeps track of all users and how
much money they have and clients query
this service location service to see
what resources are available in the
system what their total capacities and
then what is their current price in
other words what is the sum of all the
bids of the users for that and using
that those two pieces of information you
can calculate all the variations that
was asked about before you can calculate
okay do I want let's say I just want
three virtual machines okay how much do
I need to put in to get that or let's
say I can put in a dollar a day how many
virtual machine is going to get for that
so you can answer you can ask and answer
all of the different what if questions
once this client has done this then it
makes a transfer request to the bank
gets back some receipts and then it
actually contacts the resources to say
okay here's the receipt I paid for these
things now give them to me that's one
model in this model is nice because it's
fully distributed okay there's no
centralized scheduler that has to do the
allocation each the clients does the
queries independently but there are
there situations in which you don't want
the clients to do this like maybe if the
client has to be very lightweight in
some way or if you want some greater
degree of predictability in the system
so for this we have this admission
control server so the idea of the
mission control server is that clients
go to it and it makes you make a request
for some services in this admission
control server says okay how can I
provide the resources necessary to
satisfy this request and it effectively
has an infinite amount of
of credits ok so now I can do things
like provide all kinds of SLA s like we
talked about before offer penalties in
case these things aren't met and it it's
somewhat less scalable in the sense that
there is only one acs but on the other
hand it could provide these greater
degrees of predictability yes ok so the
question is in what sense is the AC is
non-essential I serve the ACS is a
centralized server but it's set yes it
is essential as scheduled but you aren't
required to go through it right so you
can allocate resources without having to
go through it's only for those
applications that feel that they need
that extra degree of predictability that
that can go through the ACS and that's
actually a very extremely high degree of
predictability I'll go on and show the
measurements we did before without an
ACS and we were able to get you know
pretty good predictability yes another
question
resources resources that a CSS
this is because at least didn't work so
we thought about two situations with
with a CSS so one is you have a
completely centralized ACS that's
actually run by the provider which case
you can effectively give an infinite
amount of currency which means it could
outfit anyone and it's it's a strong
it's a hard guarantee the other
situation you can think of a CSS is is
actually it's not a centralized think
it's rather it's more like a broker so
it's a service that has acquired a large
degree of of credits so say more credits
than any individual user let's say
credits equivalent to 10 or 100 users
and because it has this reserve it has
it can bid in such a way that it can
smooth out bursts so you know any as I
as a particular I show up on a day and
prices are super high I can't afford it
and and some days I show up and it's
super low i can afford like many days of
this so what the having this broker here
is you can smooth those things out and
provide service more steady services in
that way so dancer question it can be
outbid only in the case in which it's
the it's in the broker form rather than
when it's in the centralized form
so yeah we implemented this using zen
allocated a bunch of different resources
we also implemented this as I mentioned
before at the MapReduce layer and also
at the physical serpent layer provides
these best effort in statistical
guarantees and we tested on 400 hosts
most places they'd say that's a large
cluster here maybe not so much and we
were able to do allocation less than
five seconds and also this was not we
didn't do anything sort of special one
optimization of course you could apply
is do some kind of application layer
multicast information so you can get you
a request out to the service providers
on a very large scale at much lower cost
so I think this mentions just stuff
we've already talked about before
basically users want predictability how
you going to provide this predictability
so it turns out there are many different
questions that users are going to ask
with regards to pricked ability what we
wanted to do was come up with a model
that could cover as many of those cases
as possible and so we thought about
three different parameters in the system
there's how much of a resource can you
get okay this is what we call the
quality of service was that can we meet
a particular minimum resource level
that's what we think of as quality
service second is how much do I have to
pay for it this is the bid and then the
final piece is what is actually the
likelihood that I'm going to meet my
quality of service level you know is the
ninety-nine percent guarantee is it a 5
9's guarantee or maybe it's just a 19
guarantee so it turns out that basically
we could think of situations in which
use users specified any two out of these
three so the first one is okay how much
should I bid to obtain an expected QSR
level with a given guarantee so
basically I want a minimum service level
I have a particular guarantee level how
much drop to pay for that similarly I
have a certain amount i want to pay and
there's a particular guarantee level I I
want and how much of the resources can i
get for that and finally I know how much
I want to pay I know what my minimum is
and how many 9s of predictability can i
get for that so what we came up with was
with the model a mathematical model
where you could basically plug in any of
these two and then calculate out the
third based on historical information in
the system
so this sort of formalizes it somewhat
the we have the proportional share
quality of service which is as I
mentioned before it's your bid plus your
bid and sorry your bid over your bid
plus the sum of everyone else's bid
that's that that's the allocation for
portion air and that's also the
particular quality of service that
people are asking for then the service
level guarantee is the probability that
you're going to get that that level of
service that we can calculate out the
bid based on the cumulative distribution
function of the guarantee over some
period of time and the qsr that's being
asked for divided by one minus 2q s and
then you can take this equation and sort
of flip it around depending on which of
the three you want to calculate of
course the there a lot of details here
like how long is the cute as the CDF for
how is that data model and stuff like
that that's that's all in the paper we
can talk about more detail offline after
the talk but basically we took we try to
find some really bursty data there's a
system called planetlab which users are
these are grad students and they're
preparing for for papers for preparing
their experiments or papers it's got 700
nodes I think and about half as many
users and it's extremely bursty
basically because grad students wait
until the very last moment to do their
work right so there's this huge burst up
before conference deadlines and it sort
of quiets down it's very unpredictable
when this is because the conference
dates move around and and so it's it's
difficult to do that so we took that
data and we we applied this model to it
we we applied this chebyshev based
statistical modeler and then we saw okay
how well can you predict based on
historical information it is highly
bursty historical information what's
going to happen next now it turns out
that once you've done this there are
sort of two metrics that you can use to
evaluate your ear predictor so one is
did the predictor give the right result
I asked for a particular guarantee
five nines you know so how often did it
actually get or not 59 s but I asked for
particular service resource level how
often did I actually get that resource
level or greater but there's actually a
second metric which is that how close
was that that resource level to what
actually happened right you can give a
really reliable resource guarantee if
you basically say you will net you
always have at least you know one
one-hundredth of a cpu right but
actually sometimes most the time you you
had like ten cps worth of performance so
that's a case where you gave a very
close guarantee but it wasn't a tight
guarantee okay so these are the two
metrics we have here so s is the success
rate how successful my prediction was
and b is the tightness of that
prediction and so the particular that we
use is what's called it was a chebyshev
based predictor and so in this top table
here you can see that it ninety-three
percent of the time when we based the
predictions based on the previous hour
he was able to give a successful
prediction on what's gonna happen in the
next hour and it was pretty tight too it
was only off by sixteen percent from the
from what actually happened then the
norm in the bench were 21 was based a
normal estimator in the other was a
benchmark estimator basically they they
did not do as well they weren't as
successful in some cases there were a
little bit tighter the norm was a little
bit tighter but basically it was it was
a lot riskier in the sense that did not
give accurate predictions then when we
looked at the day level similarly the
the chebyshev predictor was accurate
ninety-five percent of time the
tightness was not as good though because
of greater variance from the day to day
to day stuff but in fact it wasn't as
good as either the norm of the bench but
the accuracy was significantly better
so there was a question before about the
equilibrium right because you can think
of this is a game where you can't do
something in isolation if you if you do
if you use this predictor for example
and you can't assume that you're the
only one is going to use it in fact
everyone's going to use it so the
results you can't just take a result of
one person isolation you have to say
what's going to happen when everyone in
system starts using this is the
performance going to be better so that's
what this graph shows on the x-axis is
the amount of work completed and on the
y-axis it's a cumulative distribution
function over to sort of groups of jobs
so the green group is the people who
aren't doing any prediction they're all
that was one run a whole bunch of people
none of them were doing prediction they
all ran together then the second run in
the red run red run everyone was
predicting was using this prediction
mechanism and so what this graph shows
is how much work across all those people
and all those runs did did they do and
so in this case what you want as lower
is better because that means that more
of the jobs were able to get a large
amount of work done you want more of
your distribution pushed over to the
right of this graph and less to the left
you can see with the that's what happens
with the red graph more of the jobs were
pushed over to the right which means
that more of the jobs got more work done
except for this tiny little piece far
over to the right where this is Green
Line is flat and suddenly it dips up so
those few jobs are basically these non
predicting jobs which somehow were able
to do more work than than any red job
and basically what happens is that these
green jobs aren't predicting so they
sort of show up randomly and
occasionally they do really well just
because of pure luck they show up at a
time in which the predictor said you
probably shouldn't run right now because
things have been really busy and it
turns out that that prediction was wrong
and and the red job or the yeah the red
job respected that didn't run Green job
did did not respect it when
and ran it was able to get more work
done but on the whole the predictors
were able to do much more work than the
non predictors on average it's about a
twenty percent increase in average
performance okay so so we also did this
at the the with using Hadoop with the
MapReduce layer and basically the
interface was described before the
Hadoop job you want to give some cash
over to it and Hadoop system this
modified market aware Hadoop system it
looks at the prices in the system
decides how many mappers and reducers to
spawn off decides how much money it's
going to put on each of them and it's
allocating virtual machines and it takes
care of all that stuff so from the users
point of view it's pretty much the same
as running a dupe job except you add
this additional amount which is how much
you want to spend and then Hadoop system
takes care of everything for you after
that
over that okay so so what happens so
again as before we ran a system in which
one system in which no one was
attempting to do any kind of Alex
special allocation and we ran another
system in which everyone was attempting
to do special allocation so they're all
running this market aware agent which is
able to look at the prices of resources
and then buys variable amounts of
resources depending on what it needs to
do at the time and so for this
particular example is a real application
it was doing a doing a social analysis
of dig and the relationships of the
different diggers and what this
particular strategy would do is it would
prioritize different stages of this
MapReduce job differently okay there's
some stages that contribute a lot to
your final running time there's some
stages that don't contribute that much
and so with this dig this dig agent will
do is it decided looked at these
different stages and based on how much
it they thought I will contribute to the
end it would adjust the price
appropriately and so you can see here
that these strategic users what we call
the strategic users basically completed
significantly faster basically all the
strategic users completed significantly
faster right so it's not just the ones
that are that it's not just a few of
them in the strategic system it's all of
the strategic users complete faster and
that's because they can convey this
extra information about which stages are
important so it's the situation in which
all the users benefit
so this is the basic approach that we're
taking in this open series test fit well
HP is working on with a yahoo at Intel
the basic idea is it's a globally
federated system each of the different
companies contribute some resources to
this and and it's geared towards
research so we're trying to it's not
we're not trying to compete with ec2 in
any way rather it's a platform for which
people can do research at the lower
layers like different virtualization
systems different MapReduce systems and
things of that nature and this
architecture that I described to you is
basically what we're trying to implement
in this thing with a bunch of sites see
more so okay so what's the future work
here so we're trying to do more work at
the Hadoop layer or the MapReduce layer
in order to understand relationship of
the resources and the actual performance
of the eight of the application
different applications are going to be
different so you know son you want
network bandwidth more and others you
want memory more and so we're trying to
try and look in different ways when
referring this using machine learning
techniques and there's a question of
power how should the power be
incorporating to the price that's
provided up to the higher layers so the
hope is that from this architecture we
can just incorporate the price the power
the price of the power into the prices
at the lowest layer this will filter all
the way up and then all of our work at
the MapReduce layer can remain the same
okay so this is the advantage of having
the strict layering now all these
optimizations that we've attempted to do
on optimizing a price they automatically
take effect now that we incorporate the
power into the lower level price you
also want to incorporate this into
issues of network topology and then
they're all these macro issues which
have to do with inflation and interest
in things of that nature okay so I think
that's it are there any questions
no okay so I'm going to be here this
afternoon but I'm going to be here this
afternoon so um anyone wants to talk
about some of this stuff more offline
I'll be here so it seems that there's
that this work applies mostly to
situations where the resources at a
given layer are very I guess homogeneous
in fungible so you know if I'm a user
you know I want ten percent of the
resources kind of any ten percent will
work fine for me I by contrast it seems
like a lot of these resource allocation
decisions in the real world don't really
obey that condition right so you you
might have heterogeneous resources you
know this one's better for my
application than that one or you might
have certain constraints that you've
imposed that you know I need you know X
resources but they've all got to be
close to one another physically or
something you know within a certain late
see you know distance of one another
right how do you wait which seems like
it makes the problem much more complex
if you guys looked at any of those kinds
of things or yes so so actually these
experiments we had you know we had many
generations of hardware in our data
centers and so there is a explicit
trade-off that you have to make between
that these agents in the work that work
that I described should you pay more on
a more on a machine that higher capacity
or should you pay less on the machine
that has lower capacity there are lots
of questions like that all that was
incorporated in here but as you are
suggesting it's very complicated to sort
of like the interface in these
experiments we just say exported the
amount of memory you had and the cpu
speed that we had but becomes all right
complex you have different architectures
you there might be some subtleties that
are explored like say how much cash the
the machine has that have a significant
effect on your application performance
and that is part of the future work
where you try and understand what is the
mapping of these particular
characteristics to your final
application performance network topology
is a very complicated one of course you
know what is your interact bandwidth and
things of that nature and that again is
part of future work but the hope is that
the this financial this financial
framework
it lets lets you make rational decisions
about that heterogeneity right because
now it's not some low level controller
that decides oh of course you want to
run it in the same rack no you get to
decide is it worth it for me to run in
the same rack given the other
constraints of the system or is it not
or maybe I don't care right and so now
you can make those decisions in a much
more rational way that fits your
particular application requirements so
getting back to I think it was a
question about contract contracts and
longer term contracts humans and so on
kind of related what about slow moving
resources so for example once I've
acquired some disk and I've actually put
data there well if that's one-third of
the resources you can't all of a sudden
take that away from me because I have
data there so if you do take it away I'm
probably not going to use the cloud that
cloud to store my data so you know how
does this apply to slow moving resources
like that where you know you'd actually
have to move out or move it around or
something that starts to get much more
complicated as they get data gets bigger
and of course that goes along with the
question about contracts and pricing i
want the pricing to be fixed for six
months or something like that so that's
where this predictability and this
contingency fund came to account so when
we allocate the disk for example most of
the agents that are doing the bidding
would allocate a very large contingency
fund for disk space because disguises
characteristic that if you go beneath a
certain level you know you it could be
disastrous for your application and so
you're able to convey that through the
variable amounts put into the this
contingency fund you know you can
basically put in effectively put in more
in there than anyone is willing to buy
on a per bid basis then it dissuades
people from trying to go in there and in
kicking them up it's still possible of
course it's still possible that someone
comes in you know with a lot of cash and
is able to to cause an eviction from
from that and that's when you need to go
to some of these hard guarantees like I
described before with the service of
admission control sir now as far as how
you're going to translate this to to a
harlot layer for
for an SLA and things of that nature
that's I didn't talk at all about that
but we looked at many different kinds of
models at the user learn how you make
that translation you know we looked at
say ten dollars per job you know how is
it going to be translated we looked at
ten dollars per job you know with a
particular deadline how we looked at / /
work types of things we looked at
variable pricing we looked at penalties
and stuff like that there are many
different ways you can translate that so
the reason that you know I think you
don't want those kinds of things to sort
of pervade the entire system from top to
bottom is that it gives you more
flexibility at the higher layer your
business people can go off and decide
okay what do what are people in the put
our paying customers used to in terms of
their model for payment and it doesn't
have to mean you re architect your
entire system it just means you have to
sort of figure out the translation from
that human SLA to what's happening at
the lower layers so first of all with
regard to the long term contract issue
on the and the the issue about wanting
to keep your desk unless there's some
other compelling use sufficiently
compelling that it makes sense to evict
you even though you have data there
another way to approach that is the way
we did in the the rental auction
algorithm in the incentive engineering
paper is that having acquired a rental
contract you then instead of having
instead of deciding what's the price I'm
willing to pay how am I willing to go in
order to hold onto this you instead
decide what's the greatest opportunity
cost I'm willing to give up in order to
retain the rights that I've already
purchased so in other words you say if
the if I could sell this contract to
another use at some particular high
price the price is high enough it's
worth it for me to cash in my disk
storage for that high price but it but
unless I decide to do that I've already
purchased a long term contract when I
can hold on to it for the length of the
contract so it gives you the same
dynamic but with but with less risk
going in because it's up to you to
decide whether you want to
conditionally vacate yeah so um one
thing I didn't talk about is you know
what happens when a allocation request
is put into system and the contingency
fund defines that it's not admissible
you know what is who gets kicked out
first and what do they have to pay what
do you have to pay them in some cases
right and so we we looked at many
different things but basically we felt
that it boiled down to you know make a
choice and make sure it's consistent it
can be exported didn't really find a
significant advantage because basically
because again these things are sort of
looked at by agents not looked at by by
humans and so as long as it's consistent
across your system and agents can
interpret the the semantics of your
decision then then you're you're fine uh
what I what I think that doesn't capture
is the there might be a significant
spread between the amount that I'm
willing to pay to get admitted in order
to start investing my data in a set of
resources reverses the opportunity cost
I'm willing to give up to hold on to it
once I've invested okay we can take it
off my tail okay
ok I think we're over time and so if we
can start shutting down the beat the VC
or the recording and life we could all
thank our speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>