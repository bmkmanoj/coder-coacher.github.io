<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Machine Learning... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Machine Learning... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Machine Learning...</b></h2><h5 class="post__date">2012-02-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5p06Xg5REj0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you very much Alice can you guys
hear me okay this roughly about right
okay great so obviously uh Jeff is very
savvy could not be here I'm also sad he
couldn't be here because I spent most of
the day you know doing this presentation
yeah the upside is that it let me create
this really great joke at Cloudera we
are all about reliability via redundancy
so it would not been okay to send but
one data scientist of the conference
because obviously one data scientists
can go down so we said three data
scientists to the conference so I'm
actually not feeling that great myself
and in case I keel over during this talk
my colleague Tom Pearce is in the back
there and we'll be ready to fill in for
me in a moment's notice thank you Tom
okay so what are we talking about today
this is probably be a little bit
different than what was in the sort of
syllabus or abstract for this talk based
because I did it instead of Jeff and so
it's biased towards stuff that I know
but I did I worked on the ad auction
system when I was at Google and I'm sort
of trained as a statistician and so my
area of expertise is really around
evaluating machine learning models
instead of actually creating them so I
want to talk about a little bit about
sort of what I see is the differences
between creating and running machine
learning models in industry versus in
academia and then I want to talk for
actually for quite a while about sort of
the current state of machine learning in
the Duke world and sort of where thing
where we think things are headed and
then finally I have just a bunch of
requests both in terms of like specific
applications and then kind of more
general abstract theoretical problems
that we we an industry need academia to
solve for us so it's kind of my
opportunity to beg and plead for you
guys to work on some of the stuff that
will be most helpful to us okay all
right industrial machine learning kind
of sound like they feed the name of like
a German band or something like that
doesn't it anyway all right talking
about deltas between industrial and
academic mission academia versus
industry right um first and foremost
model evaluation is probably this sort
of the single biggest difference i want
to say so first and foremost most of the
time in an ad system or in like a social
network prediction system really
honestly in any kind of industrial
system the machine learning component is
just one
system as part of an overall complex
system right and these systems are
usually so complicated that no one can
actually reason about them there was a
time when I was at Google where the ad
system started sort of showing fewer ads
on a kind of a consistent basis and
they're all these knobs you can use to
kind of control roughly how many ads
should be shown so when they turn the
knobs to kind of increase the number of
ads to be shown it works for like a day
and then the number of ads kind of
started falling again right and people
really had no idea what was going on so
they basically spent a few months like
trying to understand the system and I
was relatively new to Google at the time
and I asked my mentor this guy named
Daniel right Daniel is it possible the
ad system has become self-aware and that
it doesn't like ads this is like a
fairly persistent problem second big
problem is this is a business and there
are usually multiple objective functions
and there's not really a great way to
really assign weights to these objective
functions that are consistent on any
kind of timeframe so you think of this
in the ad context you would have liked
revenue right as sort of a clearly like
revenue the amount of money I make today
but I also care about click-through rate
and I also care about long term sort of
quality metrics that are basically
essentially if you turn up the ad system
you know to show lots and lots of ads
you'll make lots of money today but
you'll basically make people blind ads
eventually and then they won't click on
them anymore right so you got to have
like all these different goals you need
a balance over time and so generally
when you're working on a problem what
you aim for is like Pareto optimization
or some kind of change that can roughly
be tuned between like what your
different goals are that makes sense
something it's sort of tunes between
like money right now and money kind of
further on down the line okay just
because you don't really know what the
business goal is going to be at the time
you actually ready to launch your system
all right um predictive accuracy is only
is only useful up to a point you want to
have good models you want them to be
predictive but sometimes increasing
predictive accuracy can actually harm
other objectives you have and sort of
the two cases for this that I know about
like reasonably familiar with are
obviously computational advertising and
then friend recommendations and social
networks so computational advertising
it's often the case that a bike
objectively better machine learning
model will make you less money just for
any number of reasons again
just it's a function of being part of a
complex system and if you just changed
the machine learning component without
really anticipating how advertisers will
change their bids or how this will
affect budgets or anything like that you
can end up making less money overall
this is like in generally not a good
thing right um friend recommendations so
LinkedIn Facebook they all do friend
recommendations and this is a huge
source of growth in their social
networks it's something like forty
percent of all new edges in the the
linkedin or the facebook social graph
are driven by these systems but there
are also products in the social network
and as products in the social network
their goal is to get you to spend more
time in the social network so for
instance I'm almost sure although I
don't know anyone who can confirm this
that Facebook has like a attractive girl
sort of friend recommendation level that
they like mix in with my regular friend
recommendations like these are just like
friends of my friends who are very very
nice to look at and they just throw them
in there every now and again i'm sure
it's like the same thing for females and
all that kind of stuff right just to get
me to spend more time looking at friend
recommendations i'm not i'm like i
guarantee they do this it would be like
crazy if they didn't anyway okay alright
so other big delta systems proceed
algorithms it is essentially never the
case that you get to start from a
blackboard where you write down a set of
equations that has some great properties
and then you design a system that
implements that those set of equations
to solve some machine learning problem
that basically never happens right so
what you have instead is sort of the
existing computational infrastructure
whatever company you're working at and
this computational projector has a
number of aspects to it one it's very
general purpose it's designed to be used
for lots and things um it's pretty cheap
the number of like GPU units that are
available to industry is like relatively
limited in most fields I mean the other
big thing is that it's shared this is
running reporting stuff this might be
running production serving systems
there's a lot of at light as a lot of
activity on these systems that's not
just machine learning these machines are
generally not yours in any kind of real
sense okay um so these are all real
constraints on the kinds of machine
learning models you can develop but what
I'd like to argue well I guess what I'd
like you to think about um first of all
like you think
very seriously about choosing industry
over academia I would encourage that to
basically anybody it's so much fun
really like not for nothing and two is
that these rulings constraints drive
innovation like Michelangelo when he was
constructing the David the David is like
a tear was constructed from a very
terrible piece of marble it was like way
way way too thin right Michelangelo had
a big problem actually finishing things
and he needed constraints that were like
sort of severe enough to make him
actually finish something in this case a
terrible piece of marble was key to that
and some of the things that I love best
in machine learning at least recently
come from system based constraints Val
Val Abbott has this hashing trick they
do right where you sort of like
basically allow collisions between
hashing features and you do this so you
can fit more features in memory that's
like sort of the ostensible purpose for
doing this but it turns out to have
these really nice regularization
properties right um study at Google sort
of the original ad system at Google both
these things were really they started
with systems they started with systems
that they knew they could do certain
things and then they develop machine
learning algorithms that could fit and
work with the constraints of those
systems and they've worked out you know
staggeringly well really over the last
ten years or so okay all right other big
Delta is around the workflow so this I
lifted from the practice over a theory
blog and so if that blogger is out there
I thank him very much for letting me
borrow this um this is just a really
basic supervised learning workflow it's
kind of the way you would think about
things we have raw data we do some
preparation on the data we create a
training set a validation set we build a
model we do some validation we kind of
loop in that process make our model
better and better and then we apply this
model to new data and we make a lot of
money right sounds fantastic okay um
that's not really the way things
actually kind of tend to work you know
in industry at least the size of those
boxes isn't quite right if I one of the
sizes boxes represent the amount of time
you spend doing each of these different
steps the sizing would be like
completely inappropriate right okay so
first of all what we really want to do
is optimize like the overall process a
lot of time a lot of stuff i see is
focused on optimizing model fitting
model fitting is very very important we
want to have fast model fitting but we
really want to have fast everything we
want to have fast feature extraction we
want to
fast validation we want to have fast
evaluation we're going to paralyze
everything we can so we can basically
train and evaluate as many models as
possible because that's how we learn
things okay um a lot of the times sort
of the difference between like a
logistic regression model and an SVM
model is really kind of a roundoff error
compared to the difference you get from
adding better features or new data to
your model so basically it's a lot
easier for you to deploy a logistic
regression model because you have a
logistic regression model in production
already you're probably better off
iterating on features instead of trying
to swap that out with an SVM model that
uses exactly the same features it's not
always true I'm just saying it's a good
rule of thumb okay um you want fast
model deployment so you have these
models are running in production so they
have to basically be able to extract
features from production data as well as
like offline data right so what you
really want to have is kind of a common
sort of dsl for specifying how did if
like let you just define features and
will generate code for you that will
work in both your kind of training phase
and you're serving phase um you also
want the output of your model to be
servile you don't want to spend any time
coding you know like the implementation
of this like a district regression model
right you just want to like file comes
out boom plug it into serving off you go
all right then finally in web stuff a
lot of times the only validation you do
is just sanity checking right just make
sure the results aren't crazy what you
want to do is get the numbers out there
is like get the models out there as fast
as possible get them on a really small
sample of data learn what you can from
that and then kind of iterate and keep
going all right that's my little
diatribe just keep all that stuff in
mind please come to industry we
desperately need you all right so Hadoop
where the data is I talked a little bit
about I'm going to sue most of you are
basically familiar with the Duke I'm
just going to kind of quickly go over
the sort of the infrastructure the
platform all that good stuff and we'll
get on to the fun stuff so Hadoop is
this whole platform for managing data
right kind of every step of the way you
start with commodity servers you start
with a commodity operating system you
start with open source tools for
handling configuration and then viewing
like sort of cord
across services all this good stuff all
right you have a number of different
options for storing your data so first
and foremost obviously we start with
distributed schema-less storage HDFS um
there's also SEF if you guys haven't
just haven't seen Seth and I'm assuming
I'm pronouncing that correctly that's
definitely worth checking out a sort of
an alternative distributed file system
that can be plugged into HDFS and be
used as part of the Duke I'm sort of
biased here all right I'm biased in my
selection of append only storage formats
right but what I love I love Avro as a
storage format I love our C files which
is a storage format used in hive that's
really really it's calmer storage and
very very fast to query and then H
catalog which is the sort of
generalization of the hive meta store so
that you can use it across pig you know
any kind of other different MapReduce
job right and then finally more and more
important all the time HBase for mutable
key value storage okay um the other sort
of great thing about Hadoop is how much
work is being done right now around
integrating it with other things so fuse
is like an NFS basically internet like
interface for Hadoop fairly fantastic to
use like you just sort of you know
theoretically speaking copy files into
HDFS drag them and drag them out works
with lots of other tools jdbc and odbc
drivers for hive are kind of fantastic
and then we have flume for sort of
moving massive amounts of log data or
sensor data or really any kind of data
you can think of in to HDFS and then
scoop for pulling data in from
relational databases but probably more
commonly writing out data to relational
databases all right so we have this
whole wonderful ecosystem already this
takes care of a lot of the boilerplate
stuff that you really don't want to have
to worry about right okay so current
save the world machine learning in to do
all right so if we think of machine
learning as this kind of like data
preparation phase and this model fitting
phase and this kind of model evaluation
phase um Hadoop is pretty fantastic to
the data preparation phase in particular
to say MapReduce is pretty fantastic for
the data prepper
phase and for the evaluation phase for
the model fitting phase it really kind
of depends on what you're doing there
are a lot of times they're like options
where MapReduce is going to be really
slow and you use it if you maybe if you
have to or it's easy or it's convenient
but if you could choose something else
you kind of might like to really
seriously consider that it's kind of a
wishy-washy statement you really want to
consider that I guess it's my thought
all right so certain kinds of model
fitting problems are like a pretty good
fit for MapReduce and this is just in
terms of like what we see in industry so
recommendation systems recommendation
systems like any kind of product
recommendation friend recommendation
ending like that is a great fit for
MapReduce and then we've seen all these
sort of other models that are based on
decision trees so there's planet which
was done by some colleagues of mine at
Google and there's gradient boosted
decision trees which is a yahoo sort of
like model of this right I'm the idea
here is we're just running big MapReduce
jobs that are looking at sort of input
splits if they're too big to fit in
memory it kind of does a MapReduce job
over them to figure out where the next
split should be and if they can fit in
memory it just loads the data in memory
and processes it there even in the case
of these though like for the gradient
boosted decision trees if you look at
this paper the Yahoo guys did an MPI
based implementation of this that's like
six times faster than the MapReduce
implementation right um MapReduce
obviously is always going to be a batch
model right so you're never going to use
it for sort of any kind of online
learning system I mean basically the
biggest sort of decision point I think
with MapReduce for machine learning is
latency recommendation systems usually
I'll need to be updated like once a day
right you have 24 hours to get your
model done that's you know plenty of
time right they wouldn't let you just do
any you know any number of MapReduce is
basically right there is just
unbelievable amounts of detail and all
these different algorithms and the kdd
talk from this year so i'm going to
mostly skip over the stuff and talk
about other things instead okay all
right um again sort of back to the point
where stuff where Hadoop is really
useful for things data preparation
feature engineering so this is really
the process of taking in some data
whether it's images or log files or
whatever and I want to extract my
features from it and then prepare them
for inputs my machine learning system so
the kind of like obvious tools for this
people think it would be things like pig
and hive but I'd like to sort of
dissuade you from using those if at all
possible I'm just because there's sort
of this mismatch between the models that
hive and pig are going to like require
you just like this or way you specify on
feature extraction code in pagan hive
and how you might end up doing that in a
production system so what you kind of
would have to sort of hack around it by
coming up with like so maybe some udfs
or something like that so I mean ideally
I'm thinking your feature extraction
code is in Java you want to be using the
same feature extraction code for the
online offline process to the extent
that it's possible the way to link Java
in the hive and pig is with udfs kind of
by the point you're using UDF so you
really don't want to be using UDF
synthesis languages like if at all
possible so better off in my opinion
sticking with Java and Scala and using
one of really very kind of new and
exciting like api's had been developed
for creating big long chains of
MapReduce jobs in these higher-level
languages right so crunch I'm somewhat
biased in favor of since I wrote like a
pretty healthy chunk of it Scooby from
the nick de Guise is wonderful and if
you guys get a chance to check it out i
highly recommend it um cascading has
been around for longer than anything
else was kind of the first model like
this i'm the only thing that I kind of
like don't like about cascading this is
my personal bias is that it's sort of a
very couple oriented model it's
basically feels a lot like pig or hive
essentially but it just happens to be in
Java and then jackal which is an IBM
specific thing which is sort of kind of
similar to pig but operates on like
complex records in JSON so I mean it's
sort of like great if you happen to work
at IBM but I don't think they've open
source sitter anything like that so I
don't know how to get ahold of it then
we know if they've open source jackal I
haven't seen it anywhere no no one's
seen it ok all right let's talk a little
bit about my how how it is sort of the
starting place for people when they come
to machine learning a lot of times our
customers are like I've got a bunch of
data I want to do some data mining on it
right what they really want is kind of
like data mining in a box or like data
mining as a service and we of course
we're all experts on these things know
that there's really no such thing as
that there really isn't data mining in a
box at least right now in so far as i
can tell general nodding in the audience
ok good to hear all right ok so almost
always you're doing some kind of custom
tweaks or modifications to whatever
algorithms you come up with all right
mahad right now is a very disparate
collection of algorithms and I mean that
in the most general sense possible its
disparate in terms of they do different
things its disparate in terms of how
long the implementations have been
around how battle-tested they are how
strongly I would recommend you use them
but sort of a roughly fall into four
main classes there's a big
recommendation system there's clustering
algorithms classification algorithms and
a frequent itemset mining algorithm kind
of roughly a priori you're familiar with
a priori right these are all sort of
like independent libraries in a real
sense there's not a ton of code sharing
across these and it's not you can't just
like really sort of like swap a lot of
different implementations as much as you
might like to okay so a little bit of an
opinion on these things um far away I
think the best library amount is the
taste recommendation system so it's like
CF taste is the Javadoc API um and
that's really just a function it's been
around the longest it's been
battle-tested people been hammering this
thing for a couple of years now so it's
in pretty good shape um it's very widely
deployed if you look at like the list of
like powered by projects in the my
website recommendation systems kind of
tend to dominate foursquare did a really
good blog posts and sort of their use of
taste and like sort of their own
modifications to it recently the sort of
sbd to singular value decomposition
algorithms there have gotten a lot of
work and have become like quite a bit
better um good libraries the online so
cast a great and sign algorithm is
actually pretty good it doesn't actually
use MapReduce at all it's a totally
online algorithm um that said I kind of
think that Valve a wabbit with all
reduced have sort of laughed it in the
past few months or so um just like my
personal opinion basically um things you
should probably not use naive bayes the
naive bayes implementation is like super
simple naive bayes is super simple right
if you're going to use naive bayes just
write naive bayes like don't bother
using them at hallett based
implementation it's just it's not really
worth your time it's a bunch of
limitations with it I just I don't
recommend it okay so sort of the
challenges around the how is an open
source project I mentioned there's lots
of tweaks and lots of custom
modifications to things and I call this
a secret sauce effect like no one wants
to talk about their machine learning
implementation no industry wants to talk
about it because everybody does
something
slightly differently and everybody
thinks the slightly different thing they
do is like the magic that makes
everything work and is like totally
wonderful and I don't believe this for a
second everybody's basically doing
online stochastic gradient descent you
know what I mean the differences in your
rights I really don't care like all that
kind of stuff but look whatever no one
talks about this stuff right and in
particular no one contributes it back to
mount because you've made all these
tweaks they're really optimized for your
use case and so it's not like a
generally applicable thing most of the
time right okay there's other thing
where I mean it's a lot of these
algorithms are based on map reduce my
how it is pretty strongly associated
with MapReduce and a lot of the cutting
edge stuff and machine learning the
stuff I see here at this conference
isn't making use of machine learning and
press r e isn't making use of MapReduce
it would be really funny if it wasn't
making use of machine learning i really
i love the nonparametric Bayes stuff I
really do anyway this is big Delta right
so it's not obvious how some graduate
student can take a project they've
worked on and incorporate a directly
into mahou and make it like widely
available for everyone to use it's just
a lot of work and a lot of hassle and a
lot of overhead for not really very much
pay off and this is this is a problem I
all have a solution to this i'm just
we're pointing this out as a problem
okay i'm sadly going to blow through
these and i feel bad about this because
i love a couple of these projects um
there are lots of like currently
available sort of extensions image to
MapReduce that like are really great for
machine learning I'm system ml from IBM
is kind of like a crunch or pig for
working with matrices which is kind of
cool i'll reduce just a huge fan of this
is the thing out of like john Langford's
group at yahoo research few other people
actually i think i saw a poster for
doubt in the hallway this basically adds
a broadcast step to the very end of the
like mapreduce kind of computation
speeds up a huge amount of like very
common machine learning tasks they're
like the performance numbers they're
showing or fantastic no MapReduce at all
you guys just heard about spark so I
won't like harp on that too much but I'm
a huge fan of spark as well I pretty
much learn Scala by copying much of
sparks code and then there are a number
of our based systems which I am also a
huge fan of my favorite one of these is
probably Ricardo at IBM which gets much
less play than the other three but it
really embraces this idea of letting
Hadoop do all the things that Hadoop is
really great at which is like
aggregating thin
things counting things distributing
things all that good stuff and then
doing kind of the hardcore optimization
logic inside of our itself it's a really
very smooth interface between like the
data stored in the HDFS cluster and then
pulling the data back into our doing
some computation and then sort of
running the next step so it would be
something would be like fantastic for
IBM to open source if anyone from here
yours from IBM not gonna admit it okay
that's fine that matter all right all
right um machine learning ahead do where
we're going next okay um this is
actually I got this from the
uncyclopedia this is the of you guys
ever seen the uncyclopedia it's like a
much less useful version of Wikipedia um
but sort of this is the image they have
for the Cambrian explosion and that's
kind of where I think we're going with
the dupe right now cambrian explosion
uses were most of the phyla that it came
into existence occurred in like 530
million years it was just huge
blossoming of diversity in creatures and
animals and basically everything right
and sort of the impetus for this
Cambrian explosion is sort of mr v2 and
yarn okay so um right now the sort of
one of the big problems with a Hadoop
cluster is the only thing you can really
run on it the only type of job you can
run on is a MapReduce and there's this
big honkin server called the jobtracker
that keeps track of everything does all
the resource allocation is all the job
scheduling keeps track of all the tasks
it's basically the bottleneck in Hadoop
clusters right now you can only scale
cluster up to about 4,000 nodes right
now because of this thing especially
needs to keep all this data in memory
okay so mr be to they basically sort of
took this thing apart rewrote the whole
system from scratch it's a little scary
but we're testing it very very
thoroughly I promise they basically set
out basically split up resource
management and job scheduling and the
process of actually managing the tasks
for each individual job in two
completely separate servers so this is
going to kind of bump other sort of size
that you can basically be you have like
up to 10,000 nodes or so in a Hadoop
cluster right just kind of fantastic for
scalability purposes but is even better
because now MapReduce is basically like
a user land library so you can specify
any kind of application you want to run
in Hadoop just by interacting with these
mrb to api's yes I say the yarn API is
in particular there are a lot of
projects already that are basically
working on integrating with yarn I'm
just one called hamster which will be
MPI on Hadoop will be able to do sort of
native BSB so right now giraffe if you
guys are familiar with giraffe is a bulk
synchronous parallel kind of engine it's
basically a hack on top of a map only
job in MapReduce we able to do this
natively going forward um spark Mathai
is already actually has a spark yarn
project out on github so you can
actually like I don't sure if it worries
Matai here is it actually work one time
is he not here okay somebody asks Mathai
when you see them does it actually work
I haven't actually checked and then I've
seen you know talk at least between the
all reduce guys and and Joseph in the
graph lab guys about doing something
where they will have their systems run
on yarn as well this is just absolutely
fantastic this really i'm super
optimistic this will close the gap
between the cutting edge machine
learning algorithms and what is
available to industry to work with okay
all right um finishing up some things
these are my pleas for you a couple of
specific problems and then some more
general things okay all right um so
something I've been looking into
recently because I've had basically
three different customers come to me
with some variant of this same problem
they need to detect basically a system
of anomalies essentially on on basically
essentially time series data and you can
think of a sensor data so you might
think of this is some industrial I have
to be kind of vaguely generic is to not
give away what I'm talking about here
right I'll but you have some kind of
industrial system that needs to be
monitored in some way and it has lots
and lots and lots of different
components that are all interacting with
each other and lots of different sensors
that could be measuring things like
pressure or you know electricity or
something else sorry I'm being kind of
serious but they come in at very high
volume there are lots of these sensors
and they have a very sort of fine degree
of measurement um so we should be
thinking kind of anywhere between
100,000 writes per second and a million
writes per second like very very high
volume for these things positive events
positive events is sort of a kind of
unfortunate choice of terms since
usually positive event is like really
big explosion that would be a positive
event
right it's time series data so we have a
feature extraction challenge there are
sort of things like saks you guys know
much about time series and machine
learning actually if anyone knows much
about x here is a machine learning
please come talk to me after this talk
things like saks for basically kind of
imposing symbols on the time series data
but there's not a whole lot of work
around multivariate time series where I
have all these different sensors and
it's possible that some combination of
like sensor readings across different
systems adds up to the system's going to
explode in 15 minutes that's that's kind
of problem we're talking about here it's
not really clear what the right time
horizon is I look at time series
algorithms and usually it's the case
that you specify a sort of what the time
window is going to be that you're going
to like consider events over and I want
like the nonparametric Bayes version of
this where I don't have to specify the
time window and I can just say you know
it's like there's a chinese restaurant
process for the time window and then
just you know go off and compute and I'm
happy that's great um the models have to
be relatively compact and simple because
there's very tight SLA s on these things
I have to if the system is going to
explode in 15 minutes I have to know
that like really basically as soon as
possible um and the stakes are really
really I like billions and billions and
billions of dollars in lawsuits and this
isn't I'm this isn't an academic problem
guys this is happening right now the
companies want to be deploying systems
in like six months and I have no idea
what to tell them we're videotaping this
our way let's do that
I'm working really hard and figuring out
what to tell them all right ok so anyway
like I said seriously if anyone knows
about the stuff like come talk to me I
want it I want to hear all about this
stuff ok um this is also a request a
really feature engineering is really
really important and it's something that
I don't feel like it's like I I guess I
feel Jeff feels Tom feels doesn't really
get that much attention in literature um
there are so many great problems in
feature extraction right there's dealing
with hierarchical feature representation
there's understanding like what the cost
of adding a new feature is to my model
right all this kind of great stuff that
is just sort of massively dispersed
across different like different
industries right so you guys actually
the maybe the best resource of this is
from nips in 2003 there was a contest to
do sort of different feature extraction
models um but that was like that's like
eight years ago guys I want you to do
this like basically every year tennyson
as possible i always want to be up to
date on this stuff and then I mean it's
spread out across different disciplines
like the NLP guys have their own sort of
lingo for this the information retrieval
guys have their own lingo for this it
makes it really really hard for me to
google for stuff basically it so I'm
talking about I can't like just go
search on a term and find everything
that I want to know it's not like a
lingo I can learn and then go like
understand everything in the industry
right this is a really big problem for
me like no foolin I would love to have
like a really good solution to this all
right okay so and then kind of finally
um sort of i think along the lines of
themes i've talked about so far i'd like
to have a broader ontology for model
selection so there's just a bunch of
stuff i tend to care about above
predictive accuracy a lot of times when
I'm come when I'm making a decision
about choosing a model right um so I
want a model that's going to in
particular satisfy my latency
constraints on like how quickly i have
to respond with a score in my production
system right that's really puts a very
serious limitation on how complex my
model is allowed to be right model
refresh time is a fairly big thing like
is just a huge difference between my
model choice if the data is coming in on
in an online fashion versus like i'm
getting the data every 24 hours and so
it's not a huge
deal modeling is refreshed there's a big
difference between like a recommendation
system that could be updated every 24
hours and you know what if it fails not
the end of the world because I have
yesterday's recommendation system and
the you know world of movie preferences
is not massively change in a single day
you know that kind of thing versus like
an online ad system where it's actually
a really big deal to have an online
learning system that can be updated like
in 15 minutes you know anytime some new
trend gets picked up or some advertisers
like finds a new keyword that's really
profitable excuse me you want to get as
much competition in there as fast as
possible all right and then of course
sorry robustness and reliability the
study guys talked about this a lot sort
of said he went through a numerous
iterations inside of Google you guys
familiar with study at all it's like
just a great machine learning library
it's used on a very wide range of
project projects inside of Google but it
went through several iterations a lot of
which required a lot more babysitting
really I'm babysitting is kind of like
is the maybe the worst thing you can say
about a system inside of Google I think
carrying like machine learning experts
carrying pagers you guys familiar with
carrying pagers does that mean anything
anybody a few people right yeah carrying
pagers is really bad you don't want to
carry a pager you want the system to be
reliable and you want it to be fairly
predictable so that it kept the pager
can be handed off to somebody who
specializes in carrying pagers for lots
and lots of systems right you don't want
to be the guy getting woken up at two in
the morning because things fell over
right that kind of thing so these are
all really very important like
considerations in model selection that
we just don't really kind of fit into
our typical sort of like way of thinking
about how we choose models okay um so I
think we got like I both managed to
finish early have enough time I've five
minutes okay all right five minutes for
questions roughly knit anyone wants a
job like you know we are hiring both
like data scientists and interns I know
is it like okay to recruit people this
thing was it fun I was like hold on yeah
it's fine okay
we have any questions on my defense my
boss with a hard question Alaska
uninsurance in his level of money
our plan to rise and examines
so that the 240
dials data for days weeks that runs out
from not without that because without
our business I like I'm not super player
the tambien
first
I guess
it's like
so it's disgusting
a trade-off between Cyril a CD
sterilization versus like network
traffic is anything objects which is
smaller objects yep and so I thought
myself a lot of time in this problem i
have major exam assignment vectors to
different users anything save up short
sending big objects and small objects
that lone pairs versus strikes this
might like across does not reduce with
ya mate just going to kind of
starting your day of reckoning or Paris
or sparse data wants rights were dead
sale is that those things that people
dance now that has to get bad that they
get off that's German my role tomtom
are you go about yes okay yes yes yes
project you are here for like ESF
youjizz for the feature extraction and
wish there really was no this is very
very much it's just such a
so Panik putting license
shakti changing which part of every
single happy to take another yeah yeah
oh I would like to think so but I
honestly on so when they did Mr Neeson
they kept all the they got like ninety
percent of MapReduce apic see which is
good so maybe in this case the mrv to
yourself you have a living room or any
peanuts which your primaries and set up
our pcs for communicating
scheduler understanding ever find to
talk to me out
today morning guys learn but the engine
you've got so coming to be travel
properly changeling the two for one
such life heroes
that's a later point me MapReduce is
pretty well established it's kind of a
common way of doing things mvi has a lot
of backers Mouse's a lot of legacy code
written and Fei so if you make things
that look like a lot of like suppose
going to be three over the other but I
think it's kind of up to you guys we
GraphLab is like pretty solid substrate
for developing machine right spark is a
pretty solid substrate to develop
machinery of course I mean GraphLab cg b
and c plus bosses with a single future
maybe man sure I mean sparsely me of
Stalin but it seemed to me like that the
subjects will be successful ones that
have seen Ernie do i do you I'm
developing outreach
and once adult will roughly die off the
parents yes we have storm um so I have a
low distorted in depth on their word
system like that in you with a solecism
I don't have been gone for like six
months I don't understand of it right
now many of these training systems is
that they are usually very tightly
coupled to the sword so we use for kind
of maintaining the state of the notes
you also that BigTable on understanding
the store maybe I is nominally that
suppose being independent you can use
Cassandra over each face or anything one
I find that a little hard belief I would
produce almost certain issue there are
some that are like better my social
sources that it's optimized or like I
mean whatever one the backlight guys use
it with he says also didn't require a
lot of babysitting at least mine and my
experience so i think i think all I
Boston is kind of hands
a lot of machine learning about was I
probably started battle data and just be
honest with me
a lot of other cases hi sort of thing
incrementally and like handled a lot of
like the dashboard a kind of updating
systems but this is my personal this is
where based on google experience today
is a really smart guy and i we're not
going to discount like if anybody to
figure out like a way to do this is I
really rely
and have to open its going to be yes
although you gain from using the white
beauty assistant
Matthew's look like to lose you can make
rock on a system that dies out because
all
therefore
I mean the first question I heard is why
like preferred me like sort of
vitaminwater ado ecosystem by another 1i
usable when you how much you gain say
you guys Sally that we won't ruin that
bad boy it's only water
it's not sure what me does with the
products that accompanied the system
were things that velocity in town useful
yes right writing MapReduce extends to
be somewhat tedious so it's more
different different ways of light people
not right at least i think is he for job
scheduling is cancelled like mistreated
comp the count on cinema she is like
absolutely fantastic I mean data
ingestion is pretty much for a problem
to begin and end you know I mean like
hitting David in the cluster in the
first place is another great reason you
severe because whatever system you
develop doesn't really have to worry
about that anymore you can just but its
way that's why feels like spark at HDFS
connectors right because that's where
the way it goes I know that's much an
answer your question maybe we should
talk afterwards
yes thank you
I
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>