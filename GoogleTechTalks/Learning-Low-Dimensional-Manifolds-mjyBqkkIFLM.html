<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning Low Dimensional Manifolds | Coder Coacher - Coaching Coders</title><meta content="Learning Low Dimensional Manifolds - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning Low Dimensional Manifolds</b></h2><h5 class="post__date">2009-10-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mjyBqkkIFLM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay I guess it's my pleasure to
introduce you F Road to the faculty at
UCSD in very very light known in machine
learning and particular for our coasting
coaches and a lot of other nice stuff
and go ahead okay so yeah so thank
thanks so much for inviting me I'm going
to talk today about about some work that
kind of combination of applied work and
theoretical work I'm going to start with
the application this is actually a
recording from the application that is a
little bit old but you'll kind of see
what it is so this is my son and
basically world and of shouting at the
camera and when we speak the camera
looks at us okay so that's that's
basically what it is doing so how does
this all relate to a low dimensional
manifold well we'll get to that okay so
so this is one application that I
actually work on and now it's running
much much better it's something called
the automatic cameraman that you can
google you see as the automatic
cameraman and you will find how it works
now which is even better than how it
worked then okay so what I'm going to
talk about is low dimensional
representation for high dimensional data
and in order to talk about that I need
to define something called intrinsic
dimension so you know many people talk
about you know you have very high
dimensional data then there's the curse
of dimensionality and you can't do
anything but they know that it's a more
structure in there that makes it more
low dimensional and we can maybe utilize
that but how do we actually define that
and so I'm trying I'm going to try to
give a framework in which you basically
can talk about what does it intrinsic
dimension mean in a way that is also
practical
okay so I'm going to talk then about
vector quantization using this algorithm
that is for low intrinsic dimension or
the random algorithm we're going to find
what is the low intrinsic dimension
dataset and then I'm going to present an
algorithm that has good properties for
data with low intrinsic dimension I'm
going to talk a little bit about
something that is a little bit
tangential but I think interesting about
fast online principal component analysis
which is principal component analysis
that you do in a streaming fashion then
I'll talk about charting the manifold
using the manifold for system
calibration which is really the
appreciation I will show you so the
special thing about for those of you who
know about microphone arrays directing a
camera with a microphone array is not a
special new thing what is special in you
in this application is that the
microphone are placed ad hoc so they're
just located at the wide distance from
each other and so the mapping from the
time delays to where you should point
the camera is highly nonlinear and not
something that we know our priority
learn it and then I'll talk about some
future directions okay so starting with
a low dimensional representation for
high dimensional data so this is not the
new thing I think one of the best
example well-known examples of that is
something called eigenfaces that was I
think was started by Wahid and temple
and in MIT Media Lab and then macadam
did some more work on it and what you do
is you basically take a lot of faces
they're well registered with each other
and then you exceed describe that space
of faces using the principal component
analysis so you basically say what's an
average face and then what's the main
variation between faces is the largest
egg in fact the eigenvector the largest
eigenvalue and so on okay
and it's kind of interesting you can
basically he
they represent faces that very I guess
between people different people have
different from each other so you see
that it's kind of a vector for scholars
I guess and there's an eigenvector maybe
for glasses or small glasses big glasses
and then here
oh so that's an interpersonal so that's
one person wearing different things okay
and then here it's extra personal so
between people so you can kind of think
about this as a representation a low
dimensional representation of faces okay
so you basically would say that the face
is the projection of the image on the
with the largest eigenvalue and that's a
representation maybe you can do
classification in that representation
any other thing you want okay a
different approach to doing low
dimensional presentation is the one of
Laurens soul and weinberger and that is
that you look at the data much more
locally so you think about the data is
consist constituting a graph so edge
each point is has a neighborhood and now
you basically represent the spatial
relationship between points as
connectivity in the graph and now you
basically try to make this graph kind of
flow dimensional so you kind of try to
spread it in some dimension so that it
would be as as small as so it will have
a good representation of distances but
in this low dimensional space so here
they did it again for faces and the way
that they the kind of variation that
they did for faces is that the head was
tilting left and right and then there
was like different facial expressions
and you see that that indeed these
different facial expressions kind of
sorry this is tilt right they don't let
this is smile so you can see that as you
go along here the smile increases okay
but one thing that is a little bit
disturbing about this kind of data
representation it's highly highly
sensitive to how you
sample right so people that a person
basically smiled maybe that's like one
sequence and then you just get like a
little arrow coming a little kind of ray
coming out of the main body for that
smile because that's what you have so
it's a very kind of I would say
bottom-up way of representing while pca
is much more top-down you basically
don't look at any of the details you
just want to to represent the whole set
in some in some way so here is another
use this is a student in UC san diego
that was playing with it it's something
called active appearance models and what
they do here is that they basically
represent the whole face triangulation
actually of the face as a high
dimensional vector and then they do pca
of that high dimensional vector and that
lets them to some degree track how how
the face moves because all of these
variation as the face moves are due to
the largest variation in the
eigenvectors okay so so that is
something that i'm actually interested
in doing one of the things that is
problematic here if you see now we're
kind of at the end here and we're far
from the zero point whenever you have
the PCA you should think about the
average is the zero point you're far
from the zero point so things start to
break because this is not really a
linear manifold yes it's a manifold but
it's not linear at the edges it kind of
starts to bend okay so here is a one
kind of very purified example of
one-dimensional manifold in a very high
dimensional space so this is basically a
teapot that is turning okay and and in
terms of how does the topology of this
set of images it's just a big loop right
however if you think about it in what
they mention it is it's in the dimension
of the number of pixels right so it's a
huge dimension so the loop is really
somehow going around space this very
high dimensional
in just a move but it's a very
complicated group it's not a linear kind
of loop however small changes you can
still think about them as linner okay so
that's kind of the basic intuition is
that here you can kind of put all of the
teapots in in this loop and if you think
about small changes in the teapot just
around here that's really very close to
a linear linear changes in in the image
okay okay when you have essentially in
the lips and then we find a vector is
dominate in values dominate then we
would say with slower dimension or the
few few dimensions really a present
explain most of the variance let's say
here one dimension explains most of the
if the variance is dominated by the
largest eigen values then the set has
intrinsic dimension it's very simple
explanation but now we want to talk
about this D dimensional manifold that
is not a fun right that's basically what
what what what I want to do that is
unity if you grow along this teapot
maybe local little changes it would be
clear that it is one dimensional but if
you make a big change now well every
local one is one dimensional but but but
overall how do i can't use PCA I can't
just take all of the teapots and do a
PCA that won't give me anything useful
so the basic idea is very simple we want
to partition the space into small
regions such that in each
the set is going to be approximately a
fine okay so we're just going to chop it
up but now comes of course the question
okay how do you chop it right I mean you
want to chop it up somehow along this
path but you don't know where the path
is so how will you chop it up okay
here's another notion of dimension that
we talked about quite a lot which is
like Romanian manifold
okay so differentiable manifold I
mentioned is the dimension of the local
tangent space okay so we have a sphere
it is two dimensional object in
three-dimensional space because if we
make small movements infinitesimally
small movements along the sphere we can
express them more or less as a
hyperplane okay so that's that's I would
say it is somewhat reminiscent to the
PCA but it is much more asymptotically
small it's only holds for things that
are sympathetically small okay but now
the problem with talking about these
things is that dimension is really
something that's very much depends on
scale all right so this is something
that whenever we kind of say okay we
want to learn what is your dimension of
this manifold is it's not really a
well-defined question until you tell me
where you want to look and at what scale
you want to look so this is kind of an
example of it here is like a
space-filling curve this small circle
dimension yes it's a one dimensional
thing okay but if you look at this
circle then it's a two dimensional and
this is not like something that is
special to like space-filling curves you
can think about almost any natural
object like a shirt you know on on on
the Dominion on on the on this type of
scale that we're looking at it it is
two-dimensional however if we look at
the strings that make them well its
string is one-dimensional if we go even
deeper it's like it's a mass so it's
three-dimensional so we can go from one
dimensional two dimensional it's all
at each scale we can have a different
dimension worse than that it also
depends on location so here is a set
okay
what they mention is well of course it
can't be more than two because it from
the screen but this part I would say is
really two dimensional this part is kind
of one dimensional and this zero
dimension so cluster is essentially a
zero dimensional manifold tight cluster
is a zero dimensional matter because you
it's close to a point okay so so what is
really dimension it's not the way that
we're discussing it because we want to
talk about data and data doesn't give
you no obey like clear things that you
you know that you want to specify
mathematically location and scale okay
so then there is another notion of the
dimension that comes into it and we
actually have proofs about our algorithm
in this setting and this is actually the
notion that is most popular in computer
science kind of analysis and this is we
say that the set has a set s and W
dimension t if for any ball of radius R
the intersection of the set with a ball
can be covered by its most two to the D
balls of radius R over 2 that's called
the doubling dimension okay and that's
kind of a nice notion of dimension
because it doesn't depend on anything
like neighborhood or metric or all of
these things like PCA it doesn't really
have to be in in in a space that is that
has met a metric or a vector space it is
a much more loose it only depends on on
having a notion of sorry it does have to
have a notion of a metric that's the
only thing that you have to have but you
don't have to have it as a vector space
so so in terms of scales this when you
say that something is low-dimensional in
this case you basically are saying it's
it's all scales okay so you want it to
be for all balls
okay so then then you're basically
saying it has to be doesn't matter what
they mentioned I take it this kind of
covering what would scale I take it what
is our I always want it to be covered by
2 to the D okay so that's it all scales
while the well the notion that I said
which is a manifold is only at
infinitesimally small scales and I want
a notion that is basically scale
dependent okay
so it's more general than manifold
direction and but the thing about it is
that there is no clear connection to
physics
I guess computer scientists like that
but but because I'm talking about
applications in all the applications
that I saw the this notion of
dimensionality even though we prove
things about it is not the one that I'm
in favor okay so now we're going to
prove our notion of of dimension at this
point it should be kind of pretty simple
all that I'm going to say is essentially
okay I'm going to use a covariance but
they're not going to use the covariance
over the whole set I'm going to use a
covariance over the set intersected with
some ball let's say okay so I would say
that this set a particular set has it
has a mean vector and it has a
covariance matrix and let's say that we
order the the eigen vectors of the
covariance matrix by order from the
largest to the smallest so S has
covariance dimension the epsilon if the
first dee variances explained 1 minus
epsilon of the total variance that's
pretty natural explanation and I would
say that S has a local covariance
dimension the epsilon in the ball be XR
if s intersection with VXR has
covariance then
okay so basically this is just a
dimension of a set and when I want to
say that the set has local dimension
then I have to limit to locality all
right so now we're going to show
something that we can do with sets that
are with this kind of flow dimension
okay so first of all the vector
quantization problem that's probably a
problem that is familiar to many people
here but let me just express it in a in
a precise way so you want to find a set
of Representatives for all your data
points or your data points are the blue
points and now these are the three
representatives such that this
partitions the space or it's called the
Voronoi diagram and so that if you look
at the distances the sorry the the
average distance to the closest
representative is minimized okay so you
that's a good set of representative okay
so here's the problem
you were given a set of points in Rd and
you're required to find set a small set
of representative they represented me to
minimize this distance and for our
purposes here we're going to always talk
about the square Euclidean distance so
without taking the square root okay so
there's a well-known algorithm for it
it's called the k-means algorithm and
and basically the algorithm is that you
choose some initial representative you
generate the route of the Voronoi
diagram you take for each one of these
sets that you've got the mean and you
replace the points that you had before
with the mean now you create a new horan
eye diagram and you take again a mean
and you continue in that fashion you can
very easily show that this will converge
to a local minimum it's also true that
if you have the optimal solution it will
stay there however it is not known
whether it will get to the global
solution to the optimal solution might
hope it does there's really no
guarantees in more actually it's much
worse than that it's part of the paper
we prove that the two means vector
quantization problem is np-complete okay
so it sounds ridiculous I mean it sounds
I give you two sets of points and I just
want to practice I just want to
partition them into two sets such that
the the mean of these two sets would be
would be close with the closest on
average to the points and that problem
is already np-hard so there is not going
to be any efficient algorithm for it
even though we're all believing in
k-means so k-means is somehow good but
we really don't know how good and we
know that it's not optimal because even
though you know that the end the the
stopping condition of k-means is like
the stopping condition of the optimal
they're the same condition but it
doesn't mean that it's not so there are
actually good approximation algorithms
for facility limitations so we're trying
to give an improper no we did not know
hardnesses there are good approximations
for either one yeah I I don't know what
are the results in terms of hardness of
approximation I'm going to give you an
approximation that is very very good so
and it's extremely simple it's much much
simpler than k-means so and in some
sense it's a very good one so you can
just think about it in this context yeah
we give a very good approximation for
the k-means method that's actually the
title of our our paper it's a vector
quantization of data with low in
intrinsic item so actually doing doing
k-means for data that is space filling
is kind of anyway pointless right
because you're already filling this
space so it's you're not going to really
capture any meaningful structure you're
just partitioning the space into blocks
that's all you're doing okay
so so here is another very curious thing
that makes the problem seem even simpler
okay
if you basically look at em you this
mean that is the best representative
vector okay it's it's the mean is the
best representative vector the
interesting thing this is just algebra
to show it is that if you look at leads
at the average of the squared distances
between points it's exactly equal to
twice the average of the distance of the
points from the mean so you can you
don't need to actually find the mean you
can just calculate the squared distance
between between points okay but that's
that's just the first step that's kind
of curiosity what is really interesting
is if you want to say okay I have a set
it's now represented by one point and
I'm going to partition it into two so
it'll be represented by two points what
I'm really interested is in how much is
this the average squared distance going
to decrease it turns out that the amount
by which it decreases is exactly equal
to the square distance between the means
that's all it is
so all you are actually are looking for
is you're looking for some way to
partition all the data points so that
the means will be as far as possible
that seems very easy that still it's
np-hard okay but it is something that we
can under some assumptions essentially
the assumption of low intrinsic
dimensionality we can approximate very
easily okay so here's the core idea of
the algorithm I'm not going to go into
the actual proofs that are in the paper
and they're a little bit involved but
here is the basic idea so suppose we
have a data set that has low covariance
that has the covariance dimension okay
so it's a one-dimensional set in two
dimensions so it's immediately clear
that a good way to split it so that the
means will be far from each other is to
split it at the at the median in the
long when you project along the height
large the direction of largest eigen
value okay that's like going to give you
a very good split
assuming that this is more or less
gaussians has long tails and all kinds
of technical conditions but let's assume
that okay so that's nice so basically
all I'm saying at that level is that you
could just do PCA of your data find the
largest direction the direction with the
largest variance and just split there at
the MIDI at the median and that would be
a good algorithm and indeed it is a good
algorithm however if you're talking
about data that is really high
dimensional like these images that I
showed you actually computing the piece
the principal component analysis is not
at all trivial right because it requires
building a matrix that is N squared and
let's say if n is a quarter of a million
then N squared is pretty darn big and so
it's not really like computing the pca
is kind of simple for small numbers but
for big numbers it's not actually that
that simple right and where it is first
component right you don't need that all
right make it right but it's there is no
algorithm for finding the first
component more efficiently
I mean I'll show you an algorithm for
approximating the first component it's
actually a very simple algorithm but if
you just think about it what you want is
an algorithm that will find the first
component without ever needing to keep
something that is bigger than the
dimension you don't want anything like
dimension square you can decree in this
sense what is that you can do yes so
we're doing some kind of great Enderson
right sometimes I'm going to use a trick
that is well known okay but leaving that
all of that aside the question is how
critical is it to really find the
largest direction so that's actually the
main technical point of the paper the
main technical point of the paper is it
doesn't really matter very much you
don't have to find the largest direction
you can split the data according to a
random direction okay so you can
basically just take the data it take a
direction chosen uniformly from the unit
sphere and project the data onto that
and split according to the median on
that direction and you'd be doing
essentially almost as good as if you
split it in the optimal direction and
that's of course much cheaper you don't
need to calculate anything you just
choose a random direction that's the
first step of a gradient this sensitive
in a way yeah yeah yeah that's that's
true dude okay but but why is that why
is it the the first step in the gradient
descent why is why is the random
projection good enough in general you
know that's one of the things that is
actually very much confusing about am
like algorithms is we know that when
they're very close to the correct
solution then they will converge very
very quickly but what are they going to
do in the first step nobody knows
anything there's nothing so here is
basically on this particular case we can
say something about the first step what
we can say is remember the quality of
the split is really the distance between
the means what we can show is that if
the data is really low dimensional now
that we choose a random direction the
then we're going to actually be doing
the wrong thing right with these expect
to these triangles the distance that the
means will move is very very small with
some fixed probability that is
independent of the embedding limit so if
I give you a data set in some massive
dimension 1 million but its intrinsic
dimension according to how I defined it
is is 3 with Epsilon let's say 0.01 then
if I split the data in a random
direction and I look at the distance
between the means it's almost the same
as the optimal with fixed probability so
not not with not with overwhelming
probability but with fixed probability
then fixed probability is good enough
why because we can just try it several
times and we picked the one split that
gives us the largest distance between
the means if you have to pass by the
median what it's not just Direction you
have to see
the central point right right so but
that isn't easy that's it that's a very
easy calculation right that's a very
easy calculation that you can do in a
streaming way even right you project the
data in this direction you get numbers
and there's very efficient algorithms
for calculating the median without
keeping more than like a very small
fraction of the example even a constant
number of example all right so so the
optimal split is orthogonal to the
largest eigen vector and the split on a
random Direction is almost optimal with
constant probability that's really the
heart of the novelty of this paper okay
initially when when I presented it to
people that have experience in this like
Lawrence all he said no that's not true
because you might split you know in very
high dimensions you're very likely to
split things like this because your
Direction is orthogonal to the largest
eigen vector so it's actually likely
that you're splitting almost orthogonal
e to all of the important dimensions
however when you look at the math you
see that yes almost is almost but with
fixed probability your split is actually
pretty good it's not a split that looks
like this so there's kind of competition
here between the epsilon of the low
intrinsic dimension and the and and the
probability that you that you are that
you are orthogonal right if you're
strictly orthogonal to all of these the
good dimensions then of course it
doesn't work because then the means are
here and here and they're very close to
each other but there is a competition
you do the math and you see that there
is a fixed probability that you get
something good which in some sense I
believe is a good explanation for why
EMS first step is actually very very
good because you can think about this as
the end like once you split you find the
means and you take the vector between
the means that's almost the eigenvector
okay so what are we going to do in this
algorithm we're simply going to apply
this idea recursively so we project on
random directions played on the median
and then recurse okay so each part we
project on a new random direction and
split on the median predict a new random
direction spit on the media so in a way
it's very very similar to KD trees right
for those no KD trees but instead of
using a projection on a record in it we
use a projection on a random direction
that seems like okay like a nothing
operation but it in turn in fact it is
actually an important operation and in
fact when we talk with experts about KD
trees they say yeah yeah we do that we
do that what we do is we take the
original data and we rotate it randomly
and then we do PD trees that's almost
the same as this so in practice people
actually knew that forests but they
didn't have like a good explanation okay
so here's the turret the theoretical
explanation this is our paper from stock
zero eight and it's now in high triple-a
information theory so we have a high
dimensional space our to the Big D and
the measure of progress that we're going
to talk about when we do this
hierarchical splitting is what's the
diameter of the cells right so what's
the problem in very high dimensions that
that that we need let's say that we have
something of 100 dimensions then in
order to have the diameter of a cell
compared to the original diameter we
need to to the 100 cells we need to
split 100 times 2 to the 100 is probably
much much more than the data that we
have right so this is really like it's a
non-starter
and that's why candy trees are indeed
non-starters when you have very high
dimension okay but for when you project
things randomly so okay so if you just
do a tree structure the vector
quantization the average diameter is
half every Big D tree levels okay so you
have to go according to the embedding
dimension but if you do this data with
intrinsic dimension little D then you
can show that the RPG
algorithm will have the average diameter
every little D dimension every little B
steps in the tree okay so if the data is
actually three-dimensional you would
actually going to have the size of the
things every three level if it's one
dimensional you're going to have it
every level how does this work
okay so first just kind of intuitively
how does it look here is there some now
I'm going to get into some intuitive
figures to explain it I'm not going to
show the proof because there again too
technical but this is a KD tree
algorithm right so it splits according
to the median of this dimension then it
splits here on the median in this
direction this direction and so on if we
do the similar thing with our P trees
that's kind of what we see okay so we
choose a random direction we split in
the according to the median then we
choose another random directions that
according to the median and so on so for
high dimensional data so if you have a
like in this case two-dimensional data
that is really two-dimensional so it's
for variance is pretty big then it's not
really a big difference but what is the
difference is when you're doing this
kind of trick when the data is lower
dimensional intrinsically and here is
kind of an example of how it works so
suppose that this is kind of more or
less the data that we have that is
one-dimensional in a two dimensional
space okay we look at its covariance
it's just you know it tends to be a
little bit more this way than this way
but really if you just split the data
here you're not you're not really doing
anything particularly good in terms of
let's say reducing the vector
quantization error okay but if you
choose a random direction okay we chose
this random directions actually a pretty
bad direction but still we're going with
it so now you have these two sets and
this is their covariance matrices so
really nothing we didn't get anything
okay but if we now split these two again
now we got something we didn't get
something everywhere we got something
here we got something here
all right so those are pretty well
explained by by by the one dimension
well this very curved one and this
rather curved one are not so well
explained okay so we need to do another
split and with high probability we will
get sorry we will get something that
that is actually pretty well represented
by by low dimension everywhere okay and
so so the trick is that basically we
need the pieces to be small but we don't
want we want to kind of stop when the
curvature is not too high right when
when these pieces have small covariance
dimension and what we basically know in
general is that once you get to a lower
dimension then these plate things will
split actually very well right and it's
kind of intuitive once you're here if
you split again then of course you're
going to split into two pieces with
variance so you just need to split but
you can't split according to fixed
coordinates you have to split according
to random directions and then you can
prove that this would work it's not
going to again work in terms of fixed
probability so maybe in each level of
the tree you need to find ten
possibilities of splitting and choose
the one that that gives you the best
split but that's all you need to do okay
so here is kind of like what how we did
that just for one one kind of toits
experiment this was a paper in nips we
we take this letters from the from
business data set and these are all ones
so we do the RP three on that okay so
this is the average of all ones this is
this plate okay so you see immediately
the split gives you a very big
difference between these ones and these
ones and so on you can go down the lane
and these little graphs are the the
spectrum like how much of the covariance
is explained by the first stop and these
two at the end or how much is left over
unexplained
okay so if you do that and you just
basically now take on the whole data set
you do the standard PCA analysis you see
that it's really what you found is
really meaningful right so here is the
just a projection on to two of the the
two largest eigen vectors in the whole
dataset but now you look at the split of
the data into the into the four node of
the second second level okay so you have
the first node sorry first node zero the
root is one then you have to then you
have four so here I'm giving you
different colors for the four and you
see that it's split more or less
separate pieces but this manifold is
clearly not linear right so each part by
itself is closer to linear that's kind
of what happened and here is basically a
comparison to say that this is really
useful in terms of life
comparing to KD three so so KD trees is
just a data structure for keeping
spatial data so that you can quickly
reach it but again it's important to
have it shallow right so that you have a
lot in each piece so what we show here
in this comparison we build a particular
data set that we're essentially what you
have in the data set is that the two
there are two gaussians but they don't
split on any one Ford and it's very well
they split like on the 45-degree
direction and what you see is that this
is the performance that you get from KD
tree and it doesn't really matter if you
choose the the best coordinate or just a
random chord in it this is what you get
from choosing RP tree so random
projection and this is the best thing if
you choose split according to the PCA
okay so you get even better than the
random but the big advantage is just
going from projection on directions to
projecting on on random directions
either this is projecting axis
orthogonal this is projecting on around
what you have this axis is the is the
average vector quantization error and
this is the levels of the tree that is
built and what what what a vector
quantization error you get through these
experiments did you use the best of ten
yes yes that was actually important if
you didn't use the best of ten most of
the time the graph would be actually
pretty close to here if you were to use
k-means where would that monkey be I
think that it would be pretty close to
this I think that it would be pretty
close to this but we haven't done the
experiment but I think it will because
these sets are actually very very mixed
it's it's it's not easy to figure out
how how to split them unless you predict
okay so okay so we talked a lot about
PCA and indeed there is a fast way to do
PCA so so it's a subroutine and
indications of doing this we actually
sometimes don't don't project randomly
is closer to the largest principal
component and in fact there is there's a
slew of like what what would say the
gradient descent kind of methods these
methods are very very similar to just a
perceptron algorithm the difference is
that you take there are all kinds of
variations in terms of how you normalize
and that's like that's you know a paper
waiting to be written but but the basic
idea is that you take a random direction
vector and you project it you take a new
data set you project the data is a new
data point you project the data point on
your vector so you you see where it is
and then you you project it again on the
on the data point so if you if you your
vector is if the vector that you're now
updating is in the opposite direction to
to the example then you essentially
the direction of the example so you take
the example and you weigh it by the dot
product between the example and the
vector okay so if if it is in the same
direction then it will just stay the
same size the opposite direction then it
will flip to the other direction that's
basically all you're doing okay and this
has been experimentally these are all
physicists they they basically just
tried it and it does really converge
very very fast however the the question
is under what conditions and so what we
believe you can show is that it would
converge very fast under the conditions
of flow intrinsic dimension or low that
the PCA is really dominated by a few
large ones I mean if the PCA is not
dominated by a few large ones then
you're actually not really that
interested in finding the largest one
right so it's kind of okay that you
didn't find them because because they're
not really that much better than the
next one so so that's a that's a an
algorithm that actually works in
practice and now we believe that we can
we can do this okay so we can do this we
can prove that this works so this is
just kind of work work in progress you
have the dimensional data and a
covariance matrix as T times the entries
but you can accurately so in order to
accurately estimate all of these entry
you need actually a lower lot of data so
first of all you just need a lot of data
in order to find the largest eigenvector
if you go this way right but if you if
the intrinsic dimension is much lower
than the embedding dimension then these
kind of online PCA would converge to the
correct largest eigenvectors after
something that really depends just on
the number of largest eigen vectors and
not on the impending dimension so that's
kind of interesting right we have an
image that is maybe five half a million
a quarter of a million dimensions and if
we try to find water the eigenvectors
using using the matrix we might not
converge unless we had a huge amount of
data but if we just use online who
actually will converge faster
where which one
oh the PCA updates oh yeah okay yeah so
so the I'm we're just here writing the
PCA update that they used in their paper
so I don't think it's anything critical
but yes I'll try to explain right the
units are from missing I agree with you
the units are wrong what I would do
instead is I will just take this and I
just take the sign of it which I didn't
do the experiments yet - okay okay so
point me to that I didn't know about
that first eigenvector it's it's
basically a minimum mean square
estimation that you're doing using
perceptron it is the mean square they're
finding the mean squared error over your
data the first order gradient how easy
it is to find the mean square there it's
a linear yeah but but but you know like
here's a simple question if I give you
if I give you in high dimensions a large
set of points and I asked you find the
hyperplane that minimizes the squared
distance to the points that is also
np-hard so so yes all of these things
yes they have solutions in low
dimensions but the question is we want
solutions in very high dimensions so
translating it to mean square there is
not really helping us
okay so so now a few directions that are
not yet in terms of things that are
proving these are now the more applied
kind of directions and first one one
thing that I'm very fascinated by that
we don't yet have a really good solution
for so one thing that you'd really like
to do when you have a low dimensional
manifold is you want to chart it you
want to somehow the set of coordinates
that would go along this because that
would be very useful now now you can
represent data very succinctly using
these folders but there is a not
straight coordinate this kind of okay so
can you learn this system of coordinates
so I don't know how to do it yet but
I'll show you an example just on the one
dimensional case where we did okay so so
basically what is our goal our goal is
to identify sufficiently linear pieces
of the data and then we want to glue
these pieces for us right so so so
here's kind of like a depiction of this
you start with partitioning the data
randomly like this so you get okay this
can be represented by this one line this
by this one I'm not very useful
you split here now you have that
representing this by the projection
along this direction is very good and
this is very good but these are yet not
so good okay so then you just want to go
down to a direction that these every
place is essentially split according to
the directions and the intuition is you
want to split a quote you the the finest
of your splitting you want it to be
according to your curvature all right if
things are very curved then you need to
split them find so that you can see with
their more this leaner if they're very
straight like here you can use a big
piece so the idea is that you build
these trees you don't necessarily just
build one there are so randomized you
can build many each one will look
different and then you kind of want to
harvest them for pieces that are thinner
and then you kind of want to glue all
these pieces together so that's just a
heuristic at this point okay and so here
is how we how we did this heuristic so
what is the data the data is exactly the
rotating teapot okay so we just take all
of these vectors think about putting
them in a random order so you don't have
any sequential
information now we want to basically
recover the sequential information how
does it rotate okay so we initially want
to basically find those little pieces
that are very similar to each other
under small linear transformation okay
so this is basically how we did it each
one of these pieces here is basically a
few images that are together and and
then you can glue these pieces head to
tail and you basically get get the movie
in the back so this is a movie that
essentially the ordering was
reconstructed using this method of
chopping it into pieces and then and
then using them and then what you see
here is kind of the first eigenvector
okay so the eigen vector that represents
this small movement when you're in that
direction and you can actually see if
you look in detail that that the places
where the handle is on the sides those
places you can have a big rotation and
it's only a very small change so you
find very long linear pieces and the
places where the handle is in the middle
those are the very curved one it's kind
of curved in this very high dimensional
space so it's not easy to see but you
can clearly see it in terms of how well
is this thing representing you know you
take the average image and use how good
is it is a representation of the
slightly rotated people okay so now I'm
going to finally talk about the thing I
started which is how to calibrate
microphones using this idea okay so
that's really the first application now
we're thinking about many other
applications but this is the first
application that we really used and
worked very nicely so so we want to
control a PTZ camera using audio
triangulation and and we're doing it by
learning a low dimensional manifold from
the sample data okay so here's how it
works
this was the setup when it was in the
lab now it's a setup actually in a
public place but what we have is seven
microphones put in an ad-hoc situation
okay so we
just go and put them separate from each
other using these sponges so you don't
get vibrations from the thing itself and
and now we want to point the camera
towards where the person is speaking and
that's what the movie that you saw in
the beginning okay so how do you
basically do this thing you look at the
nakusoo source and using a cross
correlation you can find the time delay
of arrival as well so if you take two
microphones you can say what's the
timing difference between the sound
arriving this closer microphone and the
sound arriving at this further
microphone okay that you can actually
measure pretty pretty reliably and
accurately okay so you can do that and
that's kind of how microphone arrays
work in general however in the
microphone arrays world the geometry of
where the microphones are is preset
that's why they're called an array
they're actually put on on on some kind
of rigid frame and the distances are
very very accurately control so you can
pre calculate everything and you are
doing it but that's actually a
fundamental problem with microphone
arrays that because they are on a rigid
frame they cannot be too big right you
are not going to make a rigid frame that
you can barely pass through the door
it's just not practical and we want
actually the microphones to be very far
from each other because then you get
very accurate regulation so it's kind of
kitchen and egg chicken we're going to
basically just take around something
that is very easy to track visually
let's say in your life and it's
generating noise that is very easy to
measure time delay of arrival simply
white noise and we're just going to move
it around and just collect a lot of data
okay so the training data is the delay
for each pair of microphones now we have
seven microphones so we have twenty-one
pairs okay and the camera pan and tilt
that were interested is another two
dimensions that were measured
okay so we're actually every time that
we have this thing stable it's giving us
a vector of twenty three dimensions okay
basically this is kind of like the the
data set that we have that's the
training data set so we have delay
between one and two delay between one
and three two and three and maybe not
all of the delays can actually be
measured accurately it's you know it's
the problem of the thing is that if you
if you basically are far from some
microphone or some microphones have some
noise problem then some delays you don't
really know so you actually get no data
that is you know sometimes pretty spotty
like some microphones that are really
close they get a good in it and then you
have the pan and tilt according to the
tracking of the light okay so this is
the delay manifold so now we're back to
the problem of flow dimensional
applications right because we have 23
dimensional space but they're really the
data lies on a smooth three-dimensional
space why because you know sources are
in three dimensions they can't move to a
fourth dimension right so the data
somehow has to be on a medical and it
has to be smooth because small movements
of the location
don't change the time delay by a lot and
don't change the pen and fill pedal
unless you're right next to the camera
in there okay so so it is really exactly
what we what we want so now if we just
have data and again we're not very
sensitive to how we sample right it's
unlike these methods that look at the
graph structure we can just move it in
some kind of rather arbitrary way as
long as we're more or less sampling from
the areas where people will later talk
from and and it can calibrate itself it
doesn't depend very strongly on you know
the path if you if you do this kind of
thing with with the method to depend on
a graph and of course the strongest part
of the graph or just as you moved along
the path right just one next to the
other so so that's basically what we do
and and we're planning to actually add
more coordinates right once you have
these coordinates why not add also
relative volume that you lit here in the
different microphones and so on yes
sorry I didn't catch when you're this or
the camera is tracking is tracking a
little light that is on the same thing
that is generating the sound and that's
light is so bright that it's very easy
to track okay so that's all good for the
beginning but now we have it actually
much much better because what you saw in
the beginning is just purely based on
audio I actually say it in the during
video but the the system that we now
have actually also has face detection
recognition like Fabiola Jones kind of
okay and now we're basically collecting
data all the time right because we more
or less find where the person is as long
as we keep the camera zoom not too much
the face is somewhere there we find the
face and we correct to the face so that
is all the time collected and basically
every night we run it and every day it's
becoming better right and you don't need
now any so basically the system is just
calibrating itself more and more
accurately with time and now it's pretty
it's pretty accurate so that's so
initially we had to start somewhere so
we had something very easy to track but
with a face detector we now have
something that can detect it but it
needs to have a face somewhere so first
of all so the advantage of the audio
detection is it's much much faster than
this video detection you know because
audio can hear a whole space and with
video get enough size of the face you
basically need to scan all right so
audio is really fundamentally initially
I didn't know I kind of said oh will
find people using either video or audio
but what I kind of think now is rough
detection and actually not so rough you
can do very very quickly just with
microphone and then find detection or
verification that this is not just
somebody stamping their feet or or
something fell but this is a person that
that that that you do with a video so
I'm very much in love with audio now I
think you can do tons of
but okay so so future directions all of
this thing with low dimensional manifold
really got me more and more interested
in the in the place where low
dimensional manifolds really came from
the analysis of low dimensional manifold
which is control theory so in control
theory people constantly talk about you
know you have some some robot which has
degrees of freedom but but the the
pieces are constrained according to the
joints how how relative to each other
and then when you want to apply force to
the joints you have to take into account
the manifold on which things can move
and one of the places where I found that
there is an amazingly beautiful
connection between learning theory in
this control theory is this word by by
rusty Drake from a few years back
actually quite a bit 2004 he worked with
his advisors if I can zoom on on on this
like little robot that learned how to
walk using reinforcement learning okay
so the way this movement of how the
robot actually works is is very
complicated it's definitely like a
complicated manifold and they used here
something very very simple which is
basically just the fact that it always
returns to the middle the system is
fundamentally stable so it always goes
through the middle but they manage to
learn how
so it basically gave it just a rough
thing that we wanted to go forward and
then if learned the whole policy of how
to tilt these little legs in order to
make it rock in just the right rate so
that it would actually move the
activator is here are very very small
they're just little activators down here
so that is something that I think is is
an interesting
direction you can think about you know
you have this kind of Sun so where it's
broke I actually talked with Russ and
said you know what are you doing now so
now he's working a lot on flying things
but but I said what about the walking
things and he said you know the next
step was to have these things have needs
okay so basically that the legs with we
think without knees you can't really go
over rough terrain right because your
legs are just straight things so you
have to bend your knees so so he he has
the one that he built with knees but
then the reinforcement learning never
convert so ah he gave up on them so I
want to get back to them so so that's it
that's that's my talk for today hearthe
data so so the question is whether it is
so sparse that you that you can actually
see the the manifold structure right so
that's that's kind of that's that's
exactly the critical thing right if you
if you basically look at this it's what
density will you start seeing that it's
actually one dimensional well that's
huge density at what density you'll see
that it's two dimensional that is
reasonably that's reasonable as long as
it's not too you know
pushed up like this so so this is not a
well-defined question roughly you have
to think that on if you're talking about
the curvature you have to have enough
density to see the curvature and so this
is very much in data that is like so I'm
my all my intuition is in data that is
physics oriented right so when once you
start to talk about things that you guys
work on a lot you know you have like a
vector of words how you would define low
dimensionality there that is something
that that actually Chandra is thinking
about much more with a doubling
dimension and maybe you can do something
but I don't see like their the whole
intuition of of small variation what
does the what does the a conductor
really mean maybe it can mean something
but it's just too far - too far from it
the place where it's natural the the
example I like to always give is suppose
that you have something like motion
capture okay so you have like all these
dots on the body like 300 dots on the
body and you track all of them in X Y Z
so now you have basically 900 points at
each time step which is maybe every 10
millisecond so you have something that
is enormous but if you basically learn
the low dimensional structure of it
which has to be there because basically
it's all constrained by the movement of
the body and maybe even more than that
the typical movement of the body like
the person is running so it's really
just one-dimensional then there I think
I can really see an application of it
but for thus far as kind of data where
you have sparse sparse vectors I just I
I can't in my mind put it together with
low dimension
I don't know how another question</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>