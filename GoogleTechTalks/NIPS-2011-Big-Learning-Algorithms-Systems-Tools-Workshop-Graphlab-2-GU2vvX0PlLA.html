<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Graphlab 2... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Graphlab 2... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Graphlab 2...</b></h2><h5 class="post__date">2012-02-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GU2vvX0PlLA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this last talk is such an overwhelming
workshop session and it's been really
great but um we appreciate your time
here and what I'm what I'm doing really
the spokesperson for work that's being
led by my students especially you Cheng
Lo and Joey Gonzalez were both here and
also a paki dala in j gu and my postdoc
danny vixen who is also here and we also
involves some collaborations with joe
hellerstein and alex mola and what i'll
talk about today is a little bit about
GraphLab the system we were building for
machine learning over the last few years
and also the second version that were in
progress right now that addresses
natural graphs with large degree nodes
so let's start of the motivation we're
all familiar with machines have changed
process have changed programming these
things is extremely tiresome in hard so
we'll be moving towards higher level
abstractions and thinking about how to
design this algorithm special machine
learning algorithms in this context has
been a real challenge for a community
and it's been addressed throughout this
workshop and as we know my producing
Hadoop are good tools but as the the
talk from cloudera talked about it's
good for the kind of feature extraction
first pass over the data but if you want
to do these are basically data parallel
tasks but if you want to do kind of
complex state-of-the-art machine
learning algorithms MapReduce can be
highly inefficient and so we really want
to exploit this the dependency between
data so if you think about matrix
factorization belief propagation deep
belief networks and so on we're talking
about dependencies between data there's
no well captured with MapReduce and this
type of approach to the step problem we
call graph parallel problems and so as
an example of graph parallelism a very
simple example they assume everybody's
familiar with we can think about the
PageRank algorithm where basically i'm
estimating the rank of a web page based
on an average rank of their web pages
that point into me and the graph here
basically is related to the link graph
on the web and so an update in this
algorithm will say that if I'm page
number five here my page rank is updated
to be some relationship
some weighted average of the page ranks
of the pages of pointing to me and what
you do is you iterate this algorithm of
the graph again and again until
convergence and you can think about this
is a prototypical example of what
happens in many state-of-the-art machine
learning algorithms you have a
dependency graph which might be a
graphical model it might be the Voting
dependencies or all sorts of other
dependencies you see in the data and
what we want to do is local updates
where you have an algorithm that looks
at my nose and neighboring nodes in this
data and try to update my value and
perhaps the values on the edges on the
neighboring nodes and we want to do is
apply these algorithms again and again
iterating them over and over again until
convergence when you have an iterative
algorithm over spar state over graph
then you're covering a lot of machine
learning but it's not well characterized
by MapReduce and because of that there's
been plenty of thoughts about
alternatives especially what's called
prego or the public domain
implementation giraffe they try to
address this craft para lo problem using
what's called bulk synchronous approach
the bulk synchronous parallel model and
the box is a synchronous peril of models
very simple the other graph that talks
about how your data depends on each
other in parallel over the nodes you
just do some computation totally in
parallel and then you pass messages
between the neighboring nodes source
thanks Joey this is his animation at the
end of each iteration there's a nice
barrier that stops everything and then
you go back and iterate so the bulk
synchronous model says I'm going to
iterate everything and then communicate
and iterate everything and communicate
the problem is that volk synchronous
computation can be highly inefficient
and this can come both from a systems
perspective and from a machine learning
perspective so let's start off or an
example of a systems perspective
problems the postbox synchronous model
this is also known as the curse of the
slow job we can think about is the curse
of the last mapper so what can happen is
you're running a bunch of this process
in parallel but one is really slow when
you have to wait for it on to the buyer
comes and then you run in parallel and
another one it's not the same one is
slow and you have to wait with your back
arms and you have to wait and in the end
your total running time is dictated by
the slowest node in every iteration
and this can really slow de practica
this can be really slow in practice but
practical problems are not the only
thing that curses the spoke synchronous
model there's also provably improving
efficiencies when you think about it
from machine learning perspective so a
few years ago and what motivated us to
work on graph loud joy a new chain we're
working on belief propagation a standard
algorithm for graphical models and they
show the theorem that was actually very
well reflecting practice they said that
the book synchronous model when as you
scale the number of CPUs here in terms
of running time is highly inefficient
this is a theorem when compared to a
asynchronous algorithm that works
asynchronous see over the nose in the
graph so what someone node and another
one there's no the sword and if you see
in this particular example which is very
typical a one processor implementation
of an asynchronous algorithm is faster
than eight process implementation of a
bulk synchronous algorithm in this case
is not prego because we don't have
access / go but it's a very efficient
version of it that joy a new chain
implemented and so coming up with an
asynchronous algorithm that address the
belief propagation problem give us a lot
of insight about machine learning and
implementing it in a large cluster was
part of the goal of that project but the
little secret here that the sad part is
that implementing this distributed
version of belief propagation was very
very painful was a long long painful
process where every week joy a new chain
would come to me and say oh I spent a
whole week on a race condition and you
know I didn't do any research sorry and
so that's at least their excuse um and
so because of that we got really
motivated about graph blah because we
noticed that there were a lot of
problems we're both synchronous updates
like prego giraffe were not so
appropriate and we needed a different
framework a different systems framework
that would support the kinds of
algorithms that we see on the right here
and this is where I'll talk about today
GraphLab and GraphLab is really a
solution for the kinds of problems we
see machine learning we have iterative
algorithms over sparse data where
asynchronous computation can give you
big gains and
where if you implement your algorithms
in terms of GraphLab we automatically
deal with the distribution and
parallelization of the underlying system
problem in a race free execution so your
job as an algorithm designer or user is
a lot simpler because you don't have to
do of the systems issue we have a
version implemented in multi-core
distributor setting in the cloud setting
and we've been playing with my GPU
version of this so I'll talk about today
L at first is a little bit of a kind of
overview of what GraphLab is about and
then we'll go into the changes that we
have to do with graph club to deal with
natural graphs and high degree nodes so
let's start with the graph plop
abstraction so the graph of the
structure is pretty simple we define a
graph and then we'll define a lab no
wait we define a graph and and that it
describes the dependencies in your data
we define local update functions or
local computations you can perform on
this graph we have a scheduler that you
can use to tell us how you want your
computation to happen and you have a
consistency model the grantees raise
free executions so I'll briefly go over
each one of these parts the graph part
is the most natural in simplest you can
imagine in a social network the graph
might define social relationships where
data sees in the data vertex might be my
user profile and my current preferences
and daytona edges might be some
similarity measure between me and my
friends now this is a pretty simple
thing very simple representation and
based on it we can define update
functions which are the core things that
you would implement if you write your
program into GraphLab and here's a
simple example an update function in the
context of the PageRank algorithm so i'm
looking at a node and i got to read data
from the neighboring nodes and edges and
what i'll do is i'll just write a little
PageRank update function that takes in
data from my neighbors updates my rank
based on the ranks of their neighbors
and if the bronch has changed
sufficiently or the rank of my neighbors
is likely to tell you sufficiently i go
ahead and schedule it we schedule those
neighbors and here's where the dynamic
computation comes then I only scheduled
nodes as needed I only scheduled copy
station is needed and as we will see
from a practical perspective this can
make a huge difference so to guarantee
that these computations that namak
computation happens asynchronously we
have a scheduler for you and the
schedule is pretty simple it's scheduled
computations on the processors that we
have whether it's motor co cellular in
the cloud and those computations are
really update functions that run on this
graph by looking for example is 18 a and
its neighbor be and as the computation
happens new no new vertices get inserted
into the scheduler into some kind of cue
and then the computation continues and
maybe something else gets executed and
new data gets put into the scheduler and
so on and this process repeats until
convergence or until the schedules empty
now you don't have to worry about any of
this will take a graphic takes care of
that automatically for you now we talked
about the three components of the
representation the graph the update
function and what scheduler you use the
fourth one is a kind of very interesting
component thing might think is coming
from a systems perspective but it has
quite a big impact on the machine
learning problems and this is the
consistency model and so here's here's a
simple consistency problem that you can
have when you write a parallel program
let's say that I'm executing the red
nodes here so this node can look at its
neighboring data and modify perhaps this
one can look at this neighboring data
modify this can look at the decemberists
data modified what happens now if I at
the same time I schedule the center node
if at the same time I scheduled this
node what can happen is this too can
modify data in the overlapping regions
and if they could modify data one node
my right surfing well the other node
might be trying to read and you can get
into trouble and this is called a right
read/write race condition so this is an
example of a race condition you might
have and so often we hear things like Oh
machine learning algorithms are all
stochastic and all these things are just
approximate who cares about whether you
have race conditions or not and what
these people are saying our forget
consistency because if i ignore
consistency i can have higher throughput
so forget all that and i might say sure
forget all that but
for some cases you're okay but for other
cases it can be quite disasters so even
though you might be have higher
throughput you might be doing more
updates per second your updates may be
very lousy so your conversions overall
can be quite slow so here's here's a
simple example the previous talk talked
about matrix factorization here's what
happens with one particular matrix
factorization algorithm ALS was
mentioned in previous talk when you you
allow yourself to have inconsistent
updates when you have run it on eight
processors it's only eight processors
and you have the consistence update your
training error can oscillate a lot over
iterations well if you use consistent
updates raise ps3 update your
convergence is very fast to a very nice
solution so this is an example where
consistency is really important from a
practical perspective let me give you a
second example here here's a little
pseudocode of PageRank algorithm so I'm
going to go over it pretty simply I take
this variable called some i define it to
be the value of the center's node as a
reference I set it to 0 i compute over
my neighbors and then I update that
value seems like a pretty reasonable
program let me show you what happened
what happens so 5 consistent update you
quickly converge and go to the right
answer if you allow yourself to have the
program will show in a previous slide
you go down go down and then you get
this crazy crazy crazy oscillations can
anybody find a bug in the previous slide
there's a race condition in this problem
and I'll show you the race conditions
right here it isn't the sum equals zero
what happens is CTO two picks up this
node and set it some equal to zero and
starts writing stuff into it well CPU
one reads in the particular value well
thats m equals to zero is happening so
the difference between a correct code in
correct one is when you do the sum at
the beginning correct when you do the
sum at the end the difference is very
small is very subtle but impact it can
have on the answer is quite significant
and that's why i guarantee consistency
can be a very important thing even these
simple algorithms like
rank which you can think about as being
very stable and very robust so the way
that you measure consistent think about
consistency is by thinking about what's
called sequential consistency if I have
this problem here with three nodes
what's the question consistent says is
if I execute on cpu one green red blue
and a cpu to execute blue green green
there must exist an equivalent execution
on a single processor of green blue
green red green blue in this example
that will give you the same answer as I
would have at this point if you have
this you have sequential consistency and
what graphic guarantees is that this
happens and this can be very important
as I showed you in a cup in the previous
slides so as a programmer you can think
about the consistency in various ways
for example the most strict no formal
consistencies we call for consistency
where if I execute the knowledge center
I'm not allowed to execute any of them
its neighboring nodes or do anything to
its neighboring nodes so in this case
it's pretty bad because if I execute
this node I can't I can't do anything to
its neighbor or its neighbors neighbors
I can only execute nodes which are three
hops away or two hops away depends on
how you define hubs in parallel and to
address this problem in the part that
often you don't need such a strict
consistency GraphLab offers a range of
consistency models so an example is we
offer something we call edge consistency
what we say you can execute this node
you're only allowed to modify data in
the edges but not dating the neighboring
nodes in this case you can execute nodes
which are one hop away and this is safe
where you only read them right to your
data and data but you don't only read
from neighboring nodes so as a
programmer you have the ability to tweak
what consistency model you have and how
he affects your algorithm now this is a
quick overview of GraphLab GraphLab has
been quite popular recently and a number
of different algorithms have been
implemented within the graph lab
infrastructure I'm just made me a few
here many others are out there and we'll
show you for the next couple minutes is
just some results of GraphLab with
respect to a couple of these different
algorithms so let's start
with the simple pagerank setting and
here I'm comparing GraphLab to prego or
bulk synchronous updates and as is
increased running time and you measure
the error in your paper on today's
mission you see the graph lab conversion
significantly faster than prego or a
bulk synchronous update and here
proposed been implemented via GraphLab
so the systems part of it is pretty
similar and is a pre fair comparison and
to understand why do you have such a
difference we can look at the number of
updates we're doing per second and you
see that asynchronous City setting
versus bulk synchronous that's
significantly fewer updates and that's
why it gets a convergence might pass
through though there's a little bit of
overhead when you do if a synchronous
processing your conversions can be much
faster in this setting and to understand
why you can see that we can histogram
the nodes as to how many updates they
need to conversions and 51 percent of
the nodes only need one update before
they reach consistency well a small
number of the other nodes or this was a
mistake sorry a small number of nodes
meet many more updates and more GraphLab
allows you to do spend more time in with
the hard-nosed and less time of the easy
nodes now this was a demonstration
example let's talk about a problem that
came out of tomatillos group so they
were using they're interested and named
entity recognition task you want to
figure out for example that dog is an
animal the word do G and the way that
they deal with this is the form of
bipartite graph which says the dog
appeared with the sentence run quickly
Australia with travel to Catalina Island
with travel to and is pleasant and by
labeling for example Australia's is a
city or a place then you can propagate
it through this graph back in form and
figure out the Catalina Island is also
place and they wanted to deal with this
a pretty large problem and they were
stuck because it was too hard to refer
them to run on one machine you had about
two million vertices and 200 million
edges and because of this they decided
to use Hadoop so they implemented their
system on Hadoop using 95 cores and it
took them seven a half hours so they pay
because they were able to solve this
problem Hadoop then they learn about
GraphLab and they tried it out on grab
and grab given pretty good scaling so as
increase the number of CPUs they're
scaling was actually pretty nice and
this is the multi-core version of
GraphLab and with the motorcar version
grab something that took Hadoop with 95
course and a half hours took 30 minutes
with 16 cores on graph wad so that's six
times your cpu's for a 15 times speedup
so they'll be happy a thing now gar you
can say no Hadoop is a cloud-based
system and GraphLab there I described so
far as though the multicolored version
but we also have a cloud version of
GraphLab which has also addresses many
of the cloud setting problems like
splitting the data dealing with
communication and so on I won't go to a
lot of details but if you look at the
same problem here where Hadoop was
taking seven a half hours in 95 course
and GraphLab multicolored was taking 30
minutes or 16 cores to do this on the
cloud we r 32 ec2 machines which is
about cinema course a dupe was using to
grab 80 seconds so that's about 0.3
percent of the Hadoop time for the same
problem so sin dependent validation was
very exciting for us and can really
motivated a lot of the work and there's
been a lot of other algorithms
implementing validated over GraphLab
getting pre related speedups over
systems like Hadoop so it's another
example I will implemented a video
crossing meditation problem where you
have a video and you want two segments
each string of the video but you want to
say that sky this this segment here in
an unsupervised way it's kind of the
same as the segment of here later in the
video so kind of segmental label kinds
regions and the way that the
implementation work was am kind of
algorithm using belief propagation to
propagate back and for beliefs about
what segments were and this is a problem
of about ten a half million nodes and 31
million edges and it was a video version
of some work at dhruv was doing as he
drew from the audience and it's a pretty
large problem and we looked at this in
the cloud setting
and so we can think about the speed-up
of the process so as we increase the
number of nodes were using ideally we're
going to get a linear speed-up and we'll
observe is the graph lab gets a pretty
good speed of pretty linear speed-up
although that's not totally perfect but
quite nice this is kind of the hard or
kind of strong skating property another
property that is very interesting
especially in industry it was called the
week scaling property and that property
is one where you look at the number of
processors in the magnitude of data and
what you do is you double the number of
processors and you double the magnitude
of data any double number processor
number the magnitude of data and ideally
you can have a nice line here if your
algorithm is linear and you're scaling
is nice in your system and we see this
the graph lab has pretty closed the
linear line basically well we appreciate
it data site significantly and we only
have about ten percent increase in the
running time so it's very nice
performance here and see so looking at
cloud settings where we buy computer
time you can build p interesting graph
so for example in this one here we're
looking at the cost of solving a
particular problem versus running time
this is on amazon ec2 and we observe at
the beginning adding more machines gives
you a lot performance improvement in
terms of running time but at a little
cost difference but in the end you have
this diminishing returns property we're
adding more machines doesn't help you
running time that much but cost you
quite a bit more but it's nice to kind
of be able to do explore this trade-off
between computational cost so this was a
video course cementation problem also
look quite a lot into matrix
factorization problems in fact most of
our users are using graph lab for matrix
factorization problems today an example
is the netflix challenge data that was
describing the free stock and here
you're breaking in a netflix matrix into
a factors of your users and d factors of
our movies and what we observe is the
graph lab scaling this is the idea of
scale ilyas kelly it gets better and
better as you add more factors here at
the size of the and that makes a lot of
sense because when you increase d you
have more computation / update and
that kind of decreases their over how
the communication and the scaling
becomes quite nice and we've also
compared GraphLab to other settings in
this case again about two orders of
magnitude faster than Hadoop and pretty
comparable to an MPI version
implementation of the same algorithm so
GraphLab is general this is a problem
specific implementation API and got
pretty comparable performances which is
very rewarding now we talked about
dynamic computation and that M
competition can really help with some
problems here if you just do regular
computation where you visit all the
nodes you get this kind of conversions
if you have dynamic computation where
you only update nodes as needed you get
faster conversions and you can do quite
a few more evaluations again you can
look at running time versus cost graph
lobbying about two orders of magnitude
faster Hadoop means you pay about
towards magnitude less to solve your
problem then you would day of Hadoop and
you can explore also the trade-offs the
cost versus error 60 miles particular
accuracy you can look up model
complexity you'd use and how much you
would be paying now in the new version
of GraphLab in addition to the system
properties that I described so far we're
also addressing for tolerance properties
because in larger problems we can expect
some machine failures and the way we're
dressing this is using checkpointing so
every so often we we take a snapshot of
what's happening and we have two
versions of nap shorty asynchronous
version and asynchronous version and
just like a machine learning algorithm
said interesting trade-off between
synchronous and asynchronous algorithms
here so an asynchronous algorithm for a
snapshot you say run a flop for a while
stop everything a snapshot then run
graph opera stop everything is not shot
and so on and if you do this and you
look at your data if you don't do any
snapshotting this is the performance you
get so over time how many butts have
been updated so updates updates and two
conversions here at the end and if you
do snapshotting this way synchronously
you have updates and then there's this
lock time when there's a barrier and we
stop to snapshot and then we continue
but of course just like in the
competition here we have a curse of
slower machine so if one machine
slower is running rough flood you have
to wait for it before you do snot you're
in snapshotting and you can have
significant delays so the example here
is if one machine is delayed by a
certain amount of time for example 15
seconds I think here what you observe is
that your host nap shotting gets delayed
and your whole graph gets shifted to the
right and to address this problem
there's been some classical algorithms
for example there's the candy Lamport
algorithms for asynchronous snapshotting
and it's pretty hard to implement but
thankfully we can implement our own
fault tolerance algorithm as a graph
plot function so graph club for
tolerance is implemented in GraphLab
using graph live updates and I won't go
through the details but the nice thing
about it is what we had no snapshotting
at this performance we're synchronous
snap shopping you have the stopping
point with asynchronous up shouting it
more smooth but it's pretty comparable
performance the difference is you don't
have the curse of the slower machine
even we stick you'll stop shopping
things slow down when some machine is
slow with asynchronous we have basically
the same performance which is very nice
so here we implementing a quote or
snapchatting using graph lab itself
quite
or what do you mean by that
right
is the fact the question is this defects
of stragglers reduce when you have more
machines working on your new job okay so
if you schedule the question is if you
schedule same job promoting machines can
we get around this straggler problem and
this is a trick that people have used
with oh if I Hadoop because they're the
problems are embarrassingly parallel you
can just break them into the pencil
problems if you solve the sub problem
with this machine of that machine is
independent the problem with machine
learning is that they're not in a pencil
problems they're connected to each other
so i can't say i'm going to take out
this piece of data and we computed five
times because you're like very
interconnected the more GraphLab is
doing that justice interconnected
setting so it's a little bit harder to
do this kind of replication to address
strugglers in the in the graph parallel
setting any other questions so this is
the overview of craft lab I promise that
we're changing the abstractions from
what the question is why now if it's so
great well what you know what we're
doing it and the reason we're doing it
is because we want to deal with natural
graphs and the truth in advertising is
that we wanted to solve a web-scale
graph there is a nice graph from outer
vista's web page kind of crawling graph
and we could solve it with version of
GraphLab one because there's some nodes
they had huge degree so there was one
percent of vertices they had fifty three
percent of the edges and this caused
significant problems for the previous
version of graph loud and that was bad
and he i can give you pectoral ey this
was bad the previous version of graph
life I'm executing this node I I look at
all its neighbors in its in the update
function that means I have to walk all
of my neighbors I have to not allow them
to do anything and then process the
neighbors sequentially and this process
was extremely slow because some nodes
had a ton of edges 1% nose had three
fifty three percent of the edges and
because of that we had to rethink the
whole GraphLab abstraction in order to
address the significant problem and the
problem is not just a problem walking
it's also problem with distribution and
communication we can get very
so we haven't talked about how data gets
split over the network but here is a
very simple pictorial view here's a
problem node Y connected to a number of
neighbors I cut the grass across the
network these no's go over here these
notes go over here when I'm trying to
update why I have to transmit
information about all its neighbors
individually across the network and this
transmission can be really slow and in
fact this was what was really killing us
with their large-scale graph and the
issue of I degree and this issue that
was hurting us is present quite
ubiquitous Lee in social networks you
have some people who are very popular
and connected to a lot of others Netflix
you have some movies which are very
popular and a lot of people have rated
them in all sorts of machine learning
problems for example template models you
might have some hyper parameters or some
shared variables they're connected to
every plate for example in this in this
case and in a model like lda you might
have a word like obama that is very
common in a ton of documents and this
settings of high degree nodes were very
significant and we're a big hindrance to
GraphLab skating and what GraphLab is
doing is addressing this problem using
two changes to its abstraction they're
very simple yet looks like look very
promising and seem to be very successful
the first one is what we call factorized
updates it's still having a monolithic
update where you haven't known and we
look at all its neighbors at once we
have an update where you look at each
neighbor individually and you obligate
the contributions to to the update
function and I'll talk a little bit more
about that and the second one is what we
call Delta updates where instead of up
taking a node and all its neighbors at
once we perform local updates where we
only make small changes to the node then
we'll talk about that in more detail
next so factorizing doubt updates are
the big changes in graph club too so
let's talk about factorized updates in
more details the idea here is pretty
simple I have a node and i'm going to do
an update on it based on its papers when
when assume is that update function the
composer's follows a gather phase
where I get to look at node Y and one of
its neighbors and perform some
computation and I do it sequentially and
I'm allowed to aggregate those
incrementally using a commutative
associative aggregation function so you
have to define a gather and some
generalized plus operation and you can
imagine for example in belief
propagation your computing the messages
for each one of my neighbors and the
plus operation here is multiplying the
messages together now once I've computed
this update Delta I go and apply it i
change my nodes value so again in belief
propagation this might be real plating
my belief and here all you have to
define is an apply function how this
aggregated updates modify my node and
finally after modify my don't I get to
go into my neighbor something in a
distributed way using what's called a
scatter operation so simultaneously I'm
able to tell my neighbor something or
modify the data in some way and this is
kind of the opposite of the gather it's
a scatter phase and this is pretty
standard constructs in a parallel
processing and many of the algorithm is
that we love and use all the time will
fit here I won't go into a lot of
details but let me just very very very
briefly tell you that page rank fits
very well in this setting the gather
operation computes how the neighbors
rank affects me the merge operation the
aggregation the plus sign is just
summing the contributions from each my
neighbors they apply just changes my
page rank and the scatter node is the
more interesting one it goes back and
cells which of my neighbors need to be
reauthorized or recomputed now factorize
updates can have very very positive
effects so for example they can really
decrease the amount of communication you
have to do now instead of partitioning
the graph across the network as we said
before we partition the graph in a way
that the node Y is replicated across
machines and the gather is performed
independently on each machine so this
one gathers over here this one gathers
over here and now you compute a partial
solution f1 and f2
for each one of the machines and network
and then you may merge just those
partial solutions so the amount of
communication here is minimal compared
to mount of communication that we had to
do before in particular it does not
depend at all on the degree of why and
that's the big difference and since I
know how to combine these operations
that just combine them I obtained the
Delta and we estimate why and then i
propagate valsad neighbors and again the
big change here is the communication is
independent of the degree which is very
nice now we can think about if we're
doing this what effect does that have on
the consistency model and here we define
a new consistency model we call
factorize consistency and what it allows
us to do is run operations on
neighboring nodes which we couldn't do
before before no matter what consistency
model wouldn't do we couldn't execute a
and B at the same time because they
share an edge now we can execute a
gather on a and a gatherin be in
parallel even though their neighbors
when we use the apply it's a little bit
more complicated because if I'm
operating up life I'm changing the date
on a beacon gather on all of its
neighbors it doesn't have to wait on a
just it can't touch this edge until a is
done so we can do lots of work and
modify a just when it's needed and so
the consistency model is a bit relaxed
and what we talked about before but it's
still guaranteed some nice consistency
properties and seemed to work pretty
well in practice and lots of algorithms
can be implemented in this new
abstraction so for example i mention
billy propagations one God their
accumulation messages from neighbors we
multiply them together apply modifies my
belief and scatter computes a potential
out messages and schedules wherever
neighbors need to be scheduling the next
time step perhaps more interestingly ALS
can also be implemented very nicely
using this new abstraction so again als
we have a Netflix matrix fusers and
movies and what we're doing is we have a
vector W I over particular a particular
user I we have vector x j for particular
movie J and we're updating the weights
up for example w-2 based on x2 and x3
because
this is the movies that w to rank and
the f take function is basically some
matrix inversion x some vector and it
might seem unclear how you implement
this using the description i said before
but it's actually quite simple the
gather phase estimates this matrix is
from your neighbor and this vector from
your neighbor using just sums of terms
and the apply face doesn't matrix
inversion locally and multiplies the
matrix suitor vector alright so that was
our new abstraction and 11 first thing
you can do is compare how previous graph
lot in terms of running time versus
error say for example a pagerank problem
comparison in abstraction and we see the
factorize update give you a significant
performance boost so it's significantly
faster to use pasteurized updates than
it is to use the previous version of
graph blood however factorize updates do
have the limitations so here's an
example for limitation if I'm trying to
if I have some of the kitchen on the
note here that turn red that gets
propagated his neighbor node which
eventually gets propagated why in the
current in the factorized update version
will happen will be all why other
information from all of its neighbors
together and use it to update use the
sum of these to update the value of y so
all of this extra blue work is done when
only the red message has changed so all
these blue ones stay the same all these
blue ones stay the same and only this
red one change and we're going to do all
this extra work and we can address this
using what we call Delta updates which
is a very simple observation we're
assuming that their operations are
commutative associative so we can just
flip them around and redefine this red
update as a difference between the old
blue update and some Delta so rare this
blue plus delta i substitute blue plus
delta here and i change my parenthesis
and say whoa this part over here was the
previous car solution and all i have to
do is apply to delta so in fact all i
have to do is propagate deltas
without ever having to gather of the
nodes and this is going to be a huge
change this Delta update is what's going
to allow us to deal with the web scale
graph in the end without doubt updates
even factorize that pace we couldn't
deal with the web scale graph and again
pagerank can be implemented in the
setting and not going to talk about
details of how it's done we can compare
this is the graph that we had before
this is a preview type implementation
this is graph blob one on our page rank
running time versus l1 error if we do
factorize updates we have an improvement
this is exactly this part is where i
showed you a few minutes ago with delta
plates we get a significant improvement
a significant improvement in money time
and and that's all because we only
update when it touches they have changed
now we can compare GraphLab 12 GraphLab
to in a distributed setting the result I
showed you so parable multi-core this is
over up to eight machines in a
cloud-like setting and we'll see is that
GraphLab of delta plates is
significantly faster in a previous
version of GraphLab which was previously
significantly faster for example Hadoop
and if you try to understand why this
happens why there's a difference between
GraphLab one and the other updates you
can think about Delta places being much
smaller much less communication and this
is exactly what happens if you look at
their total communication we have to do
for computation with GraphLab to the
scaling over the number of machines of
communication is significantly smaller
then of the old GraphLab version and
this is what all the gains came in but
as I was building up our goal was to
solve this big problem this is the
AltaVista web graph which was released
by in 2002 which was released by yahoo
recently come recently he has 1.4
billion vertices and 6.7 billion edges
and there's been a couple people have
worked with Hadoop on this type of data
including are a drag competition overlay
on Hadoop called Pegasus which got about
9,000 seconds to compute on eight hun
cars and using GraphLab too we can get
about 400 seconds on close to half of
those cars and this is a big change in
significant value here and there's some
some sistas of GraphLab two is infancy
so far there's some overhead that every
think we can shave off this value even
further so again we're getting order
plus speed of over Hadoop computation so
just summarize GraphLab two changes we
have decomposable update functions which
address high degree node so so the
previous monolithic update we have the
scattered start update so this is it
this providing is parallelism about the
update function so parallelism with
respect to high degree vertices what
data updates do is provide us by exposed
for the asynchronous in where I
synchrony and provide us with for the
parallelism even inside update functions
themselves and they said the two changes
in the new graph pad version and as we
implement the graph lab one and now
GraphLab too we have significant
insights we feel about the kinds of
problems that in machine learning of
need to be addressed in parallel just a
few lessons learned here asynchronous
computation can be significantly faster
than the BSP models the preggo like
giraffe models and I think that's
something that we should be really
addressing dynamic computation can often
be faster but sometimes it's hard to set
some of the thresholds when we decide
for example the some computation does
not need to be done so there's some
science to be done there might be even
now rhythm specific consistency can be
very important so not only that
sometimes required for conversions but
often is imperative to get any
reasonable results of course there are
some problems where our consistency is
not that important and graph levels to
tune different consistency models from
the systems perspective is of course
hard to build an asynchronous system
than a bulk synchronous parallel system
but i synchro systems give you many
vantages Francis in perspective for
example the lack of need to barriers as
shown by the stop shouting work here
skinning up and all that language really
required us to rethink our presentation
and rethink our abstractions and I think
we might have to do that in future of
course as with your larger and larger
problems now GraphLab has become pretty
popular recently we've had a ton of
downloads from a lot of different
companies about five startups are very
actively using GraphLab and i only own
one of them and another universities are
playing exploring or actively using
GraphLab we've had the two thousand plus
downloads since we started talking and
as an example in this year's kdd cups
several groups are using GraphLab and
their overall observing about tours in
magnitude speedups over Hadoop over
mahout which is a Hadoop implementation
so in summary reply provides us for an
abstraction that focuses on the needs of
commercial any problems it makes it easy
for you to implement your algorithm and
we deal with the parallelization
challenges the race condition challenges
and all the lower level data partitions
and data management challenges for you
and you can achieve state of the art
performances on really large-scale
complex machine learning problems rather
than simple ones today you can use
GraphLab one and you can download this
from grab the org we're currently
working GraphLab too and you can be able
to download that hopefully piece thank
you very much
he's a question mark there yes
we have the people Samson
k my age and I put together at this will
be my daughter realized there's a memory
overhead Spartacus items which are say
so many processes it I don't think so
far would work seeing significant
this bag fitted with modified at least
at this element where you wouldn't eat
all the neighbors of ewes and rams level
for them and so basically
we use a fraction of the power
with that what do we have options
and this consistently better I think so
don't have a large pockets
so right now we use randomization in a
sense of additional data whether by
Machine later so equally Noah so
machines an interesting thing to think
about there are more or less as i'm
currently to unify machine but
nonetheless the sutra versa ? acidity we
r memory and we kind of reserved
three wooden rod spoil however inviting
the future that could be a
redistribution cell weather as you
execute to discover that there's some
article about the resistor will I
department initially my brother BTW
series of them we r address a problem so
far
Oh
sighs needs to be done and whether it's
continuous orange at best everyone will
do Chi the second have to say I an X on
the road app acid whereas announcer this
is kinda question and we don't have a
general theory of human basing all from
his neighbors is how to get a little
result in general rachell errors for
several this is it graphically modify
the drug like generally you are running
this competition eldest a figure out
conversions
to me like that 15 a bench monday and
then
or some tricks on it but it wouldn't be
like that as said the British party will
be starting to autograph repartitioning
as many times of this work and he has
seven
we have to get that and see what
machines as we're going to change
something
rupees
this example I was
change
this is
aww</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>