<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Stochastic Quasi-Newton Methods for Online Learning | Coder Coacher - Coaching Coders</title><meta content="Stochastic Quasi-Newton Methods for Online Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Stochastic Quasi-Newton Methods for Online Learning</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3zmI2g_tNcI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you for coming everyone I'd like
to introduce you to Nick shot off one of
our esteemed colleagues in machine
learning I be talking about stochastic
quasi-newton methods today I first met
Nick at the AI stats conference in two
thousand where he taught me a drinking
song which claimed there was no beer in
Hawaii something of a jealous denial I
think by parts of Europe from here I'll
hand it off to Nick thank you alright
can you hear me okay I have to add to
that that to a Bavarian whatever is
available for purchase in Hawaii does
not qualify as beer so I'm my background
is in machine learning I grew up as a
graduate student in the neural network
days of machine learning when everybody
was excited about neural networks and to
give a bit of a background what bothered
me more and more was that training
neural networks proved to be very
difficult and this is not really a
machine learning problem it's an
optimization problem finding the optimal
waits for the neural network and it
turns out that neural networks are real
beast to to train they're very hard very
hard optimization problems and so the
field as a whole has moved away towards
other techniques that are now used in
machine learning a lot and one of the
underlying reasons for this movement
away from neural networks was that these
other systems were much simpler to
optimize however what's happening lately
and what I see more and more since about
two years ago or so is that people are
now realizing that these other
techniques they're using kernel methods
support vector machines and so on and so
forth have their own drawbacks and
there's now actually a kind of the
pendulum swinging back and neural
networks even though nobody calls them
by that name are actually coming back in
disguise and so I now find that the work
that I've been doing which was concerned
with improving the
optimization of neural networks for a
long time is becoming more and more
relevant pointing to the future so what
I'll talk about today is actually not
the well rehearsed talk that I've given
for the past five years when I talked
about this which was about an algorithm
called SMD I developed in the past
couple years but now I have something
brand-new and what you'll see was
presented for the first time last week
literally at the AI stats conference in
in Puerto Rico and the snowbird learning
workshop which also was important to
recon olean over this time so this is
all brand-new stuff so I apologize in
advance if it's not a completely
polished presentation but I hope it will
be all the more interesting to start off
I don't think I have to dwell here on
the fact that we're drowning in a flood
of information you're well aware of that
at Google there's two basic reasons
sensors are becoming more and more
affordable and plentiful and they're
more and more networked that's the basic
underlying story so you get more and
more data here's some random examples in
science in business in security you
probably can only laugh at this kind of
volume I don't have the numbers for
Google but they're probably a couple
orders of magnitude larger what I signed
striking is in London apparently there
are over 500,000 security cameras and
I'm wondering who's watching all that
video right so it's it's clear we need
some automated methods to process this
data because we're generating far more
data then the entire humanity can
actually handle manually now there's a
problem so we needed intelligent filters
to cope with that but the problem is
that in order to cope with such a large
flood of information we need machine
learning algorithms so machine learning
is nothing but reacting adaptively and
intelligently to data and but we need
algorithm that kind of fulfill three
conditions at the same time
since we have a lot of data we may want
to fit very large and complex models
with millions of degrees of freedom the
data is comes in high volume and it's
low quality nobody has time to do any
quality control right so you have noise
outliers everything non-stationary data
correlated signals so you've moving far
away from this ideal machine learning
world where you have some small finite
set of data that is iid sampled from
some known distribution and so on and so
forth and finally we want the answers in
real time as things happen and typically
one of the reasons for that is that if
we have these sensors generating data
there's always new data so there is
never a fixed training set where we say
okay if we learn that then we're happy
right there there's always more and more
and if the data is non-stationary we
need to adapt to it as we go along now
my claim is the current techniques have
difficulty with this and in particular
by that I mean with fulfilling all these
three conditions at the same time
there's lots of techniques that can do
two out of three but all three is quite
challenging and there's a basic
underlying reason for that and that's
the way classical optimization is set up
so in classical optimization for
nonlinear problems you have some
iterative optimizer that goes around the
loop and tries to let's say minimize
some objective function however to
evaluate the objective function which
needs to be done every time you go
around the loop since that's defined
over some training data set you have to
go through all your data so the idea is
say if your objective is to do a least
squares regression from some inputs to
some target values and you try to
minimize the squared error you're
summing that squared error over every
item in your training set so that's the
inner loop and then there's an optimizer
around the outside so you have two
nested loops now as the size of the data
set grows that becomes extremely
inefficient
because just to get a single evaluation
of the function or its gradient for
instance will take you far too long so
what you need in that situation is an
online optimizer where basically what
happens is your interleaving those two
loops so as you're optimizing your model
you're sampling new data and those two
processes are interleaved and go hand in
hand so now you've you've interleaved it
into a single loop and that turns out to
be far more efficient for large data
sets so this goes by many names so you
could call this adaptive filtering
stochastic approximation online learning
none of the names fit very well I think
this is a fragmented area where people
depending on what field they've been
coming from have called these different
things now here's the same story in
equations classically you'd formulate
your optimization problem like this
you're looking for some parameter vector
theta star which is the argument so it's
a setter it's the parameter vector that
minimizes the expectation of some loss
function J over some data X and the loss
function is jointly defined on the
parameters in the data and since you
can't compute the expectation what you
do is you approximate that by the
average over some training set of data
that's called the empirical risk now as
I said before for large data sets that
becomes inefficient and if you have a
never-ending data stream you know
there's no point to cut off this to
compute an average so it's completely
the inappropriate framework and so
stochastic approximation what you're
doing is you're roughly you're
performing a rough minimization at every
sampling step of your parameter vector
but then you move on to the next data
sample now I have a picture here that
shows you how that works this is actual
an actual objective function from a
video hand tracking
ask so we're looking at a
two-dimensional slice of about 30
dimensional parameter space and these
are contour lines for the value of the
objective function and here's a
trajectory of a stochastic gradient
descent so we're going down the gradient
of whatever sample of the objective
function we have at the moment and what
you see is because it's stochastic every
time you evaluate the gradient you're
actually sampling from a slightly
different function and that's because
you have different data available at
that point in time and what you can see
is this is not just Gaussian noise this
is not just some some clean additive
noise on top or so but this whole
function is pumping and breathing in
some in some sense so this unfortunately
poses very tough problems for
optimization so stochastic approximation
actually breaks a lot of these
optimizers and in particular two things
that happen one thing I i talked about
actually at the AI starts conference for
years ago that the pole mentioned
initially is that conjugate directions
break down when you do stochastic
approximation the process of conjugating
search directions in building a Krylov
subspace for optimization is very
sensitive to noise and so basically any
amount of noise in that process will
just completely derail it now it happens
that trial of subspaces are one of the
key tools of efficient optimization
algorithms so you can basically scratch
that as soon as you do stochastic
approximation that doesn't work another
key tool Arline minimization so a line
minimization says once I've decided on a
direction in parameter space to go into
how far should I go and a line minimizer
basically gives me a point on that line
in that direction that fulfills certain
conditions and there are various sets of
conditions are called strong wolf
conditions goldston Armijo conditions
what have you and it turns out that
the convergence of many standard
optimization algorithms critically
hinges on these conditions being
fulfilled so there's a strong dependence
on the line search actually returning a
good point for you now if you're doing
stochastic approximation you never know
the actual function you're trying to
optimize all you have our samples from
that function noisy samples so whatever
condition you would like to be fulfilled
you can never guarantee it for the
actual function you can only say well on
this sample the condition holds on the
next one maybe it won't or you can say
if you sample a lot you can maybe say
well this condition might hold with
ninety percent probability or more so
you can never guarantee these conditions
that are needed by the classical
optimization methods in particular
conjugate gradient again and causing
Newton methods rely on these and finally
if you take the more expensive
optimization methods such as living bird
mark ward or Newton's method they
require an inversion of a matrix at
every point at every iteration and that
matrix is n by n if your parameter
vector has n entries and so that's an
order n cube close to order n cube
process that's fine if your iterations
are slow so if you have a large data set
and you're going through all your data
before you do one iteration then it may
well be worth to invert a matrix at the
end of that process but if you're online
every data item that comes in you want
to run an iteration of your optimizer
then these algorithms just become too
expensive to run that at such a high
rate so what does that leave oops sorry
I'm behind what that leaves is what
people have classically used when when
learning online when doing stochastic
approximation and it's sort of the poor
cousins of the of the real optimized
algorithms namely you can just do direct
search by which I mean search only using
the function values not the gradient and
if you have a smooth function that has
nice gradients but you're not using the
gradient by comparison this leads to
really glacial conversions these are
very slow optimization algorithms
they're very robust and reliable but
they're very very slow to reach results
next one step up is simple first order
gradient descent so what what you do for
that is you literally say the new
parameter vector is the old one minus
some step size times the gradient of
your function the gradient always points
in the steepest uphill direction so if
you're subtracting the gradient you're
going in the steepest downhill direction
so you're going to end up finding a
local minimum of your function
unfortunately this is can be very slow
if your function is ill conditioned and
ill conditioning you can visualize say
if your function has a ball shape but
it's very elongated like a canoe imagine
a canoe shape right and if you're on one
side of that canoe your gradient is
going to point across the valley and so
what you'll end up doing is you'll go
back and forth across that Valley but
you'll only very slowly make progress to
the center of your canoe actually and
this is very well it can be very well
characterized mathematically and has to
do with the condition number of your
system and simple gradient descent the
convergence is basically proportional to
the reciprocal of the condition number
the convergence speed yes
yeah oh okay yes so it was mentioned
that in neural networks people have
worked a lot on these acceleration
methods say looking back to see what
what happened over the last couple of
steps and using that somehow to to speed
up the convergence and that's true and
that's one of the good things that that
happened back then and a lot of the work
that I've done is actually in that vein
so I've developed an algorithm
stochastic made a descent which is which
adapts the step size for the gradient
descent and it has a separate step size
for each dimension in your parameter
space and it looks back and does the
other patient actually by looking back
over an infinite window and it carries a
long additional gradient information it
basically carries along partials between
the objective function and the step size
or the log step size actually and you
end up with a fairly complicated
algorithm which does substantially
accelerate convergence and there have
been simpler things like momentum and so
on and they all work to some extent
they're all at another level a bit
unsatisfactory because they cannot
change the fact that you're dealing with
the first order gradient descent you can
accelerate it but you're going to get a
factor out of it of maybe three times
five times perhaps if you're lucky ten
times faster convergence but it doesn't
change fundamentally the order of
convergence it's still a first-order
algorithm and so these things are very
very useful in in practice and I use
them all the time as well and I've
developed some of them but today I'm
actually happy I'll be talking about
something that's that has the chance of
being fundamentally better yes finally
to finish this slide actually I see on
the next slide I'll come
to that the smartest thing people have
done online is basically two things that
are very similar one is natural gradient
which basically means you multiply your
gradient your pre multiplied by the
inverse of the Fisher information matrix
of your system and official information
in this context is just the covariance
of your gradient so your gradient is now
a stochastic quantity it's a random
variable since it depends on your
sampling and you calculate but then you
move on to the next data sample now I
have a picture here that shows you how
that works this is actual an actual
objective function from a video hand
tracking task so we're looking at a
two-dimensional slice of about 30
dimensional parameter space and these
are contour lines for the value of the
objective function and here's a
trajectory of a stochastic gradient
descent so we're going down the gradient
of whatever sample of the objective
function we have at the moment and what
you see is because it's stochastic every
time you evaluate the gradient you're
actually sampling from a slightly
different function and that's because
you have different data available at
that point in time and what you can see
is this is not just Gaussian noise this
is not just some some clean additive
noise on top or so but this whole
function is pumping and breathing in
some in some sense so this unfortunately
poses very tough problems for
optimization so stochastic approximation
actually breaks a lot of these
optimizers and in particular two things
that happen one thing I i talked about
actually at the AI starts conference for
years ago that the poll mentioned
initially is that conjugate directions
break down when you do stochastic
approximation the process of conjugating
search directions in building a Krylov
subspace for optimization is very
sensitive to noise and so basically any
amount of noise in that process will
just come
pletely derail it now it happens that
cry love subspaces are one of the key
tools of efficient optimization
algorithms so you can basically scratch
that as soon as you do stochastic
approximation that doesn't work another
key tool our line minimization so a line
minimization says once I've decided on a
direction in parameter space to go into
how far should I go and a line minimizer
basically gives me a point on that line
in that direction that fulfills certain
conditions and there are various sets of
conditions are called strong wolf
conditions Goldstone Armijo conditions
what have you and it turns out that the
convergence of many standard
optimization algorithms critically
hinges on these conditions being
fulfilled so there's a strong dependence
on the line search actually returning a
good point for you now if you're doing
stochastic approximation you never know
the actual function you're trying to
optimize all you have our samples from
that function noisy samples so whatever
condition you would like to be fulfilled
you can never guarantee it for the
actual function you can only say well on
this sample the condition holds on the
next one maybe it won't or you can say
if you sample a lot you can maybe say
well this condition might hold with
ninety percent probability or more so
you can never guarantee these conditions
that are needed by the classical
optimization methods in particular
conjugate gradient again and causing
Newton methods rely on these and finally
if you take the more expensive
optimization methods such as limburg
mark word or Newton's method they
require an inversion of the matrix at
every point at every iteration and that
matrix is n by n if your parameter
vector has n entries and so that's an
order n cube close to order n cube
process that's fine if your iterations
are
slow so if you have a large data set and
you're going through all your data
before you do one iteration then it may
well be worth to invert a matrix at the
end of that process but if you're online
every data item that comes in you want
to run an iteration of your optimizer
then these algorithms just become too
expensive to run that at such a high
rate so what does that leave oops sorry
I'm behind what that leaves is what
people have classically used when when
learning online when doing stochastic
approximation and it's sort of the poor
cousins of the of the real optimization
algorithms namely you can just do direct
search by which I mean search only using
the function values not the gradient and
if you have a smooth function that has
nice gradients but you're not using the
gradients by comparison this leads to
really glacial conversions these are
very slow optimization algorithms
they're very robust and reliable but
they're very very slow to reach results
next one step up is simple first order
gradient descent so what what you do for
that is you literally say the new
parameter vector is the old one minus
some step size times the gradient of
your function the gradient always points
in the steepest uphill direction so if
you're subtracting the gradient you're
going in the steepest downhill direction
so you're going to end up finding a
local minimum of your function
unfortunately this is can be very slow
if your function is ill conditioned and
ill conditioning you can visualize say
if your function has a ball shape but
it's very elongated like a canoe we met
in a canoe shape right and if you're on
one side of that canoe your gradient is
going to point across the valley and so
what you'll end up doing is you go back
and forth across that Valley but you'll
only very slowly make progress to the
center of your canoe actually and this
is very well
can be very well kakak characterized
mathematically and has to do with the
condition number of your system and
simple gradient descent the convergence
is basically proportional to the
reciprocal of the condition number the
convergence speed yes yeah mm-hmm oh
okay yes so it was mentioned that in
neural networks people have worked a lot
on these acceleration methods say
looking back to see what what happened
over the last couple of steps and using
that somehow to to speed up the
convergence and that's true and that's
one of the good things that that
happened back then and a lot of the work
that I've done is actually in that vein
so I've developed an algorithm
stochastic meta descent which is which
adapts the step size for the gradient
descent and it has a separate step size
for each dimension in your parameter
space and it looks back and does the
adaptation actually by looking back over
an infinite window and it carries a long
additional gradient information it
basically carries along partials between
the objective function and the step size
or the log step size actually and you
end up with a fairly complicated
algorithm which does substantially
accelerate convergence and there have
been simpler things like momentum and so
on and they all work to some extent
they're all at another level a bit
unsatisfactory because they cannot
change the fact that you're dealing with
the first order gradient descent you can
accelerate it but you're going to get a
factor out of it of maybe three times
five times perhaps if you're lucky ten
times faster convergence but it doesn't
change fundamentally the order of
convergence it's still a first order
algorithm and so these things are very
very useful
in practice and I use them all the time
as well and I've developed some of them
but today I'm actually happy I'll be
talking about something that's that has
the chance of being fundamentally better
yes finally to finish this slide
actually I see on the next slide I'll
come to that the smartest thing people
have done online is basically two things
that are very similar one is natural
gradient which basically means you
multiply your gradient your pre
multiplied by the inverse of the Fisher
information matrix of your system and
official information in this context is
just the covariance of your gradient so
your gradient is now a stochastic
quantity it's a random variable since it
depends on your sampling and you
calculates note that it's basically it
involves this matrix and it uses a lot
of inner now the products between these
S&amp;amp;Y vectors so the the parameter and
gradient displacement they are often
called it just sort of get the the
general idea that we're going to have
these two displacements that were
measuring so comparing the last
basically quantifying the last step we
did in parameter and in gradient space
and then we'll build the update out of
that now as it stands this algorithm
doesn't work with stochastic
approximation and people have tried
actually since I'm giving this talk
several people have have come and said
oh I've tried intensely to get this to
work online and I'm really happy that
we're the ones that actually managed to
get it to work so here's a list of the
modifications we're doing to get this to
work online first of all when you're
online your gradient depends on the data
you're sampling in addition to the
parameters okay first trivial change
second the line search doesn't work
online as I told you before so we have
to replace it fortunately it turns out
that you can do this in when you do
conjugate gradient then you're trying to
do it online
you get stuck because conjugate gradient
the optimizer actually critically
depends on doing an exact line
minimization at every step otherwise the
conjugation of search direction doesn't
work either so if you can do line
minimization you can't do conjugate
gradient for bfgs it actually turns out
the update for this b matrix for the
scaling matrix is quite robust and
independent of what your actual
trajectory in parameter space is in fact
you can feed random samples of
displacements in parameter in gradient
space into the B update and you'll get a
good curvature estimate and this works
even for negative curvature for
indefinite curvatures it always works so
this is very very nice this means that
you don't need the line search you just
need to take a step in in a night in the
nice direction and so what we're doing
in the experiments here is something
very very primitive we set our step size
8 RT 2 tau over tau plus T times a 20
where a 20 and tau are tuning parameters
that we just hand you so this is a decay
schedule this is a typical online
stochastic approximation gain decay and
this is the type of schedule that
fulfills the technical conditions for
convergence of a stochastic
approximation algorithm you may have
heard Robbins Monroe conditions these
are the most well-known set of
conditions so this is this type of game
decay what happens there is as you see
more and more data you'll take smaller
and smaller steps and you do this in
order to converge on the overall minimum
of your function instead of chasing the
individual sampled minima right so
intuitively it's a very simple thing
that happens here this is clearly far
from optimal I hope in future work that
we can improve on this now now we have
this update yes
like this would like get a G such that
like yeah yeah so the question actually
aims towards what other more opens
Monroe conditions and I can I can
briefly mention them they're quite
simple to understand they basically say
that the squared step size is a tatty if
you if you sum them all up and over all
time to infinity the squared step sizes
must be some herbal so they must give
you a finite value if you sum them all
up but the step size of themselves must
not be summable so you must get infinity
if you sum up all the step sizes and
it's essentially a statement about the
rate at which the step size is decay to
zero as time goes to infinity and
essentially if they decay too fast 20
you'll get stuck before you reach the
optimum if they decay to slowly you'll
never actually converge to the optimum
you'll keep jiggling around and these
conditions basically formalize that and
this this schedule fulfills these
conditions so it's it's of that type
that and things converge nicely all
right so here's the be up update now we
have a trivial change first obviously
the gradients depend on our data now as
well however this is not what we do this
is what everybody else has tried it
before has done so it's completely
natural when you do stochastic
approximation and you think of a
difference of two gradients at different
time steps to take this difference right
you have the gradient on the new data
and the new parameter value minus the
gradient on the old data and the old
parameter value the problem though is
that this allows sampling noise to enter
into y and hence to enter into your
update for the b matrix and that's
because xt and xt plus 1 are different
data samples and they may tell you very
different things
the objective function so you get a
sampling error basically entering your
update here and that that noise that
enters is basically derails the whole
algorithm there's a very simple way to
fix that but it comes at the cost of
increasing your cost per iteration by a
factor of two unfortunately and that's
this you take this difference evaluated
on the same data now there's no sampling
noise here this difference is a
deterministic quantity however what you
have to do to get this is you have to
compute the gradient at the new
parameter value but on the old data and
then at the next step you'll need the
gradient at the new parameter value but
on the new data so it's twice as many
gradient evaluations so that's something
I cannot avoid this factor of two
however what I'm seeing now is there's
actually once you have these twice as
many gradient evaluations there's many
many interesting things you can do
besides taking this difference for
instance you can use these gradients
also to compute the difference between
gradients at the same point in parameter
space but for different samples of your
data what does that give you it gives
you a very nice estimate of the Fisher
information right because you're now
really not looking at the curvature of
your objective function you're just
looking at a single point at the
sampling process that and how that
affects the gradients so I think there's
a lot of algorithms still hiding in
there once you realize that you have to
distinguish between the gradient at
theta T plus 1 XT plus 1 the one at
theta T plus 1 XT and the one at theta T
XT plus 1 and the 1 and theta txt
there's four different quantities flying
around here and depending on how you
take difference is in how you compare
them to each other you get quite
different you can measure different
quite different things so that you have
to look here at the Hessian and
Fisher information is two completely
separate quantities that are unrelated
in fact they measure different processes
so this will be an area that will be
very interesting to look into for for
optimization now this was the key change
to make the algorithm work but since we
don't have a line search we just have a
simple game decay turns out that we have
to add some more seat belts to the
algorithm to prevent divergence on nasty
functions and one of these seat belts is
that we try not to estimate the inverse
Hessian but we take the Hessian plus
some scalar lambda some tuning parameter
lambda times the identity in the inverse
of that it turns out that's very easy to
do basically for why you just add lambda
times s to Y and then you run the normal
update and that's it so that's that's
pretty much it there's one more little
change so here's the standard bfgs
update formula and we've added another
scaling factor in here which empirically
we found to improve behavior a factor
that's somewhat smaller than one we
might use 0.1 0.2 0.5 there's a lot of
literature on scaling bfgs updates and
so this is another area where we just at
the moment doing something heuristic and
that can probably be improved and we'll
link this actually it looks like this
kind of stuff both the lambda and the
sea can be linked to regularization in
different metrics of your objective
function so we hope to actually get rid
of these as tuning parameters by by
calling them instead regularization
parameters and then it's the fault of
the machine learners not the optimizers
that you have them and you have to find
a value for them right so that's the
idea we will sweep them under the rug
anyway that's that's a lot of what keeps
us currently occupied now once you have
the C parameter you'll also scale the
step in the reciprocal e with see that's
just the technical detail because it
scales the B matrix
okay now that's the whole story that's
all the all the changes we made now for
limited memory bfgs look why am I not
going forward here we are limited memory
bfgs is a variation by Mourinho saddle
on bfgs and what it does is instead of
trying to estimate the full inverse
Hessian it constructs a rank 2m
approximation to that estimate and M is
a tuning parameter it's your choice and
due to that flexibility it's become
really the dominant algorithm for
nonlinear smooth optimization and
literally whenever you call a standard
optimizer for some on some nonlinear
smooth problem it will under the hood
run l bfgs at least I'd say ninety
percent of the time this is an
incredibly widespread algorithm the way
it works is you maintain a ring buffer
of the last n values of the SNY vectors
there's also a scalar roti that you
remember the last end values of and then
you use this algorithm to calculate your
direction so the direction is it a TBT
times GT with GT is the gradient and
this loop actually calculates this
quantity I don't want to explain it in
too much too much detail here it's sort
of a bit of linear algebra magic going
on and it does so without ever
calculating the B matrix in full it's
another one of these implicit matrix
free methods so it calculates the matrix
vector product B TGT and does not need
to construct the full matrix and that
reduces the cost per iteration from
order n square if the B matrix is n by n
to order em n where m is your choice of
buffer size and this has made this
algorithm incredibly
popular for M equals one you can
actually relate the resulting algorithm
to conjugate gradient methods so another
way to think of quasi of l bfgs is it's
a generalization that sort of gives you
a smooth interpolation from a conjugate
gradient method to a Newton method and
you decide where on that on that slope
you want to be depending on how much
computational resources you have and I
just want to say the online variant is
very very straightforward we take the
same type of modifications we did before
literally the identical modifications we
just apply them here to this algorithm
and we get a variant that works online
okay let's get to some results so to
benchmark these algorithms what we do is
we take a quadratic model so since these
these algorithms are basically designed
for using a quadratic model of the
object if we first evaluate them on an
actual quadratic however we make the
quadratic stochastic and we make it ill
conditioned and so the first type of
quadratic is what I call realizable and
the objective function here looks like
like the following here it's theta minus
theta star transpose J xx transpose a
transpose the demented theta star where
X is a matrix of data so this is your
data that you're sampling and the
individual entries are just a normal
district or malicious tributed iid
random variables so this is basically
pure noise that's coming in here the
noise is then colored by a Jacobian j
and this is just a constant matrix and
this is how we get ill conditioning in
because we pick a Jacobian which is
essentially we modify a Hilbert matrix
to make it semi sparse and give it some
nasty properties and we'll we'll have
here about a condition number of 5,000
in a five-dimensional
them so it's quite ill conditioned just
to emphasize the difference between
different algorithms yeah that's that's
really it and then finally note here
that your X matrix is basically comes
from our to the N times B where n is the
size of your parameter vector and B is
the mini batch size so it's literally
how many data vectors are you patching
together before you run an optimization
step so if B is one you're fully online
because for every new data vector that
comes in you run an optimization step in
the limit as B goes to infinity you are
approaching the batch algorithm the
conventional optimization algorithm that
wants to see all the data before it does
an optimization step but you can do
anything in between right so you can set
up your system depending on the data
rates your computing power to say every
five data vectors that come in I do an
optimization step and turns out these
mini mini batches of data is actually
one of these successful techniques from
the neural network era these are often
the most efficient setups so what I'll
plot here is this summarizes a whole
host of experiments so what you'll see
here is along the bottom on a log axis
the mini batch size and this goes from
one which is fully online here and going
in powers of 10 up to 10 to the five so
up to the right here you are in the
batch regime and as you go to the left
you're more and more online until here
you're fully online and on the y-axis
you have also on a log scale the number
of data points needed to converge to a
given criterion which in this case was I
think reaching 10 to the minus 15 in
function value and this goes from
hundred thousand ten thousand up to 10
million on a log scale and this is
literally how many data vectors do you
need to see before you reach that point
so this allows it to compare all these
algorithms
with each other now let's start with the
bad ones so the gray one here is first
order stochastic gradient descent with
the hand-tuned optimal step size so this
is this is how well you can do the
problem is very ill condition so this
algorithm looks quite bad incidentally
we are all topping out here as an editor
all these curves go flat out there
because that's just when we break off
the simulation so even if it doesn't
converge after this many iterations we
just give up and we call it that value
so if we'd really plotted these these
things here would keep going up but it
would take an infinite amount of time to
compute what the curve actually looks
like so this is basically very bad
behavior up there and we don't care
anymore what happens up there now let's
take the next algorithm natural gradient
you can see why people are excited about
it it does a lot better than first-order
conjugate gradients so that's the green
one here and so we're getting down fully
online it needs about between a thousand
and two thousand data points to converge
which is very fast for condition number
of 5000 and then as you go more and more
to the bachelor regime it becomes more
and more inefficient now the blue dashed
line here is batch bfgs so this is
unmodified bfgs in particular what we
did here is we did not take this
gradient difference in the right way we
just took it in the naive way but as
other people have done before and what
you see is you get very good performance
here in the batch regime but for batch
sizes below 1000 it starts to diverge
and that's just because the sampling
noise becomes too much with our
modification we have online bfgs that's
the red curve and the good behavior goes
goes right through until at fully online
we join up with with natural gradient at
the same performance I believe that this
is actually represents the optimal
performance achievable on this
now for limited memory bfgs we have two
curves this is with a relatively small
buffer size of four and what you see is
that as you go more and more online the
buffer is just too short to do to get a
good curvature estimate if we increase
the buffer size to M equals ten we're
essentially the same performance as full
bfgs now this problem in some ways is
hard but in others it's still a
artificially simple problem and that's
because it's a realizable problem
realizable means that if you just find
an optimum on every data sample the
intersection of all these Optima on the
individual samples is the global optimum
at and in other words there's no the
data doesn't contradict each other
there's not a situation where one data
item says the optimum is over here the
other one says it's over there they all
point towards the global optimum so it's
very easy to find a much more realistic
scenario is non realizable problems
where basically there's contradiction in
the data and the the global optimum is
the best compromise location where
everybody is moderately happy but no
individual no individual sample sample
are you actually having zero loss and so
we have an unrealizable model it's
basically the same as the one on the
left but you add additional noise in a
particular place you add another source
of gaussian noise in a particular place
and you can tune how much and that makes
the problem far more difficult and what
we get now interestingly is that natural
gradient is now as bad as first order
gradient descent and i have a reason why
this this is and it's connected with the
fact that at the optimum since nobody's
really happy on the individual samples
you get non-zero gradients and if you
take the covariance matrix of that it
has quite large eigenvalues
the inverse has small eigenvalues and so
the algorithm kind of freezes up because
it gets contradictory information
entering pressingly when you talk to
natural gradient guys they say it's a
feature they say oh that's the purpose
of natural gradient when you get
contradictory information you're
conservative if you try you don't react
to it but interesting you know we react
to this contradictory information and we
do just fine right we can actually
optimize far far faster by not by
basically subtracting out the sampling
noise and then saying I don't care how
large the sampling noise is how much
contradiction there is in the data I
trust my averaging over time to do the
job for me and for the optimization all
i care about is the curvature of the
objective function not the sampling
process of the data so i come more and
more to the conclusion that there's a
fundamental sort of difference in
opinion coming up here and i'm wondering
to what extent i should actually go at
some point and stand up and say natural
gradient is the wrong thing to do i'm
doing this now in a small audience but I
wouldn't do that yet at a conference but
maybe next year you know I need to run
some more experiments and firm from this
whole story up but it looks to me like
it's really there's a really fundamental
difference and we have the potential to
be doing much much better than natural
gradient so another exciting thing will
tries will go into areas where natural
gradient is used with our algorithm and
we'll see if we can you know beat the
pants off them and if so then we can
brag about it right so here the briefly
the story is both for for a limited
memory with large buffer size and the
BFGS we're doing just great now finally
let's look at a real example some some
more realistic problem this is training
a conditional random field with online
limited memory bfgs this is on the
chrono 2000 texts giant chunking pass
this is a natural language processing
problem
and the the thing to keep in mind here
is you have half a million parameters it
is very richly parameterised and because
of that half a million you could never
use full bfgs or natural gradient
because it would be order half a million
squared per iteration so here we use
limited memory bfgs I believe with M
equals ten and some other algorithms of
my stochastic mater descent algorithm is
in their first order stochastic gradient
descent and the green dashed line is
batch LOL bfgs which was the state of
the art of training before we moved in
into that and you can see here this is
passes through the data here is one pass
here's ten here's a hundred all the
stochastic gradient methods are far far
faster than the batch methods and this
is here still on a fairly small data set
I think it's about 9,000 sentences of
English but the stochastic approximation
gives you such a huge advantage by
running the optimizer for every sentence
you get in you're already doing
optimization so by the time this lb-ft s
you know this is the L bfgs is actually
doing very few iterations here it needs
about 30 iterations to do well but by
the time it's done these 30 iterations
we already done so here's a zoom in on
the final area here so we're all of
stochastic algorithms are much faster
the stochastic first-order gradient
algorithm has the worst asymptotic
performance and then SMB does better
than that and with the online
quasi-newton we're doing better than SMD
asymptotically though initially SMD is
faster and that has to do with the fact
that SMD gives you an optimized step
size whereas here we don't really know
yet how to do step size optimization or
adaptation well so what I have here is a
great way to compute quasi-newton
direction search directions online but
the future work will focus on the step
size and the other tuning parameters
that we have
we need to get that working online I've
only have time to talk about the Ark HS
stuff but basically just trust me we can
do this in a reproducing kernel Hilbert
space if you're interested in a
particularly email me I can send your
draft paper it submitted to k DD and let
me just skip to the conclusions so as
I've said we we can do the directions I
think I consider that a solved problem
now the all the implications of that
will of course be exciting to explore
I've talked a bit about the relationship
to natural gradient there's another
bunch of question interesting questions
they're on our to-do list is analytical
proofs of convergence for this algorithm
and we are quite close so we have a
proof of the fact of convergence for
very closely related algorithm to the
one we're actually using we don't have
convergence rates yet but we think we
know how to get them and so we will
probably come up with a revised
algorithm and a convergence proof in
conjunction I would say that's within a
few months we should have that what's
more difficult is handling all these
tuning parameters that we had to
introduce because we had to throw out
the line search and here we'll have to
come up with some ways I've mentioned
that maybe some of them can be turned
into regularization parameters for the
step size maybe we can do some adaptive
mechanism because I've developed
adaptive step size mechanisms before so
I kind of know how to how to do that so
I'll try that in this context obviously
in the end what we'd really like to have
is sort of a black box algorithm because
that's what made batch quasi-newton
methods batch bfgs and lb-ft are so
successful there were no tuning
parameters you just throw your function
edit and it works I don't know if we get
to that kind of level of rope
fastness with this algorithm but we'll
try in particular this is true for non
convex optimization problems so right
now we are only using this algorithm for
convex problems and the reason is in
that we don't have a good way to handle
these tuning parameters for non-convex
problems say neural networks for
instance multi-layer perceptrons you
really want a good adaptive way to
handle these parameters and we don't
have that yet and finally yeah the
colonel l bfgs bear version needs to be
explored in more detail okay thank you
very much if there's further questions
I'll be happy to answer them I'm around
also today and I think for lunch there's
a there's time and so on so do come up
and mix and mingle thank you yes ah yes
so so the question was that in
stochastic approximation often they're
two schedules it's basically one one
step two to estimate the change in
gradient and another is the actual step
that you're taking and yeah we are not
making that distinction at this point so
this could come in at some point right
now where we've been sort of refraining
from that because it's we feel it's
something I can always be added for
instance I mean there's polyak averaging
which does that right it's one form of
doing doing this so there you your
optimization algorithm runs on an
actually rafa trajectory and and at
every point the solution that you're
actually using is the average of all
previous solutions and that's sort of
one way to get these different step
sizes so we haven't thought much about
that yet this is maybe on should be on
the on the to-do list it could be
interesting yeah
yeah yes excuse me oh oh it depends on
the algorithm right so for bfgs you need
a fairly small dimension but you know
whatever you're comfortable with on
order n-squared update whatever end
you're comfortable with and it depends
so I could imagine end up to a thousand
to be quite reasonable unless you have
some sort of high real time requirements
but for larger dimensionalities what
people just uses el bfgs and that goes
up to I guess it's limited in the end by
your RAM right because you need to
somehow process your data but so we've
used 500,000 on this on a CRF problem
and it's no particular challenge so you
can certainly go up into the millions if
not more</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>