<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>ZuriHac 2015 - Discrimination is Wrong: Improving Productivity | Coder Coacher - Coaching Coders</title><meta content="ZuriHac 2015 - Discrimination is Wrong: Improving Productivity - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>ZuriHac 2015 - Discrimination is Wrong: Improving Productivity</b></h2><h5 class="post__date">2015-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cB8DapKQz-I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">
MALE SPEAKER:
Hello folks, again.
So, now we have our second big
presentation by Edward Kmett.
I don't think anyone-- Edward
needs to be introduced.
So let's keep it
short, take it away.
EDWARD KMETT: Thank you.

So, I was at YOW!
Lambda Jam last year
and Declan Conlon
gave a talk on Fritz Henglein's
work on discrimination.
And I skipped the talk
and didn't notice it
until I went back down to Sydney
and he was giving a talk right
before me.
And I said, oh, I should
go watch his old talk.
So I went and lined through the
papers and found the old talk
and I said oh, there's
this wonderful thing
I can do to build a
library in Haskell
that hasn't been written yet.
And so now I have
a new library and I
wanted to talk a bit about it
and play with some people's
preconceptions about
how long it takes
for doing a number of
operations like sort and nub
and these things in Haskell.
So, the name here is a bit of
a lie, a bit of a distraction.
What discrimination here is
actually a technical term, not
the sort of social
good term that we
were-- some people might have
been misled into thinking
I was going to talk about.
The idea of discrimination
is that what I want to do
is, I want to provide a
generalized form of sorting
and partitioning that we
can define generically.
We're going to use this,
more or less, as an exercise
in optimizing Haskell
code again, so kind of
in the same vein as
Declan's talk the other day
using somewhat different tools.
And how many people here
are familiar with the notion
of a Radix or a Bucket Sort?
OK good.
So about 50%, 60%.
So the idea of a
Radix Sort is that--
well unlike what you've-- what
many people have taken away
but not quite what was said
during your algorithm classes
and the like, sorting doesn't
necessarily have to take n log
n time.
Sorting based on
pair-wise comparisons
has to take n log n time.
But if you had something where
you wanted to sort integers
and you had one
bucket per integer,
you could go through and take
each one of your integers,
put them into the
appropriate bucket
just count-- bump the
counter or whatever
you're going to put there--
consign to a link list
that you put into that bucket.
And then you could run through
all the buckets in order
and spit out your answers.
And this doesn't
care how many numbers
you have just the number
of distinct-- it just
cares about the space of
equivalence classes of integers
that you have.
The number of distinct
buckets that you have.
The buckets kind of take
over for that log n factor
that we had before.
Now the notion of
a Radix Sort is
what you do if you
wanted to sort integers.
You would look at the
least significant byte
of your integer and you would
break things in the buckets
by that.
And then you gather
those up, break
by the next most
significant byte.
Gather that up.
Break.
Gather it up.
Break.
And by the time
you're done you have
sorted by your integers stably
keeping all of the properties
that we want but we've
now done a sort in time.
That's the notion
of a Radix Sort,
is that you kind of work your
way bottom up through something
kind of a fixed shape.

Back-- is it 1990 or so I
think Burke, Miguel Ryan Burke,
did a paper on how to
make Radix Sort fast.
And what they built was
sort of top down Radix Sort
where normal Radix
Sort goes bottom up.
They came up with something
called an American Flag Sort.
And the American
Flag Sort is-- what
we're going to do is we're going
to break things into buckets
and then we're going to zoom
in on one of the buckets
and continue to sort using
the same kind of techniques
until we're done
with the [INAUDIBLE].
And so the nice thing about
this sort of top down approach
is that it can stop work
the moment you get down
to a bucket of size one.
Right, you don't have
to keep walking down.
Whereas with the
normal Radix Sort
you have to do one bin per level
and have to pay for everything.
And the nice thing about that
sort of American Flag technique
is that we're going to show that
we can now start applying it
to things like ADT's.

So this is a high level, or
actually fairly low level,
overview of what my goal is to
make a nice efficient library
for doing sorting and
partitioning and grouping
and all these things,
for pretty much anything
you want in Haskell,
just by breaking
that assumption that we have
to do everything pair-wise,
all right.
So, let's try and do
everything in linear time,
sorting, partitioning joining
tables, constructing maps,
and sets.
Everything that we've kind
of conditioned ourselves as n
log n or worse.

And the ideas here come
from a guy Fritz Henglein.
He's the director of the
HIPERFIT Research Center
over at Copenhagen.
He's a really nice guy.
And he's written a bunch
of really nice papers
on this topic and I haven't
heard too much about it
by anybody except for him.
So he gave a couple of
wonderful talks on the topic.
And then it kind of went quiet.
So I'm going to try and build
a nice API on top of this
and whenever you start
talking about data structures
I think the first thing that
goes through everybody's mind
is that monads are monoids in
the category of endofunctors.
What's the problem?
So I'm going to take
a very long digression
into the very definition
of a monoidal category
and where monoids or
where monads come from.
And we're going to see if
we can use this to build
a nice API for sorting.

So, what is the problem?
We have this definition
of a monoid in Haskell.
And it's not the most
general definition
that we can give for a monoid.

What I really want is a
place to sort of define
my monoid so I need this thing
called a monoidal category.
And we're going to wind up with
this kind of crazy construction
where we have some kind of
bifunctor on our category.
How many people here
know what a category is?
Let's start there.
OK, so most-- I'm
sorry to everybody
in the newcomer section
I'll try to make it swift.

So a category is
basically anything
where you have objects
and arrows that
go between objects,
such that you
can compose arrows heads to
tail and at that point in time.
If that's all you've got you're
going to get lost, I'm sorry.
So a monoidal
category is a category
where we've added a
bifunctor, something
that acts like product or
coproduct, so comma or either.
And we have some object that's
with respect to this bifunctor,
a unit, but basically
it adds or removes
no information to put it
on the left or the right
using my tensor.
So A tensor I is isomorphic to
I and I can go back and forth
between these two things.
And I have the ability
to reassociate things
in my bifunctor.
Now, if you squint
at it, there's
something kind of
monoid-like going on here.
So, monoid just gives us an
associative operation and unit
loss.

And then we get these sort
of crazy diagrams, which
are being clipped off
nope-- on that screen
it's being clipped off.
Good.

Look at one of the
monitors that does have it
if you got clipped-- all right.
And these side
conditions are basically
once you try to categorify
the notion of a monoid you
need some extra conditions.
So this basically
says that I need
to make sure that if I
associate all the way to here,
I get the same answers in
either way that I can get there.
And then the other condition
is just that lambda,
rho do kind of the same thing.
The names here, rho
is for R for right,
it's putting out and taking
out the unit on the right.
Lambda is for L for left.
It's not my naming.

Mathematicians.
OK, so the first definition of
a monoidal category that we have
is this notion of the category
of Haskell data types,
where in our category,
our objects are our types
and our arrows are functions
that go from this type
to that type.
And so we can
compose two arrows as
long as the head and the
tail of the arrows that we're
composing agree, which are
just composition of functions.

And here what we
need is the ability
to-- we need some
kind of tensor,
some kind of thing that we
can glue together values with,
or types with.
And here I'm going to choose
the tensor to be product.
And the unit for
product is unit.
Not terribly surprising there.
So rho can put on and
take off something
from the right hand side.
We can just use first, reject
out A leaving the unit behind.
There was no
information in the unit
we can hide a bottom
in it, because we're
in Haskell, whatever.
And we can just put a unit back
if we want to go backwards so
we can go in either direction.
With rho we can go in either
direction on the other side
and it's not a terribly
complicated exercise
for the reader to write
a function with this type
signature.
You just cover up
the [INAUDIBLE].
And that is the body as well.

And then products.
We can also have coproducts,
I've used plus for space,
it's either, in
normal vocabulary.
And so we can say that
the inverse of this
is to put left on,
but then what we need
is we need something
such as either A
or that is isomorphic
to A. And so we'll
use an uninhabited data type
which we have called void which
moved in to base I think in
710, we just did it so, yay.
It was previously available
in the void package.

And this is just
a unit for either.
If we pretend that bottoms
don't exist in these things.
And so we can reassociate
either as well.

And I'm looking for another
example of a monoidal category.
Which is, we can go
build a category where
our objects, our functors
and then our mappings
are these things called
natural transformations where
a natural transformation
says basically
for all the possible arguments
you pass this functor F of I,
I can give you a or-- A of
I, I can give you a B of I.
It does it in a fashion
that we call natural.
Which is related to the
notion of parametricity
that we have in Haskell but
isn't quite the same thing.
Parametricity is a
slightly stronger condition
but we let it play naturally.
And so we have this
category, hask to the hask.
This is functors
from hask to hask
where the tensor that we can
actually go through and build
the product of two
functors and we
can build the coproduct
of two functors.
And we can build those
with tensors as well
and they have units
and we can proceed.
But another tensor that
becomes available to us
in this category is the
notion of functor composition.
Compose F G A is F
of G of A And then
the identity functor is
actually unit for composition.
We can put it on and take
it off on either side
of functor composition.
And we gain and
lose no information.

And we can associate
composition however we
want and we can go from there.

So now I can finally give
the definition of a monoid,
to a category theorist.
So a monoid object is
just like the notion
of a monoid that we know and
love except previously what
we would do is we would
say we have mappend
or mempty and mempty was just
M, right it was just an object.
But we really need this
to be an arrow that
comes from the identity
for our monoidal category.
So when our monoidal category
is the Haskell data types
with product and unit
as our definition,
then this is a function
from unit to M.
And we just throw away the
unit from things in Haskell
so that's why we get mempty.
On the other hand,
if this was product
this would be-- I take
the product of M and M
and I'll give you an
M. Now we're in Haskell
so what do we do we
curry that just because.
So we don't-- you've got to
squint at the definitions that
we use in Haskell to see what
a category theorist would call
a monoid.

And then we get
these unit laws that
say we can create an M out
of whole cloth on this side
and go from here and there.
And this is basically
the unit laws
that you're used
to with a monoid.
And this is a fancy way of
writing the associativity law
that you're used
to with a monoid.
And in the end all is good.
So again in Haskell
we're basically
expecting to write so
our-- even in our mu
is that eta here is-- we're
going to take unit and give you
an update and mu
is going to-- we're
going to have to uncurry with
that because we curried it.
OK so now let me tell
you what a monad is.
A monad is a monoid object
in the monoidal category
of functors from hask to hask
with functor compositions
as my tensor.
Using identity as the
identity for composition,
where we have a natural
transformation from identity
to M, a natural transformation
here is identity of A
goes to M of A for all A. So
we usually just drop the word
identity entirely then you see
A to M of A, that's return.
And we have mu here
says that there's
a natural transformation from
the composition of M and M
to M. So this is M of
M of A goes to M of A.
And so that's joined.
So we've now made more
rigorous at least,
the notion of monads or
monoids in the category
of endofunctors.

So let's go further.

Here's this lovely
thing I'm going
to call day convolution which
has to do with applicatives.
So if monads or monoids in the
category of endofunctors, what
the heck is an applicative
because it also
lives in the category
of endofunctors,
kind of has an associative
law, and it kind of has a unit.
We have pure and we
have this app thing.

So I can make a
data type day here
instead of having compose
F G A FF I have day F G A
and this looks like I kind
of took the signature of app
from applicative and
threw it in a blender
and changed out the functors
and changed out the result type
a little bit.
And we can make a datatype
that has this type.
And then the monoid objects,
with respect to this,
are our applicatives because app
is effectively the thing that
lets us take day F F to F.
AUDIENCE: So what's
the identity?
EDWARD KMETT: The identity here
is the identify functor again.

Technically, it winds up
being the morphisms from unit
so it's unit to A,
functions from unit to A.
Would be unit two,
functions from unit.
Which will matter
in a minute when
I turn this whole thing around.

So here are eta and our
mu are basically this
given the applicative laws.

So applicatives are monoids in
the category of endofunctors.
What's the problem?
The only difference is, we
chose a different tensor.
We swapped one word in
this original definition
and we got applicatives.

Now the definition that I gave
in terms of day convolution--
what we did was-- I'm going
to rearrange it a little bit.
In here I want to
say, this is comma
like A comma B goes
to C F of A, G of B.
I want to write this basically
in terms of liftA2 instead
of app we could have written
it-- we have defined all
of applicative with
liftA2 instead of app
as the sort of
fundamental operation
that glues together
our applicatives.
There's other things that we
could have chosen as our way
to deal with products.
So here if we uncurry
liftA2 I have a tensor here,
which in this case was product.
And I'm going to
products out here.
But there's nothing here
that was fundamentally
about products.
This could've worked with
either A B goes to C.
But also if we were--
if we're working
with contravariant
functors where
if you had a function from A to
B I can take F of B to F of A,
like where I have something
like a predicate is
a function from A
to bool, now you
can map backwards with
a function from B to A
and you'll get a
predicate of these.

I can start looking for another
notion of day convolution.

And so one of the
things that when I first
started doing all these
comonad co- et cetera
machinery in Haskell,
people started
asking me well is there a
contravariant version of blank?
Or what's a cofunctor?
Is a common buzzer.
Right, so if you take
a functor and you
turn all the arrows
around you get a functor,
it's nothing new.
Some people started
using the term cofunctors
to talk about
contravariant functors
but it's something else.

And so then they can ask me well
is there a contravariant monad
and the answer's no you get
this kind of weird twist
and it gets in your way.
Is there a contravariant comonad
and the answer again is no.
But this last summer
I was at Boston,
working with James
Deacon, we sat down
and realized that there
is a contravariant notion
of applicative.

And so we get this.
If we kind of squint at this
day convolution thing and we try
to do the same thing that
we did before with app,
you can say that if you break
C up into or-- if you can break
A-- let's do this innovation
I am unfortunately awfully
in between those two things-- if
you can break A up into cases B
and C, if you can break
A up into two parts,
and you can handle B's and you
can handle C's then you can
handle A's.
That's what this
wants to let you do.

And then there's
a default handler
that you can basically
supply that works for all A.
And this thing has to be a unit
for divide just like pure winds
up as kind of a
funny unit for app.

And here what we
did was we chose
that-- we took the sort
of general definition
that was in terms of the
tensor in the category.
It came from the tensor I'm
going to, we chose both of them
to be products.

And I can give you some laws.
So, they're a
little bit ugly, not
as bad as you're going to get.
So if you can give
me a function I
can kind of give you
the diagonal function
that just kind of-- if you give
me a value I can just give you
the diagonal function that sort
of replicates that value twice.
And then if we divide
with the diagonal function
and conquer one of the sides
it should be the same as
if we just use the other thing.
So this has to act like
a unit for divide delta.
And then there's
an associative law
that let's me shuffle
divide delta's around.
So that's the pretty
version of all this.
And then we get this
horrible monstrosity
if you actually try to state
them in their full generality
where this could
be any function.
So it gets uglier
but the spirit of it
is the same as saying that
pure acts like a unit for app.

But there was
nothing that really
forced me to take
the definitions when
I was going from one
monoidal category
to another monoidal category.
Nothing forced me to start with
products and end with products.

I could move another
class which acts
kind of like a contravariant
form of alternative.
And here what we're going to
say is that if we can split A
into either a B or a C, and I
handle B's and I can C's, then
I can handle A's.
And if A is
completely impossible
then I can handle it.

So now what we did was,
you do the same thing
we did before with
day convolution
earlier but we did it
with-- day covolutionists
is something that comes up
rather deeply in category
theory, and it's
usually defined in terms
of this contravariant
form of thing
rather than the covariant thing.
So what happened was we
squinted at applicative
and we said, oh that's
day convolution.
And then we said,
but it's weird let's
go right down with what the real
category theorists would say
and then we got
this other thing.
And so we found this thing, we
just kind of went out there.
It was just like a
little piece of lost art
and we started playing with it.
And we were going to borrow
like some distributive laws,
kind of like there should
be distributive laws
for applicative and alternative.
They're not really.
No one-- you can't get anybody
to agree what they are.
Different alternatives
satisfy different laws
with respect to
different applicatives.
It's great fun.
But I don't think I can
introduce to anybody
in actually fixing the
problem, so I'm not
fighting that battle right now.

So now we get this a
little class hierarchy.
Which is a little bit like
saying we have functor,
applicative, and alternative.
But we've turned the arrows
around so that given a function
from A to B, I can go
from F of B to F of A.
If I can break A into
cases that I can handle,
and do cases that I can handle
into two things that I need
to handle, I can handle both
of them together and handle
my A's.
If I can break A into two
cases, I can handle it.
I can handle it.
And if A is
impossible, I'm good.

So lose is kind of funny,
there's a sort of A to avoid.
And if you think
about alternative,
alternative doesn't have an
argument to the less than
the M+ equivalent.
So why does lose have
an argument and conquer
doesn't, but applicative
takes an argument,
a pure-- everything
seems weird it
doesn't seem to be quite
lineup, intuitively.

So why is there this
argument to lose?
And I want to take apart pure
and empty and conquer and lose,
and put them in a
symmetric form and then see
why we can get rid of
some of the functions
and not all of them.

So with pure, if we
actually wrote down
what the identity was
for day convolution,
it would be functions
from unit to A.
So this is the identity
of day here and s--
but functions from unit to A are
boring they might as well just
be A in Haskell or not in NL.
So this is A to
F of A that's why
pure has the arguments goes
to F of A. On the other hand,
empty here winds up
with- we're working
with respect to either-- and
so it's the unit for either.
It was void, we
established that earlier.
So just where before we
went from the unit to A
here we're going to from
the unit to A again.
And here our unit
is void but there's
only one arrow from void to A.
It's this sort of absurd thing
that's says ex falso quodlibet,
from falsehood whatever
it pleases, you can
do whatever you want
if you can prove falsehood.
OK so this is trivial.
This is just absurd.
So we can just give
you F of A. And that's
why empty doesn't take any
arguments in alternative.
Yes?
AUDIENCE: Does it have anything
to do with laziness because--
EDWARD KMETT: The only thing
that had to do with laziness
is the fact that I can
get rid of this unit to A,
I can just go to A.
Everything else here
is sort of fundamental
in a nice way.
Conquer here, says that if you
can give me a function from A
to unit, I can
give you an F of A.
But unit has this
property that it's
the terminal object in Haskell.
And so these arrows are
completely determined.
They're all basically
constant unit
if we ignore people
playing game with C
and doing other horrible things.
So ignoring C this is always--
got no information in it
so this is just F of A.
So in these two cases
there's no information here.
And in this case I
got to simplify it
a little bit by using
laziness but in this case
I don't get to
simplify anything.
A to void says
basically has to be--
you have to tell me
that A is impossible.
That's your job.

So that's why lose
has this argument
and why all of these
things that look
a little bit
different to us if we
use category theory as a little
bit of Vaseline for the lens.
They clear up and
look very very nice.
All right, so now let's
look at a couple instances.
By the way I'm warning you
I'm going to run really long.
AUDIENCE: That's fine.
We have time til 8.
EDWARD KMETT: Yeah, I figured.
As long as you bolt
the doors we're good.
OK so here we can have
this notion of a predicate,
this was the example I
tried it out a little bit.
If you have a function
from A to bool
I can map backwards
along its inputs.
And so given a function from
A to B I can take a predicate
on B's to a predicate on A's.
And then-- somebody's already
running for the doors just
in case I was [INAUDIBLE]-- now
what we need is that if you get
you can split A it into a B and
a C and you have a predicate
on B and a predicate on C and
you just N together the results
we will-- this will be my action
for divide and the unit for N
is true.
So I want the predicate
just always true.
And it doesn't matter if I try
to conquer one half of-- if I
split my A into
two A's and I say
OK true N whatever I
was going to get before,
because it's a unit.
And on the other hand,
if A can't happen here's
a predicate for you.
And if I can split
A, if A is either
B or C I've got a predicate that
can handle B's and a predicate
that can handle
C's, I can handle A.
So this gives me
sort of a vocabulary
for talking about
a bunch of things
I might want to talk about
in terms of predicates.
And it's a nice
reusable vocabulary
that's going to work for lots
of different choices of monoid.
So why don't we-- nothing here
was peculiar about N and true,
why don't I just do this
in general for all monoids?
So there's a
contravariant functor, op,
which is basically work over the
input argument to any function.
And its contravariant so
we can map over the input
just like we did
with the predicates.
Nothing was fancy about bool,
we can just replace it with R.
And given any monoid,
this is divisible.
So we're just going to
impend our two results
and we're going to use
the unit for our monoid.
This is a definition for lose
that's valid for any of these.
Choose says if we
can spit out a monoid
for either one of these
things we can spit it out
for either of them.

OK so now that
handled everything
that's a contravariant
functor from A to something.
But what if we have two
A's or multiple arguments?
So yes?
AUDIENCE: What do you
need the monoid constraint
on the lower left [INAUDIBLE]?
EDWARD KMETT: Because
I have divisible
is a superclass of decidable
just like we have applicative
as a superclass of alternative.
So I need it for this and
this picks it up by proxy.

I'm sorry the
question was what do
I need the monoid
constraint on decidable for?

So equivalents.
All right.
So with predicate
we had A to bool
and with the notion of
an equivalence class what
I want is a function
from A to A to bool
with the usual equivalence class
caveats that it's reflexive,
that it's transitive,
that it's symmetric.
We want these things
to hold, right?
And then now what we're going to
do is we're going to borrow on,
or something like that, to let
us apply f to both arguments
to this function.
And we're going to go
through and say, to do this,
we're going to split
our a into an a prime
and an a double prime.
And we're going to split
our b into a prime b
and a b double prime.
Then we're going to test
both equivalence classes.
We're going to compute
the product of two
equivalence class structures.
So if one of them was trying
to break things up this way,
and one of them was trying
to break things up this way,
we're going to break
them up into points.
And then, if we had sort
of-- if a is impossible,
it's still [INAUDIBLE].
We can use the same trick
to say, just throw it away.
And here, the only
thing that we have to do
is say that, when we go to
choose, if we both choose left,
we use the left hand handler.
If we both choose right, we
use the right hand handler.
And if we disagree,
they don't match.
They're not in the
same equivalence class.
So this is a way to
use that structure
that we just kind of
pulled out, to talk
about equivalence classes.
And there really will
be a point to all this.

Next, let's talk
about orderings.
But you want to give me an
ordering as a data type.
So we have a function
from a to a of ordering,
with all the conditions you'd
expect for this to be a total
ordering on a's.
And we can map
over the arguments,
just like we did before.
Here, we're actually using
the monoid on ordering.
And that one's kind
of funny monoid.
So ordering has three different
values-- less than, equal,
or greater than.
And what the monoid does
is it says if the left hand
argument is equal, then it
takes the right argument.
Otherwise, it takes
the left hand argument.
So it's useful for
computing things
in lexicographical order.
It's a fun little monoid
to work out on paper,
while might want to
zip things, whether you
do something like that.
And what we're
going to do here is
we'll use eq, because
eq is the unit
for that particular monoid.
And the absurd case is
still just as absurd
as it's been all long.
We haven't changed any logic
there-- it's kind of boring.
And then to handle our lefts.
If we both go left, then we
use the left hand comparator.
If the first one goes left,
and the other one goes right,
then the left one is
less than our right,
because it's the same case that
we put on either, and gives us
sort of a lexicographical
ordering to our sums.
If we both go right, we
use the right hand handler,
and otherwise right
is greater than left.
OK, so we were able
to build this up
for this particular
monoid, as well.
So these last two examples
of decidable functors
will be the workhorse that I'm
going to be comparing something
more general to.
Because I want
something that's going
to work a lot like
breaking things up
into equivalence classes.
And I'm going to
want something that's
a lot like doing
comparisons on them.
But we're not going
to work pair-wise,
because everything we just
did was working pair-wise.
So I wanted was this
sort of vocabulary,
for talking about things, for
how to split things apart.
And these are good
working examples
to build up our intuition
for what the heck it means.

So now, we're going to
go a little bit further
down the rabbit hole.
And we're going to go play
with GHC generics for a second.
And if you're not familiar
with GHC generics,
then you can probably
skip this slide,
and just look at the
next one, and say
that there's magic in between.
But what we're
going to do is this.
If you have any abstract
data type in Haskell,
it's made out of, effectively,
sums and products,
and some other bits.
And so what I want to do
is I want to tell you--
if you can tell me how to handle
all the leaves in there, that
are not made out of
sums and products,
I'm going to use a
decidable instance
to handle the sums
and the products,
and the units in there.
So I'm going to
try and supply you
some generic plumbing
for how to consume
anything you want in Haskell.
So here, what
we're doing is when
we have something that
looks like a product,
it takes on this form.
It uses a slightly different
symbol than the comma,
because this is on the functors.
If you give me a unit,
we're going to conquer it.
If you give me a sum, we're
going to break in into sums,
and we're going to choose
our way through it.
And if you give me something
that doesn't exist,
it's not possible.

And so, what the heck
can I do with this thing?
So I put a little wrapper on
top of that, and I call it,
deciding.
And I make a little type
alias for all the constraints
that that balls up.
And so the end result
is this-- if you give me
some kind of proxy that tells
me what the constraints are
I'm going to need at
each one of the leaves,
and if you can-- given that
constraint-- you can give me
one of these contrarian
functors that can handle b's,
I can handle any structure
that is made up of all leafs
that I can handle that way.
And so now, what
we have is-- like,
if I wanted to build a generic
version of compare that worked
over any tree structure
using GHC generics,
and then use that boilerplate
that we built earlier,
for handling comparisons,
then what I can do
is I can say, hey, deciding
that the leafs I'm going
to need an ord instance.
And given that ord instance,
it's possible for me
to call the compare function.
I can wrap that up
in this constructor,
and give you a comparison
on a's if I have ord on a.
So now I can handle the leaves.
This thing deals with the entire
tree, to get me down there.
It deals with lexicographical
ordering, and anything
that looks like products.
And it deals with lefts
come before rights,
and anything that
looks like a sum.
So if you have
multiple constructors,
the earlier constructors come
before the later constructors.
And by the time
we're done, we've
build a thing that does
generic comparisons on whatever
ADT you have, as long
as it's GHC generic,
and all the parts are sortable.
Are ordered.
We can do the same thing
for equivalence classes.
We can do a generic
equality, that says,
given two a's, I can tell you
whether they're equivalent,
as long as the leaves can
be compared for equality.
Because given eq of a, I can
give you the equal function,
and we can just ball that up
and call it an equivalence.
OK, so now we're finally
able to actually talk
about the demand I care about.
Sorry for the 43 minutes
worth of preamble.

It's a little bit of a warm up.
So the topic I'm
interested in talking about
is this notion of stable
ordered discrimination, which
was what Fritz implemented as
sort of the first of two things
in his discrimination paper.
And he built this lovely little
data type that is very simple.
And unfortunately,
it's an instance
of basically none of the
classes that I care about.
So I'm going to switch it to
this more complicated data
type, that is an
instance of everything.
So here's the idea.
What I want you to do is
give me a list of pairs,
of a's and b's, where
be is anything you want,
that you're just
going to pass me.
And what I want to do is I
want to break this thing up
into equivalence classes
by a, sorting them
by whatever criteria
I wanted to put on a.
And I want to give
you back the members
of each equivalence
class, ordered by the way
that I've selected them.
AUDIENCE: Are there
any constraints on a?
EDWARD KMETT: That's up to you.
This is a sort.
It's a data type.
You can make any instance
you want of this.
As long as you
break these things
up into some order, that
just depends on the a.

So you can gather up all
the things that share
whatever property of a you want.

AUDIENCE: This is a
bit like [INAUDIBLE].
Because, I mean a is possibly--
EDWARD KMETT: Yeah, you can
think of this almost like
a Schwartzian transform,
or something, on your b's.
You've got the pair of
the b's and the thing
actually want to sort by.
But you're not necessarily
using the ord instance
on a-- or the equivalent
of the ord instance on a.
It could just be that
you have a thing that
takes all of the b's, and puts
it in one equivalence class.
It just takes this thing, and
maps second over this list.
It gives me a list of b's, it
gives me a singleton list that
consists of that entire
list of all my b's.
That would be a perfectly
admissible sort.
AUDIENCE: So the only
thing we get here
is that we have not too many
class equivalences for a.
EDWARD KMETT: Yeah, the
only thing we care here
is that we don't have too
many class equivalents for a.
What I'm trying to--
AUDIENCE: I mean, we probably
don't want to have a to b
just float, and having--
EDWARD KMETT: I'm
perfectly willing to have
a b float, and discriminate
on every individual float.
We'll deal with that
when we get there.
AUDIENCE: But you
do have properties.
I would expect that you're not
allowed to duplicate or forget
about b's.
EDWARD KMETT: Yes, correct.
We're only allowed to
break these things up
into equivalence classes by
a-- by some criteria on a.
We're not allowed to
forget b's, and we
have to give back these b's,
within each equivalence class,
in the order they
come in in the list.
So this has to be a
stable discriminant.
So there's a number of
these little side conditions
that we're all going to
try and preserve as we go.
OK, so we're going to try and
do this sort of final encoding
of this particular problem.
And so my goal
here is to improve
the code for
discrimination by switching
from this initial solution
to this final solution.
And what we have here is
the contravariant for sort.
a only occurs here in
negative position--
it's only ever an
argument to something.
So we can map
backwards over it, just
like we could with predicate.
It's kind of buried under a
couple of functors or something
like that.
But we can still map
backwards over it.
So you give me any
function from a to b,
and I can take a sort
of b's to a sort of a's.
And then, we have these
divisible and decidable things
that we have to implement.
But if we have that, then I can
make a class sorting for things
that I can sort.
And as long as I decide
sorting at the leaves,
I can sort my a's, that are
made generically out of whatever
the pieces are.
So you give me a
sorting, and this just
tells me that storing is going
to need sorting, the class,
in order to be
able to be called.
And then, just go.
You can write instances without
actually writing any bodies.
So if we have those two
definitions, or the two class
definitions, for
divisible and decidable.
All of these instances
write themselves,
except for the hard ones,
like, I want to deal with int,
and stuff like that.
And then the trick is is showing
that none of these things
destroy the asymptotics of
the resulting algorithm.
And then we can get all of
this stuff in the near time.

And the particular law that
I want to stick on this class
is a little bit stronger
than the ord class.
And it's a little bit gratuitous
that I'm putting it on there.
But what I want is
this-- if you give me
any monotone-increasing--
strictly monotone-increasing
function, f, I really want
the contramap f on sorting
is equal to sorting.
So if you give me a monotone
function, from your inputs
to your outputs, then this
is a sort of requirement
that my sorting is structural.
And that's stronger than ord
ever claims, but I want it,
so I'm claiming it.
You can throw that law away, but
it motivates something later,
that I want this
law for sorting,
and a related law
for something else.
OK, so now, let's
talk about divisible.
Divisible was that we had
divide and conquer, because I
knew applicative names,
and co-app give nobody
any intuition for anything.

So what do we do?
Conquer says, put everything
in that one equivalence class,
that I used as an
example earlier,
that we could just
have the thing that
gives us no information.
And then divide has to
do something tricky--
what we're going to
do is you've given me
an a and some payload worth
of information that you want,
about each one of these things.
I want you to break a
up into a b and a c.
Now, we're going to discriminate
with the first discriminator,
using and b and
c,d as my payload.
And that's going to break
things up into this sort of list
of list of b's.
And then I bind that into
the second discriminator,
and that's going to discriminate
on my c's, recursively.

And that is, as you work
it out, it won't actually
change the asymptotics.
Each one of these
things was linear,
in the number of elements
that it was dealing with.

So I leave that
as a fun exercise
to show that this didn't
actually ruin any asymptotics.
So we can divide these things.
And we can do this
decidable piece, as well.
We can choose and lose.
So lose says that,
if a is impossible,
then just mapped on this list,
and any time you find an a,
you're good.
That whole case was impossible.
We can run this
on an empty list,
and that's perfectly fine.
So it's a little
bit more refined
than the previous cases, where
we were dealing with absurd.
But here if the
list is inhabited
[INAUDIBLE] and something
went really, really wrong.
And on the other hand, to
choose between two sorts,
we have to break ourselves
into our lefts and our rights.
So we're just going to
run through the list
here and then take all
the lefts and run them
through the left discriminator--
take all the rights
and run them through
the right discriminator.
And we're just going
to pin these things.
So we're going to put all the
lefts before all the rights.

And then the last
thing we really
need is a sorting for
integers up to a number 10.
So you give me a thing.
And I'll sort an array.
I'll sort integers that
are most at most n.
And then if I have this as
a building block, which is
go to Computer Science 201.
It's not quite 101
material-- do a radix sort.
I can then go
through and say we're
going to sort more dates
by using 256 buckets.
We're going to sort ord 16s
by using 65,000 buckets.
As we get bigger, it's going
to get a little bit trickier
because each one of these
things-- what was happening
was when I did that bind,
this was effectively
doing that American
flag top down style.
What it was doing
was it was starting
the most significant
thing-- what came first
in my pair-- in order.
It did the most
significant bits basically
and then the least
significant bits
by recursively
breaking them down.

When I go to do deal
with larger integers,
I'm always going to have--
it's probably better for me
to deal with four
things from the bottom
up than it is for me to deal
with lots of little sorts
from the top down.
So instead of doing the
American flag style sort,
actually I want to
locally do a radix sort.
So here to do the Word32
case, what I would do
is I'm going to do
the 65,000 case twice.
But I'm going to break it out
into the least significant bits
and then the next
most significant bits.
I'm going to divide into
the less significant bits--
then the more significant bits.
And then discriminate that way.

But it's better for me to
go through and do something
horrible to build a radix
sort where what I have to do
is work my way up through
from the bottom to the top.
And what we're
going to do here is
we're going to do
exactly two sorts rather
than a lot of little
sorts after one big sort.
Yes?
AUDIENCE: Is the instance
for-- what was it divide?
I think the one with the bind.
Are you forced to
pick the first element
of the first
discriminator or could you
pick the second element first?
EDWARD KMETT: The first element
is the first discriminator?
AUDIENCE: If go down and back
on the slide-- more-- more.
So here you picked B, right?
EDWARD KMETT: Yes.
AUDIENCE: B, C, D.
Could you pick C first?
EDWARD KMETT: I could have.
But I would have
gotten a weird order.
You would have
read the thing that
was different from what ord
was going to try and do.
But it would have been
basically using dual
ordering instead of ordering.
AUDIENCE: But would it be
a legal instance for this?
EDWARD KMETT: It would be--
this thing doesn't care,
if I recall correctly.
You might have to--
I don't know if you'd
have to flip the
either case as well.
AUDIENCE: My question
is do this [INAUDIBLE]
is in your example performance.
I am assuming that this
implementation [INAUDIBLE].
EDWARD KMETT: Well, this
is the only implementation
that we have here.
We don't necessarily
have the ability
to do the radix trick without
changing our asymptotics.
Because we're something
that I had to do in order
to make the radix thing work.
We did the American flag--
we went top down, right?
In order to make the
radix thing work,
I'm going to make it
kind of a wrong answer.
And then I have to fix it up.
So I have to go through
and group these things
up into runs manually where
runs go through-- I've flattened
this whole thing down
into just a list of Bs--
or a list of actual
Nths and Bs where those
are our list of Word32s and Bs.
So I've done this whole thing.
I've sorted it.
But I didn't wind up
with a list of Bs.
I just wound up with a
list of Word32s and Bs.
I left my original
Word intact here.
And then I have to go
through and break them up
into runs by that.
So this adds a little bit of an
annoying step at the end of it.
But as a result, it
wins on the wall clock.
So Duncan would be proud.

And now, with that, we can
go right a couple functions
that people like.
Sort.
So Sort a list of As--
what I want you to do
is run sorting for my As.
We're going to break--
we're going to take each A
and give you.
We're going to
discriminate on A.
And I'm going to
give you back the A
and then concatenate all
those little sublists.

And that is Sort.
And sorting unnaturals
with linear time--
sorting on sums and products
doesn't affect my linearity.
So by the time
we're done, we'll be
linear in the amount
of the ADTs that we
have to inspect in order
to break all of the ties.
So in the size of the input
that I need to look at in order
to discriminate will
actually be linear.

When you sort with, which
is a Schwartzian transform
thing where you give
me a function on what
you want to sort
about and this is sort
is just sorting short with it.

So now let's do
something more fun.
To map.
So map has a From List
method that's NlogN.
So if you had a stable
order discriminator
that's compatible with
your ord instance.
Then what you can do is you
can just sort using that,
take each equivalence clase--
take the last member of it
because that's what
From List will do.
And then use the map from
distinct descending list
combinator, which expects
that we only ever give
distinct elements
and that we give
elements in descending order.
And that's linear
time map construction
off of this machinery.
So if you have any
data lying around
that you need to
build maps out of,
which sometimes
happens in Pascal,
this is kind of a nice
way to go about building
your map because we get
rid of the logN factor.
Yes?
AUDIENCE: But if [INAUDIBLE]
data types, right?
EDWARD KMETT: It works.
AUDIENCE: Yeah, but I mean
would it be constant time?
EDWARD KMETT: It's constant
time in the amount of that data
structure it has to inspect.
AUDIENCE: Right, but I mean--
EDWARD KMETT: If
you think about it,
it's still-- it's a constant
in a different thing.
But it is constant.
I'm sorry.
It's linear in a different
thing, I'm sorry.
AUDIENCE: Yeah, I said constant.
But I meant linear.
EDWARD KMETT: So
the question was
if you have a
recursive data type,
and you have to keep
exploring down it
in order to find
these differences,
is it really linear?
And the answer is that
it's linear in the amount
of the structure
you have to inspect.
Whereas a normal
pairwise comparison
is going to do that
same kind of-- it's
going to have to walk
down those elements
anyways to compare them.
So it's got the same cost.
They just bury it in the
cost of their comparison.
NlogN times that
same cost except now
that you have to pay it
multiple times because you
have to inspect the same
elements in different ways.
So it's actually worse in
the normal comparison case.
So I am actually-- I
actually win there too.
Why not?

So the end result here for this
first phase of the solution
is that now we
have OM time stable
ordered structural
discrimination for any ADT
that we want.
And using the radix sort
instead of American flag
sort on our integers, let
us speed up a little bit
over what Fritz's does.
By using this generic
class framework,
we're now instances
all the things.
You can sit there and
contramap over these things.
You can do whatever you want to
work with your discriminators.
Whereas with the
original implementation
of this that comes
from the paper,
it really wasn't
terribly Haskelly
in of modern Haskell
sense like none of the--
there were no instances.
Everything was a
one-off combinator
for working with this
particular data type.
And you kind of had to
make it up as you go.
You made up your own rules.
But here we can
borrower our intuitions
from what ordering does
and what comparison does--
or what equivalent and
what comparison does.
So next what about
unordered discrimination.
What if I don't actually care
about the order in which I
get my equivalence classes
out-- just what things
are in the equivalence class.

So Fritz has the solution there.
He builds this lovely
little data type.
And we'll go for this
slightly more complicated one.
So this data type
is probably wrong.
And so here's where I'm
actually going to turn this
into more of a call for help.
We're going to go through
the pieces that I know.
What we can do here-- we can
go with what's wrong with this.
And we're going to what I want.
And people are welcome
to reach out and help me.

So why do I care about
unordered discrimination?
Well, we have an eq
instance for ILref of A.
This should be ILref
of A. But there's
no ord instance for ILref
because IL references are just
a place in memory that's
got a pointer in it
that points to something else.
And so we can discriminate--
or we can compare for equality
to IL references to
see if they actually
point to the same
object in memory--
not that they're
pointing out [INAUDIBLE].
But if I changed
one of them, I would
be changing the other ILref
because it's the same ILref.

And I can't put an
ordering on that
because these things
move around in memory.
Now people in
general if they need
to do something like with an
STref or something like that,
they just build their
own little monad
and they make a variable supply,
and they pair up their STref
with an integer.
But now you always risk somebody
going through and saying,
well, give me all
STrefs less than 4,000
or something like that.
It just doesn't-- you get
operations that don't make
sense under the semantics we
want to assign to references.
Like the fact that
pointers are numbers.
But you do all sorts
of weird comparisons
with pointers that
don't really make sense
in a garbage collected setting.
So it'd be nice to have
encapsulation that avoided us
ever saying anything
bad, but let
us discriminate
references in linear time
because the problem is right
now to discriminate references,
I don't have pairwise
comparisons for equality--
not even for ordering.
And given that, the best I
can do is theta n-squared.
I can't do better than n-squared
by any pairwise comparison
means that [INAUDIBLE]
for equality.

So to do grouping for
naturals, what I want to do
is I want to hack
up a radix sort.
And instead of a radix
sort going through
and gathering all
the buckets in order.
What I'm going to do is this--
as I go to compute each--
as I go to put a
thing in a bucket,
if that bucket was
empty, I'm going
to thread the bucket itself--
the number in the bucket--
onto a list of all the
buckets I've seen so far.
And so what I'll get is I'll
get my buckets in the order
that I encounter them.
And then what I can do is I can
twist that whole list around
and then gather up the buckets
in the order I saw them.
And then for at least
the integer case,
I get the fact that
the equivalence classes
aren't going to come out in
any intrinsic order based on A,
but instead on the
order in which I first
encountered the A member
of that equivalence class.

And so making this thing
tricky is a little bit--
or making this one efficient
is a little bit tricky.
So Fritz has a slight
variation on the them.
But by the time we
get done with it,
we wind up having to
do unsafeperformIO--
inside unsafeperformIO in
order to make something
that actually runs efficiently.
So what we're going
to do is I need
to make these slabs of
like 65,000 buckets.
And I could naively go
off and do this STref.
I could make an
STarray-- 65,000 entries.
And all of them
were empty lists.
I'd go mangle say 10 of them or
something like that-- however
many I need for my
particular discriminator--
and then I would go through and
grab all the individual entries
out of each of those
buckets, reverse them,
and spit out my answer.
But now I did all this work
to set up this gigantic array.
And then I waste it
by throwing it away.
Duncan would not be happy.
Sorry-- you're the
bogie man on that.

And on the other
hand, what I can do
is this-- I can maintain
my own free list
of these buffers of this size.
And then what I'm
going to do is I'm
going to try and grab
something off the free list.
And if there's one
there, I'll just take it.
And I'll unsafe
coerce it around.
And everything that's
on the free list always
has empty lists in it.
So it's OK for me to
convert an array of lists
of As to an array of
lists of Bs as long
as everything in the list--
everything in the array is nil.
And then I go through.
And I fold-- I visit
everything content on my list.
And then I take the
list that I have--
reverse all the elements
in each one of them,
spit out my answer, and then I
return my list to the free list
having restored all the
nils that I touched.
So it's kind of horrible.
But it's really fast.
And it works.
So I'll take it.
So without this, we don't
win on the wall clock ever.
With this, we can beat like
a quick sort implementation.
So those are kinds
of-- the motivation
here is that you need
something horrible like this
for the stable,
unordered case in order
to actually start winning.
And then I have this
law that I want,
which is that if you give
me any objective function,
I should be able to
contramap that over grouping
and get grouping.
That should be equivalent.
You shouldn't be able
to observe a difference.
And this is where
the problem comes in.

But why do I want this law?
In many ways I want this
law because with it,
it's possible for me to
say that these things could
be productive.
And that would let me have a
nub that was an online nub,
but now runs in O(n) time
instead of O(n) squared time.
And we have an online nub
that we can write this,
like, O(n) log n time using
data.stack or the like.
But that now needs
an ordering instance.
But what about things I
can't order like IOrefs.
I can nub a list of IOrefs.
But I can't put it in order
and run it through that thing.
And I want to be able to
do like a version of group.
And in group in
Haskell today, it
goes and just only
looks for things
that are next to each
other that are the same
and then gathers all of that
into a group and then moves
on to the next.
But if wanted to be able
to do group at a distance?
We can get a version
of group that
uses this notion of a grouping.

And there's theoretically
a potential for streaming
here because if I wanted to
run grouping on something
like this-- and let's just
say I was grouping them up
by integers-- here I'm going
to say the moment I encounter
this first equivalence
class, I know
that the payload that
we're going to put here
is going to be an A.
And I don't know what
the rest of this list is.
I don't know if I'm done
here if the string will end.
And I don't even know if
I'm going to have any more
equivalence classes.
But up until here, I'm good.
So if I were to look at the
result, I could see that A.
And I'd be making progress.
And the moment I see this next
element that I know this--
I know that this next element
is a list that starts with B.
And I don't know
what's the tail of it.
And I don't have any more lists.
But I at least know that much.
And when I see this next
element, now what I've done
is I can make this thing strict.
I can start forcing this thing.
So it's this weird thing
where it's like laziness.
But what's happening is
we're creating a promise that
is I will tell you what the
tail of this list is eventually
and then fulfilling
it on demand.
So you could build the monad
in which you could write this.
And that's actually one of the
open issues that I have here is
how do I build nice
little ST-like monad where
I can create promises and
fulfill them and give you back
the result where
the promises can now
reference the new promises
that I'm making forward.
And when I look at a
promise, what it will do
is it will run this
entire computation
up until it fulfills that
promise and then puts it back
to sleep.
It basically puts
it back in the box
until somebody else comes
along and says, hey,
can you please run
your computation
further until you get
to another promise.
But with that, then grouping
could be entirely productive,
which means that nub would
be an online operation.
And that's something that
goes above and beyond
what Fritz's papers do.

And the trouble with
that law is that if we
look at the actual
paper that backs this,
then unordered discrimination,
which is defined--
that doesn't use the classes.
It actually builds a
combinator that walks over
that ADT we built before.
So we build an interpreter
for a simpler ADT.

There, some handler goes through
and breaks things up into lefts
and breaks them up into rights
and then appends them in order.
It's not passing my law.
It's doing what we were
doing for the sorting case.
But the problem
is the moment you
do sorting we're dead
in the water when
it comes to productivity.
So I want to be able to spit
out the results of nub on mine
as I go.
But let's say that I wanted
to nub a list of Booleans.
And I had an infinite
list of Trues.

I am not ever going
to be able to admit
that first True because
what's going to have to happen
is the False-- it's
going to try and put all
my Falses before all my Trues.
And I don't know that I
don't have any of them.

So I'll just starve trying
to give you an answer.

So I'm not sufficiently
productive.
The moment I do
any sorting I fail.
Now, for many cases,
we don't actually care.
If I just wanted to do that
grouping thing like that--
and I didn't actually
care about the
ordering that I got my
answers out in the end-- then
I would be perfectly
OK in this instance.
And then also when
we go down to deal
with these products,
what's going to happen
is we're going to get our--
our answers are also going
to get all jumbled together.
So the product case is also run.
So that's why I pointed out the
type for group-- maybe the type
that I gave for
group there is wrong.
And when you needed to do
something more like a stream
fusion thing with
skip nodes where
instead of doing
a blank skip node,
we tell you which
equivalence class
I put a thing into and
then juggle it upstream.
I don't have all
the answers there.
I can point to you as
far as we've gotten
and how far I think
I can go with this
and where we think this
can be interesting.
And there's a fix that at least
gets us the correct ordering
that I want for that law.
We can go through
and what I do is
I can take and zip
with every element
the number that is its ordinal
position in my original input.
Then I can go through, run my
sort of unordered discriminator
using those additional
classes-- those implementations
that we had before that
were a little bit naive.
And then I can
finally go through
and sort using the position
of the first element
of that equivalence
class that I saw
that uses the fact that we're
still stable even if we're
kind of-- we're doing
a little bit of sorting
when we shouldn't be.
And this would give
me a correct answer
but because it had to
do a sort internally,
it can never be productive.

And we can fix the sum
case pretty easily.
We can go through and do
what we just did there
where we tag everything by the
order in which I encountered
it.
Basically, I'm just going
to zip up all of my things
with a list with a number,
which is positioned in the list.
And then I'm going to go through
and do the mix operation that's
going to go through
and try and run
through these lists
of equivalence classes
and then look to see
which one of them
came first in the input.
And it's going to do a
merge of a merge sort.
It's going to merge my set
of equivalence classes.
So we can do this.

And that will fix our sum case.
But it doesn't get
us the products.
The products are
more interesting.
And that's where we start
talking about something weird.
And I am open to
suggestions and patches.
AUDIENCE: So does this
fix you list of Booleans?

EDWARD KMETT: This fixes
my list of Booleans
because what will
happen is we're
going to choose between
lefts and rights.
And it will do the right thing.
So nub will now give
me True and then stuff.

That's right.
Does this fix my list of
Booleans was the question.
So now I want to talk
about something weird.
We went out off into
category theory for a while.
Then we played around with
some algebraic data types.

I want to talk about another
weird corner of optimization
in Pascal, which is
the notion of-- we can
build custom foreign prims to
do things that we want to do.
And let's say I wanted to
do groupings for STrefs.
The problem with
this STrefs before
was I can only prepare them
for equality because they
can move around behind my back.
So I want to build
this custom primitive
that is responsible
for doing-- it's
going to take an ANY
because I don't actually
have any other way to pass
an actuals [INAUDIBLE]
Pascal data type to a primitive.
So a custom foreign
primitive is this-- GSC
itself compiles time
from Pascal-- the surface
language that you know-- through
this thing-- through CORE
to this thing called
The Spinless Tagless
G-machine machine down to
this other thing called C--.
And then from there,
it goes to LOVM.
And it spits out of assembly--
yeah, just a couple of passes.
That C-- thing can
be written by hand.
Simon Marlow has actually
given fairly recently
a fairly nice syntax for working
with writing our own custom
C--.
So in order to discriminate
STrefs which I can only
normally compare for
equality like pairwise,
I'm actually going to
have to get in there
and keep the garbage collector
from moving them around
while I compare them all.
And so in order to
do that, I'm going
to have to first make sure
that my list is entirely
strict because I can't have to
do any allocation because if I
have to do any location,
I can be forced
to do a garbage collection.
So what I do is
the first thing I
do is I run through
my list of STRefs
and I force it all
the way through.
So I know that I
have a list of things
that have already been forced.
Then I call my
little cust-- then
I make sure that I have
a mutable byte array that
is as large as I need to
hold all of the addresses
for all of these STrefs.

Then I call my little custom
prim and get an answer.
And then I freeze
the resulting list
and match the
addresses that I've
scribbled into the
mutable byte array
and into my original list.
And then I discriminate
on addresses,
which are like 64-bit pointers.

But now they're just
64-bit integers.
And I can move things
around after that--
after I've pulled out
all their addresses.
So we can build this horrible
thing which probably doesn't
show up at all on the slides.
But what it's doing is it's
this little C-like dialect where
every allocation
is very explicit.
And instead of where
we would normally
jump into a closure,
what I'm doing is
I'm just trying to figure
out a new direction
and then looping back.
This is done completely
without allocation.
It goes through and walks
that whole structure
and bangs out the addresses
into my mutable byte array
in a tight loop.
Now, this is terrible.

We're stalling the
entire garbage collector
while this thing runs.
Simon Marlow was very,
very cross with me
when I mentioned to him
that I could do this.
But it works.
So it's a thing.
So if you have to
discriminate a lot of STRefs.
And you can't bring yourself
to make that little integer
tag that goes with it, it
is possible to discriminate
STrefs or IOrefs or MVAR-- or
a whole bunch of other things
in linear time.
And using some of the
machinery from the paper,
you can actually
use this for if you
have bags or sets of
these things as well.
So I'm going to leave
that to the actual--
to the paper in the code.

And then I have
one last use case.
I did tell you I was
going to run long.
AUDIENCE: It's fine.
EDWARD KMETT: I have
five more minutes--
10 more minutes-- yeah,
about 10 more minutes-- 15.
Anyways-- so the next
thing is occasionally I
don't actually care if I have
a stable-- if it's ordered
or an unordered discriminator.
Sometimes it's
just get me answers
in some equivalence
classes, right?
So if we really want
some kind of thing
that I can discriminate on
it-- run, sort, or run group--
no matter what it's going to
be, get me out my list of these.
And now I want to talk about
how we can do table joins
because that's-- table joins
are one of those things where
people are used to having
to come up with a scheme
or a schema where they have to
sort their data in the right
way.
And they hope that
they can query it.
And then every
once in awhile, you
have something that
you have to take out
of your relational store and you
have to process it in memory.
And what do you do?
You come up with these
horrible little schemes
where you hatch this
against that or you
sort this against that.
And you try to do your unjoins.

So what I want is a join
that is just linear time.
You give me a
discriminator that you
want to use for the thing
you want to join on.
And I want it to
just be linear time.

So let's do a join.

This I've stolen from Fritz.
And what we're
going to do is we're
going to join on whether or not
our values are even or odd just
to make it interesting.
So it's not actually
joining on the integer.
We're going to join
on the evenness
for the oddness of the integer.

So you can contramap even over
the discriminator for Booleans.
And you'll get a
contramap for even or odd.
Or you'll get a discriminator
for is it even or odd.
So what I want to do this--
you've got two tables.
One is like we've got a list of
keys-- 5, 4, and 7 with values
associated with it.
Here we have another set of
keys with different values
associated with it.
I want to join the
two things together.
I want to join all
evens over here
with all the evens over there.
And all of odds over there
with all the odds over there.
So what we're going to do
is I'm going to inject these
by tagging them with left.
And I'm going to inject these
by tagging them with right.

Now we append these
two lists together.
And we feed them to our
one discriminator on
is it even or odd?
The result will
be-- so we've got
this list here of all the
lefts and then all the rights.
So all the lefts come
before all the rights.
Now, once I've
discriminated, I'm
going to get all the
things that were odd
and then all the things
that were even as two
different equivalence classes.
Now what I have to
do is break this out,
take all the lefts,
and all the rights
within the same
equivalence class
and take their cross product.

We've already broken them
out into equivalence classes.
And now these will be
a full cross product.

Then when we're
done, we have done
in linear time in the
total amount of input
and the total amount of
output we have done a join.

And why I care about that
unordered discrimination case
that I was just
talking about is I
want to be able to start
spitting out results online.
I want to be able to start
giving you the things
that you want to
group out of this
as you're feeding
me inputs still
not as-- not after I get all
the input because right now
in order to do this,
what I have to do
is I have to get
all of the input
then I can discriminate it.
And then I can start
screening results.
You get no answers
until I'm done.
So that's why I care.
And so what did we have here?
We have the fact that we're
using a stable discriminator.
So all the lefts in
the result in each one
of the equivalence
classes individually
are going to come
before all rights.
So I need some
kind of like stand
that's going to take out
the lefts and the rights.
It's going to take a list
of eithers of As and Bs.
And it's going to give me a
list of As and a list of Bs.
These are all the lefts
and then all the rights.
And we can do this in CPS style.
And Duncan will be mad
at me because there's
all sorts of closures
being allocated here.
And it's a little bit slow.

I don't know how to do it
more efficiently though.
And then we can do outer
joins a whole table at a time.
So here what you're
going to do is
you're going to tell
me this-- if you have
any discriminator--
ordered or unordered--
and you know how you want to
merge on the entire table of As
and the entire table of
Bs to give me something--
we'll call it C.
And you have selector
that knows how
to get the column
that you're interested
in on the left and
the column you're
interested in on the right.

Give me a left table
and a right table.
And I'll join it and give
you the final result.
So you might just use app here.
And you'll get the-- you
[INAUDIBLE] comma like that.
And you'll get
the product table.
And you'll get each
one of them grouped.

When you do inner
joins just by handling
some of those cases for you.
So you tell me how you want
to match the two rows together
instead.
And then what I'll do is if
either one of the input list
is empty, then the inner
join there will be empty.
And we'll spit out
the result. We can
do left and right outer joins.
All of those things fit
into this framework.

So, again, the open
problem that I have here
is that I really want this form
of productive, stable ordered
discrimination.
So I really want this--
AUDIENCE: Ordered or unordered?
EDWARD KMETT: Did I say ordered?
Productive stable
unordered discrimination--
because it is going to need a
better version of grouping that
and a better version of divide.
We've done-- or no,
we did-- not divide.
We did decided.
But we didn't do divide.
And then it likely needs
a different encoding.
And there's some sort
of lazy ST-like monad
with IVARs that should exist--
that we can kind of reason
about from what it has to do.
And we can't afford to make so
that each one of these things
is just a thump that forces the
whole list up to a given point
because that would
require us to traverse
the list many different times.
Each one of them is going to
try and zoom along the list.
So it really has to be this
spooky action at a distance
where one of these things is
fulfilling a promise that's
for a portion of the
data structure that's
all square inside
of our environment.
So in conclusion,
only a half hour long,
we have discrimination giving
us O(n) sort, nub, group,
inner, left, outerr-- inner,
left, right, outer joins.
And it worked basically
for any ADT you want.
It works for integers.
It works for pointers
because we can
do this thing with references.
So what else do you really
want to be able to discriminate
on that I can't?
And that's a question that you
can come up to me with later.
And regardless-- of
code is up on my GitHub.
You can go to [INAUDIBLE]
discrimination right now.
As soon as we
figure out some way
to do this productive or
unordered discrimination thing,
I'm happy to put
it up on Hackage.
I may put it up on
Hackage earlier just
get some more eyeballs on it.
And the documentation
you can actually just
go to the GH pages link that's
available on that GitHub
account.
And, again, I really want this
form of productive, unordered
discrimination.
Please help me get there.
Thank you.
[APPLAUSE]

Are there any questions?
AUDIENCE: Does bucket
sort have a bit
of a problem with
very small lists?
That's the constant--
EDWARD KMETT: Does
bucket sort have
a problem with small lists?
Sort of.
The sorting case-- the real
bucket sort does, right?
Because you have to create
and use all those buckets.
So you have to walk
that whole list.
And there's little games you
can play to try and speed it up
by keeping track of is
there anything used inside
of this block and changing,
unfortunately, changes
the asymptotics to log in.
So there's ways to try
and make it faster.
One way to do it is to
use a lot fewer buckets.
If I choose 256 buckets,
my constant factors
go down considerably relative
to the 65,536 buckets
I'm using here.
I have not tuned those
constants in this case.
So the usual answer is you
just use smaller buckets
and you use more passes.
AUDIENCE: Then it won't be
[INAUDIBLE] the linear time has
to be taken [INAUDIBLE].
EDWARD KMETT: No,
let's think about.
What happens is I have to do
eight passes-- each of which
are 256 versus two
passes that are 65,536.
It's a sure-- it's a pure
win when you have a smaller
number of buckets
until you get down
to some grain size is too small.
AUDIENCE: Just to understand
to me it sounds like, well, you
should always use that.
I have a feeling that for
short input sequences-- so
for short amounts of elements,
standard logN-- standard
comparative sorting had
actually [INAUDIBLE].
EDWARD KMETT: And so
the question is can--
or the observation is that
for short input sequences
that standard logN-style
sorting is probably a win.
I don't have good
benchmarks in the small.
So it's more that there's
an interesting point here
in the design space that
scales indefinitely.
And how small it scales
is another question.
It's interesting though that
I haven't found anything
that actually tries to get
down to a small grain size
and then internally do
pairwise comparison sorts.
So I don't have good
comparison literature on that.
AUDIENCE: Another topic
is how does this work
when you go to external memory?

EDWARD KMETT: How does
this work in external?
Well, where would we
run into a problem?
I'm trying to think.
AUDIENCE: Well, it
assumes that you
have low latency and
random access, which
is not really true [INAUDIBLE].
EDWARD KMETT: What am I
doing that's random access?
I'm using random access
for my buckets, right?
But I'm not using random
access for my data.
I'm walking through stream one
after another after another
after another putting
things in the buckets
in small ascending
order and then
giving tighter and
tighter groupings
that I'm [INAUDIBLE] on.
So I actually work pretty
well for all my caches.

Yes?
AUDIENCE: So what
is the difference
between-- in terms of memory
usage between the pairwise sort
and the linear sort?
EDWARD KMETT: So what
is the difference
in memory usage
between a pairwise sort
and the linear sort?
So there's variations on this.
The original American flag
sort was an unstable sort.
What I've got here is
like Pascal quick sort
isn't really quick sort.
This is sort American flag sort
that isn't really American flag
sort, right?
So the original version of this
that was the American flag sort
was an in-place sort.
So in that sense, and there's
in-place quick sorts that do,
of course, get the same bounds.
So it's a matter of are
you fiddling with a stack
or something like that.
So it's not an appreciable
win either way.
Now here what I'm doing doesn't
really lend itself to in place.
So I'm probably wasting
a lot more memory
than a nice in place
quick sort [INAUDIBLE].
But in exchange
for that, I'm also
getting different asymptotics.
AUDIENCE: Yeah, it's a
trade off you lose memory,
but you gain speed.
EDWARD KMETT: Yeah, so we lose
memory, but we gain speed.
Questions over here?
AUDIENCE: Is there any
reason why you would
need grouping for pointers?
Like, basically, you
have to force them all.
So it doesn't matter
if you use sorting.
EDWARD KMETT: Is
there any reason
why I have to use
grouping for pointers?
Grouping is faster
because the trick
that we use there that only
visited the buckets that I
actually touched what wins.
It's sort of the story
behind why you want
to do grouping for pointers.

And the other thing is there's
a deeper mystery there.
I turn them into a
bunch of integers.
But the integers are going
to be different from run
to run at that function.
So the ordering would
be completely random.
And it's not really
a pure function.
If I can share
results and use it,
it's not going to be
the same as I would
get from calling it twice.
So there would be
an abstraction link
that you could see that this
thing is doing something
weird behind your back.
Whereas with the grouping,
even though I could give you
completely different addresses
for all the pointers,
they all wind up in the
same equivalence classes.
And I still put the equivalence
class out in the same order.
So the result of the
function is actually
stable only for unordered
discrimination case.

Yes?
AUDIENCE: So when you
have the data type that's
[INAUDIBLE] A or B or C
or D. And at the level
where you're doing
your sorting, you
want to do this as some
kind of bucket sort.
But then when you write a
generic instance because
in the generics instance,
these alternatives
are expressed as
pairs of class, right?
Don't you-- are you going to
also always working on just two
buckets in the right one?
EDWARD KMETT: So
the question was
if I have a data
constructor like A or B or C
or-- if I a data type with
several constructors like that,
and I wanted to treat
it like a small number,
isn't this effectively a
two-way American flag sort.
And the answer is yes.
We're effectively at
each point in time,
we're forking our way down.
And we're working bitwise.
AUDIENCE: So if
we just did this,
which is a constant
factor depending
on the size of the
alternatives, right?
EDWARD KMETT: Correct.
AUDIENCE: But you could probably
flatten it out in some way,
right?
[INAUDIBLE] first.
And then you could--
EDWARD KMETT: So if
you flatten it out,
then you could then
do a single bucket.
And so you can supply
your own discriminators.
You don't have to use the
ones I get by generics.
So when you define
the instance, what I
gave you was a default
definition for how
you can get a discriminator.
There's nothing
that prevents you
from going through and
saying, hey, look contramap
from [INAUDIBLE].
I am not sure where
it's going to go.
It's contravariant.
Ow!
AUDIENCE: So the best case in
this case would be if we had
an instance that is known to
be a compact in integers .
And then we could
do that really fast.
EDWARD KMETT: Yes, if we had
one instance that was compact,
and we had minbound
and maxbound,
we could shift by the
maxbound-- put that at 0-- take
the difference between them.
As long as it's compact and
well-behaved, we're nice--
all set.
AUDIENCE: If it's flattened
with generics SOP?
You can probably do the
flattening quite nicely.
EDWARD KMETT: Yes,
with generics SOP,
you do the flattening with
the sum of products stuff.

So those options work.
There's plenty of other
generic solutions in the sea.
GH generics is supported
out of the box.
AUDIENCE: So it was two bytes
apart from the [INAUDIBLE]
that are coded in
like [INAUDIBLE].
EDWARD KMETT: Yeah, we're
always splitting pairwise.
AUDIENCE: It's always two, yeah.
EDWARD KMETT: And
there's potentially
some interesting
things that have
to do with trying to do like a
product that acts like your tag
and then sort of do a dependent
step for the next layer
of the system.
So there's some
things that relate
to tagging in the
[INAUDIBLE] thing
that we can talk about offline.
AUDIENCE: Silly question-- why
is it called American flag?
Do you know?
EDWARD KMETT: So
there was a puzzle--
a problem I think Dijkstra
first talked about, which
was the Dutch flag
problem, which
is you're given a bunch of balls
that are colored red, white,
and blue.
And what you have
to do is you want
to put them in the order of
the colors of the Dutch flag.
And I'll get them wrong--
so whatever it is.
And then what you have--
so one way to do this
is that you can run
through all of your buckets
and count up how many
you have of each type
and look at the
prefix sums over that.
And then what your goal is to go
through and grab the first ball
and put in the right bucket.
And that's going
to displace a ball
and put that in the
right bucket and keep
track of the high watermark
as you go through it.
So this was done
with three colors
to begin with-- or
three different bins.
So if you had lots
of different things,
you'd have something that
has more stripes in it
like an American flag.
So that's where the name comes
from as far as I can tell.

AUDIENCE: I'm just curious about
that-- this divide [INAUDIBLE].
Is it something you could use to
create the [INAUDIBLE] generate
by creating products
or stuff like that?
EDWARD KMETT: Is it something
I can use to create a generic--
I didn't follow the last.
AUDIENCE: Do you know
the parallel collection
where you can just go parallel?
EDWARD KMETT: Can I do something
like parallel collections
or something like that?
AUDIENCE: Yeah, exactly.
EDWARD KMETT: So you can use
to consume a lot of things.
There's also-- what's
another good example.
So you can actually build an
example that goes with that.
It's like you can build
a notion of NFData.
NFData itself-- if
you think about it,
is that you give me an A,
I will give you a unit.
That's what happens with RNF.
So you can make a generic RNF
as well using that same thing.
You'd have to make a
contravariant function that
can handle sums and
products and other things.
It's A goes to unit.
And that makes my
statement earlier
that A goes to unit
being trivial-- that
makes that align.
But if you had
that, then you can
something like NFData or
something like that, right?
And then you can
do-- you can perhaps
do [INAUDIBLE]
things inside of that
or break things up into bins.
But for that and for
[INAUDIBLE] and stuff like that,
I have not tried it in that way.

Yes?
AUDIENCE: Once you have
a productive learner
discriminator, do you
think you could wrap it
into a [INAUDIBLE]?
EDWARD KMETT: So once you
have a productive learner
discriminator, do you
think I could wrap it
in an [INAUDIBLE] style design?
Very likely.
The trick is that the
intermediate stops
are going to be weird.
But then that final output
can probably be online.
You [INAUDIBLE] output.
So it can look be the
more modern [INAUDIBLE]
and stuff like that
where's there's not you
give me a string.
You give me continuation
stuff even more.
So [INAUDIBLE] style, yes.

I think that's enough.
Anybody who wants to grab
me offline, feel free.
I'm happy to talk
about the stuff
pretty much incessantly,
as you may have noticed.
So thank you.
[APPLAUSE]
</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>