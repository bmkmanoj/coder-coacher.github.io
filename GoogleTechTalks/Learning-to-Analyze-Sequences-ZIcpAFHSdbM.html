<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning to Analyze Sequences | Coder Coacher - Coaching Coders</title><meta content="Learning to Analyze Sequences - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Learning to Analyze Sequences</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZIcpAFHSdbM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we're very happy to have with us today
mr. Fernando Pereira from you can I
think most you here probably already
know they don't have to say much yes a a
long career at Penn Nadine t whiz-bang
and other places really sort of one of
the pioneers in applying machine
learning to large data related to text
and I know he really influenced me in
thinking along these directions when he
said the older I get the farther down
the Chomsky hierarchy I go and the fact
that you can get a laugh out of that
means if you can't the right place good
all right so okay so this is a top this
talk touches a bunch of things that some
of you I'm sure I know I know some of
you know quite a bit about probably in
some ways more than I do so what what
I'm going to do is to cover it may
basically give you a little bit of what
some of the main things I've been
working on relatively recently some
results relatively recent results on
work I've been also doing over the last
seven years or so my former student Ryan
McDonagh living in New York has done a
lot applied some similar techniques to
parsing that I'm not going to talk about
you know he's a much better person talk
about that but what so also i mean
there's a bunch of related themes that I
don't have slides for but to kind of
come out of this Connor this I this work
or touch on this work which are broader
than just the specific technical work
here so if you have any questions from
time to time I might not cover all the
slides because of that and if you ask
questions which I really would
appreciate you know you can drive me
into other either the directions as well
so with this kind of served think of the
talk or the slides are sort of a prime
somewhere with priming a conversation if
you can do that so kind of for a long
time I've kind of been driven to work
with sort of sequential problems and
whether it is doing a natural language
syntax that's an example actually from a
paper
with Ryan on on doing dependency parsing
for English and again I'm not going to
talk about that much today I or I you
know one task that we've been working on
quite actively actually Ryan also worked
on extensive when it was in my group
which is to do named entity extraction
for biomedical text pubmed abstracts and
i'll actually talk about more a little
bit more about the motivation for that
and sort of the saloon some of the
underlying the applications that
motivate that so the the point there is
you know people have lots of biomedical
text where they like to know what what
is it about what what biological objects
does this text discuss and we've been
trying to help a number of biochemical
biomedical researchers do that as many
other people have been doing and the
final example sequences that so this is
when i went him to pan one of the things
i was multiplied motivated music that
was really getting interested in
application of sorts of various kinds
machine learning models for sequences to
gene prediction and now there was
computational gene prediction was alive
and well and I kind of saw what people
do and say I think I can do better and
well they took four and a half years to
do better in one student was extremely
persistent guy and so it wasn't as easy
as I thought it worthy I thought it
would be six months when I started why
not really six months but a little bit
look maybe yourself but finally I think
we got somewhere so now I know we'll
have a mixed audience in certain sense i
mean some of you guys know a lot of what
the work I've been doing and there's you
know other people here I probably who I
don't know probably dont les so again if
you ask me questions if you if I'm going
to slowly say hey come on move on you
know yeah we know that or if i'm going
to you know if i bring fast just stop me
and again let's try to make it
interactive so i'm going to use for
purposes of exposition i'm going to use
a schematic picture and that doesn't
mean that this picture is exactly what
we're doing it in each application
but generally what I'm looking for is we
have sequences or some kind whether it
be text or speech I've also worked a lot
on speech but I'm not I've not don't
have anything new to say about that um
although my former you know so one of
the students at Penn was a advised by
world saw there's some really nice work
applying the techniques I've discussed
today to speech recognition is now
postdoc at Berkeley fascia so these
wonderful people and knowledge in this
in the slides so we have sequences and
we're going to find try to identify some
structure in those sequences now I write
those little structures a streets
because it makes a kind of illustration
nice but you can imagine that the
structured sequence might be different
kind so actually let me go back in the
exam this slide here here the structures
in the gene case is i have a genomic
sequence of some kind and i'm going to
try to label segments of it as being
playing particular roles so for instance
the gene is organ in a new chaotic gene
is organized in the number of things
different exons which are chunks of a
meaningful DNA sequence that typically
code for protein although not compare
some complications i won't go into that
and then those pieces one two three here
three exons have gaps there's there's
going to be some piece of sequence here
that is not part of this gene which is
called an intron so there's this
organization is very complex and very
you know if Lucien airily its role is
you know subject a lot of discussion
still but the end result is that
whatever machinery in the cell comes
along and say so here's a gene I want to
transcribe and there's and that's gated
by some stuff that's up here that I
don't know anything about but there's
machinery comes along it is a grind
grind grind it's going to go along and
copy this do what's called transcription
which is copy this sequence subsequence
a year and then the some other machine
it comes along and says oh clip that
away click it away it's called splicing
and the end result is that the the
meaningful part of the gene in terms of
protein coding is going to be these
three pieces and that's what the gene
predictor is going to try to find in a
case like this so the analysis in this
case is three segments these three ll
segments separate in the stuff here here
and here is sort of stuff that you kind
of label a different way so here it's
intergenic meaning that is in between
different genes and here is an intron
meaning it's between the two exons of a
given gene so that's when a now one
structure for gene case here the
structure might be labeled these words
as part of a protein or gene name the
stuff in red and the other words there's
other I don't care and in kids like that
the structure is some kind of graph
structure so structures can be vary
depending on your application in the
what I'm going to talk about applies
with some caveats i'll discuss to a
variety of different structural problems
so sequence you have a sequence you're
going to map it to some structure which
I schematized as a tree just for fun
okay so I'm going to talk about two min
uplands one is a very all immediate pick
you old applications in this world of
sequence prediction one is named entity
extraction are so where the purpose here
is I have some mechanism that is going
to identify mentions of entities of a
type of interests a protein names in
text and then hopefully help us link
those the text to other resources that
describe of those entities so and I'll
give you an example of that in a moment
ah gene prediction is the task I
described before again find the proteins
the genes that code for proteins that
determine an organized form function so
these are tasks that people have been
trying to perform from one time and
using a variety of different techniques
for and there's a syntactic analysis
tasks that have also worked on with Ryan
and he is a person to talk about that
and I'm sure some of you have already
seen his work on that so the sort of the
technique that sort of the kind of
standard you know
technique that people use for these
information extraction problems where
the goal is to link documents and
structure databases and there are the
ways of doing this which some of which I
will allude to it near the end but the
methods we are using which in a sense
are pretty traditional where the sort of
the innovation is isn't how we do one of
these parts is go along the texts are
using a machine learning model to
identify to tag mentions of the entities
of interest so persons protein names or
person names or company names and and
then once you find these mansions you
have to map them or normalize them to
actually to the things that you too in a
way that allows you to link to other
external databases about these entities
so to make that more concrete I'll give
you an example that we've been working
on many of you I'm sure no of PubMed
talk matt is a search engine run by the
National Library of Medicine which is a
division of NIH and indexes abstracts of
pretty much all biomedical articles
published in English ah so it's a query
up there that I I got in the for no
particular reason except that it fits in
my next example and the I I'm interested
in a particular condition the genetics
of a particular condition and I get a
bazillion articles 661 that match it
query the search engine uses some
ranking policy that I'm not sure about
but you know you can organize you can
sort by date and so on so forth so it's
a sustained that specialized search
engine I mean if you go you can do it
the same in Google Scholar get a lot
more articles I mean you a lot of the
same pubmed stuff is indexed there as
well and that the ranking will be
different but this is a resource that
all working biomedical researchers use
every day it has nice properties you can
download the references in the standard
format that you can put in your
bibliography things of that kind and
sometimes you can get to full-text
articles for journals that are you know
that are you know open ax
and things of that talk but it only
indexes the abstracts which is what
we're working on this is a experimental
system called fable and this has been
built by the Children's Hospital of
Philadelphia which is when the premiere
children in the country fact a very high
well funded play so there's a very
innovative work in the biomedical
informatics and they've been working
with us on trying to help their
researchers find articles of interest in
the better way and so here's when fable
there's a falling so yeah I use the same
query but I want to find the genes that
are mentioned in that query so the
articles that talk about that query so I
mean since I you know I mean I'm I'm
starting to do a study on the genetics
of autism I want to know what the people
what genes people have discussed in the
context of that query ah and we run the
tagger that that is that will you
developed using the techniques that are
about to discuss and then they the
people at Children's Hospital developed
a set of actually rule base machinery to
normalize the mansion's I mean there's
ways of using machine learning for that
which we are exploring but they just
need a baseline which is simple kind of
doing some basic edits and so on looking
at abbreviations and what they do is so
given the mansion in the text you map to
a standard identifier and you link to an
itune existing database so these are
multiple places including when you know
and a bunch of others things like gene
cards and MCB a in one of the main
breaker bases ncbi gene which tells you
which is a structured record about this
gene that was found it will tell things
about the fringes the function there's
all sorts of information that people
have put there by hand or by
semi-automated man's about this gene
maybe codes for protein who is a kinase
say we say how many articles mentioned
that gene and so on and down here for
example might be that you you being a
research in this area no the top five or
six of these but they might not realize
that people have implicated this gene in
the pretty condition now this is very I
mean there's a lot to do here for
instance are you doing any proximity
kind of thing you know I looking for the
words close to the mansion of the gene
or not and now in the end they and the
system isn't under constant development
but this is the type of application that
you know that motivates doing all this
information extraction is can i link our
articles scientific articles to two
databases that people are building
independently for the same cast and this
system is actually people have really
enjoying it because they were before
completely unable to do this kind of
query and am getting this list which
they can then invest investigating for
instance i think this is one that they
were kind of people were like
interesting that and because this is
sort of one of the developmental genes
that controls the differentiation of the
of the sort of nervous system yes so
instead of banging out of it I mean the
way you frame the problem I mean one
thing he didn't mention is whether you
want to learn these to go sequences
structures whether that is meant to be
on supervised or unsupervised or book so
in the case of information extraction
supervised seems to make more sense
where is imposing it's possible you
might also do on super so actually in
both cases you would like to do more and
supervised and we do not write the
weight it is done which I'll describe
you know we have annotated so we got a
bunch of a cheap biomechanical graduate
student health to label a bunch of
abstracts with men like the way that
example I had before and they and then
those those premium but basically
premiums those are the plenty of round
pan and and those and those annotations
then we use this training father for the
learning algorithms I'll talk about now
we've been doing some
work in fact it was a graduation values
was an intern here couple of years ago
and actually last year's well who did
some work on sort of bootstrapping
techniques for named entity extraction
and we exploring those techniques
further and I'm not going to talk about
those I'm still I think I still
bootstrapping techniques that try to
infer still do things like this oh I
know these are names of genes so let me
look in the text where those names occur
and see what the context in which they
occur can i extract some features on
that and can use to learn a better
extract that kind of thing it's a kind
of an idea that many people are pursued
over last 10 or 15 years I I think the
jury is still out how well it works I
mean I certainly for this task doesn't
work well enough yet ah this does too
much junk is brought in and we don't
quite so we have a new paper in
submission that you know move the ball a
little before but I'm not convinced
technically that what we're doing is
right I think there's something we're
missing in that in that space still wet
that's what we like to do because
labeling data is a pain right and also
it's difficult to get quality high
quality annotated data that any other
questions at this point so this is multi
one motivating application and I
mentioned that the gene prediction is
you know the obvious important task
which is thought so the main challenge
is in building structure the structure
so signing is that there are a bunch of
decisions you're going to make so I use
this kind of toy example where to
elucidate the question which is you know
if I am trying to analyze this sequence
the sequence and try to say yeah what
goes together with well well is it this
show about fake news or is it a new show
that is fake well I there are two
decisions to make there you are making
two decisions in the interact so if I
decide to put this together now I'm
forced that way was if I decide to put
this together force that way so that
those interactions you know now how do I
decide make this decision right you know
and you know many of you work natural is
processing now that you like to exploit
a wide variety its features of the text
and the general picture is if I can use
those features and I can have some
inference mechanism that given those
features you know tries to evaluate
possible analysis and decide which one
is the best now that can be costly
because in general there might be a very
large number of analysis for a given
sequence typically exponential long as
the sequence so this is all kind of
basic stuff so the base general setting
and now we're going to get a little bit
more technical for a while again not not
very techincal that I'm going to talk
about is the notion that i have
sequences were so I haven't sequences
were some alphabet though bold X is
always going to be an input sequence
that event right try to analyze and
that's been enough for a safe or text it
might be a character sequence or maybe
just pre tokenized by some method um and
I said I use bold white to represent
possible analysis of the sequence now
you know i'm not going to it for much of
the discussion it doesn't matter too
much what that is but there are some
constraints on what it has to be and
then what I want is to say well I have a
way of scoring despaired of an X and the
y I'm going to say well given an X and
some parameters that I'm going to learn
I'm going to find the best why that is
why that satisfy some some optimality
criterion according which is
parameterize with those parameters for a
given X and so so that's the task that
we normally do now people have worked
saying things with e3 the Markov models
are very familiar with this just one
quick point I'm just going to again for
terminology a lot of the time the
analyses are going to be done this
falling away ignore this here so which
is I have the input sequence I'm going
to assign to which to each of these
input tokens a label corresponding label
that tells you what the role of that
token is in the analysis so that's sort
of the
just like in terminology notation so
this very often that's how we do this
when representatives so they're
basically two approaches that people
view before the work I am going to talk
about which is work actually we've been
going on for seven years so it's not
really new but ah there are two general
approaches that people used one else
generative methods and that basically
think about this pairing of an x and y
is a stochastic process some kind which
is parametrized so here's my making up
generating a generating a structure in
the corresponding text so i'm generating
x and y together through some stochastic
process and my decoding my might is
going to be I have a probabilistic model
of that stochastic process those pairs X
Y I have the parameters I'm going to
find the Y that maximizes that joint
probability for the X and I'm interested
in and that's what I get that is a
standard story that you people do with
hidden Markov models or with the with
the probably context-free grammar
another technique that people have often
used is and i'm going to use here is it
is one way to say well i see the word
fake what do i do oh I don't know I'll
just wait and get see the next word oh I
dont still don't know what to do I could
either put those two together or not so
I I'll decide not to put them together
the next one I say oh what do I do well
I have two choices no not really I mean
it is the end suppose I have an end
market they say well there's only thing
I can do it put those two together and
now to do those together so this
basically what you're doing is you're
bringing one decision at a time and
you're going to the you you have to
decide whether to wait for the next word
which is shifting the ship reduced
parser or reduce put them together so
you have a classification problem just
decide shift of reduced at each point
and you learn a classifier so basic what
you want is a classifier see that given
the input sequence maybe you look at you
can look ahead and the decisions you've
made before is going to give you the
next decision so these are the two ways
that people have done were until
basically a 2001 basically these are the
two ways people do this type of problem
as far as I can see and they were very
successful but both types of approaches
have difficulties now some of you have
heard this story before and you know by
now it's kind of a digesting your lunch
but you know since I against exactly I
don't have a completed model of the
audience I I'm just going to remind you
a bit ah the generative story like a
hidden Markov models probably grammars
has this bill requires a complete
representation tensity stochastic
process model other input output
relation and that makes it very hard and
this is a technical point in graph in
the theory of with graphical models to
model known independent features of the
input involved that the rise here so if
I'm doing text you know for instance
information extraction features like the
word ends in ing the word is capitalized
the word appears in the dictionary first
common first names in United States all
of these features are correlated so
maybe capitalize that period in
dictionary first name are positively
correlated engine I and g is negatively
correlated with the others ah so however
if you want to capture those
correlations precisely in New York to
realistic model you produce a graphical
a very complicated graphical model
that's very hard to do inference with
order learning with so that's so this is
a very nice clean probabilistically
motivated approach but it suffers from
this difficulty now there are ways
around or attempt to go around phrases
you can use sampling techniques rather
than using exact inference and many
other things if you have tried but for
very large scale problems for it
certainly the problems like gene
prediction this is a this is can becomes
a very very challenging thing to to deal
with the sequential approach doesn't
have this problem because I don't have
to model the input i only just look at
the input say what do i do given given
what i've done before and the input what
do i do next so i can exploit variety of
features of the
that I don't have to worry about which
features to use but the problem is that
for the weight strain your training to
optimize these local criterion side do I
get to read right decision at each point
and that can give us give it basically
move the push the learning down the
garden path where you are blind to the
future decision so I can make it a
decision that's book good locally but
gets pushes me into a dead end globally
and this is something that under
McCallum John after and I call label
bias problem in the paper that we wrote
in two thousand one that introduced his
ideas so so what we so that's kind of
the setting so this is where things were
seven roughly seven years ago as far as
you know stories now other people might
have different views of the world but as
far as I can see that's basically the
two options we had if I was doing
information extraction on gene
prediction so for instance you take
about your all the gene predictors in
existence actually that are widely used
they all hit the mark of model-based
with some tweaks some hacks here and
there but essentially high the mark of
model-based with very poor ability to
model curl it no local intercorrelations
in the input features ah the best text
information extraction tools developed
princes of the work at BBN was all also
hidden Markov model based and that was
basically the state of the art for these
problems around two thousand and one no
two thousand and one or so now what
we've been doing sense and this is work
that I say the Android McCallum and John
Lafferty and I work English such
initially they then they have continued
andrea has continued to do a lot of work
along these lines in various ways and
I've gotten into a slightly different
direction technically but essentially
with the same similar goals and looking
at different problems then he has is
like to retrospectively think of it
although I the time I didn't think of it
that way as a simple generalized and
this is actually the way of thinking of
it this way it's a kind of
owes a lot to your on back there this
student Kobe Kramer who is now a postdoc
with me is the idea of generalizing
linear classification so these builds on
the paper of theirs which is work on
multi class support vector machines and
other ideas around that so basically
what you're going to do is to say well I
have X's and Y's and I going to have a
bunch of features of these pairs you
know as features our observations look
at the x and y this good pairing of an X
an input X and output Y or not and you
can have it so you have this big f here
is the feature vector it's good say oh I
like that is like that I maybe like I
know so you basically might think of it
as binary not let me not be might be a
real value but in mostly for
applications we write that's a binary
feature vector that says whether i like
all there's good agreement between what
the input looks like and what the output
the proposed output looks like and each
of those features gets awake and then
finding this particular finding the
output Y which sort of the proposed
output Y hat is maximizing over this now
of course as you many of you will notice
immediately that is not you know that's
not tractable in general because their
explanation many wise so what we're
going to do is that we're going to do
exactly the same thing that people do
for HMMs and uh published context-free
grammars which is to impose some kind of
a Markov assumption in this into this
inference problem so what we're going to
do is think about this feature function
as a sum of feature functions that look
basically at all can look at all the
input X but only a little piece of Y so
a bounded piece of of the output
structure why so these f sub seeds with
call these local domain see our is going
to be a function just this depend
on a subset of the choices that you can
have for y so for instance if i'll give
i have a picture here right don't I
shall do pictures so here's an example
where we're doing something
part-of-speech tagging which is another
task where you will sign to each word
its most likely in context parts of
speech so this isn't JJ's a pen sort of
a brown corpus label for an adjective
and use for it and a noun and show for
another noun and I the local domains
might say well I'm going to look at
pairs consecutive pairs of labels and
the course sect of pairs of inputs but I
could also find set sail consecutive
pairs of labels and I will also look at
that word that were that were than what
the input what I look at how I look at
the input is is completely material from
the point of view of the combinatorics
what how many labels I can consider at
the time is critical because what you
want and this is a very general idea
comes out of the theoretical models is
you want to so here's a case where I
would do the same kind of thing for
trees again you you won't have these
local domains and what you want is a way
where these structures be composed in a
nice way so you can do dynamic
programming so it is essentially what
makes a hidden Markov models work well
so if you have these types tree
structure interactions the zoo please
talk to the graph involving these local
domains and their overlap this graph
from the tree and you can do efficient
inference and this is well known and so
the only point here is that although
this was developed for graphical models
in general you can think of it
independently of a probabilistic
interpretation and simply in terms in
the with respect to solving the
inference problem which computers max in
this particular objective function so
now
given that I want to learn one of these
things so I'm going to uh I came up with
some local domains for my task and is
like my head or my students has most of
the time which is say well you know we
want a model that looks at say the last
label and show the last two labels
consecutive labs or maybe three
consecutive labels and come any engineer
some feature functions that correspond
to kind of what's important about the
problem and now we're going to do is to
adjust w to approximately that's
optimized some objective function on
some training data and there are many
ways of setting these up and again lean
on your own singer back there knows a
lot more about this than I do but the
general picture here is that I want to
have minimize some loss so I have some
training pairs X iyi these are truth or
at least as much of truth as I can get
to and i can compute given a pair xinyi
in the current weight vector W a loss
how bad how much do i how much did I
goof in trying to label XII will produce
some y which is not doesn't agree why
the art why I so I'll pay a price I can
do the loss overall training instances
index here by wat by I and then I want
to have some means of saying I don't
want all EE weight vectors equally
because if I do if you allow w-2 very
without any constraint I could / fit the
training data very badly so I could
decide that what I want is to take the
the norm is with vector and keep it
small and you have some penalty term for
that you can do other things you can
instead of having the l2 norm you can
have the l1 norm you can do a variety of
other things along those lines now some
of you I'm sure I've heard of this note
of this idea that is in fact what's the
what r is paper with John and the and
the ender
it was about is this thing called
conditional random field and as a
particular instance of this where the
objective function has this loss
function here I reproduced that equation
there is essentially minus log
likelihood according to a sort of a log
linear model or maxent model basically
this is the law minus log of the
probability of Y given X where the
probability of Y given X you know is
just basically a bit is the lik e to
dis- so divided by some normalization
and I just took logs out of all that and
this is sort of analyze each make sure
that this is these are you know
condition public initial probabilities
so this was a CRF model I'm just
recasting it in the charge anyway so
this is what we had in mind you just we
had a conditional probabilistic model is
generalization maxent model to the
probable sequence labeling and then
later on and sparked influence again
diver by your Amina in and and kobir
sighs thinking that we wanted to
decompose the problem of design
describing the problem and you're what
optimization I do so this is a pretty
good optimization to do but not
necessarily only one you want might want
to try now for all the applications and
for a reason which I am about to
describe so just to be the most
technical and it's not very technical
part of the this I'm going to use some
not against the dark notation i'm going
to say what we're going to look in the
training algorithms that we're going to
use is this notion of a margin which is
comes out of the Furious project
machines and other sort of statistical
learning theory be too so I have an x +
2 candidate wise why in candidate
structures y and y prime and I'm going
to compute the advantage the score
advantage of Y over Y prime this is the
margin between the two of it so how much
do I prefer white y prime basically
Weta's quantities says and then given
that margin I can the the original so
the CRF loss can be rewritten this way
ah
some multiple some constant it doesn't
matter so basically this is a log loss
which is maximize the probability of
correct output which is CRF does ah but
the often what I'm interested in
Maximizer ability the correct output is
often not what I want because that's not
what why measured on I measure not on
probability i measured on how well do i
label things so let's try to optimize
that directly what leaves as close as we
can as close as it can so for instance I
could say there's a given to a why in
the Y prime there's some distance
between them and for instance it could
be the Hamming distance between the two
sequences and I want is to minimize this
miss classification so is that if I want
if say why is if this distance is huge
and why is not preferred by at least as
much / y prime by at least as much of
that distance I should pay some price
ray I want to say i want my score 20 to
overwhelm the gap so if i want if y is
much better than Y Prime in terms of the
distance say the Hamming distance then I
want the margin to be the margin of Y
over Y prime to be at least that big
otherwise it so if it is not I want to
pay a price so the general idea here is
I want to keep things apart by as much
as the at least as much as the disks or
the Hamming distance between them yes to
be extremely right yeah take off did you
mention the journal in the city
yeah not as it is not it not necessarily
not for this task is easily seen from
you're not really use it in minimizing
their you actually say if I sort of
predict the sequence between selling
this is too far from the correct
sequence I'm actually being penalized
much more then I because the margin
actually so you defined it was to give
the Hamming distance minus exactly
actual sequences so if your prediction
is pretty close in having this yeah
correct sequential being penalized less
well question is what is the decision
model that stands behind that I don't
know I mean that's the kind of thing you
think so oh but but so actually let me
just say that for the applications I
would talk about I don't use the Hamming
distance and and now there is no
theoretical decision model behind these
these are empirically chosen losses that
worked well for the applications so i am
going to be completely an engineer with
respect to this now and i left my
theoretical friends try to to explain
come up with the decision theoretic
explanation for explanation is very
technical yeah most of the methods not
yours so use that to duel approaches go
to the drill dming distance decomposes
over the sequence and you get to give
very compact global use just a 01 or say
is the correct sequence or not if there
is no simple decomposition
okay so this is the paper that uses hit
me that i have read yet so so i had i
had actually to give it i had to prepare
to talk and decided have titerito pay so
any in fact these are the two loss you
know so 0 1 error that's what you talked
about its verses that using the hammer
arrow in these two examples and yeah to
New York yes but about your previous
Laden and viewing this when working a
loss function into the into the into the
objective function and doing this is a
classification problem it seems like in
some ways it would be natural to view
this as a as a regression button you
just comment on right this is a
regression problem you're trying you're
trying to you try to predict the
structure right yeah so this can this be
viewed as a as a regression as a
regression framework where you're trying
where the value then you're trying to
predict is that it's the Alpha little
wide and then help us start your why
well when you say I mean so the what I
don't quite understand it's a regression
I mean this is there is a sort of a step
here Bo so you could imagine that the
scope completing the score is a
regression in fact is analogous to
computing the exponent in logistic
regression the problem is that then
there is a combinatorial optimization
problem which is to find the best
discrete structure why the discrete
structure why with the maximum score so
okay so in the case of classification
this is a strip you finding them because
you have a small finite set of labels or
maybe in this case which is regression
just too so finding the best label the
best scoring label is trivial so you can
say that the score part is doing a
linear regression the point is that then
i have this combinatorial optimization
problem to solve in addition does that
answer the question well the
optimization from is trivial the the
inference problem the part of choosing
the best scoring label is
trivial that's a trivial problem because
you just have a small finite number of
labels you choose the one that has the
highest score so that's that's about
that's bounded that's a kind of a
constant factor whereas in here the the
number of possible wise is exponential
on the length of the input okay so
that's that's the difference you can
look at the dewalt loss if you look at
the binary classification problem and
the Lord loss is exactly the same as
logistic regression and if you found it
around and you can look at the log loss
with multi-class classification as a
generalization of the physical living
yeah and all the other loss functions
you introduced in our game
generalizations of the Lord loss by
introducing its distance function which
you can t moon according to application
tonight f score on blues yeah whatever
score lieutenant right yeah I also had I
wanted to comment on on your history
yeah of the of the of the whole of the
of the model so you said first ever on
the generative models things about the
sequential mode well as first it was
concurrent right yeah i'm just i just
wanted to point out that there in all
those models so there are discriminative
conditional log linear probability
models they have been around before the
CAF's came alone right for particular
applications me so they were I mean the
main one I mean being parsing him so so
there was a Sullivan innovation is to
Eastern people introduced these are one
of these
this new parameter of the loss function
into of the distance function for an
Oscar so I think there are certain there
several in I mean I gotta say that I
don't Who am I to say what it was a real
innovation I think that there's a
package of things here it's very often
the ideas and the package of product in
a in a in some package like this the
ideas been around in some form or
another what we ended up doing you know
and I does not sign it kind of very
enlightening in it's sort of a you know
you know sort of knowing everything in
advances by kind of we stumbled over
time into breaking the problem into
modularizing the problem into different
parts and seeing that actually so for
instance these log linear models for for
problems with a very large number of
labels for incest parse trees or
whatever so people always thought about
that you know everything was presented
okay these are sort of a generalization
maxent models and then you can actually
decompose that into two things there's a
linear model which kind of gives a
scoring function and and what's
important about that linear model is
whether you cannot you know what the
computational problem doing inference in
it so that's so so inference in that log
leading model which is one one set of
issues so the log linear model or the
sort of leaning model in doing inference
which is find the highest-scoring
structure for that and then what you pin
a lot what objective functionally useful
for learning so is essentially is what's
going to do surgery on what people
already doing and say that there are
these different degrees of freedom we
have that maybe were not obvious before
or at least what certainly wasn't
obvious to me maybe was obviously the
people but said nothing to me actual
function
interested in for your application and
all the all the things you did before
yeah in the probabilistic model an
agenda it can lose all of the audio
dynamic programming stop it babe all go
wrong and it can work anymore and so
that's really it you have to start from
scratch once you look at those kind of
model ah yes the two I mean I out let me
just move on so to stay on time here so
what we ended up doing and this is again
motivate based on a work that you know
it's writing a work of that kobe lee
those first dissertation is I mean we
meaning now what I know what we do in my
group is sort of the way approach these
problems in the practical it's been move
away from global optimization of the
kind we talked about Frances in the log
linear model and and move to online
training methods where you process one
training issues at a time which they
kind of their nice because it you know
here's roughly the learning algorithm
right although of course there's some
some things in blue and red there that
are sub routines but you basically I
have a set of training instances want to
add numbered 1 to N and for a number of
epic I start with the weight vector
which is 0 and for a number of epochs
i'm going to classify an x and XII
incurring some loss l and then update w
in some way appropriate way to reduce l
and the technique that we use to for the
results i will describe in a minute is
again based on the work that your arm
and Kobe did is this idea I mean that
you know he'll forgive me that I will
ascribe it this way but that's the way I
like to talk about it and there's a lot
of theory behind is that they are much
in a moral if I talk about than I am so
basically you have the way to vector and
what you want to do is for each training
in sense you you want to do is to
project the current weight vector onto
the subspace
defined by a set of constraints that say
the correct labeling should be in above
the incorrect to incorrect candidates by
this by an amount that is proportional
to the distance between them whatever
that distance is so phrase that might be
the hamming distance so in a kind of
formally I'm going to find this I mean a
way to think about this is if signed the
smallest update to my weight vector so I
moved from w 2 w prime so I'm going to
find a double sort of the smallest
update here that if that allows me to
have it all the wise for any why why I
outscores why that's the margin biotin
but at least as much as why I is better
than why so if so so now the problem
here is that I'm conch and this is what
the what is effing was alluding to you
now accounting over all possible wise
here and I in general cannot do that so
now for the particular case where this
is the Hamming loss you can do some
special things because the Hamming law
the Hamming distance decomposes in Iraq
in an appropriate way there's something
called you know you can do this thing
called lock loss augmented max which is
a technique that phrase is a been tasker
used for its max margin sort of the max
margin mark of network work that he did
for Steve's at Stanford but in general I
mean if this is not a composable in a
nice way and this is at that point then
it this is not possible so we cheat I
know I'm I don't have a you know any
false humility about cheating these
things I just I just say since I
expansion man many wise that select the
clay highest-scoring ones with k the
small num and very often k is one in
sometimes for some applications k has
been like things like five but for the
action for the two applications going to
talk about k one works as well as
anything so so I just take basically
take the correct labeling and the one
that my current model produces and they
say oh the one think my current model
produced
is which is the best scoring one has a
score that's higher than the correct one
I don't like that so I'm going to change
w the minimum amount to correct that to
make the this the score advantage to the
correct one be as much as the distance
between them so if if my current model
is very wrong then I'm going to change w
a lot if my current role model is not
very wrong I will change w just a little
bit so it's kind of a is like perception
with the very center algorithm with a
variable learning rate I just have to do
how much how how bad am i doing that
might work so all of you are muscle in
the you it is it is a slide that's here
because i am using these abbreviations
on the result i'm about to kind of
overview ah so you know to positive to
negative response or there's have a
specificity or precision sensitivity
recall by the way I don't know you would
you know this but biology computational
biologists are evil because these terms
specificity and sensitivity are in
biostatistics used in a different sense
but they decided unilaterally
computational biologists to call to call
specifics what they cost specificity
what we call precision what they call
sensitivity what they call recall
whereas biostatisticians actually what
they call one of these I can never
remember which is not it's some of the
combination of true false and so on then
this one this is this confused me to no
end for a cup of years and you like
until I just kind of figure out that oh
ok they mean precision and recall so we
are happy but because we are publishing
some of the results in bio journals we
have to talk specifics enca tivity even
though of course that confuses the
biostatisticians that who think as
sensitive as faceted meaning something
else and often we have a we want to
combine these two quantities and the
user harmonic mean of the to call the f1
measure which for you who do I are and
so on it well know so I'm going to just
quickly go over the two applications
that we've worked on one is and they
have some kind of interesting things to
illustrate with both of them one is for
text so in
text we use a wide variety of features
things like label configurations and
various properties of the inputs such as
the identity of terms membership in term
lists very sort of graphic patterns
things like regular expressions say oh
this is a token made the digits only or
start a capital letter with the capital
letters followed by a digit some digits
which is very useful phrases for gene
code names that has some abc1 ABC five
common prefixes and suffixes in the now
you just throw in everything that you
think you're going to need into the
extent of the memory have and combinate
and then conjunctions of those to some
extent so it's this is the main
engineering task now it is not a huge
one for this text for the the protein
stuff the Ryan D the first version of
this and he probably took him two weeks
to do the future engineering and then
guzmán Gann chefs another student mind
did another two weeks of work on that so
doing a little bit of feature
engineering again and the result some
guy talk about is the result of his two
weeks on top of Ryan's two weeks so it's
total four weeks of graduate student
time in designing the features we've
used remember that using the first okay
yeah when you do that approximation so
on what do you do about waiting this hey
lighten we don't eat treat me yeah but
as I told you most of time we just do
the top one remember they are weighted
implicitly because I waited by their
distance so the the they not all the
same distance of the correct one because
he is distance design the girl
none of these so so basically there's
implicit way to that in that day the
ones that you you know in the constraint
you know if the third of those is very
very far so this is it's actually this
leads to an issue which I may be what's
your arm also referring to if I have one
of those top k which is enormously fine
Hamming distance from the crack once i
suppose i'm using Hamming distance it's
going to have a huge influence in that
my update but that guy might have been
some candidate that I would never want
to consider anyway so it's going to do a
big update to my weight vector and
distract me so that I think part of the
reason actually we are doing this top K
typically doesn't help much is that the
competitors that it appear in this top
kk is big enough are just total junk
make them understand you're right but it
seems like also can you talk and you're
always guarantee that there is a
solution or the top one there always is
a solution that satisfies your
conditions if you have k higher than one
it's done that and you have a linear
mods let's not necessarily guarantee
that you can actually satisfy all of
your conditions of conversion well
remember it's greater or equal right so
you're telling me that it could be a
little bonus because not so they're not
equalities right right but you know
there's like four young well yeah well
okay so good we we should use slacks but
we don't because it in fact it doesn't
matter for these applications we we've
used tried you know we have a solver
that uses slack variables and then it's
not doesn't help so yeah it could we
have that solver but we it's in practice
it never never plays a role these things
have because we use so many features
that these things are really
approximately separable that's reality
you throw enough features of anything it
you can separate it just that that's
basically the default theorem right yeah
actually there's i'll tell you in
there's a caveat on that on the gene
prediction i'll tell you
increase K your actual empirical every
right to go up is actually I've seen
down so that means that maybe that
underlying theoretical model is not
actually very useful line this is going
back to what Stefan saying maybe you
should start from the beginning and say
well I'm doing generalization of the
perceptron and I just have some function
of F and X and f of x and y and y prime
and I'm just fiddling with that function
the other stuff is just what other stuff
but I'll show with you well I mean that
you're talking about the margin having
to sum over all all the mark you know so
so in terms of summing over all possible
wise oh that the right well so really I
mean it what right I i agree i think
that your might have a different way or
left i have sort of a very medical
explanation is um what happens is that
the perception is actually not a dream
these are not join a session of
interceptor with 17 is a special case of
these updates going work on 3600 but
unfortunately if you look at the
theorems no other out all the others
achieve the same mistake about the
perception that you then and why is that
because in the case of each example is
completely orthogonal to any of the
previous examples all of the updates
actually and all of the different
settings give you the exactly the same
up you there is a complete innovation in
the new example and we vary these sort
of very high dimensional spaces it's not
that all the examples orthogonal each
example school
the information but they're roughly are
they very small similar at least at the
beginning of the run here and I've seen
trivia so in fact you can see that if
you're learning with perception there's
not a huge difference using perceptron
or using a mirror for it to start with
you know sort of the beginning of
learning but then later on you do
there's different you start getting some
difference when you when you get
relatively few errors is when it really
matters of your five percent from
learning mode do you guarantee on barges
in the same sense that which one is my
room the update step where perceptron
learning bill takes a small step when
you take a big step yes so I mean so
that's the those theorems are in the
kobes dissertation and I guess in the
paper that you guys have in a jml are in
2006 right so don't I mean I say i
delegate that to those guys I'm the I'm
the hacker here so but but basically
they are similar convergent I mean
they're slightly different I mean so if
you're in correct me if I'm wrong but
I'm just slight differences I mean the
in some into some of the results with
respect to two conversions right but
they are pretty similar to the
perception was just interested in states
yeah so it's an authentic of what these
are groups give you in addition is
suppose you also want to count the
number of times you were in just in
confident in your connection here you
get any more general so just kind of
wrap up on the results you need I don't
want to go too much over time here so
when I just wanted to lose a give an
example some that's kind of a nice point
that I want to make about using
different losses in training so this is
due in identifying gene protein mentions
in pubmed this is the test that was done
his bio creative to competition just
finished a few months ago ah and this is
one of the best not the best result that
people i don't know actually we don't
know yet who to get the got the best
number which is led around 88 here ah
this is the
again so this is a result of doing some
additional feature engineering and using
this different out learning algorithm so
log lost basically is using a CRF
learner and this is using that algorithm
using with the I'm talking taking number
of false positives will sell to twice
number of false negatives as my my loss
function and this kind of it's
interesting that this this is actually
very significant gap in Ryan's or is no
set of features we threw away a few that
seemed not to help then we added some
clustering information these were
unsupervised clustering of words and use
those as those clusters as initial
features it's based on an idea by Scott
Miller that Gideon man that UMass
implemented for this task and that helps
a little bit and then dictionaries which
you know of course can help
substantially although they don't have
much here help a little bit here so in
fact you see the gaps between these
things vary depending on which law
glossy you are watch lost you optimizing
so we kind of the take-home message and
this is there's a sort of subtle and not
obvious interactions with the choice of
loss and how features contribute are not
contribute to your quality of your
results so there's a sort of a piece of
engineering here in this table this is
what Koosman did you know the feature
part and figuring out different losses
when we were doing this by creating yeah
this is one numbers I'm sorry yeah so
80s I mean so this actually in practice
these numbers actually pretty good the
system that the the guys that system i
showed fable using an earlier version
actually ryan's version which is like
more like 80 to something and so on the
slightly the easier task
turns out that turns out for reasons
that are a little complicated which have
to do with how this is being evaluated
which is not exactly by f1 they are
measuring f1 in the competition but do
what the evaluating in a funny way
because there are two there's a gold
standard that a silver standard for each
gene there's sort of so you're actually
competing not necessary against the
correct one this is sort of a Saltine
how how they they are measuring quality
and this f1 is a convenient number to
give here but in fact when you did on
the development data this is work better
than just optimizing at Point a little
bit better I don't have that number here
ah well it wasn't so clear exactly how
they were going to measure it that's a
problem these competitions remember if
you ever run a competition like this I'm
Ryan over there is run a competition
recently you know that there's all sorts
of little dark corners in deciding how
you evaluate any decide that you
evaluation metric after it's maybe not
the right one so I cannot be optimizing
their evaluation metrics and I did it
it's not totally clear what is going to
be so I have to come up with something
that's sort of reasonable in the
ballpark because it will change some
aspects because people say oh if you do
that then there's this counterintuitive
result racist people can cheat in this
way I'd I mean there's debate going on
all the time is the competition girls
what about these things you know if we
will come out with the issues and so on
so you cannot really track that exactly
is this to this to factor in the most
positive I mean if you just had false
positive it's probably not significantly
different i I don't I don't remember
that you know Koosman just say well this
is what I got that is on the development
data that seems to work well you know he
tried how many were you know did it
right 50 North I don't remember that
yeah kind of a lot more move all this
actually sort of find it disturbing that
adding all these features and stuff
actually made less of a difference than
just
changing the boss function to something
which is different than what the
competition was measuring anyway right
that that made a much bigger difference
in terms of the numbers then all the
other engineering and went into it yes
it that could be a comment on
competitions actually we I mean the
reason to do competitions review our can
academic well well other people hear the
conversation so they they think I'm sure
they have their explanation so it's it's
actually that allows you to measure a
little bit what you're doing is what
other people are doing and and also
forces forces you know you've been a
graduate student now that you know
sometimes that have to force people to
actually try something practical rather
than writing papers let me tell you
about other competition and actually
this is a more important one so Jean
prediction this is a very challenging
problem and I just want to wrap up
because people have to go places but
this is actually the best results of the
mood thing i'm most excited about gene
structure is quite complicated we
represent the possible structures is a
complicated finite state machine that
has to do with the relative the
structure of introns exons there's some
biological constraints that go into this
finite state machine I won't describe
this is a paper in pio pio OS
computational biology in March 16 that
describes in detail there are lots of
features that we use things like
statistics of protein coding DNA are
individual codons and I codons as
individual features the length of the
exon States is very important to length
distribution the empirical in
contributions very important so you have
a use some sort of non parametric
representation of length distribution ah
and very many patterns motifs and motive
conjunctions they have to do with the
structure of the DNA around the places
we change between introns and exons also
the beginning of the exons in the end is
very important biological important but
portals not fully conserved biological
evidence around there so that's the
point they know none of these things
that frequents of revolution
so there's a lot of variability but
there's also some patterns that often
informative so this is very large number
of features the loss function again is
one of those things you know why did
that use our loss function well I didn't
use it actually Axl come up with this is
a loss function that's often using the
literature this is correlation based
correlation coefficient that's what he
used to optimize in the which is
actually could have used f1 is it
wouldn't be too different but that's
what you used in the Inca just to give
you an exact asset's here there's a nice
thing so the training set is a very
large number of genes human genes mostly
but not only exons a symbol from various
sources and notice that most of the DNA
is non coding so this is actually a hard
problem you only to identify a
relatively number small number of X of
useful information out of this big very
extremely varied data set 54 mega bases
in mind and then these are three test
sets the most interesting one is in code
and code is a very recently annotated
very carefully created a set of German
jeans this is a lot of work has been
done to make sure that this is very
biologically get you know verified so
this is the most accurate to now
knowledge the most accurate human test
set in exist gene in existence there's
some other good ones for C elegans and
the Arab adopts is which is a plant
watercress basically but but this is
this is the highest quality that the
sort of genomics community can put
together even lower density of coating
material it's very large compared you
know larger than the other two and this
long table which comes out of the paper
these are gems can gins ill these are
four major previous gene predictors all
hmm based all with lots of tweaks in
engineering this is basically Augustus's
a state-of-the-art as of
you know before we publish this let me
show you something who's really striking
gene prediction axwell how do you
measure sensitivity specificity this is
the exact match did I find the gene
exactly and this is quite important for
biological reasons I tell you later if I
have time or if you you can ask me even
for human which is the most studied
genome this is sense the numbers i get
these are percentages this is terrible
you know and you notice that if you just
let the exons all exons you know just
over 50 you you find only all correctly
only over fifty percent of the exons
that are there this is appalling I can't
really do anything with this stuff so
there's a huge amount of headroom here
so um we move substantially better in
just about everything I marked in red
the significantly better things ah it's
still terrible right our store results
are still tear like thirteen percent for
twenty six percent for genes
twenty-three percent for you know twenty
four percent specificity for genes but
we move substantially over everything
and the only reason why we could do that
is there are two reasons actually one is
that we can exploit many more features
than everybody else does and optimize
them together second one we can play
with loss function too and that was very
important and the third one is the
learning algorithm so this so here's an
interesting point we actually spent a
lot of time before we move to Mira we
were using perceptron because based on
collins and before that even we are
trying to use maximum light you know
maximum crucial likelihood and they were
we couldn't get anywhere these things we
just fail miserably overfit horribly the
training data there were all sorts of
bad behavior and the reason is this data
is extremely complex and you have not a
very large number of training businesses
you know use well still but the each of
them is a long complex complicated thing
and you know empirically me and I don't
understand this full if you're at we
mirror is a much more robust method with
a situation like that perception failed
completely because the problem is highly
non-separable and mirror even though
without even without slack variables
still can handle that pretty well
whereas where is the the perception out
the perceptron algorithm totally flail
and oscillate it never got anywhere or
at least sometimes it would but you have
to play with learning rate be very
careful and then you work on one day
test set not another one mare is only
one that we have confidence that you
will work across different species in
fact we apply this to CL against we had
never worked with C elegans axel over
the Christmas break instead of going
away on vacation enter the competition
for n gas which is the nematode gene
prediction competition and I we don't
know what the ranking is yet but
certainly his results were better than
any of the gene predictors they could
get his hands on and we had never worked
in C elegans we didn't do any
engineering for cialis ah it's a nice
picture which is actually in the cover
of our article to show you the
annotation the quality of our
annotations in a bunch of examples so
this is the standard annotation this is
a our prediction for a bunch from some
for some examples and they know I've of
course to make mistakes as well but you
know compare their everything else from
the most part we do way better and that
shows by the way finally the first exon
especially important for lots of
applications and we do quite a bit
better for first exon than anybody else
actually that's our biggest advantage
just wrap up our training data is a pain
for the supervised map so you know with
none of us likes wanna take data in fact
annotating that is very hard for the
genomic case annotate the building's
annotations is extremely expensive high
quality so we've been working a cup of
different things which I won't have time
to talk about but you're kind of talk to
101 with you later one is this idea of
unsupervised adaptation
one domain to another we do this for
parts of speech tagging between newswire
and all sort of wall street journal and
the biomedical text and got some right
nice results it's a way of essentially
finding correspondence between features
within the two domains for a linear
model this work at John Blitzer some of
you know and you know actually the part
of speech tagging was also done with
Ryan and we've done other applications
of that since then and then we've been
working on suits trapping techniques
that I mentioned early before this is a
partner to look there who's also as an
intern here before for a couple a couple
of times and actually what I at this
point what I is most interested excited
about is possibly applying this
bootstrapping settings 14 prediction
there's some real you can exploit
evolution to help you because you can
use comparison to other species for
highly conserved genes to give you some
guesses initial guesses or wear jeans
are which are pretty reliable for a
small set and use that as a boots to
bootstrap agenda data this is a new
project which is starting uh so using an
label annotating a label data from other
sources and trying to use that exceed
data for learning is certainly something
a lot of people are doing and now I
don't know whether they have any really
good ideas for this but this suddenly
work something we're working on final
thing I'm work we are working on his
theory which is what if the inference
problem is intractable these two cases
the inference problem is tracked why
actually I'm lying in the gene
prediction case I cannot do full search
so we do in vim search because of the
semi Markov property ah we need to keep
legs and you cannot you know so if what
if infants is approximate so we're using
the approximate inference because full
inference is intractable can I learn can
I learn properly turns out that you can
have the teeny simple counter examples
that show that even a good approximation
algorithm for inference can lead to talk
to enough total inability to learn in a
suit kettrick actually quite
counterintuitive until you actually look
at the exam so but the only other hand
people do this all the time you know
sounds for into some of the vent asker's
results for image segmentation you're
doing is using approximately
it is pretty good even though it's not a
nose using approximate infants why does
he learn we don't know we have some
ideas we have made some progress both
negative and positive results but we
still don't know I mean we we know
better what we don't know but it's a
very this is a very important thing
because when you move these techniques
for more complex comunitario problems
for instance in image segmentation sore
video segmentation you would like to
know whether by using an approximate
inference method to do in the inner loop
of learning that you could get in
somewhere rather than just come in going
completely astray and we don't know we
don't have any theoretical foundation to
set to decide when are we doing
something good reasonable or
unreasonable so that's all sorry further
on the time to hook it was still useful
thank you so this is this funny thing
correlation coefficient basically tells
you one minus correlation coefficient
you try to find the correlations between
the decision the sink the new single
nucleotide decisions say you say oh is
this nucleotide part of an enter on an X
on an odd so this is basically a forward
you know forward or a three-way decision
really and you're trying to decide
whether that the correlation between
those and the true the two labels so
that's that correlation coefficient so
if that's big it's good so u1 minus that
is your loss function and that's why did
I we do that we wanted something getting
the soul in a combined precision and
recall and and the it's just axle to
that one after the literature because he
had I'll tell you know had killed for it
and so on and to measure its they say
well I will use his loss function they
work better that then certainly much
better than Hamming distance in fact it
was another the tricks it we need so so
I mean part of this story is a story of
a lot of detailed engineering overtime
and but also a set of recipes I think it
useful for that you know if you are
doing something like this then I think
there's a sort of building blocks here
that can be used over and over again
with modifications but which you have a
certain confidence which not necessarily
based on yet a lot of theory
unfortunately but but is certainly I I'm
feeling much more confident today then I
was you know six or seven years ago or
ten years ago that you can apply
techniques like this where you try to
optimize across a very large number of
features for sequence problems and get
comes it you know system at with pretty
good results maybe they're the best
results you know you could always
engineer a solution of a particular
problem that is bad but you have to say
in the case of the gene prediction which
is the most challenging thing I've ever
done in that this kind because you know
I didn't know anything about Jim
tradition to start with you know besides
you know kind of the Scientific American
and I know Axl there that you used to
work at a commercial biotech company for
a number of years before I came to Penn
but the problem is that there was a lot
there was also a lot of mechanics to do
with you know various issues with the
data the nature of the data various
issues of the nature of the other the of
the annotation in mistakes in the
annotation but also issues which which
learning algorithms would work you know
it was you know for a long time we were
tried many different things I said
mention perception and see our episode
one they were not really making progress
this is kind of a little kind of
vignette side story here so they would
when we introduced the relative CRF
paper several other people thought Oh
CRF six good idea for maybe forging
prediction so there was a group at
Stanford and there's a group but at the
max planck and there was a also son
someone at UMass when they've compiled
person there and they all fought this
and I kind of help oh gee we're going to
be scooped you know these people are as
know a lot more about the problem
they're going to so and so we've been
working on this for four years basically
very quietly because you know is a small
group just me going to have a day job
and my student
and and we had to deal with lots of
difficulties in along the way but in
terms of the amount of total amount of
effort you can say is like four and a
half person years of effort for a new
gene predictor which is not a lot you
know most of the other ones that are
there are much you know groups of ten
people working for three or four years
exactly not the original chance can
chance kinda just basic one one person
in half working for like a few years so
but we basically beat them all to get
you know no one else has disguised
results using these types of methods and
the main reason is because we're very
cynical about the whole thing in sense
that we did not get too wedded 22
theoretical model so some people say oh
we introduced this with CRF trade so the
guys here at standard so they've been
building more and more complicated CRF
training stuff stuff try to handle this
large knives problem but not being able
to scale because the open session
problem can't really back when you try
to do global of you know trying to do
something like you know convex
optimization with this very large number
on to date the guys that Max Planck are
like support vector machine fanatics
they like the dual world it has to be
all duel you know there's no way that
you can at least the method they were
using they could possibly scale to the
whole problem again because these when
you move into the dual space you have
this quote you know you have to compare
everything with everything in the
sequence is very long so you have to
compare every piece here everything
think of whether piece there in
computing this sort of a in essentially
doing this sort of Colonel computations
you die and that's what happened with
them they couldn't they may in fact they
were talking about this at the nips
workshop and they were saying in the
William novels that kind of competition
Utley no compilation biologist
University watchin and say hey when you
guys think they're so they were working
on C elegans and only doing part of the
problem not the whole gene prediction
problem when these people think that you
could do this for a human is that all
week impossible because you know even
training for this it takes us two weeks
to Train just find the splice sites
which in a 44 c elegans for human we can
never do it with these methods well at
that point where we had our paper about
to be accepted it would take it as a day
to Train
his mom which on a in our normal
computer so basically what we did was we
really drove hard away from saying okay
I want to understand everything from you
know have a nice completely cleaned
theoretic model threading model but
rather I'm inspired by theoretical
advances but we really what we want is
have a real problem to solve and what
are the pieces is the building blocks we
have in this collection of methods that
are most appropriate and you often you
find you have some intuition you will
find empirically what works best and and
and so I I don't i believe the basic I
mean whether the single chef and said it
was really important point which is by
modularizing the loss out of the other
things you can start playing different
games maybe in hindsight maybe this is
the most useful thing out of this line
of work but once you do that you can
start doing many different tasks in with
a certain expectation of relatively good
quality I mean imperfection okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>