<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Customizing collections | Coder Coacher - Coaching Coders</title><meta content="Customizing collections - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Customizing collections</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8Jd6CyF713Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you very much a shock and I'm
going to be talking about the questions
of how you would go about customizing
large bodies of material and it's a
pleasure to come out here in California
it's like visiting America when you come
from New England and I remember visiting
Xerox PARC over 20 years ago and seeing
the first color digital image I'd ever
seen and seeing if projected and that
was the moment when this the work that
we're starting that I'm talking about
now began because the fundamental
question is what happens when absolutely
everything can be represented in a
digital form and what are the impacts
upon intellectual life and those impacts
those effects are only just now starting
to be felt so obviously thanks to all
the nice people who have supported us
especially since this is a public talk
and I want to start by answering a
question that that's been posed to me
which is what is the grand what are the
grand challenges in the humanities as
opposed to other disciplines and i would
say that there was really one big
challenge and it's a pretty general one
and that is how do we understand human
expression and that really three layers
that i would look at this question in
and the first is just general systems of
expression now systems of expression can
be visual they don't have to be
linguistic i'll talk about textual
systems of expression and especially
digitize textual materials but i want to
emphasize that this general principle
extends beyond that and so you have the
system of expression can be very simple
and factual and then the question may be
how would you understand expressed in
one language say arabic the directions
from point A to point B that may be a
translation which you can perfectly
capture because there's a very will
define task will defined set of actions
being described but way as you move into
different cultures you have more complex
interactions you may be dealing with
concepts of honor or of kinship that do
not translate from one culture to
another and then of course there's
another layer of individual style and
the humanist generally tends to minous
have often focused historically on out
stay
meaning elements of style what separates
Beethoven or Dickens or for Plato from
the people around them but most of what
I do is try to think about how one would
go about addressing these issues at
scale how would you under to understand
better human expression when you have
lots and lots of it online and
especially across cultures and languages
so there are two sub questions I'll look
at the first is what might ditch true
digital documents look like if you go
past this sort of first generation of
digital materials and this first
generation is really retrospective as is
often the case with new media it's
imitating the past and the second
question is if we have a model in our
minds or to come up with a model for
what we think an ideal environment would
look like how do we get all the legacy
data that we have that you that this
organization Google is doing more than
anyone else to put online how do we make
that actually useful for new purposes so
I'm going to talk about in kanab ulip
and for those of you out there who
actually know anything about early books
I apologize for my oversimplified
description of what inky Nabilah are but
I use this term to describe the first
generation of printed books in this
period of experimentation when no one
really knew what a printed book would
look like and when the first books came
out they were imitating manuscripts and
then manuscripts imitated books and
there was a generation or so before you
had tables of contents page numbers and
the the layout that you take for granted
now when you work with information and
the point to emphasize is that as a
process that takes a while to take 22 to
run and I give an example here of a page
from a Gutenberg Bible where of course
what was printed was the i believe the
black print I'm not sure if the red was
printed or not but none of the
illumination none of the pictures the
illustrations around the edges were done
mechanically you have the mechanical
print and the center but then someone
painted by hand all the pretty pictures
around there and the real question right
now is to what
dent in what we're doing what is the
movable print and what is the
illumination and I emphasize that people
don't sit around talking about the evils
of illuminated manuscripts they're
rather attractive but you could not have
modern society if you were illuminating
every met every page of every copy of
every scientific publication you needed
to leave that behind so that we could
all live more than 30 years and travel
across the country in less than two
years so what might digital post into
nah Bueller digital libraries look like
well they're not going to look like PDF
files and I think PDF files are going to
be the classic instance of the Incan
ovule of a digital age because there
doesent they are successful because they
look like the past and they add some
marginal utility to the past they're
searchable I have a whole pile of PDFs
on my laptop and in fact I won't collect
paper anymore for serious work if i
really emphasize work i can do in a
wholly digital form but nevertheless
most of the objects are pdfs okay there
are four dimensions in my mind that
define at least the space of truly
digital documents that are separate from
the dimensions of print there may be
more things out there but I think these
are for minimal criteria that you would
need to meet and i'll talk about each of
them in a little bit more detail
separation of data from presentation
recombinant data dynamic data and books
that talk to one another in in homage to
Marvin Minsky so separation of content
and presentation is probably the most
trivial of them all it's like saying you
should all eat lots of vegetables and
not drink too much and get lots of
exercise I don't eat enough vegetables
but I know it's a good thing and it
doesn't hurt to be reminded of it so
this is an example of a complex chunk of
text as the dictionary entry as we
represent it in our digital library and
it's wonderful of course we will spend a
lot of time thinking about how to
represent it the underlying data is of
course substantially more complicated
and that's a heavily marked up piece of
X
ml that was worked on took shape really
about 10 years ago but the key is that
if we just make it available in HTML
then that's what you get but if you make
this available and we do buy a service
for every addressable chunk of text in
our digital library that other people
can build clever new services and I just
killed the trivial example not actually
not such a trivial example which the
example of another person who took this
dictionary changed the layout on the
left hand side using colors to
communicate semantic information and
when I think is a pretty effective way
but more significantly that person built
a service on the right hand side that
links etymologically related words based
on the information within the lexicon
that that person was able to mine
directly that's a service I can tell you
is useful and I at some point I'll get
around to building it ourselves maybe
when I'm 125 but obviously by making the
content available in a fashion that
other people can build on at a deep
level then you enable other creativity
come in and do things that your group
can't do and I quite conscious that my
research group is a bottleneck right now
but there's just not enough of us so we
have to roll things out and let more
people work on it now recombinant data
is a little more interesting and that's
the the idea that you want to be able to
disassemble your documents into pieces
and that machines need to be able to
reassemble things on the fly I give you
an example again that doesn't look like
much but it's actually kind of
interesting from my perspective and this
is a chunk of text it looks like a lot
of other chunk of chunks of text in the
world but in fact it's not just the
chunk of text this is a library catalog
and object viewer rolled into one one of
the problems we have is our library
infrastructure is wonderful it's
actually a catalogs are kind of an
amazing thing but they do not have the
granularity that we need in fact this
would be a thing to think about in
google books where the focus is on the
book nobody reads books I'm not
interested in a book I want a couple of
pages or paragraphs or chunk a logically
useful chunk I don't have time to read a
book unless someone asked me to review
it I don't know anybody reads books
unless they're on
airplane or they have some exceptional
purpose most of the work that people do
is based on finding logical chunks the
right article from a journal the right
paragraph the right table whatever so
absolutely there's nothing else that I
emphasize the book is throughout the
level of the unit of functionality in my
view certainly in my life in life
everybody I teach and everybody i work
with maybe there's somebody else for
whom books per se are interesting now in
this case what we've done is we've
gotten asked for a display of
information not only about a work but
about a subset of a work in this case
classes were really smart I have to say
my my colleagues 150 years ago did
something really clever they agreed upon
for most authors a coordinate system
essentially whereby they could describe
chunks of text so zouk one point 86
could be spelled differently their
variants of expressing that concept but
it is the scribe the same basic chunk of
information for a hundred more than 150
years nah I'm not even sure when we
picked up one who started this but for a
long time which means you can assemble
in a great and and fuse data from many
different sources if you're able to
parse the the mappings within the
documents that you're digitizing assume
the pages don't matter it's the chapters
and the citation scheme within and in
this case which you have as a report and
it shows not just one translation but
multiple translations are the same
object various people who have added
notes to this canonical text because
people will talk about important texts
and store their annotations information
that's been extracted automatically and
it could be combined with other sources
of annotation references pointing into
this so you can find dictionaries and
grammars to talk about this particular
passage you know it's all very
commonplace stuff but it works because
you have a common identifiers to
describe the work and then the chunks
within the work and the chunks within
the work that are small enough to be
useful and with this of course you can
drive
once you have translation in Greek for
example side by five you can start doing
parallel text analysis there's another
layer actually of granularity within the
sneak canonical structure that would
break this down even more finely but
even if you have chapter to chapter you
can do autumn automatic alignment and
get about 75% of the of the sentences
lined up side by side and you have
enough sentences you'll do clever things
so dynamic data I mean again this is no
news but it's not how human is tend to
think and that's where the data itself
changes your reference works that grow
over time and this is one thing that
humanists who have a tendency to get
really excited about their own
expression need to rethink because we
need reference works that can evolve in
a real time as and if it's the some
extent the smaller the discipline the
more important it is that we have
community driven resources because we
can't spend millions of dollars paying
people to do our work for us and the
clearly the historic example this is
google is wikipedia and here's an
article on an aspect of Athenian
democracy from wikipedia from march two
thousand six and it's pretty serviceable
classicists can look at this though and
make fun of it because but greek which
you I don't have a pointer for but
there's Greek up there which does not
have accents which it should and in fact
there's an error because the form is
said to be plural what in fact it's
genitive so you could say well how
nothing can be right here because they
got the Greek wrong and then six months
later of course in this self-organizing
system I just happened to discover when
I did my presentation that people come
in and of course it fixed the problems
and added structure and so the grin a
grand challenge for humanists certainly
is to figure out how to work with this
new current of energy rather than
against it or to dismiss it and to
dismiss it is the usual course among
academics and how do we get there this
is an example of the first tenure book I
know of it was entirely electronic on
its on Athenian democracy it is the
source for Athenian democracy and if you
google right now at the knee and
democracy I think you'll see Chris
Blackwell's de mas show up right next to
the Wikipedia but here is a this is an
article or book that was written to be
part of a digital library where the
citation the habits of citation are
fundamentally different he has links to
the primary evidence for every
proposition that he can think of the
idea is that everything is supposed to
be transparent this actually addresses
the problem of Wikipedia the problem
with Wikipedia isn't that its innovative
the problem of Wikipedia is that it's
conservative it's imitating the
encyclopedia britannica which made which
reflects a tradition of leaving out
information because of print or because
you think people are stupid or they
don't care uh and that is actually a
problem that's for me the problem with
most reference works and so here you
know my colleague has put all the
citations in it's all on XML so we can
filter them out if you don't want to
read them but what you want and what
he's interested in other people could be
is you might start with this as an
initial state where everything has been
added added with with some kind of
evidence and then open this up and let
it flow and evolve on its own as a wiki
or whatever medium you want but I would
say that if in a hundred years or a
thousand years and people look back to
the opening of the 21st century I do
think Wikipedia will be thought of as
the most important intellectual
development not necessarily because of
its content but because it represents
the first different mode of intellectual
production that I can think of and
several hundred years and because I
thought it was impossible and was
ridiculous would never work and it has
so I like things which reminded me of
how stupid I am or how limited you know
how hard it is to predict the future and
the last feature that I think
distinguishes minimally true digital
documents is that they talk to each
other they react with each other
interact with each other in real time to
serve people's needs and I give this as
an example that we've been using for 10
years and I published something about a
year ago where I thought I would use
this phrase then I said now I've been
using this phrase on and off since 1993
and it's an illusion to marvin minsky
who said something along the lines of
people the time would come when no one
would believe that the books in the
library had not always taught to one
and I think Marvin Minsky was thinking
about Plato and Descartes having a
serious philosophical conversation or
Dickens revising you know his own novels
and so on but I think and that hasn't
happened it may not happen anytime soon
but the idea works already in a very
trivial way and the triviality of the
technology is what makes this
significant and so what you've got here
is a Greek text someone's reading the
Greek text and they've had a question
about what a word means and they've
clicked on the word and what happens is
the Greek texts as a no I have a
question about this word in this line it
goes to a database of language which is
equivalent to what used to be a printed
book called to Iver be grouchy and then
the database of language says this word
is this form and then the users is well
what does it mean and it dictionary gets
called up so there are three different
books pulled off the shelf as it were in
the analog to the print world but what's
different is that the books all know
about each other and when you get to the
dictionary the dictionary as asked the
the text well who are you and what is
this person reading and the text that
said I am Aeschylus Agamemnon a play
called Agamemnon and the users asked a
question about line nine and so the
dictionary says okay what do I know
about Aeschylus and goes through and
highlights all its Aeschylus citations
and in fact realizes AHA actually have a
comment about what this word means in
this line of the Agamemnon and adapts
itself on the fly using some
highlighting and then setting itself up
or no scrolling into this larger
document now nothing has changed no no
the content has been modified we have
the style sheet has changed a little bit
but in fact the value of the information
in this in this reference work has been
enhanced because it's easier to find the
cost of getting it is lower and the
books essentially if three books has had
a conversation among themselves
conspiring to make the life of the user
more or simpler and to bring that
information more quickly that is the
principle that the books in fact have
these conversations
among themselves dynamically every time
the user does anything that is it to me
the most important fundamental principle
what I would imagine the digital library
to be so you've got this hybrid library
now and in the old days it was sort of
you know the arrow of time ran forward
inevitably and and and and a
heavy-handed fashion authors produce
content libraries collect books and so
on and then readers sit around and read
the books now everybody interacts in
real time and your everything can change
and there's much less fixity in the
system essentially your user performs an
action and every time anybody does
anything you should you probably already
are on this organization you're mining
it to see what you can learn the
question is how much more semantic depth
can you get out of what's remaining how
much structure can you know about what
this person is doing and then you know
the system gets smarter ok now I'm going
to talk about some of the automatic
processes that I would think of as
essential an effect there's lots of
services and these are just some of them
one to me that is everybody seems to
need in the sciences and the humanities
is named entity analysis and I spent
years building a system in part for
pedagogical purposes so I would
understand how the problem worked and in
part to deliver some services to deal
with named entity identification of
American English which is actually a
hard problem because of the ambiguity of
American English which is worse than
European and worse than ancient
languages and so essentially your
digital library needs to be mining its
own resources you need to be able to
read drone encyclopedias your own Gaza
tears to make yourself smarter so that
you know you know who the people who are
people and you know that how often
Washington is a person versus a place in
the given context and in 1855 you know
which Washington's exist if you're
trying to identify you know semantically
it's a place and you want to figure out
which place it is this is just an
example of you know that process done to
some text
scale nothing new to you guys but this
is all the mark up there is of course
automatic except for some of the
structural markup but then you can start
building services now this is something
that looks a lot like what you have for
google books that was released the end
of January where you have a thing at go
to the about books what places are in
the book and you get a map which I think
it's a really cool service in part
because we've been thinking about the
problem for a long time this is similar
to what's been done but it illustrates a
couple things that are needed to make it
really usable first of all this is a
smaller chunk of text this is the set of
places in a chapter ok so again that's
not that big a deal but you got to think
about it and these are the places
referenced in a chapter of a civil war
history and of course you know the
results are pretty good there are errors
in here up here at the top that's I
think there's like a muddy creek or sign
ambiguous place name that gets placed in
Rhode Island because there are too many
of them but the basic geographic focus
of this chunk of document is captured
pretty well and this is something I'm
sure you could do the trick is how do
you make this work well optimizes for
different domains because the place is
referred to in the 19th century in a
19th century English are really
different from those that would pop out
if you use the Getty at the source of
geographic names or a modern gazetteer
and as you move away from the focus of
the modern world your results are going
to get worse and what people often want
are the five or ten percent that you
don't get they want the places whose
names have changed they want the things
the high-value services depend upon
knowledge of domain specific functions
okay lexical analysis language tools I
just skip over that I just say it's nice
to be able to understand what things
mean now I want to talk about user
interactions where the rubber really
hits the road and there are three things
that I think we need the humanities I
think a lot of people need them but I
know we need them the humanities across
the board and I would say that's where
readers talk in books listen and then
customization and personalization
and the when I talk about readers
talking in books learn or listening if
you have a system with many many
services all of which were automatic
doing they're clever best to try to
identify one thing or another they're
designing it's a person to replace and
if it's a place which Washington is it
there's always an error rate and people
are really good much better at
classifying and clustering often than
machines we did a study what we call
disambiguating links in Wikipedia how
often do people connect Springfield to
the right Springfield basically they're
always right i mean ninety-nine point
some percent i mean there it's it's
great so people you may argue about the
content of a Wikipedia article but the
implicit links within the which is in
each link is a propositional statement
saying this string it this place is this
entity those are very useful so there's
a lot of classes of that here's an
instance of a complicated of Latin
morphology here's a word whatever its
possible forms there's a bunch of
possibilities because it's highly
ambiguous and the numbers the simple
Bayesian statistics or whatever machine
learning we're using it's hard to pick
out the right one what do you think and
we were lazy didn't have time we didn't
get people accounts we just let people
the world vote and the results are
anyone vote is about eighty percent
accurate but as you get more and more
votes and that system itself is seventy
percent accurate comes up with the top
answer seventy percent of the time and
but as your votes is you had get more
votes they converge on the right answer
and so even if any one vote is not very
useful or as problematic and has an
error rate you know they quickly
converge and if you have a divergence
you cluster into two different chunks of
votes you actually probably have
something significant you probably have
ambiguity and that by itself is data but
how do you make it possible what are the
mechanisms whereby you accumulate
propositional information essentially
what someone has done is said you know
generate a piece of rdf or triplets
saying this thing at this place is this
class and it could be person place it
could be some tax in syntax it could be
any classification job that you want to
think of
how do you make this possible for the
classification jobs other given
discipline whether its chemical
compounds or genealogy where you're
trying to figure out which Smith is
which hey customization that's obviously
where the as i understand it where
people decide say i want to see things
about the Red Sox or I make a decision
how the information we will be filtered
to me and so this is an instance of
analysis of words and a chunk of latin
300 running words 221 unique words and
so we done an analysis by filtering out
inflections and there's a lot of
actually fair amount of work in this and
so this is a summary of what's involved
in a chunk of text you could have other
features of interest in a in a document
but this is what we really want in this
case what we've done is we've provided
the system with a profile that describes
what all elements of this language it
happens to be Latin it could be Arabic
it could be technical terms in a
scientific document which of these words
which of these symbols has the user
encountered in the past and then I want
to have separate the ones that are
clearly knew from the ones that are that
have been encountered in the past and
then rank the ones that are new so that
you and you can rank them according to
different criteria you can do a tf-idf
kind of thing or you can if you're more
clever you might know this user is
interested in the history of law and so
legal terms would be emphasized more
than other terms would be you want to be
able to customize that but the idea is
to to filter to analyze the search space
for every individual or the analysis
space for every individual one and
provide a pro pace of customized
briefing form reflecting their own
background and their own knowledge and
then be able to say well you learned
this ten years ago in this class so you
encountered this word in these contexts
to give people access to their own
background more effectively that's a
pretty general issue and it reflects the
need for kind wrote the interest in a
kind of learning medical profile
that you might develop over time that
would allow you to control the things
that you've been conquered and thus
maximize your background again that's
not rocket science in the simple case
but it's very powerful and we'll be
doing this with Arabic this fall where
will be modeling what people know is
they go into intermediate arabic and
when they're reading al-jazeera which
are the terms they see are the ones
they've seen in the past and so on and
I'm actually trying to follow along with
the Arabic students myself in a mother
site at this now personalization is of
course where the system rather than the
user takes action on its own and
everybody even classicists all have had
the experience of seeing people who
bought this book also bought the other
books in giving way to temptation and
buying more stuff than they plan to buy
so a recommender system but we could
apply this sort of thing to a learning
context as well we discovered as when a
guy named e Scully who's a student of
Carla broadly one of my college computer
science looked at user logs and analyzed
questions people asked when reading
chunks of text in this case it happens
to be Latin and looking for patterns he
discovered that after after people would
ask three or four questions about words
and you know weren't sure about
vocabulary you could predict two-thirds
of all the other questions they were
going to ask any did this by breaking up
these you know previous logs into
different chunks and comparing them
against each other and doing all the
good things you would do to have value
no valid results so you're able to
predict the kinds of things that might
be of interest to people and this is
obviously a very powerful idea because
then you're able to use a little bit of
action to get a sense of what people are
going to need and you can think about
this I think about this as a way to
identify went before flailing what kind
of questions you asked me well maybe you
should go back and learn the subjunctive
which you never did before or maybe you
should look at this or that this is so
this is kind of a proof of concept to us
and illustrates the sort of thing that
you can do to enhance people's ability
to understand
to see patterns on their own reading
their own questions which they might not
themselves have seen okay so that's kind
of this is kind of where we would like
to go so this is what an ideal future or
present looks like mrs. what we work on
right now were the modest collections
that we are able to curate by hand so I
remember when I first heard about the
Google Books project I we built a couple
of thousand a few thousand books over 20
years and they're all have wonderful
markup people going over every page and
we doing all this stuff and you know I
heard well 10 billion books are going to
be digitized in mass and I kind of SAT
there and said well have just like my
what and place of my life wasted you
know what did I do with all this time
it's all going to be done at scale but
of course what you realize when you
start putting in large bodies of
material is you can't you can't look at
everything and there's values is it the
a million book library is really
qualitatively a different space and
that's really the question I turn to now
the next few minutes before I finish up
so what can you do with a million books
well you want to have scalable services
obviously you want to have something
that can just run at for as much
material as you have and give you some
kind of handle on it this is an example
that we thought was important years ago
and built for various collections of a
temporal spatial browser to look at the
dates and places in a given chunk of
material and this is quite a scalable
technology gets run on everything it was
built by a guy named David Smith is now
getting a PhD in machine well in machine
translation at Johns Hopkins but
essentially this is a collection from
the library of congress on the history
of the chesapeake bay there's lots of
noise because American names are really
ambiguous but the actual center of the
cluster accurately reflects the
geographic focus of this collection of
text and you can see that the dates
extracted reflect the temporal focus and
the point of this is of course that you
can do this arbitrarily for as much
stuff as you want and the results are
useful and this has been sort of a
on the our homepage for seven years now
and this is a slightly different
collection from the Library of Congress
this is the kind of thing you want I'm
really glad you guys are working on it
because you can do the technical work
and I can use it but I think this is
this is how would you go about
generalizing that now the trick with
million book libraries is how do you
build these personalized collections or
services and collections of the type
that I was pointing to before I the open
content Alliance is made available about
200,000 books already that's about ten
percent of what seems to be two million
books that we can find doing our best
estimate of what's available in Google
book search right now and probably about
so it's ten percent of the existing
corpus and order of magnitude about 1%
of what would happen if you actually
digitize all the books that's in all the
libraries you've got your hands on right
now so even with 1% of what the ultimate
corpus might be it's too much there's
way more than we can manage so we
already can play with some issues of
scale and we just finished a workshop at
Tufts University where we got a lot of
the a lot of the energetic people in the
humanities who've done practical work
whether in the humanities or they're
human or their computer scientists with
the commitment to the humanities to talk
about this and the results will come out
later this summer so with million book
libraries an um it's important to bear
in mind that a number of quantitative
features or dimensions change at once
your magnitude you're getting
collections that are order of magnitude
10 to the third or more bigger than
we've ever had with curated collections
of printed materials you're getting much
less structure in the past we've
expected a lot of markup you know get
none your content there's no limits it's
whatever is in the library whatever's on
the Shelf gets sucked into the into the
mall the machine there's no manual
checking of errors so you have maybe ten
times as many errors may be less but you
have a different set of different set of
issues there you have access you have to
think about twenty four seven anywhere
on
earth especially if you have an open
access model as you do since you signed
Google signed an agreement in its
agreement with Michigan the statement
that books would be made available
without charge to the end user or the
agreement is void their languages most
hand collections assume a handful of
languages usually one but you've got
there are five hundred languages in
Harvard's widener library how do you
manage all of this they're going to be a
lot more languages as this material
expands and your audience is global and
massive and that's really exciting if
you're in a small discipline like mine
because what I want as I when I think of
as a classicist a person who studies
greco-roman antiquity is to figure out
how to deliver as all we know about
Plato or about the Peloponnesian War to
someone who might be sitting in China or
India or Indonesia or Africa in 10 years
in whatever language they happen to
speak and to customize it so we know
roughly what their background is what
they would need to know for back around
and give it to them in the way that they
can understand it I mean then I think is
really exciting out and of course I want
to do the same thing and you know if you
were asking me the great thing I'd like
to I'd like to have a conversation with
people who speak no English who might be
Shiites or Sunnis in Iraq to understand
what's really going on and understand
what I needed to know about their
background in so far as I could
certainly it's not going to be the same
as living there and knowing their
language but it's going to be better
than nothing so there are core
technologies here and here i guess i
really like the darpa gail a program
which is their current language
technology program which initially broke
everything down into three types of
service i think this really simplifies
the welter of language technologies that
are out there so you have to go from
analog to text I think breguet loses
cell phone conversations or speech but
from honors perspective it's page images
an analog median to some kind of textual
medium and that can be linguistic
symbols on the page but it's also page
layout ie what is a footnote what is a
header what's a page number so there's
semantic and firm
should encoded in layout then there's
multilingual technologies that's
includes cross-language information
retrieval which classes this actually
would use I have to read four different
languages so if i could type in english
and get German French Italian as well
our community would use it immediately
as a problem with cross-language
information retrieval no one uses it
translation support which can be a
machine translation but could be also
helping people who know some of the
language get more on to their existing
knowledge so me with my pathetic Arabic
you know as I get to the point where i
can actually read some of al-jazeera
getting stuck in an idiom and I want to
know what that idiom is I don't want a
simple translation I want to see how
this phrase idiom is used elsewhere to
understand maybe it's insulting maybe
it's complimentary maybe it's it's
colloquial and then there's text the
data as the last core service unit named
an area identification information
extraction and the real question I I ask
you where I ask myself is how you can
optimize this for various domains now I
have built a complete name than any
identification system for English that I
can apply to other languages it works
really well it's been evaluated we
publish it and so on I don't want to
peel the system now I built the system
understand how it works what I want to
do is build the knowledge bases that
would drive someone else's system and i
think that the work of scholarship for
the next generation will be figuring out
how to build the knowledge bases that
will drive general system so there might
be someone who has an indenting
identification system and we would
figure out but ultimately what kind of
gazetteer information what kind of
features you would want to be able to
find washington north carolina as
opposed to Washington DC and I actually
think that will be the grand task of
humanists in the next generation we're
going it's like the 21st century
equivalent of what two Germans did in
the eighteenth and nineteenth centuries
when they built the foundation of much
of academic scholarship for the 20th
century so here's just an example of one
page image that I'll look at where this
is a page image of a an edition of the
Greek order and statesman Cicero and
these are letters that he wrote to his
friends
and they were supposedly not for public
consumption they're actually really
important because they're the first real
set of documents that allow us get a
sense of how someone thinks in their
private lives them it's in the
continuous tradition of European
literature unfortunately for Cicero he
spends a lot of time whining and talking
about how he's not been treated blowing
up and he's much better than he has been
treated in the past so it's not always
played out well for him but we have his
letters but it's a real window into how
people thought in politics and society
of the first century BC in Rome and so
it's a really an important source and
people have done a lot of it versions of
them and but the simple page exhibits a
number of problems for each of the three
core services that I've talked about so
first of all you need language models
for something besides English at anybody
who tries to pump things in Roman
alphabets through OC Omar and the OCR
thinks it's English is going to have
instances where the EOC arm angles your
language so you may get every character
right my one example is the Latin word
tomb t um means then it's a high
frequency word it always gets turned
into turn if the system thinks it's
English which is an arrow so you have to
fix it back but there's a lot of places
where you really need to have the right
language models even if you know even if
you can read the character script and
then of course you'll add something like
Greek and that is if you just pump
classical Greek into a modern Greek OCR
system as we did we got eighty percent
accuracy eighty percent of the
characters were right twenty percent
were wrong because classical Greek has a
bunch of complicated accents that in
modern Greek has been wisely left out
and so that's noise for the OCR how do
you go about getting past that and then
how would you deal with something like
syriac the language of the ancient near
east of Palestine Syria Palestine this
is really important historically or
other sources that don't have modern
equivalents are then aren't the subject
of business memos in a important economy
yeah dad
example other a lot of these cases where
in the end the amount of content that
exists is fairly small as in 50,000
pages hundred thousand pages where as
opposed to developing technology to
automate OCR it's actually cheaper the
time that I have transcribed in it
you'll get everything answer is of
course that guess what now you know
actually I would think anything that has
its own script probably defies that
precisely because if you have a living
script at some point that's recorded you
may have so much material when you
actually get at it that it doesn't you
can't double key at all but there will
be cases where you know you might as
well just type it in I think there are
but it's but you have to deal if you're
doing cultural heritage stuff you often
find the case of say Arabic manuscripts
or you know Arabic is different I think
there is and Sanskrit is huge there are
languages where you know you can't you
know you've got a problem but yeah there
will be instances where the corpus is so
small like linear a what are these
languages from Crete which nobody really
understands there's a finite number of
these things it's not worth doing an OCR
when we don't know what it means in the
first place so then there's and there's
the issue of page layout and this thing
there's actually logic here this is a
commentary on that these notes this is P
zone m in italics with little square
bracket this is quoting something up
there and this is an annotation and so
essentially this is an anchor or or this
is a work sees me a pointer to a span of
text up there so you need to be able to
recognize here's boos oh gnome find the
right peas onnum up there and link this
annotation to a span of text in the
letters so that's something you know
that's not rocket science but you need
to be able to understand this this data
structure implicit in the page layout
which is common to many canonical texts
many disciplines with canonical texts
okay of course what langster the other
you need to have specialized tools you
need plug-in or the capacity
to create a morphological analyzer for
the language of your choice I don't I
know you can do I don't have any desire
to work in part of speech tagging for
English because other people do it
better but I built morphological
analyzes for Latin and Greek 20 years
ago because I didn't have any
expectation that anybody was going to
solve that problem for us anytime soon
so how would we be able to plug the
results of that into a larger system and
you need morphological analysis for
highly inflected languages to do good
analysis to do CL are you need two
prosthetic because you can't anticipate
every inflected form when a verb could
have 20 million permutations you want
syntax of course you want to be able to
figure out that this word is the object
of a verb and so on you want to have
little translation tools which again
we're going to depend upon language
specific data and you want things like
across language information retrieval
and then you also even in the text the
data range there are domain-specific
phenomena everybody has people in places
or virtually everybody as people they
have places as categories organizations
as a fairly global but even if you can
your names are structured the same and
they aren't in most cultures you need
different language models to figure out
which weather Cicero is a person to
replace cicero illinois and if it's this
row which cicero and then their domain
specific site classes for example
citations in and lighting shakespeare
classics any discipline working on
canonical text while have a citation
scheme which is different from what
sightseer analyzes or Google Scholar
analyzes and it's often highly ambiguous
th period 115 could be theocritus Idol
115 or Thucydides book 1 chapter 15 in a
classics document you have to have
context awareness to disambiguate and
sometimes you have citation schemes
which are common to a discipline
sometimes they're specific to a document
like those are both things that showed
up on that page I cannot over emphasize
enough the importance of managing the
citation 'old atta that links
information
in data it's like chemical compounds or
mathematical equations is at that level
of fundamental need and you're not going
to be no one group is going to do this
what you want is the ability to train a
large system at a deep level from the
beginning from the start to extract this
information and then of course you want
to be able to find compound statements
certainly the simple ones that are
meaning bearing that conform to some
ontology that people actually agree with
so you know Calpurnia spiezio
father-in-law of Caesar that's a pretty
good example something you would turn
into a trance ism rdf so finally what
you want is what I'm thinking what we're
thinking about what are the global
analytical services that you want apply
to everything and here's it a set of
examples that I think there's a pretty
good consensus that everybody wants this
for every document you ever put online
ms some with more emphasis some with
others named energy identification
morphological analysis syntactic
analysis source citation mining
quotation identifications someone quotes
Hamlet but doesn't give you a citation
you want to be able to recognize that
and then say you know measure the
popularity of hamlet in the
english-speaking world up and down time
for example automatically addition
alignment and collation I can't
emphasize this enough for disciplines
that they care about your text you may
have 50 versions of Shakespeare you want
to find all of them line them up and see
where they're different and use them all
to correct each other and figure out
when a difference is of is a conscious
variant when the difference is an error
you can do that pretty well without too
much work we've already done experiments
on it and we have a person from Norbert
fours group coming over from Germany to
help us but spend a couple of months
focusing on that and then markup
projection where you might have one this
goes back to the question of what do you
do with a curated collection of giant
library if you have one well marked up
edition or document you might use that
to catalog 50 other versions of Hamlet
and so you have how do you integrate
domain specific resources and there's a
whole bunch of things
we might create that you want to put
into a larger system and then you know
how do you use these structure
collections as programs this is an
instance of what I just mentioned it's a
chunk from Antony and Cleopatra and here
you have your text with a bunch of
complicated markup essentially what you
want to be able to do is take one
heavily marked up text like this and
then use this to align against all the
other editions you may have and then you
can say well you find an original
spelling there's all sorts of variants
but you can nevertheless figure out
slightly varying versions of this and
say they all these other here's all the
other places that correspond to line 789
of Antony and Cleopatra in the first
folio so given one marked up text use
that to classify and catalog everything
else that's that is a fundamental
technology that's easily implemented
that is a result that spins out of
having a small amount of curated text so
you might have many different objects
knowledge-intensive structures like this
that you would integrate in to structure
a much larger body of material so the
big question how did domains provide
services upstream into these very
general services so how do we get input
it's not enough to have an API at the
end if you you know I may have an API to
google book search but if you can't get
any of the Greek ocr'd you know what's
the point there's garbage in garbage out
and how do you add things like language
tools and so on and how do you make
progress I mean our my feeling is you
pick a topic and do it well and solve
all the problems maybe take several
topics our problems that I'm describing
here in the humanities are actually very
similar to those that my colleagues and
chemistry are solving that they have
chemical compounds and I've got Greek
syntax but the architecture of the
system is strikingly similar and so if
you really do a few pick a few domains
and do them very well and think about
them and make sure that the architecture
is aligned you figure out what's
domain-specific and what the common
foundation is then you have a really
powerful architecture and that's sort of
what we're working on right now and my
I've worked in 19th century English
Shakespeare and the Renaissance right
now I've gone back to my home base of
classical studies for a variety of
reasons to my surprise impart it's hard
you have to have a bunch of languages
you have to have all the European
languages as well as Greek and Latin
there's a large public domain knowledge
base you'd be amazed at how much you've
got in Google right now that you have at
your disposal and that I can digitize in
the open content alliance and there are
you have art archaeology Ole Miss
sources as well as text you have places
and there's a critical mass of tenure
digital classes more so than any other
discipline so with that thank you very
much and we might have room for one or
two questions I think I'm going over</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>