<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Near-optimal Monitoring of Online Data Sources | Coder Coacher - Coaching Coders</title><meta content="Near-optimal Monitoring of Online Data Sources - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Near-optimal Monitoring of Online Data Sources</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/903dUNSGbGM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this is ryan peterson from cornell
university as a student there he works
with Gooden seer who is a grad student
colleague of mine is now on the faculty
cornell and he's going to tell us about
a basically a distributed publish and
subscribe algorithm that he's been
working on in system Jeff everyone hear
me know okay now you can hear me alright
so I hear that Google occasionally
visits new sites to collect information
and even crawl the web sometimes I'm
going to talk about a related problem to
this that I've been working on at
Cornell for the past year this is joint
work with Rama down the street at MSR
now and goons here at Cornell so the one
of the reasons that crawling the web is
so difficult is because the web is big
and it's getting bigger there's your
words of wisdom for the day there
including frequent an increasing number
frequently changing data sources such as
web pages that can change multiple times
a day the sensor networks that the
future promises and extensive online
databases that clients want to know when
their updates posted to these so
continually continuously monitoring
these are important to clients who have
some specific data some specific content
server that they want data from and they
want an efficient way to get that data
so the current way to accomplish this is
using a standard called RSS but as
you'll see RSS has some problems so on
the left here we have clients and on the
right side we have content servers that
presumably have content that the clients
are interested in so the way RSS works
is that if a clients interested in some
content it pulls that source of content
directly if there are multiple clients
they all pull it in I'll pull it
independently they don't share updates
with each other this is a real problem
here's why as you see here if you have
multiple clients all pulling for the
same data they all pull independently
this causes a problem at the content
servers they see way too much load more
than they can handle and so they've had
to impose bandwidth limits in other
words slashdot for example doesn't allow
a single client from pulling more than
twice an hour this place is a problem at
the client site
they pull every half an hour they can't
detect updates better than 15 minutes on
average so we decided to build a
publish-subscribe system for the web
that can efficiently get content from
content service to the clients who are
interested in it we call this system
Corona and it decouples the clients from
the content servers allowing the clients
to communicate with the with this cloud
in the middle by sending in
subscriptions and the cloud will then
pull the content servers in an efficient
manner so as I said this is a
publish-subscribe system in particular
it's a publish-subscribe system that
interfaces with the existing web many
publish-subscribe systems have failed or
have even been unable to be deployed
because there's a requirement on them
that the content servers must push the
data to the clients in other words it's
not compatible with the poll based
architecture of the web so it's
important that this is backwards
compatible with the web this cloud in
the middle achieves an efficient polling
of the content servers by using
optimization framework that's under the
covers here and I'll explain that later
in the top but this provides a way to
most efficiently pull the content
servers for the data we're interested in
near the end of the talk I'll talk about
the implementation of this system as
well as how well it performs and a wide
scale deployment so our main focus here
is what's inside this cloud how can how
can we make it efficiently pull the
content servers so we have several
underlying properties or goals that we
want it to have first of all it has to
be scalable we have an increasing number
of clients we have an increasing number
of nodes in our system that is only
growing as the number of subscriptions
increases and so it should be able to
tolerate large workloads larger than we
can foresee right now second we want it
to be robust there should with this
system should be able to tolerate
multiple failures without degrading the
performance in the system and lastly we
need it to be distributed this system
should be distributed because it's
inconceivable that anyone node in the
system could possibly pull on behalf of
all the clients and all the
subscriptions in the system this is
especially true because of what I said
about the
content servers they've imposed a
bandwidth limit there's no way for a
single channel to even our single node
to even pull for a single channel
because it would only be able to pull
every half an hour or so so we have a
distributed Network here each node
polling on behalf of the clients and in
order to provide some of the guarantees
some of the underlying properties that
we would like our system you have we
organize it as a structured overlay
Network this provides some guarantees
that i'll talk about later and it also
sets up our problem of choosing which
nodes pull each channel in our system so
intuitively we want more nodes to pulse
to pull channels that are more popular
and fewer noticeable channels that are
less popular this depicts it a concept
called cooperative polling what this
means is that the nodes all pulling one
channel are sharing updates with each
other so that they can detect the
updates more quickly in particular we
frame this problem as a resource
allocation problem we want for each
channel to choose which nodes poll for
the channel and how many nodes pull for
the channel those are the two important
key factors here so how do we do this
well one way we could do this is to
introduce some heuristics and there's
some pretty intuitive ones for example
what if for each channel in the system
we pulled it with a number of nodes
proportional to the number of
subscriptions for that channel there's
some channel with 50 subscriptions it
should be pulled with twice as many
nodes as a general with 25 subscriptions
this makes sense but I'll show that
there's a better way to do this instead
of relying on heuristics in order to get
the performance we want let's frame this
mathematically and so we can achieve the
optimum optimal number of nodes pulling
for each channel in order to most
effectively make use of our resources so
the mathematical optimization realizing
coming up with fundamental trade-offs in
our system and as I show as I'll show
you there's a fundamental cost to
pulling these nodes and a fundamental
performance trade-off we want to be able
to detect updates as quickly as possible
using as small as a little bandwidth
there's a few poles to the content
servers as possible and in order to set
this up when
some efficient way of optimally choosing
these nodes and we need it to adapt to
the system as the number of
subscriptions changes our system should
constantly be reorganizing itself
changing what's not rich nodes are
pulling which channels in order to keep
the performance as high as possible so
in order to do this we need to set
several performance goals these
performance goals allow us to say what
we want our system to achieve in a very
formal mathematical way corona is a
general enough framework that we can
come up with any number of performance
goals and I'll show you a couple that
we've implemented in our
publish-subscribe system for the web so
one such goal is called Corona Light in
this case we set up the optimization as
follows we want to detect updates as
quickly as possible over averaged over
all the subscriptions we should make the
delay between the contents changing and
the clients detecting that change as
fast as possible such that we don't go
over a certain network bound say for
example we don't want to impose any more
load on content servers than they
currently see through RSS or perhaps the
network has a certain amount of
bandwidth available to it system
designers should be able to choose that
value in order to customize their system
I'm a flip side of this suppose that we
want to know that will detect updates on
average only 30 seconds after their
updated for example some set time value
to do this we can set up the
optimization problem by minimizing the
total amount of bandwidth that the
system uses while guaranteeing that the
average subscription detects updates in
a certain amount of time later in the
talk I'll introduce another optimization
goal that we've set called a Crone
affair I'll bring that up again in the
evaluation section so in order to
achieve these performance goals we take
advantage of certain properties of our
underlying structure overlay so here's
our structured overlay with each of the
machines in our system and they're
organized in a logical ring structure
the logical ring structure is a circular
identifier space and each node has some
identifiers here now remember our goal
here is to choose how many nodes are
pulling each channel
in order to do that we need some way of
formally and uniquely identifying
clusters or groups of nodes that will
perform this cooperative polling for a
channel as I've talked about so to
formally come up these with these
clusters we introduce a notion called
polling wedges so suppose that some new
subscription or new channel enters our
system and it gets randomly hashed to
this red node here with a code with
identify or 2200 that's called the home
node for the channel and for the time
being it'll be the only node polling for
this channel however this node somehow
decides that there should be more nodes
polling for it it can increase the
polling wedge of the cooperatively
pulling nodes by assigning nodes
surrounding it with a common prefix to
pull as well so in this case we define a
level one polling wedge is all the nodes
that share the first three hex digits
with our home node likewise if we want
to increase the number again we can go
to a level two polling wedge and make
the prefix slightly shorter so notice
that since we're doing this based on
prefixes every time we increase the
level by one we're increasing the number
of nodes pulling by an exponential
number that's growing exponentially
since we're matching a bigger and bigger
prefix or smaller and smaller prefix and
once we define what our polling wedges
it's exactly those nodes that can be
uniquely identified that pull the
content server so why did we do this why
did we come up with this polling wedge
concept so as I said we want to find how
many nodes to pull each channel one way
we could conceivably do this is for
every channel choose a value between 1
and n where n is the number of nodes in
our network and choose exactly those
fine-grained control of exactly how many
nodes are pulling each each channel but
this is a huge problem it's difficult to
choose a value between 1 and n for every
channel so it's to optimize our
performance goal and so by introducing
these polling levels we've reduced the
problem complexity from n choosing
between all the nodes to just log in
choosing the size of our polling ledge
we're pulling wedge of 0 again is just
the home node and a polling wedge of
login level login would be
all the nodes in the system polling so
now that we've set up this convenient
concept of pulling wedges we can more
easily model our optimization
mathematically so as we see here the
number of nodes polling we can compute
as B to the l or again L is the polling
level in our system and B is just the
base of the underlying structured
overlay and now we can calculate both
the performance and cost of our system
here we have the update detection time
as our performance and we have the
bandwidth costs or how much data were
polling in total from all the content
servers with these two we can set up
this is just an example optimization
problem we can set up that uses this you
were minimizing the performance or
minimizing the update detection time
while guaranteeing that we don't use any
more than it oh sorry we're minimizing
the total vote on the network while
guaranteeing that we detect updates and
at least or at most time t ok so I'm
only going to give a high-level
description of how this optimization
part works I refer you to our paper in
order to get the more mathematical
details but in order to cover all of the
math and this would require a longer top
so but i do want to give a high-level
description by saying that once we have
this general optimization problem we
recognize that it's NP hard because we
need to choose an integral value for
each of the levels in the system this is
reduces from solving the multiple choice
knapsack problem which we know to be
np-complete however we've come up with a
fast efficient optimization
approximation algorithm for this
optimization there runs in time md log
md we do this by taking our optimization
problem and transforming it using
Lagrange multipliers into a single
expression that needs to either be
maximized or minimized in doing so we
introduce one new variable lambda but
notice that for a fixed lambda we have m
independent equations one for each
channel in the system that can be
minimized independently so we need to
choose a value of lambda and we do this
by first recognizing that
there are a finite number of lambda
values that could possibly affect the
solution and we can pre sort these
recompute and sort these values of
lambda and then use standard bracketing
algorithm techniques to zero in on our
value of lambda and then independently
maximize or minimize the other equations
are expressions so the takeaway here is
that this is fast and simple and it's
near optimal we've shown that this
algorithm comes within one channel per
node of the optimal solution so m by
running this periodically we can adapt
to changing workloads in our system so
the question is how do we do this in a
decentralized manner so if I've giving
you this optimization problem assume
that we just have global knowledge about
the whole system but as I said it's a
decentralized system each node here
cannot talk to a centralized Authority
because introducing a centralized
authority would bring down the
robustness requirement we don't want a
single node failure to bring down the
system instead or so what we will have
here are three questions that you should
be asking yourself as it relates to this
decentralized essential ization how do
we achieve this optimization algorithm
in this decentralized manner once we've
chosen the nodes that are pulling each
Channel how do we assign how do we
assign the polling level to that channel
once we know the level that we want and
when a node detects an update how does
that update propagated among the nodes
that are pulling the channel so starting
at the top how do we solve the up in
this optimization problem and
decentralized there well one thing we
could do is propagate information
between all of the N nodes so that each
node has a complete view of the network
in other words each node knows some
subset of the channels in the system and
we could have the other nodes pass all
the information that they have about all
of their channels however this is highly
inefficient passing every single piece
of information at every node knows to
every other node is going to be too
time-consuming and bandwidth intensive
to do here so instead we have nodes
approximate
or we have nodes approximate trade-offs
based on their none of the non local
channels and knows about its own
channels but we're going to have nodes
aggregate coarse grained information
about the other nodes about the other
channels in the system and send those
the other nodes so in particular this
curve here represents the performance by
cost on the left hand side and all the
channels in this whole system sorted by
popularity across the bottom so if each
node knew this curve perfectly then it
would know where its channels fall in
this curve and it would be able to solve
the optimization problem locally based
on that based on what it knows about
this Kurt however each node only knows
about its own channels initially in
other words it has a few sparse points
on this graph so what we do is we split
the normalized performance versus cost
on the left into a number of constant
size buckets and each node looks at the
channels it has in each of these buckets
and aggregates the information about
those channels into one cluster so one
questar would represent one bucket on
this p p / c and using these clusters
each node can then come up with a has a
close enough approximation of the global
view of the system in order to run the
optimization algorithm locally so once
we have these clusters we need to
disseminate them to all the nodes in the
network how do we do this we rely on the
underlying structure of the overlay
Network in other words each node can go
out to any other node in log time that's
one of our requirements of the
underlying system and so by if you
imagine that each home node a tree
rooted at that node we essentially
propagate these clusters that the nodes
have come up with up to the home nodes
and so eventually by each node passing
the clusters to their one hop neighbors
every node has an approximate view of
the system at this point each note runs
the optimization algorithm locally based
on what it knows and uses the same
underlying hierarchy in order to
disseminate the polling levels that the
nodes decide to define the polling
wedges
now you may notice that it's possible
that two different nodes since every
node is polling here or every node here
is running the optimization algorithm
locally that they may disagree at some
points there may be two nodes that both
looking at the same channel one decides
it should be at pulling level three and
another decides it should be pulling
level four but that's okay at worst the
polling wedge will be one size too big
or one size too small and in a few
minutes the system the system will
stabilize so it's okay if one wedge
pulls too many times in a short you know
34 minute period of time so once a node
detects an update we need to get that
update both to the client and to the
other nodes pulling for this for this
channel again we can use the same
underlying structure of the overlay if a
node detects an update it can strip away
all the old stuff in that update and
just leave the new piece of information
because all the other nodes assume
presumably know what has been seen
already and then it can pass that new
information to the home node which can
then disseminate it to the other polling
nodes in this wedge and likewise the
node detecting the update can somehow
send a message back to the client so we
I've described the the how the system
works under the hood but I haven't
talked at all about the implementation
of the system how we actually built it
in some of the problems we ran into so
we layered this on top of pastry which
is just a structured overlay it's out of
rice university and we've set it up so
that it monitors RSS feeds and atom
feeds as well as detecting changes on
web pages we do this using a difference
engine which is a small module that we
build that can compare the old polling
results to the new polling results in
order to check for data in our specific
application for the front end of our
system we use an instant message
protocol in order to quickly send the
updates to the users who are interested
let me I'm going to talk a more detail
about this difference engine and how it
can be customized for different
applications system designers can build
these and plug them into the system in
order to pull different kind
of data for example when if some
researchers want to design one of these
first sensor network they could design a
difference engine that knows how to read
the data out of their sensors we've
built two specific difference engines
one for news feeds RSS and atom feeds
and another for detecting changes in
generic HTML pages I talked about this
front-end system just a tiny bit let me
introduce that first notice that all
we've talked about here is what's inside
this cloud the the polling nodes for the
content serves we call this cloud in the
content servers the back end of our
system we introduce an instant message
front end to this system which is
actually what communicates with the
clients I'll call this the instant
message gateway but just think of it as
a machine that connects to all the
instant message protocols we use
currently AOL instant messenger yahoo
instant messenger and msn messenger the
clients consent just regular instant
messages to this IM gateway and the IM
gateway can then forward these to the
krona cloud to any node in the chronal
cloud and the chrono cloud knows how to
handle it then when the corona cloud
detects a change any of the nodes detect
the change it can again make this just
encapsulate just the change to the
content and send it back to the instant
message gateway our aim box let's say
that box can then send the new
information to the clients in just
instant message format we've decided to
pull this instant message gateway out
because of limitations that existing
instant message systems place on our
system in particular they don't allow us
to log in from many points of private
presence at the same time so ideally
we'd like to log in from every node in
our system into the same screen name
then any node when it detects an update
could immediately send that update to
the clients however since we can only
log into one place and instant message
services generally only allow a single
user to send a certain number of
messages per amount of time we've pulled
this out into this gateway that handles
all of this for us so our backend still
operates exactly as before detecting
updates according to our optimum
engine and the IM gateway is just the
front end that could easily be
replicated in order to provide more
robustness or fault tolerance so by
having clients interact with our system
using this instant message front end we
make it very easy for them to adopt the
system they don't need to learn any new
software in order to do to use this
because most people are already familiar
with how to use instant messages so by
sending a single message they can
subscribe to any website or feed they
want and they receive updates while
they're working if this becomes
bothersome during work if you're getting
too many items you can use different
features to restrict this you can put it
in digest mode you could ignore certain
channels for a certain amount of time
and things like this just to give you
finer grained control over how the
system is acting for you so in
particular the most important benefit of
using an instant message front end is
that we have pushed based notification
while we can't change the content
servers and ask them to push updates to
our system we can once we find these
updates push them directly to the
clients this relieves the clients from
having to pull such and such as an RSS
it allows them just sit there and the
updates will come to them as I mentioned
this is easy to adopt and as an added
side effect bonus we get front-end
authentication users have to log into
their favorite instant message services
in order to use it and by logging in
there guaranteeing that they can only
see the channels that they have
subscribed to and receive the updates
for those channels so we've built the
system and i want to show you how it
performs in the wild so first of all we
recognize that since we're comparing
this to RSS we could not find any RSS
trace studies in order to form as a
basis for comparison and so we did a
trace study at cornell between the
gateway between the computer science
department and the rest of the
university for a period of time and just
collected statistics and then also at a
wider scale we tracked a bunch of poles
at Syndic 8.com
a bunch of RSS feeds that we've been
able to track in order to learn how they
update and how users interact with them
we've also tested our system against
this foundation both in simulation at a
very large scale and using planetlab in
order to do a wide area deployment of it
and see how it works in practice so in
looking at this in a in simulation mode
the large scale the large-scale
deployment with a lot of nodes we see
that on the left we have the update
detection time how quickly we're seeing
the updates and across the bottom we
just have the time of our six-hour
experiment and we see that legacy RSS
has a constant update detection time
each note is polling periodically and
getting updates every 15 minutes on
average however notice that Corona Light
is performing much better than this
remember that Corona Light is the
performance goal where we want to use
the same amount of bandwidth but limit
the art but mix maximize our performance
in other words in other words detect the
updates as quickly as possible so when
you see here that we gained more than an
order of magnitude and we do even better
with corona fast where we set an update
detection time of 30 seconds in this
case so it's not surprising that that
does level off at 30 seconds and it also
isn't surprising that we're able to get
this update ection time this slow after
all getting update detection time is
just as simple as making for example
every node poll every channel but the
real the real beauty here is that we're
not doing that we are efficiently
choosing which node should pull so that
we don't increase the bandwidth of our
system so here RSS uses a constant
bandwidth but that same corona light
which achieved more than in order to a
magnitude of order faster updates is
using no more bandwidth than RSS across
the system Crona fast is using slightly
more bandwidth but with this mirror
increase of about twenty percent
bandwidth we are achieving a much faster
update detection time our target of 30
seconds and on the left here you see two
spikes and those are just the amount of
time it takes the system under this
deployment
to stabilize in order to propagate these
clusters and run the optimization
algorithm and again this could be
tweaked by having the system designer
shoes how quickly these propagation
values move around the system so I
talked briefly about using heuristics in
order to solve the same problem we have
this resource allocation choosing a
number of nodes for each channel so
again we have legacy RSS on the left of
900 seconds 15 minutes on average update
time and now suppose that we did use
this proportional heuristic that I
mentioned we have a number of nodes
polling proportional to the number of
subscriptions for our for the channel
here we see that we do indeed perform
much better almost three times better
than just regular RSS and we achieve
even better by using the square root
metric instead of going polling with the
number of notes proportional to the
number of subscriptions we pull with the
number of nodes proportional to the
square root of the number of
subscriptions introducing a square root
in this fashion is a common thing to do
when you're trying to maximize things
like this using heuristics but in this
case we see that Corona Light performs
much better than obviously legacy RSS
but also then the two heuristics that I
talked about and Corona fast performs
even better albeit with slightly more
bandwidth about twenty percent as you
saw a chronal light uses the same amount
of bandwidth on the left but performs
much better so the question is is it
possible to design heuristics that
perform as well as Corona Light it could
be we could we could sit here and come
up with some heuristics and try them out
and test them and see if we can get that
bar a little lower but the problem is
that heuristics don't provide any
guarantees on how well this is going to
work and indeed when the when the
workload in the system changes if the
number of subscriptions increase or the
distribution of the subscriptions over
the channels drastically changes we
might have to sit at the drawing board
again and design a new heuristic whereas
with krona we've designed one
optimization that we know we want our
system to achieve and it propagates the
values within the nodes and does it on
its
so now let's look at a slightly
different view of our channels in our
update detection times in the system
instead of looking at an average of all
of the nodes of all the channels in the
system let's look at each one
individually so here we again have
update detection time on the left again
a log scale and across the bottom we
have each channel in the system sorted
by how quickly that channel updates on
the content server so how like a new
site might update more quickly than a
blog for example here we see that
there's a cluster of nodes are a cluster
of channels rather that are updated very
frequently the content server there are
constantly changing and yet we're not
detecting the updates that quickly
depending on the application this might
not be desirable and so we could go to
the drawing board and come up with
another just optimization algorithm that
would help achieve something better in a
particular application where we want to
quickly find updates in a channel that's
constantly changing so I've already
introduced to performance goals that
I've talked about within the previous
experiments we introduced another one
called Corona fair by taking into
account the update interval of the
channels as well as the update detection
time we come up with a different
optimization problem that favors those
channels that are updated more
frequently using this corona fair metric
by introducing this new performance goal
I hope I've convinced you that we can
just come up with new optimizations and
just plug them into the system and the
system will know what to do with them
looking at this back at the same graph
we just looked at before we add some
blue nodes which are the update
detection times of the channels once
we've employed this Corona fair goal and
in this case we see that we have
increased the increased the performance
of those channels that are updated
frequently at the cost of detecting
updates a little more slowly on the
nodes that aren't updated is often so
the system designer can design their own
optimizations based on what their
applications what the clients want and
what the application is used for so in
addition to doing the
simulations on a large scale we've also
done a wide scale deployment on
planetlab planetlab is a distributed
testbed over the internet that allows
researchers to deploy services at a
large scale and experiment with them in
more realistic settings as you see here
we have a significant number of clients
we're building up steam and I think you
should watch out because we might be
cutting into your revenue soon but let's
take a look at what planetlab is doing
with its wide scale deployment again we
look at the same corona light metric
trying to optimize the performance and
we see that we have indeed increase the
update perfect performance by more than
an order of magnitude this is over about
7500 real channels with a natural
distribution of subscriptions on those
channels totalling 150,000 subscriptions
and again we have not gone over the load
imposed by legacy RSS because that's the
bandwidth limit that we set we don't
want content providers to see any more
load than they already do so we've shown
you that we have this high performance
system that we've actually deployed on
the web backwards compatible with the
web and people are currently using it in
order to detect updates to their
favorite sites I've also shown you that
it's easy to create these optimization
functions based on the application and
I've shown you three in particular so
the system overall is was practical and
efficient to deploy a large scale so
there are some future directions we want
to take this and if any of you have any
insights on this I'd be happy to talk to
you afterwards one particular direction
is looking at content-based
publish-subscribe systems so so far
we've been looking at a topic based
topic-based subscriptions in other words
each user subscribes to a specific feed
or a specific web page and whatever
appears on that web page is sent to the
client but for some subscription or for
some systems it would be useful to be
able to subscribe by keyword for example
looking for any occurrence of
nose or maybe earthquakes in this area
over all the news sites and getting
instant messages when it finds those in
addition we've been looking at we've
been talking to the database group at
Cornell about adding support for this
and predicates to our to our keyword to
a keyword-based subscription service
where we could say things like when this
when my favorite stock increases by more
than five percent a day I want to be
alerted so by / by allowing clients to
come up with more complicated queries it
gives them more control over the updates
that they receive another thing related
to this problem that we want to look at
can be expressed by taking away the
clients of our system and adding some
legs and eyes to our cloud you obviously
all recognize this as a web crawler this
the problem here is very similar we want
to again pull the content servers in the
most efficient way possible in order to
keep for example a web cache fresh so
here given a set of pages we
continuously crawl these pages it's a
type of web crawler we're designing here
and update the or detect catch the
changes in the system as quickly as
possible this problem is essentially the
same the only difference here is that
we're looking at a different goal or a
different system that we want to
optimize instead of trying to get
updates to clients as quickly as
possible we want to detect changes in
put them perhaps in a web cache so we
need to create a new freshness metric
and that would be the first step towards
achieving this goal after we come up
with this again we have a choice of what
we do with it we have a metric and we
can maximize that metric more
constraining bandwidth like before
that's similar to Corona Light or
minimize the bandwidth that we use in
order to achieve some set performance in
our in our cash for example depending on
how we structure the freshness metric
maybe we want it to be seventy-five
percent fresh whatever that means but
the point here is not that we have a
specific metric but that by coming up
with a metric we can plug it into the
system and have it stabilized on that
ization so some past work by jang hochu
and hector garcia-molina have looked at
how to optimally pull web caches in
order to optimize some metric the way
they have two metrics freshness and age
and I'm not going to go in detail what
these are but our work is different in a
number of ways first of all their
freshness metric is such that pages that
update too frequently are not hold at
all this may be undesirable for some
applications you might want to guarantee
that everything in your cash is at most
a certain age but so they've analyzed
these two metrics and come up with a
closed-form expression for polling based
on these but the point of kron is that
we have a general framework where we
could plug in these two metrics or come
up with more and without analyzing them
mathematically plug them into the system
and see how well they perform for our
application also corona is a distributed
system so we can scale the larger sizes
than simply simply being handed a
closed-form expression that you would
have to use in a centralized fashion
there's also been a lot of related work
on publish-subscribe systems again I've
talked about topic-based versus content
based systems whether we use keywords of
just subscribe to specific data sources
and again corona is a topic based system
but differs from previous work and that
it uses this optimal fundamental
trade-off between the between the
resources in the system in order to
achieve a goal there are many
peer-to-peer overlay networks so we've
built ours on top of pastry but there
are plenty of other ones that provide
similar guarantees the only the only
guarantee that Corona needs is that the
underlying network supports log time
hopping long time number pops from one
node to another in the underlying
network and so it could be built on top
of these other networks and it would
work just the same and lastly there have
been other micro news dissemination
projects in particular feed tree for
example also uses this cooperative
pulling technique in order to detect
updates more efficiently but
creates a tree over the clients based on
heuristics and using these heuristics
the clients know about their neighbors
and can forward the updates to the
clients through the street again we
we've chosen not to rely on heuristics
as we believe it perform provides better
performance in a general sense so in
conclusion we have a new approach that
we've introduced for building systems
using fundamental trade-offs between
resources and coming up with
optimization problems and we've shown
you that we have an approximation
algorithm that for a wide range of these
optimization problems we can achieve
near optimal performance quickly and
stabilize with changing workloads I've
also shown you an example prototype
application that we've ran simulations
on and even deployed in a large scale
that exemplifies this systems approach
in practice so overall I believe that
instead of relying on heuristics that
systems become larger and more complex
it's important to look at some of these
fundamental trade-offs in the system and
decide how we can best take advantage of
for example the cost and performance in
order to guarantee that our systems are
behaving most efficiently even as our
systems become much larger than they are
today and thank you with that I'll take
questions yes I'm interested in the
bunker
so how people in your life that okay
when they really haven't right so so the
two different engines we've built do not
deal with this because we're looking at
feeds in particular we designed this
originally as an RSS publish-subscribe
system that performs better than legacy
RSS and so we look at RSS feeds which
have a very specific format and by
studying or just by looking for the new
headlines in this format which the
publishers placed there we know which
ones are new and when we detect web
pages right now we're we're just looking
for changes and we're not looking for
things that the client might necessarily
be interested on that web page but the
idea is that it's general enough that
someone could build a difference engine
using any number of techniques that
detects what they want from the data
sources yep
very need the third okay right like this
yeah the explanation here would have to
be that since we're dealing with these
exponentially growing polling wedges as
we increase the polling level the
polling watch is getting has
exponentially more nodes in it that in
this case it must be that because of our
distribution of subscriptions we have
some nodes that fall in a pulling wedge
of some size and some nodes that pull
that fall in a poem edge of the next
size and in this case those those are
those updates are being detected that
would be twice as fast let's see all
right so right so I mean one solution to
this would be for example if like I said
it would be very nice if for each for
each channel we could choose exactly how
many nodes are pulling that channel you
know but an integer between 1 and n
whereas we're using an exponential
pulling pulling wedge so I be happy to
hear any techniques that you might have
for increasing in granularity of our
optimization algorithm
a bit pattern on the next
yeah we are just looking at the prefix
of that and that was just an example
we're not you know our identifies are
longer and we're using different-size
prefixes but the idea is that we are
just using the prefixes of this in order
to choose which nodes are pulling mmm
all right thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>