<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Engineering a Fast PCIe-Attached Storage Array for Next-Generation Non-Volatile Memories | Coder Coacher - Coaching Coders</title><meta content="Engineering a Fast PCIe-Attached Storage Array for Next-Generation Non-Volatile Memories - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Engineering a Fast PCIe-Attached Storage Array for Next-Generation Non-Volatile Memories</b></h2><h5 class="post__date">2011-01-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hA_947C01Og" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for coming today we're fortunate
to have Steve Swanson here from UCSD to
talk about non-volatile memories I met
Steve at a conference that he hosts
there at UCSD a non-volatile memory
conference and they're having the next
the next one in March so if you find the
topic interesting today you might think
about attending that workshop I'll I'll
turn it over to Steve and let him talk
with us thank you
all right thanks thanks for having me
here today so I'm going to tell you
about some of the work that we've been
doing at the non-volatile Systems lab at
the University of California San Diego
which is a lab I run with another
faculty member Rajesh Gupta and of
course a whole bunch of graduate
students who actually do all the work
and the purpose of the non-volatile
systems laboratory is to look at
emerging solid-state non-volatile
memories and understand what their
implications are going to be for
computer systems and we do a bunch of
different work kind of in that very
broad area and today I'm going to tell
you about a prototype SSD that we've
built that targets some of the very fast
non-volatile memories will be coming
down the pipe in the next few years that
will probably replace flash memory and
some applications so just kind of my way
of motivation to give us all in the on
the same page you know we're kind of at
the cusp of a revolution in the way the
computer store and access data and we
can sort of get a handle on this by
looking at the change in i/o performance
over the past few years so for the
past's past several decades we've used
hard drives hard drives are pretty slow
if you're doing random 4 kilobyte reads
in this case I've shown measurements for
a 4 disc raid array the latency is going
to be about 7.1 milliseconds and the
sustain bandwidth you can get will be
around around 2.1 hour pardon me 2.6
megabytes a second now since the hard
drives were introduced processor
performance is increased by a factor of
maybe 200,000 or so hard drive
performance is improved by much smaller
factor but that's about to change and if
we look at sort of the current
state-of-the-art in storage we
choose a PCIe PCIe attached flash-based
SSD that'll give you about a hundred ex
relative to disks in both latency and
bandwidth and so that's pretty good but
if we look a little farther down the
road and look at some of the memory
technologies like phase change memory or
sprint or transfer memory that might
replace flash will be a lot faster than
flash and put that into the same kind of
device we can get latency is down to
about 12 microseconds and our bandwidth
up to 1.7 gigabytes a second that's
actually more limited by the
interconnect than by the actual memory
technology so that's a factor of about
700 7.62 700 in terms of latency and
bandwidth relative to disk and what's
you know interesting about that from a
computer architects perspective which is
which is what I am is that I'm a
compounded basis that's about 3x per
year in terms of latency and bandwidth
improvement which is much higher even
than we saw in sort of the heyday of
Moore's Law 1 processor performance was
increasing very very quickly
and so this is a very abrupt and very
large change in one particular aspect of
computer system performance and we can
expect that and we've seen as we built
this prototype that has impacts all the
way throughout the rest of the system I
should also say I'm happy to take
questions during the talk I probably
don't have enough slides to fill the
whole hour slot so if you ask questions
it'll make it more interesting for
everybody so just take a look at latency
in particular and understand why this
might change systems pretty
fundamentally I've taken here the
latency for a 4 kilobyte random right
and broken that down into 3 different
components the green is the hardware
access time the red portion is the file
sister is the operating system time and
the blue portion is the file system time
the horizontal axis is microseconds on a
log scale and what you can see is that
for the disk the hardware access time is
about six orders of magnitude larger
than the software access time and as we
move to flash which is sort of where we
are now it gets better but it's still
about four orders of magnitude so
nothing is really fundamentally changed
about the system software is still very
cheap compared to the the hardware costs
and as we move to something faster like
PCM
which is what I'm going to be talking
about today there really is a profound
shift in where the latency lives so now
we're at a point where the hardware cost
is on the same order of and maybe even
smaller than the software latency that
we would see in a conventional system go
ahead so this is these are measurements
off of the prototype system I'm going to
talk about today oh sorry
the question was how did we get the
breakdown for the PCM memory technology
and so this is off of the prototype I'm
gonna describe measured values alright
so how would we go about architecting a
solid-state drive for one of these fast
memory technologies so what we found is
that both the hardware and the software
components of the system are going to
limit performance in different ways and
so we'll need to deal with both of them
and that getting the actually the
interface between the hardware and
software is very critical to getting
good performance out of the system if we
can do that if we can co.design them
together we can get increased bandwidth
significant decrease in latency and also
a large increase in concurrency as also
see we can actually reduce the amount of
CPU time required to do i/o as well so
with that sort of motivation out of the
way this is the outline of what I'm
going to talk about first I'm going to
tell you about our prototype system it's
called Minetta I'll tell you about its
architecture and how it works then I'll
describe some of the optimizations that
we've made to Minetta to try to get the
kind of performance out of it that we
would like to see I'll talk about a
little bit of application level
performance analysis and then I'll wrap
things up so this is Minetta this is the
the storage architecture that we've
built and it's sort of give you a tour
of my data I'll go through the lifecycle
of a request as it's made from the host
side so Minetta connects to the host
system over PCIe which provides - you
can think of it as two interfaces one is
a programmed i/o interface and the other
is a DMA interface so to issue a request
the host machine issues a Pio right
which transfers information about the
request to hardware that goes into a
request queue when there's room in the
scoreboard the request will go into the
scoreboard
the scoreboard is going to keep track of
the accesses progress through the rest
of the system in this case we're looking
at a right so we've got to grab some
data from main memory from the host we
do that by allocating a transfer buffer
to hold that data then we talk to the
DMA controller which will go pull it out
of main memory once we've got it we can
push it over our ring inner or the
network interface that hooks up to a
ring network that runs at 4 gigabytes a
second and connects to four independent
bank's of our non-volatile memory in
this case I've drawn it as phase change
memory but it could be any of these fast
non-volatile memories that'll be coming
down the pipe we do that transfer the
ring tells us when it's finished when
the scoreboard sees that the request is
complete it sets some status registers
and then it raises an interrupt so that
the host can come back and check the
status of the commands and complete the
commands in the operating system above
the hardware there's a bunch of other
layers there's the PCIe interface so our
prototype uses a buy eight PCIe 1.1
interface which gives us two gigabytes
in each direction for reads and writes
on top of that there's a driver then
there's the rest of the OS IO stack a
filesystem if one is present and of
course at the top the application and
all of these different layers are going
to add latency and we're going to focus
on the software pieces and how we can
minimize that latency as much as
possible so these memory technologies
I'm talking about just to give you an
idea of what memories Minetta is built
to work with we're looking at these fast
non-volatile memories and the idea here
is that they'll they should be memories
that have DRM like speed and also a DRM
like interface and there's a bunch of
different memories that are sort of
competing to sort of be the memory that
we use in applications like this
things like phase change memory spin
torque transfer memory the memory stir
racetrack memory there's a whole bunch
of them no.not is more or less agnostic
about which memory technology you use
I'll talk a little bit about our
sensitivity to things like device
latency later we're just building for
one of those technologies here for the
results I'm going to present today we're
modeling phase change memory which is
sort of the nearest Inn of these
advanced non-volatile memories we have
prototype phase changed
in our lab so you can actually buy these
there's you know someone just cited that
there's someone shipping a cellphone
with a phase change memory device in it
it requires a little bit of wear
leveling much like flash but the wear
leveling is much simpler and just sort
of for completeness up here we we use
something called start gap wear leveling
which was presented at a academic
research conference last year so this is
our prototype that we've built the the
system is built upon something called
the b3 prototyping system it's a box you
can buy it has four FPGAs and 64
gigabytes of DRAM we use the DRAM to
emulate the fast non-volatile memories
that were interested in the Box connects
over PCIe 1.8 our design and the FPGA is
runs at 250 megahertz and the way that
we emulate the PCM memory is by
adjusting the memory timings inside the
memory controller so for this work we've
modeled a read time of 48 nanoseconds on
a write time of 150 nanoseconds which
matches kind of aggressive projections
for a PC I might be in a few years the
current devices are a good bit slower
but we can modify the timing so this is
modeled a at a very high degree of
accuracy so if we had a memory like this
we can be pretty certain that this is
how such a device would behave so that's
sort of the the Minetta architecture the
whirlwind tour now I'm going to talk
about how we went through optimizing
Minetta and the changes we had to make
to get good performance so when we built
the first version of Minetta we ran it
underneath the standard Linux i/o stack
we just built a block device with a
low-level block device driver and then
ran some i/o benchmarks and we found was
that the performance was actually
somewhat disappointing so our
measurements showed that our Hardware
latency for a 4 kilobyte access was
about 8.2 microseconds but the software
latency was about thirteen point four
microseconds and this includes a bunch
of stuff you can see on the stack bar
graph on the right there's the overhead
of getting in and out of the operating
system the i/o scheduler the copy you
have to do for the data issuing the
request processing the interrupt and
also waiting around for there
quest to finish and so what we want to
do is try to figure out how we can
reduce that software overhead because it
was really limiting our performance and
so the first thing we do is to get rid
of the i/o scheduler and this is a
pretty common optimization other state
of the art solid-state flash based
solid-state drives do this as well the
problem here is that in the standard
Linux i/o stack when a thread wants to
make an i/o request the thread enters
the kernel puts a request onto a queue
and then goes to sleep another thread
wakes up and then pulls it off the queue
and actually issues it to Hardware
there's two problems with that one is
that you end up paying two context
switches to issue the request and the
other is that the i/o scheduler thread
kind of a serializing effect on the
accesses that go out both of those are
bad so if we just get rid of the i/o
scheduler completely so that the thread
that comes in from user space actually
issues the request performance improves
we get about a 10% decrease in latency
and we also get an increase in
concurrency which we'll see in a couple
slides that that turns out to be
important as well the next step was to
make our tag management more efficient
so like a conventional SATA SSD our
system has multiple tags and every
request every concurrent request that's
in the storage array has a unique tag so
to make our tag management more
efficient we went through and profiled
kind of where it was spending its time
and it turned out it was all about
contention for locks so we improved our
lock management code by turning it the
data structure that holds the pool of
free tags into a lock free data
structure so that we can access it
without acquiring any locks and then we
also then we go through and we index all
of the other data structures that the
kernel uses to track the request we
index based on that tag number and so
once you've allocated the tag you've
allocated a bunch of other things as
well
and the final thing that we did is we
use the processor ID as a hint for which
tag the thread should try to acquire and
what that means is that if the hint ends
up being if you get the one that you you
asked for initially that means that all
of the other data structures are already
going to be in your local
on that processor and so you don't take
those cash misses to get that that data
over to that processor and this turns
out to you know we didn't actually
realize how important this was until
we've gotten most of the way through
this process but we're at the point now
with this optimizations that even a
single extra cache miss kind of on the
path through the through the i/o code
ends up being a measurable effect on
overall performance these kinds of
optimizations end up having a pretty big
impact the second thing that we did was
to co.design the hardware and software
interface so that they're a very good
match for each other so in our baseline
design you had to make multiple Pio
requests to issue a single IO operation
and then the driver we had a lock that
would protect those i/o registers so
that you would get a consistent request
written to the hardware at once it turns
out that lock was limiting concurrency
and causing a lot of overhead so we did
is we compress down the request word so
that a single request would fit in a
64-bit word and then we can write that
as an atomic 64 bit store and the PCIe
request will make sure that it shows up
that they'll be serialized on the way to
the hardware this combined with the
previous changes means that there's no
more locks less than our driver at all
so the threads
I don't contend for any shared state in
the common case the one drawback is that
we lost the DMA address in our request
because the DMS DMA addresses a full 64
bits so to get around that we pre
allocate buffers in kernel space and
then write those addresses down to the
hardware and then we index into that
table that's stored in hardware to
determine where we should read or write
data from and in host memory the the
impact of this the combination we get
increased concurrency again and we also
get another 10% reduction in operation
latency versus the previous version the
final change that we made was to use
spin waiting for small requests so in
the standard i/o stack when you issue a
request you go to sleep and you wait to
get woken up by the interrupt that comes
back it turns out that took a fair
amount of time so instead for a small
request request smaller than four
kilobytes
we actually
just spin waiting for a flag to toggle
that tells us that the request is
finished and that ends up saving a bunch
of latency although it does increase CPU
utilization a little bit so the question
is what is the impact on programmability
from pre-loading the DMA DMA addresses
into hardware from the if you're a
device driver author it actually makes
things pretty simple when you request
when you go and allocate a tag you can
look up what that what the Associated
DMA buffer is and that's just the target
that you use for the DMA in the
conventional driver there'd be some
targets that you'd use anyway unless
you're doing something very aggressive
like zero copy this doesn't really
impact things I'll talk a little bit
about zero copy in a moment and what the
implications are from that perspective
from the app from the applications
perspective nothing changes at all it's
still a normal block based block based
interface yeah so we're still we're
still in that same that's a model
although we don't be this this sort of
does get rid of the ring buffer I mean
there's still a series of buffers that
you use but they're allocated more
dynamically so this gives us 54 percent
reduction in software latency versus the
previous version and also total is a 62
percent reduction in latency versus the
the baseline design and you can see on
here that we're down to about five
microseconds of software overhead from
the previous you know thirteen or
fourteen or so yep we're actually spin
waiting for a a well where the the
interrupt handler wakes up and so the
question is why are we spin waiting why
do we have interrupted for spin spin
waiting in this in this work the
interrupt handler sets a flag that the
thread is spinning on you could also do
you could also issue a DMA from the
device to set the flag directly we're
working on that now that's an
optimization looked up for some other
reasons but it also turns out you kind
of need both anyway because you don't
want to always be spinning so for a
large request spinning is a horrible
idea
it's only for the smaller requests that
spinning pays off all right so this was
all latency but we can also get some
good improvements in concurrency and
overall bandwidth so this is our kind of
the graph we draw to sort of evaluate
our storage array the horizontal axis is
access size so these are random accesses
of of a given size the vertical axis is
sustained bandwidth that our array can
do and this is all from user space using
an IO IO testing tool so the the bottom
blue line is our baseline design
performance is pretty poor for small
requests it increased increases pretty
rapidly as a request size goes up and
tops out at about 1.7 gigabytes a second
that's sort of the the limit of the PCIe
bandwidth minus the PCIe overhead the
line at the top is the full PCIe
bandwidth the theoretical peak and then
as we add these different optimizations
we're basically going to push this curve
over to the left so when we get rid of
the scheduler you can see our low the
performance for the small accesses goes
up by quite a bit as do the request for
or the the bandits for a lot of the
smaller request sizes and then as we add
additional optimizations whoops things
get better and better here's you can see
for atomic atomic starts paying off for
request sizes that about two kilobytes
are larger and then if we have you know
all requests we get actually a huge
increase in through in AI ops for small
requests and this is because of the
increase in current in increase in
concurrency and because there's less
work to do per IAP after we've gone
through and made all of these all these
changes the one interesting thing here
is is the green line for the atomic in
theory the atomic optimization should
increase performance pretty dramatically
for the small requests but as you can
see here it doesn't know what's actually
going on here is that you end up being
dominated by the wait time and so those
benefits don't show up until you get the
the some of the tell you get the the
cost of waking up down quite a bit and
then we get a better performance so we
can do about a million 1.5 kilobyte I
ops which is pretty good for for a
storage array and we think our
limitation here is primarily the PCIe
bandwidth if we had more wider PC of us
we should be able to get a simpler and a
similar number of AI ops for larger
requests as well and there was a
question this is all 16 threads I should
also say this is running on a 2 socket
Nehalem machines there's there's 8 cores
running with 16 threads for all this
data it's an excellent segue also so
this is a CPU utilization across these
different software stacks so the the
horizontal axis is the different
configurations the vertical axis is the
number of CPUs required to maintain a
gigabyte per second of throughput and so
as you can see for the baseline it would
take about 12 of these and haolam CPUs
to get a gigabyte a second going and
this is why this is for 4 kilobyte reads
and writes this is why we don't actually
ever get to a gigabyte a second at that
exercise for the baseline once we've
gone through and made some of these
optimizations in particular removing the
scheduler the CPU utilization drops a
lot and we can do a gigabyte and with
just about 3 cores atomic doesn't have a
huge impact although it gets things down
a little bit more spin weight the CPU
utilization actually goes up this isn't
so surprising since you're spinning
waiting for the requests to finish and
so there's some trade-off there to be
made about performance and CPU
utilization good
this is this is aggregate yeah so in all
these this is everything previously yeah
oh sorry it's four kilobytes and smaller
yeah up to four yeah okay so one common
optimization is for making i/o go fast
that's gotten a lot of press and say the
the networking world is zero copy where
you can actually DMA directly in and out
of the user buffers and PCIe nowadays
supports pretty sophisticated memory
mapping that allows this to be done with
pretty low overhead
we tried zero copy it turns out that
it's faster for us to do the copy in the
driver then it is to do zero copy
because if we do zero copy then we have
to write down the DMA address along with
the request we end up introducing a lock
in the hardware we have looked at also
it's possible to sort of DMA requests
out of main memory and so avoid pio
altogether and that might be a way to
get us back to being able to do zero
copy efficiently another solution would
be to have a little bit of support at
the processor level for instance if we
could do 128-bit atomic write we could
get around this problem as well just
because we could write more data down at
once the same if we could for instance
similar effect we could get if we had
say a cache line size atomic right you
could flush and guarantee that all of
those all that data would go out to the
memory controller in one go all right so
that's the optimizations on the
interface side and the software side
that we've performed to speed things up
but there's also some things to be done
inside the Minetta array itself and one
of the things we've given up by making
the changes we did to software is for
instance we've dropped the scheduler so
now there's no handle the operating
system the system doesn't have any
handle on how those requests are going
to be scheduled so if you want to do
something like fairness you're going to
have to do that somewhere else
that ends up being in the hardware so
there's two things that we've looked at
here the first was a pretty
straightforward change and that was that
since we have a full duplex PCIe
connection we should be able to get two
gigabytes going in each direction
but if we do a combined read/write
workload our bandwidth ends up topping
out at about 1.6 gigabytes a second
which is about the same as it was for
reads or writes on their own so the
solution is pretty simple we just split
our request queue into two so we have a
read Q and a write Q and then we draw
from them kind of in a round-robin order
that guarantees or if there are reads
and writes president we'll get both of
them going and that'll allow us to use
both both directions on the PCIe
connection and then ends up having a
pretty large effect on overall bandwidth
we can get up to just under three
gigabytes a second there's still some
you know lingering inefficiency there
I'd like to see that higher and we're
working on improving that a little bit
more trying to understand what's what
exactly is limiting our performance the
second change was to do round-robin
scheduling between requests to improve
fairness so one of the problems that we
observed in some of our workloads was
that if one thread was making very small
requests a 4 kilobyte requests and
another request was making much larger
say 512 kilobytes requests that they
would get the same number of AI ops but
that actually means much much less
bandwidth for the smaller requests and
so in order to improve a fairness
between these kind of competing request
streams we go through and modify our
scheduling algorithm a little bit so the
way that our Hardware worked even the
baseline is that if you had a large
request the scoreboard would sort of
take one piece of it and go and process
it and then take another piece and go
process it so what we do now is we take
the first piece for a larger quest start
up processing now they put the rest of
the request at the back of the of the
request queue and that means that if
there are any smaller requests waiting
behind it they'll sort of get the next
crack at the at the scoreboard and we'll
be able to get better fairness between
the competing flows of requests so if we
do this we get about a 12x improvement
for the small request bandwidth without
and without impeding the overall total
and bandwidth at all so we're not really
giving up anything to improve the
fairness and I think you know what this
points do you know I'm a computer
architect I like to design chips and so
forth so what this means that is that
all of the many decades of work that
goes in do schedulers now gets to be
redone in silicon and so we can
understand what exactly the right
interface is for configuring this what
the right mechanisms are that will give
you I'll let you implement interesting
and flexible scheduling policies you
know to adapt this to the to the
applications you're looking at so one
really simple thing is you might want to
attach some sort of an ID to this so
that individual threads the requests
would look different to the hardware so
that it could deal with them differently
otherwise everyone's gonna end up
getting treated the same which may not
be what you want yep oh the the i/o
scheduler it is just the context
switches in the serialization so the
first thing you're going to do if you're
gonna get rid of the i/o schedulers you
go and you google and you find out that
that that Linux has a no op scheduler so
that sounds like a great idea but it
turns out that no op scheduler just does
nothing after it removes the the request
from the i/o cue and then issues it to
hardware and it's actually just the cost
of putting the thread to sleep waking up
another thread be queuing the thing just
that whole handshaking process that's
where the overhead is so it's not
actually doing anything at all and it's
still a large cost yes well there's a
lock on the queue certain obscure lysing
on that and in in a conventional disobey
system there's no point in having
multiple threads because you can
actually you can issue much faster than
you can complete and so you end up with
parallelism in our system
we found out is that no matter how fast
we issued you know the requests would
finish faster than the no ops scheduler
could actually issue the requests that's
one of the reasons for the large
increase in performance we get with the
with no scheduler at all so the other
thing we looked at so our memory
latencies are based on projections and
actually somewhat optimistic projections
about where a phase change memory will
- and as you can imagine the
manufacturers of all these different
memory technologies I'll make very
optimistic projections about where their
particular technology will get to but
we'd like to know how how sensitive our
design is to increases in latency so
what this graph shows is we've got we
vary the the memory array latency on the
horizontal axis from 4 nanoseconds up to
128 microseconds and then each curve
measures bandwidth with a different
number of memory controllers and so you
can see for the very very fast memories
you know we can get a good performance
with a single memory controller and as
things get slower we just need more and
more controllers and we can maintain the
same level of throughput in the system
how to about a microsecond so even at a
microsecond we don't see any drop-off in
bandwidth of course the latency is going
to increase by however much your your
Hardware access time goes up but the
basics of our design still kind of hold
even if memories are a little bit slower
then we might then we might ideally like
them to be what's interesting here and
there's some other reasons for this as
well which we'll talk about in a moment
is that you actually there's some
tension in the in in the companies that
are designing these memories about
whether they're going to target a
storage application or say a dram
replacement application and we found
because of effects like this as well as
some other effects I'll talk about in a
moment is you actually probably want
different devices for those two
applications the access patterns are
pretty different between the two the
requirements in terms of durability and
how writes complete are pretty different
and also these applications can be much
more latency tolerant and so perhaps you
can make trade-offs in favor of reduced
cost or increased density if you know
that your application can tolerate a
little bit more latency so this is the
what I was just mentioning so there's
there's been a bunch of work in the
computer architecture community looking
in particular at phase change memory as
a dram replacement and there's a couple
of optimizations that they proposed that
do really well in that space but don't
work as well in a storage application so
the first thing is something called
write coalescing and basically what this
means is that you
when you open a row in the memory you
hold it open and you keep kind of
combining rights for as long as you can
maybe you'll have multiple row buffers
inside your memory chips to prevent you
having to go and do a write and do an
actual write operation which is a little
bit expensive in phase change memory the
problem here and why this doesn't work
for a storage application is Ana storage
application when the access completes if
it's a right you need to know that that
data is actually in a non-volatile
storage location so if you lose power
that's going to be there so the idea
that you could just sort of hand the
data to the chip and leave and hope that
eventually it writes it into the storage
array isn't going to work in something
like Minetta you're actually need to
commit things immediately or at least
have or have some other means of telling
the operating system when the the write
operation is actually completed the
second thing is they looked at varying
the row buffer size inside the the
memory chips and it turns out this is a
source of a whole bunch of power draw
inside of d rams and they actually
suggested moving to shorter smaller row
buffers to reduce power that doesn't
work as well here either because the
accesses tend to be pretty large so the
smallest access is that we issue are
about 512 bytes it's actually the
smallest request that Linux kind of
allows you to issue well there's no
particular hardware limitation but most
of them are much larger so they're four
kilobytes and so it actually makes sense
to have a very larger buffer so that you
can write a bunch of data in and do many
many bit level writes in parallel in the
array let's see there's also there's
some limitations on power so there's
some power concerns with PCM and it
turns out that in our system because the
PCIe bandwidth is not as high as that as
high as it might be our total power
limit our total power consumption is
actually limited by the PCIe bandwidth
because that limits how much work the
the storage controller has to do overall
so the bottom line here is what I said
on the previous slide is that we need to
optimize these technologies pretty
differently for storage versus main
memory applications
so the the key here is actually that the
way that those facility the question was
why can't we just use the operating
systems facilities for guaranteeing
durability and the answer is that the
way that those systems work is they
prevent the writes from completing until
the storage device says that they're
done and so that means that it has to be
all the way down to either the disk
platter or in our case into the memory
before the storage device can say yes
this write is actually finished the sink
operation at least as I understand in
Linux actually just works only on the
operating systems buffer caches and so
that's just issues a bunch of writes to
to make sure that the buffer cache is
clean they do that's true and so the the
the actually the analogy is pretty
direct here so in order to make sure you
have good durability you have to turn
off the right buffer on your SATA device
which guarantees that the SATA device
won't tell you that your device your
your right is complete until it's either
hit the platter or hit the NAND flash
and the same thing applies here it can't
the moneda can't say that the divideth
device that the request is complete
until the right to the actual memory
device has completed and we know that if
we lost power and look back up that the
data would still be there
i sorry you could do that also you could
issue a request that said go and you
know flush it flush everything that's
true you could do that right you know
capacity we already need a capacitor
because you need to be able to complete
the request anyway and that's sort of
assumed because usually would like the
requests that are in flight to complete
so you don't just kind of get arbitrary
garbage there so if you're willing to
put enough is essentially an OPS on the
device then you could be play much more
fast and loose with your Rea the one the
one comment about the the sync operation
that would certainly work I'd be a
little bit worried or I would be
interesting to know whether how the
performance trade-off comes down because
if I'm going to be issuing a lot of
these sync operations that's gonna take
you know the round-trip time just on
PCIe is a microsecond yep and maybe you
know at least on this well yeah your
program performance your ability that's
right
alright so onto the application level so
this is the test system that we've built
to test moneda and compared to other
storage technologies so it's an 8 core
Nehalem machine it's got a whole bunch
of DRAM which we're not particularly
using in this study we've got moneda
hooked up to it we have a four disc raid
array and a 4 SSD raid array as well we
also have an 80 gigabyte fusion i/o card
which
high-end flash-based SSD the final thing
we compare to is something that's
labeled appear as Minetta 4x so arm
another device actually has twice as
much PCIe bandwidth as all of the other
devices we're comparing two and so just
kind of for fairness we've put this up
as well cut its bus bandwidth in half
and then we can see how it stacks up
kind of apples to apples so these are
the applications that we're going to
look at xtd
is the i/o benchmarking tool that we use
we do experiments both with and without
the file system then we look at some
database applications so two little kind
of database micro benchmarks implemented
with Berkeley DB one that's based on a
b-tree though that's a hash table these
are very simple they actually just
atomically swap two values in a large
table and then we have two big
scientific workloads that we got from
our colleagues at the san diego
supercomputing center the first one is
called biological networks it's a very
large database this is a smaller version
of a much larger database that encodes a
bunch of information about the
interactions between different
biological molecules and then you do
your science by issuing these very large
and complex queries against the database
that will sometimes on a hard drive
based system might run for a day or two
then the last one is something called
the Palomar transient factory this is a
Sky Survey application where they're
taking a picture of the entire sky or at
least large portions of the sky each
night and they look at each point of
light and they're trying to determine
whether that piece of light was there
previously and it's pretty cool actually
if they find something new and are able
to determine that it's interesting they
can in real-time focus a whole bunch of
other instruments from all over the
world on to that spot in the sky to see
what's going on
and so these requests are actually very
small they're very simple but there's a
whole lot of them and they have a real
time component to them and so
performance is very critical and
actually on our system for at least this
database size we can take this from
offline to real-time by running it on
Lunetta
the final set of applications are memory
hungry applications so these are memory
intensive codes and we restrict the
amount of drm in the system and to force
them to page
- the Minetta device and this is an
application that's getting this kind of
application is getting a lot of interest
in the supercomputing community where
they have extremely large data sets and
would be willing to sacrifice actually a
fair amount of performance to reduce the
enormous costs in terms of DRAM so if
you have a terabyte of data you want to
work on right now you end up getting a
very large super computer that has a
terabyte of RAM performing petabyte
you'll get a very large supercomputer
with a petabyte of RAM and you know
actually that much compute if you could
buy that back with a solid-state memory
you could do it on a much smaller system
and they're building a system like that
at at SDSC so this is the X DB bandwidth
comparison this just Rob bandwidth for
reads and writes combined across all of
our memory technologies you know the
harder bit here is that even when we
restrict our bus bandwidth to match that
of the of the fusion-io drive we're
still a little over twice as fast and
then we're much faster than the other
technologies we can also see here that
our performance scales pretty well with
the bus bandwidth that we have so
between the 4x and the 8x versions we
get about twice as fast in terms of
bandwidth which is what we'd expect this
is a slice through that data again
without the file system looking at both
large accesses and small accesses
independently and the reason I put this
up here so that we can come see the
effect of the file system which I hope
you can see so for for the smaller for
the large accesses the file citizen have
much of an impact for any of the
technologies but for the small accesses
it turns out especially for writes that
the file system is actually pretty
detrimental to overall performance and
this is you know we're looking at this
now we suspect that it's the kinds of
inefficiencies we saw in the rest of the
i/o stack playing out in the in the file
system and so this is one of the places
where there's clearly needs to be some
more work if you want to do small writes
the file system is not your friend
this is going across the entire device
this and one one large file yep yeah
it's one large file but XFS is pretty
good about concurrent access to a single
file so it should be okay if you do this
on ext anything it's a disaster but it's
interesting you know if you have across
many files ext is not so bad so the this
these are the the graphs for the
database and the paging workloads so in
each of these I have our different
storage technologies and the performance
is all normalized to the disk and then
the blue bar on the far left of each
group that's the raw IO performance and
so we would like to see is that the
application level performance would more
or less track the Rayo performance that
would mean that all the money that we're
going to be spending to buy Mineta if we
decided to buy Mineta would be
translating into a good increase in
performance and this is on a log scale
and so we see is that for the slower
memory technologies it does pretty well
you get more or less application level
performance back that's about the same
as the raw IO performance but for
Minetta that's really not the case
there's about a factor of you know over
10x of performance that's sort of left
on the table there and this is again
probably the same kind of software
inefficiencies efficiencies that we saw
in the operating system layer that we
just need to go and attack in the
database world and potentially in the
paging system if that makes sense and so
there's just sort of you know motivating
future work in the area and clearly
there's a lot of work left to be done if
these are really gonna pay off in a big
way so to wrap things up
these non-volatile memories are coming
and they're coming pretty quickly we've
built moneda to try to understand what
the impact of them is going to be if you
try to build a solid-state drive out of
them we found that the interface and the
microarchitecture are both critical to
getting the kind of performance you'd
like to see but there's a lot of
opportunities left to make optimizations
above the heart
of the hardware level above the hardware
level and also to move pieces of
software software into hardware the
limitations you know the things that are
left on that stack of latencies are the
OS protection overhead so getting in and
out of the operating system and the file
system both of those present some really
interesting research challenges about
how to eliminate or at least minimize
those costs and so there's a bunch of
really interesting open questions about
these these SSDs and the systems that
they interact with and how those should
all be designed and there's all my
students who actually did all of the the
hard work and I'll take any questions so
that's a good question so we've just
we're in the process of building some
dim shaped cards that hold some of the
prototype memories that are available
the performance obviously varies with
the memory technology the current PCM
devices that you can buy are pretty slow
the right times are in the many many
microseconds three times are reasonably
fast so I think we're gonna see very
asymmetric performance when we get that
up and running and then you know as they
keep optimizing those technologies it
should get closer to what we've seen
here
realistically unless you're pretty close
to the density of at least DRAM and
maybe even a flash and your performance
is pretty good definitely much much
better than flash your technology is not
going to really go anywhere and so that
sort of sets some you know the area of
interest and if we're outside that area
then the technology effectively doesn't
exist
well there's a couple of obvious things
you can do one if you have an
application that where it'll work you
can export the interface directly to
user space by just mapping those pio
pages into user space and that'll
eliminate all of the file system
overhead there's no file system anymore
and all the operating system overhead
you well almost all you still need to
have some way of doing completion so
either going to be spinning waiting on a
DMA completion she talked about a little
bit earlier or you'll have to forward
interrupts from the OS and user space
either those are possible you you're
obviously limited in the applications or
a lot of work databases that should work
fine
they're usually well if they were
rewritten they like to run on bare metal
anyway but for more general applications
there's open open questions about how
you do protection and so forth and in
that kind of system yep
so with respect to PCIe it is a little
bit clunky the biggest problem is
latency the the graphics folks are
making sure the bandwidth is nice and
high which we could we could leverage
pretty nicely and you know you could
probably shave a little bit of time off
the latency but there's some fundamental
limits about you know even the the
hardware that does the serial
communication and so forth you know run
into the other place to put it is on the
DRAM bus and that gets a lot of
attention also I think there's certainly
opportunities to put it there as well
one of the things that we've found is
that in our experiments where we've
taken a lot of DRAM and built just a
block device out of that as a ram drive
is that the the i/o requests end up
competing and interacting sometimes in
negative ways with other memory requests
because they're both contending for the
same bus so there's definitely some
advantage to having the i/o on a
separate bus from the normal DRAM
because they have very different access
patterns the access searle are accesses
are larger there's many better locality
for the i/o so one simple thing you
could do you know in a hey l'm the chips
we have have three different memory
channels on each on each core so maybe
you designate one of those as the i/o
chandler you configure them somehow so
that you steer all their requests or
reconfigure the memory controller or
something to make that particular memory
channel a little better behaved for
storage kinds of applications I think
they're still going to always be a place
for something like PCIe and the reason
is that it makes it possible to connect
multiple servers to a single box of
memory which a lot of storage
applications would benefit from and you
can do you know it's vastly faster to
connect eight machines over PCIe or over
fiber channel or something else to a box
of memory than it is to put a server in
the middle that's gonna you know pull it
out and do you know all of the server
kinds of things and things like parallel
NFS and so forth
make it possible to do that kind of
direct access pretty easily so I think
they're probably going to coexist I
think you're also going to get better
scalability in terms of capacity from
something like PCIe I don't think we're
ever going to get you know
a terabyte of anything on your DRAM bus
or if we get that then I'll say we're
not going to get 64 terabytes of
anything on your DRAM bus and so there's
there's always going to be a space for
that kind of device
all right thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>