<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Non-Myopic Active Learning: A Reinforcement Learning Approac | Coder Coacher - Coaching Coders</title><meta content="Non-Myopic Active Learning: A Reinforcement Learning Approac - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Non-Myopic Active Learning: A Reinforcement Learning Approac</b></h2><h5 class="post__date">2009-03-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hFBe9L1xCyY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody welcome to the tactic
series for Google Waterloo today's
speaker is pascal poupart he is an
assistant professor at University of
Waterloo and he has also received to
Google research Awards which makes his
research quite interesting for Google
and today he will talk about active
learning and arrays for reinforcement
learning approach so Pascal well thank
you very much ok so again ok the topic
is non myopic active learning a
reinforcement learning approach and i
guess i should say at the outset that
this will be a fairly theoretical talk
so I know Google a lot of people care
about scalability and so on but then I
guess you need to have the data to worry
about these problems so this is work
that I've done in the past few years but
mostly from a theoretical perspective
but the idea is that if you understand
well the theory then perhaps it'll help
you in designing practical algorithms as
well so that's the philosophy know that
I use when I do research and hopefully
you know everybody can appreciate it
from that perspective too ok so here's
the outline we'll go over some of the
basics for active learning then i'll
talk about Beijing reinforcement
learning and how we can use it for the
non myopic version of active learning
then I'll talk about if we find this as
Beijing reinforcement learning how we
would come up with a good strategy and
that'll mean coming up with a purchase
asian for the value function and then an
algorithm called beetle and then a few
experiments okay so what is active
learning so let's start with supervised
machine learning so I'm sure that from a
lot of you that won't be anything new
but let's just make sure we've got the
basics in place so let's see we've got
some data x and y where x is going to be
our input it can be you know of any
dimensionality it doesn't matter and
then why just for the sake of you know
this stock is going to be something
binary either plus or minus and then so
let's say we've got a data set right
here okay and then the question for
learning is how can we come up with
perhaps a separator that would identify
I guess those two classes and as well as
any other point that's not already part
of our data set right so that's a
classic supervised machine learning
problem now in many situations though
what happens is that we start with just
data that's not labeled so we don't know
what the classes are right and then we
still want to do classification but we
have the option of requesting the labels
okay so in that situation then we could
ask somebody let's say for the sake of
this type which is not call this person
in Oracle you know to tell us what is
the answer for some of those points and
we just submit those points okay so
perhaps I just submitted a point it
returned and came up as a plus here then
I could ask for another point maybe this
one there and it's a minus and we keep
on going like this we keep on requesting
some points so we convert them into
label data and then eventually we'll
have enough information perhaps and then
we can classify the data okay so but
then the question with this is what's
the best way of I guess requesting the
data what point should we ask the Oracle
to label right because you can see that
I just drew the line somewhere that kind
of makes sense based on those four
points but I could have drawn it
elsewhere maybe some other points could
have been more informative right okay so
that's the main question how do we
choose the data to be labeled okay so
there are some basic ideas out there and
I mean active learning is not a new
problem and perhaps the most popular
approach is what is known as a greedy
approach okay so basically you simply go
for the point that will give you the
most information and let's say that
we're in a situation where we're coming
up with linear classifiers like this and
they say i've only got those four points
that are labeled then the point that
would make sense to label after that
those that i put in dark blue because
they're still uncertain right the other
points the green ones if I go with the
assumption that my data is going to be
linearly separable right you could
assume that these guys are going to be
pluses and these guys are going to be
minus s right so the only real
uncertainty is with respect to the blue
ones and then we just need to ask
ourselves perhaps you know which point
is going to give us the most information
and then go for that point right now
ideal if we really want to be optimal it
would be nice as well to consider I
guess an entire sequence of queries and
then see perhaps which sequence is
really the best one for a simple setting
like this often greedy does well but
ideal if we truly want to be optimal
that's what we should do now we're going
to see some other settings where I
actually going for an optimal sequence
will actually be more important and this
will lead to non myopic active learning
so when you're greedy you essentially
being myopic right and when you're going
for the optimal sequence then you're
trying to not be myopic okay so let's go
as well through some examples just to
make things concrete and I wasn't quite
sure which example to pick and then
since I'm here at Google I pick to that
should make sense to people here but
then I know there's also a danger that
you guys know so well those examples
that I hope I won't see anything wrong
here but what will keep those examples
simple anyway okay so document
classification classic problem where
you've got some data that's a corpus of
documents and then so that means for
each document we want to come up with
the topic that it should be part of and
then let's say we've got a classifier
let's say just a nice base model so then
agree the active learning approach could
simply estimate what is the topic
distribution for each document and then
based on that we could simply request
the label of the document that is the
most uncertain okay so this this would
be a simple 3d active learning of
approach assuming that we've got a
classifier like a naive based model that
gives us a distribution over the
possible classes all right so if we've
got that and we know how certain we may
be or not certain and then we can refine
I guess or estimate or classifier by
always picking the document or the point
that's that were the most uncertain
about okay all right now let's look at
something more interesting budgeted
learning so in many cases the data that
you would have to get it labeled doesn't
come for free okay so so here you could
think of your data as coming up with
like each point could be having a COS
assigned to it and then when you request
a point then you'll pay the price for
that point and then simply get the label
right so again we could go save for the
cheap ones and then perhaps they're
going to give us plenty of information
but in some cases maybe the more
expensive ones will give us more
information so it's not always clear
which ones to pay right by the other day
at some point we're going to have a
classification again and it's really
just a question here of maximizing I
guess the value of information and again
we could be greedy but in some cases as
well it'll make sense to go for
something that's non myopic that is over
a long period of time okay so in the
context of document classification this
will be an example where say we've got a
budget where let's say we've got a
research assistant maybe a linguist was
just going to read documents for one day
and then label them but then the problem
is that if the documents are long right
then it will take longer so it would be
better if we just submitted to you know
the linguist shorter document so that he
or she could label more of them right so
in this case you see just going for the
documents that are the cheapest may not
deal the best strategy or those that are
going to those that are the most
informative in a sense of reducing the
uncertainty the most that may not be the
best strategy either so you really want
to look at I guess the value of
information ok and then so that's our
dilemma here so do we go for shortest on
label document or document with the most
uncertain topic ok now here we can add
one more twist to this where you could
say that in many situation the Oracle is
not perfect so the perhaps the person
won't label the document per se but
simply tell you how good is your guess
about the topic or the label of some
document so here in this setting right
we're going to have some data again but
we don't when we request what we submit
to the article is not just a point for
which we're going to get the label but
instead we submit a point with our gasps
of the label and then we get a
reinforcement just some kind of score
that tells us how good that label would
be is in some situations all labels are
kind of ok jizz that some of them are
better than others ok so it would lead
to this setting where again we've got
our inputs we don't own the label but
now the cost is a function from every
input labeled pair right and that cost
essentially just tells us how good that
label was and perhaps this could be
noisy as well ok and then here we simply
want to come up with a policy that would
map every input to a label but then we
don't get to have like the fool that I
guess the correct answer from the Oracle
so here this could correspond to ad
placements and and I know some of you
guys are working on this and know a lot
more than me on this so I'm sure I'm
going to see things that are not quite
the way it works at Google but I thought
I'd come up with a simple example so
this is really you know simplified
scenario of ad placements ok so let's
assume that we
got a set of keywords this is our data
right and then we need to decide what
ads we're going to show so that's our
labels and then you can think of each
keyword as having potential labels that
are the ads and we simply want to get
the best one the one that will perhaps
give us the best payoff okay so so there
in this scenario what we would really
like to know is where the click-through
rate is and then our dilemma is always
well should be showing that that has the
best expected return so far or do we
simply showing that that has the most
uncertain click-through rate so that we
can learn about it right and then so you
can think of this is like trying to I
guess maximize needed profits right
where this is more like long-term
exploration so that you could maximize
your profits down the road because you
would know better what is a
click-through rate of different okay
okay so we could make this even more
complicated okay and then you could see
all of these scenarios that I've
explained as really just being special
cases of reinforcement learning so in
reinforcement learning the added
complication is that you really only
have what you don't get to pick the
points that you submit to the Oracle
it's more like when you submit something
with a label that will actually
influence what is the next point that
you're going to get okay so the next
data point is influenced by the label so
you're kind of constraint in I guess
what will be your future choices so in
situations like this you really need to
have a strategy that's non myopic
because if you only try to I guess
maximize expected to return for I guess
your immediate label then you're you're
disregarding what could happen in a few
maybe the future right the possibilities
are not going to be as great ok so for
situations like this then you really
cannot be myopic ok and then so to model
this right the added piece is here where
we've got a transition probability that
says if we if we are looking at a
specific point we've submitted a label Y
to the Oracle then we got a value for it
but on top of that the next point we
will end up in we don't get to pick it
it will be decided by some transition
probability ok and so that's why in
situations like this whenever you've got
like this coupling from points that you
visit then being myopic is going to be
an issue ok so this would correspond to
situations where you've got ad placement
with budgets and again I'm not going to
claim that this matches what really
happens at Google but it's a simplified
example we're here let's say that
advertisers have budgets and the idea is
that if you keep on displaying the ad of
one advertiser at some point once the ad
has been clicked on enough perhaps the
entire budget will be depleted and then
you won't be able to show it again right
so that essentially constrains what is
your space of I guess next points ok so
here you can think of this as having
data that's the set of keywords and a
budget for each app right so before we
simply had data that was a set of
keywords but now we're going to augment
this with the budget ok for each ad and
the next point you're going to visit you
can definitely select well I guess a
keyword is not decided by you I mean
it's whoever you know submits a query I
guess on google search engine right and
and then for the budget well that
decreases with each
click through or yeah with each click so
that's why in a sense the points that
you get to observe data about right our
constraint so you don't get to pick them
all the time so there's this transition
probability of going from one he worked
with budget to the next keyword with
budget and so on ok the the labels are
again ads and then again the goal is the
same but with the condition that we want
to satisfy our budgets okay so the
problem is the same as before unknown
click-through rate we've got the same
dilemma but with the added complication
that and that cannot be shown when the
budget runs out okay so this is one
example here where perhaps I guess not
being like you don't you don't you
really don't want to be myopic or at
least not when the budget is about to
run out maybe at the beginning it's okay
but not near the end right okay does
that make sense to everybody okay all
right so now I don't work on ad
placements so what really motivated my
research is actually this problem so
what's the connection here we're
developing assistive technologies where
there's a video camera that monitor is
what a person is doing at the sink in
the bathroom and the idea is that people
with alzheimers disease or other forms
of dementia have trouble just carrying
out simple task of daily living like
hand washing so hand washing is nothing
complicated for you and I simply because
we're just fine cognitively but there's
actually a sequence of steps where you
need to first turn on the water wet your
hands put soap on your hands then rinse
and then turn off the water and then dry
your hands and some of those steps can
be interchanged but otherwise there is
an actual sequence and then the last
thing you want is perhaps a person too
skip a step like rinsing and therefore
just go to dry the hands that are still
full of soap right and then so it's so a
person that has dementia in a hospital
will often need to be accompanied and
then a nurse will have to essentially
guide the person through those steps so
then the idea here is can we build a
system that will monitor what the person
is doing and then give a prompt at every
step if necessary to guide the person
and then so here you can think of having
data that the various states of the user
okay and these days could involve like
you know what step is a person at and
then what's the status of the hands are
David full of soap are they wet are they
dry where's the towel that sort of thing
and then the label is a prompt so give
it a situation should we give a prompt
or not that's the first question and
then the second question is should we
give a prompt for a specific step and
then how detail should that prompt be
okay so that's our space of labels and
then the goal is to come up with the
best prompt for each of those state
that's our our policy okay and you can
think of this as an active learning
problem again because the user is
unknown so if we already had I guess
some information about how the user
would react to different prompts then
you could hard code the policy may be
like if you've got a grandparent that
you know well enough then you would know
like what step you know your grandpa
grandma is likely to do well and what
steps needs to be prompted and so on but
if it's a new person like lettuce on a
hospital and you know this system has to
interact with many patients and then so
each person will behave differently and
so you need to learn I guess this
information right so the system here
ideally should just try to give the best
prom but the other hand you'd like the
system to also learn
the person all right so in some cases
the system may decide to give a prompt
just so to see how the person is going
to react even though the system knows
that this is not likely to be the best
prompt okay so we've got again this
dilemma of exploration and exploitation
okay so in any case that's that was the
research that was motivating I guess
this work on that I'm going to present
on Beijing reinforcement learning and
then this particular system of
hand-washing has been deployed in a few
hospitals including in Toronto and I
believe there's probably one well
actually I yeah so Sunnybrook was a
hospital in Toronto world was first
tested and then I likes me how this in
Toronto is the one who's leading the
project for that and they've got a few
prototypes that are in a few hospitals
okay okay so reinforcement learning the
big question again is exploration
exploitation and and then so that leads
to the following difficulties we're in
contrast to I guess other task from
machine learning we don't know what is
the label for any given point all we get
is a reinforcement and then we cannot
select the data directly either so it's
really like some kind of constrained
active learning problem that's noisy
because we don't get to have the label
right we just get to have a
reinforcement and so for these types of
problems then solutions tend to be
complicated and you usually don't want
to just have something that's myopic
okay so let me now go over some of the
basics of reinforcement learning and
then I'll propose an algorithm that can
tackle this and can be used for active
learning in general and will be
interesting I guess for active learning
where myopic solution would not be good
okay all right so reinforcement learning
we can think of it just in this
framework where you've got an age and
that's your
system it can perform some actions that
will influence the environment and then
it will sense what the effect of those
actions are through some observation and
then here I'm going to assume that these
observations are fully observable so I
get to see the actual state of the
environment so we could make that even
more complicated with partial
observability but for the sake of this
stock we'll just stick with full
observability and and then with that
will get as well a reinforcement a
reward that will tell us how good the
action was just for that time step and
then the agent has a goal or some
objective it will essentially mean that
it tries to maximize those rewards so
that's usually how reinforcement
learning is framed but I mean active
learning can just be seen as a special
case here okay now more precisely our
formal model comes from mark of decision
processes so we've got a set of states
these are what I call before the inputs
we've got a set of actions these are
what I used to call the labels and then
a transition function that tells us what
is the next state that I could reach
after executing a certain action in a
given state and then a reward function
that's kind of like if you want the
opposite of a cost that we talked about
before for labeling data okay now if
we're looking at reinforcement learning
not mark of decision processes the
difference is that we don't have this
transition function all we have is a
state space action space and depending
on the problem you may also have the
reward function so here I'm going to
assume that I've got the reward function
but also remove that okay so this really
corresponds to I guess the same as those
funds we saw before where if we don't
know I guess what is the next point that
we could be reaching as as a result of
labeling the current point then
mean that we don't know what is that
transition function and and then you'll
notice here that we don't like we don't
know what is the mapping from States to
action so it's the same as before we
don't know which label goes with which
data point okay so this is a formal
model and to come up with a policy here
will want to maximize expected total
rewards so that's one way to set up an
objective and that simply means we're
going to maximize the sum of all the
rewards that are earned that every time
step essentially maximizing I guess the
expected returns of all the ads you know
that we're placing ok and then so a
policy formally is going to be a mapping
from states to actions or if you prefer
from inputs to labels and and then we're
going to look for an optimal policy
meaning a policy that that has higher
value at every possible states and beats
every other policy ok and there are some
theorems that says that you cannot
there's always one that's better than
all the others ok for every possible
state so we're just going to aim for
that pause alright so reinforcement
learning algorithms come in different
flavors so there's a large body of
literature out there but very often it's
divided in two groups model free as well
as model-based and what that means is
that either you try to learn the part of
the model that you're missing so if you
remember we define the reinforcement
learning as a mark of decision process
where we don't know where the transition
function is so if in our learning we try
to estimate what that transition
function is then we're doing model-based
reinforcement learning if we don't try
to build that directly and we simply try
to compute a posse and we're doing model
free reinforcement learning and so
classic algorithms for this there's
q-learning TD lambda pegasus neural
dynamic programming these are all model
three and traditionally this was the
most popular class
in the model base camp what's actually
quite popular these days our Beijing
approaches and the ideas that we're
going to put a distribution over the
unknowns and learn that distribution now
if we look at the implications of doing
model free versus model based model free
tends to be much simpler because we
don't model anything whereas model-based
is actually quite harder however in this
case here if you don't model I guess
your unknowns then it'll be much harder
to come up with a strategy that's non
myopic and that trades off exploration
and exploitation whereas in this case
we'll have explicit information about
what we know and what we don't know
about our model and then it will be much
easier to have a policy that does
properly optimize this exploration
exploitation trade-off okay so given
these differences I'm not going to focus
on vision reinforcement learning because
I want to have something that's non
myopic and perhaps does the best in the
long term so therefore I'll want to
optimize my exploration exploitation
trade-off and will show that in the case
of vision reinforcement learning with
the right under the right assumptions we
can come up with an optimal policy okay
so we are able to optimize that
trade-off now what's really interesting
is actually Beijing reinforcement
learning is not new ok so what people
usually remember is Q learning and then
all these other algorithms that I put
here and in the literature d started
like in the 1980s right if you read
about Sutton and bartels book right
that's essentially the main focus well
it turns out that in the operations
research literature people are already
doing reinforcement learning it wasn't
color reinforcement learning but they
were doing it
and it was really what is known today as
Beijing reinforcement learning so back
in those days it was called dual control
or they had different names okay but in
any case you can look at the papers by
Ron Howard and some of the students in
the 1960s and it's it's quite elegant
okay so how does vision reinforcement
learning work the idea is that we've got
some unknowns here it's our distribution
over the next state given the current
state in action and beige in learning
simply says well let's treat that as a
variable and then learn a distribution
over that and that distribution will
encode how certain or uncertain we are
about that unknown so here because what
we don't know is a distribution itself
it means that these variables are going
to be numbers between 0 and 1 and then I
guess a bunch of those numbers that much
sum up to 1 is an unknown multi-node
unknown multinomial distribution and and
then we'll have a prior distribution
over these variables and then the key
will be to come up with a posterior that
will simply incorporate the evidence we
get so at every step when we execute an
action and see the next state right then
that's additional information that we
can use to refine our distribution just
like I guess with ad placement each time
that you display an ad and then you get
to observe whether it was clicked or not
that's additional information that you
can use to refine your I guess belief
about what is the click-through rate of
that ad okay so so what I'm going to do
next is show you how we can set up a
prior and then refine it to come up with
a posterior distribution based on the
evidence once we've got that explained
properly and we'll see how we can use
this as the input to our policy
and then have a mapping that will toss
what action we should pick given
distributions like this okay so sign
letter prior computing the posterior is
also known as belief monitoring so we've
got a belief about our unknown model and
then we compute a posterior simply using
Bayes theorem so we simply multiply the
prior by the likelihood and renormalize
and in this particular setting what's
really interesting is that the math is
actually quite simple because if we're
working with a discrete model right if
our distributions that are unknown or
multinomial then you will notice here
that p of s prime given s a and theta is
really think that itself that's our
unknown so why it means is that I've got
a prior distribution that I simply x
theta and now the interesting question
is is there a class of distributions
that I could use x theta and get a
posterior that's in the same class again
so this is known as what our conjugate
priors and for these multinomial
distributions it turns out that the
earth lads work well and so there are
sleds okay if you're not Farrah with
these distributions or the name you know
is a little intimidating just think of
them as multinomial okay so so their
defection is that you take the product
of theta theta for every unknown
probability okay every unknown number
between 0 and 1 raised to some exponent
so this is Regis a multinomial and why
this works well is because if you
multiply multinomial by theta then you
get simply again a multinomial where one
of the exponent will be raised by one
okay so so this is very simple and quite
elegant but then the next question is
how do we start like we have to have a
prior and what should be the
distribution so it turns out that
there's a nice interpretation for these
distributions we're here I've drawn
three of them and the idea is that these
distributions they have hyper powders
and that are known as hyper counts so
the idea is that you can think of those
hyper counts is like some virtual data
that you could have observed before and
have led you to set up your prior
distribution in the following way and
then so it would be essentially how many
times you've seen each cita I event and
then if you take the relative frequency
counts this should kind of give you the
mean so the distribution will have its
peak right at the relative frequency
counts and and then the height of the
peak will depend on how much data like
this you could have observed before okay
so so if you've got the number of
virtual accounts that's high then it
would mean that it's as if you had
already seen a lot of data and therefore
you should be more certain and therefore
this distribution should be more peaked
right if you haven't seen that much data
then you you think that the distribution
should be more flat and perhaps if you
haven't seen anything it should just be
uniform right so that's the intuition
here so i guess here you see if we set
n1 and n2 to be 0 point to and point
eight times K or I very k all that will
happen is that I will simply make my
distribution more or less peak and then
it will have the peak near point to
which corresponds I guess to the
relative frequency counts here okay
does this make sense yeah okay all right
so so now we have some ideas about how
to set those distributions then we can
compute the posterior by belief
monitoring and so we just apply Bayes
theorem like this and what it shows is
that you multiply the prior by theta
that raises one of the exponent by one
and then you get back at the earth shell
distribution okay and then so the key
here is that this is really simple
because the computation just means that
we're incrementing one of the hyper
counts all right so now we know how to
do monitoring so we can learn i guess
the unknown parts of our model very
simply next question is how do we act
ultimately with respect to this
information so we're going to define a
policy but now instead of defining the
policy just with respect to the current
state we're going to also define it with
respect to our belief about the model
because that's information as well that
tells us you know what part of the model
is well known what part is not so well
known and perhaps sometimes you know
will want to behave differently so that
we could explore or exploit more based
on that okay so as I explained before
policy optimization simply means that
were maximizing expected total rewards
so we're not looking for a distribution
we're looking for a policy that will
have a value that's greater than any
other policy for every state and belief
pair and so to do this what's convenient
is to use what is known as Bellman's
equation and what this equation says is
simply give you a recursive definition
of the value so the value we said before
that was simply a sum of rewards but
that would be a possibly infinite sum so
perhaps what we can do is make that some
recursive where we define the value
you add the current step as a sum of the
current free work plus all the rewards
in the future that are not captured by V
the value at the next step okay so
that's all there is to balance equation
but the key now is that if we stick in
here a max that says at each step we
simply should take the action that will
maximize this son then we'll have our
optimal policy right because this is
making you could work out an argument by
an induction that says well if your
optimal for the last step and then you
just ask for one step before what should
you do well you should be optimal with
respect to that step in the next one and
that will arise as a result of this max
here and then if you just do this all
the time then for every step you're
always going to be optimal and that
gives you the optimal policy okay so in
a sense balance equation is what allows
us to not have a policy that's going to
be optimal in a non myopic sense okay so
what's interesting us to compare balance
equation for classic reinforcement
learning versus Beijing reinforcement
learning and by classic I simply mean
when we try to come up with a policy
that only depends on s as opposed to s
and B and so if we take be into account
and that means we're being Beijing and
so it's the same equation just that the
arguments of s are slightly different
right but this is critical because here
if we're trying to come up with the best
action but we don't take into account
what B is what is our belief then how
can we optimize or trade off between
exploration and exploitation we don't
know what part of the model we know well
and what part we don't know so well so
this won't work at least we won't be
able to come up with an optimal policy
so in practice people usually use your
wrist 6 and the popular ones would be
epsilon greedy boltzmann etc ok where is
envy
reinforcement learning well be tells us
how much uncertainty we've got four
different parts of the model and this
distribution here the distribution over
the next state well we can come up with
its expectation simply by doing this
integral here so that gives us a model
that's completely known so we're back to
I guess a Markov decision process here
and actually a partially observable
markov decision process so before you
see balanced equation the problem is
that we don't know this part right so
that's by definition what we're missing
for reinforcement learning so we have to
come up with your risks for exploration
and then often if we don't estimate that
directly then maybe through sampling
we're going to introduce more
approximation and that works to some
extent but on the other hand we won't be
able to get something out tomorrow
whereas here we know every part and we
also know what is are we yeah we know we
can quantify what is our uncertainty
about the model and so that's how we're
going to be able to get an optimal
policy alright so let me walk you
through now some algorithm as well as a
petrol station for the value function
and and then this will give us I guess a
concrete way of optimizing policy so how
much time do I have left 15 minutes ok
so yeah I should have enough time ok so
now the tricky part of working with this
form of balance equation is that s here
is discrete be is continuous and B is
really during sleds and I mean it's
something that at least conceptually
looks complex right so it's not clear
that we can come up with a value
function here that will be easily parent
risible so the traditional way of doing
dealing with this and reinforcement
learning is you just use your favorite
function
scimitar like a neural network or
gaussian process or anything and then
you just fit the value function but what
I'm going to show you is that the value
function actually has a Patrick form
that's fairly simple so we can actually
work with the right thing from the
outset and so the key will be to show
here that V is the upper antelope of a
set of multivariate polynomial so in
other words V is simply the max of some
polynomial and the proof of this is very
simple we can do it by induction the way
it works is that we're going to really
find V in terms of not the belief but
the underlying unknown model theta and
then show that V of theta is really like
this here max of polynomials and then we
do this by induction by saying okay the
last step we can have a polynomial
that's trivial just zero then if we
assume now by induction that the last
step we looked at we've got a polynomial
or something that's max of a polynomial
then we can regress that true Bellman's
equation by multiplying by theta that's
our unknown distribution sum over s
prime add a constant that corresponds to
the reward and all of this computation
can be rewritten so that we've got one
max at the beginning and then a
polynomial after that okay so so in
other words value functions stay in the
class of polynomials where we take a max
over them so this is great because now
we'll have a way to represent and also
approximate value functions in a more
informed way than just using a neural
network or anything else okay so here's
an Al Gore that I exploit this
representation there's a few caveats
actually even though this is a neat
representation the problem is that now I
should be able to come up with something
like this for every belief
I've got an infinite number of them so a
classic solution to this is simply to
sample some belief points and then run
balanced equation for each belief in my
sample okay so here those three steps
are simply balanced the equation that
I've rewritten in a more detailed
fashion that's usually how you would
implement it in three steps in practice
and then we're going to essentially
estimate what is the value at a bunch of
belief points and then come up with I
guess I function over theta for each one
of them and then merge all of these
functions into a set and each one of
them is really a polynomial so that now
when you put them all together you
simply need to look at the one that's at
the top so that's where the max comes in
okay so this is essentially just doing
balance equation on or on a bunch of
belief points okay another issue is that
the number of polynomials are sorry the
number of monomials in each polynomial
will grow exponentially with time so
even though we've got a nice Patrick
form it's a form that isn't bounded in
science it grows over time okay so a
polynomial is simply a sum of monomials
and here the number of monomials grows
by a factor of s at every step and this
arises from the summation that we've got
here this sum over s prime here okay so
this is going to be problematic so
here's a simple idea whenever we've got
like mixtures of gaussians or in this
case they're really I guess linear
combinations of monomials or darish that
what you can do is always project on to
a smaller number of components so we can
take our polynomial and approximated by
a linear combination of just a few
monomials that we're going to treat as
basis functions
so this way at every step if you project
down like this we're going to keep our
representation bounded now okay how can
we optimize for the coefficients here so
let's say that we've got basis functions
already predefined and we simply want to
come up with the best coefficients well
we can solve an optimization problem
that perhaps would minimize some LM norm
and if we use the euclidean norm it
turns out that this optimization problem
here can be done in closed form and it's
quite simple so it simply solve a linear
system ax equals B where a is this
integral V is this integral and X is
really the unknown that you're looking
for okay and so I guess what may be a
little tricky when you look at this is
all these integrals but this arises from
the fact that we're kind of doing I
guess a linear regression by minimizing
Euclidean norm but on functions instead
of vectors if I had vectors right I
would simply be maximizing Euclidean
norm and then I would have a linear
system like this but I wouldn't have to
talk about any integrals the reason for
the existing rule is simply because I've
got functions but these integrals are
easy to compute because these things are
monomials so the integral of a monomial
is quite easy to do okay all right so if
we have employed this kind of projection
scheme then the next question is where
do the basis functions come from and
here we're going to use a trick that
there's a similarity between how we do
believe monitoring and how we compute
the value function so belief mattering
is this process where you start with a
belief yet well execute some action get
another state and update that belief
right so it's a process that goes
forward whereas when you do a bellman
backup it's kind of going the other way
around you you look at the last step how
much value you've got and then based on
the state you ended up
in the action that was executed he can
kind of regress and figure out what
value you would have for one step to go
so it's it's the same thing but in one
case its forward in the other case it's
backwards and it turns out that the
monomials for the beliefs and to be the
same as for the value function and so we
can do believe monitoring easily we can
figure out what those monomials are so
let's use them as basis functions for
our value function okay so to summarize
um the algorithm would work as follows
there's two parts and not flying an
online part the offline is where you do
the optimization of the policy compute
the value function the online part is
where you would learn by belief
monitoring the transition model and now
what's a little confusing sometimes is
that it looks like because we're doing
the optimization of policy offline that
our pulse is not going to adapt but it
does because the policy takes as input
where the belief is here so it's
essentially a policy that would kind of
capture all the possible situations and
then at runtime you simply tell ah now
you're in this situation or even that
situation and so on and therefore it
adapts implicitly because of that the
caveat is that you have to essentially
precompute what to do for every possible
situation and that's usually not
possible and therefore that's why we
only pre-compute for a bunch of belief
points as I mentioned earlier and and
this leads to an approximation ok ok so
the advantages of this approach is that
it's going to be fast enough for online
learning and this was not always obvious
before for model-based approaches
because they construct a model and and
and therefore it's more computation and
then people who do robotics they'd like
to be able to I guess select an action
in real time so a fraction of a second
and then we could do that here and then
the reason is because most of the
computation is actually
on a fly ok ad online there's not much
to be done ok the main disadvantage that
our policy is not going to be good
everywhere and that's simply because
we're trying to pre-compute for every
possible situations which is not always
realistic ok so here are some
experiments we're going to compare this
reinforcement learning strategy to two
other strategies exploit is the classic
myopic strategy that you would use in
active learning we're simply trying to
be greedy and maximize expected value
just for the current decision so there's
no notion of exploration here the other
possibility is that this bellman
equation I didn't tell you this before
but it was really a partial observable
markov decision process because be the
belief is really a distribution over
some unknown the model so in the sense
you can view this as a partially
observable markov decision process with
a complicated one because it has
continuous and discrete States and then
we don't have good algorithms for the
continuous part so but we do have good
algorithms for the discrete part so
let's just discretize the continuous
part the drawback is that discretizing
anything continuous it will pro x
financially with the dimensionality ok
so this doesn't scale well in practice
but it's a good test for comparison ok
and we tested this on some toy problem
so when I was telling you earlier that
you know we're going to do some theory
so you see in this talk I showed you I
guess how you can frame the problem what
would be the right way of I guess
dealing with the exploration
exploitation trade-off and so on but
then computation were not at the point
where this is going to scale a lot right
so so here these are really just toy
problems just to show you I guess what's
possible and the first problem is a
simple chain problem with just a few
states the more interesting one is here
and washing so this is a
simplified version of what i mentioned
earlier and to judge the difficulty of
the problem here the column to look at
is this one here about the number of
free powders so usually people in
reinforcement learning look at the
number of states and actions to know how
difficult the problem is but that's
actually the wrong way of looking at a
problem in machine learning what makes a
problem difficult is how many degrees of
freedom you have and let's read a number
of free powders it has nothing to do
with the number of states or actions
okay so here you'll notice that 273
pounders is actually a difficult problem
and even though it's a small state space
okay so it's all about how many free
pounders your problem is going to have
ok and then okay beetle is here this is
vision reinforcement learning opt is the
optimal solution if you knew already all
parts of your model so so this is really
an upper bound that tells you how far we
could be from optimal but it's not clear
that we can instantly reach that value
ok and you'll notice that beetle does
well on the problems that have just a
few free powders but when the number of
pounders grows it's far from optimal but
on the other hand it will do I guess
better than the other techniques like
this one here doesn't scale at all so it
runs out of memory and this one here it
really depends if the problem requires
exploration or not it turns out that for
the first one there's actually no
exploration needed so it does extremely
well but on the second one there is
exploration need it so it doesn't do so
well ok ok another interesting
experiment is to see since the laning
problem is difficult when we've got lots
of free powders now what if those free
partners we actually have prior
knowledge about them that could make the
problem simpler and in practice you if
you know well your domain you would not
start with like like something from
scratch right you would know some
information and then you could make you
can start with a prior that's
informative
so here k is as I explained before when
i talked about bearish lads it's the
powder that tells us how peaked the
distribution is so as K increases I've
got more information and therefore it's
an easier learning problem and what this
shows is that the algorithm actually has
better performance as one would expect
okay so it generally increases with cake
and then here are some learning curves
that shows how old all these algorithms
doing some of the harder problems so
hand-washing tree this was the hardest
one and then you can see that I guess
with enough prior knowledge so when e is
equal to 30 right then beetle does very
well but if it doesn't have enough prior
knowledge is still far from optimal and
then exploit is lower and then the other
approach the one that this criticizes
palm DP doesn't even run here because it
it just doesn't scale ok so to conclude
I guess the topic of of this talk was to
look at active learning but in the cases
where being non myopic is critical and
then and the reason for this is because
at that point you've got an important
exploration exploitation problem and so
what I try to argue is that if we use
Beijing reinforcement learning which is
really like I guess something that me
feel like is a more complex tool than
you need for a lot of active learning
problems but in the case where really
we've got an exploration exploitation
trade-off is really the right tool then
in that case then we can really do this
optimization optimally if we're being
Beijing so that was the main thing here
and then the contributions as I showed
you how you can characterize a value
function in a way that's exact as
opposed to just using your favorite
function approximated without having a
clue whether or not this is going to
converge ok and then there's an
algorithm that exploits that
okay so for future work well okay what I
didn't talk about in this talk i've
already done is extend this to the
partially observable okay so it turns
out that all of this theory applies as
well to the partially observable case
but then like in the future so we want
to try this to learn user behaviors for
assistive technologies and then see as
well how we could elicit information
about users to set up the priors in many
situations the transition models also
not stationary so how can you extend
this to dynamic systems and then finally
consider as well dear slats that would
be correlated so if you've got priors
about two parameters instead of just
assuming that they're all independent
but if they're correlated okay so maybe
just have one last slide about my
research interests oh ok I talked a lot
about vision reinforcement learning and
active learning and I guess that's
because I'm very interested in machine
learning okay so that fits right here
but in general i'm interested in
sequential decision making as well as
semi-supervised learning so these are I
guess the main research areas and then I
get to apply this on in health
informatics so currently I've got two
projects beyond the one for hand-washing
where we're developing smart walkers and
another one about monitoring the
symptoms of Alzheimer's disease and then
in another line of work which is
actually in collaboration with Google
we're looking at document clustering and
more specifically how can we label the
clusters so I hope to give a talk on
this so otherwise maybe a thing who's
working with me on this could give a
talk in in your future once we've got
some good results about that because
this is some of the work that we're
doing a collaboration with Google okay
so any questions so I have in mind a
problem where if I want to do the cost
on every input is computational time
okay to get which label it is it takes a
lot of time to get how much reward do I
get by trying to how much faster will I
be by using your your strategies rather
than just spending a lot of time
computing every input and do my
classification with standard algorithms
right okay so when the reward is in
terms of computation time so this is a
very interesting problem that's known as
doing bounded reasoning okay okay and
there's been some work not well actually
people have used reinforcement learning
to look at this where the idea is that
you can gather some statistics about how
much time we would take to compute
different things and then run a
reinforcement learning algorithm on top
of that to optimize and decide at any
point in time which thing you should be
computing that will you know be short
and perhaps gives you the most
information so here are we talking about
reinforcement learning on top as a metal
learning problem or metal learning
algorithm right and then keeping all of
your options open at the lower level and
gathering some statistics about them so
that's one way people have approached
this and yeah the literature is bounded
reasoning okay thanks
any other questions ok else I have a
question soul ok so this isn't super
related to your talk or if it is I
missed the connection but the last time
I built a linear classifier which wasn't
actually at Google the one of the things
that we did which I don't really know if
it's standard practice is we introduced
random dimension to try to make sure
that the output was reasonable that you
know the random dimension attracted a
nonzero coefficient that probably
indicated a problem I'm just wondering
is that sort of standard practice in
machine learning circles and and if it
is how does that affect sort of your
approach here in terms of trying to
optimize the sequence of choices ok i'm
not sure i'm totally understanding a
question so so you've got a problem
you're adding random dimensions to it
just one random dimension one random
dimension and then what's the purpose of
adding this so at the end of the day
with linear classification you get a set
of coefficients that divide your points
right right and so the coefficient on
the random dimension ought to be zero
because it's not informative if it's not
close to zero then you might worry that
you haven't really done a valid
classification because apparently the
random signal is strong and that's not
that would be an indication that the
whole thing did not work the way you
expected it to Sookie by adding this
random that this additional dimension
that where everything is randomly
essentially verifying that your
algorithm is not over fitting is that
what's going on here sure yeah you're
essentially verifying that the labeling
that you fed into the linear classifier
caused you to do a true classification
as opposed to pick up on random noise
right ok yeah so that's you could think
of this as an alternative to let's say
cross-validation or other techniques to
ensure that you're not overfitting now
ok I think I understand the question but
or at least the setting so but you
wanted to know if I'd if it would make
sense for me to do this in my setting as
well no mate so I guess that
even directly answered my first question
which is I just didn't know if this was
a standard practice in machine learning
circle sounds like it's not this yes
I've never seen this but then it seems
like a good idea and now yeah my mind it
I would think of it really as just
another approach for verifying that your
classifier is not over fitting right so
then I was wondering if if it had been a
standard practice which is not whether
it affects your approaches for
optimizing the sequence but since since
you've not tried it then you wouldn't
know for sure whether it would make a
difference now I haven't tried it so
here so in reinforcement learning I
guess I mean in any types of machine
learning rate I guess overfitting is
definitely a possibility but okay i'm
using a bayesian approach where i guess
the key is how I define the prior so
normally one way of dealing with
overfitting machine learning is that you
define a prior that will penalize the
more complex high policies so that it
perhaps you only going to use these if
they're really going to help and if
they're only going to capture I guess
random noise which you don't want then
the penalty will essentially help you to
roll them out okay so and a lot of
machine learning approaches then people
would either use a notion of a penalty
or prior that implicitly puts penalties
on the more complex functions to
regulate and deal with overfitting in
this way so in my case that's how I'm
dealing with that problem that being
said there's probably a way to use the
idea that you just suggest it and it
would be very interesting to see yeah
half I guess how well it will work and
so on thanks
what is the gamma in balance equation Oh
gamma is the discount factor I didn't
mention it the idea is that you would
normally maximize the sum of rewards but
depending on the set and you may decide
that you want to discount as if there
was some kind of inflation rate the
future rewards and you would usually
take it to be smaller than one and non
zero so for your algorithm or even for
the for reinforcement learning problem
assuming that machines and time wouldn't
really be a constraint machine power n
times will really be a constraint is
your arguing or the other algorithms
really bound to the number of features
that they could actually handle for for
learning ok don't spray all over space
is not a constraint that that's a good
question well so in some sense i guess
if time and space is not a constraint i
mean we've got Downlands equation and
then we know that the optimal policy
satisfies Bellman's equation so it's
just a matter of finding something that
satisfies it and then you could use your
favorite search technique for that or I
mean I guess yeah we could I guess yeah
we could do Beijing reinforcement
learning perfectly in that sense yeah
but then on the other hand I mean that's
more or less what the guys in operations
research were advocating at the
beginning in the 1960s data it out all
the equations but they just didn't have
the computational tools so that didn't
really go anywhere and then it's today
that now we're looking at this work
again and and then going back to against
those equations that were quite elegant
and giving us exactly what we want but
then finding out ways of dealing with
the computational complexity so i'm not
sure that well i guess i think i think
there's a lot of problems out there that
if space and time was not an issue
already have the solution it's when you
include those two constraints that all
the problems arise just note that in
some algorithm some algorithms are not
able to converge or reach any good
learning point if you increase the
dimensionality order number of features
they're just not able to handle okay
right so I guess ya hear your question
was how does this varies with
dimensionality and and so on well so in
this problem I guess I didn't talk about
the computational complexity but so so
okay here I was assuming I've got a
discrete state space and then I could
think of features as being discrete
variable to have a certain number of
values each right and if I increase my
number of features usually increases my
state space in an exponential fashion
right but and if I don't care about time
or space it doesn't matter so I would
argue here that the math will work out
it's only in the limit if you if you
deal with infinite spaces that things
would be a bit more tricky you borrow
wise if you're still working in finite
spaces I would argue here that there's
no issue all right thank you okay let's
thank the speaker if nobody else has any
questions
what lose suspiciously it'll be that
even if we are completely wrong in our
assumptions about prayer I believes we
still better than exploit algorithm is
it true like according to a graph we
still better oh ok so here I guess I
made the right assumptions I said of a
prior in a way that it's a good prior
like you showed two cases with bad prior
and a good prayer as I understood like
yellow red and green lines it's more as
an informative prior verses and
uninformative prior so both of them are
good priors in the sense that be well ok
so let's go back to have this graph yeah
so it would be kind of like looking at
this red curve and the green curve right
let's say that the true model is at
Point 2 here right both of them are kind
of good in the sense that they have the
peak at the right location it's just
that one is way more informative because
the peak is much more higher is much
higher at Point 2 right and so I guess
in what I showed so with K equals 0 and
K equals 30 so a equals 0 would actually
correspond to the blue line so it's
completely uninformative so you start
with a uniform distribution and caicos
story will be kind of like I guess here
maybe the red line right so we already
are pretty certain about where the true
model is what would be interesting is to
see now if I didn't have the peak at the
right location right so if i had an
infrared of prior that's wrong right
then it could hurt me in the sense that
it would take more time and in any limit
i guess if I if my prior probability of
the true model is 0 then I'll obviously
never converge all right so i have to
have a nonzero probability for the true
model
okay thank you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>