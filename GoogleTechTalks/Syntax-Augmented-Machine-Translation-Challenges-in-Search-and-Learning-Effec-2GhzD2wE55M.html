<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Syntax Augmented Machine Translation: Challenges in Search and Learning Effec... | Coder Coacher - Coaching Coders</title><meta content="Syntax Augmented Machine Translation: Challenges in Search and Learning Effec... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Syntax Augmented Machine Translation: Challenges in Search and Learning Effec...</b></h2><h5 class="post__date">2007-12-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2GhzD2wE55M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright thank you guys for coming this
is a presentation on syntax augmented
machine translation and be discussing to
general topics of focus on the first one
which is the challenges in search and
learning effective syntax which will be
a little bit more about how we how we
expect to extend the system so brief up
a little bit introduction as to what
we're generally trying to do with
probabilistic synchro's context-free
grammars for a machine translation a
little bit of motivation as to why we
think that's the right way to do it will
then talk about to pass decoding with
these kinds of grammars and we try to
address some of the challenges that we
face and a couple results in syntax
augmented NP some that are actually
being generated here at Google and and
finally we do a little brief section
about learning effective syntax so
trying to go beyond the kind of grammars
that we've dealt with so far so very top
level over you just to make it clear
what we're actually doing we're starting
with parallel texts and we're trying to
extract a translation model from that
that we can then use with additional
models to translate new sentences so
today's presentation is really going to
be focused on and on two components
primarily the decoding and that's what
the the primary part of the presentation
is going to be about and then a little
bit about learning more interesting
translation models as well so why do why
are we making this this jump from
phrase-based transition to a syntax
based translation so phrase-based
translation says let's start off with
that corpus and let's extract a
translation model that consists of
surface form phrase to phrase
translations right and these for surface
form models right off the bat they don't
generalize too well when you when you've
actually learned stuff about specific
words involved when you get other words
that have similar meanings or similar
context or usage that generalization
just doesn't happen on new test data and
the reordering models for these systems
tend to be separated from the actual
words instructors involved so the rowing
models tend to save do the translation
and then jumble it up or distort it to
try to produce something that seems more
fluent and these models tend to it and
to look very locally or tend to be more
about just penalizing reordering as
opposed to letting it letting it occur
in a structural way and also phrase bae
systems have this issue where
translation cannot always be broken up
into contiguous units so if we want to
translate from one language to another
and we don't have contiguous notions of
meaning that can fit nicely into a
phrase that correspondence is going to
get broken and we'd like to have
something that generalizes that better
so probably synchronous context-free
grammars so prob list probabilistic
because we we have weights on each one
of these rules synchronous because it
represents translation from one language
to another and context-free grammars and
that that's coming from the actual
grammar form that we're the grammar
formalism that we're using so here's an
example of a kind of rule that that we
could expect so we have a left-hand side
result and we'll get back to what that
that's doing in a second we have a
source side then a vb paw translating to
does not and a vp so what's happening
here is that the source sentence which
might have the words now and paw in it
with something in between something that
we call a vb is being translated into a
target sense in this case the nun the
paw their translation is does not and
the VB is being substituted back in and
this whole thing produces another VP
that can be used in in further
compositions of rules of this form right
now and again all of this has a weight
this is not a rule based or transfer
system where these rules have been
created by hand all of these rules are
being learned automatically from corpora
now some key points about this the
translation of nipa in this in this
automatically learn rule does not depend
on the on the words in vb and only and
all VBS can be used inside the structure
and so what we have here is a process
where translation is tied intimately
with reordering we're not saying
translate first and then jumble later
we're saying translation and reordering
happen together and rewarding decisions
need to be made in context so as a bunch
of flavors of this these kinds of
grammars being used used in the
literature and so they have a couple
different perspectives and a lot of
those perspectives come from where you
started and so if you believe that in
the beginning there were syntax trees
there were parse trees and that's what
the real data is there are syntax based
approaches so we call them at a really
tree driven and so there's a there's a
listing of them there they're really
tried to
I to produce and generate translations
that respect syntactic restructure and
and implicitly respect the kinds of
rules that have been manually created
the kind of data that's been manually
created to build parses from the other
perspective we say that we don't really
care too much about the trees and then
the process of building syntactic trees
but rather we want to focus on crease to
just augment augment the process of
translation to maybe constrain it in
some ways but in very limited ways and
what we really trust are the things that
have worked well in machine translation
which are learning word to word
alignments automatically from large
corpora learning phrase to phrase
associations and then generalizing them
and there's a whole set of work that
follows this line and what's important
to note is that this work typically
doesn't get restricted by target
language tree structure or even source
language tree structure we simply try to
extend what's worked which are the
phrase based systems so if we want to
add all this this functionality we have
to pay the syntax and the syntax is the
cost for actually introducing this kind
of modeling into our system and so we
have a couple of basic problems that we
have to solve we have to make sure that
we can decode efficiently and one
additional problem is that when when we
don't have syntactic parse trees
available we still want to be able to
represent this kind of richness and
maybe learn these labels from somewhere
right so the first problem how do we
decode efficiently is going to actually
be the bulk of this talk the decoding
process from from the form of the rule
you can sort of imagine that a bottom-up
cya parsing right and where the goal is
to find the most likely translation for
spam compose these bands together and
produce a full sentence translation and
what complicates this is and it is the
use of an Engram language model the
minute you introduce an Graham language
model this decision about simple
bottom-up parsing sort of falls apart
because you've introduced a model that
introduces dependencies across spans
right so that's sort of the focus of our
discussion today so let's just give a
quick example of decoding with these
kinds of grammars if the Ngram language
model wasn't involved so we have a
source sense ilima bappa and we have
some
bank of rules that we've learned
automatically from corpora and we
perform translation so in this case the
source word VII is being translated to
go and the sorcerer is being replaced
with vb right and that says now what
we've done is we've actually translated
not directly from source to target from
source to some intermediate symbols that
are associated with the target symbols
right so now we can apply it additional
rules now we see no vb puff we have a
rule that matches that like this one
this can be applied on top of it right
and we keep going we apply this final
rule and this this just an important
point here this is just one possible
derivations what a decoder actually
needs to do is to search through the
space of all possible derivations using
all the rules that we've learned
automatically from from our corporate
some of which may be noisy some of which
may be good and it has to do the search
to find the most likely translation
based on the weights we've assigned for
these rules now when we introduce an
anagram language model this simple
search space this simple Viterbi search
that we did through this through this
did to translate the source sense to the
target sentence becomes a little bit
more complicated right so the anagram
language model introduces dependencies
on adjoining regions in the in the
parsing process so it says that so in
their traditional parsing with her
research if you find the translation for
a span you can make a comparable
decision you can throw away all
alternative chart items for that span if
they have lower cost but in this case we
can't do that because this Ngram
language model introduced dependencies
next to the items so the theoretical
runtime of introducing this rank this
this Engram language model gets really
hairy real fast right so we have the
basic s cube grass is the length of the
source ends we have the basic s cube for
for the chart based decoding but then we
have this huge grammar constant sitting
out in the side right and there's a
couple of them important terms that's
based on one is a maximum number of non
terminal symbols they could be in there
and it could all because that could be
in each rule and also the order of the
in Graham language model as well as the
the number of non terminals we'd like to
represent and some of our work we
actually try to try to build systems
that involve using a very large number
of non terminal symbol so 38,000
non-terminal symbols and using n-gram
language models of three and higher so
what happens what's what's the real
problem here so when we actually do the
decoding with an n-gram language model
we have this additional model that's
trying to say what's the probability of
the of the target word you translated
given its left context right which as
you're doing decoding you actually don't
know the left context right because
you're doing a bottom-up parsing
approach so you trade in this case where
again we're going to focus on it one
particular path the decoder in reality
has to focus on all paths and compare
them all together so what we're going to
focus on just one I'm going to see what
happens so in this case the ba
translates to go or goes or going
several alternatives and produces a vb
right so in that cell that represents a
translation of just vow of the source
word vow we have three alternatives
right now we can't throw any of them
away even though they all produce a vb
because we don't know how that go go's
are going is going to combine from the
language model perspective to the rest
of the sentence and so as we go up to
the next level we have to propagate
these information right we have with
this information we have to keep
remembering that these are three
separate hypotheses that cannot be
thrown away right so we propagate that
up to the next level let's say we have a
couple of different alternatives for the
Napa translation does not or no yep so
if we didn't have a trigram language
model right why can you throw away it's
one of those so we would just want to
find the most likely translation right
if as long as we just want to find the
most likely translation for a particular
span right then they all produce a vb
right and so therefore they're
comparable particle assumption and
deceived you exactly so we go up the
next level now we have to keep some of
these around and then now we have to
keep even more around when we get to the
very top of the sentence right and this
was just one derivation
so the materas chart structure in this
case for each cell IJ where the the cell
IJ represents I being the number of
words translated in the source and J
representing where it's starting we have
these coarse grain bins for each of the
left hand side non terminals right so
the NP or the VV stuff that we saw and
for each of those we have equivalence
classes where hypotheses are being
equivalence class based on the left and
right context and each equivalence class
may contain many chart items so when we
actually have to form a real chart item
based on rule and and and completed
hypotheses and completed equivalence
classes underneath you can sort of see
how the number of charred items really
grows right so for for applying this
complex rule with three non terminals
within each of those non terminals we
have these equivalence classes right so
we have two two for each of these
equivalence classes we have to consider
how they might combine with all the
edition with all the alternatives from
the other equivalence classes right so
and for each of these we need to compute
language model costs and we have to make
this decision and potentially we might
even have to make some decisions about
throwing some of them away simply to
make sure that the the search finishes
in time and so we might actually have to
approximate the left and right context
when making these decisions right
because again we this is all happening
for one particular chart span what q
pruning introduced by charing 2005 in
2007 says instead of generating all of
these different hypotheses let's rank
them approximately and only generate the
first k right so we maintain an ordered
set of equivalence classes in each of
those for each of those three
non-terminal spots and only generate the
first k out of them right now so this is
not an exact first cave because we don't
know how the language model interaction
the language model interaction between
alternatives causes the ranking not to
be monotone right so this is just an
approximate first K but the hope here is
that the the best translations of a
sentence are not going to require really
poor translations going further down and
so the causes of search air for Q
pruning are the fact that when you
select just this first K whole
equivalence classes are pruned away you
don't even consider whole equivalence
classes that are lower down in each of
these non-terminal symbols and you make
this decision using left and right
boundary costs that are approximate
because we we make this decision for a
particular cell without knowing how it's
going to interact with it with the rest
of the sentence so what we suggest is
let's go one step further with this with
this assumption that we only need to
consider the first K we propose a to
pass decoding approach where in the
first pass we only propagate the first
best hypotheses and what this means is
in this example here instead of
considering everything from XMP
everything from xpp everything from xbp
we only consider the first best from
each one and we only propagate this
first best up the chart we leave all the
other alternatives there we don't throw
them away we leave them in there but we
only propagate the first one and so our
new first pass runtime dramatically
drops right now by the time we get
through to the top of the sentence now
what we have because we retained all the
other alternatives who is we have a
hypergraph of translations right a hyper
graph that we can now research but now
we know that we're only going to be
searching within transitions that we
know span the whole sentence of
effectively right so we have translated
the whole sense approximately and now
our goal is to go back and look at all
the alternatives that we represented
during the first pass search but what we
also do is we try to make this search
the second pass search in a structured
way to allow the direct integration of a
left-to-right and Graham language model
right and in this case and so what we've
achieved is we've been able to limit the
language model calculations to only the
successful decoding derivations those
that do translate the whole sentence and
now we want to guide our search through
the alternatives with the Engram
language model there's alternative
approaches to this
is alternative and complementary
approaches to this so of course define
parsing users use a similar paradigm
where you start off with a simpler
grammar and once you do translation with
a simple grammar you come back and use
that as a backbone to restrict
additional search and there's also
Watanabe now introduced an approach
where by making certain strong grammar
restrictions on the synchronous CFG s
you can actually simplify your your your
translation search but these
restrictions actually prevent you from
being able to model some important
reordering patterns so what actually
happens in 1st base 1st fast decoding
right so if we look at the same graph
that we talked about before so what
actually happens here so when we look at
the the translating to a vb instead of
representing gogos and going we all we
send up to the next level is the go and
we keep the other two will come back to
them later at the second level now we we
have a couple of alternates we have does
not and no but now we only have to see
how that matches up with the go right
and same for the next level right we
keep all the other alternatives there
but we only propagate one hypotheses as
we go up so we want propagate one
charter in purcell the one with the
highest weight that includes the
language model costs go yeah not the
other two it's it's a their search air
is being made there so if you have on
one word there that you still need so if
I only have the just a couple points
there these i'm using these this goes
and going and stuff to represent
equivalence classes right in this case
go would be its own a closed class
because it's just one word but in
reality that there would be left and
right context for all these hips right
this one is the sequel that i do k plus
1 cube fooling yep but if you do do k
equals 1 q pruning rather you would
throw away all the hypotheses we
actually keep them around right because
we're going to go back through it again
okay so the result of this because we
kept all these seas alternative
around is going to result in a
hypergraph of alternative sentence
spanning parses and if we just look at
the first pass it's going to be exactly
right it's going to be cool k equals 1 q
printing but there's going to be
significant search errors right and so
now the question is in the second path
can we recover from these search errors
given that we're applying a
left-to-right language model so we'll
want to do is we want to explore this
this sentence banning hyper graph of
derivations in a way that allows the
Ngram language model to keep making
updates and to really drive the search
right we don't want to instantiate the
whole hypergraph that's a very bad idea
right because we're now we have an
exponential number of hypotheses we
don't want to evaluate every single one
of them we only want to evaluate a small
fraction and we want to pick that small
fraction based on the Ngram language
model and if we if we structure our
search in a certain way we can actually
introduce high order language models
into the search process right and this
is going to be significantly better than
just doing restoring alone so how are we
going to do this we're going to start
off with a sentence spanning hypergraph
at the very top there this underscore s
that has a weight all right well I use
underscore s to represent that it
translates the whole sentence and we're
going to look at the first alternative
in there so let's say we we have this
three non terminal rule this NP to VP 3
PP that has a weight right now we look a
little further and we say can I find any
alternatives if I use the second best
alternative for the NP 2 right so when I
use the alternate Fran p2 we have to do
we have to do a couple things so let's
say the first best was the nice dude
that was the first alternative in NP 2
and that's the one we actually found
doing search because we were looking at
first best for everything but inside NP
2 there was an alternative that we
didn't consider there's great at one the
kind man so what we'd like to do is we'd
like to find out the weight of this
alternative hypotheses and so the
questions are how can we officially
calculate W to when W includes both the
rule weights and language model
probabilities and W also includes
language model estimates that have been
made during this first pass we want to
do this by making the least cost and
Graham language model now one critical
point here is w2
let's say we're dealing in cough space
w2 could actually be a lower cost
hypothesis than w1 right so now what
we're doing is this is this is the whole
goal we know we've made errors in the
first pass now we want to come back and
correct them so what do we do we
maintain a sentence spanning beam of
crease that we're always partially
expanding at the left right and so when
we calculate this this alternative wait
all we have to do is we have to divide
out the language model probably said
that came from using the nice dude and
factor back in the alternatives that
came from the nice man and in practice
this is not going to be too bad because
we only have to be we don't have to
recompute the language models
probabilities for the whole sentence but
just up to the first non terminal right
in practice it could even be up to the
second non-terminal but we simply go up
to the first place where we need to keep
tracking right so if these on terms are
very small you could potentially have to
bridge one but but it's it's a constant
factor all right so we factor out the
estimates that we made and factor back
in these alternatives right we do this
until the full context has been
considered okay now when we do this
second pass we actually find some
interesting things right if we expanded
vp3 to gave me right that's one of the
alternatives in there we actually now
produce the nice dude gave me and some
non terminals and that a kind man gave
me and some non terminals just like in
phrase-based decoding we can actually
recombine these two hypotheses right
because at this point we know the right
context of this hypothesis is only gave
me and if one is better than the other
we can throw the worst one away because
we know that it can't suddenly have a
great language mount probability right
and so we can do language model
recombination through this process as
well so how do we measure the impact
we've proposed a different decoding
algorithm and now the question is how
can we actually evaluate this is it
faster right now and we'd like to do
this by looking at what components of
the search really take take time
and see if we've reduced that now this
two-stage search easily outperforms a
naive method which is simply saying
forget the language model during the
first pass and just use it to rescore
the the end best hypotheses right cube
pruning should be the the baseline that
we compare against right so the
alternative single pass approach that is
basically saying let me explore up to K
at each step and how do we do this what
we want to do is we want to evaluate a
couple things how many times are we
asking for novel language model
probabilities from the language model
and compare that versus model costs
right model cost is a representation of
sir chair the lower the model cost is
that means we've found according to our
model we found a better translation and
what we don't want is to leave good
translations on the table and walk away
from them right and so what we'd like to
do is we would like to keep searching
and searching and searching until we
found an acceptable model costs that
will produce a good translation how do
we measure model costs we simply measure
the cost of all the first best
hypotheses on a test corpus lower costs
implies lower search errors and the
reason why we measure against language
model cache misses are these are the
expensive ones right so how are we going
to evaluate this we're going to evaluate
this on a it's a reasonably small
corporate but this is sort of the the
resource constraints that we had when we
were doing these experiments so chinese
to english translation task about
120,000 parallel sentences and we
evaluate a couple systems we evaluate
our hierarchical system as well as our
syntax based system I have a quick time
check great thank you so we evaluate two
systems one is a purely hierarchical
system where we the grammar has only one
non terminal in it and a syntactic
system where the grammar has a large
number of non terminals in it and we'd
like to see how the two systems differ
in the way they behave in these two
decoding approaches so here are the
results the graph on the left is the
hierarchical system graph on the right
is is the is the syntax based system now
what is this curve
present right so the different points on
a curve represent selecting different
values of pruning during decoding right
selecting tighter beams during decoding
lead to larger search errors so larger
model costs and selecting loser beams
lead to lower model costs right and so
the goal that we're trying to evaluate
is to see which decoding approach can
get to the lowest cost faster right
which decoding approach can really do a
good job of exploring the search space
quickly and get us to a low model costs
right and what I've also annotated along
the way at each point so these different
points again are different pruning
levels i've also annotated the blue
scores that are associated with these
points right and this is primarily for
for just an understanding of how well
the blue score correlates to our models
but the blue score is the automatic
evaluation metric for a machine
translation that's not based on our
models it simply is trying to compare
translation your output versus some
reference translations and so what we
see here is we see we see these these
two systems and where we're evaluating
the number of language model cache
misses so we saying which system with
fewest cache misses can get to a low
model costs right and so in a
hierarchical system we see that the the
heuristic search which is what our two
past approach system is is reaching the
model costs more quickly for most for
most of this curve than the queue
printing system right and it's same it's
a similar picture for the for the syntax
based system right we quickly achieve
the model costs now a couple interesting
things are going on here one is with the
syntax based system the blue scores
annotated each of these points doesn't
seem to change much all right so as we
search more and more and more we don't
really see too much of an improvement in
blue score but for the hierarchical
system as we search more and more and
more as we make wider and wider beams we
see this pretty steady improvement we
see a pretty steady improvement that
shows that as you search more as you
find alternative language model
combinations of the
hypotheses your score is actually going
up where's for the syntax pay system you
very quickly achieve a higher score in
this case in the hierarchical system but
searching more for more and more
language model alternatives doesn't seem
to help so much there's a couple of
different interpretations there's
interpretation that favors the use of
syntax which says well as the syntax is
providing nice a good syntactic
structure providing to produce flu and
translations and that's why searching
for more and more language them all
alternatives is sort of unnecessary the
syntax is doing that for us that's a
very optimistic way to look at it more
practical way to look at it is there
might be some of that going on but there
might be also the fact that the sin tax
base system has a much larger search
space and we could be still at the tip
of the iceberg here yeah yeah the
significance level is 0.8 to school what
does that mean so if you there their
methods to try to evaluate the
significance level so when when when is
the system really different from another
system right now it's done by sampling
right so you sample the the range of
blue scores you can get if you change
your data set and so it's about point
seven eight so the idea here is if you
say one system is really better than
another system if it's got points have
any difference but there's a lot of
controversy as to whether that's a valid
thing too so the difference is that you
have there mmm are they within points ya
know there aren't within their there so
19.1 or point 191 compared to 20.7
that's more than the significance so i
need to multiply this by 100 yeah so I'm
given the blue score in in the way that
it's yeah right come you're right
compared to the the significance on the
previous slide compared to this you
should multiply this 500 right so these
are sequins yes that's sigma 2 right so
syntax vs. I require the changes their
main offices they may not be but for so
for example on the on the language model
on the hair
system across the whole range it is but
for this intention for the other yeah
from one end together but on the same
tax rate system it's not right what kind
of first step distichum significant
stink is it in this case so this is a so
this is a confidence interval that's
built based on translating different
selections of the test set and other
point just want to make this point at
the bottom we were comparing into
language model mrs. because these are
the expensive ones the language model a
total calls we make significantly fewer
language model total calls right but
typically your assistant will will cash
these calls to not have to make them
again and again now the next thing to
evaluate at the end of day is just raw
time right so we're the solid line and
the alternative approach is that it's a
dotted line so are we doing it faster
and for some part of the range we are
doing it faster and further for the rest
of the range we don't quite do it as
fast we actually cross the curve right
so we went into a little bit of an ounce
as to why this is happening and it turns
out that most of this time is actually
spent because the the memory access
pattern of the second half of the search
is actually really poor all right so we
do a lot of creation of hips a lot of
delete them a lot of creating a lot of
deleting and that's what that time is
coming from so we've used these kinds of
approaches for in a couple different
systems right so we've we've applied
these the syntax based approaches well
as compared against a hierarchical based
system for french to english translation
and we see statistically significant
improvements and currently at Google
we're also running a set of experiments
where we've we've started integrating
our system into into the Google
environment and we see right now
improvements initially with with a
purely hierarchical grammar and the
syntax pay system is coming now these
are the results that are created with a
5 gram
ash language models with two sort of Rio
architect our code a little bit and the
results are definitely still preliminary
right so these are not solid results yet
so a couple examples cases where one
system is better than the other right so
this is a kind of tree that we produce
in our translation so it says that I
present the scope of this investigation
is gradually narrowing that's the system
that the hierarchical system produces
and a phrase-based system says now it is
gradually narrowing the scope of this
survey it's so slightly different
translations we have alternative cases
as well obviously cases where the
hierarchal system is worse and in this
case it's it's pretty significantly
worse we produce the teachers I suddenly
ye xiang began quietly celebrating and
the phrase bae systems at least seems to
make slightly more sense so a couple
different alternatives and a couple
trees to look at now we've done all this
the experiments even here have been done
with with a simple hierarchal grammar in
weeks if we intended to add free
structure trees to to improve this to
add syntax into the mix but can we learn
these labels without phrase structure
trees phrase structure trees are created
for a totally different purpose they
were not created for translation
purposes right they were created based
on Cree banks that were created based on
people that felt like they understood
how sentences were created and they were
not created primarily for the purpose of
improving translation quality and so a
small small addendum to this
presentation is some current and future
work about how we we plan to address
this problem so if we don't have free
structure trees what can we do can we
learn labels that actually improve
translation quality right so we have two
examples here one is a we have a
sentence with with alignments and we say
we're missing the tree what can we do
right and so currently we can actually
just produce exits and we say that we
have a single generic non-terminal it's
not labeled we can produce some decent
translations so actually propose a
extension a novel formalism that's based
on preferences for hidden label
so these hidden labels the supposed
syntactic structure behind the sensors
never actually observed on any trees but
we can we can talk about them being
hidden variable so we'd actually like to
learn so for each rule it can prefer for
each of the four each of its non
terminals it could have a preference for
some hidden non-terminal symbol right
and these hidden labels are just indices
just like in clustering right and our
goal is to actually say can we learn
such labels discriminative Lee and as
well as learning these labels
discriminately can we marginalize that
Margit marginalize them out during
decoding because at the end of the day
we want to produce good translations as
opposed to good labeled tree
translations and so there's there's
significant present for using hidden
labels and literature to do tasks of
this form and so how does this actually
look right so previously we just had
these X's or these these non-terminal
symbols with with their name being
whatever we call them what are we wrote
them down as so now what we say is that
X spot it's not just X its X that has a
preference for being index 3 which might
be NP of point 7 and some other weights
for all the other potential different
options all right so each non-terminal
gets a preference of weights and when we
calculate the probability of a
derivation we also introduces preference
compatibility we say how compatible with
the two preferences if one rule produced
something that's a hidden type VP and
the other rule needed something of
hidden type VP and that's a good match
and if it didn't then that's probably
not such a good match and maybe that's a
bad derivation and so we can actually
write a translation model so we don't
have time to go into too much detail
here but it's simply two components the
left component is there's a traditional
probability of a tree derivation and
this additional component that measures
how well all these rules that we used
join up if they join really well it's
going to favor certain derivation if
they join poorly it's going to down
weight that derivation once we have this
we can actually take a discriminative
disk
an approach to maximize the the
probability of Oracle selected tree so
Oracle selected based on some automatic
evaluation metric we can maximize this
conditional probability and we can train
the parameters of the system all right
and so we have two kinds of parameters
in the system one is our traditional
model costs and then millions of little
features and these little features are
for each rule for each non-terminal
there's a preference for being one of a
set of hidden labels right so we can we
can actually easily take the gradient of
those features and we can optimize the
system yep welcome here or goofy and
yeah pieces you consider yeah they're
coming from completely different data
right well the tree they consider is
from the only vote data that you learn
science fair but the Oracle is from
steel wingless no no the Oracle is
simply generic single non-terminal what
you said selected by automatic metric
what do you mean by that so I say use a
simple grammar to translate a sense and
generated some best list right so now
you have n best trees with a simple
grammar one of them was really good by
the blue score so it's just elected by
the most absolutely not like yeah oster
you know know that that Oracle is by
blue score right so that's that's the
whole thing we want to introduce we want
to pick labels that produce good
translations not produce good linguists
all right okay so so it's a simple
estimation procedure that I just
actually described to you we extract a
simple grammar we train the monolithic
model parameters on some development
corpus and we translate some other
corpus right we and we iteratively
translate that other corpus to produce
an best lists let a Oracle selector
which in this case is an automatic
evaluation metric pick good translations
and run this optimization right and we
can repeat this until no new trees are
found during translation step and the
goal here is that ideally good labels
are learned that actually explain good
translation
right as opposed to these non terminal
categories and so the impact of this
should ideally you should be able to
reduce spirits ambiguity in the grammar
because we actually do get to some out
all these hidden non terminals that's
how the model is written and this
formalism generalizes the previous work
in the field which is using a single
non-terminal symbol using these multiple
non-terminal symbols from syntax and
it's a discriminative approach we're
really trying to improve translation
quality so general top level conclusions
have spent the most time talking about
this to pass algorithm that really tries
to avoid unnecessary language model
calls and we have some results to
demonstrate the value of using these
these kinds of systems and a small
proposition for a formal aslam model to
learn labels tisk romantically that's
about it
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>