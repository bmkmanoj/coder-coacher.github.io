<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Block splitting for... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Block splitting for... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Block splitting for...</b></h2><h5 class="post__date">2012-02-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Fn8WU6WpgQM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Garcia learning so this is a joint work
with us meeting point
so the problem considering is how to
decompose many problems by both features
and examples at the same time and the
problem for me oracle gap is essentially
regularizing loss organization so
writing it as the name is ella y + RX
subjective y equals ax if your a is the
matrix and x4 and rebecca and the reason
my demons slightly unusual form is
it'll make some decomposition stuff
we'll see my commander
so now we will consider partitioning ax
into and blocks consider positioning the
future mate they started to potentially
uneven blocks of data and features 40 j
IJ is an mi really is in mi r mi by NJ
and you never actually formed this total
a matrix it's just to visualize kind of
what's going on and the idea is to
handle each a IJ separately in the
fitting procedure and never transferred
over the network now you could also
apply this kind of method where either M
or N or one so you don't necessarily
need to split across both data and the
example under features excuse me but you
have that option also it of course
applies to cases where you might want to
do parallel fitting on a single machine
though you know the examples i'll be
using are distributed nature all right
just use this so the outline is we'll
talk about the high-level algorithm
that's the basis for the algorithm
considered here then the block splitting
method and then we'll wrap up so the
core algorithm will be using is
something called the alternating
Direction method of multipliers or 80 mm
I'm not going to go into motivating how
the algorithm works but we're just going
to sort of use it wholesale so if you
consider the generic convex optimization
problem minimize F of Z this is not the
same F from before but minimize F of Z
subject is Z and some convex set C then
one form of a dmm which goes under many
names one of which is Douglas Rockford
splitting it's also a special case of
something called a proximal point method
as many many other names is this one so
essentially it splits into three phases
the first one is a proc step and we'll
review exactly what that is the next
slide is a there's a proc step then
there's a Euclidean projection onto the
set C
etsy and then we do a dual update which
is just didn't you know vector sum and
I'm not going to go into all the
convergence theory but they'll be a
reference at the end if you're
interested in that but basically it
converges under very general conditions
including when now f is extended real
valued and things like that you can
embed constraints in in the domain of F
so the proximal operator just to review
is a small typo this X update but
essentially you're the proximal operator
of the function f with parameter row
which is a positive constant positive
scalar rather you minimize F plus this
quadratic regularization term which
penalizes deviation from B where B is
the argument in the rest of the slides
the parameter row is omitted just to
make the notation little clearer cleaner
but it's in there the entire time so
some special cases that most you're
probably familiar with her if f is the
indicator function of the convex set
then the proximal operator reduces to
projection onto the set so you can think
of it as a generalized projection and it
shares many properties with projections
as well if F is l1 norm then this
becomes soft element-wise soft
thresholding you know a very simple
operation to evaluate and if F is a
quadratic then its element wise scaling
with this multiplicative constant so in
it's not going to be the case that you
always have an analytical formula for
evaluating the prox operator but it
happens reasonably often and even when
that's not the case it can often be
evaluated very efficiently that's the
main takeaway here
it's now a look at the block splitting
method so the basic approach is we'll
take the standard form problem we had
before the regularized loss minimization
in that two variable form with X's and
Y's will transform it in a certain way
we'll apply this form of a dmm to it
will simplify it a little bit and
that'll be that'll be the algorithm so
it's not actually a new algorithm it's
just sort of running it on a different a
different problem okay so this is the
slightly death slide so now assume that
the loss function l and the
regularization function are our block
separable for any kind of blocks and
then the problem becomes this so in
general we're going to use the index I
to index shards of data and the index J
to index shards of variables so the
there is a loss it's some of the losses
over chunks of data and those m of those
so that's going over block rose and then
summing the regularization function over
sets of variables and many problems can
be written in this form so you know
regularize generalized linear models
linear support vector machines and so on
so now what we do is we introduce a lot
of additional variable so essentially
just rewrite the problem in a higher
dimension I guess so now we have Y eyes
which are again / you can think of those
are sort of partial responses for the I
set of examples X J's which is just your
parameters for the jf block and then we
also have X I JS which are the IJ blocks
opinion of what XJ is and why I JS which
are sort of the partial response is only
using the data a i j and then in order
to make the problem equivalent to what
we had before we have to add these two
constraints we constrain that the xij is
overall I are equal to XJ
that's a consensus constraint and then
that the the y i's some up in the
appropriate way and then we have that y
IJ is equal to a aij xij you couldn't
squint to this a little bit in it it's
it's equivalent to it the problem at the
top the point of this is that it'll make
the problem split apart so now what we
do is final is the final step which is
that we move the last constraint y IJ
equals a aij xij into the objective and
we just include that as a sum of
indicator functions of that set so now
the thing to observe is that the three
terms in the objective involve different
sets of variables so the first one
involves only why I then XJ and then why
I jxj xij sorry and the two sets of
constraints also involve separate sets
of variables so the first set of
constraints is most X's and the second
set involves wise which is what's going
to split everything apart and now we
apply the algorithm from before and we
simplify so for time reasons I'm not
going through the simplification but
this is what you get it looks a little
complicated but it it isn't so bad so
the first the first three steps are
again if you recall the algorithm had
three phases it was a proc step a
projection and then a duo update so the
simplest one to see is the dual update
you've got you know all of these
different variables and now they split
it's just they're just element wise some
so you can write them as three distinct
updates here you can see those those
three of the dual updates at the bottom
the top two are the prox function and
that's because the objective has
different sets of variables so you get a
1 proc step for the loss function li and
one prox update for the regularization
function or a J and then the the last
term here was this indicator and if you
call the proximate indicator function as
a projection which is where this pi IJ
comes from so that's projection onto the
constraint set for y IJ equals a aij xij
then these averaging and exchange
operations are projections on to the
consensus constraint x IJ equals x j and
the we call an exchange constraint why I
the sum of these why I chase and those
are the steps that do the communication
and you can see it's written out here at
the bottom that this is what the
exchange step is averaging is just
element wise averaging the main point
here is just that the collaboration is
just involves summing a set of vectors
so that's the part to look at here I'll
leap this up for just a second but it's
you still would just read this as a
three-phase kind of out of them now just
to visualize this is a little cartoon of
what the phase of the algorithm look
like so if you picture each of these
cells in the grid as being a particular
block in that original matrix and
suppose you have a process or thread or
machine or whatever handling that um and
it's laid out in the sort of grid form
just to visualize it then the proc steps
the PI J and the dual updates don't
involve any communication those are just
handled for with each local subsystem
okay then you do this averaging step
that involves sharing information across
blocks of data for a given columns or so
for a given set of variables so this is
now you do big n separate communication
steps here in parallel and then the
final thing is the exchange operation
which collaborates across sets of
variables but within each block of data
so you never have sort of diagonal
communication or anything like that the
blue dots and the red dots are just X's
and Y's so it's just to emphasize that
the consensus is on the the X's and the
exchanges on the Y so in principle you
could actually do the consents the
consensus in exchange in parallel also
but
you know that's a little bit more
complicated
so now the one remaining thing that I
hadn't described is how to evaluate pi i
j and this actually ends up being the
bulk of what you're doing in
computationally and running this
algorithm so involves solving this again
it's a projection onto a constraint of
the form y equals ax and involves
solving a quasi definite system of this
form quasi definite basically means the
top the top left block is positive
definite and the bottom right block is
negative definite which is obviously the
case here and one way of solving this is
just to you know give a concrete example
is this thing given here you wouldn't
necessarily do this if a was sparse but
just to take the case where a is dense
might solve it this way so an important
thing to observe is that this is the
only operation and the entire algorithm
that actually touches the data so the
the proc steps that handle the loss the
regularization don't ever touch the data
it only appears in this those are the
nonlinearities and the algorithm the
data only ever appears in this linear
operator and the way you would actually
implement this is you factor at least if
you're using a direct method which I'll
focus on here you'd factor this matrix I
a transpose a negative I or if you did
it this way this I plus a transpose
matrix let's take one ski factorization
of that that's a somewhat large upfront
cost but then you can cash that
factorization and reuse it in all
subsequent iterations just using back
Sol's which are much cheaper
particularly in the dense case and we'll
see some experiments with that in a
second
so here's just a example of what the
algorithm looks like for a particular
problem so take the lasso so here you're
minimizing ax minus B squared plus an l1
term so in the standard form we had
before the loss is quadratic and the
regularization is l1 the small typo here
should be lambda over N since you're
splitting it at the end blocks but
anyway then the prox operators are the
examples that we saw before so the
quadratic one becomes this
multiplicative element-wise scaling on
vectors and the l1 proxes soft specialty
so those are both very simple vector
operations so you get that the the first
step is on why I is the element wise
scaling then you do a soft threshold
step and then the rest of it is the same
so then you do this pi IJ projection for
all the aig blocks and parallel then you
do averaging and and so on and so forth
so in general if you were to derive an
instance of this algorithm for whatever
problem you're looking at you basically
just have to work out the prox operators
of li and rj swap out the first two
lines than the rest of it is the same
so here's a numerical experiment so here
we're just doing it with a with dense
matrices just I mean you can obviously
do everything with the sports feature
matrix but it's just to emphasize that
it doesn't the scaling doesn't rely on
sparsity or anything like that so each a
IJ is a 3000 x 5000 dense matrix and
this solvers written in C use of the MPI
for the communication and the canoe
scientific library for the linear
algebra edness is all done on Amazon ec2
talk a little bit more about that if
anyone is interested so the computation
times all these times are in seconds we
have three scenarios so one is that we
have a wee tile these AI JS into a four
by two systems we've got four blocks of
data and two blocks of features then the
second one is eight blocks of data and
five blocks of features and then eight
and ten so the nonzero the number of
nonzero entries because the matrices are
dense are 120 million six hundred
million and at one point two billion
here the number of cores use is one per
block essentially here so you've got 8
40 and 80 the factorization time is all
completely in parallel so that's the
same regardless of how large your
problem is that takes 15 seconds in this
case so that's a one-time cost again
then you cash that all the subsequent
iterations take around a tenth of a
second to actually carry out and these
problems didn't get large enough where
those start the communications started
to degrade the speed but I mean here
they stay pretty pretty much constant
the total number of iterations you end
up running in these cases or 9230 and
490 which seems like a substantial
increase but if you look at the the
actual amount of wall clock time it
takes to run the main loop excluding the
initial factorization it's 10 seconds 27
seconds and 60 seconds respectively
which gives a total time of 28 50 and 80
if you add in time for data loading and
and stuff like that
so just a couple of observations in many
but not all cases evaluating this pi IJ
opera operators the majority of the work
you're actually carrying out you can get
large speedups via this factorization
caching trick and if you're using an
iterative method to solve that linear
system and you could warm start it and
there's you know other tricks that you
can use they're all the communication
involves summing up a bunch of vectors
right because you've got an averaging
step that's just signing and then the
exchange the collaborative part of that
was also summing so you can do
everything via all reduce operations or
equivalent in whatever programming
framework you're using so MPI has an all
reduce valpo rabbit has a Hadoop
compatible all reduce and any of those
would be fine and again emphasize this
sort of unusual aspect of the algorithm
it splits apart the nonlinear parts
which is the objective and the data
which is handled using this linear
operator and not of this is nesting
point in doing this but you could in
principle fit multiple models
simultaneously right because the
factorization that you cash is the same
regardless of what model you're actually
fitting so another way to think of it
might be is in analogy with sufficient
statistics or something it's like a
sufficient operator you could chuck the
data after doing that initial
factorization cash and then just reuse
it whenever regardless of what were you
doing and anyway that's basically it the
method basically coordinates a bunch of
processors each solving you know small
convex optimization problem to solve a
larger problem you can split by data
features or both the algorithms are
quite easy to implement in general
that's basically it here are some
references there's the workshop paper
then there's a longer paper on 80mm for
distributed optimization that doesn't
include the sort of to a splitting case
that has a lot of lot more background on
the algorithm itself if you're
interested and it's available from
either stephen boyd site or my site and
any questions please
I
no well not exactly because he
it's the same book it's more like one
variable how you're calling a variable Z
a stack version of tax at body
variable
just a solid one because replication
various all the same converted results
Monique sunblock yes what's more
gasps yes so I mean it's
privacy-preserving
DJ I James
blocking how you split up your overall
matrix and number point into the AF K
consider here who might have to go to be
stored enough slip for more important
might have some sometime you're saying
other problems will have slid it for
example if you're
glass office regularizing Chancellor
not one position
yes yes so for instance if you want the
old regularization tab say the three
different lambdas
this
so mentioned with the
where
so then
Oh
and also serve a
I
then we allow
ah
there's no you I think so that the pie
and Jace or the ball to work with the
proximate simple so for the last so
that's the case of a possible process or
not
these cases till now really
is it a fitting multiple model the same
time
application
you're not
case with us working play
what's a truce and classification
pair of them
paralyzed the suppression of
lambdas
you could fit that area
I don't follow there but
the sea
this one is quickly so this one happens
yes of this again with opportunity
apologies God does this heavy lifting in
parallel by itself no communication
occasions just element-wise vector songs
and they were doing these wires and sj's
this yep
not necessarily an additional benefit
but he also gave a very public saw that
this way is north one is a transpose but
there are direct direct methods for
solving that positiveness system the
topic we've taken WL cancellous
factorization catch that
any of the whole matrix of clocks cars
and you'd also get there is an official
many gifts for senior is possible an
entire day on 80 and it would just drop
out with the party
these actually yes x usually women yes
change them yes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>