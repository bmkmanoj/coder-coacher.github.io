<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Brains to Understand the World's Data | Coder Coacher - Coaching Coders</title><meta content="Building Brains to Understand the World's Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building Brains to Understand the World's Data</b></h2><h5 class="post__date">2013-03-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4y43qwS8fl4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm mincing gall
invited me to dinner with Jeff here a
couple weeks ago and it was I think we
would probably talk for three hours in
that and we probably could have talked
for 10 hours because the area that he's
he's interested is one that Larry is
also interested in which is machine
intelligence and how do we push to the
next generation and it's obviously
really important to our company but he
has an entire career where he's
basically built from graduate school on
a mechanism to basically allow him to
study what is I think one of the most
fascinating fields that any of us will
ever be involved in and it's
biologically inspired you know machine
intelligence and I you know for me it
combines two really really interesting
thing one is obviously computing and how
we commute and things like that but but
more importantly how we think cause you
know how humans think and I think what
was really exciting to me is the
advances made in biology to understand
what the mind does and and how it
processes things and the different
mechanisms it uses and what it really
came out it came to me was it's an
entirely different computing model and I
think that's what's so exciting about it
so I will go into a you know the long
and illustrious history I'll let Jeff do
that but but I'm just so pleased and
happy both that he could come and and
all the interest in the room here so
thank you very much thanks Alan I don't
need that I should I should I should be
good with this Mike hopefully y'all can
see me well thanks for coming out here
and just keep piling in and sit wherever
you want
I guess it's a pleasure to be here I'm
not gonna tell you about my long career
I'll skip all that I've had two careers
one in mobile computing and narrow
another one in brains and neuroscience
but really it's the brains and
neurosciences for the one that's been
going on for a long time so I'm gonna
talk about machine intelligence I'm
gonna talk about what it is I'm gonna
talk about how I believe we can get
there and what we're gonna do I'm gonna
get there and some of the parts of the
talk will be pretty technical some will
be less so I'm gonna start off a little
bit sort of an intro history here my
approach to this hopefully this is gonna
work now let's try so head it down
that's it my approach to this has been
the following it's a very biological
approach is first to discover the
operating principles the New York
and once we understand those principles
we can then build machines to work on
those principles and when the neocortex
just in case you've forgotten it's the
big wrinkly thing on the top of your
brain it's about 75 percent of the
volume of your brain it's where all
high-level thought language vision
occurs anything you can tell me about
the world stored in your cortex mind
speaking yours is listening
that's the organ of intelligence our
approach is like the follow-ups oops
only go back which I could get me just
to this
our approach is the following we start
with very detail in atom in Physiology
I'm more of a neuroscientist same and
machine learning person in some ways I
view the brain as a set of constraints
this is the one proof case we know that
this can be done and and we know a lot
about the anatomy physiology the brain
and all kinds of stuff and and if we're
going to have a theory about what
intelligence is it has to be consistent
with that we don't have to emulate all
of that but we at least have to realize
that that is a set of constraint from
that we developed the radical principles
we can test those empirically back in
the neuroscience but we can also test
and by building the stuff in software
and and and experimenting this is what
we're doing today ultimately this is
going to go into silicon we've got
actually got one project starting this
year with another major computing
company to do some silicon of our
algorithms but today it's all in
software now I thought I added a few
slides here to beginning my talk which I
don't normally do I wanted to give you a
little bit of historical perspective
because I know there's a lot of people
here in machine learning community and
some really experts and I just want to
give you a little my view of where we've
come from and how we're getting there so
I'm going to go back in time we're gonna
start so the history of AI this guy Alan
Turing is where I began in AI he in
starting the 1930s he started talking
about computers as universal machines
the universal Turing machine as we call
him now and he was very much interested
in computer intelligence machine he
thought we can make computers
intelligent he didn't want to get in
arguments about it he said he wrote
about this is I didn't want to argue
with people whether it's possible
whether machines can be conscious she
said I came up the idea of the Turing
test specifically so we just said look
if machine could pass the Turing test
we'll just up the Great's intelligence
and so that's what he did so and he
wrote that paper in 1950 the one that's
shown right here and this is sort of the
beginning of the AI movement
unfortunately what this did is a set the
idea that the goal of AI is to do things
that humans do and I don't I
don't think that's the goal of machine
intelligence we don't have to replicate
what humans do we can do things are
super or less it doesn't it's not that
specifically so the field of AI went on
for many years it's still going on I
called the no neuroscience approach
there's been many projects and
techniques developed over the years I'm
quite successful some less so I've shown
some pictures some of the great
achievements here playing chess winning
at jeopardy I put the Google car there I
want one major initiatives my summary of
this is the good news about it you can
come up with good solutions to problems
we care about the bad thing is that
they're very task specific they're kind
of brittle they don't really do the
Google car doesn't know how to play
chess it's not gonna do my laundry and
it's not gonna think about physics and
so there's very TAS specific and very
I'm limited and no learning and and what
makes us makes intelligence for human of
course is we learn about everything and
I can do all those things and so can you
and so I don't never view this as a
really great way of getting to true
machine intelligence now let's talk
about the history of artificial neural
network so the brain approach if you
will I start this back in the age back
with these two guys Warren McCulloch and
Walter Pitts they wrote a paper in 1943
where they talked about well you know
neurons could be like logic gates and if
we put them together we can do computing
with them so that's an interesting idea
well it's really not the way brains work
at all but they but what they did is
introduced the idea of an artificial
neuron that can be used in an artificial
neural network now their approach was
kind of odd they on the left you see a
real neuron in the middle you see what
is a classic artificial neural neural
neuron which is I sort of a sudden two
weights and activation function that's
nothing at all like a real neuron and on
the right is what McCulloch and Pitt's
showed which is his little like really
really ridiculous neurons were doing an
x' and ORS and nuts but that was the
beginning of the AI I mean the
artificial neural network error over the
years been many many different things in
this category you really can't put him
under one one category but I call him
minimal neuroscience you'll see in a
moment so starting in the in the 80s we
had a real resurgence and interest in
artificial neural networks back
propagation both machines and so on
this is continued on today into the
field of machine learning and the
current hot topic there is Dilip beef
belief networks
I would say the good news about this is
well these are learning systems that's
good they're distributed that's good
they're really good classifiers that's
what mostly they're used for so sorry
classification problems the down
Sciences are very limited they don't do
much more there's a few exceptions to
that and they're not brain like at all
and I don't didn't feel like this is
ditte like when these came hot again in
mid 80s and I was working on this in the
mid 80s I was very disappointed because
people didn't they weren't really paying
attention to the neuroscience they were
just saying hey we have these neuron
like things and we're doing stuff with
them and we know a lot from the brains
we should be going back to them and
lately there's been another thrust here
which I call it which you might call
like the whole brain simulator since a
maximal neuroscience you might have
heard about this there's a project in
Europe under Henry Markram called the
human brain project and their goal right
now this got huge amount of funding
their goal is to simulate an entire
brain from the ion channels to the
spikes to the neurons to everything up
the sudden you know psychology that
chart in the middle is the principal
researchers on this project it tells you
how many people working on this they do
these great simulations the problem with
this is there's no theory they'll omit
this there's no theory they have no idea
what this is supposed to do is they like
hook it all up and see what happens and
it and it's not going to lead to into
machine intelligence we need theory and
we need principal ways of going about
this so that takes me back to my
approach which is essentially like look
you've got to pay attention the brains
we don't have to copy everything that
goes on the brain but you better
understand what it's doing before you
decide what else to do and that's the
approach I've been taking so I'm gonna
walk you through the progress we've been
making recently and that's really cool
um first of all we have to talk about a
little bit about the new your cortex
it's a memory system so if you think
about as a computer forget it it's not
it's a memory system it's a when you're
born it's a blank memory system and as
you grow up you fill it up and you learn
things now it's a it's a memory system
that's attached to some some your senses
and you learn everything through your
senses you have from your eyes the optic
nerve is about a million fibers your
somatosensory nervous about a million
fibers your auditory nerve is about
thirty thousand so you got a couple
million inputs going into your brain
they're fairly high velocity not in
computer terms but they're there change
on the order of tens of milliseconds or
hundreds of milliseconds your eyes are
moving every few hundred milliseconds a
completely new innovation going on and
my speech is changing in the orders of
tens of
so you've got this fast data stream
coming into the brain the brain has to
build the model of the world when you're
born you don't know about things like
Google you don't know about buildings
and chairs and cars and computers and
and operating systems and bananas none
of this stuff you have to learn
everything it's an amazing amount of
stuff and you have to learn it through
this fast stream of data coming in and
so the built in York torque sticks
builds a model the world and from that
model just three basic things it makes
predictions about future events it says
hey given my current state and what's
coming in what's going to happen next it
can detect anomalies which are just
predictions that don't come true and can
take actions it turns out actions are
actually the same as predictions and so
when we take actions we back interact
with the world and so actually turns out
that most of the things that you
experience in the world most of the
changes on your sensory organs are
caused by your own actions so as I move
around the room and I've turn my eyes
and so on I'm controlling what the
patterns that are changing here and and
so there's this real big type feedback
loop that goes on between a sensory and
motor action if I were to sum this all
up and say what is this system doing
this is what it's doing the neocortex
learns a sensory motor model of the
world it learns given a sequence of
sensory patterns in a sequence of
actions what's going to happen next what
do I have my expectations of what's
going to happen next
can I predict the future and can I
predict what I should do to get the
future that I want and it says this is
basically what it's all about to
building a sensory motor model of the
world and we want to know how it does
that well we know a lot about it we've
learned a lot over the years and I'm
going to give you the top six principles
right now and this is no particular
order and this is my I made these up I
mean I didn't make them up I put my I
made the list up but but just going
through them here first number one it's
an online learning system so it's a
memory system but it's it's got to work
in an online fashion meaning it's got to
work in a streaming mode there is no
batch processing the brain doesn't get
to look at the statistics of a database
it says it's coming in I got to act on
it immediately I have to incorporate
into my model immediately it's an online
learning system and this is essential in
a world where patterns are always
changing and where the underlying
structure in the world is changing you
have to have an online learning system
you can't do this in a batch mode
don't work in a batch mode they don't
know statistics and that in that sense
the second thing is we know that the New
York part exit is a hierarchy it looks
like a sheet of cells it's just 2 or 3
millimeters thick it's about the size of
a dinner napkin for a human and but it's
on the reason all that looks very very
similar but we know from Anatomy and
other reasons that it's that it's
actually different regions that are
connected together with these nerve
bundles in a hierarchical fashion
there's an estimated anywhere between
dozens and maybe a couple of hundred
regions in the New York cortex a human
neocortex depending how you're counting
we know that these regions are in this
hierarchy we know that information flows
up the hierarchy and down the hierarchy
we also know that the regions amazingly
seem to be doing the same thing there
are slight variations to this but the
basic idea is that every region is doing
the same thing at all doing the same
type of memory function so it doesn't
matter if you say well this is a visual
area that in your cortex it's visual
because it's getting input from the
optic nerve or other visual regions and
auditory areas because what is where
it's getting input from and we can
rewire the brains and they in these
regions take on new meanings and new
roles so we have this hierarchy of
memory regions that are all doing the
same thing which tells us that we
understand what one region is doing then
and we understand how the hierarchy
works we're a long way towards our goal
the third thing is what is the kind of
memory that it's storing 90% not all of
it but 90% of what things with the
neocortex is storing its sequence memory
it's its patterns over time this may not
be obvious to you let me walk you
through it this is almost all the
inference and motor behavior is sequence
memory so you're listening to my speech
and hopefully I'm not talking too
quickly but the patterns are coming in
over time the order matters you you have
in your brain stored what words sound
like and what certain phrases sound like
and your brain is matching them up in
time if we if we move the order and
pattern of those those patterns are in
different order it would be garbage
totally garbage and people tend to think
like well visions not like that is
vision is like that real vision is you
are moving constantly through the world
your eyes are moving every two of three
four times a second and your and every
time this is this constant change these
are not images that are just randomly
presented to you your brain is directing
this and it's figuring out what pattern
and so when I look to the right I know
what to expect I look to the left I
should see Alan Lee over here again
there is and so I have these
expectations about the world
so even vision is this all inference you
think about language music vision even
when I touch things it's all temporal
patterns the second thing is we generate
motor behavior and motor behavior is of
course another sequence my speech right
now is involving dozens of muscles being
exercised in a very precise patterns
extremely precise patterns over time to
generate these these speech patterns and
and and that's true for all my behavior
and so we're playing these back these
are stored patterns there's a story I
could repeat them they're stored I can
repeat them because they're if you know
I've got these things in my head I know
these I've learned how to do these
things I could give this talk blind and
probably my sleep not true just for you
guys so it's all about sequence memory
and that's the key and that's an area
that has not been explored enough the
fourth item here is that when you look
in the brain we find the same type of
representations it's called sparse
distributed representations everywhere
you look you see lots of neurons doesn't
matter where in the brain very few are
active most are relatively inactive this
is true on the sensory streams as well
we now understand a lot about this and
it's a critical component of how the
whole system works I'm going to go into
a detail in this talk the fifth one here
is that we used to think that the
regions of the brain that some are motor
regions and some are sensory regions
like this is the primary visual area
this is the primary motor area and so on
we now know this is not true every
region in the neocortex has cells that
are both both inference or sensory and
motor and they're differentiated the
layer of five cells of the motor cells
but everywhere you go even primary
visual cortex has cells that project to
something that's motor the muscles that
make your eyes move so there is no pure
sensory or pure motor in fact this is
another beautiful discovery essentially
every part of the cortex is doing is
sensory motor learning and so we want to
figure out how it does that finally the
last of my six elements here is
attention attention there's different
types of attention I'm not going to go
into detail here today but essentially I
need to be able to attend to various
parts of my input stream various parts
of what I'm doing at different point of
time now I claim these are the
primary six things that are going on the
neocortex now make a further claim I
think they're both necessary and
sufficient for biological and machine
intelligence I don't think you can
you're going to get there without these
L elements it doesn't mean there aren't
other things we can do but these in my
mind are necessary and sufficient
they're necessary to learn a sensory
motor model of the world all mammals
have a neocortex and they all have these
principles from a mouse to human
dolphins monkeys cats dogs they all have
these principles operating in their
brains so you don't have to be human
level to need these things but they all
have this going on there's things I
didn't put in this list for example I
didn't put language you don't need to
have language to be to be smart you need
to be a human smart yes but to be to
build a model of the world no dolphins
are really smart they have very limited
language I didn't put other things
episodic memory dreaming I think you
don't even have to have a body you have
to have motor output but that could be
all virtual in some you know cyberspace
someplace so you know there's a lot of
things you don't have to have to be
build machine intelligence but I believe
these six elements and if you're going
to forget everything else I talked about
today just remember there's six things
and the most important one is sparse
distributed representations which I'm
going to talk about next I'm going to
talk about three of these actually I'm
talking about smart strip
representations gonna talk about
sequence memory and I'm gonna talking
about the online learning we know we
know a lot about some of these but
there's a lot of things we don't know
there's a ton of stuff I still don't
know and don't understand but we know
enough that we're actually starting to
build the stuff and it works pretty
pretty well okay we're going to jump
into sparse distributive representations
the best way to understand sparse
distributive representations is to think
compare it the way we do things and
computers which I'll call dense
representations so what are we doing
computers we want to represent something
we take a word 8 to 128 bits another
world up to these days we consider all
combinations of ones and zeroes from
Alton's 0 0 0 0 2 1 1 1 1 an example of
course is ASCII code and there's a
letter represented for a letter BAM now
we can say things like well what do the
bits mean in this code they don't mean
anything if I say what's the third bit
in an ASCII code mean it doesn't mean
anything it's just the whole number
means something well something could say
well it means an 8-bit you know that's
not what it means
I mean don't tell me anything about the
letter M and these representations are
arbitrary they you know we could assign
to different represent
the letter M and it would have been just
fun as long as we all use the same
convention in in brains it works very
differently first of all you always have
lots of things when I talk about bits
you can think of as a neuron right when
I say the bits of one the neurons active
move it to zero the neurons are not
active program so we have many many bits
thousands of them and they're mostly
inactive so mostly zeros and a few ones
in our work very often and for this talk
I'm going to stick to an example where
we're using 2000 bits of which 2 percent
are active so I have 40 ones in 1960
zeros that's how I'm going to represent
everything I can do there's many many
ways I can pick from those that's not
going to represent everything now the
key thing about the several key things
about sports distribute representations
but this one is that each bit has
semantic meaning you couldn't actually
say what it means if you if you knew it
these are learned no one's going to
assign it in the brain or an intelligent
Shane it's learned but it's relatively
stable and in these there's mean
something and and so when we want to
pick representation what we just sort of
a competitive process we took the top 40
semantic attributes and those are the
ones are going to be in our
representation if I were to do if I want
to represent the letter you know the
letters of the alphabet using sparse
distributed representations and I would
not do this this is just purely for
example if I wanted to engineer this I
could say okay I have bits that
represent is this a consonant or vowel
is this what does it sound like ISM an
AE IO sound is it a fricative sounds
they're hard sound a soft sound how do I
draw it do I have a senders or
descenders is a closed shape where is it
in the alphabet what's it next to what
other meanings does this thing have and
I could come up with all these
attributes and then I pick the top 40
that represents any particular letter
okay that's the basic idea now there's
some there's some properties with sparse
distributed representations I'm going to
go through a few of them here that are
really really important the first is
semantic similarity basically if I took
two Sparsit representations and I
compared them bit for bit if they share
a bit in the same location then they're
sharing semantic meaning this is not
arbitrate this doesn't happen by chance
it's meaningful even just a few bits of
overlap between two representations is
statistically very significant but it's
also semantically significant now what
have I asked you to store one of these
patterns I want to remember this
and I'm gonna ask you here's a new
pattern coming in I want you to tell me
if you've seen it before
alright so you might say well I'll store
2,000 bits and then the new 2,000 pits
comes I'll check and see if it's the
same thing that's not the way we're
gonna do it we're gonna store the
indexes of the one bits
so we're say okay all you need to
remember where the one bits are and so I
have 41 bit to my representation so all
40 indices to the one bits and if I see
a new representation I look in those
locations if you see ones I know I got
the same representation because every
representation has 41 what if I told you
you couldn't do that I said you could
only subsample you can only sample off a
few of the one bits
you can't store the locations of all 40
so I'll say 10 you can only pick 10 will
randomly pick 10 now you have injustice
to ten of the 1 bits now I show you a
new pattern you say is this the same
pattern and you're looking say yes the
ones are all in the same location the 10
ones I you know I know about is it the
same pattern or not we said well I don't
know what about the other 30 bits that
could be different right well turns out
that's very unlikely to be different but
even if they were different I'd be
making a mistake but a mistake for
something that's semantically very
similar to the thing they're stored and
this is the key to generalization in the
brain is that you don't need to store
the location of all the bits to know
everything is basically when you make
errors you're making errors of semantics
generalization in fact I could up it I
could say you know what it's good enough
to just match 5 or 8 of these things and
still like have a semantic
generalization so I have a way of
scaling it back up and down I'm gonna
take the last property here and you'll
see why all this works in a moment how
we're gonna use this is one of union
membership I could take this I took 10
sparse distributor representations and I
ordered them together so now you have
2,000 bits but instead of 2% of the bits
being active it's about 20% of the bits
being active and that's a one-way street
I can't undo that I can't say oh what
would the ten that that we're in there
can't do it but I can do something
almost as good I can show you new sparse
distribution and ask is this one of the
original ten by looking at the Union and
I can do that I can say well just look
for the ones that are in the new one and
see if they're in the Union and if they
are I'm gonna claim it's it's one of the
original ten now you might say hey it
could make a mistake right it could be
picking someone's from the first one and
someone's from the second one so on
statistically extremely astronomically
unlikely to happen but even if it did
make a mistake it wouldn't matter
because I'm I'm going to be making a
mistake for something that's
semantically very similar to the thing I
stored earlier and that's good enough in
fact it's what we want it's not even
good enough it's actually desirable
okay again if you want to you can forget
everything else and zone out the rest of
the talk remember the future of
intelligent machines is sparse
distributor presentation I'm telling you
is no other way around it if you want to
and and if you want to I was just
talking to John down here earlier about
you know this I believe is the future
for understanding language as well and
text because this is how the brain does
it okay we're now going to skip to the
next thing sequence memory this is 90%
of what's being stored in your brain at
various types of sequence memory it's
not so simple but the various types and
we spent years trying to figure out how
this works and we think we got it let me
just give you some the neuroscience zoom
in on any section in your cortex doesn't
matter where you'll see there's these
layers of cells typically five layers of
cells I'm arguing that they all are a
type of sequence memory they have
similar attributes it doesn't really
matter what layer look at what I'm
talking about here would apply to any
layer cells they're all different types
of sequence memory so if we zoom in on
one of those layers what you'll see is
you'll see the cells packing they're
really tight there's about 10,000 per
cubic millimeter but they have two
organizations which are worthwhile
noting one shown by the green arrow
which is that it cells that are in a
very skinny vertical column have a
similar response properties especially a
feed-forward response properties they
all seem to respond to the same thing in
the world at feed-forward basis however
90 percent of the connections are
horizontal connections across the
columns in different areas of the brain
so 90 percent of connections are
elsewhere we have this very strong
vertical orientation if we zoom in
further and look at one of those cells
we see that the cells are dominated by
this dendritic Arbor which is this tree
shape structure around it all the
connections the positive connections to
the cell are on the dendrites they're
not on the cell body and so on a typical
neuron there's anywhere from several
thousands a few tens of thousands of
connections on those dendrites if we
zoom in further and now you're looking
at one little section of a dendrite in
this picture here you can actually see
the synapses in this electro micrograph
there's those little spines coming off
to about 1 micron apart rings along the
the dendrite there we now know and we
didn't know this 15 years ago but we now
know that there's a very nonlinear
effect that's happening here if a number
of these same
has become active at the same time
relatively the same time within a few
milliseconds in a short distance from
each other with about 40 microns of each
other nothing happened at that time then
you get a very nonlinear event you
generated what's called the dendritic
spike and it goes to the cell body and
it depolarize the cell body the somebody
goes in a hyper active mode it's ready
to get ready to fire it's anticipating
it's predicting so the every little
section the dendritic tree is like it's
like a coincidence detector it's it's a
thresholded coincidence detector it says
if I see a bunch of it but at the same
time
bingo if I see the same number of inputs
spread out over time or spread over the
generator garber nothing happens this is
such a important feature that hardly
anybody who's modeling this today but
this is the key to understanding how the
whole thing works we didn't know about
it too many years ago we model all this
on the here's a picture of one of our
simulations and the left is a layer of
cells with four cells four columns show
more detail pictures of this our neurons
our artificial neurons capture a fair
amount of depth of what is going on in
in the brain in real neurons these
colored dots represent the synapses and
I'm only showing some of them in this
picture the green ones are ones that are
close to the cell body I'm not going to
talk to them further those are how we
form the sparse distributed
representations the blue ones are on the
distal dendrites these in the 90% of the
connections this is how we're going to
learn sequences so and we're gonna model
these as I said each cell is a set of
coincidence detectors and when the
pattern comes in if it detects it it's
going to make the cell into predictive
state all right how does this all work
how does this learn sequences let's
start with a picture here's a picture of
our sparse distributing representation
but now showing as a sheet of cells and
the red cells are the ones that are
active and the white ones are the ones
that are inactive this is just a part of
our 2,000 bits and and so at any point
in time all right
I'm not just a reminder I'm not going to
tell you how we form this but it's
through a local inhibitory react
inhibitory competition at any point in
time I have some pattern on here and and
and another point time I have a
different pattern and and so as I'm
talking and as you move around the world
this is what's going on your brain
everywhere you look you see these smart
cells sparsely activated and they're
trips I went too far over there and
you've got these patterns are changing
over in time like this and we want to
learn the sequence
we want to learn how do I learn the
sequence of these distributed patterns
and the answer is the brain does it a
cell at a time each cell learn to
predict its own activity so when a cell
becomes active what it does is it says
let me look around for guys who are
previously active just a moment ago and
me see if I can find a bunch of them and
I'm gonna form connections to them and
so and I'm gonna form those connections
as you see on the bottom right here on
one of my dendritic segments so if I see
that pattern or again I will predict my
own activity and this is the beginning
of sequence memory if we did this and
let's say I showed a pad here's the
situation where I've showed a pattern
the red the red cells of the cells that
are getting a feed-forward input they're
active the yellow ones are hyper pole or
a depolarization
they're more yellows and Red's what have
I trained the system an a followed by B
and then a followed by C and a followed
by D and I show it a I'm going to
predict B C and D this is a union of
predictions this goes back to our Union
property earlier I have a union of
predictions and I can tell if what
happens next was one of the things I
predicted or not even though I'm
predicting multiple things at the same
time which is really what we're always
doing so now this memory I've just
showed you this is a transition memory
of a it's a first order transition
memory meaning I can only make a
prediction based on my current or
they've written the previous time step
well you know what's happening now I can
make a prediction return I can't use a
history of time but we need a high order
memory the reason we need a higher our
memory because that's the way the world
is structured the higher order memory
says I may need to go back a long way to
make the correct prediction so imagine
I'm listening to a melody I can't
predict the next note by just listing
the previous know I may need to hear
five notes or six note to ten notes the
same with speech they attend to know
what I'm going to you know predict what
I'm going to say or understand my speech
you have to understand a long context
and it's same for you walking down the
hallway it's just a high order temporal
pattern you know it's like oh you have
door door doors and a third door on the
left type of thing this is the way the
world is so we need to make a high order
memory this is a first order memory so
the way we're going to do that is we're
going to use columns of cells and let me
explain how this how we think this works
so imagine I gave you a sparse
disturbing representation but instead of
each bit being a cell I'm going to make
each bit be a column of cells so in this
case I show ten cells
/ / bit and I'm going to randomly choose
one of the bits to be one of the cells
to be active in that column so if it's a
one cup of one bit I pick a column I
pick one of those cells I have a much
sparser representation now instead of
two thousand cells I have twenty
thousand cells now I could pick the same
sparse distributive representation a
moment later and use a different set of
cells I just randomly pick a different
set of cells in the columns and so I
have a different representation it's the
same sparse discrimination the same
columns but different cells if you just
think about this we have 40 active
columns there's 10 cells per column so
there's 10 to the 40th different ways
that represent the same input in
different contexts so here's an example
I'm gonna I'm gonna say a sentence it
has a sound repeated four times there
are too many tutus to count now the
sound 2 was used four times in that
sentence you didn't get confused if it
was a force first order member you would
what get confused but it's not you
didn't and you heard them even was the
same sound
they had different meanings at different
points so one point in the brain you had
to have the same representation because
the same sounds coming in on your
cochlea at another point in the brain
you had to have a different
representation because you didn't see
that hear them as the same and that's
what's going on here we have these
column of representations and allows us
them create very long high high order
sequence memories I'm not going to walk
you through all the details of how this
works you'll just have to take my word
you can read it on our website if you
want but in the end if you do this and
use the same side of learning rules I
was just talking about a moment ago you
form a sequence memory of sparse
distributer presentations it's a it's
variable order can be as high order
statistics allow so it's not like fixed
order its distributive it can do
multiple simultaneous predictions about
what's going to happen next it's very
high capacity this little memory here
can learn millions of transitions and it
allows that semantic generalization
imagine if I train this on a series of
patterns and now I give it a new series
of patterns that are not exactly the
same representations but they have some
bits overlapping with them so they're
semantically similar I can apply my
sequence memory to what was a previous
learning to a new input that is
semantically similar and that's what the
brain does okay my last thing I'm going
to talk about is how we do online
learning
this is the third of my six elements
I've been talking about here and what's
this all about
well basically means because we're doing
island learning you have to train on
every moment in time every new input to
the system and essentially you don't
know if it's noise or something valuable
so you have to train on it and basically
if it doesn't repeat you forget it and
if it repeats you want to remember it so
that's pretty much what it is now here's
a little bit of neuroscience you
probably don't know unless maybe do but
probably don't we used to think they'll
pay attention just listen me for a
second
we used to think that all memory in
formation in the brain was the
strengthening and weakening of synapses
and clearly that happens to some extent
but we've now learned that something
much more important happens in memory
which is we can form new synapses very
rapidly and we can forget them very
rapidly so instead of just strengthening
a connection we can form new ones and
that's a much bigger pool of potential
things you can connect to we can do this
in the order of just a few tens of a
seconds that you can form a completely
new connection so when even one exposure
is enough to do this often to form a new
neuron so these guys on the dendritic
tree those spines
if you look on real neurons some of them
are very there for very long periods of
time some come and go every day as you
were learning things so we want to it's
there's much higher information capacity
in forming new synapses than
strengthening old ones in fact in the
real brains synapses are highly
stochastic they're very unreliable so if
anyone shows you a neural model that has
you know precision of two digits or one
digit of precision that is required in a
synaptic weight forget it doesn't work
like that so the way we do this is we
model this growth we say that we have
something called the permanence which is
a scalar goes from using 0 &amp;amp; 1 which
essentially modeling the growth of a
synapse so I can start growing a synapse
and not make a connection if it gets
above a certain threshold if the
permanence gets above a threshold we
then say that synapse is connected and
we just give it away to 1 we have binary
weights we don't try to be more fancier
than that but we have this idea of a
permanence and once I'm over threshold
if I keep printing
I keep reinforcing this my permanence
can can go up this the connection
doesn't get stronger but the permits
goes up it makes it harder to forget so
you want to do that because if things
rapido many times or you want to make it
two it's very hard for you so that's how
we do that if you put this all together
and you say hey I want to simulate one
of these things which is what we do all
the day
long you have 2000 coms 30 we do 30
cells per column 128 segments per cell
40 segments 40 connections per segment
these are all very realistic numbers in
neuroscience you put this all together
you're basically about 300 million in
this little model about 300 million
synapses each one has a connection index
and connection permanence the
connections are very very sparse there's
a lot of tricks we can do to make this
run fast and noticeably there's no
single points of failure here you can
dropout synapses tripower dendrites
dropout cells dropout columns the system
keeps behaving very nicely this is a lot
of Appeal for a hardware guys who if
you're talking about building this stuff
in in silicon ok I'm now going to switch
gears I'm going to talk about I think
it's very important to build this stuff
and make it work and prove it and make
commercial value out of it so we're
going to do that we have been doing that
I'm gonna give you a we're applying this
in the space of data I'm gonna give you
a lot of my take on data this is the
data company I'm nervous as hell talking
anybody about data here because you guys
are the data you own the data you be the
data so but today this is my view of the
world by simplistic view of the world
today we're King to number as a source
of the data it's growing exponentially
we stick them in databases the vast
majority of the data in the world has
never looked at ever it just sits there
we have two ways of getting value out of
it one is through visualization tools
and the other is creating models and
then we free to use those models we can
act on them there are challenges here
one part one of the biggest challenges
that this whole system is not very
automated and it takes data scientists
people like you to do this stuff and we
want to get to a world where there's not
just hundreds or thousands or millions
of miles we want to get them on with
this billions of models the Internet of
Things you know everything in the world
is gonna be creating data and we need to
be able to model all this stuff the
other problem is inside today it takes
lots of people it's not automated the
old problem is the models can get
obsolete if you're not doing online
learning and most techniques today's or
not online learning you have to rebuild
your models all the time because the
patterns in the world change and and
people just aren't really they're not
looking at temporal data very much much
of the patterns in high velocity data
especially high velocity data is
temporal patterns and that's almost very
rarely do people take advantage of that
they actually try to get rid of it so I
my view of the world tomorrow it's not
think the current world is going to go
away but this is where
I think the growth is going to be is
we're gonna go over over this literally
I'm not joking billions of machine
learning models out there the data is
going to stream right into the models
there's no storage required you're not
going to save the stuff the models are
going to build and continually update
themselves and you're gonna immediately
take it to actions and so this if you
look at it looks just like what brains
do go over back to what I said earlier I
said whoa look at that
so let's try to apply our techniques to
this and so that's what we've been doing
we've the criteria here is you need to
have automated model creation for
billions of models you need to have
continuous learning and you need to be
able to find the tempo as well as the
spatial patterns in the data so we built
a product called grok it's a it's an
engine for acting on data streams it's
essentially a productized version of the
thing I was just telling you about on
the left you can see we have we have
streams of records of data coming in
through time give me one or more fields
we run those fields through encoders
which are just like your sensory organs
literally they're modeled after sensory
organs are modeled after cochlea and we
can turn them into sparse distributed
representations those encoders can be
fairly generic we do not have to do new
ones to do every different problem with
solve if we use a set of generic
encoders we then get sparseness
different representations we and albeit
we can put in field any kind of numbers
categories text dates times you can do
custom things as well if you want to
some of summary structure data we run
this through the sequence memory I was
just telling you about and it basically
looks for that's the spatial and the
temporal statistics of that data it can
make predictions you can detect
anomalies and then from that we take
actions what the user has to do in this
case they have to define the problem
this is actually tricky they have to do
a good job at this then they have to
stream the data
grok creates the models it learns it
learns the spatial temporal patterns in
the data and outputs predictions with
percentages you know it's a probability
distribution and it can detect anomalies
and we're finding lots of applications
for this we're finding in energy and
I'll tell you some more product
forecasting anomaly detection server
those I'm just going to walk a few a few
simple examples and then I must
speculate a little bit for you today
this is all running on an Amazon Cloud
doesn't have to be but that's this the
way we implement it to begin with is
there simple REST API to use it and some
web apps to help you get started
although it's only in a private mode
still we're still doing this with
customers handing handheld holding them
we see a lot of
any space I'm gonna show you this one
because it's very simple to see and it's
its it that's the main reason I'm gonna
start with it's very simple you may not
be realized that this is a thing called
demand response which is large consumers
of electricity actually bid on price of
powers throughout the day and utilities
will say if you can use X amount of
power at 3 o'clock I'll give you this
price for use Y amount of power I'll
give you this price and they're trying
to they're trying to figure out how to
do all this and if you could predict
both your demand or the supply you can
save energy and you could save money so
there's a lot to be saved here here's a
Factory in France and in Paris actually
and this is a very simple one it's just
showing the electrical usage throughout
a week you can see the five days they're
not working on the weekend apparently
because it's kind of low there at the
last few days and the problem with the
customer to do is they said you said at
midnight they have to sort of make their
bids and at midnight they have to they
want to make predictions every hour for
the next 24 hours we had to put a little
wrapper around the this learning outcome
to get this to work but we can do that
so what we do is you get something like
this now here's here's the actual and
predicted now this looks great
pay no attention you really can't tell
if it's good or not because you have to
really look at statistics and look at
the data carefully but externs out this
was very good the customers happy with
this the red is predicted the blue is
actual we can follow it pretty well
here's a situation where the system was
just trained on a few a few months of
data and here we are on Wednesday and
Thursday morning it starts picking up
and it says her thirds pick up and
didn't happen and the reason because it
was a holiday and we didn't know about
this holiday and system was never
trained on holiday so it says up Nana
says I that wasn't right and Starr says
this looks like a weekend so it start
acting like a weekend here's another
example same idea a little bit more
complex the dissension it's not so
obvious all the time this is a company
that does does video encoding they have
a service level agreement with the
customers they have to guarantee the
turnaround so they have to leave extra
servers running on the cloud all the
time in case the peak and demand so
they're always trying to manage how many
servers do I leave running which are
wasting energy and electricity and power
and money but I have to meet my service
level agreement if they could predict
customer demand better then they could
leave fewer these extra servers running
around so here you can see the data it's
quite spiky there's no obvious patterns
in it
we can't predict all the spikes it's
impossible but the question is can we
discern some patter than this day to do
better than any other technique they've
had and can we do this in an automated
way and the answer is yes to both those
we can do better than they can do it and
we can do it in an automated way so it's
basic we just feed the data and rock is
saying you know you know I won't go
through all the details of it but it was
a successful application I want to give
you a little sense of what it's like if
you looked inside in this system so just
pay attention to the right side of these
images here we're looking down on the
2,000 extra 2048 but 2,000 columns and
and the green dot means that this was
predicted and it actually occurred so
we're looking for so probing inside of
this cortical learning algorithm right
now and this is a case where we you know
what occurred was exactly what was
predicted this occurs quite a lot it's
not not unusual here's a situation where
we predicted multiple things but one of
the ones we predicted was occurred so we
have more of these little blue circles
of things that were predict him but
didn't occur but that's not a mistake as
long as the things that did occur were
predicted those are the green dots so I
have 40 green dots and maybe 80 blue
dots something like that here's a
situation where things didn't work too
well I'm sorry you probably can't see
this too well in the back of the room
there's a bunch of red circles in here
as well and those red circles are things
that word predicted but did occur those
are true anomalies I didn't expect this
to happen and it did happen but this is
shows you so typically what happens here
we it's not an all-or-nothing affair and
an anomaly it's not one thing it's like
well some of the things I predicted did
occur
some of the things I predicted didn't
occur and some of the things the curd
didn't weren't predicted so I could even
if I wanted to I could go in here and
say well semantically what was what was
wrong and what was right we don't do
that per se but that's how the system
works on its own so there's a lot of
subtlety here here's a case where we use
it with the this is an offshore windmill
and this is looking at the the oil
temperature in the gearcase and a large
offshore windmill in the North Sea the
blue line is the temperature and it's
going up and down throughout the day as
the windmill speeding up and slowing
down and the question is could we detect
an almost behavior and the red line at
the bottom is is the anomaly score which
which croc is putting out and here you
can see an interesting event we had two
peaks here the earlier stop is when the
system was first started being trained
here we have two peaks and the important
thing is the first peak is when the
system started acting a little bit
unusual it wasn't out of range it wasn't
out of it wasn't out of spec in terms of
like it was too high or too low the
temperature but it was sort of
oscillating in a way that hadn't seen
before
so grog says that's unusual I haven't
been able I'm not able to predict that
as well and so you have a peak and then
the second peak is when the system
actually went down for failure and they
worked on it we think there's a larger
applicant there's a ton of applications
you're detecting anomalies okay here's
my dangerous slide my dangerous I said
hey if I was Google how would I use this
I'd love to have you guys as a customer
that's not now why I'm here but I love
maybe as a customer I said well we've
had a lot of interns in the advertising
space online advertising and we've shown
that we can for people who basically
have you know real estate to sell they
they're always trying to pick what ad
network to pick and and so we've shown
that we can predict the expected you
know return on a particular ad network
which changes throughout the day all the
time you want to do this almost like a
15 minute basis we can do that on a per
network per app per demographic basis
and you essentially the user can decide
how about prioritize what where do I
serve my ads from we don't really care
what the patterns mean it's just throw
patterns there and we and they change
and we find them we can't get all right
all the time again we're just trying to
do better than they're doing today we
have a lot of interest in the finance
world this is not you can't predict
stock prices forget it but it's not
going to happen but you can we've shown
that we can successfully predict volume
and volatility better than the industry
standards today and this is valuable for
various reasons we also believe we
haven't done this yet but we have a lot
of interest in detecting anomalous
trading both is internal to companies
because they're trying to pick rogue
traders you know we also think we can do
it in across a huge number of obtuse
weird trading combinations so when
something becomes all of a sudden more
predictable it represents a trading
opportunity which for some reason not
many people in trading is saying bingo
you could do that not my favorite
application but I think there's give me
a lot of applications here I mentioned
this because Google because you guys I
have to admit you're you're not first
and everything Yahoo's still ahead of
you in finance I think and you could add
some really cool things here your
financial stuff if you had these kind of
capabilities you guys do a lot of
computers you have the biggest server
farms in the world we found some really
interesting applications and
managing computer resources like I
mentioned with the earlier example
predicting demand resource balancing
I've remembered a crazy idea a great
idea where someone says you know a
different service of different
efficiencies running different
applications and if you could predict
the efficiency you could switch down
some servers and bring up others and
save energy I think it's really cool
a lot of ups a lot of a lot of work in
the energy space for us we're getting
hit on this all the time you guys are
involved in smart grid solar wind demand
response this could be something great
for you guys and finally I love your
cars and you know maybe something in
error you predict where parking spots
are gonna be your routing people things
like that okay if you want more details
on grok you want more details on these
algorithms there's a white paper on our
website and you can read about the stuff
on the website I'm now going to switch
to my last part of this presentation
which I'm talking about the spec a
little bit about the future of machine
intelligence because I'm really
passionate about this stuff as you
probably can tell so what's the future
is it like this Skynet and the matrix
the Terminator I know I'm a big
science-fiction fan but I tell these I'm
told these are bad things or is the
future something nice like this like you
know the robot Butler's like c-3po or
we're gonna play games of watching or
maybe what form of new ways of
entertaining ourselves you know is that
the future or is the future ambiguous
you know maybe it's good but it turns
bad so I don't know but I'm gonna take
some things I have some prognostications
here I'll just tell you where I think
this is going here's some things I think
it's definitely gonna happen and I'm not
talking 100 years from now then this is
gonna happen and we in reason I'm here
cuz I'm trying to make it happen sooner
we can make machine intelligence it's
faster and bigger than than biological
intelligence so we can definitely make
machine intelligence it's a million
times faster than biological brains
neurons can't do anything less than 5
milliseconds that's their maximum
throughput we can do a lot better than
that now I can't just speed up a brain
if I don't speed up its sensory organs
and I don't speed up its data streams
and so on but I don't see any reason why
we can't do that in virtual worlds we
should be able to make machines that are
a million times faster and think million
times faster than humans think we can
make them bigger that's not the only
goal but there's no reason at all we
can't make bigger in New York or Texas
and you know
you can't make it smarter just by making
it bigger that's that's a that's a
mistake you can't just say make it
bigger and to be smarter you still have
to learn you still have to be exposed to
things it takes 20 years to train a
human you know we have to come up with
these our training systems and they have
to be exposed to environments but
there's no question about in mind that
we can make deeper thinking machines and
humans we this is their I get very
excited about we can do super senses we
should not be thinking about the senses
of machine intelligent machines is
hearing and vision and touch why not you
know I was just showing we have sensors
are looking at oil temperatures we can
have sensors that look at anything we
could have distributed sensors we could
have microscopic sensors that work
inside of cells all kinds of things were
humans we have an impedance mismatch
because of our our own senses we'd spend
a tremendous amount of time trying to
come up with ways of looking and
thinking experiencing stuff that we
can't normally experience but we could
build artificial brains and experience
it naturally we can do fluid we bought
robotics we're nowhere close to this
today but I think we can get there and
finally this is another idea the
neocortex is the hierarchy and in the
brain they're all co-located because we
have to run these wires between them and
that the neurons between them but in a
machine intelligent well we don't have
to do that we can have parts of the
hierarchy all over the place we can have
a distributed hierarchy we can have
higher regions on top or hierarchies I
don't even know where that's gonna go
yet but the idea that it doesn't have to
be co-located and as long as we get the
communications right that would be very
interesting stuff this is all gonna
happen here's some things that might
happen I don't know maybe they will
maybe they want humanoid robots maybe
maybe not well we have something like
c-3po I think it's probably technical
possible it's gonna be very very
difficult because if you wanted to be
human life they have to have all kinds
of other stuff that makes them human
like they have to have the rest of brain
and they have to have all these
emotional things and so on so and I'm
not sure if that's really where the
business is gonna be you know I know a
lot of people want to do this but to me
it's like this is sort of a sideshow and
I mean new technologies come along we
always sort of imagined these things
when the steam engine came along they
imagined steam engine robots right
that's what the term robot came from but
it didn't happen so I don't know it may
happen well we have computer brain
interfaces for all like you know the
matrix you plug it in the back and go
whoa you know I don't know maybe maybe
not there's a lot of technical problems
there I'm not sure
really want to have that who knows
here's some things I don't think are
gonna happen um I don't think you're
gonna upload your brain to anything
sorry to say and there's there's two
reasons for this really one is I mean
forget about the incredible difficult
technical problems just forget about
that but but the memory in your brain is
instantly tied to the wetware of your
brain and the wetware of your body and
you would have to recreate the entire
thing in some sense to get those
connections to be meaningful in another
form I also think would be quite
unsatisfactory imagine if I want to pee
right today so you know you cannot build
your brain of this computer you want to
do it you say yeah sure they say yeah I
want to live forever you know and then I
said okay we did it
then the computer comes oh hey that's
great I'm awake and we're done with you
we can get rid of you and he says oh
wait I'm still here I mean it's like
you're not gonna feel so good it's and
in the end and then those two things
will diverge and it's just like how you
might as well just have kids it's the
same thing finally I don't think we're
gonna have evil robots you know these
things aren't gonna turn one day become
sentient to say I don't want to be
controlled anymore you know you are dead
it's not gonna happen you know these are
not replicating things these are not
emotional things these are not humans
they don't want to have sex they're not
hungry we're just trying to use the
principles by which the brain works to
build really really useful things for
society and finally we can be certain
that it's not going to be only for
friendly uses as well people will do bad
things with this but that's true of
every technology alright my last slide
why do this why do we care why am I so
passionate about this why do I try to
get other people passionate about it
well there's two reasons first is it's
to live better there's no question in my
mind just like computers have improved
our lives tremendously and the products
that you guys built have improved our
lives tremendously and my life has
benefited from that I think having
mathilde machines is a way of improving
our lives we can make the world safer we
can make it more energy-efficient we can
make the world a better a better health
all the things we want to do it's
there's no question all that this is a
ability to move that needle
significantly but there's another reason
too which is is is to
learn more if I sit back and say what's
the purpose in life why do I you know
why should anyone care that I live here
and you live here and so on yeah in the
end long after many drinks and so on I
come to the conclusion that the that the
goal in life is to acquire knowledge and
to make sure that knowledge is preserved
and this is what we do as scientists is
what we do is we're inquisitive species
we want to understand how the world
works you want understand the universe
we can understand where did it begin and
when did it end I want to know those
answers - and we could use tools to help
us do this imagine we could have
physicists that are a million times
smarter and faster than us and never get
tired and they think about this stuff
what if we want to explore the universe
are we going to set what you know we're
finding human earth-like planets only
thirteen light years away
wasn't that great that was in the news
this morning I think that's wonderful
how long would it take to get a human
there and will they survive probably not
if we want to explore the universe I
think we have to do it with machines
that don't breathe oxygen and are not
sensitive to things we are sensitive to
him so to me in the end here it's all
about accelerating knowledge accretion
and I think there's a way of amazingly
selarÃ³n accelerating them so that's the
end of my talk thank you
so with that we have a few minutes for
questions if you have questions please
come to the mic great presentation on
your model I agree I like it and I agree
with the thrust of it I wanted to focus
on one aspect of it which is the use of
scalars you mentioned scalars in the
context of the completion of a
connection with the view towards its
permanence but the properties are
basically represented by binary values
either there is a fricative or the loop
is closed or it isn't yes so in building
systems that have at least many of the
attributes of your model in trying both
binary properties and then probabilistic
properties as an 82% chance as a
fricative and then using Bayesian
reasoning to combine them appropriately
I've gotten better results with the
probably yes
property so I was wondering yeah I'll
rephrase the question just make sure
everyone understands it's a great
question and I'll expand it because
we've done two things one is our neuron
activations are binary as well and our
synapses are binary the synaptic weights
of binary and the question is can you
get better results or you get better
results using probabilistic or scalar
values for those and we actually know
for certain that the in the brain that
neurons actually have scalar outputs
they have firing rates and we know that
synapses are also scalars now I
mentioned earlier also that the the the
synapses are very stochastic so that a
large percent of the time they don't
work at all so that's an argument for
not relying on scale of properties but
the thrust of the answer here is we've
taken a shortcut we've said that because
we have distributed representations you
do not need to rely on the accuracy of
any scalar value anywhere in the system
and it's much quicker and simpler to
implement this as binary activations and
binary synaptic weights I'm not saying
it's realistic but given the principles
I understand what's going on I
can back off to it and makes my system
run much much more reliably we spent a
great deal of time trying to make Rock
run fast we can do an inference learning
inference cycle in 10 milliseconds and
we need to do this because you know if
you're gonna build a practical system
you have to make these things perform so
that was an engineering choice we made
it's not a biological choice and I'm not
disagreeing with you probably would get
better results if I did it with a scaler
just a quick question on your view of
Markham's project because I've had this
debate with him over the past summer and
he expects simulating at the molecular
level which of course is not the right
way to build AI but may be a good way to
verify our models of biology but the by
2020 he'll be able to simulate it at a
hundred times real time and you'll be
able to actually have a conversation
with it and said how you can have a
conversation with it because if you're
absolutely perfect in your model it's
not going to do anything just like human
brain doesn't do anything unless it's
gone through years of learning and if
you're at a hundred times real-time how
are you going to have it learn about the
world and have a conversation so yeah
there's a lot of fundamental issues with
that project I would agree but you know
I look I'm excited anybody wants to do
any of this stuff and you know maybe
we'll learn something from it and so
look at the positive side
I think they've they now taking that
project and view it more as a way of
simulating like drug interactions and
all the kinds of things like that so yes
so I want to extend Ray's question a
little bit which is I appreciate that
you've taken the last 15 years of
advanced neural architectural
understandings and both adding to your
model fantastic in the past 15 years has
also been an advance in slow things like
slow potentiation hormonal neural
response and so on and people like
Antonio Damasio and Joe lado are saying
that that's actually fundamental to
cognition and yet it doesn't appear to
be in your model anyplace well so can i
sum up it's sort of the emotional
affective content in the brain is that
is that it but there's also underlying
neural and hormonal mechanisms yeah sure
for a lot of that yeah so so so just I
want to make sure I address the question
but so there's a lot of stuff in the
brain a lot you've picked one that I
didn't talk about
there's synchrony there's all kinds of
rhythms and so on right and so this
particular one is that there's there's
this the hormonal aspects there are
multiple neuromodulators that are
distributed throughout the neocortex
that are fundamental for learning but
again when you look at it and you can
say well is it important for that for
machine intelligence to have those why
you know we have ones that are based on
fear and based on reward and so on we
have an emotional system in our system
in our models it's a switch it says
learn or don't learn and it's a very
crude emotional system but if it's good
enough for what we need to do if I were
to build a system that's interacting in
social networks with people and having
conversations and trying to get food and
trying to have sex and trying to stay
warm there's a whole bunch of other
things that might you want to you might
want to have other effective things and
just be clear in the brain the parts of
the brain that actually evaluate
emotional content or emotional salience
here not in the neocortex or small areas
that there's subcortical they project
their neurotransmitter throughout the
neocortex so they have a global effect
like learn this don't forget this you
just nearly died you know this was a bad
piece of chicken don't eat again then
but you know from what we're trying to
do which is you know figure patterns and
data and structured data and so on it's
not necessary at this point in time but
I don't think it's it's not a
fundamental aspect of it's a fundamental
ask of being a human but it's not a
fundamental aspect of intelligence all
right that we've probably put that in
all of your examples of practical
applications you seem to be predicting
one dimensional data did you use only
one dimension of input as well no so I
mean you say one dimension I assume you
mean like multiple factors I mean that
it's it's a scalar value think well the
scalars it could be we handled one or
more fields of scalars enumerated types
dates and times and so did I answer that
question yet
I didn't accept what we do is we end up
forming a sickness your model perform
significantly worse if it doesn't have
the addition well we find out so I
didn't tell you how this works well the
way they actually user would use grok if
they provide multiple data streams they
tell us what they want to predict we do
an evolutionary search through model
space to figure out which factors help
make better predictions in which don't
so and then how to encode those factors
so in the end you end up with you know
and sometimes we actually run these as
ensembles of models we have multiple
models running at the same time and and
they're competing with each other so so
the the answer the question is grok
tries to figure out what if all the data
you give it which ones are the best and
most predictive or which ones in the
most which factors help how to use them
sometimes some factors help sometimes it
doesn't for example in that particular
case with the windmill if you if you
included wind speed as well as oil
temperature it's usually helpful but it
does pretty well without it so it
depends it really depends all right now
what you've described as a system for
predicting data that does not seem to me
to be a complete solution to shall we
say general intelligence at least or an
autonomous system do you what do you see
as being necessary to complete the zone
all right I hope everyone heard that
question so there's a lot I only showed
three of my six elements right we did
not have a motor component we don't have
attention we don't have a hierarchy we
built something it's very small we build
something that's one millionth of size
for human neocortex one thousandth of
size of a mouse near cortex okay it is
really small it's sixty thousand neurons
it's teeny I'm not calling that
ascension you know B this is like a
teeny little piece of tortoise that's
learning patterns but the key elements I
my oh you mean is that if you if you get
all six of those elements all six of
those things including so the hierarchy
a so it's basically a scale and
necessary for scale the sensory motor is
a huge component of this because you
have to explore the world and I'm
working on that right now we're making
good progress on that and and then the
attentional opponent so you have to add
all this
stuff together before you can start
claiming you've got something that's
close to machine television okay great
it's it's 12 o'clock right now so uh
thank you all for coming and thank you
Jeff it's been a really really great</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>