<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Reiser4 Filesystem | Coder Coacher - Coaching Coders</title><meta content="The Reiser4 Filesystem - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Reiser4 Filesystem</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mIrMVPnxa04" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everybody so it's always
interesting to find people to spend a
boat part of their life doing technical
stuff so um it's my pleasure to
introduce Hans Reiser
it's the creator of a ricer file system
and this talk is going to be covering
both the ideas the background the
algorithms and and the life arise
therefore it's correct so I mean tell
you a little bit about our corporation
before I get going we have a group of
programmers most of whom work in Moscow
one in st. Petersburg one in San
Francisco I'm in Oakland where back in
1993 I went to Russia and hired a team
of programmers and that allowed me to
avoid having to engage in venture
capital and so in this talk I'm going to
discuss how unifying namespaces improves
closure within the OS I'm not really
going to discuss the semantics that I
want to use because that would take me
way past my time limit I am however
going to discuss performance and the
ways in which we accomplish a
high-performance storage layer and these
various other items that are on the
slide I would like it if you would ask
questions during the course of the talk
because the difference between my
website and my talk is that I answer the
questions
and so the natural question is well so
so what is it that the riser for is
trying to accomplish
well hierarchy doesn't scale well for
human beings and hierarchical namespaces
scale extremely poorly relational
databases also don't scale well there's
a reason why relational databases are
not used for searching the web and the
reason they don't scale well is because
they add structure to information
structure that is not inherent in the
information but at the same time that
they add structure the problem with
boolean algebra and search engines that
resemble boolean algebra and what they
do is that they don't allow you to
represent the structure that's inherent
in the information and so what you would
ideally like in a general-purpose file
system namespace is something that
matches rather than moles the structure
that's inherent in the information
and if you can't match rather than mold
the structure that's inherent to the
information then you're better off using
something that resembles boolean algebra
and I think you folks all know why that
works better because he well let me just
state it formally if you have a choice
between ignoring known structure so
prevent if you have a choice between not
allowing the user to use structure that
they know about and then just present
and the information versus requiring
that they use structure that's in and
thereby requiring that they learn the
structure that you require in order for
them to be able to use your namespace
you're better off ignoring the structure
than requiring it but ideally what you
would like is to match the structure and
inherent in the information when you
insert it into your namespace and you
would also like to allow that you also
like to allow the user to specify what
fraction of that what random fraction of
that information he knows and use it in
his queries and structure you can think
of structures just one more random piece
of information that the user happens to
know when they're looking for something
the file system is the most central
namespace of the OS now this was
probably more true 20 years ago but it's
still substantially true even with the
advent of search engines and the web and
I would like to suggest you the
namespaces in general are like roads and
waterways and in 1776 I think it was
Adam Smith wrote a book called The
Wealth of Nations and in this book he
suggested that civilization tends to
develop around roads and waterways and
the reason for that is connectivity
where you have roads and waterways you
increase the amount of other people that
you can interact with if you're on that
road or right away and he found that
there was a very tight correlation
between where civilizations tended to
develop and where there were roads and
waterways and I would like to suggest
that in general and in great many large
systems the power of the large system or
the utility of the large system is
proportional not to the number of
components in the large system but to
the connectivity of those components and
in particular I'd like to suggest that
for an operating system its expressive
power is determined not by the number of
objects that you pay for the development
of within the operating system but
instead excuse me it's proportional to
the number of connections between the
components and these components are in
general connected by the namespaces of
the operating system
it's the namespaces that conduct
the interactions between these objects
and that serve as unifying bus along
which all these different components can
communicate with each other and if you
have a unified namespace
instead of a fragmented namespace then
you have more connections with the
unified namespace than when its
fragmented namespace a trivial example
of that is well if you have a choice
between two namespaces each holding half
of the components versus one namespace
holding all the components the one
namespace is going to have more total
connections because it will be
proportional to N squared instead of n
over two squared and
you have a great variety of different
namespaces within the operating system
you have the web you have file system
you have databases you have
environmental variables you have a whole
bunch of different namespaces within the
operating system and to unify the
namespaces within the operating system
is a bit of a quest for a holy grail and
by that I mean that King Arthur sent his
Knights out to find the Holy Grail and
if only they could become good enough
the Holy Grail would appeal would appear
to them none of them ever became good
enough that the Holy Grail appeared to
them and yet the quest for the Holy
Grail
made them all that they became and so
even though we will never succeed in
unifying all the namespaces within the
operating system the closer we can if we
can just get ourselves closer to it
we're in a better place than if we don't
get any closer to it yes different
namespaces are in trying to use
well so for instance you typically have
say an Oracle database which has a whole
bunch of pieces of information within it
and components within it and these
components aren't part of the same
namespace as the file system namespace
and so you have a whole bunch of
utilities that are designed to use the
file system namespace but aren't able to
casually interact with the the Oracle
name namespace or the or any of the
various other names another example is
if you look in slash Etsy you have a
great variety of different file formats
for all kinds of different things that
are recorded in various flat files
within slash Etsy and each of these
different files has its own syntax its
own way of being parsed its own way of
recognizing that this is an object and
those are all fragmented namespaces and
it's convenient when you can just
when you can just say oh another example
of a fragmented namespace is the
metadata for a file so the owner of a
file you can get using the stat command
the stat system called and this is a
different name space from the namespace
for accessing file bodies and if those
name spaces were unified then you could
do something like a cat file name a
greater than file name
I'm sorry cat file name a slash dot dot
dot slash owner space greater than file
name B slash dot dot slash owner and
thereby set the owner of B to be equal
to the owner of a and you can do a whole
bunch of little tricks a whole bunch of
little convenient things if you have
unified namespaces that allow you to
casually connect the output of one
object or the data that is one object to
the to another object within the
operating system okay
yes opportunities biscuits in the
network we do in quiet
our sister will expect
okay so the question it was I'm going to
repeat it because I think you were
inaudible the question was should you
should this namespace spam machines span
span networks and the answer is yes but
it's a holy grail and I'm not going to
get there this year okay so if you would
like to read about the semantics for the
file system that are what we are headed
towards and read about a set of
semantics for a semi structured data
query that attempts to match rather than
wall structure that's on our website and
solely because of lack of time I'm just
not going to be able to ever that
so the first thing we decided to do when
we wanted to do this integrated
namespaces to deal with the problem of
small files and conventional file
systems for Kaline all files and what
that means is that if you want to store
a phone number sized object it's
inefficient and ineffective to store
that as a separate file within a
traditional file system and so we
started with this problem first because
it was the hardest problem it was
technically the problem solved and the
reason it was the hardest is that
addressing it required a complete
redesign from scratch of the file system
and in particular we had to design a
file system is based upon balance tree
algorithms and a balance tree to put it
very simplistically a balanced tree
takes a whole bunch of small little
items
squish system squishes them together
maybe I should do that as and squishes
them together and stores a whole bunch
of small objects and one pork a note
there was a problem with doing that and
the problem with doing that was that it
was generally accepted that balance
trees were performance and effective
traditional file system usage patterns
and that had been tried and it was
considered a failed experiment and and I
was told more than one person and the
problem and so what it turned out was
that there were problems with the
algorithms that were used in balance
trees in traditional databases that were
the reason why it was performance
ineffective to use them for traditional
file system usage patterns
and so we had to fix those problems in
order to make it work and that's why I
took ten years myself so let's talk
about the implementation so the general
idea in the file system is that you you
have objects and these objects exist in
them you have objects and these objects
we chop them up into little items and
the items we squish together into it
into our notes and so notes are 4k in
size and there's nothing magic about
that it just happens to be easy to code
that way in Linux and an item is a
portion of an object that is small
enough to fit into one of those notes
and for every item there is a key and
items are fully ordered in the tree by
their key so this means that we've added
an abstraction layer to optimizing disk
layout so if we want to completely
change how things are laid out on disk
and completely change what kinds of
items tend to be aggregated with what
other kinds of items what we do is we
just change our key assignment algorithm
and this means that if we want to try
something completely new about how to
stick something with something else we
can in a couple of hours of code
accomplish a change that in another
cloud system would possibly take six
months to come
and it might take more than six months
to code because in another file system
would probably require a change in disk
format and disk formats are the grand
Bugaboo file systems if you file systems
have great difficulty changing disk
formats once they get out there and it's
extremely painful and expensive for any
OS distributor to change the disk format
for a file system and that tends to mean
then and when you write a file system
you have to get your disc format right
the first time and nobody gets it right
the first time
and so everybody suffers and by adding
this abstraction layer and by using our
plugins which I'll get into later we we
completely avoid them and file system
performance is highly empirical you can
measure the performance you can measure
whether you got it right and the thing
is true of everything that's highly
empirical is that you're going to get it
wrong a lot of the time because nature
is just so much more complex than our
coding little brains and so how well
your file system performs is very much
determined
by how easy it is to make little changes
to it and the more little experiments
you make the higher your performance is
going to be and when doing things like
this key assignment abstraction layer we
make a lot of things that for other file
systems are a lot of work to code for us
into a small amount of work to code and
that gives us performance advantages and
later I'll talk about some performance
advantages that were required primarily
because for us it was a small amount of
code to experiment
well I guess I guess that's actually the
next slide in the riser for file system
compared to the rise of three file
system and riser for the keys for file
bodies have the same order as keys for
the first directory entry they are
created with and this turns out to have
a 2x performance e gives us a 2x
performance increase for operations that
occur and read your order and it turns
out that there's quite a lot of
applications that access a bunch of
files in reader order when they go
through a directory operating on the
road and it's also useful in that if you
get a performance advantage from going
through things in a read your order as
an application you can choose you can
frequently choose to go through things
and read your order and get a
performance advantage from it now the
disadvantage to what we did is that it
made our keys larger so in order to make
the keys being for the file bodies being
the same order as keys for the first
directory entry we had to put some
information some some extra information
into the key it changing the code to try
this new key assignment order with
something that the programmer has to do
it just didn't want to do he told me it
would take him six weeks and it actually
took him three days and I think the next
time we did it would probably take him
about three hours because in the course
of making the change she cleaned some
things up to make it easier and this got
us a got us a major performance
advantage by performing this experiment
and this experiment might or might not
have happened at all if it had taken us
six months to perform the experiment or
even three months perform the experiment
probably wouldn't have taken to six
months but it might have taken us three
months in version three to do it yes
well the earlier versions the battle
bodies are ordered first by the parent
directory ID and then by the object ID
of the file and the object ID tended to
get created in time order and some of
the time that was good and some of the
time that was bad
and unfortunately because of the length
of talk you know usually I give you a
complete discussion of how you resolve
the key of course finding something
within the tree and I think that's going
to be one of the things that I draw so
let's talk about B trees
in the traditional beetrice the data was
stored in the internal notes so I in B
plus trees the major innovation was that
you shoved all the data down farther
down into the leak level of the tree and
the effect of doing this is that you
increase fan-out and fan-out matters a
lot
fan-out determines two things one of
them is the determines the cache ability
of internal notes so if you have if
you're a journal notes are less than 10
percent of the the total amount of notes
it makes a big difference in your
ability to cache all the internal notes
another advantage that it gives you is
that you tend to have seeks cropping up
when you cross boundaries between
internal notes and in version 3 seeks
tended to get added when balancing
change to the parent was and to things
improve that in version 4 one of them
was allocation on flush which I'll get
into later and the other was increasing
a note
now with respect to caching I'd like to
say that caching is dependent on
temperature and variance now you can
think of I should say by temperature
what I mean is the tendency to access a
certain object so some objects tend to
get access more frequently than other
objects and for caching to work it's
dependent upon there being a temperature
and variance
excuse me there being a temperature
variance there is no variance and
temperature between objects if there
isn't at least a temporary short-term
time-based variance and the frequency of
accessing objects then caching cannot
work segregation can increase
temperature variance and by that I mean
that if you have two sets of objects
that have different average temperatures
and if they are stored in some larger
containers that are the units of caching
then segregating them can increase the
temperature variance so if you take all
the pointers and put them in one set of
notes and then you put the data in
another set of notes and if by doing
this you increase and if the pointers
tend to have a different average
temperature from the data then by doing
this you have increased the temperature
variance and
so if you think about it for all the
pointers when you go through them to
find the data in the tree in order to
get to the data you have to go through
one of the pointers so I'm sorry nowhere
to get to one of the leaf notes you have
to go through one of the pointers so
each of those pointers is on average
worth as much as an entire leaf node and
so then naturally tends to suggest that
pointers on average have a higher
temperature than data DOS version three
of riser FS and most databases do this
also since they also use blogs they
unbalance the tree so in the original
papers describing blogs their idea was
they would for objects that were too
large to fit into a 4k node they
wouldn't search into the tree a bunch of
pointers to these unformatted nodes and
they defined these unformatted nodes is
not being part of the tree
well this may of fooled the computer
scientists but it didn't fool the
computers
just because you've defined them to not
be part of the tree doesn't mean that it
performs just like an unbalanced tree
and so for all of the reasons that
balance trees are superior to unbalanced
trees blobs are a bad idea as they are
traditionally implemented you have
reduced fan-out when you have a less
balanced tree having an unbalanced tree
reduces fan-out
mixing pointers with data gives you a
nun gives you a unbalanced tree and
reduces your fan-out in version 4 what
we did was we took those blobs and we
stuck the pointers to them called the
twig level of the tree so we have leaves
above that we have twigs love that we
have branches and the other thing we did
was we at the same time we also chose to
use extents and the basic concept behind
an extent tissue define the starting
block and you define a length from these
two counters give you a whole extent and
if you do a good job when your
allocation you can define a four
gigabyte file using one extent and
that's very efficient to do it
yes if you're using contiguous
extensions you wind up with problems
with internal presentation if you asked
if you're using contiguous extents don't
you end up with a problem with
fragmentation did I repeat your question
correctly
and the answer is yes and the answer is
we use heuristics and we can observe
that they have a tendency to work and in
the worst case extents to make things
worse so in the worst case you cannot
find a the worst case you cannot find an
extent that's longer than one block and
so in the worst case when you allocate a
file you end up allocating a you end up
allocating it in one block extents and
because the extent has to have both the
count and block number it's a bigger
pointer and so in this worst case an
area I just defined you have worse
fan-out and well the difference is
between worst case analysis and the real
world as well you know there's just sort
of an art to knowing when you have to
worry about the worst case and when in
practice things are just going to work
so in practice we tend to get good
allocation if in practice we didn't get
good allocation this would be a bad
design but in practice we do get good
allocation and there are a number of
things that contribute to our getting
good allocation
practice and one of these things is the
one of these things that we do
allocation and flush time and I'll get
to that a little bit later and the other
is that we're working on a reach hacker
and I hope that we have one are not too
long and the repack er it's my belief
that for our worst case scenarios it's
actually more efficient to let the
repack er come in and clean things up
later than to try to optimize them on
the fly or the worst case scenarios and
I may have to skip a little bit for it I
think that so I can see that this scheme
will improve performance for some tote
loads but are there some specific
workloads where it'll make things worse
like lots of frequent updates there are
workloads for which we do need the
rebounder and I believe that when you
have frequent updates you're better off
having a repack err reoptimize things
later they probably probably will turn
out to be we're closed for which this is
not the right approach but I believe
that this
that this gets 99% of the user base and
I was once taught by David hits the
founder of network compliance he said
something that I thought was very valid
which is when you're designing a product
you should go after the 80% of the
market and you need to be careful about
the remaining 20% because the remaining
20% can be the hard part and you can end
up throwing so much resource at the 20%
that it distracts from solving the needs
of the 80% I'm sort of paraphrasing what
he said my apologies to David
so
traditional trees is it by the way is it
going to work for the video conference
thing it might draw on the board it will
yes okay
so traditional balance three outward
that was used fixed criterion for
determining when to balance and the most
common criterion used
so most common criterion is if you
delete something from this node so if
you delete something from this node then
you would look at this node the left
neighbor would look at the right
neighbor and if these three nodes can be
squeezed into two or fewer nodes then
you perform a balancing and you squeeze
them together and replace the three
nodes with two or fewer nodes now you
what you can also do
if you want tighter packing instead of
looking at the left neighbor in the
right neighbor you can look at two
neighbors to the left and two neighbors
to the right and if these five nodes can
be squeezed into four fewer nodes then
you squeeze them all together and
perform that analysis with every
modification to the to the tree and you
can do it for three nodes to the left
and three nodes of the right so what is
the reason why we don't go all the way
to the left all the way to the right
well the answer to that is how far you
go to the left how far you go to the
right determines what your overhead is
per modification to the tree and so you
have a memory bandwidth overhead and you
also have an overhead that's due to any
of these nodes that aren't in memory at
the time that you need to access
and so what we decided to do was to be
lazy about it and we decided to instead
of modifying the tree with every
modification to the tree modify the tree
in response to memory pressure and
instead of making how far to the right
and how far to the left we go dependent
on some fixed criterion so fixed number
we wouldn't stand gathered together an
entire swamp now a slum is a set of
contiguous entry order dirty notes so in
this thing that I have drawn these four
nodes are what flood because these
little X's mean dirty
and so if the if vm in the case of this
node is dirty and it's been in memory
for too long then what we do is we come
down the tree oh we we start from here
in the tree we go all the way to the
left and keep going until we get to the
first non dirty node and then we stop
here and this is the left part of the
tree and then from here we go in this
direction in the tree and as we go we
squeeze things all the way to the left
and if there are a hundred thousand
nodes we will attempt to squeeze those
hundred thousand nose into 99999 notes
and at some point we decided that rather
than trying to estimate whether or not
we could squeeze those notes into fewer
notes we would just do it we just shove
everything as far as the left as we
could put it and that turns out to be
kind of reasonable to do in terms of
performance and its effect and it makes
the code a whole lot simpler because
when you don't have to estimate when you
can just shove things to the left cause
it's a good way of calculating errors to
just try and see if it works and when we
perform this squeezing we have
eliminated the overhead versus tightness
of packing trade-off and at the same
time that we do the squeezing we also
have a but some other much plugins that
we process so we also have encryption
and compression plugins they get
processed at flush time and we also have
block allocation plugins that occur at
lunch time and so we have all these
things that we do at flush time and this
gets us a much better result one of the
problems with compressed file systems is
that if you is that they don't work so
well for when you have a file set I
should say working set that fits into
Ram and now to the computational cost
they add having to compress with every
write even though all this stuff fits
into RAM and doesn't really need to be
compressed and by deferring compression
until time the time to flush things out
to disk we greatly reduced the overhead
cost of a compressed file system same
thing goes for encryption we don't
really need to encrypt things when
they're in memory because memory is in
some gear anyway and with encryption
what we're really trying to do is ensure
that when your laptop is off and
somebody comes along and steals it they
can't do anything with it we're not
trying to protect you from me
and so the disadvantage of this approach
is what happens when you slum sizes one
and I would suggest we sort of make that
mean the decision and I would suggest to
you that it's valid is that if your slum
size is one
you're better off letting the repack or
do it you're better off letting a repack
to come along and once a week it'll go
or once a day however you choose to make
it it'll go through your file system
from beginning to end and suck things in
and for megabyte chunks and write things
out in four megabytes instead of trying
to repack one little 4k note at a time
it'll suck in a whole bunch of poorly
balanced 4k notes that were modified
during the course of the week and I
believe that's just more often
our file system is designed to reduce
your coding costs when using it so we
have file plugins file plugins will
allow you to create a new file plugin
they'll perform any arbitrary operation
you want to perform in response to a
read or write for any of the other
standard BFS operations
directory plugins hatch plugins
key assignment plugins note plugins item
plugins space allocation plugins disk
format plugins super glory plugins the
disk format plugins allow us to come out
with completely new versions of browser
for and not have the users have to
notice it much less reformat the desk
yes
I was asked to give a few practical
examples
well the compression plugin is a good
one the encryption plugin is good one
copy and white plugins
there's users who went to write that
there are
there's somebody
if you want to write a clustering
filesystem you're going to want to have
a few plugins for your cluster
filesystem if you wanted to implement a
search engine with this is a storage
layer you could write a set of plugins
to do that and so you could create a set
of objects that would be used by the
search engine and have our balance tree
algorithms efficiently squish something
together store them on disk suck them
back in and because of our item plugins
I should explain that in version 3 all
the different kinds of items we have our
balancing code special cases them and I
started to realize I'm looking at the
code that the complexity of the
balancing code was going to be
proportional to N squared where n was
the number of different kinds of items
and so in version 4 we have these item
plugins and the balancing code doesn't
understand the internal structure of the
items that it balances at all what it
does is it invokes an item plug-in and
it has a set of defined methods that
each item plug-in must be able to
support and if you write an item plug-in
that supports those methods the
balancing code will be able to
efficiently manage the space for those
items without understanding them at all
so long as the item plug-in understands
their internals yes sir
pre-operational fries but we do a lot of
those optimizations that's what
determines our performance but
actually I should say most of the
optimization actually gets done in two
key assignments area and the
optimization that we do is simple we try
to ensure that tree order correlates
with disorder so when we and when I say
tree order I mean parent first preorder
so I've now number these nodes and
parent forest pre-order and if you
modify if we modify this known actually
let's say we modified this long ok then
we look at we look at the left neighbor
of this of this slum which is number 3
here and we see what it's block number
is on disc and starting from there we
search for a free for a free block and
we then attempt to assign block numbers
in this order before 5 6 7 8 9 we try to
put parents before their children and so
we're by doing that we're attending to
or optimized for tree order access which
it turns out to be a good thing to
optimize for
and that is the simplistic version of
what we do and there are other things
that we do to handle the case where the
bitmaps start to get crowded and I can
also talk about when we relocate things
for small slums say this is a large slum
then the cost of introducing a large
seek is small so if you put a large
stick here and it allows you to put
everything into a nice neat order that's
you know perhaps worthwhile to do
unfortunately and there's only so much
time in this talk so I have to get
really glossy
but the main point that I'm trying to
make here is that working on a storage
layer like this it's very cost effective
it is a lot less work to do something
when you have all these different layers
of abstraction and you can just write a
new plugin to do what you want and you
don't have to rewrite everything in
order to make your one small change
and to some extent this is a reflects
the difference between a first
generation product from the second
generation product so my first
generation product we had an idea we
just tried to get it out the door now we
don't know one just get something out
the door we want to have something that
will we'll be able to enjoy working on
for a long time
and I think I'm going to skim over the
miscellaneous lock could somebody tell
me how much time I have left
ten minutes okay
so
in our locking going towards the right
and going up is considered high priority
and going down and going to the left is
considered low priority and what this
means is that all of our balancing
operations which go to the right and go
up our high priority and all of our
searching through the tree operations
our low priority and the reason for
doing that is that for our code having
to undo a balancing operation would be
very expensive but having to undo a
search operation is very cheap and
whenever a high priority thing bounces
into a low priority thing when the low
priority yields to the high priority and
I would give a much more rigorous
description of this if I wasn't a long
time we only have time for the very
simplistic explanation of
during long so I'm going to give that
and that idea is that instead of writing
everything twice in order to it surely
is committed atomically first writing it
once to the journal and then running it
to where it's supposed to be on the desk
we first write it to the journal and
then we change our definition of where
the journal is and when we define where
we're going to have our journal we
happen to choose a location that's where
we want things to end up being and I
could give you a much more sophisticated
explanation if I was it's run time and
we also have interest in making things
transactional and I'm going to skip over
that and I'm going to skip over the
repack er you know 80% of files don't
move for long periods of time and if you
read back once a week 80% of your files
are going to be perfectly laid out for
reads
yeah
these benchmarks are all
is it possible to connect to the
internet here they must be
somebody help me with it
oh this laptop is too old for wireless
okay it's okay I'm gonna I'm gonna match
it so in this file set 80% of the files
are 8k in sizes are smaller 80 percent
of the remaining files or 80 K in size
or smaller 80 percent of the remaining
files are 800 K or smaller and this sort
of strange distribution function turns
out to be fairly close to what real file
systems have and in most file systems
most of the files are small and most of
the bytes are in large class that would
be different if they were if the files
were indexed and each of the keywords
was considered to be an object but
that's the traditional statistic or
traditional Clow system uses patterns
and so in this benchmark we first create
a bunch of files then we copy them then
we read them and we perform stats on
them then we delete them
and we generally get good performance
and these benchmarks are a little bit
obsolete because we just got our
compression plugin working and I don't
have the statistics compared to our
competitors but in general are our code
compared to our code with compression
turned on the creates take our take 76%
of the time that they used to take and
poppies are like 56% and
reeds are like 55% and deletes were
reduced but I don't remember by how much
time and for a file system for a
computer that has a decent CPU and a
disk drive the CPU can compress faster
than the disk drive can write and this
is a change in technology that's
happened in the last 10 or 20 years that
CPUs have been getting faster faster
than disk drives have been getting
faster and their transfer CPU
compression time has improved compared
to disk transfer time and so now you can
a compressed file system is higher
performance than a uncompressed file
system and you can improve your
performance by using
and I'm going to stop with giving the
talk and get to the questions which are
the important part
yeah our to keep my chin to let our
users do that and they post I'm sorry
she doesn't say what the questions did
have we done real-world benchmarks like
compilers and the answer is yes and our
user students have done them and I tend
to I am pleased to say that the kinds of
performance that I get in this benchmark
my the users tend to report getting
similar performance improvements and to
get a really thorough answer to your
question you should ask my mailing list
Reiser at best - last names just calm
and you know if you want benchmarks that
you can rely on you don't ask the author
you ask as mailing list yeah I should
say that
that in Version three one of my great
disappointments was that when you turn
on tails so you squish those files
together we got a performance drop when
the file size was randomly within the 1
to 10k size range and I have since
understood that actually with ffs when
they introduced fragments one of the
things that they didn't put into their
paper was that there was a performance
drop from combining those tails together
so whenever you take multiple pieces of
different files and squish them together
and you you have to be careful you don't
introduce seeks and with version 4 we
don't introduce SIG's and with version 3
we did and so now with version 4 if you
efficiently store the small files you
get a performance increase period and
with version 3 you had to pay a
performance cost because we were
introducing things
questions
no questions yes
so I gather you're asking will Reiser
for support accessing the name of the
owner of a file within the file system
namespace is that is that what you're
getting it I'm trying to objects with
offshore entities and then you can
create the object collection right at
your answer to things like
so it's not exactly wish you know about
his motivation
yeah we want to do that stuff I should
repeat your question I think your
question is when FS allows you to query
the metadata will Reiser for or allow
you to query the metadata or just are
just act what yeah that's correct okay
the answer is yes we want to be able to
do that the code isn't currently there
it should be done it's part of our
direction we'll get there and it will be
sadly political the road to get there
and we will overcome this political
problems</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>