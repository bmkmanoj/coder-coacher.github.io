<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning Rules with Adaptor Grammars | Coder Coacher - Coaching Coders</title><meta content="Learning Rules with Adaptor Grammars - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning Rules with Adaptor Grammars</b></h2><h5 class="post__date">2009-07-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TauFdjTNOHQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you very much Keith yeah it's
actually it's great to be back here
amongst all these friendly familiar
faces so yeah so actually this is the
the very first slide sort of shows a
little bit you know this is the familiar
drunk under the lamppost story so
there's a I admit to actually sort of
doing some serious lamppost looking
under here right this is you know
nonparametric Bayes was the flavor of
the month you know maybe a year or so
ago and so this is me as someone that
likes to think about grammars trying to
think about how one can you know can we
do anything interesting with these
nonparametric Bayesian methods so I
admit to that you know to being a
lamppost looker so what are some of the
ideas here so I think actually the
nonparametric Bayes stuff really is
interesting because you know standard
parametric learning methods you know and
I'd say at least all the successful
methods I know about really really are
sort of reducing learning to
optimization so you wind up having a
vector of parameters we've got some
objective function and essentially what
you have to do to do learning is to find
the the values of the variables and this
vector of variables that optimize some
objective function and that's that's
really what learning boils down to so
for example in general the way in which
people wind up modeling learning of PCF
Jesus you imagine you start off with
some super set of grammar rules and then
you're going to try and use a parametric
estimator try and estimate the
probabilities on those rules and then
you'll keep the rules that have
probabilities that pass over some
threshold so non parametric estimation
techniques you know have the potential I
think to sort of provide us with a
systematic way of actually learning the
parameters as well as learning the
values so that's what nonparametric
means here and just out of interest how
many people have seen things like
Chinese
restaurant processes and stuff like that
so we're pretty much every so I can
assume that it's moderately familiar
here right so as you guys then would
know when you're actually sort of doing
these type of estimation MCMC methods
are relatively natural the second half
of the talk which I can spend more time
focusing on is actually on some of the
methods which we've actually had to sort
of develop in order to inference is the
big problem and some of the methods that
we've developed to make the inference
more efficient okay so in the version of
the talk that I wound up giving to to
linguists actually I put this up
actually saying that sometimes even say
more strongly that trance Keys should be
a Bayesian right because Bayesian
inference is actually I think sort of
fairly natural you know you the
posterior depict decomposes into these
two terms into a likelihood term which
is data-driven you know and then the
prior which encodes the information that
you've got about the learning problem
before the learning task actually even
starts
but of course what makes this tricky is
that in general you're wondering when
you're doing this inference you're in a
situation where there's infinitely many
possible grammars ok so how many people
haven't seen context-free grammars
before everyone's sort of seen them so
basically you know they have rules that
look like this over here they generate
trees that look like this the
probability of a tree is the product of
the probabilities the rules that are
used to build them is that relatively
clear and so we can use them to describe
fairly simple models of what's honest
phrase structure let's go on so it turns
out that there are lots of sort of
fairly standard Bayesian techniques for
doing pcfg inference they work really
well on artificial data they work really
badly on real language data there's lots
of reasons why you know you can come up
as post hoc rationalizations as to why
they might do badly the strategy I've
been taking
for a while as to say gee maybe in fact
actually we could learn something by
studying simpler cases and in fact I'm
going to start off wondering about just
the morphological segmentation okay so
basically I'm going to say all right I
want to try and see if I can learn given
a bunch of inflected forms like walking
I want to try and see or walks and
walked those things I want to see if I
can learn that the walk is a stem and
ing as a suffix and if I can do that
I'll declare success I'll get a little
bit beyond that in this talk but let's
start off there and we'll actually sort
of show us what some of the problems are
with context-free learning models so if
your goal is actually just to reflect to
build a tree that can encode the the
correct segmentation here of talking
into talk plus ing this tree here will
in fact actually do it so this tree this
grammar is going to be the sort of
grammar we'll work with for a while
right so the grammar says that a word
consists of a stem followed by a suffix
and stems and suffixes are both
generated by sequences of characters and
characters are generated by this right
branching tree structure over here
that's just a sigma star essentially so
yeah I mean what you can see over here
is these trees certainly can represent
that segmentation but I would hope all
the computational linguists in here with
sort of the boggling at this stage right
because in fact actually this these
trees over here even though they can
represent the distinctions were
interested in there's no way that these
PCF G's can actually learn that and the
reason why is because inside of a pcfg
the parameters are associated with rules
but the rules here are two smaller unit
of generalization right so this pcfg can
learn things like the probability that n
is a character but that's not what I
want to learn I want to learn the
probability but talk is a stem but
there's no single rule it encodes that
here so no matter how I tweak these
parameters I can't learn the
generalizations that I want to learn so
what you really want is something that
looks like this that says that a word
consists of a stem and a suffix stem
expands to all fossa
Stan's suffix expands to all possible
suffixes and at this stage you guys
should be keeping your hand on your
wallet in fact I gave a presentation at
the University of Arizona where I use
this example and somebody in the
audience said yeah I should should have
put your hand on your sidearm right
because there's an infinite number of
possible stems and possible suffixes
right and so this is no longer a
context-free grammar because there's
these need to have a finite number of
productions but there's actually a sense
in which so first of all it should be
clear right that here we can with these
sorts of things we can in fact actually
learn the probability that stem rewrites
as ta LK which is exactly the sort of
thing that we do not alone
there's a sense in which this is not a
even though this is no longer this this
is not a you know because the infinite
number of potential rules there's it's
not a real problem here and the reason
why it's not a real problem is because
actually given any particular data set
there's only a finite number of stems
and a finite number of suffixes that you
could possibly find in that data set and
in fact actually this sort of says that
gee if I actually get to pick the rules
after I've seen the data but if I let
the data choose the rules that I want
but in fact actually there's not really
any problem here at all right and
there's a sense in which actually that's
sort of one of the intuitions behind
this nonparametric work we're going to
let the data pick the rules as well as
picking the rule probabilities right
okay I want to now show there's still a
few more problems here so okay all right
if I said that stems consists of all
possible sequences of characters
suffixes consists of all possible
sequences of characters you know we do
this thing that I just said before well
I'll let the data I'll only look at the
subset that could possibly be used to
generate the data now can I throw my
pcfg learning procedures at it well it
will it work will it find the right
segmentations
factor turns out it won't it's fairly
easy to show that if your goal is just
to maximize likelihood the map the ml
solution is always going to be the so
called saturated models which wind up
saying say
for example at every would consists of
the stem and the suffixes always now
these saturated models have one
parameter for each possible word type
down here so in fact they can actually
nail the empirical distribution exactly
and there's a theorem that says
essentially the maximum likelihood tries
to minimize the KL divergence between
the predicted and the empirical
distributions and if you if you can fit
the empirical distribution exactly then
you get 0 KL divergence and in fact
accurately you can show that eeehm
methods for example which try to
maximize likely what they do in fact
actually find this trivial solution so
we're still not there okay so you know I
started off talking about how Bayesian
inference is great so all of you are
probably thinking well you know we know
how to use Bayesian priors to enforce
fast solutions and so the way in which
you can do that then is you can say gee
I'm going to have a so here you can see
I started off if I put a douche Li prior
the uniform douche low prior looks
something like this as these alpha
values start to get smaller and
additionally what actually happens is
that the prior becomes increasingly
concentrated around zero what that says
essentially if I use a dual a prior on a
rule that looks like this what this says
is you know actually I really prefer
grammars where this where this rule has
probability zero in other words where
I've learnt to turn off the rule so you
can think about this these dirty priors
is letting us do a type of feature
selection you know and in fact those of
you that do MaxEnt with different types
of you know l1 regularization they're
essentially using a prior to do feature
selection for you to I think you could
use an exponential prior it would make
inference harder because the delay is
conjugate to the multinomial so that
lets you that lets you actually
integrate out the the rule probability
parameter but but at this sort of
conceptual level you could you could
certainly do that i i've actually played
a little bit with a few other priors to
see if they have properties that might
be useful
no in fact actually so it turns out if
their conjugate it really gets what
actually happens is the the inference
really just boils down to you know
taking the same expected counts that you
get with something like am+ adding these
douche Lake Counties are this become
like pseudo counts and if you want to do
variational Bayes then you pass it
through a squashing function so so
there's no there's no linguistic
justification for saying the douche
layers more plausible than anything else
that's Noah Smith actually I think has
done some experiments on some non
conjugate priors and on some
applications he's been talking about he
actually clients that he gets in for
learning dependency grammars
unsupervised induction of dependency
grammars he actually claims that a an
entropic prior in other words if so so
given these rural probabilities theta
you can define the entropy you know for
each for each multinomial and then I
think you actually he winds up saying
gee I want an entropy which is as low as
possible
which means that it's going to be as
concentrated as possible and he claims
that for his dependency grammar
induction I've got no reason to
disbelieve him I'm just saying
he says for used it for his application
but the entropic prior does better than
the - like the math becomes more
complicated because it turns out you can
no longer just analytically integrate
our first things so yeah here I'm doing
some serious lamppost looking under
right okay all right so here we go a
little experiment I'm going to sort of
crank this alpha down towards zero let's
check to see well if they look that so
this is where basically I've taken all
the all the verbs from The Wall Street
Journal and so you can sort of see over
here with alphas fairly large there's no
segmentation whatsoever so I'm still
getting the maximum likelihood
segmentation as alpha starts to get
smaller I'm saying to pull off some
suffixes but it's
great different I mean you know
including is still sitting here
and you shouldn't some statisticians
pointed out to me that if I'm using an
alpha of 10 to the minus 15 I should do
10 to the 15 iterations at least before
I should expect convergence so you
should not believe these numbers over
here but in fact the point is that even
the Bayesian sparse prior is really not
coming up with anything reasonable here
so why isn't it well one way you can
sort of take a look at it is here's the
dish like prior parameter I'm looking
here essentially at the posterior
probability as a function of alpha and
what I can actually do is I can plot it
for null suffixes down here so there's
no segmentation the true suffix is
because I know what the gold
segmentation ought to be and then these
are the optimal suffixes that my search
procedure finds right and so you can see
essentially that the true suffixes are
like many orders of magnitude of
probability separated from from the ones
that are that the search procedure is
finding so search procedure is not
guaranteed to find the optimal ones of
course still a heuristic search but the
fact that so the optimal life is still
further up there so what this is
actually showing is that in fact the
model is just plain wrong right I mean
so that's that point there so what's
wrong with it
well such a simple data set that we can
in fact actually try and figure that out
and one of the things that's wrong with
it is that the model basically assumes
that the probability of each suffix
should be the same you're independent
with the stem so here I've just simply
plotted the probabilities of different
suffixes for a few fairly high-frequency
stands if you actually calculate the
error bars and here the error bars are
not really visible their frequencies are
so high so in fact these differences
here are incredibly significantly
different right so and in other words
what this is just simply saying is that
you know a verb like expect appears more
frequently or including appears far more
frequently than of this one over here
expecting you know I mean and that's
sort of pretty obviously true so that so
that so the the model has built into it
independence assumptions that
simply aren't satisfied and the data you
see far more diversity in frequency then
that than the independence assumptions
in the pcfg
model would would predict and there's a
number of ways of trying to deal with
this one method that we've been looking
at is to say to you maybe in fact what
you really ought to be doing is is
inference from types rather than tokens
that is you want to sort I and dampen
the frequency variation so by types what
I mean is if the data is something like
you know the cat chased the other cat
you can see the types and the tokens
listed here so if you do inference from
verb types then because there's safe
there's only for common verb suffixes
but in general each suffix is going to
appear here people that fix it at that
point two-five in types and in fact
these types of reasons actually have led
some psycho linguists to actually
propose that people learn from types
rather than tokens and in fact actually
if we do exactly the same procedure but
we ran it off off the types then in fact
actually discovered that the
segmentations come out looking actually
sort of quite good and in fact even some
of the things that count as errors in
the actual procedure actually are sort
of fairly plausible so you know the
reason why this e is pulled off here and
continue is so it can capture the
high-energy generalization and
continuing right so so it generally
looks sort of pretty good and then if
you look at the same posterior
probability plot over here you actually
sort of discover that in fact the the
true suffix is you know you know sort of
wind up lining up with the optimal
suffixes so again that's exactly what
you're sort of hopeful yeah
where we are down here so I think what
happens is that as this starts to get as
this alpha gets low it's starting to
become far more I think in fact it's
starting to pull off a prefix here I
don't know what's happening down there
yeah but in fact I think if you actually
look at the plot on the next on the next
one over here you can actually sort of
actually asked what value of alpha turns
out to be sort of close to optimal and
effective I didn't plotted a tenth of
them I didn't give you the solutions at
10 to the minus 3 but this is 10 to the
minus 5 over here so in fact somewhere
in between these two this is probably
where the models giving the best results
so anyway so basically you know here
what we've identified as sort of two
problems with the standard you know
independence assumptions inside of pcfg
ES and hmm right so one is that the
rules often tend to be too small to be
effective units of generalization so you
really want to generalize over group
source entire sub trees and then the
second problem is that that in fact
actually sometimes you want to be able
to dampen frequency variation you want
to be able to do type based inference
rather than sort of token based
inference no it doesn't it doesn't come
for free in in you know in this case
here right
so so yes so one thing you what you
could try to do is to essentially I mean
you can build a mixture model inside of
a pcfg and so you could then try and
have say for example you know suffixes
that tended to have low probability
suffixes that tended to have medium
probability suffixes that tended to have
high probability so you could try and do
that and in fact we before we started
doing this we in fact tried that sort of
mixture model and but it certainly does
better than the model without that I
mean the mixture model can fit a greater
diversity of probabilities I'm a little
what I mean
I actually actually think that there are
two ways of making piece of models like
pcfg nonparametric and in fact they're
not that they're not exclusive they can
in fact actually be combined together
the mixture model approach corresponds
to something that's like state splitting
you know we're essentially where you
don't know how many mixture components
you need to put in and that's where
things like Lda and those types of
methods go I chose not to focus on that
within these adaptive grammars but in
fact I think you know that's obviously a
very successful and very you know I'm
not going to say that stuff is bad it's
been a very productive line to go yeah
all right so now I just like to sort of
briefly remind you guys about Chinese
restaurant processes that really that
they're the extension of if you take a
dish li multinomial and extend it to an
unbounded number of outcomes but it
makes sense just to briefly go back over
Chinese restaurant processes okay all
right so again just imagine that you you
we just actually had a finite number of
outcomes so this is like you know just
imagine that you know Massey's rolling a
die but it's this
strange old Italian dice oh I don't know
how even it is right so I want to
estimate the probability on the
different sides of it right so that's
what these disease are I've seen roll Z
1 through Z and I want to predict the
probability that Z n plus 1 is K and if
I have a - Li prior with parameter alpha
sub K this in other words it's the price
the pseudo count for side K and I've
seen side KN sub K times in my
observation sequence then the predicted
probability for seeing a K on the on the
n plus 1 roll is going to be
proportional to alpha k plus n KO right
pretty straightforward okay so the logic
behind the Chinese restaurant process is
okay what happens if I let the number of
sides on this die go to infinity so I
now want to imagine that there's a very
strange die that Massey's presenting me
with I can't even count the number of
sides on it so this is kind of like if
I'm trying to do clustering I don't know
how many caustic components I might need
for example so as I let the number of
sides go to infinity and I'm keeping my
data set fixed what's going to actually
happen is that most of the outcomes are
not going to be seen in my data set
right is it you know if you know if n is
equal to a hundred and M is equal to a
million I'm going to have seen I'm not
going to have seen 999,000 whatever it
is you know outcomes so I can actually
then ask okay so what's the probability
of seeing an outcome that I have seen
before
well this alpha over m winds up sort of
going towards zero so it's going to be
essentially this alpha sub K term winds
up going to zero so basically all the
the probability of seeing some previous
outcome is just going to be proportional
to the number of times I've seen it
books in it before the probability of
seeing some other outcome but I haven't
seen before is going to be proportional
to alpha divided by n right so that
that's because my pseudo counts now get
divided over these M different
the outcomes over here so so so two
things to remember so first of all the
probability of seeing a previously seen
the outcome is proportional to the
number of times you saw it the
probability of seeing any outcome not
seen before is M times alpha divided by
M so it's going to be proportional to
alpha right so now my dot module a
parameter essentially is telling me how
likely it is that I'm going to see some
unseen outcome so the idea behind
Chinese restaurant processes is that if
I'm doing something like clustering the
actual sides of the dye are really kind
of meaningless I I am the person but
it's the data that that's in the cluster
that gives meaning to what the cluster
is the clusters are otherwise equivalent
so I can imagine
say for example Massey's rolling this
funky Italian die I can't even count the
number of sides on it every time a side
comes up I write the label on it so
musty rail you know so Massey's die
starts off initially completely
unlabeled he rolls it the first time and
I go okay all right I'm going to call
this side side number one so I'll label
that one my seer oles it again and then
and then according to this formula over
here I'm going to say the probability of
seeing side one again is going to be
proportional to the number of times I
saw it before namely once the
probability of seeing some fresh outcome
is going to be proportional to alpha
when muskie rolls it again if it's a
fresh side I write a two on it now I
know
now I've identified two sides on this
die but I'm the one that's identifying
those sides so this basically is now
we've now gone from multinomial to
Chinese restaurant processes where the
probability of seeing some previously
seen event is proportional to the number
of times you saw it the probability of
seeing a fresh event is proportional to
an alpha and then the Chinese restaurant
processes are normally sort of shown
with this type of analogy over here
right where you wind up saying
essentially I'm sort of learning this
customer the table mapping over here so
we're starting off with an end
restaurant and then the first customer
the rule essentially is that a customer
sits down either at an occupied table
with probability proportional to the
number of people sitting out of that's
this part of the formula over here or
sits down at the first unoccupied table
with probability proportional to alpha
so since there's nobody in the
restaurant they have to sit down at the
first unoccupied table like that and the
probability of that is alpha over alpha
so second customer comes in they can sit
down either at the at the and and when
the second customer is coming in with
probability proportional to one they sit
at the first table with probability
proportional to alpha they sit at the
second table so the probability of this
particular arrangement is going to be
the prior probability times previous
probability times 1 over 1 plus alpha
straightforward so we keep on going like
that so now you can now imagine I'm in
fact this is this is like the set of
rolls of dye from masses die here that
we're seeing here so we've now rolled a
1 1 2 because the customer sat at table
at this table 2 up here we actually have
got some interesting performance
improvement by using pit manual
processes instead of just Chinese
restaurant processes so pit manual
processes are slightly more complicated
Chinese restaurant processes the thing
that I just described for you have a
very strong rich get richer effect what
tends to happen is that just a few
tables will get all the customers and
hit manual processes can be regarded as
sort of having a Robin Hood system where
basically each table you know you
decrement its count by some by some
number a over here and you're going to
give the mass from that table to the
next unoccupied table so it's going to
make the system become more encouraged
to posit new tables over here and so you
can see if there are M occupied tables
and
taking mass a away from each table then
I wound up giving mass MA
to the next unoccupied table and so the
the formula then winds up looking
something like this so this B here I'm
just simply copying Pitman and you're
who called their parameters a and B this
B is in fact exactly the same as that -
like a alpha that we saw before so you
know you have to sit at that first table
now in fact you can see the probability
of sitting here is not just proportional
to one it's proportional to one minus
alpha 1 minus a ok so so far what's
actually happened is that you know I've
been just treating the labels
essentially as arbitrary integers these
processes get really interesting when
you actually use them to generate not
just sequences of integers but you
actually use them to generate sequences
of arbitrary other things and the way
the obvious way to the most
straightforward way of doing that is to
imagine that the tables get labeled with
something you can imagine you know if
you want to carry out the restaurant
analogy you imagine that in fact
actually each table has a dish of food
that's placed on it but there's some
other process that's known as the base
distribution there's p0 that's
responsible for essentially generating
labels up there so so in terms of the
table to label mapping over here what
you'll actually see is the same sequence
of images that we've got before so this
is now when our now we're imagining that
instead of me writing numbers in order
on the die on the sides of the die as
masse rolls them what in fact I'm
actually going to do is I'm going to
stick I've got some got some set of
objects of you know an urn or something
which contains little stickers every
time Massie rolls his die and turns up a
fresh side I get a sticker but I just
draw out of that pot and I stick it on
there on that side so we start off first
customer comes in and I might decide to
put some fish
on that table out there i generate the
word fish so now the probability of you
know I'm generating the output sequence
fish is the sequence of tables ever sent
over here as one as before now the
probability of the output sequence is
the probability of assigning the
customers to these tables times the
probability of generating this
particular labels on the table so it's
whatever the probabilities are
generating fish from my base
distribution okay now when the second
customer comes and sits down at this
table up here there's already some
there's already a label on this on this
table over here so I don't need to
regenerate that same label again
so you'll notice even though I've
generated the word fish twice over here
I've only paid the probability cost once
for generating that label on the other
hand now when the next customer comes in
and sits down at this table over here I
need to generate a fresh label for it I
could in principle generate even the
same word fish again you know it depends
on what p0 is there's nothing that says
these labels need to be distinct but I
wind up paying the probability cost
twice I wind up paying up the cost for
two labels all right yeah they're
independent draws of p0 at this stage in
fact what's actually going to happen of
course is that I'm going to be
generating these tables and their labels
conditioned on some input sequence and
then all of a sudden that couples all of
that stuff but at this I'm giving you
the generative model here now and so
under this generative model they're end
of hand yes in fact actually I mean
another possibility is to imagine that
the tables come pre labeled you know
yeah okay so just to sort of summarize
so basically CRPS generalizes to an
unbounded number of outcomes
that's they've been heavily used before
in terms of clustering models where you
sort of use the CRPS to generate the
right number of clusters
you wound up using Pittman your process
is generalized CRPS by by having this
discount of parameter essentially that
lettuce that's at model lets us describe
a wider set of distributions by using
this labeling mechanism we can actually
use CRPS and Pitman your processes to
essentially sort of generate
distributions not just other integers
but over essentially arbitrary arbitrary
sets of objects okay I think I'll skip
that here that's about the relationship
with dos late predecessors so there's
there's two sort of obvious ways if you
imagine you're starting off with a
grammar but making it go nonparametric
one way I don't know how many people
here have come across the infinite hmm
so the infinite hmm
uses essentially state splitting so the
idea is that you start off with some
base hidden markov model which has some
some number of states in it but the
actual states that you wind up in the
machine you wind up learning essentially
are refinements or splits of the states
in the original machine so what you
could imagine essentially is that just
in terms of a pcfg is you say okay I
start off with a pcfg that's got rules
like sentence goes to noun phrase verb
phrase but in fact actually that's not
the grammar I'm trying to learn the
grammar I'm trying to learn in fact
actually has split the sentence category
into maybe a hundred different sentence
categories and so idilu' and say
something like sentence some you know
with split thirty five goes to noun
phrase under split twenty seven and verb
phrase under split seventeen so and in
fact actually this is what's what's been
called the infinite pcfg another way of
making pcfg is nonparametric is to
imagine that the so this is essentially
saying that the number of non terminals
can grow unbounded li another way of
doing it is to say let's let the rules
the number of rules grow
boundedly so we'll say in fact actually
the new rules of compositions of several
rules from the art from the original
grammar and that winds up saying
essentially that we're going to cache
entire tree fragments we're going to
wind up learning probabilities not just
a of rules but entire tree fragments and
that's actually what I'll call adaptive
grammars down here and the two ideas are
quite compatible you could in principle
do both although as far as I know nobody
has so so so the informal sort of a
description is that we wind up saying
that inside of an adaptive gram of the
trees that are generated by an adaptive
grammar or that are the same as the ones
that the basis CFG generates but you as
the designer of an adaptive grammar need
to identify some subset of non terminals
as adapted the unadapted non terminals
expand just as any as in an ordinary
pcfg that is you pick a rule
proportional to its probability and
rewrite the non-terminal but the adapted
non-terminal can expand in two different
ways either just as in a pcfg or else it
can just simply reuse an entirely you
know a subtree that you used before so
the adaptive grammar and a sense sort of
caches entire trees and it's implemented
by having a CRP or a PYP for the adapted
non-terminal so let's actually see how
that might work for this little sec
morphological segmentation example over
here so we want to learn how to you know
we want to generate words in terms of
stems and suffixes so the first customer
comes in we need to generate the first
word you can imagine that these rules
are kind of like recipes so this says
that in order to put a label put a word
label on that table over there we need
to find a stamina suffix this is a funny
sort of restaurant because they these
guys order out so in order to generate a
stamina suffix we send our customers to
the stem and the suffix restaurants they
say give me a stem giving me a suffix
we've got the recipes over there
stems are sequences of phonemes and
suffixes a sequence of phone and so we
might generate cap and s bring them back
and then we have the word cats on that
first table up there the second customer
can come in now we need to repeat the
entire process to generate a label on
the second table and again we can order
out now we can say okay I'm going to
reuse the suffix that I used before but
I need a fresh stem so I might generate
dog and now we generate dolls the third
customer comes in and then it can sit at
the same table so now we've generated
cats twice but you'll notice that in
fact we didn't go through this and we
just reused the entire subtree we didn't
go through the stem and the suffix again
so what this actually means is that when
we're learning stems and suffixes we're
learning them from types rather than
from tokens which you know as I said
before we're sort of one of the goals
that you want in something like this so
an adapted grammar basically sort of
says that the you know it's it's it's
interesting that the sequence of trees
that are generated aren't actually
independent of each other and that's
because if it's generated a subtree
before you're actually far more likely
to reuse it and that's because it's
actually learning from the trees that
it's generated before so over here you
can see the rules that are done used for
that sort of expansion here this winds
up actually producing a model which
actually naturally generated zipfian
distributions which i think is sort of
kind of interesting you know if a
martian linguist came and looked at
human languages i think one of the first
properties they would note there's the
zipfian distribution that we've got but
as far as i know this is like the first
you know generative model of human
language that actually generates sofyan
distributions I think I'll skip this
here and we just sort of show one other
entries maybe go back here let me just
and just simply show you one one other
application that we've been playing with
for a while which is trying to learn
structures of named entities so in this
little grammar here we say that a noun
phrase can consist either of completely
unordered words or else it can consist
of optional units a 0 through a 6 or
optional units B 0 through b6 where
units a 0 is just some sequence of words
I mean basically that's all we're just
telling is that their sequences of words
we're going to be training this thing of
noun phrases noun phrases that contain
proper nouns and so the goal here is
just to try and see can we learn some
interesting internal structure inside of
these noun phrases so this is sort of an
example of some of the internal
structure that this this type of model
winds up finding so you can see here for
example that it's pulled out group a3
has turned out to be something like
proper names so the surname so you get
Smith appearing inside of group a3
because these groups can wind up being
sequences of words you'll see in fact
it's actually sort of said okay one of
these things like find although but
that's playing the role of surname group
a to again I haven't sort of told it
that group a to should be something like
middle initials but it's decided that
all the middle initials tend to play a
similar sort of role group a 0 you can
see over here are things like first
names but it's simply pulled out a 4
it's decided to actually use as you know
suffixes like junior and stuff like that
down here you can see that it's actually
learning structures of company names so
you know the very last element of a
company name is usually something like
Incorporated or limited or company but
you can see it's sort of starting to
pull out prefixes and suffixes and then
that's it's also got this possibility of
generating these sorts of names as just
simply purely unordered things so sports
seems to appear all over inside of these
sequences so it lines up in fact
actually saying that the best way to
actually generate you know these sort of
sports names is not either passing them
as a company name or as a person name
but in fact actually sort of generating
them as that as I'm just sort of an old
word sequences well let's just see I
think I have to end by a 5 right so you
just say a little bit about inference
for some of these things yes right so so
in fact most of what I've worked on has
actually been this word segmentation
example here so maybe I should just
quickly go through that just to show you
the types of things yeah so let me sort
of show you that maybe that's maybe that
rather than going through the inference
procedure is the best way of spending
the remaining time so this is another
sort of Cox I type application although
actually people are using this for doing
ants doing word segmentation in Asian
languages now but so the idea is that a
child learning language doesn't actually
get to hear the you know spaces in
between the words so the the input to
the learner is going to be something
that looks like this you want to see the
book that's obtained by taking a
transcript and looking each word up in a
pronouncing dictionary and just
basically catenae ting all the words
together and then your task the learners
task is to figure out where to insert
word boundaries just simply using the
purely distributional cues so here you
can see I've actually shown you what the
correct segmentation
so how can we do this with something
like an adapter grammar so one sort of
fairly straightforward way is to say
let's just generate a sentence as a
sequence of words each word is a
sequence of phonemes and then now the
adapter grammar is going to wind up
learning essentially if we wind up
adapting the word subtrees will
essentially wind up learning the
probability of each one of these
subjects so if you wind up just doing
that just simply adapting these word
trees and and you actually ask what's
the you know what are the typical
optimal segmentations you get about 56
percent of the words correctly retrieved
so that's this unigram model a slightly
more complicated model would be to say
gee let's actually imagine that we
generate sentences as sequences of
groups of words and then nature of these
group so-called collocations and in each
collocation is itself a sequence of one
or more words so this actually lets us
capture some some dependencies between
adjacent words so here for example in
this little tree you can see we're
learning the book as a single unit but
we actually learned that it's a unit
that's composed out of the unit for the
word followed by the word book right so
essentially it winds up doing type based
inference so the type actually at this
level is going to be the collocation
level there so you're not told what I
mean it's one of the tricky things about
these sorts of models is you're not
actually told what the units are you're
going to learn those and in fact you can
get even further so here you can see
what I've done is I said okay let's
actually imagine we have a collocation
that's Jenny
rated as a sequence of words each word
itself is a sequence of syllables each
syllable is a consistent and onset a
nucleus and a coda so so basically what
this actually says is in order to
generate a collocation generate a
sequence of words and I'm going to
memorize this collocation so the second
time I see at a drink
I'm not going to repass it and said I'm
going to reuse that same pars how did I
generate the collocation a drink
well I generated it by generating a word
in fact two words followed by drink so
how did I generate a drink I generated
that as an on-set and a nucleus and a
coda so I actually learnt that by
actually sort of modeling the syllable
structure down here it's not too
surprising right I mean in general word
boundaries occur at syllable boundaries
and so I'm in fact I'm actually learning
what the possible on sets of syllables
are even though nobody actually tells me
where the syllable boundaries are and in
the language and that as you can see now
all of a sudden our score has gone from
around 56% to about 87 percent accuracy
so so accuracy really sort of improves
quite a bit yeah so so if we go back
let's go back to them maybe like the
simplest grammar here so this is the one
that just simply says that our sentence
consists of a sequence of words so after
learning it the the statistics that I
wind up having to keep the number of
times that I've used each rule so I've
used you know I've generated 15,000
words you know but then I also wind up
keeping statistics on the number of
times that I've seen you as a word 402
times in 137 times the doggie hundred
times that sort of stuff there and so in
a sense what I'm doing is I'm actually
keeping track of the number of times
that I've actually I've actually
expanded word
through that particular sequence the top
three rules of the input ones so so you
is the grammar writer needs to write
those rules and you also need to say and
word is a good thing for you to memorize
and then the sampling procedure
essentially does that memorization but
there was a slide here that said
something about Bayesian hierarchy it's
interesting that the Bayesian hierarchy
goes in the opposite direction to the
grammatical hierarchy so those of you
that know something about hierarchical
dershlit processes essentially what
happens is that the lowest level of the
Bayesian hierarchy is at the very top
level of the tree if you need to
generate a fresh one of these co-opted
tubes you actually generated by using
the colic as its base distribution if
you need to generate a fresh colic you
generated by using words as the base
distribution and so on like that so in
fact it's interesting that the
dramatical tree and the Bayesian tree
actually go in opposite directions yeah
so that's the learning structure and
names I'm gonna take too much of your
time up most of the work that's required
is actually sort of finding ways of
making inference in these types of
models more efficient the datasets that
are involved are still really rather
small you know I mean a million words is
a big data set for doing Monte Carlo
sampling on for this type of stuff but
let me just sort of give you a very
high-level overview of this sampling
procedures actually produce a sequence
of samples from the posterior and it's
fairly standard in in most work just to
simply you know run for say like a
million samples and then stop and take
whatever the final you know in this case
here is what the final path trees are
and say that's my output
the right thing to actually do in
Bayesian inference is instead actually
just sort of run your samplers for a
while and average over that output right
that's you know if you read the
textbooks that's what they say you ought
to do if you wind up doing a very simple
form of averaging over here that
actually gains you about five percentage
points in accuracy so that's actually
sort of quite quite pleasing to see
these things are really sort of it's
it's a very hard search problem search
infrastructure you know I don't want to
pretend that's that these things solve
that problem and in fact exactly how you
initialize the search procedure really
matters and one of the points is that
with a with a sampling approach there's
a fairly tempting initialization which
is where essentially you you generate
the first the analysis of the first
sentence at random you generate the
analysis of the second sentence
conditioned on the one that you pick for
the first sentence you generate the
third sentence conditioned on the
analyses picked for the first two and
stuff like that that's what I call
incremental initialization that seems to
be a very good greedy initialization of
the words as against say just randomly
initializing the probability that you
get by doing this incremental
initialization is much higher but at
least in all the cases I looked at I
wound up getting stuck in some sort of
local max and that in fact I get much
better results in the long term if I do
a random initialization and then this is
this is really sort of this table label
of resample and really actually has to
do with inference procedures for
hierarchical dershlit processes
hierarchical Chinese restaurant
processes when you have a very you know
a fairly fairly complex hierarchy in
order to get more mobility it turns out
that as well as actually sort of
sampling parsers for individual
sentences it's also sort of fairly
important to actually sample parsers for
that sample the labels that are actually
on the tables
that also ones up producing much better
results so so just a sort of wrap up
here so these adapter grammars you know
adapt the called adaptive grammars
because they actually sort of adapt to
the strings that they generate and one
way in which they adapt is because they
learn the probabilities of whole
subtrees not just rules the
nonparametric because which subtrees
they learn depend on the data this the
MCMC sampling procedures that are
required here you know if you get the
details right you can wind up improving
performance so they're quite
significantly and so we wind up actually
again the the competitors for this word
segmentation tasks that I was looking at
are generally around you know sixty or
seventy percent accuracy and we're
getting around eighty seven percent so
thanks very much guys
yeah yeah so there's a guy up at Harvard
Tim O'Donnell that's you know that's
actually sort of trying to do far more
syntactic inference that the way that in
order for in order to make these the
inference for these at a program is
relatively efficient I'm caching entire
sub trees so these are sub trees that go
right down to terminals so that's fine
for these sorts of applications I'm
looking at where the things that you
want to learn are things like words or
morphemes or something else like that
Tim's idea is to say gee maybe these
these fragments that you want to try and
learn can be kind of like you know they
don't need to be cashed out as terminals
but that that actually makes inference
far harder you know it's very easy for
me to tell whether a fragment could
potentially be reused in a string I just
need to see whether or not the the
terminal yield for the subtree actually
appears in that string and if it doesn't
I don't even consider that rule because
of course this thing is essentially
learning rules learning you know I start
off with a grammar that might only have
four or five rules in it but I wound up
with a grammar that's got 50 60 thousand
rules in it but most of the rules are
not in the adapter grammar stuff what
I'm doing aren't applicable in a given
stream but Tim's trying to do is as I
said is trying to do this syntactic
stuff but he's facing this problem that
he's trying to do inference with you
know this very very large number of
rules and of course he's finding that
parsing is running extremely slowly so
yes somebody is trying to get what we
are trying to do this stuff but it's so
I think it's hard work you know there's
a part of what makes this possible I
think is precisely because the tree
fragments which are being memorized you
know so so actually what Tim O'Donnell
is trying to do can be viewed as a
statistically sound implementation of
rennes pods doll
yeah yeah so that's actually what I've
recommended to to Tim that he do is that
basically he he because the the
unconditional inference as I said it
really seems to be too hard so I've said
let's just imagine that you just
restricting attention to the trees that
that are given and say something like
the penn treebank and your goal is just
to figure out what the fragments ought
to be so that that should make inference
much easier you know once you actually
condition on the trees then your
condition then all you really need to
learn is where to the brakes appear
inside of the trees so yes you can you
can do that and that should make
inference of course much easier because
it's supervised much better as well
right right right
so so before the the high-level short I
mean there's nothing stopping you from
restricting the search procedure you
know according to all those things yeah
yeah but so the high-level story I sort
of take on this stuff is that you know
it's not as if we're the first ones to
try and learn structure to try and learn
rules right but the the previous methods
are what I sometimes call the Chinese
Revolution approach you know the
Cultural Revolution approach so you you
know you let a thousand flowers bloom
and you pick the you pick the possible
rules according to some heuristic
procedure that's not really related to
the objective that you're trying to
optimize you you you let here this is so
this is one glenncarroll tribe you let
em then run for some number of
iterations and then you prune away the
rules that those probability is below
some threshold and then you run you a
heuristic procedure again to try and see
if it can propose some more rules to fix
up errors so I'm in that sort of model
the rule proposal step really isn't
integrated in with the the actual model
itself you know the real proposal step
is just some additional heuristic
procedure and so I think the strength of
this Chinese restaurant process stuff is
it gives us a systematic way essentially
of putting rule proposal inside of the
learning algorithm as well right so I
think it's philosophically it's trying
to do something very similar to you know
what glenncarroll and various tolki and
all those people tried you know all that
time ago but I think that it's you know
for the first time we actually now have
a statistically sound framework for
actually doing the rule proposal as well
as the the parameter search
so stole key did tolki did the bayesian
inference but but just just parametric
so so here the prior is actually being
used to propose the rules so from from
50,000 feet the objective is the same
you know so yes you know it's trying to
do something that people have tried to
do before I'm sorry for putting you
through this I should have given the
parsing tool that's stuff you should
have you should I should have just
overwritten you I should have just said
conditional random fields yeah
so so so right now I wound up using a
Monte Carlo system where you know
essentially what happens is you need to
run through the entire training corpus
many times over and essentially you wind
up repassing each sentence conditioned
on the trees that you're currently
guessing for each other sentence not in
the adapter grammar world but just in
terms of the nonparametric Bayes world
there's been a lot of work on
variational inference variational
approximations for for nonparametric
Bayes so in in principle I think those
techniques ought to be applicable here
and then then you get a much more am
like update in other words what happens
is that you need to parse all the you've
got a set of parameters you need to
parse all the sentences with respect to
those parameters and since that's the
most time-consuming step that that would
then paralyze very well there are
interesting issues here because in fact
actually one of the ways in which you
know one of the things that you actually
often have to do in order to make these
these variational approximations is to
in fact actually sort of truncate the
the set of say potential structures that
you want to cache to some finite set of
structures and in general like I can't
do that ahead of time so one of the
technical challenges that still needs to
be faced here is that at each step you
need to identify the set of potentially
useful structures this would then
actually make it much more like actually
the sort of stuff that andreas tolki and
glenncarroll we're doing there's some
interesting work that's actually going
on with willing and uit so in terms of
inference for these nonparametric
Bayesian models there are sort of two
basic methods sampling methods and
variational approximate
and so basically what seems to happen is
that the variational approximations are
only accurate on structures were just
seen say five times or more whereas the
sampling methods actually become very
inefficient as the counts get high
so basically samplings pretty good way
of exploring you know structures that
you haven't seen before or structures
that you've maybe seen once before but
you know that one of the problems with
these rich get richer type models is
that when the counts get fairly high it
can take a lot of samples to resample a
table where there's a thousand customers
on it takes at least a thousand samples
to move all the customers off that table
and in fact because of the rich get
richer attraction you're actually fairly
unlikely to do that so variational
approximations are much better on the
high count things and so Welling and tey
have started to develop methods where
you use a variational approximation for
the high count structures so that's much
more like traditional e/m but you use
sampling for the novel structures that
you haven't seen before and that's I
think sort of very promising because in
applications like this there's an
infinite number of potential structures
which I haven't seen before but which
might be useful you know and so sampling
seems like a very natural thing to do
there but variational bayes should give
you more efficiency as the counts</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>