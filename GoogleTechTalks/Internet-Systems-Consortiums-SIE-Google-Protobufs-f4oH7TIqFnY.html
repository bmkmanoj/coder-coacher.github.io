<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Internet Systems Consortium's SIE &amp; Google Protobufs | Coder Coacher - Coaching Coders</title><meta content="Internet Systems Consortium's SIE &amp; Google Protobufs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Internet Systems Consortium's SIE &amp; Google Protobufs</b></h2><h5 class="post__date">2010-04-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/f4oH7TIqFnY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and as I use the security information
exchange which is kind of a more or less
freeish way to get Paolo till later to
get access to a bunch of data which
includes things like passive DNS and
spam feeds and things like this Eric the
gas is a is see it's a program manager
for a site a jersey job okay good and
Paul vixy is one of the ISC founder poke
and here to prices pose thermal white
hat okay all right so sae security
information exchange um people are
familiar with different kinds of
exchanges you'll have exchanges of money
on like nasdaq i have internet exchanges
which paul actually founded one of them
palo alto internet exchange where they
needed to exchange internet traffic and
they may exchange pieces of copper
between all the telco or fiber these
days between all the telco gear back in
two thousand seven Paul vexy David Egan
kind of thought up this idea that
there's a lot of pulled through a lot of
pull data out there that a lot of people
don't have access to and we need to find
a way to get it all together so one of
the things that we're doing because this
is security data people don't like
sharing it because they can get in
trouble for it because either they are
snooping on their customers or because
or because it can cause harm to people
just because of the fact that someone
knows something that they shouldn't have
so what we've created is a legal and
privacy framework which is basically a
contract and a bunch of privacy
directives to say alright everyone is in
this together you can share stuff freely
within the within the infrastructure but
you don't take the raw stuff out as to
go through lots of processing before it
can take stuff out of there on the legal
document keeps everyone honest another
reason we're here is to centralize the
data collection when you have pools of
data that are all over the Internet is
hard to
cross-correlation you don't have any
standard way of you don't have any
standard way of sharing the data they
have different formats if you can get
all the formats the same and you can get
all the data together in a one data
center or within a framework of data
centers all connected to each other
there's a better chance you're actually
going to be able to do some cross
analysis which is one of the things
we're trying to do you may have some
passive DNS you may have some net flow
you may have some some dark net data or
whatever you have but if that's all you
have you're not going to be able to do
much with it but if you can combine some
of that you're going to find you're
going to be able to find out a lot more
and find it out a lot more quickly than
if you don't have access to it all one
of the reasons we're doing this is to
create a network effect between the
security researchers one model can
suggest that it's like Stone Soup ever
heard the parable we're bringing the
we're bringing the soup and the stone
which includes our infrastructure and
the network and the tools that we have
and then a lot of people are bringing
their carrots and onions and potatoes to
make it taste better and the more people
to add stuff the better tasting the soup
gets and eventually you can actually do
some really effective work with it
typically these days you have
relationships between these various
participants businesses ISPs law
enforcement the people on the top
typically have some of the data or
they're the victims and the people at
the bottom usually have the ability to
do something with it and they all have
their own independent non-disclosure
agreements contracts whatever you need
to manage to keep everything private and
and functional and keep everyone honest
that's a lot of paperwork and it's a lot
of trust that has to be built up between
people and it's inefficient so one of
the things we're doing with sae is to
create the efficient sharing with it a
common legal and privacy framework
people can bring their data into sae
have it be sort of a clearing house and
where it's all available freely you can
share freely within there you can sign a
single agreement that everyone else
signs and everyone can can work with NS
ie we're not going to replace everyone's
cherry we're just helping to enable
things which might be inefficient right
now typically for the infrastructure we
have a bunch of sensor operators out
there you know it might be something
sniffing packets off the wire might be
something attached to your mail server
that the spam flows into or you know you
have a web crawler and you're going out
searching but you just go ahead and
create we're basically packetized
bundles and i'll get uploaded via our
sake and some in some scripts that too
are redundant servers so that we can
broadcast that onto an Ethernet
infrastructure within a data center
inside of that broadcast infrastructure
are a bunch of researchers who all their
own machines and hopefully some of their
own data to compare against as we build
out we're going to build a note on the
east coast and we'll relay data between
the nodes and additionally each of the
researchers will be able to talk to each
other over the private network and as we
add other areas like for example go into
Europe or Asia or or wherever we may
have a relay where people can upload
stuff into the cloud and at some point
oh boy use the wonderful there's that
cloud thing let's let's strike that we
have relays where the data can enter and
at some point it may get promoted so
that can talk to all of the all the rest
of those on an equal basis and we
basically take any of the unique data
that's going from one area and pass it
to the other so the others can see it
may not be all the data there's there's
a lot of data and a lot of it doesn't
need to get shared but maybe some of the
aggregate stuff will get shared between
the nodes here's some of the types of
data that there is out there we started
with Pasadena
s in DNS blacklist data we have some
dark net net flow but there's a lot of
other different types of data which
aren't necessarily conducive to plain
old packet captures and we had to invent
something so that we can describe these
various types of data so they can be
efficiently shared on the broadcast
network the first thing we did when we
worked with was passive DNS foreign
Weimer out of Germany he pioneered the
capturing and and we modified some tools
so we can do a better job of of
collecting it on the on the name servers
collect more of the data typically the
passive DNS is at least the way that we
collect it is you have a name server and
your clients all go out through a
recursive name server or caching server
to find out the names that you're
looking up looking up w google com well
the clients not going to talk to the
Google name servers are going to talk to
your cursor at their isp that recur sir
sends a request out the com name servers
to google com name servers and then they
all feed the data back to the recursive
server and then once the answer is found
there cursive server returns that to the
client the position where we listen is
on the downward arrows that are going
back to their cursive name server was a
great benefit to that and that it helps
enable the privacy of the clients
because the recursive name servers
during their queries at that level not
the clients so if you have a large
population say a thousand people or a
million people you won't necessarily
who's making the query but you at least
find out the information that's out
there and that's part of the goal is to
actually get a better map of what's out
there as far as IP addresses mapping two
names mapping to name servers to get a
better map of what's out there that you
would not normally see if you didn't
have these sensors out there so we asked
a bunch of ISPs the universities and
insta donates and data and we're very
appreciative of that and we have been
building up and trying to get more data
on more data types from different
sources the main way to do DNS data
collection back back then was a TCP dump
or DNS cap there are some addict there's
some inadequate inadequacies in those
programs that they couldn't capture
everything so he created a new program
called end cap and end cap tool and
added a bunch of features into that so
that you could so that you could
replicate the passive DNS data into the
broadcast infrastructure that we're
setting up we added some features for
doing plugins so you can do filtering on
the data as it passes into one you can
filter out the things that you don't
need to see so that you can spend more
time crossing sitting the data that you
want for the other data types for
example spam or link pairs or malware or
whatever we create an end message and
we'll get to that later also it's enable
some collaboration between the
researchers we set up a VPN between our
sites so that researchers at one site
can talk to another using unicast
basically your typical act as a web
server or database or who is or DNS
lookups or whatever but it's all with
the private within the framework some of
the hardware that we need for this needs
to handle high packet rates you need a
fast switch that won't drop packets the
servers will typically be 64 bit with a
lot of the lot of RAM and your storage
you know if if all you're doing is
logging disqus fine but if you want to
do anything active with it you're going
to need SSD or a lot more RAM because of
their oh hi packet rates for a lot of
what we're dealing with encap tool here
are some of the here are some of the
things that we did to improve upon a
pcap or DNS cap with larger DNS packets
these days you actually have
you actually have fragments that is what
used to be able to fit 512 bytes this
doesn't fit anymore especially if we're
doing things like DNS SEC and increasing
the number of servers and the amount of
data is coming back with each of these
requests so we need to be able to
reassemble the packets and end cap does
that automatically whereas you might
actually miss that data if you're using
just pcap we dropped the link layer info
we don't need to care about the Ethernet
MAC address we're just really interested
in layer 3 and above normalize the
network format so that what we collect
on the Sun will work on openbsd on a on
a PC will work on a on an HP you know
running risk you know just basically
ignore you know the net Network byte
order nano second timestamps instead of
millisecond and then added some
user-defined flags so we can actually
track what was the sensor that actually
gave us the data what's key to sae is
the fact that we have this common
infrastructure where everyone can listen
to an ethernet bus of the same data so
when it comes into one of our modes will
rebroadcast sit on a local area network
on a VLAN and everyone who's on that
VLAN will get that packet at the same
time so it's not like we're just sending
it to each researcher we're actually
just broadcasting it that makes for a
lot of efficiencies and we need to be
able to take data from files or put them
out to files and we can do all sorts of
passing
the packets once you have the package we
can shift them around in many different
ways one of the best benefits we have of
end cap was when we started making
modules to do deduplication which is
very necessary some pattern matching
internal database lookups like if you
want to match what you're seeing off of
the wire against an internal table of
something that you know and that was
really important typically when people
are setting up security they were doing
security get data gathering they'll put
everything into a database and it may
not scale at some point they'll become
discount and we need to be able to keep
the information flowing in real time and
not just being trapped into a database
which will eventually slow down and I
can't just log the data because it's
really not useful people need to create
real time tools to be able to analyze
that on the fly what we ended up doing
is we built a was a loop called a
loosely coupled multiprocessor where one
machine would actually start out with
the data broadcast is onto the network
another machine would do some different
processing and then once that machine
does processing it would broadcast it
back out onto network and then a bunch
of other machines would be interested in
that and they do further processing so
you have a bunch of machines all
together on the same broadcast network
and they're all doing the processing in
real time I'll have a diagram of
describing that more we partition our
data like for the various data types in
two different vlans so you got passive
DNS on one deduplicated passive DNS on
another to have some net flow and one
you'll have some spam on another and
then you can choose which channel vlans
or channels that you want to subscribe
to you know cut down on your overhead of
what you have to process so this is a
typical this is a typical use of a
filter or the data is coming down on the
left this is the raw
passive DNS data it's coming in from the
sensors it's very high at a very high
rate of speed we have a program which
operates on a server runs completely out
of RAM that does a deduplication of of
what's in there you know you may have a
hundred people looking up w wo com but
you don't really care about that you're
really interested in fact that wwwom is
out there and here's the information
that came with it so deduplication takes
it down to a reasonable level where
people can actually do the processing
with it and then you can do some
additional filtering like we have
something that helps detect fast fox
fast flux in one example would be say
when your nameservers keep changing
their IP addresses that's very useful
for helping to check botnets because
that is one of the behaviors that they
use so this is a graph from last night
where you get about 40,000 packets per
second and that's like somewhere around
80 to 100 megabytes of all just DNS
packets and to do any real analysis with
it you know unless you got you know like
you know 20 or 40 servers that can kind
of split or take a part of the feed off
of it it's really going to be
inefficient to process so yeah the
deduplication you know compared pardon
to graph but just look at the number but
now you're down to about 5,000 packets
per second which is a little bit easier
to process it's like an eight to one
we're getting about me to one benefit
out of our deduplication and it's less
about every four hours it'll it'll roll
over for fresh data so gives you an idea
of how that works and then just for
finding fast flux well you know we get
about two to four packets a second of
just things they're changing it fast
flux and the ant you just watch that
stuff scroll by on the screen and use
your human intelligence figure stuff out
like you know you have a if you have
like a bank fishing site and it's
changing its name servers well you just
see it scroll by on the screen and
saying hey there's a domain I'm
interested in the concept of loosely
coupled multiprocessor
is very important Dave Boggs was doing
this at Xerox PARC and death back in the
80s the invention a lot of people were
just using it as here's I get my package
from A to B but he was starting to use
different he is starting to use methods
actually is broadcast so you can
actually take one piece of data in
broadcasting out to multiple servers
efficiently and another feature this is
that we're doing we're we're leaning
toward real-time analysis a lot of
people in a research peel these days
they build their big databases and they
do queries against the databases well if
instead you know if you do that here
it's taking too long you know the bad
guys they're moving on after a few hours
and if it's going to take you a whole
day to figure out something well you're
pretty much losing so if you can
basically find ways to put the stuff you
know in the ram you can compare it with
out what's out on the feed and can
actually give yourself an advantage do I
say anything more about McGrath later
some of this isn't new except it's
what's old is new again because the way
this kind of thing used to work when
computers were fast in the real world is
slow or computers were fast enough as
you put in a database trigger so that
everything got down to the database but
then when certain things the certain
lines would cross and you would learn
okay you just tried to put something in
the database that caused the following
an exceptional behavior you should
analyze this certainly in the case of
SQL there's no way you're going to keep
up with even 5,000 per second let alone
hundreds of thousands of things per
second if their triggers place and all
the SSD in the world isn't going to help
you do that so I will say that this is
to me the money bullet point in this
presentation is that the security
community has gotten in and abbot of
storing pcap files or restoring
those tables and then having cron jobs
that will look for things and we are not
keeping up bad guys are winning I'm
tired of that so the idea of teaching
people once again how to look at things
in real time and look for cross
correlations in real time was at the
heart of this project originally there
there is some benefit to to analysis in
arrears and that it you can perhaps go
back and look for things that would help
you change your patterns or what you're
looking for in real time just much like
a stock analyst would look at their
historical trends of a stock to figure
and try to predict what's going on in
the future well you know at some point
you have to have something automated
that's pulling that trigger for you to
buy yourself and that's perhaps some of
the kind of kind of analysis we'd be
doing for internet security data so here
are some of the things that we've got on
SI a right now raw passive DNS is what
he come in comes in from the set from
the sensors and then we filter it out
fast flux is one example we do some
comparisons to some things like CBL or
sorbs Spamhaus CBL that's interesting
because you may know the IP address it's
something that's bad but you don't
necessarily know all the names it's
using so using this you can actually in
real time gather all the names that are
being used by a particular machine or
that people are looking up against it
how to do some cross correlation and
preventive work we gather some dns
queries from some dns blacklist from
dynamic dns providers some top-level
domains but I osita Oregon there a s112
is a project where all the stuff that
shouldn't reek out for DNS lookup
someone looking up 10 nadareh darpa for
example or something within there well
you know that's basically misconfigured
networks or name servers out there some
of the root servers it's no longer
operate
they still have an IP address that
people seen the query so so we're
gathering a little bit of that too now
we don't gather root servers and we
operate our own root servers but we
actually have a far wall between us and
them and a lot of that analysis is
actually it actually goes out to DNS Oh
arc so end cap was great for packet data
but there's more that we needs a capture
it needs to be extensible that was maybe
a mistake we made with end cap when we
set out we need to put version numbers
in there we need to be able to create
new formats as new data becomes
available it needs to be fast and
scalable you know if you're looking to
say it described this stuff you can
imagine now let's just use XML well that
doesn't work very long that's part of
why we're here at Google today is
because you guys had something that was
very lickable it needs to be fast needs
to use all those features from end cap
for working with our infrastructure and
and we also need to have filtering
methods that we can plug in and
developers can use so that we can keep
up with the with the time and Robert we
we gave a lot of this to Robert and
Robert pretty much picked it picked it
out and he created end message so now
we're going to switch over to Robert you
want signal yellow yeah switch cables I
was doing 800 x 600 but we'll see it
whatever you have works
so something Eric's had reminded me of
this from RFC 1034 the sheer size of the
database and frequency updates suggest
that as we maintain a distributed manner
approaches that attempt to collect
assistance copy of the entire database
become more and more expensive and
difficult and hence should be avoided in
1987 well fortunately we like doing
expensive and difficult things talking
about postal stop to yeah that's what
yeah yeah well this case we're
all the host names
so we have this in message file format
which is a successor to the endcap
packet capture format and the idea is
that we're not only capturing packet
information but also things that are not
necessarily best represented as packets
on a wire or datagrams on a wire so the
idea is we don't know what types of
information we're going to store so we
should make it store opaque blobs of
information and perhaps that at runtime
will load a module and be able to learn
how to interpret that blob so we have
blobs on the order to tend to 10
kilobytes in length you probably don't
want to optimize transmission of DVD
ISOs I UDP broadcast network we're
interested in things like DNS and email
at HTTP things of that order of size we
optimize we decided to optimize for UDP
or jumbo Frankie thur net in order to
minimize number of socket receives that
have to be done to read a particular
quantum of data and turns out that
google has a ready-made encoding for
medical protocol buffers it's
essentially a an extensible binary wire
format for encoding fields of data
primitive types integers floats byte
arrays unfortunately protocol buffers
are not self delimiting and they're not
self-describing so we have to added some
additional framing and some additional
intelligence in order to be able to use
that for our UD be broadcast media and
for the protocol engineer assistant is
essentially a description of the
protocol so maybe a constant length
header verte
every will like hurt which can encode
what are more pillows so the in cup
format captures one packet and
represents one packet when it's
rebroadcast but since we're bashing that
data and we're buffering that data we
can pack more than one payload into a
jumbo ethernet frame so the average DNS
packet probably by little less than 512
we can fit preps 16 of those into a
jumbo frame user that packet or even
more so why not minimize the number of
socket calls system calls that you have
to perform in order to read that data
off the network what if your payload is
larger than a jumbo frame ethernet frame
we should be able to fragment that so
I'm sure you've seen spams emails that
are longer than 8 kilobytes so we want
to avoid having to have a truncated bit
that indicates this blog has been
truncated and you have to deal with that
well we can just fragmented in fragment
the payload and the receiver can
reassemble it and pass that reassemble
that reassemble payload to the client
application and we have moving photos to
the cosmetic work so there's a 4-byte
magic value at the beginning of the
frame or the beginning of the wild
buffer Flags octet and is a bit that
means fragment was a bit that means
compress there's a version octet your
version is too and we're going to
represent up to about 4 Giga bytes of
payload we've not come anywhere near
that level yep
so fragment there's a fitness s fragment
every fragment into multiple frames
receiver has to reassemble it and we
cannot use IP fragmentation because at
Luann's us to 64 kilobytes so we do the
fragmentation the segmentation and the
application layer much like TCP there's
a bit that will compress the data so we
can fit even more DNS data or email data
into a given ethernet frame and if you
see both of the bits and you compress it
and in fragment because doing it in the
other direction it and the other water
is problematic well you won't use the
maximum you won't use as many bytes in
the frame if you pregnant uncompress so
the payload header this is the variable
length apart and is now encoded using
google protocol buffers there is a
vendor ID and a message type the message
types are per vendor so you want to
create your own payload message types we
would assign you a vendor ID and you can
assign whatever message types isotype
values you want a timestamp 64 bits the
second suppose 32 bits nanoseconds so
you get a intersect position timestamp
and we have a few optional field for
classification source operator and group
so cooperating senders and receivers can
further classify their data and the
payload itself this is the opaque blob
of information you can ready message a
message type tuple will identify a
particular unique type of message and we
don't necessarily require that the blobs
be encoded with GPB but they frequently
are
we optimized for that particular case so
now we have a the ribbon message see
client API and this is client
applications that want to process
senator series right setup multiplexing
demultiplexing all sorts of apologies
with their in message payloads we
include both a simple single threaded
interface and we have a multi-threaded
octopus io engine that you can utilize
if you want the multi-threaded code is
good in that we could spread the load
across multiple CPUs when we decode
those packets those messages and if you
happen to have a chunk of code that
processes those messages and you make it
re-entrant so it can be cold at multiple
times from multiple threads you get
whatever speed up as possible from that
very currently developing Python and
Perl bindings the python bindings are
stabilizing we haven't yet made a stable
release of the python bindings yet and
matt Sisk at so cert is working on the
purl bindings because i do not use perl
showings aids well python is probably
just a few years younger than 40 but
there's also a message module interface
so that we can extend the message types
that library understands without having
to recompile it and we link all the
readers and writers so essentially this
is a DSO that exports the particular
structure and fills in particular fields
and may optionally provide function
pointers that perform a specific
processing on specific you know pretty
printing parsing
so for example DNS yes DNS has a variety
of interesting and particular wire
formats for its for its data field is
the traditional label octet label
encoded named which is live there's a
security vulnerability recently based on
this concept and the ssl certificates
that where happen to have embedded
knowles in the labels which is valid
according to the dns wire protocol so
there's a nis cdns message type that
will provide a specific function to turn
a label encoded dns name into a human
readable you know dot delimited name and
we don't want to put that type of logic
into lipid message we want to push that
out into a plug-in that particular
message type since we try to make the
core library as agnostic of the upper
layers as possible and typically the the
message module is a fairly short amount
of code that usually just glues some
generated object code from the protocol
buffers compiler yeah so we make it this
allows additional complexity we keep out
of the core library and that's the end
of my presentation cool this is all
available online download ftp from the
IC website and you can actually see some
of the descriptions we had where is c is
vendor ID one you know if you want to
start using this yourself you can make
yourself a vendor ID one of you would be
would ask us for bedroom oh yeah yeah
everyone just picks their own
I don't watch John Purcell no yeah yeah
register with us and we'll make sure
that in the source code that everyone
will play nicely so now that we have so
now we have a message we now actually
have some new channels that we can make
available to people spam we have we have
some bunch of spam traps out there and
some there's actually another provider
sending us this is spam reports we
basically take all the envelope
information some of the headers extract
some URLs out of it and then packet eyes
that people can actually use that a
search provider has given us a URL link
pairs so that we can actually if someone
wanted to they can actually make a map
of here's the here's the normal web and
then if it's not in that maybe you want
to spend some special attention to it
net flow is actually packetized you can
actually we don't do anything with that
we just netflow already has its own set
of tools like the silk toolkit people
are aware of conficker we got ourselves
involved a lot with that and helped
aggregate and collect a lot of the data
that was going through sinkholes so it
created some types that we had all the
web servers report in and we also
captured the DNS data and we actually
capture in some of the p2p data and we
created channels for them so we can
actually have people see that stuff
coming in real time we are making some
development work from malware I'm a grad
that more on another slide we're also
getting some dark net peas for herself
in a large ISP and we expect to be
getting some more and that's just normal
pocket stuff it's not necessarily end
message we might choose to create an end
message module to describe stuff
particular with malware you know you
might start with hashes or md 5s at
another layer you might above that you
might have people passing around I'm
interested in this or or here's I saw
this too and
that you might have something more
descriptive you know maybe even
encapsulate some XML as I oh def it's
very popular with that channel just too
so it's just a single blob that has all
forms in fos flow or is it a single its
we we have we have some we have we have
some version 5 just from our own routers
but getting net flow from other
providers has been difficult it is not
it is not an active channel but yeah it
expected esflow version 9 is pretty much
the standard for that because it does
ipv6 and great and all that other stuff
some other people are working with net
flow and using combined with passive DNS
but we're not doing internet development
our songs right now they're already
tools out there so for passive DNS you
know you can sniff step off the wire
that 255 just basically typifies there's
a channel number that's 202 and here's
the broadcast address we're listening on
port 80 434 all these packets are
spitting out there and you'll see some
information like the timestamp there is
a type is CN cap the identifier of the
sensor operator who submitted it which
is kind of randomized and kept separate
but you kind of tell where the data is
coming from one source there's the name
server that it came from our name server
i commented out that's the sensor
operator so you can see where it came in
and some of the flags that go with it
the first part is the answer part are
actually the first one is the query the
0 means that there is no answer but that
it told you where the name servers were
in in the name server section and
additional info they'd actually ND the
IP addresses so you can take all that
you can put that into a structure if
you're using live end message but you
can even just do plain old text
processing based on that oh you should
use the library it's much more efficient
don't parts arbitrary tech um but you
know some of us old-timers use aux said
Carolyn and such
and don't keep up with that so some
people have done some useful things
without being efficient but yes do you
take the time to actually learn to see
or Python or whatever you guys out there
you'll save yourself some money on your
hardware so another one here's a here's
a sink hole for conficker so here we
have another type I you know for ISC
HTTP and and so we get where the request
came from these are people who are
infected or coming back to try to talk
to the command and control over the web
and we just happen to take over the
domain of that they are using for that
so we can kind of do some things we we
do some POF we look at the all the stuff
that we can get out of the requests that
would help identify for which particular
strain of conficker they were infected
with and then we can create a database
people can use some of this data for
remediation tools and some people do so
wherever you know chris lee but did a
lot of work he's out of georgia tech now
a shadow server put a lot of that
together and make it useful for people
so for spam we do a bunch of
pre-processing scripts that basically
take the email message you know out of
your standard input and then extracts
the things that people are interested in
there should be the hello from rcpt to
the IP address that came from you know
they're seeing their received headers
and the URLs that are found in the
message no it's really interested in the
image blobs yet but if someone does
become interested we might do something
with it we have plugins for postfix and
QPS MTB d QP smtpd which which is a pro
tool which is actually very fast and
efficient and linux and bsd servers so
we have several spam traps of that type
where it's just basically taking unused
domains
people will just keep some expand to it
anyway they're seated or populated and
that's very useful because that really
is spam we have methods where you can
actually tag this is spam reports so if
someone has a reporting address and
abuse address or if they have a button
and click on their client we can
actually create packets that say here's
a user report now there's a loan you
have to go a little statistical because
there are false positives and then that
some people may take some marketing and
say it's family may not necessarily be
pure spam something we haven't
implemented yet but would be interesting
would be say as ever email message comes
in take the headers from that or the
envelope info and say what spam assassin
score was with it and then you can
basically get everyone reporting into a
central source you have a really good
chance of real-time reputation may be
that some commercial services have and
might be able to do a public domain one
all this spam is a great starting point
for analysis you'll typically find that
people are using botnets or in some
cases even just buying services off of
bulletproof hosting it will lead you to
some other data here's an example of
some spam you get off of spam channel
again the timestamp and the type and
sensor it's a spam trap I helped you
skated some of this except that's a real
domain and there's a URL in there and at
FF 24 490 gift oh yeah it points to
basically a UH but you know a viagra bot
vioxx you know kind of advertisement so
this is obviously some kind of a bad
domain as being used for fishing or
actually in this case just spam so one
of the things we do is we take some of
the passive DNS that we have in there
and we look up that domain it points to
an IP address well lo and behold here
all the other domains that are being
used by the IP address that we've
collected via Pasadena
so once you find one you can actually do
blocking or ad blocking on all the other
domains even before they're used like
maybe they don't use them all right away
so you can actually start to be
proactive you can find some in other
information like that JW are you is
probably something close to what the
real name for that server is and then
you can go go probing around though and
find some stuff when you have multiple
data types like passive DNS or spam or
or other information about the networks
you can do a whole bunch of data
combining find some more interesting
info Hosea nazario Thurston holds back
and malware oh wait they made a paper
where they create a point system for all
these different things when you combine
it together and add up the points
actually say hey hey that's that's a
fast buck spot you know like being on
multiple networks within night p address
ranges or how often or how many hosts he
have in the a records Dave Dagon waikele
they just basically took past the DNS a
little bit of string matching like
looking for the work in tavares inside
of the packets coming by and they are
very fast and successful at finding fake
ad sites even before they were
blacklisted by other services Richard
Clayton as a professor who was
investigating a lot of the censorship
that's being done in the UK so he takes
um he takes a list of hosts at a passive
DNS combined that with some active scans
and he does himself and he can pretty
much determine which on the URLs were
getting blocks including at one point
internet archive which is which is a
pretty website to be blocking Andrew
freed he's consulted with us he's gonna
be talking at black hat DC in January
about a lot of stuff that he did of
combining the spam the spams the BGP
info passive DNS analysis of some top of
top-level domains own files to basically
go after things like Zeus avalanche or
fishing or whatever he's very active in
the community
you just saying hey you know here all
these new domains they get stuck in the
servo and they start to get blocked but
he used to this full time for the IRS
back when they were having fishing
problems and now he's actually helping
not only the IRS but all these other
people who are getting hit with a lot of
the same methods ed stoner works insert
and flick on in January he's going to be
talking about how he puts together
passive DNS with net flow to help expand
on what botnet knowledge you already
have you can already get some botnet
knowledge out of net flow but you take
the passive DNS to the next level you
can basically use your IPS to help find
more names which will help you find more
IPS I'll help you find more names
eventually get map of everything we're
actively looking to start the Malheur
channel some other people are creating
products for DNS reputation based on the
data we're interested in perhaps
offering scanning because you know
people who are doing scanning right now
for DNS they're finding out the bad guys
are figuring out who they are so they're
getting blocked so it may offer some
some scanning infrastructure for people
automated abuse or do not distribute a
service attack reporting might be a way
to standardize and have some real-time
reports saying hey I'm getting flooded
and then you tell a whole bunch of other
people which might include you know your
antivirus vendors or ISPs directly and
where it's opposed to picking up the
telephone with the URL search data you
can practice finding that a whole bunch
of people or come to some place at once
and that might be because as Britney
Spears did something that day or it
could be that there's a new virus that
everyone's downloading at once but if
you have everyone looking at a new URL
all at once you can perhaps take a look
at that and you know bgp updates you can
that's security data as well that can be
helpful for finding out people's
networks getting stolen out from under
them
um and here's how people can help you
know if there's sharing methods and
people are using right now that are
between themselves they want to
incorporate more people into that we can
help reduce some of the overhead by
having it put in one central place and
using our broadcast infrastructure to
get to all the people who need it not
have to resend or recopy that data
between multiple places if you don't
like working with service agreements and
ndas and stuff like that we can help
simplify things something else you can
do is bring some servers SI d SI e and
actually take a look at what's out there
I mean there's a lot of good minds here
at Google I can imagine some of them
might be interested I'm security side of
seeing what's out there and seeing what
they can figure out and see if they can
combine it with some of the data that
you guys already have you can also
install sensors you know particularly
with ISPs or corporations or security
companies everyone's got some kind of
data that is perhaps worthless junk to
them but to someone else they can they
can do something effective with it when
they combine it with something else so
people can go ahead and send us more
data we'd appreciate it and so the rest
of the security community that works
with us so we're SI e you can send us
email that info we'll get to all three
of us we got a website there's my phone
number and forend message we can go and
get you download it yourself we have a
developer mailing list that we recently
set up where people can talk about how
they use it is this is generic it's open
source this is not SI a specific you can
use this stuff internally and then tell
us how you're using it might be
interesting and caps available from us
as well we thank thank you guys for
making google protocol buffers and
especially thank all the sensor
operators are out there donating data to
make all this useful
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>