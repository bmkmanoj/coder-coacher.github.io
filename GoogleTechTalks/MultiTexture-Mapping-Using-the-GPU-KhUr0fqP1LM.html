<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Multi-Texture Mapping Using the GPU | Coder Coacher - Coaching Coders</title><meta content="Multi-Texture Mapping Using the GPU - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Multi-Texture Mapping Using the GPU</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KhUr0fqP1LM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm pleased to have with us here a
professor Gottesman I've worked with
what all right I work with the processor
Gottesman in the past and I'm glad that
he's here with us to talk about some of
the last interesting stuff is been
working on in the Technion so thank you
so as my first visit to Google very
pleased to be a thank you for coming to
to hear me so I'll talk about a problem
which arises in computer graphics which
is what I do for a living basically and
the title the formal title of my talk is
multi texture multi texture mapping
using the GPU so I hope it'll be of
interest to people are working on
computer graphics more specifically the
Google Earth and maps groups I think it
will so typically what is done in
computer graphics in visualizing urban
scenes and what I mean when I say urban
scenes I mean scenes are basically of
buildings streets and things like that
as opposed to let's say terrain
rendering which is slightly different
application so typically what is done
and here you see an example of a small
piece of an urban scene and in Germany
some way show you a demo yeah
well so this is your urban scene and
it's basically a synthetic urban scene
meaning we have this is not a real
photograph or anything like that it's
basically three-dimensional geometry
overlaid with texture maps that's what
we have here this is a VRML file which
is which describes the the scene meaning
the three-dimensional shapes of the
houses over here and you can see the
three-dimensional shapes that actually
the polygons here in wireframe so it's
pretty it's pretty pretty modest in
terms of the content the geometric
content and most of the content is
basically the texture maps which are
pasted on the Front's of those buildings
that's what makes it really interesting
i'll put it back that's what it looks
like so what we have here is the
geometry and we have the texture maps
paste it on top and I can show you what
the individual texture maps look like
oops so he has one right so this texture
map belongs I guess too I don't know I
can't really see it right now he has
another texture map it's a piece of
piece of some force a another texture
map
and so on so there's basically a bunch
of images which is what we call texture
maps each one of these individual images
is a piece of the the front of the
building and basically it's pasted onto
that geometry and it's fixed meaning
that specific piece of geometry will
have this specific texture map forever
and ever until I change the definitions
so basically when I'm looking at the the
image you see over here and I apologize
for those of you who do this every day
it's probably it's probably probably not
telling you anything new but for those
of you who haven't really seen it'll
work with it very much this is basically
the introduction so you have these
texture maps which are pasted on the
Front's of these buildings and if you
want to walk or fly through the scene as
you want to do with some virtual reality
or some simulation program basically
what you do is you you'd have your
virtual camera and you'd walk through it
much in the same ways I'm doing right
now just using the mouse and you could
see every given from every given
viewpoint you could see that geometry
paste it with that specific texture map
which was associated with it and you get
the result that you see over here now
this is the traditional and totally
standard way of doing computer graphics
using texture mapping in this case is
just the vrml
so they get rid of that so here's the
same type of type of application this is
a scene from Israel where I'm from this
is the Israel Parliament the Knesset and
you can see it from various different
angles right it's the same building see
the building here from the top and from
a different angle and from all different
angles and basically what you do is you
take these photographs if you want to
build a scene like we saw before you
take these this bunch of photographs and
you'd cut out from the from each one of
the photographs you'd cut out a piece
which you think would be best used as a
texture map and you'd use it to to text
your map the geometry this is the
geometry over here I actually pulled
this off google earth it happened to be
there so i didn't invent this and i
didn't build it myself i just went into
google earth and i happened to see the
knesset somebody had modeled it so I
said oh that's a nice good example and
then I went and I looked for pictures
because these photographs are not
weren't necessarily ready to this and
look for pictures of the Knesset and I
found these pictures and I said if I'd
really want to build a fully textured
model of this building these pictures
would probably be a good source to cut
the texture maps out of line so we'd cut
a piece out of here right so here are
two pieces which I might want to cut out
this piece over here on the top on the
on the left and over here the roof right
the roof of the building so I basically
be able to if I really wanted to I could
basically cut out bits and pieces of
these different photographs and use them
as texture maps like the ones you saw in
the previous example and build this nice
textured model of this particular
building and then I put it in google
earth or put it in any way basically in
any vrml or or visualization
visualization type of scenario or
application and I'd be able to do
exactly what you saw previously but this
is a pain in the neck why is that it
requires you to actually go out and
photograph this building from all
different angles and then to
painstakingly go and look for the
individual texture maps for each one of
the polygons in the geometry right so
the geometry is what you want to paste
it on so you have to find the texture
map for each piece for each polygon on
the scene so you might want to cut out a
piece from here and a piece from there
and a piece from there but it requires a
lot of work manual labor so to speak
going in there cutting out the pieces
making sure they fit to each one of the
different polygons and
putting it all together into one one
file and then you might have the result
you saw earlier what I'm going to talk
about today is an alternative to this
method of doing things this way of doing
things this is what I call the
traditional way of doing things is
called a single texture scenario because
once I pull out or cut out this piece of
the texture over there and I associated
with the this polygon over here it's
going to live there forever and ever and
it's never going to change that piece of
texture is always going to be displayed
or rendered together with this polygon
no matter which angle basically I look
at the structure from so the alternative
and this is basically the topic of my
talk today is the multi texture scenario
the multi texture scenario means the
following in a nutshell before tell you
the details every polygon or in fact
every pixel in the end of the rendered
image is going to get its texture
content meaningly the color basically
coming from a texture which is
determined on the fly as it's been
rendered and it's not going to be fixed
ahead of time an idea is to choose the
best possible pixel at every given point
in time for that particular rendering of
the image that way we'll get it
hopefully we'll get a better result so
how do we do this in practice and
practice you have your scene for example
this little village over here and you
take a number of photographs from it of
it from a number of different a small
number of angles let's say six seven
eight or ten angles and hopefully you're
more or less covering the scene you see
more or less in those in the union of
those ten photographs you see more less
everything there is to see inside the
scene over here okay so now you have ten
or small number of photographs and if I
want to fly through this I'm going to
have to produce new images from
different viewpoints which are not
necessarily one of those ten photographs
right so the question is how am able to
take those ten photographs and then use
them on the fly together with the
geometry of the scene to be able to
produce a new war as synthase what we
call a novel view of the scene from any
particular viewpoint which does not
necessarily coincide with one of the
photographs
so I'm not the first person to think of
this particular idea and to work on it
there is work and it dates back to
nineteen ninety-six Paul Deb avec at
Berkeley did some work which was
relevant or related to this it at some
point was called image based rendering
although it's not exactly that as I'll
tell you about in a moment and there is
some other work which has been done over
the time these are the main references
the problem with the work done was that
none of them actually demonstrated
real-time performance so basically even
though in principle the idea is the same
idea that i described over here there is
no real interactive a real time system
which allows you to fly through the data
and do the type of stuff that i'm trying
to demonstrate here so that's basically
what i'm going to tell you about now how
to do this type of stuff in and get
real-time performance which is what you
really need if you want to do it
properly in a real application okay so
as I said before the the input is there
some of the input is a number of
photographs of a scene so he has a
village from from Israel and what we're
going to do is we're going to photograph
this village from a number of different
angles he has just one one angle it's
been photographed from the air and we're
going to combine this with a
three-dimensional model of the scene and
produce the new images that I'd like to
have that I'd like to have from any
different angle based on just a small
number of photographs in the 3d model so
one input is the oblique images and the
second input is the actual 3d model 3d
model I mean the geometry of the
buildings inside that scene and the
first thing we have to do is for every
one of our photographs we have to be
able to register it correctly with the
with the register the photograph for the
model so basically that means to be able
to determine where they where the camera
was when it took that photograph them
i'm going to know where the camera was
when it took that photograph in the
coordinate space of the 3d model then i
can actually align it in the way you see
over here and i can get a good fit
between the the 3d model and the
photograph itself so if i wasn't trying
to do what I'm what I am trying to do
and I go back to the previous scenario
of using single textures / polygons what
then might attempt to do is to say well
hey this is this is pretty good let's
just cut out little pieces of this
photograph and stick them onto the
polygons which happen to be visible over
here that would be a single texture we'd
be back to the single texture / polygon
type of scenario and then I could
actually do but I'm not going to do that
what I want to do right now is to align
the photograph there would be a lot of
work what I want to do right now is to
focus to align the photograph to
register it with a 3d model and this
I'll do for each of the different
photographs that have taken 10 or 6 or
small number of photographs I need to do
that because I need to have some sort of
way of determining how the two sources
of data actually correspond what the
connection is between the two so if your
camera has a navigational device on it
meaning it has six degrees of freedom it
tells you where it is in space and way
it's looking as it takes the picture
then doing this is relatively simple
well I just have two more or less figure
out the the calibration between the Duke
the to coordinate spaces once you have
that you're basically done you don't
have to fiddle around and try and match
it to two in a complicated way which is
what you would have to do if you don't
have any information at all about the
car the relationship between the camera
and the the angle from which the sorry
the angle from which the photograph was
taken and the 3d coordinate space in
which you have your model so if your
camera is equipped with the correct
navigational device this is very simple
so I'd like to draw your attention to
this area over here I hope you can see
it from where you're sitting there is a
building there it's actually a mosque
and it has a minaret a little tower all
right it's over here see the minaret
it's this vertical thing over here and
it hasn't been modeled correctly model
correctly meaning i don't have the
geometry of that minaret inside the the
blue the blue lines is the geometry and
somehow when they built this 3d model
they didn't either they didn't know
about they didn't care about the minaret
of the mosque and therefore it's not in
the 3d geometric data set so something
bad is going to happen there that's just
all i wanted to tell you and if you try
and look at the same scene from a
different angle using just the fun
without the 3d model it's going to look
very flat of course because if you have
no 3d model you don't know what's
standing up in the air and what really
is flat and all you can do is basically
just just move this flat thing around in
space and look at it from various angles
this is what you're going to get which
is obviously no good if you use the 3d
model and you look at it from a
different angle you the building
suddenly start to to grow and to appear
the way they should and this is what a
new novel view meaning a view from a
different angle not from the angle in
which the photograph was taken we'll
look if I combine the 3d information
with the texture the texture meaning
that photograph and look at it from a
slightly different angle so basically my
objective is to be able to produce these
types of images all the time on the fly
in real time as I fly through the data
and change the camera the virtual camera
parameters so again this is the
difference between the two this is if
you don't use the 3d you get this flat
plate if you do use the 3d you get get a
real nice something which looks like the
way you expect it to look but even
though even though we do have correct 3d
here we do not have the correct 3d of
that minaret of the mosque so if you
look at the result over here the minaret
is basically gone this is the mosque and
there is no minaret because it wasn't
model correctly didn't didn't feature in
the 3d data set so obviously it's not
going to be here as well but she has
another angle this is what it looks like
this is the same mosque in the red
circle this is what it looks like again
without the 3d information it's
basically just a flat thing looked up
from a different angle and you can see
it the buildings are looking kind of it
look like they're falling over right
whereas if you put in the 3d information
it looks better the building's no longer
look as they were falling all over the
place but the minaret is still not
really where it should be basically it's
look it's it itself was flattened out
instead of standing up vertically in the
air so that's something you have to live
with if you do not have that information
inside your model I just wanted to point
that out something you have to be
careful of that so this is mrs. will
take a little break here and I want to
tell you about something related it's a
little bit amusing entertaining there is
a an artist and a British artist
called Julian beavs who draws these type
of pictures this is sidewalk sidewalk
art meaning this is a picture he's drawn
with chalk on a sidewalk and if you look
at it from a specific angle it looks it
looks pretty real it looks at you have
the correct perspective and it looks as
though it is a true three-dimensional
object so this is one of the examples of
of a drawing that he drew in a on a
sidewalk and if you look at it from this
particular anger that looks like this
woman is inside this pool and her leg is
up in the air and so on in fact the
drawing is that that is the actual
drawing on the sidewalk and if you look
at it from the incorrect angle or from a
different angle the whole thing just
flattens up you don't see anything you
know anything which looks like anything
and basically this is the same type of
artifact that you saw in the previous
examples that I showed where you were
looking at that flat thing from the
wrong angle right so if you don't have
the three-dimensional information there
which is equivalent looking at it here
from the correct angle you're going to
get all sorts of crazy things you can
see this is just an amusing point look
at the length of the leg right over
there looks like or her foot but it
looks like it's very short the correct
proportions of a foot but in fact when
you when he drew it on the sidewalk it's
a very very long thing and it's the
perspective foreshortening which
actually makes it look so don't know why
we have english united states away I
apologize for that this is one example
of his work and here's another one this
is in Scotland somewhere there's a globe
and it looks like there's a guy standing
on top of the globe there in the middle
of the street right and in fact this is
in reality what it is right so it's just
a picture drawn on the sidewalk when you
look at it from an arbitrary angle it
doesn't look like anything special
that's quite flat but when you look at
it from that particular angle you get
the correct perspective and it looks
like the real thing including all the 3d
and including all the special effects
like the guys standing on top of it so
for those of you who don't know about
this guy's work I encourage you to look
for some of it is there's a whole bunch
of these drawings is maybe about 20 that
you can pull off the internet and
they're quite amusing and they all show
this type of effect some of them are
quite sophisticated in terms of the
three dimensionality that it projects to
the viewer from the correct angle
sorry it's just his head and he imagines
how things should look from a very
specific angle he just draws it like
that I don't think he uses any specific
mathematical or tools or machines to
help him he just figures it out in his
head yeah he's pretty pretty talented
and he's his lifetime work he does
things like this okay so back to
business what I want to do in multi
texture mapping is not to use one
specific texture map for every single
for every polygon of the scene and to
cut it out from some photograph and and
and associated with that polygon ahead
of time and then use it during the
rendering I want to do something
completely different i want to just use
a number of photographs and not
necessarily cut out bits and pieces of
the photographs in order to align them
with polygons and in fact i want to get
rid of the whole polygon concept i do
not want to use texture maps / polygons
I want to do the rendering per pixel in
other words when I produce an image as
I'm flying through it I'd like each
individual pixel of that image to get
its color from one of the photographs
and not necessarily the same photograph
at every point in time the the identity
of the photograph from which I'll pull
out the pixel which I'm going to use to
render it is going to change all the
time depending on the viewing parameters
so it's multi texture mappings i'm using
a number of textures simultaneously so
to speak and the specific texture from
which i pull out the relevant text cells
or texture elements is going to depend
on the viewer so basically i can
optimize it on the fly to get the best
possible result depending on a variety
of parameters shall talk about in a
moment the two most important ones being
a visibility because remember that in
not every photograph is going to give me
a coverage of the entire scene a
building might be visible in two of the
ten photographs and not an invisible or
occluded in the other eight so that's
something I have to be careful about and
I want to take the of course I want to
take the texture pixels only from the
photographs which are relevant those in
which the building is visible and on top
of that I want to use among the ones
which are visible I want to use the best
possible photograph or some combination
of the best possible photographs the
with the highest resolution the one with
the best angle which matches the
particular viewing angle that I'm
looking at right now and so on and all
this has to be done in real time so
basically have to make all these
decisions as to which pixel to pull out
of which photograph and actually do the
texturing operation all this has to be
done very very quickly in order to
produce the real time the real time rate
of image generation which I need to be
able to fly through this towards the end
I'll show you a demonstration and you'll
see this actually for real running well
it's actually going to be a movie but it
was taped of the screen and in order to
be able to get real-time performance we
have to use GPU programming because for
two reasons one is the traditional
graphics pipeline allows only texturing
of polygons which is the old way of
doing things if I want to do the
texturing on a per pixel basis I have to
somehow change things in the graphics
pipeline and the way modern graphics
hardware allows you to change things and
to intervene and the graphics pipeline
is using the GPU the GPU allows you to
program using a shader fragment and
vertex shaders and actually get into the
pipeline and do things differently than
what it would be doing if you weren't
helping it or changing it so that's
we're able to do using a vertex and
shaders and fragment shaders and the
shading language that we use is glsl
there's a number of shading languages
out there that you could use this is
probably one of the more popular ones
more modern ones and it's the one we
used so the view of the individual the
particular photograph that we're going
to be using depends on a number of
parameters like I said the most
important parameter is visibility
meaning if a particular pixel or piece
of a building is not visible in some
photograph and of course you're not
going to use that photograph you have to
determine which ones are visible I'll
talk about that in a moment how we
determine that the orientation if you're
looking at it from the correct angle you
want to look at it from an oblique to
too much of an oblique angle we'll see
some examples resolution if you have one
image which happens to be a close-up of
that spot that you're about to render
you might as well use that it's probably
you probably get a better result and if
you use any which was taken from far off
and has very low effective resolution in
that area now there is no it's not
really rocket science there is no real
way of determining
the best thing to do and there's a lot
of heuristics that we use here and we
mix basically these these type of
parameters in order to try and come up
with the best possible best possible
result so basically it's a combination
of these things so here's an example
let's say has there has your building
and you have three photographs one is
taken from view 11 is taken from view to
when it's taken from youth view 3 so we
have snapshots of this building from
these three angles and I'd like to
render it now from this angle over here
the blue the blue era so I've been
traveling around in space with my
virtual camera and I happen to end up at
a position where I'm looking at the
building from this angle over here now
the question is which is the best
photograph from these three given
photographs to use to texture this area
over here so that's a question which
arises so you have three different views
and let me tell you why each one of what
what would be the let me tell you what I
have to say about each individual view
so view number one is probably not going
to be good at all why is that it's not
going to be good because it's secluded
you're probably if you're looking at the
building from this angle you're probably
not even gonna see anything on that wall
because this portion of the building is
getting in the way so that's going to be
basically occluded and invisible it's
not even gonna be present in that
photograph so if you number one is going
to be relevant view number three is
relatively seems to be good because it's
very close to the orientation or the the
camera angle at which I'm looking right
now the blue one so it seems to be is
ready the it would seem that the the
view number three is very close to the
image that I'm going to be generating
right now from the blue angle view
number two actually gives me a something
quite different it's looking at it more
less head on at this area of the wall on
the side over there so actually the the
if we look at view number two we're
going to have a nice clear picture of
that wall whereas if we're looking from
view 3 that was going to be in a very
very very small grazing angle and that's
probably not going to give the best type
of quality for that that area there on
the other hand we're not really
interested in that much quality because
you're looking from this angle as well
so
there is a balance of the different
criteria over here so this is what we
call a view which is best normal meaning
we're looking at from an angle which is
very close to the normal meaning the at
the vector which is perpendicular to
that particular polygon we call this the
closest to I or closest to camera
because it's closest in terms of camera
position to sorry in terms of the eye
view point to what the camera is looking
at right now so theoretically at least
it looks very similar and these are the
type of views which which are going to
be probably the most relevant view
number one is not going to be relevant
at all because like I said we probably
don't see anything there's a penny
anyway so he has what you would has the
the building that I showed you on the
previous slide so if you're looking at
it from this side then that wall down
the side is going to be taking up just a
small number of pixels in the image
there's not much information over there
whereas if you're looking at it from
what we call the best normal you're
looking at it more let's head on and
therefore we have a very nice bunch of
pixels which represents that wall and we
should be able to use it there's more
information in there ok so so what do we
actually do in practice so here are my
six photographs of that village the
village scene that I showed you
previously so I've taken just six and if
you look at it carefully although it
looks right now like a jumble of
buildings but if you look at it
carefully you'll see that they're quite
different angles the first thing that I
do and this is after I've registered
each one of these six photographs with
the 3d model and I remind you I do have
a 3d model like the 3d model is not it's
not necessarily derived from these six
images but it's given from some external
source so I have to have this 3d
polygonal model no matter what I was
going to get those flat things that you
saw before I'm not to be able to
basically do anything anything
interesting so here are my six
photographs I register each one of the
six with that 3d model basically i
overlay it determining the viewpoint
from which this was taken there's let me
just tell you about the scene that's
basically a village square over here up
there as well and there's roads leading
towards this village square and that's
more list where we going to be looking
now once i have a correspondence or once
I've basically registered the 3d model
with a photograph I can then build what
we call a a depth map meaning what
meaning that i rendered a 3d model from
the angle from which each of these six
photographs was taken and i look at the
z-buffer right this is the z-buffer what
you see here is basically a
visualization of the z-buffer meaning
the dark here is a closer to the camera
and the light areas of further away from
the camera and basically it gives you a
depth map of each of these different
photographs that I can get only by
rendering the 3d model if I do not have
that 3d model I cannot render these six
pictures so i render these six pictures
by basically just doing a simple
polygonal rendering and stealing the
zebras are grabbing the z-buffer so I
grab the z-buffer I have six of these
now and I keep them for future reference
future reference meaning that i'm going
to be using them when i'm doing the
rendering so i need to have these the
reason i need to have these is to
determine the occlusion right so to be
able to determine for each individual
pixel in the new image that i'm trying
to generate which of the corresponding
or corresponding more hopefully
corresponding pixels in these six
photographs are actually visible meaning
they actually represent the correct
piece of the surface in space that i'm
looking at ok so this is it this is very
similar to what is done with shadow
mapping for those of you familiar with
the rendering process in which shadows
are generated this there's two there are
two families of algorithms the first
family is what they call shadow maps
which are very simple algorithms and
they're based on this type of concept
meaning build some sort of depth map
from the viewpoint of the delight right
so the shadows cast by a light so
typically the way you you simulate that
in the rendering process is you build a
shadow map from the angle from which the
light is looking and then you use their
to do all sorts of visibility
calculations and then get the shadowing
as you as your rendering the other the
other family of shadow algorithms is
shadowed volumes it's a completely
different technique and
and related to what I'm showing over
here so this is very similar to shadow
mapping those of you familiar with the
technique will see that it's basically
identical and basically the way shadow
mapping is done is by taking the the
point that you're trying to render and
transforming it backwards in the
direction that the the shadow map was
was taken from basically the direction
meaning one of these six directions over
here and once I have that I can then
compare the height in the z-buffer to
the height sorry I compare the height in
the z-buffer to the height in these maps
and by doing a simple comparison I can
then know whether that point is visible
or not so that's typical shadow mapping
and it works here as well and that
enables me to determine the visibility
and that's basically what the slide is
all about I'm sure most of you are
familiar with it so I won't get into the
the gory details so it allows me to
determine per pixel visibility very
efficiently okay so let me tell you a
little bit about GPUs this is a slide
that I stole off the nvidia website GPU
is basically a processor which sits
inside the computer next to the CPU and
allows you to dedicate a lot of
processing power to graphics
computations so it doesn't take up any
of your CPU it works as you can see in
this nice little graphic over here
instead of having just a single CPU
where everything goes through including
the graphics you have a separate
processor for the graphics and all the
green guides which are the graphics
operations go through him and don't clog
up the CPU and that gives you a boost in
performance that's the whole idea of
GPUs and like I said before it allows me
to do two things that allows me to
intercept the graphics pipeline because
I can actually program this GPU whereas
previously in the more traditional
architectures you're not able to
intervene in the graphics pipeline all
you can do is send down textured
polygons and that's the end of the story
you can't really do anything different
from that and it not only allow you to
in to intervene and to intercept the
graphics pipeline it also allows you to
actually program it to do very specific
things that you might want to do and in
our case we're doing what the standard
way of doing it is to is to write things
called shaders shaders a little segments
of code which
tell you how to operate on two different
pieces and later so one piece of data is
the geometric data which you get through
vertices the transformations and stuff
and the second piece of data is related
to the scan conversion or the the image
generation at the end the pixels and
that's what we call a fragment shader so
you can actually build two different
types of little programs one which does
some sort of geometric information and
one which does some sort of
rasterization type of type of operation
and we need actually both of these types
of shaders in order to do what I
described earlier right so we use glsl
this is just to scare you or to convince
you that we're actually writing real
code and this is not just a this is not
just a pretend scene so this is the type
of code that we have in the in the
vertex shader part of it all sorts of
fuzzy familiar with GL a sell some of
this might be familiar you might
recognize some of these type of things
it's pretty standard operations is the
type of code you see in those type of
shade is that's we have an odd verdict
shader in our fragment shader we have a
number of other things I'm not going to
get into any of the details this just to
flash some code at you and here I want
to show you some results so let me let
me tell you what you're seeing over here
basically what you saw previously yeah
we have a three-dimensional model of the
village right the the lines which I'm
not sure you can really see but there's
blue lines yeah overlaid on the
buildings that's the geometry or the the
coordinates and the polygons of the
actual scene and these are three-foot
the same six photographs that were taken
previously and what I'm going to do now
is I'm going to fly around the scene
with a virtual camera so you'll be
seeing the the viewpoint changing all
the time and what I want to do is to be
able to get the correct rendering of the
scene based on this 3d model and these
six reference images where again as I
showed you before I've built a depth map
for each one of them individually I've
store that in the system and that's what
I used to do the actual rendering by
determining which of the six at every
given point in time is visible at every
given pixel and then determining among
those
visible which is the best to use or
which combination of those is the best
to use so so this is it basically now
what what are you seeing over here it's
not running as smoothly as it should
some reason this was taped off the
screen of our computer where we have the
software installed what you see over
here is the result this is the rendered
image at every given point in time as
the camera is moving allow around right
and it looks pretty reasonable now what
you see over here and this is quite
interesting is a color coding of the
pixels we have your six different colors
right so there's red green blue and
other than six different colors over
here and each pixel has colored
according to the one of the six
photographs which contributed to that
pixel right so that's that's the game
we're playing over here and notice that
it changes all the time some changes
more some changes less in this
particular rendering I have three
different renderings which i'm going to
show you in this particular rendering it
changes less why is that because in this
rendering we're choosing the let me run
it again in this particular rendering
we're choosing the photograph in which
to texture a given pixel we're choosing
the photograph which were had the the
normal which was closest which is the
best normal in terms of the the
direction that you're looking at that at
that at that polygon where the pixel
happens to be and this is actually the
closest to static texture mapping rights
if you're trying to think of static
texture mapping as a special case of
multi texture mapping this is actually
the closest you can get to it because
typically when you when you cut out a
piece of a photograph to use as a
texture you usually cut out the piece of
the photograph which is looking straight
on at that particular wall so it has the
best normal view so yeah I haven't done
this offline ahead of time but I'm doing
it on the fly but the effect is very
similar because in the end I'm end up
choosing a very similar pixel as I would
have used had I done
texture-mapping in the traditional way
so you can see that what stopped you can
see that if you look for example over
here you see that the front of this
building is textured using photograph
which is coded by the light blue and
that stays more or less that way all the
time even if I'm moving around the front
of that building is still going to be
textured by the same bunch of pixels
that's because I'm choosing the sorry
that's because I'm choosing or they have
what we call the the normal view now
let's let me run a result of what you
would
now this is using the view which happens
to be sorry per pixel we're using the
the photograph for the view which
happens to be closest to the to the
camera view point the most similar to
the camera view point alright so you can
see that you see these massive changes
mean that we're flipping from almost
flipping from one photograph to another
depending on the viewpoint when we get
without virtual camera close to the
angle from which a particular photograph
was taken then massively the lot of the
scene is textured using that particular
photograph and as I move on to another
viewpoint which happens to be close to
one of the other photographs again we
get that wash of the entire scene using
that photograph but it's not it's not
collective you can see that even though
the mass here or it's dominated by the
blue one there are still patches where
I'm rendering when I'm rendering using
one of the other photographs let me
point out as well you have some black
pieces over here the black piece means
guess what the black piece means you run
it again if you want to know what the
black piece means have a look over here
hope you can see it so for example he
has a black piece down here it
corresponds to this black thing over
here there's a hole in the image that
means that that specific piece of the of
the geometry is not visible from any of
the photographs so basically we have a
complete occlusion a complete black hole
none of the photographs covers that
portion of the of the geometry therefore
we can do whatever we want but we're not
going to be able to get the correct
information so there is a challenge how
to be able to fill this in I'll talk
about that in a moment but you can see
that there are many holes in the data
set so even though we have six
photographs which more or less cover the
data cover the scene it's still not
complete it's not perfect in there still
are pieces which are missing
I don't know why it's jumpy it really
shouldn't be yeah my machine has running
more smoothly I don't know why it's junk
it shouldn't be okay so that was what
happens when we use that particular
criterion and he has a combined version
meaning in this particular rendering
over here we're using a criteria for
choosing the pixels which is a
combination of the normal criteria and
the best view point will be closest view
point criteria so you can see there is
there is things are quite dynamic over
here so the same area of the image for
example this house over here at some
points will be will be textured some
angles will be textured from one
photograph at some angles we textures
from a different photograph sometimes
the same piece of the geometry what we
call previously the same polygon will be
textured half from one photograph and
have from another photograph for all
different combinations of the two so
actually we're not we're not committing
to anything in advance in terms of the
texturing everything has been done on
the fly as we're rendering and hopefully
we're getting the best possible result
because we're taking the best possible
texture pixels and using them at every
given point in time we still have these
holes of here because there are still
areas which are not entirely covered
yeah
he moves around it looks like it just
jumps from one textfree others will be
feasible to like things in the blend you
could you could yes you could yeah
you're right you're saying that over
here it's red and then sunny it's all
going to jump to blue or something like
that right
yeah so you could we're not doing any
blending the reason we're not doing
blending is that sometimes it blurs the
image if you're not if at the
registration is not a hundred percent on
then if you have let's even a one or two
pixel miss between the two photographs
then when you do the blending or
averaging between the two photographs
you might be going to be blurring if you
don't have a hundred percent accurate
registration so in some cases it makes
more sense not to blend the data it's
just sharper that way even though you
might get a slight mismatch and
discontinuities which are not really
visible it's probably more desirable
than having blurred imagery as a result
of not such good registration but if you
have a hundred percent registration you
could try what you learn if you have
registered
okay so those are the type of results
we're getting just wanted to show you a
few static snapshots from those videos
just to point out just to show you to
compare a couple of things over here
this is the this is the real photograph
in other words what you're seeing here
in this particular comparison is I've
taken a new photograph not one of those
six but a new photograph this was
actually from some video real video
sequences those six were taken from so I
basically have an entire sequence which
I chose not to use but I have I chose
just to use six of the photographs but I
have an entire sequence I took one
picture from the sequence which is
basically this one over here this is the
real this is a real photograph from not
one of those six angles and this is what
the rendering technique produced which
is supposed to be as supposedly close to
this one over here so you can judge for
yourself how close it is and how real it
looks it's pretty good there's some
places which are which I've circled
which are worthy of of closer attention
in which there are slight differences
let me point out over here in the real
photograph there's some bricks you see
these over here you should be able to
see some stacks of bricks well they are
not over here no stacks of bricks up
here the reason being is that the 3d
model that I had of the scene did not
have any stacks of bricks in it and
therefore it's basically lost when I do
this synthetic rendering there's all
other types of artifacts that you get
here and there you can see that this
structure in the in the village squares
slightly bent sorry over here it's it's
it's slightly smeared on the on the
ground because some of the registration
was 100% good and there's some some
artifacts also in the 3d model so that's
the those two again is the real
photograph and this is what you get if
you do I rendering meaning you're using
the photograph which is closest to that
and look at the bricks again the bricks
are there
so this is the real photograph with the
bricks and in the synthetic photograph
for the the one that I've generated on
the fly we have stacks of bricks but
they look sort of bent over the reason
that I have those stacks of bricks is
that I'm actually using a photograph
which is a which is quite close to this
one therefore those bricks in there but
since I don't have the real 3d geometry
they're not going to be rendered quite
correctly and they're basically swaying
over and this is just a comparison of
the the two synthetic images there's no
real one over here basically you've seen
these before but now they just signed by
side so there's various artifacts that
you have in one and you don't have any
other and so on and so forth you can see
for yourself so what our conclusions so
far this is work in progress where we're
still working on improving it the
rendering quality is definitely improved
relative to the static or simple texture
mapping the traditional way of doing
things it's certainly more convenient
you don't have to actually sit down and
put in each polygon each scene polygon
go in there and cut out from some
photograph a texture map and do the
texture coordinates UV mapping which
sometimes is quite a pain it's very
time-consuming so it's much easier to do
the only thing that you really have to
do properly is that registration of the
3d with the photograph if you're doing
that manually meaning you don't have any
real data from your camera and
navigational data that might be a little
bit painful but hopefully when you're
out there doing this for real with a
real camera you will have navigational
information we need six degrees of
freedom and then it's very very easy
you're able to get real-time performance
thanks to using the GPU properly one
thing we did come up against and I think
this is more of a technical issue we're
working on resolving it right now is the
number of texture maps that we can use
right now using a simple relatively
simple rendering software is limited
we're using up to eight photographs
eight eight of those pictures like you
saw and that's that's because of some
technical issues we're having with
programming this stuff but I don't think
it's a real limitation was in terms of
actual texture memory you can pack it
much more than that so that's the
limitation we have right now which I
think will go away as we proceed and
something which i think is is
certainly it's important and I think
it's also quite possible if you're using
this type of rendering technique versus
the traditional texture mapping is being
able to fill in those black holes right
so there's always going to be a couple
of pixels where you're missing data for
because these pickles will happen happen
to not be covered in any of the
photographs and we're looking into ways
of filling in that missing texture
information using something called in
painting and painting means taking the
information that you have around it and
somehow interpolating or extrapolating
it inside that black hole and I think
that using this this particular way of
rendering will give will make this
easier than just regular texture mapping
right so is this really practical not
really practical why is that because
this still is one main problem here and
that is the the geometry we still have
to model very precisely all those
polygons of all those houses in that
village but photographs easier to get
you just go out there with your camera
snap snap snap record the viewing
parameters and you're in business so you
have the photographs easy to acquire but
how do you acquire a good 3d model of
the knesset or of that village or of any
other urban scene right to actually
generate a good quality polygonal model
requires a lot of work and that's
basically the major the main bottleneck
in any modeling process and even in this
as well so how do you make that go away
well using this technique if you're
doing texture mapping the way that I
described it here as opposed to the
traditional polygonal texture mapping
single texture mapping thing you could
probably make that go away we haven't
done that that's something we're trying
to we're trying to think about right now
the idea being instead of using polygons
you'd use what we call point clouds and
point clouds are very easy to obtain
because there's no real structure to
them there's no ages of polygons which
all fit together properly point clouds
are something that you can obtain using
your your scanning device you have these
large industrial grade scanners and you
can go out with them into the streets
you basically pointed at the buildings
and you you you shoot laser beams and it
comes back with points in space and that
way you acquire very large quantities of
points in space and you don't really
know or care where these points belong
to does it belong to this polygon does
it belong to that polygon doesn't belong
to the roof does it belong to the wall
just a bunch of points in space all you
have to do is then align your
photographs with these bunches of points
in space and you can use exactly the
same rendering technique that I
described you have to render that data
set so this this rendering technique is
amenable to a much easier way of
modeling the 3d geometry and I think
that'll be probably be the future way of
doing things because that's it's very
easily done in practice by just taking
out the scanning device and basically
pointing and shooting so here you see a
couple of examples these are not my
examples it comes off the site over here
this is a company which does this for a
living it's called 3d laser mapping com
basically what you see here is point
clouds or colored point clouds of these
two scenes here at some some castle I
guess in England and here it's a couple
of houses from a village so there's no
polygons over here it's basically just
points which have been colored using
some sort of texturing technique and you
can see it looks a bit crude because the
rendering technique is not very not very
good what they've done over here there's
no real real surfaces in terms of
polygons and so on is just bunches of
points all over the place so it looks
kind of pointy doesn't really look very
clean it's very noisy but I think this
is the way to go if you do the rendering
properly then the fact that you have
points which is easy to acquire as
opposed to polygons which are not very
easy to acquire and to set up and I
think life will be much simpler and will
be very well be relatively painless to
be able to do these type of to enable
these type of applications so that's a
future type of direction we haven't done
anything about it right now we're
working on polygonal data which we get
basically from people who do that for a
living as well and there's companies out
there which which produce polygonal 3d
data and they spend a lot of they expend
a lot of effort into doing this and it's
a very expensive process as a result
this would be much cheaper and much more
effective and much more much easier
basically so basically that's what I
have to to tell you if you need more
more details you can go to this website
and there's some stuff over there thank
you
more questions
have you thought about using stereo
techniques to get the geometry you've
got a lot of use their rights so the
question is that we thought of using
stereo techniques to acquire the
geometry that's exactly how the geometry
is acquired people who require the the
3d polygonal data on a commercial scale
to actually do that they take they take
pairs of photographs stereo pairs and
they have a special machine called a
stereo something stereo plot I think and
people sit there and look at it and to
sort of turn these dials and the point
moves up and down in some virtual space
and that way they actually measure the
heights of points and then they can
actually they have software which which
supports all this and they're able to
use that to to actually construct the
the polygonal forms of the data set so
it isn't automatic at all there are of
course you probably know that in
computer vision people are trying to do
stereo sorry geometry extraction using
stereo automatically all the time with
very limited success so far so first of
all takes a lot a lot of time computing
time I mean and the results are so so
they're not very good because there is a
big issue they have correspondence we
need to have a stereo pair you need to
be able to tell which pixel here matches
which pixel they're basically that gives
you afterwards the the height so with a
manual process it's a person who
actually does the matching and therefore
that problem goes away at the at the
expense of of more human time involved
so it's not automatic but the results
are better so stereo sort of stereo is
what is used to produce those three
dimensional data sets yes
for example if your light source changes
give a car burning down the street
that's room
rosemary handle Mariah so we haven't
really done much about dynamic see so
the question was what happens if the
scene is dynamic if cause and moving
people to walk cause of driving and
people are walking and that one
photograph is taken in one at one point
in time another another photograph is
taking another point in time you might
have a car the same car in two different
photographs at different positions
because in the meantime it's moved from
one place to another and in fact you do
in this example over here oops you're
not going to see it because they're too
small but there are two photographs over
here in which you have the same car in
two different positions because the car
had driven and in one photograph was
taken one point in time and then later
on at a different point in time and also
different point in space because it had
driven so you'll see the same car in two
different positions we haven't dealt
with that problem at all so what I've
described over here is certainly good
for static scenes for dynamic scenes
it's a whole different ballgame but
those type of things can have can and
will happen yes
the political model is used for
generating the so the question was
what is the polygonal model used for why
do we actually need it why is it so
important now it's used for two things
it's used for generating the shadow maps
which then allows me to do the the
determined to determine the occlusions
but it's also used for the actual
perspective computations as we're
rendering
I think we're done thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>