<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AQC 2016 - An Optimal Stopping Approach for Benchmarking Probabilistic Optimizers | Coder Coacher - Coaching Coders</title><meta content="AQC 2016 - An Optimal Stopping Approach for Benchmarking Probabilistic Optimizers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AQC 2016 - An Optimal Stopping Approach for Benchmarking Probabilistic Optimizers</b></h2><h5 class="post__date">2016-10-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fyEi8osUmV4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">first of all let me thank the organizers
for give me the chance to give this talk
and all the people who decided to stay
in this room I'm going to talk about
this work done in collaboration with
Danny leader which is about applying
optimal stopping ideas to resist the
benchmarking of randomized optimization
algorithm now do we need to revise this
process I don't have to convince you
about the importance of doing bench mark
ii will allow you need to pick our best
solver and we also rely need to try to
detect quantum speed-up for example but
today you will have heard already about
at least a couple of ways of addressing
the performance of our optimization
algorithm it usually what people do and
we can be happy about that is compute
time to solution but the helmet cuts
greg was talking about doing a fair
sampling so already you heard this first
day about a different way so doing
benchmark in and maybe tomorrow you'll
learn about to do in time to target
measures so my first important point is
that there is not a unique way of doing
benchmarking but whenever you choose
your strategy this should be always been
very clearly motivated by a practical or
theoretical treason and what is our
practical motivation or practical
situation yeah we put ourselves in a
situation where all solutions that we
get from a solver are acceptable and of
course we want better solutions but we
have to also deal with our computational
length and we deal with a situation
where the competition length is is not
fixed okay so I think we understand we
somehow there's a way to go beyond time
to solution in time to target measures
and this is because this is a intuitive
trade-off between time and energy of
course the more the computational last
the higher are the chances that I'm
going to find a better solution a better
approximation of the ground state so
if we have this energy time trade off
can we find an optimal balance and to
try to formalize this question i would
like to define a total cost which is a
function of time and this is the sum of
bests energy which is the best energy
found at time T and T of course is my
computational length and then a function
on a cost function that measures the
computational effort and think of this
as what you have to pay for keep your
competition going your energy bills so
you can put into this function the price
you had to pay for your hardware these
kind of things and now the question is
can we minimize the distorted cause by
choosing a time to stop the computation
now this looks like a hard problem and
in this indeed it's a very hard problem
in general but we can actually make
progress if we have if you make a
sumption and the assumption is that
simply we deal with randomized
optimization Alchemist and pour us a
randomized up this optimization
algorithm plea a black box well that's
not really a black box that's more like
a black box oh that draws random samples
from an intrinsic distribution okay and
this intrinsic distribution will depend
on the problem instance on the
parameters that I used to set up the
solver but that's the money assumption
the sober is a basically a sampler of
random energies okay of a sampler of
energies and assume that once I have
fixed my my my instance and my
parameters the solver I can run it many
times and it will the compression will
last children which is basically fixed
now most soldiers are easily modeled by
by this assumption classical heuristic
like simulated annealing quantum Monte
Carlo genetical donates include parallel
temperance and basically all the
arguments
that you have seen so far they can be
modeled by Eddie's and also quantum
optimizers whether they are at the
buttock or quantum annealer so whether
it's an algorithm and on the gate model
because it's a pond to Margaret because
you have some randomness in the
measurement process you can think of any
quantum optimizers as a randomized
optimization are going so it's a very
general assumption if you do this we see
that our total cost becomes discretized
in steps of tehran and now we would like
to define my total cost like this some
of the best energy that i found that
step and i assume here i draw n samples
i run my solvers n times and then a
little slightly smaller additional
assumption my total cost now is a
function linear in time okay if I do
that I can actually make even further
progress but you see here I have a I
have an hour sequential decision problem
that I have I can either stop at step n
and accept en as my best energy or pay a
price city-run and I draw in another
sample and minimizing this cost function
is equivalent to find an optimist option
rule that tells you where I should stop
my competition actually the field of
sequential statistical analysis provides
a theoretical framework to tackle this
optimal stopping problem and actually
what I have defined here is a problem
that is known as the house selling
problem and actually I can solve this
problem a little this problem has been
solved analytically and the solution is
made of two steps one the first step is
there is the principle of optimality and
it tells me that if I know the optimal
cost which is the cost they minimize
total cost then I have to stop as soon
as I found an energy that does that is a
smaller or equal done as soon as I found
us an energy that is as large as the
optimal cost this is simple and the hard
part is to compute this up
our coast and this is the second part of
the solution I had to solve the
optimality equation which is an integral
equation for this optimal cost and
distance that the optimal cost depends
on the probability distribution that is
property of my soul where of course was
my instance and the cost per unit of
time C so it's a function of C 2 which
is now for me as a parameter now an
important point for me and for us is
that the contribution of the tail of P
of the lower tail of P is negligible for
if the values of C star is a large
enough so I can actually compute sister
even if my even if I don't know they the
ground state so I can apply this event
to do benchmark even when the ground
state is not known and actually that the
optimal cost is basically an energy
target this is because the sequence of a
sort of the observations is stopped as
soon as I found an energy that is small
enough and in this case I can split the
optimal cost as an optimal energy which
tells me what is the optimal quality of
the function that I'm getting plus an
optimal computational effort which is
basically proportional to that time to
obtain an energy that is below this
target ok so this optimal coach is
really an energy target but by defining
the my target has an optimal cost I
achieved the optimal balance between the
quality of minor the energy or the
quality of the solutions that i can get
and the computational effort given by
this function and i would like to stress
again that our optimal cost plays the
crucial role in our benchmarking process
to solvers can perform liquid well if
they if they achieve the same optimal
cost but they may do so with a different
balance between energy and time and I
just want to say that the optimal cost
actually
you include other common measures a for
example when this when the cost of my
computation given by the main parameter
C goes to zero it's optimal to stop when
I found the ground state so the optimal
course is basically equivalent to time
to solution this optimal cost is
proportional to the time to find the
ground state so it this limited reduces
to a time to solution in the opposite
limiter might cost my D effort that my
computation is so expensive that it's
optimal to stop after just drawing one
sample so you see in this opposite limit
a total cost reduce to a quantity that
may be of interest which is the average
energy of them by the solver is just the
same thing so actually we dunno premiere
appropriate choice of my cost function
TN the optimal cost can be reduced to
other more or commonly used benchmarking
measures and what I want to say is that
from this point of view optimal stopping
and optimal cost provide a flexible and
general framework for benchmarking
randomized optimization algorithm in the
second part of my talk I would like to
illustrate these ideas by doing some
numerical experiment so we have
generated range three instances with
planted solutions if you don't know what
these things desert doesn't matter you
can read this paper but it's not
important for for this talk but we have
generated 100 instance where pesticides
on on the Chimera connectivity of daily
the way to X device that it's a
installer QSC we have done that for
several sizes this is the sides of the
Chimera connectivity and we have
considered a bunch of solvers simulated
annealing hf s and the way to x okay so
so let's stop with simulated annealing
and so let's pick one instance on the
largest size we run simulated annealing
10 to the five times and we build awara
and we do this as
benchmarking exercise to learn the
energy distribution of that is
characteristic of my simulated annealing
solver and if you do this for two
different values of the and number of
spin updates the blue line is for 10 to
the 5 spin updates the red one is for 5
times 10 to the 5 spin update is of
course that if I increase the number of
spin updates the energy distribution is
pushed towards a smaller better values
of the energies this is intuitive of
course but what is optimal what is the
optimal choice for this solver now in
terms of optimal cost the answer is
shown on the figure on the right this is
the optimal cost for the corresponding
energy histograms and so you see that
the answer depends on the value of the
constant C which in this graph is a cost
per micro seconds okay and so you see
that when C is more its optimal to to do
more updates while when C is large the
computation is expensive so expenses
that it's optimal to run similar than in
with a fewer number of updates so the
answer actually of how should I optimize
a simulated annealing depends on the
value of my cost and see okay even on
the same instance i'm using the same
solver and you can see the effect of
this optimization problems if i plot the
optimal cost in blue the optimal energy
red and the optimal computational effort
in yellow the bold lines are object when
SI SE optimized the number of spin
update is optimized and the dashed line
is with milady nearly run at a fixed
number of spin updates and so you see
that when c is more it's by running
stimmell dealing with a larger number of
spin updates i can get better solutions
than what i can if i fix the number of
spin update but on a hard on the other
hand if my cost is very large
dsh optimizing the optimal course is to
avoid a blow-up of the computational
effort I have to keep the computation of
net control but I pay the price of
getting a solution with a higher energy
of wars quality so this energy time
trade-off it's even more obviously shown
in these figures is the same instance
simulated annealing the number of spin
updates is optimized as a function of C
and here you see see is a level you see
is co defined in the color of the dots
here on the y-axis I have the time to
stop the optimal time to stop my
competition and on the x-axis I have the
optimal energy of the solutions that I
get so you see that when sea grows dim
receive when C is very very small it's
like a red dot it's optimal to find the
ground state so I'm adhere at zero and
on the y-axis here I basically have a
time to solution whence this larger have
to give up in finding the ground state
but I have the advantage of keeping of
reuse of reducing the computational time
so and this energy time trip trade-off
is what I need to do to optimize my
total cost okay so now i would like to
compare optimal cost on different
solvers and i have averaged the optimal
cost over 100 distances for the largest
size in blue it's simulated annealing in
red is HS and yellow is d way to X and
the comparison here there is a wall
clock time this is a world clock time
comparison so this is done by using
these numbers here which is a typical
timing for standard CPUs on a single
core so you see that again the choice
for example of your best solver
depends again on C so for example you
see that when C is more HFS is slightly
better than simulated annealing so for
the longer computation on when the cost
to compute this is a small I would
prefer HFS but then for large you see
actually I should prefer simulated
annealing and the web seats here which
seems to be better on this wall clock
time comparison for most of values of of
C but D way to X is not optimized there
kneeling time of other ways is not
optimized well it's optimized instance
that it's always optimal to run the wave
at its shortest a million time five
microseconds but for exam you see that
at the very end similarly kneeling is
better than the wave just because
computing is too expensive that I should
try to make this computation and length
shorter but that's the minimum value so
again the main point here is that in
terms of optimal cost the choice of the
soul well depends on what's my guess for
the constancy which defines how much
effort and how much how expensive is to
do my competition and the last slide is
a scaling study just to illustrate how
scale in a work with optimal cost so now
i have here similar annealing i have the
averaged optimal cost for 100 instances
for the side and it is actually i didn't
see before but is actually the optimal
cost minus the ground state just to show
just to have a better just to be able to
plot this on our logo but it's just a
constant so you see that when the sides
increase the distance between the
optimal cost and the ground state
increases this is intuitive it tells me
that the problems are getting harder and
then it's optimal to stop at a larger a
larger distances from the ground state
and so I can see how this optimal course
is scale with the problem side which is
what I'm plotting here on the right for
the three solvers it recovers and for
two different values of the constancy
these three lines are for a small value
of the coffee and district for a large
value of the cosi so you see that for
very small C as I said before that the
optimal cost is basically equivalent to
computing time to solution and so you
see here an exponential scale as you
expect now look at large see here I'm
not looking for the ground state because
it's too expensive for me to to hope to
to find a ram state but the optimal cost
is minimized and actually the scaling
here is a disabled this feet are a
linear fit so you see actually that
experimentally see this kind of scale in
and actually the separation between the
exponential scale in and the linear
scaling is given by the value of C where
C is equal to the first excited state so
as soon as the optimal cause is smaller
than the first existed i basically
looking for the ground state and this
has scales exponentially when they
thought the optimal cost is equal to 2
the ground state to the first existed
I'm starting to give up to look for the
ground state I have to optimize my costs
differently by accepting war solutions
and from this point going on at the
scaling is linear okay and this what i
want to say i have time two minutes for
the 20 or 40 15 ok yeah so let me just
say something that may be interesting
em2 minutes for dating ok so i will
conclude so we propose an approach to
benchmarking that is based on optimal
cost and we put this into an
optimist open framework and we show that
optimal stopping provides a flexible and
rigorous frame of framework for
benchmarking and we have shown how to
use optimal cost in practice to optimize
and compare different optimization
algorithm on a more technical side we
have seen that optimal cost have this
exponential + polynomial scaling and
although the scaling the texture before
our special of the kind of business data
a the square root square root here and
the linear scale in here is special of
this type of instances but an
exponential + polynomial scaling is what
you expect in general so so yeah that's
all questions for Walter
skimming there's a range three random
yes these are so we have here 100
instances by problem sites runs three
instances generated on the Chimera graph
yes and yeah and if I understood you
correctly the bottom curves on the right
are finding ground states on the right
edge so so you see here there is a there
is a small sea region where C has a
linear dependence on see this is the
regime where I'm funding the ground
state okay it's optimal to find the
ground state and it's basically when C
is below the first excited state which
in my unit is like about one or two is a
few units okay so so this is minus the
ground state the first excesses will be
around here so you see that below this
sea grows linearly that means that it
basically does is telling me that I'm
looking for the unfunny in the ground
state so this time to solution x the
constant C that's why there is a linear
increase but then there is a after this
linear linear dependence there is a sub
sub linear regime which is when I am
actually giving up to find a ground
state okay here its optimal to stop
before i found the ground state so i
give up and I have to give up to keep
the computational effort under under
control otherwise you can imagine this
you know disk because of this
exponential scale in the computational
effort grows exponentially but the on
the right hand side yeah the bottom
curve is finding optimist yeah so these
three lines okay this is only si si okay
all right take
so with some of these constraint
satisfaction problem there is hardened
its results not just on the problem but
on the approximation ratio this does
your work factor in some of those ideas
are they aligned at somewhere no so of
course this I think the fact that
eventually you get a polynomial scaling
is related to to this the fact that you
have you may have polynomial a
polynomial time approximation algorithm
this approach doesn't to use those ideas
at all and actually this is from a
different perspective it tells you more
in a way it tells you which so for
example sometimes people have this time
to target measure where you fix the
quality of your solution to be a few
percent of the ground state okay now
that you can think of it has been a
arbitrary decision where should be one
percent or two percent okay so what you
do with this optimal causes that these
optimality equations if you tell you
exactly what's that percentage you have
to aim for okay so so by balancing the
your target energy and you know your
computational cost you know what's your
target energy and when you have to stop
so it actually tells you these two
things you know thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>