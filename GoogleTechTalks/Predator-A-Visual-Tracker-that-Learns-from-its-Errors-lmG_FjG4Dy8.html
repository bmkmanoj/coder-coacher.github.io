<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Predator: A Visual Tracker that Learns from its Errors | Coder Coacher - Coaching Coders</title><meta content="Predator: A Visual Tracker that Learns from its Errors - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Predator: A Visual Tracker that Learns from its Errors</b></h2><h5 class="post__date">2011-05-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lmG_FjG4Dy8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you thank you and I would like to
start with question to the audience how
many of you have seen this video on
YouTube so maybe 60% so this video is
the reason I'm here I basically show the
result of my PhD thesis there which is
basically the red dot which you can see
there and currently there are over
almost half a million people who watch
this video I have received quite a lot
of emails asking for source code asking
for a different kind of cooperation
offering job and I try to basically
solve these emails based on these labels
so you can see sort of the structure of
the emails there and first I would like
to show you why why this invention
happened I would like to show you a
small demo of the code so I will switch
the screens so here I'm just running my
club and so basically the input to the
algorithm is a single bounding box which
I selected here and after that the
algorithm is is tracking the motion of
the object so when I when I start moving
its following my eye but at the same
time it's learning how my eye looks like
so these are these patches on the right
side these represent what is my eye and
patches on the left side represent what
is not my eye and using this information
the algorithm basically is able to
detect the eye when I come back so when
I when I try different object such as
these fingers you can you can do very
similar thing
now the algorithm is learning these
three fingers how they look and
basically it's it's trying to learn all
the appearances how it appeared and then
you can consider it as a virtual mouse
or something similar you can apply it to
face and you can have something like a
face recognizer so you have lot of
flexibility there we tried to combine
all these things tracking learning and
detection into single package and which
is running in real time so it's useful
it's useful for number of fields such as
human computer interfaces robot
navigation and over surveillance so now
I will switch back so I would like to in
this in this talk I would like to show
you small journey how how I came to the
algorithm and it will basically start in
my master thesis where I was studying
control theory and basically control
theory is a field which is controlling
dynamical systems such as a helicopter
or car or the whole industrial plant and
we were we are studying this on on
simple toy examples such as this one so
in this case this is very common example
in control theory it's called a ball and
a beam and your goal is to control this
system in order to put the ball always
into the middle you can see that every
time the ball is away the system is
reacting on this change and stabilizing
the ball in order to get it into the
middle another example of the same
category is the ball on the plate you
just do it in two dimensions so here you
can see that you throw a ball and the
system is able to balance it and it's
able to put it always into the center
and when I first saw these systems I was
excited about it because it's it's agile
its interactive so you can play with
that and
the importance which which which we were
supposed to learn is that these systems
are intrinsically unstable so if you
have a plate only a small tilt of the
plate will put the ball completely away
it will fall so you need to stabilize
the system somehow you need to have the
system able to put it into the center
and the question is how to do it and
this is basically what we were learning
during control theory and the answer is
feedback feedback was introduced by
Norbert Wiener
in his book cybernetics and basically
feedback is very general concept which
can be applied machines but also life
organisms and the idea is the following
you first tell the system what you want
to achieve you have some reference value
so in the case of the ball on a beam you
know that you want to go with the ball
into the center so you know where you
want to be your preference then you
somehow measure the output of the system
you measure where the ball is in fact
and you feed this information back into
the floor into the enter into the input
to the system so we have some sensor
which is measuring and you make the
difference and you get an error and
based on error you basically control the
system in order to be stable so this
idea of feedback is at the core of
computer off of control theory and
without it Plains wouldn't fly for
example so the question is how computer
vision came to the picture and if I come
back to the example of the ball on a
plate you can measure the output of the
system by a camera for example which is
measuring where the ball is coming gives
you something like that and if you know
the reference value you the task is only
to estimate where the ball is which is
in this case very easy it's just a white
spot on a black background not big desk
so you measure the output until measure
the error
so in the sense computer vision is
something like a sensor and many people
understand it this way so measuring the
white blob on the black background is
not a big deal so people started doing
more complicated things such as face
detection in this case you want to
localize all the faces in the image and
it's used for a number of applications
so basically the algorithm which which
is doing this out of this this function
is working with sliding window so you
get an image you start in top left
corner you test in this sub window is a
face or not and you slide it you slide
it up to the end then for different
scales you do the same things and
eventually you find the face or couple
of faces and but in this case you need
to have the model of the face already
it's not so easy as before you don't
have just the white blob you need to
encode the whole information of the face
so just to compare it here the idea of
the model was very simple maybe color
histogram would be enough and you
described all the appearances of this
object here however for this object it's
not trivial people cannot devise or
design rules in advance which which tell
how the face looks like so there are
millions of parameters which describe a
face so we cannot do it in advance by by
hand so we use machine learning to do it
we call a lot of examples of a face we
align them we call a lot of examples
what is not on face and we then run some
machine learning algorithm in order to
get a model or classifier which is able
to describe the features of the face we
discontinued from the known faces and
this this learning typically takes days
or it's it's basically quite long
process so I was working on this problem
during my master master thesis with
jorge montes and we were working on
features how to describe the face how to
synthesize the face and how to do
learning more effectively and I see the
price price of the Dean and this
motivated me to go for
PhD later so in the first year of PhD I
started working with Christine
Mikolajczyk at the University of Surrey
and we were basically continuing in this
line we were working on learning of
object detectors in particular on
learning or from large databases so we
realized that our two main approaches
how to how to learn detector one is
boosting where you learn the decision
boundary between places and known faces
based on weighting of examples
second is bootstrapping which is doing
similar thing but based on sampling of
examples and we managed to combine these
two things into boosting which is
sampling by weighted sampling and we
achieved slightly better performance
than state-of-the-art then we applied it
to the problem of detection of faces and
multiple poses detection of faces in
infrared images which are these little
bit darker images you can apply it to
different problems such as recognition
of a brand of a car or detection of
license plates and so on but in
conclusion from from the detection it's
still considered as a sensor so it's a
system which gets an input and returns
output and can be done very fast it's on
mobile phones as we know but when you
want to build a system you need to
process large data sets of training data
and this takes time and therefore you
cannot apply to any objects which we
would like to do so after the first year
I basically wanted to apply the object
detection into arbitrary objects and get
rid of the offline learning so this was
basically the situation where I was
testing my ideas quite often basically I
wanted to track the fingers here and I
want to give the system only the
information about the fingers in a
simple frame and then without any
offline training be able to detect these
fingers anywhere so the idea was
basically simplify and speed up the
learning so I didn't have the training
data but I could generate the data from
the
first frame consider all the plus
patches as positive all the rest as
negative and trained very fast
classifiers in every frame so this is
basically adaptive tracking and if you
do this if you update your classifier in
every frame you can you can do something
like this so you can't wrap the fingers
you are learning at the same time how
the finger location or appearance
changes but eventually you will fail
because at some point you will make an
error and because of the adaptation you
will just think it's correct and you
will never recover so the message is
adaptive tracking will fail sooner or
later and we were you're thinking how
how we can do with how you can tackle
this problem and the idea which I got at
the at the point was to recognize the
tracking failures and update the
classifier only if we are tracking
correctly which I can say now it's not
the best thing to do but it was
basically what we wanted to do so we
wanted to recognize certain failures so
we simplified the problem
we didn't track by a classifier we
tracked by something very standard
lucas-kanade a tracker and the question
in this slide is which of these points
is tracked correctly so you can see
point number one is tracked correctly
because we know that this is the same
fetch but number two get occluded in the
second image and it's on something which
looks similar locally but is different
and the question is how we can recognize
this automatically so standard way how
to do it is to describe the points by
patches and compare the patches if the
patches are similar we will say ok we
are tracking correctly if they are
different we say we which art
incorrectly the problem of this approach
is that we typically already optimize
this measure by the tracker so this
information doesn't give up give us much
information
you can you can see it here if you
compare this page and this page it's
very similar and you cannot get much so
there's different information which you
can use and that's forward backward
error which is commonly used in computer
vision you basically track forward and
then you reverse the time and it
backward and what you get is the
difference in these trajectories and if
the descent the first is big we say we
got lost and it turns out that this is
much more reliable in order to detect
tracking failures it's a perfect but
it's it's giving us more information so
we extended this into multiple frame
you're tracking for key frames forward
difference backward which which turns
out to be a little bit better and I
would like to convince you on this
example if you have for example 50
frames sequence of 30 frames you
initialize all these points at the
beginning then you track them forward
and then backward what you get is this
error map basically every pixel here
represents how reliably you are able to
track corresponding point in the first
image for 50 frames and you can see that
there are some spots here basically red
color means that we are able to track it
reliably and notice that these red spots
in the circle correspond to the Paris
trains and if I play the video you can
see that the pedestrians basically at
the top of them it's fairly static so
therefore you can track it reliably on
the other hand the lower part of the
pedestrian the legs are not so static
you cannot read it so reliably and this
is by the way camera motion because this
literally will disappeared and this the
see are included so this is something
which is telling us how reliably we can
track parts of of the object and two we
went back to checking of objects and the
idea was to use this information in
order to do better tracking of bounding
boxes so we basically take a bounding
box we put all the points on it we track
them from frame to frame
then estimate their error and we filter
out 50% of outliers so we are left with
something with this more reliable and
then we estimate the motion of the
bounding box based on the reliable
points so this is good idea because we
are doing better estimation and I would
like to show you a video where I compare
tracker which is red which is blue here
it tracks based on 50% of the most
reliable points and the one which is
behind it it's it's red the tracks based
on all of the points so you can see that
the red tracker basically drifts a
little bit faster because of these
occlusions all this information is done
in a way so by focusing on 50% you are
able to track longer but sooner or later
as before if the occlusion is Blanc in a
plot such as here it will fail as well
so this is this is not not enough to to
track forever basically
and so to conclude from tracking we know
that recognition of tracking fillers can
help the tracking itself so we can track
longer but every tracker eventually
fails and the idea is that you would
like to run a detector which which will
be able to help the tracker in order to
find the object later on and this is
quite common people do it but in in
addition we would like to learn this
detector online so we would like to have
detector which is able to accommodate
all the information which have been
observed so far so this is basically
what we want to achieve you want to have
one example which is selected by the
user and then we want to have a video
stream where where the same object is
moving somehow we don't know much about
that but we would like to use this
information in order to train detector
and this problem is quite difficult if
you compare it to offline training of
detectors we
millions of training examples these
examples of labels and it takes a lot of
time to train it in this case we wanted
to have a single example of the object
then you want to have no labels from the
video and we want to be real time so
it's related program but quite different
so in order to to tackle this problem we
cannot use the same source of a in
training of detectors as before you have
to change it somehow
so traditionally error in the runtime of
the detector is considered as something
bad which should be avoided and people
people do a lot of training and a lot of
feature design in order to avoid the
error in runtime that's the standard set
up but in our case we we consider the
error as something what will happen we
accept the failure will happen and we
are trying to learn from these errors we
are trying to identify them and learn
from them in order to be better so this
is the idea of the algorithm so at the
beginning we we train the classifier
from the single example you can imagine
that we select the box you rotate it a
little bit do some I find warps or
Gaussian noise and so on then we train a
classifier which we have at the
beginning the better we do it the better
we we basically get but this is not the
priority we have some classifier and
later on we want to have this loop we
would like for every frame evaluate the
classifier another frame then we want to
estimate its errors which is quite
difficult but we will come to it and
then using these errors update the
classifier so basically the more errors
we identify the better because if I
should get and the question is how we
can estimate the errors so put it into
contrast with what we had before in this
example when we were estimating the
location of the ball we know the
reference therefore we can estimate its
error and this error can be feed back
into the system in our case we want some
do something similar we also want to
estimate the error and
to feed it back to the system but we
have no reference here we have no idea
where the motorbike is and the
difference is that this error is sort of
continuous and here we have discrete
errors we have classification of the
patches which seem quiet just to make it
perfectly clear if the classifier gives
responses like this one as a human I can
tell that because we are detecting mock
motorbike these two our force our alarms
and this one if it wasn't detected its
miss detection so we would like to have
a system or a feedback which is able to
give us this information and in order to
identify it we cannot rely just on one
image if we have just one image we
cannot get this information but what is
happening in in a video is something
more we have more information which we
can exploit in fact we are first
scanning by the sliding window every
frame so this is this grid basically
each point corresponds to the window and
then we have the video volume so they
are stuck into into this space and the
action classification of the patches is
repeated repeated all the time and this
is something what we are using because
if the classifier gives this kind of
responses we know that there is
something wrong we know that the object
cannot appear on all locations here and
there is no time continuity so there is
something wrong
but if we get this kind of response we
just say okay maybe it's correct so we
can somehow identify what is correct and
incorrect by considering the responses
to the classifier in bigger picture and
this is how we can put it into the
relation to control theory so we have
some reference here which is which is
the behavior of the system which we want
we have the response of the classifier
which we get and based on comparing
these two somehow by the feedback we are
able to identify error and update our
classifier so this is the idea of the
learning there so how to realize the
feedback
first we would like to simplify the
problem we don't want to identify all
the errors at the same time so we split
it into two parts the first part is what
we call P expert and P expert is
something what is giving us positive
examples which is able to identify
missed detections second is an expert
and an expert is giving us negative
examples it's able to identify false
positives so just to give examples of
this we we are using tracking during the
detection so tracker can give us more
and more positive examples so this is
for us the P expert that's why there is
some time continuity indicated by the
line and negative expert is using
different information spatial structure
basically we know that the object can
appear at one location only and
therefore if it appears on more
locations we can classify the one as
positive and all the rest as negative so
these are simple rules which will not be
correct all the time because traffic
tracker will fail or we will identify
the top of the pit incorrectly so this
will fail but there are simple rules
which are describing what is in the
structure so if you are applying these
rules into into the learning process
basically you start with the motorbike
which is a dot here in feature space
then you have a classifier which which
is covering a little bit more space
during the learning and by P expert you
are expanding this space and by an
expert which is independent you are
cutting pieces of this space so there
are two independent forces which are
shaping the manifold of the classifier
in order to put it to some optimum
manifold which we didn't know before so
now I would like to show you a model so
we wanted to analyze the system and we
wanted to know what are the properties
of such a thing so we devised a model
where the experts are modeled by two
numbers such as a classifier so we model
it by
precision and recall which is which is
the ability to estimate miss detections
and then we have an expert which is
characterized by the ability to estimate
false detections again by precision
recall so we are making this quite big
assumptions that we are able to model
this experts by two numbers each but if
you do it we can you can write a nice
formulation here we we characterize the
classifier performance by two numbers
alpha is number of false negatives betas
number of false positives basically
after each iteration of the learning
this number gets transformed to the next
state by this matrix which is function
of the experts and so this is in fact
dynamical system and the dynamical
system is characterized by this matrix M
and there now we can use the whole
theory of control theory in order to
find properties when when the system is
stable and stable for us means when the
errors of the classifier converge to
zero and the answer is simple we just
take the matrix M we get eigenvalues
alpha 1 lambda 1 lambda 2 and if both of
them are smaller than 1 the error of the
system converges to 0 which is this
situation if one of them or both of them
are bigger than 1 the errors basically
grow so we have some understanding how
how these experts should be combined in
order to to make the learning stable so
we have tested this idea on number of
sequences which you can see here
basically ten sequences containing
various objects and they are fairly long
some of them we trained in the first
frame the initial detector by affine
warping of the patch and then we ran
this this the learning and we got at the
end the final detector
and in this case we are comparing the
performance by precision recall and half
measure basically you want to have
numbers here close to one so you can see
here see quite substantial difference in
the in the learning so the learning is
doing something here we are also showing
that the statistics average statistics
of the P and an expert all the time so
these are some numbers which don't make
much sense here just looking at them but
if you compute the eigen eigen values of
the corresponding matrix you you can see
that all of them are smaller than 1
which which is supporting the fact that
the final detector got better so to
conclude the learning part we observe
that by defining how the system should
behave we can estimate its own errors
and this allowed us to learn from these
errors so we are not defining the system
itself but its behavior and second we
trained a reasonable object detector by
a single example and the video sequence
so this can be done in real time so
these are two things which we learned
there and then we basically had all the
components which we needed for the
entire system we had tracking which was
the median flow tracker we had detection
or the whole scanning window detector
machinery and then we have the learning
which is able to train this guy here in
real time so we combined them and this
is basically the flow in our algorithm
we are tracking the object and detecting
at the same time we start from the
single patch and then tracker is giving
us some fragments of the trajectory the
vector is giving us detections these two
are integrated you select at every point
which one is correct and using the rules
from learning we are able to identify
some errors and improve the detector
and once once the object disappears
detector is still remembering in the
entire information and it's able to
initialize the tracker so the idea is
that the longer you are tracking the
more you are learning and the better the
system is getting so essentially you are
learning a model here which is at the
beginning we were when we were an
offline stage we needed to change this
model by processing tons of training
examples here we give to the system the
single example then we have the
classifier and the P expert which is for
us the tracker is giving us more
examples and at the same time there is
the N expert which is giving us the
negative examples and this is
essentially for us the morning which you
have seen in the demo at the beginning
those these patches which were growing
there so we have these examples which
you can represent as a dot here and you
can do the simplest thing possible which
is nearest neighbor classifier this is
what we have done so at the top the
detector is realized as a single nearest
neighbor classifier but if you do this
you will not be fast so you just need to
make it faster therefore the classifier
the nearest neighbor classifier is stuck
into into the sequence of classifiers
which essentially represent the same
thing but these two stages are just
designed to be faster so patch variance
is simply measuring the variance of the
box if the variance is slow such as on
the sky here it will be immediately
rejected so we don't spend much time
there if the variance is high we go to
an sambar classifier which is doing some
simple pixel comparisons in order to
find the object and the answer no
classifier will reject another space of
the patches and the remaining ones which
is typically around 50 goes to the
nearest neighbor classifier and it makes
decision so here are some so basically
that concludes how the system works and
now I would like to show you a couple of
examples for example this motorbike
so you can see that it's able to track
it even though if the motorbike changes
appearance and jumps out of the video
sequence the same is for panda here we
are using the symmetry of the pandas
when we are training detector we are
able to train if you see it from one
side we can immediately see it from
second side as well four faces is the
same thing or for cars where they are
similar objects so this demo I presented
at cvpr at the demo it was running about
eight hours there then we are running a
demo similar demo in in University of
Surrey where there is a screen we
combine this the system with the face
detector which is initializing the
tracked object and then it's able to
learn and distinguish other people so
you can you can have some funny funny
situations there I like this one the
black black face at the top it's
apparently somebody who came there in
the dark and the guard here he is
repeating there every day actually so
recently I found the video that some
guys took this idea and implemented into
it into drone so the idea here is to
navigate
automatically this flying robot which
has a camera at the bottom so they they
just are able to select the object as I
showed before then the system is
learning this object and use this
information in order to navigate a robot
so I I quite like this
okay so current development the code of
the of the algorithm has been released
and the GPL license and the
corresponding discussion group has I
don't think Sanders member members now
so it's it's quite it's growing and at
the same time we are trying to sell
licenses of the code and it's an ideal
case I would like to find some
industrial partner who is able to use
this on the idea in industrial
application so to conclude the whole
talk
I found that vision is missing feedback
and I think it's a good idea to use it
thank you
for security I think it's a little bit
tricky because there is like 40 years of
research in physics recognition yeah so
I'm I don't have ambition to to have a
system which is better than these kind
of security applications but you can
think of security which which cannot be
trained advanced maybe you have you want
to have a bench of network of cameras in
one camera you want to select an object
you want to learn its appearance and
detected somewhere else and you don't
know in advance what kind of object it
is so for this kind of applications well
you cannot train perfectly this detector
it's useful so security is one thing
human computer interfaces you for
example if you if you consider Kinect
it's it's a sensor which is designed in
advance it doesn't change it properties
so therefore you cannot learn by that by
by this kind of system you you have the
ambition to to learn gestures for
example you would like to have human
specialized gestures my gesture like
this is different from yours and it
should distinguish it
so basically human-computer interfaces
which which can be learned for specific
persons navigation of what's another
field as I showed in the end of drone
this approach which I have presented is
designed just for formal object and if
you would like to run multiple you can
run it multiple threads for example but
they will not know about each other so
if you would like to have a system which
knows that it's tracking multiple
objects and you use this information and
learning this has to be researched
basically but I believe there is there
is scope for improvement
I can maybe show you something what we
have done recently
it's basically checking simplify this
system simplified into really fast
algorithm where we are tracking here 30
by 30 points each of the point is
independent they don't communicate into
each other and each of the point is
learning its appearance which we cannot
do in advance because these patches
under it have deformation which which
are not affine so you cannot you can
chain it in advance properly but using
using this idea to train and detect and
learn you can do something like this so
here it's just tracking basically but
because of the direction you are able to
find this points back which you can see
again so basically the idea which I
present it can be simplified into
something quite fast this was running
something like two frames per second on
standard laptop which I have here so it
can be paralyzed and I believe if you if
you do it on GPU can do it real-time
without problems so as you can see it
there is a makeup on the face to make it
simpler it doesn't work that well on
bare skin but we believe it's it's
interesting way to go so yeah
yeah okay that's cool
I don't know
yeah tracker for me tracker is the thing
which I represented here this this idea
your tracking point by Lucas Canada you
filter out all flyers and then you
estimate the bombing books location and
so this is running in parallel with the
scanning window if I want to track
something I need to make decision where
the object is so I'm making this
decision in every frame I essentially go
for the maximally confident patch
it really depends on the sequence so it
wouldn't work for example if you select
the object if I select my oval here then
I disappear and appear in a different
frame like this there is no way how to
connect it by this approach so you need
to basically it can in in practice
detect everything what you have observed
before and which you tracked correctly
if you if you don't track this thing you
cannot detect it later on if it's from
front sighting in theory you can train
detector which can detect this without
problems no no it can be anywhere that's
the idea of the detector because
detector doesn't use any time coherence
it can detect it anywhere and tracker is
purely relying on the temporal
information so we don't we didn't want
to do something between many people do
tracker detector which are sort of
combination of this we said no we want
to have tracker and detector doing the
same doing completely different things
motion motion of the camera is not a
problem maybe it wasn't what is what is
problem well okay if you if you have out
of plane rotation is in general problem
for this approach just because of this
tracker because this tracker is simply
following the points and if they
disappear the tracker will drift away so
if something is rotating
that's a principals problem of obvious
approach so it can learn a lot of things
up to maybe 90% out of plane rotation
but then then it will
fail yeah but that doesn't mean that
approach cannot work for out of my
irritation because simple thing of
different trackers which are able to
handle that they can be maybe based on
color and but we don't have any static
camera assumption you can you can take
the camera and move it so maybe you
later I can show them which I haven't
done that no that's something
interesting to research because once you
have this information may be calibrated
cameras as well so there is something
much there is much more information
which you can use so if you have let's
say two calibrated cameras you know that
if you find something in here there is
some restriction where you can find it
in second camera and this can be used in
order to feedback the system some more
information so there's definitely
something interesting
yes
in this approach which I presented it
equal all of them at the same weight so
basically they have no weights and but
it makes sense to to weigh them and this
is what we use actually in the in the
later-later approach where I was
tracking a number of points so it makes
sense to weigh them well no I wouldn't
say that it's it's hard if you put more
weight on recent samples you will drift
more physically so you need to have
something more clever behind it but in
the algorithm which I presented a there
is no weight so
okay so some sort of higher level
feedback oh I hope so but I don't know
it really really in this case it's about
adding new and examples so if you have
some other information from the way how
you draw which is feeding sort of
different patches why not I don't know
how to realize it but in general
whatever information you have you can
use it to to feedback that into the
system but the feedback is so it must
classify the error in this case error
means that the classifier made incorrect
classification</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>