<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Supersymmetry, Extra Dimensions and the Origin of Mass | Coder Coacher - Coaching Coders</title><meta content="Supersymmetry, Extra Dimensions and the Origin of Mass - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Supersymmetry, Extra Dimensions and the Origin of Mass</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bSEv4vF5CTs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and it's my great privilege to introduce
professor Mark Shapiro from Berkeley she
is a chair of the physics department at
Berkeley and she also does her research
in high energy physics at Lawrence
Berkeley Laboratory and formula and CERN
and today she's going to tell us more
about highness the latest trends in
high-energy physics and its unique
requirement in terms of computing so
here is it professor Malaysia Pierre oh
thanks thanks very much i really am
pleased to be here and i'd like to thank
jay and matt for inviting me i feel a
little presumptuous giving a talk that
has anything to do with computing here
and about the only thing i can say in my
defense is that in the old days particle
physics really was one of the drivers
for high-performance computing I'm
afraid that really isn't true anymore so
I'm actually hoping that during the Q&amp;amp;A
I'll learn some stuff from you where you
can say why the hell are you doing it
this way there are much better ways that
we could show you how to do the
computing that that we want to do so the
title of my talk is supersymmetry extra
dimensions in the origin of mass
exploring the nature of the universe
using petascale data analysis and what I
really want to do today is I want to
spend some time explaining to you why
those of us who are working at the Large
Hadron Collider are so excited about the
possibilities for the next few years
what we might see and what the relevance
is to problems of interest to anybody
who cares about science and then since
this is google i really have to say
something about computing so i'll try
and emphasize some of the aspects of our
experiment that make it challenging from
the computing point of view in
particular the fact that we have very
high data rates in an online environment
that we collect very large quantities of
data that's going to be looked at by
people all over the world so this is a
small enough room please if you have
questions just yell it out in the middle
i'm much happier giving an interactive
talk then then having you guys just sit
there like sticks so don't feel
embarrassed about interrupting me in the
middle so let me just start by talking
about something that really isn't
particle
physics but it's becoming more and more
in common with particle physics and
that's astrophysics when I was a
graduate student particle physics and
astrophysics to a totally separate
fields they had almost nothing to do
with each other and in fact those of us
who were doing particle physics used to
laugh about astrophysicists because we
always said that the uncertainties there
were in the exponent and in the old days
that's really the way it worked and one
of the amazing things is that's changed
so much in the past 10 or 15 years at
this point you can really use the
universe as a laboratory and there are
lots of amazing experiments out there
that are trying to understand about the
fundamental forces and the nature of
matter by doing experiments where they
use the universe as a laboratory it's an
era of very high precision astrophysics
observations with very large amounts of
data and very precise measurements
there's lots of discussion about what
the systematic uncertainties are and you
can see the quality of some of this data
by these images that really look back
into early time and tell you what the
universe looked like so why is that
relevant for this talk well I wanted to
point out that things happen in the
other direction as well we can use the
universe as a laboratory to tell us
about interactions that are important on
earth but we can also use the laboratory
to tell us about things that matter in
terms of the universe and when I talk
about laboratory that really can have a
bunch of different meanings there are a
number of non accelerator experiments
for example at the top here I've shown
an image from super-kamiokande which is
an underground experiment that uses
water tank off detectors to look at
neutrinos and at the bottom I've shown
an experiment from just down the road at
slac that looks at matter-antimatter
asymmetry using the babar detector so
both accelerator and non accelerator
based experiments can really tell us a
lot of things that are important for the
fundamental interactions that matter for
the universe now in fact if you want to
describe the universe you need to
describe it in terms of the fundamental
forces and interaction or interactions
and the fundamental particles and it
turns out which are the relevant forces
and interactions are function of what
time in the universe you're talking
about the reason for that is the
universe has gone undergone a number of
phase transitions
so the particles that are relevant
depend on what the temperature of the
universe is which depends on what the
age of the universe is so if you're
talking about the very early universe
you have to talk about super strings go
a little bit later and a little bit
later means 10 to the minus 35 seconds
so it's still pretty early that's called
the grand unified era then there was a
period of inflation followed by a period
which is where the particles that we
know now begin to to form the
electroweak era it's in this electroweak
era where particles gain their masses
and then as you can see you go on to
longer periods of time where galaxy
formation begins and the universe as we
know it is so if we want to have a
theory of cosmology that goes as far
back to the beginning as possible we
need to understand the particle physics
that goes into it because that's what's
going to determine whether we can model
the evolution of the universe and in
particular this electroweak era is one
that's extremely important for the
experiments that I'm going to talk about
today and it's possible by doing precise
measurements we can also learn something
about the grand unified area as well
okay so the next generation of
experiments plays a very special role in
this and the reason is because of the
energy of those experiments I pointed on
the previous slide that we had a 10 to
the minus 10 seconds was the electroweak
period and in fact the next generation
of experiments has enough energy that it
can produce interactions that occur
comparable to what happened during that
electroweak gara so it's a chance for us
really to look back in time and
understand what those interactions look
like in detail so the higher energy
reproduces the conditions of the early
universe and in particular by looking at
our theories we know something has to
happen to these energies if you look at
the models that we have now of how
particle physics works if you assume
that there's nothing except the
particles we've already seen it turns
out that the equations that govern them
that give us very good predictions or
what's happening start to break down
once you get to high energy and in fact
they break down so badly that they
produce infinities they produce cross
sections or measurements predicting
rates that are unphysical so we all know
something
has to come into the theory that's going
to cancel that breakdown in what's
called the standard model the
description of particle physics that we
know that now there's a single extra
particle called the Higgs boson that
gets rid of those infinities and makes
them very well behaved all the way back
to the beginning of the universe we
don't know that that's the right theory
but independent of whether that one is
right or not something has to happen
when we get to the tev energy scale
because if nothing happened then our
equations would break down and so at the
very least we have to find that our
predictions aren't going to work at
those energies so in fact there are lots
of theories that explain what happens at
the tev energy scale but theories are
great the real question is can you
confront them with data and you can you
tell which ones are correct or not so
the real goal of this next generation
generation of accelerator experiments is
to try and distinguish between the many
possible models of high-energy
collisions that are relevant for the
early universe and decide which ones are
correct and which ones are beautiful but
really your only fantasies so what are
the things we might find when we look at
these high-energy collisions well within
the standard model the mass of all
particles is actually not just a
god-given thing it's something that's
generated dynamically and it's generated
by the interaction of a specific
particle with all the other particles
that specific particle is called the
Higgs and it's the absorption and
radiation of these Hicks's that give all
of our other particles their mass so if
that mount if that model is correct we
ought to be able to see that Higgs boson
and in fact it's predicted to have a
mass in the range that would be
accessible but these new experiments the
second thing we might find is a new
symmetry of nature there's a model
called supersymmetry where every
particle we know about has a partner and
and those partners we haven't seen
because they're too heavy one of the
interesting things about this theory is
not only would it give us employment for
the next 30 years for particle
physicists because we have a whole raft
of new particles to measure it also
would explain something that's really
important to the astrophysicists
astrophysics is always talked about dark
matter we know from gravitational
measurements that there's something out
in the universe that we can't see
telescopes but we don't know what that
is in super symmetric models the
lightest supersymmetric particle is
stable and it interacts very weakly with
other things so it's a very good
candidate for dark matter so if we saw
this in an accelerator the next question
people would ask is to the properties of
what we see in the accelerator explain
what we would see for the dark matter of
the universe and finally another wacky
idea but one that's really being touted
by a lot of a famous theorists is the
idea of extra space-time dimensions the
string theorists believe there are 10 or
11 dimensions we only have four three
space and one time to mention in the
world that we know and what they argue
is the other dimensions you're just
really tiny so we don't see them and
there's a number of models that some of
those space-time dimensions actually are
big enough that when we get to high
energies such as what you will have at
the LHC we would actually see those
extra dimensions that would be
absolutely revolutionary if that was
found and in fact one of the
consequences it would we would make
gravity become strong on the scale of
the LHC and you would have to talk about
quantum gravity which is something that
most of us have always thought that you
wouldn't be able to ever experimentally
address for probably hundreds of years
and again these are only some of the
possibilities whenever you reach a new
energy regime you don't know what you're
going to find and the important thing is
to keep your mind open to look at as
many models as possible and to make sure
you design your experiments and your
analysis of the data so that no matter
what's there you have a chance of seeing
it okay so what's our next machine it's
called the lad Large Hadron Collider LHC
for short its energy is 14 TeV its
proton protons colliding so there's two
rings they go around in opposite
directions and they bash into each other
okay the energy of this machine is seven
times the highest energy currently
available right now the highest energy
is to TV that's at the Fermilab Collider
in Illinois the intensity is very high
factor five higher than the current
intensity at the tevatron in the first
year and going up an order of magnitude
more than that after three years so what
I've shown
on the right hand side here oops but
I've shown on the right hand side here
is a picture of the tunnel that the
magnets for the LHC go into so when
since we're accelerating protons what we
do is we inject them we have big magnets
that force them around in a circle and
they collide with the experiments are
now what's the scale of this experiment
is pretty amazing it's actually being
built in Geneva Switzerland because
there's an existing tunnel from an
earlier accelerator and the civil
construction it turns out is one of the
major costs of building a machine like
this so so using an existing tunnel was
a major plus the tunnel is right on the
Swiss French border near Geneva and in
fact half of the machine is in France
half the machine is in Switzerland and
the circumference is 24 kilometres so
you can see a picture drawn wear the
ring goes with the towns and the fields
all around it the machine itself is
underground because it uses the earth to
shield the people in the area from any
radiation that you would get from the
accelerator other was ok so what are the
challenges of working at the LHC well
first of all high energy collisions
require very complex detectors because
the energy of the particles produce a
very high you need a really big detector
in order to capture all that energy so
that happens in two ways some particles
you cap you measure their energy in
what's called a calorimeter you do that
by stopping the particles and seeing how
much energy the deposit if something's
got really high energy you need a very
deep thing to capture it the other thing
is with charged particles you measure
their momentum by having them curve in a
magnetic field the higher their momentum
is the less they curve in the magnetic
field and so you need a big lever arm to
get good enough measurements to measure
their momentum so both of those things
need really big detectors the second
thing is in these high-energy collisions
lots of particles come out typically one
or two hundred and if you want to
distinguish one or two hundred particles
you need very fine segmentation in order
to be able to do the measurements so
these large detectors have very large
numbers of electronic channels and so
the data rates are quite high the second
challenge is the process as we care
about in particular the Higgs particle I
just told you about
are very rare so in order to see a very
rare process you have to produce lots of
collisions because only one in a very
small number of them produces the
process you care about so we need very
high intensity beams now the problem is
something gets produced every time you
collide these guys so even though the
stuff we care about is very rare there's
lots of other stuff happening so we have
to try and pick out the few events we
care about from a bunch of debris that a
real interactions but are just sort of
boring things that that were not very
interested in now we always talk about
that being the needle in a haystack
problem but recently there was an
article in The New Yorker and they said
that they really thought to the right
way to describe it wasn't a needle in a
haystack it was a needle in a needle
Factory and I think that's a really good
comment because the events we care about
don't look all that different from the
ones we don't so one of the real
challenging from the analysis point of
view is to be able to tell what are the
ones we care about make sure we keep the
ones we care about while throwing out
the vast majority of the ones that we
don't okay so in terms of Detectives for
the LHC in fact there are four detectors
there are two large general-purpose
detectors that are trying to do the kind
of physics that I was talking about
before their goals are similar the
design trade-offs are one chooses one
thing one chooses the other I would say
it's like Google and Yahoo and yeah I'm
on atlas so I think Atlas is google but
you guys can can choose yourself there
are two other detectors that have very
special purposes one is designed to
study be decays the same sort of thing
they do at Stanford the other is
designed to look at heavy ion collisions
there's going to be one month of heavy
ion running per year at the LHC I'm only
going to concentrate on Atlas because
that's my experiment and so it's the one
I know the most about most of what I'm
saying would be pretty true for CMS
although the actual details may differ
the overview really isn't that different
so the first thing is these experiments
are big so on the left hand side what
I've done is I've taken a drawing of the
Atlas detector which I'll tell you a
little bit more about later and I've
super and posted on a drawing of a
five-story office building where we have
our office
it's earned and so that that sets the
sense of scale it's a five-story stall
detector so it's pretty phenomenally
large compared to the detectives we've
known in the past it's also way
underground and so the pieces are built
on the surface they're brought down in a
shaft and are assembled here and that
assembly processes in the middle right
now and in fact if you go to the Atlas
webpage I don't have to give you the
address you can just google it you'll
find we have a webcam in the collision
hall and you can watch in real time
what's happening as the experiment is
being put together okay second thing is
Atlas is a complex detector it has lots
of different people again to set the
scale we have our canonical person over
here there's another one over there so
you can see it's it's really quite a
large detector the detector is built
like an onion with different pieces of
it that serve different functions if you
look in the horizontal direction this is
where the beam goes so there's a proton
beam going this way a proton beam going
that day and they collide right in the
middle of the detector things on the
inside are to detect charged particles
they're inside a magnetic field and that
magnetic field is a solenoidal magnet
that looks kind of like a coke can and
that's what those red lines are there
then outside the magnet there's what's
called calorimeters they measure the
energy of the particles by stopping them
and seeing how much is deposited and all
the stuff on the outside or to detect
one species of particles called muons
muons are kind of like electrons only
heavier and one of their features is
they go through a lot of material
without interacting so if you look at
what comes at the end then what you'll
get is a muon yep there's four collision
points each one has its own Hall and the
beam is forced to collide in four
different places in fact one of them at
least runs at a different time because
it's only heavy ions but these are just
too massive to move in and out so so
each one is is is separately built on
the
be mine and in fact we have a problem
when we're running we can't service in
the detector because when the beams on
it's a high radiation area so while it's
not as bad as being in space it has some
of the similar problems that the access
is quite limited during running okay so
an experiment of this size needs a big
team and what's shown on the right is a
picture that was taken at one of our
collaboration meetings of people in our
office building who are members in the
experiment and it's hard to see all
their faces but typically we have a
group of about 2,000 physicists and
engineers and because of the complexity
of these detectors the engineers really
are full collaborators in terms of
deciding how the detector is going to
operate you can see that we have a
worldwide collaboration the areas where
we have collaborators are in yellow
about the only continent that's really
not covered is Africa and we don't do
too well in South Asia because India is
on the CMS experiment not on Atlas but
it's it's a real challenge to find a
meeting time that works for everyone
it's pretty much impossible and
unfortunately the poor japanese tend to
get screwed because you can find a time
that works for the US and for europe but
doesn't work for a for asia at the same
time ok so how does this experiment work
as I said we have this detector and what
we do is we record the particles that
are produced in the collision we record
the time the location the momentum the
energy and the charge of the particles
and this onion of detectors allows us to
tell the properties of those particles
in terms of those things and we infer
the characteristics of the interaction
from those properties we use highly
specialized custom electronics and dead
acquisition systems that's a sort of
messy picture on the left of what the
data acquisition system looks like in
one of our test beams and on the writer
pictures of some of the custom
electronics we use and almost all of our
electronics is custom asics that are
designed
by engineers and are produced solely for
us just because they need to be very
fast they need to be very radiation hard
and they don't really have too many
industry applications we do use the
semiconductor industry for doing our
production runs of the electronics and
for some of our detector elements so we
work with the same companies that
produce chips for computers although we
send them our own masks and designs for
how to produce that electronics okay so
here's a schematic view of how how the
detection works right at the middle
again is where the collision occurs
after that we have a bunch of what are
called tracking detectors energy is
deposited through ionization and that's
detected on individual elements in that
detector the charged particles curve
because this thing here is a magnetic
field and so they curve in that magnetic
field after they exit the tracking
detectors they enter what's called the
calorimeter and that calorimeter is
divided into a front section that's
optimized for finding photons and
electrons a back set segment that that's
optimized for finding charged particles
there aren't photons and electrons those
are called hadrons and then a this big
section on the back for looking for
neutrinos there are some particles that
don't interact at all neutrinos have
such a small interaction rate that they
escape without us seeing them we can
infer their existence because when we
look at the energy in the event we see
an imbalance so if we see a big bunch of
energy on one side with nothing
recalling against it that tells us
something must have escaped without
being measured and so in fact we also
have sensitivity to the neutrinos by
saying it's minus whatever else we we
didn't find that of course makes an
assumption you can't tell a neutrino
from another non interacting particle
and that's an issue for models like
super symmetry which also have heavier
non-interacting particles the signature
for us would look the same so one of the
issues that we have that's different
from what you guys have is that we have
to work in real time
the collisions of the LHC happen every
25 nanoseconds and there's no way that
you can record every 25 nanoseconds and
even if you could you wouldn't be able
to handle the amount of data that's
produced something happens every
crossing so you can't say well I'm only
going to record when something happens
what happens may not be very interesting
but something always happens so what we
need to do is we need to find some way
of deciding what the interesting events
are in real time and selecting those
events and throwing out all the others
so that's what our trigger system does
now because it has to work at high rate
and because the decisions are difficult
we divide the trigger system into three
levels the reason is that the first
level has the first level has to take
the full rate of the LHC every 25
nanoseconds so it has to accept every
event look at it very very quickly and
say yes or no I think this might be
interesting okay that's done with
special purpose hardware now you might
say how can you make a decision in 25
nanoseconds we don't we have a very long
pipeline so we keep all of the the event
in memory while we're while we're
deciding and the length of the pipeline
tells us how many events we can pass
through the system while we're making
the decision but in the end yep I'll
show you that in a few minutes it's big
because there's lots of channels but I
should comment that even here as part of
the readout we do do zero suppression
because there's so many channels in the
detector that if we read every one out
we have a real problem even even after
the trigger so i'll show the triggers
the rates in a couple of minutes okay
then the second level trigger unlike
with specialized electronics or the
first level is just pc based it's it's
running fast algorithms but its standard
C++ code the thing that makes it special
is the fact that it still doesn't see
the whole event what it does is it looks
at everything in a road sort of a wedge
around each of the trigger objects from
the first level trigger and it tries to
make a fast decision based on the other
information in that road and finally in
the third level which is also pc-based
you build the whole event you see the
whole event and you can make more
complicated decisions where you where
you basically have everything about the
event in order just as hide and you can
correlate information I see this on this
side of the detector that on that side
of the detector so the rates are the
level one trigger text the 40 megahertz
from the machine accepts 100 kilohertz
level two takes 100 kilohertz accepts
three kilohertz level 3 has an accepted
of about 200 Hertz and that 200 Hertz is
actually cached on disk then written to
magnetic tape for archiving and sent
offline for further processing okay so
just to show you this in more detail
schematically in the level of one
trigger the kind of information you can
use is you can say do I have a muon if I
have a bunch of hits in a muon system
that line up to form a stub or a track
that would be called a trigger there's a
magnetic field here so you can make a
momentum cut by saying it has to bend
less than a certain amount because the
more it bends to lower the momentum and
we have a calorimeter trigger that looks
for clumps of energy in contiguous cells
and gives you a trigger based on that
there are additional triggers that do
things like say take one every 10,000
event that's called freescale triggers
the one out of ten thousand if you just
say take one over 10,000 events with no
with no other requirement we call that a
minimum biased trigger so by taking
these loose additional triggers without
a lot of requirements we can try and
understand what kind of bias we're
taking in the data that we collect by
comparing it to an unbiased sample now
of course there's something is very rare
you'll never see it in the unbiased
sample so in fact what we do is with all
of these triggers we build them up by
having pre scaled one every n events at
low momentum so you take one every 100
events at six Jeff one on every ten
events attend Jeff and every event above
20 Jeff that way you can use the lower
energy pre scaled events to understand
the efficiency for the higher energy
okay so then you take these triggers
from level
one we typically run with a menu of on
order 50 or 60 different triggers in
parallel because unlike an astrophysics
experiment where you point the telescope
somewhere else and that's that guy's
data we all take our data together and
we divide up the band were within the
online trigger so that everybody's
triggers are mixed together while
they're being taken that makes you less
sensitive to systematics and it also
means if the machine breaks you don't
have somebody who has no data at all
everyone suffers the pain at the same
time and for rare processes it means
that you get them for the whole time so
we take the data here we have data
that's added to the DEQ system that says
what triggered and what were the
characteristics of those triggers and
you pass it on to the level to region of
region of interest trigger and again
that looks at slices around each of
those trigger objects and it can ask
more complicated questions so for
example for this muon it can look in the
tracking detector and say did I find a
track we don't have a level one track
trigger because at the moment we don't
have a hardware processor that can
quickly enough look for patterns of hits
to find tracks a lot of people are
talking about the possibility of adding
a track trigger as an upgrade to the
experiment it would really help our
capabilities but it's hard to have one
that works with a high enough bandwidth
for what we needed the LHC okay then the
level 3 trigger puts together the whole
event instead of just these regions of
interest it has it has everything and
the code rerun in level 3 looks pretty
much like the code we run offline but
with looser selection selections are
less sensitive to whether we have the
final alignment whether we've calibrated
our detector purpose perfectly but
there's a lot of movement of code that
gets developed in the offline and gets
moved into the level 3 trigger once it's
proved to be robust needless to say this
has to be very robust code since it's
running in real time it's run on a Linux
form and it scales pretty well because
basically you just keep adding boxes as
long as you have enough network
bandwidth okay so how about the offline
reconstruction well once we've passed
through level 3 we put it on tape that's
for safety's purpose we try and avoid
actually having to go to tape as much as
possible because tape is a nightmare
it's not a technology that's ever been
made to scale very well but
unfortunately it's the only really cheap
and safe way to scale to take lots of
mounts of data and then we perform a
common processing for the whole
collaboration so I would view this as
similar to what you guys do when you do
your pre caching of important
information here at Google you know
people are going to make a lot of
queries and you don't know what queries
are going to make but you know that if
you have to answer all those queries in
real time you're never going to make it
so what happens is we do a general set
of feature extraction calibration
pattern recognition and we write those
results out and those become the
starting point for the queries that all
of the scientists are going to do now
because the data volumes are so large we
have a hierarchy of data storage and
this is one of the areas that I would
say it's not clear how well it's going
to work this hierarchy is based on the
assumption that most people will be able
to work with the higher levels of the
hierarchy very few people are going to
have to go back to the earlier stages
because we just can't afford to handle
every collaborator going back Andrea
redoing all of the raw data we don't
have the CPU we don't have the bandwidth
so the raw data which we call byte
stream because it's just a bunch of ones
and zeros or archived raw data es d
stands for event summary data this is
the result of the reconstruction along
with calibrated hits so if you have the
esd's you can pretty much do most of the
reconstruction over again it doesn't
have individual cells and the detector
it's clumped together neighboring cells
to try and save some space and do the
calibrations but it's pretty close to
read data plus all the results of the
reconstruction then analysis object data
is a summary of those things that are in
the SD that you would actually do to
talk about physics so it has tracks
electrons muons missing energy the basic
concepts of physicists use and then
there's a very tiny thing called a tag
which is just a summary of high-level
objects it's really meant to just do
fast queries so you can navigate through
the other data quickly the tag is
something that I would say the jury's
still out we don't understand how useful
it's going to be the idea behind the tag
is you can say give me all the events
with three electrons and missing et more
than 20 and then when you navigate
through the data you don't have to look
at the events that don't meet the query
unfortunately the main issue for us is
getting the files that hole those data
delivered to the user and so these kind
of queries are very effective if the
fraction of events are keeping a small
compared to 1 event profile but how
useful it actually is when you have on
average more than one event profile
isn't so clear so I would say it's an
open question how useful the tags are
going to be ok so so here's the answer
to Matt's question about sizes a raw
data is 1.6 megabytes per event so these
are these are big events lots of
channels and that's just store it as a
as packed byte streams with 32 bits per
channel and an identifier that's usually
32 bits that gives you the channel
number since we suppress zeros the esds
are about half a megabyte and these are
target sizes i should say right now
we're close to a factor of two off on
the ESD size and it's a big issue
because we don't have enough disk space
if we don't get that factor or two down
and it's going to cause real problems if
we have to go back to tape to read them
ard is 100 kilobytes and the tags are
about one kilobyte we have lots of
simulated data because the only way you
can understand a complicated detector
like that like this is by simulating it
and the simulated data is a bit bigger
because we keep the truth information as
well as the reconstructed information
the time for reconstruction in some
units at least to me are random or 15k
5112 k seconds per event and
significantly larger for the simulation
and again we operate about 200 days a
year and the event rate after the
trigger is 200 Hertz so we typically
take two times ten to the ninth events
per year so we're talking about
petascale data samples and
large CPU usage as well okay so how do
physicists work with this data well I've
already told you the bulk reconstruction
is done once for everybody we probably
won't get it right the first time so our
computing model says that we reconstruct
all of the data once every year that
means you need more CPU as you've
collected more data since you go back
many years worth but Moore's law pretty
much helps you with that since typically
you only keep your machines about three
years and the process data is a starting
point for the analysis in order to try
and make data access more efficient we
stream the data according to the trigger
and physics channel so we put all the
electrons in one set of files all the
muon triggers and another set of files
all the missing ET triggers and another
set of files and right now our default
is an inclusive streaming model which
means the same event appears on more
than one file if it satisfies more than
one trigger and this is basically just
physical placement of data to make it
more efficient to access it we
distribute the data to multiple sites
and then we have infrastructure which is
quite complicated and it's probably the
most fragile part of the system to allow
distributed analysis and data mining
okay so here's a picture that says what
I've just said the data itself from the
detector would be a petabyte per second
if you read out every event which of
course you can't do we read out about
100 megabytes per second through our
data acquisition system send them to a
Linux farm at CERN then the results of
that get sent out to various what are
called tier 1 centers their large
computing centers that everyone on the
collaboration can use the distributed
internationally for a number of reasons
one of which is funding agencies like to
have boxes in their own country so it's
easier to get money if you have a
funding agency in your country and it
also makes you less sensitive to any
kind of infrastructure problems yep
gzip type of compression so we do but
nothing nothing much fancier than Jesus
if we could find an algorithm that
worked significantly better it would
save us a lot because because data
volume is a huge at the moment data
volume is our biggest cost driver the
reason is we want to keep all of the
analysis data on disk because the
latency for tape access is a disaster
and the rate of failure on tapes is too
high but if you look at the costs of
discs they're so high that a doubling of
our data size we could basically get rid
of all our CPU and we still couldn't pay
for the disk so fancy compression if
someone could tell us something that was
better than gzip would definitely
definitely help one thing we do to try
and make that more efficient as we
stream each object with a separate IO
streamer and so gzip separately the
objects and that makes gzip do a little
bit better on okay so once we have these
tier 1 centers that analyze the data
then it gets passed it would have called
tier 2s these are mainly universities
and they're they're the place where the
users mainly do their own analysis you
can also do your own analysis on your
laptop or your home machine we call that
a tier 3 but I always joke that because
we have two laptops and a desktop at
home I'd probably be classified as a
tier 3 by the Atlas computing model so I
guess the only other comment on this
slide that I would say is important is
even though we're a 2,000 person
collaboration it's a very broad physics
program so typically about 10 people
work together on an analysis for
high-profile analyses you'll have
parallel efforts looking at the same
data but it's very hard to work with
more than 10 people in a tight
collaboration and so we still have a
model where it's a few students postdocs
and a couple of faculty members working
closely together and then comparing
their results to other people okay so
how to physicists analyze this data well
in spite of the fact that I've shown you
a lot of event pictures event
visualization really isn't a primary way
of doing analysis the main thing that's
used for is
debugging now if you find that your
pattern recognition isn't working
looking at an event picture and being
able to interactively run the pattern
recognition might help you understand
why it isn't working well but almost no
analysis is done by viewing single
events typically what we do is we do
statistical analyses on ensembles of
events and we compare the observed rate
for a given process to what the
combination of theory and detector
simulation predicts that rate should be
we search for deviations from the
prediction and then the characteristics
of those deviations tell us if there
hints of new physics so for example this
happens to be a simulation of a model
called Technicolor that has particles
with well-defined masses in it and so
what's shown here in yellow is what you
would get in the model it didn't have
Technicolor and these various white
Peaks show four different masses of what
the techni particle looks like what you
might see in your detector so what
you're looking for is this excess over
the standard model signal and so it
points out the fact first of all that
simulation is very important because
that defines the baseline of what you
expect to see and it also says that
we're going to have to do statistical
analyses and we're going to have to do
fits I didn't show fit on this but
you're going to have to ask how many
events do I have above that peak and
that's going to involve techniques where
we try and fit for shape for the
background and subtract the signal okay
so some comments on the software the
lifetime of the experiment is 10 or 20
years that's longer than most OS is less
so we have to ask ourselves how do we
keep our self where robust with such a
long lifetime and the answer is we just
have to plan for change it's not going
to be possible to keep a given version
it's also longer than the term of many
developers so there's a premium for
having the code maintainable documented
and accessible to others so in that
sense we're closer to industry than most
academics are because we can't deal with
one graduate student typing in the
corner and no one else knows how to run
the code it just isn't a model that
scales to an experiment this
the code is shared by several thousand
people so robustness and document
document ation is key we use cbs4 code
management we have a homegrown system
for configuration management on top of
that which I would say is marginally
satisfactory but I don't know anyone who
has a good system for configuration
management I know there's some
commercial products but they don't
necessarily scale and they tend to be
very very expensive so we basically have
something that's built on make files and
scripts that runs in conjunction with
CVS we do we do releases of major and
minor release versions and bug fixes to
the releases so all that's pretty
familiar to all of you okay the youth
patterns are likely to change with time
so we need flexible code and the input
parameters for the reconstruction and
analysis improve as we learn more so we
try and separate our data and our
metadata so alignment constants
calibration constants and anything that
we think we can make better by looking
at the data we keep in a relational
database at the moment we have a system
where we use Oracle at CERN and mysql at
the other institutions in retrospect we
might have been better off with mysql
everywhere but there's a tendency for
people who aren't experts to think if
you pay money it must be a better
product one of the big issues for us is
needing to know where the data is and
how it was processed so having access to
metadata about the data processing is
important and about the location of that
data because because that's the starting
point for all of the users okay so
what's our chosen software architecture
we have a multi-purpose C++ framework
it's got well-defined abstract
interfaces plug-in components that
satisfy those interfaces where services
algorithms tools and each of them have a
haven't defined algorithm and the code
that the users right can inherit from
one or more of those those interfaces
the whole thing is done with dynamic
loading so the framework is brought up
you specify what libraries you want to
load at runtime
and that allows you to add your own code
trivially to the existing system and we
use python bindings for runtime
configuration so much of the code that's
written has parameters of the algorithm
so for example if you're looking at a
clump of energy you say I only want to
look at a clump within a cone of a given
radius the size of that radius is a
parameter the algorithm and can be set
at runtime via Python script the data
objects have a persistent transient
separation originally we tried to have
our data persistency automated where we
had a parser that read the header files
and wrote the classes to write the data
out we found the performance on that was
truly awful in fact brute force having
somebody who knows what's in the data
actually write a streamer that defines
the data was was a major plus we gained
factors of three or four niño
performance by customizing those
streamers and one of the advantage of
the persistent transient separation is
make schema evolution much easier so you
can change your transient representation
and deal with the fact that you have old
data just by by putting an if-then
statement in your in your streamer code
it's low-tech but it works and anyone
who's ever tried to do schema evolution
in a relational database knows that it's
not an easy problem without some kind of
separation we have lots of calibration
metadata we handle that with an interval
of validity service C++ handles with
callbacks we encapsulate the interface
to the database so you can have multiple
implementations of a database Oracle
MySQL text files or even data stuck on
the front of a file that you have in
your in your directory and the code
doesn't have to choose the
implementation until runtime and then
for the actual statistics making of
plots we use a certain develop package
called root it's a C++ based package and
it has a PI root Python 2 root interface
that allows you to again do bindings to
the same data objects that we use within
the framework so this is something that
for us is new in the past we always had
summary data that had very different
data objects
and we've changed our mind completely on
that and that's partially because the
power of being able to use C++ for
things like collections and using stl to
do all of the things stl used people
just didn't want to go back to having
flat lists of numbers with no tools and
so we've really found we're much better
off sticking to a very object-oriented
c++ type of analysis even when you're
doing interactive analysis from the
prompt and certainly when you're doing
scripts that are going to run over lots
of events okay so I'm just going to
finish with some examples of what some
analyses would look like with this data
and one of the things that I really want
to emphasize is we don't know which are
going to be the most exciting things so
the flexibility and being able to change
is a very important part of what we do
so one example I've already told you
about the Standard Model Higgs it's the
thing that gives a master all the other
particles and it turns out in this
theory the theory does not tell you the
mass of the Higgs but it tells you
exactly how it decays if you know its
mass and so what's plotted here is as a
function of the mass of the Higgs what
its probability of decaying and
different channels are so what that
means is that you can develop a strategy
for looking for it as a function of mass
and there's been a lot of work on both
Atlas and CMS and devising those
strategies because it is such a rare
process okay so here's some examples
from our technical design report of how
one searches for the Higgs and some of
the difficulties one should highlight so
if the Higgs is light it turns out it
likes to Decatur photons not all that
often it only does it one out of a
thousand times but photons it turns out
you can identify cleanly and measure
their energy very well so even though
it's a low rate it's easier to see in
this channel than a lot of other
channels now easy in a relative word if
you look at the sister Graham here that
little blip is the Higgs that straight
line is the background from real photons
and other in other events so again this
isn't a case where you can look at an
event pick
sure and say there's my Higgs you have
to do a very careful statistical
analysis and it's a case where you
really care about your calibration
suppose as a function of time your
energy calibration changes then this
peak would get wider given how small
this peak is compared to the background
if it got wider it would disappear
pretty fast so this is an example of why
we really have to be able to remake the
meta data make sure that as we improve
our calibrations we can reprocess the
data multiple times it's also an example
of why we need good and robust fitting
tools this shows the excess once we
remove the smooth background with that
dotted line this is a dicey measurement
so this is one where we want to have
multiple channels because you can't do
it and only one channel and believe it
here's a second example where he goes to
a particle called a B cork and Abby
auntie cork and here there are two
issues one there aren't a lot of events
but also again the shape of this
background is something we're going to
have to measure from data so all the
work in this measurement isn't going to
be looking for that peak itself it's
going to be all of the things you have
to do to convince yourself that you know
that that dotted line has the right
shape so there's going to be lots of
going back to simulation looking at
control channels that also decayed a BB
bar and say can I predict what their
shape are an iterative process this is a
reasonably easy one haste goes to ZZ
there's not a lot of events here but the
peak is big compared to the background
so that's a nice one yep two we only
does he goes to leptons so either
electrons are muons and we see both the
electron on the muon channels so this
one is called the gold plated it's it's
easy to see and this in fact is that is
one event from that peak just showing
what it looks like on a on a detector
simulation so I think one of the things
i want to emphasize here is that if you
ask how do you do this analysis it ain't
SQL queries the way physicists do
analysis isn't sitting there and looking
in a relational database it really is
coding mathematical formulas with
complicated objects with the
relationships between them
this was a photon or an electron where
is it the detector and do I need to do
some kind of shimming of the calibration
and so our whole model really involves
people writing code to access C++
objects at an earlier stage people
propose putting all of our data into a
database as opposed to just keeping it
in files and that was a point where
objectivity was viewed as a potential
solution because it was an object
oriented database what we found was
first of all there are real scaling
problems that these object oriented
databases had a hard time handling the
size data sets we had scheme evolution
was a nightmare as it always is in
databases and once you're using C++ the
advantages of having a database over
having a bunch of files where you said
these files have all the electrons these
files have all the muons really was not
apparent so we we've pretty much gotten
to the mode where we say metadata goes
into a relational database data goes
into C++ objects that we try and read as
fast as we possibly can and try to
navigate to as fast as we can okay
here's a second example supersymmetry
and supersymmetry every particle has an
own partner so fermions which is spin
one half particles have spin zero
partners bosons which is spin one half
while particles have been soon one
particle to spin one half partners it's
a theoretically favorite extension to
the standard model both because string
theory requires it and also because it
would provide a Dark Matter candidate
and it gives you a lot large number of
particles to look for now fortunately
there are a lot of variants in this
model so it's not as if you can say
there's one model and that's it so you
really need to have a very broad based
search for it many of the models involve
missing energy things that like
neutrinos don't interact in the
detectors so there's lots of emphasis on
missing energy in these searches and an
example of how one might first find
supersymmetry here's a plot where the
green shows the prediction if there's no
supersymmetry and this is a log scale
the red shows wear purple shows what the
supersymmetry Sigma
would look like in a variable's just the
sum of the momentum of the for our
highest momentum objects in the event so
it's sort of a simple how much energy do
I have coming around variable so the
good news about this is you have orders
of magnitude more events than you would
predict from theory the bad news is
you've got a lump of events and no peak
so if you saw this you know pretty fast
that something new what's happening but
it's not enough just by seeing this to
be able to say I know this is definitely
supersymmetry so this would be the first
stage in what's then going to turn out
to be a long detective story to try and
characterize what the new physics is you
said gee we're getting all these events
where we didn't expect any what are
there and so this plot is that we're
getting all these extra events the what
are they is very model dependent because
the supersymmetric particles decay into
each other and depending on the exact
model there's different decay chains so
once you see in excess what you need to
do is start making plots of various
quantities so here's a model where the
supersymmetric particle decays into an
electron and positron and missing energy
so you can you can select events that
have electron a positron and missing
energy and some jet of hydronic energy
and then look at the invariant mass of
the two electrons and you see a sharp
peak that sharp peak tells you something
about the mass of this guy that you
missed the difference actually between
these two masses so depending on the
model you'll get different signals but
you basically at that point you're going
to have to be a detective so again you
require a rather rather flexible system
because you don't know what you're going
to see then the third example is extra
dimensions and here i'm just showing an
event display at first blush a lot of
the signals look similar to
supersymmetry and so one of the
exercises going on now is if you saw a
signal how would you be able to tell it
was extra dimensions and not super
symmetry and that's a long discussion
that's going on in both the experimental
and the theoretical community we're just
hoping that we have that as a as a
problem because it would be great to be
arguing about what the source of the new
physics is rather than asking why didn't
we see any okay so this is just a
picture of a simulated mini black hole
produced it at the LHC okay so finally
let me just conclude the LHC will
provide access to conditions not seen
since the early universe analysis of LHC
data has the potential to change how we
view the world but the LHC analyses
require finessin care substantial
computing resources and I haven't talked
much about it here but there's also of
course sociological challenges when you
have 2,000 collaborators all of them
feel they have ownership of the data and
you have to understand how to decide the
collaboration as a whole says yes we've
measured something in that kind of an
environment so turn on next summer and I
hope you guys will all be reading in The
Chronicle about exciting things
happening once we turn on that's it yeah
throughout everything except like
so one in 40 million divided by two
hundred because it's it's 40 megahertz
for 40 megahertz in and 200 Hertz out so
yeah most of the data throws up it's
it's a big question so fortunately the
machine will turn on at lower intensity
so at the very beginning when the
machine turns on with lower intensity we
can write 200 Hertz still so we can keep
more events but even there we're going
to have to do a lot of triggering so one
of the ideas with these pre scale
triggers as you say let's take every ten
thousandth event even if we don't know
anything just randomly you can look at
those random events and try and say is
there anything we don't understand there
but but it's always a problem and it's
particularly bad at the LHC because
there's such a big gap in energy when
the Tevatron turned on there was only
about a factor of two and a half
difference in energy between the
Tevatron and the Machine below and so it
wasn't so hard to extrapolate at least
the known science the temperature on the
LHC is a factor seven difference so the
question of how well you can extrapolate
the uninteresting physics is a big one
but the one thing that saves us a little
bit is almost all the exciting physics
involves very high math skills and when
things that are very massive decay they
tend to produce things with lots of
energy so most of the triggers say keep
things that have lots of energy throw
out things if the two protons just sort
of go past each other and tickle each
other most of the energy just goes down
the beam pipe if you're looking at here
you see a few tweaks you don't see very
much so that's that's the basic idea the
triggers but it's always a problem yeah
because protons art opposite particles
are made of quarks if you ask how likely
are you to get a collision of a cork in
the proton and the one going the other
way with enough energy to produce a
Higgs and small and also the Higgs
production rate itself is small in
principle with enough time the Tevatron
could see the Higgs and in fact people
who aren't on LHC experiments are really
busting their tails try and do as much
analysis as possible of the data because
there is a chance they might be able to
see it at the tevatron depends
critically on what the mass is if it's
at the lower range of what's possible
they might see it yep
so some analyses yes it's very dependent
on the kind kind of analysis that you're
doing so there's some analyses where you
might do a frequency measurement the
data in terms of a general technique no
because what you want to do is you want
to take all of these data and you want
to extract features that are they
correspond to num particles so you want
to look for trajectories you want to
look for clumps of energy so it tends to
be a first stage pattern recognition
that's feature extraction you'd either
look for a bunch of hits that are
consistent with coming from a curved
line or a bunch of hits that are coming
from a clump of energy and then refining
comes in doing some kind of a fit to
those hits to get the best measurement
of the curvature or calibrating the
clump of energy to get the best estimate
of what the deposited energy was so some
of the higher higher level analyses you
can use a Fourier analysis as part of it
yep that's right that's right that's
right so you have some threshold that's
usually said it's some number of Sigma
above the noise level and it's very
detective dependent on what you do so
for example a pixel detector the pixels
are basically it's a piece of silicon
that's etched into little little
rectangles and the rectangles are 50
microns by 500 microns so there's a
zillion of them those you have to have a
very you have to have very low noise per
channel and set the threshold high
enough that you only read out 10 to the
minus 5th channels if it's only noise
because otherwise you're swamped by data
on the other hand the calorimeter it
turns out the calorimeter integrates for
a long time and so it sees the previous
crossing and there they keep threshold
low enough that they do in fact read out
all the channels because they use it to
correct for any under flow from or
overflow from the previous crossing
the code the code is available but the
data isn't so all of the code is
available from lxr but but the data
itself is not available unless you
remember the collaboration yep sure i'll
pour i send me email md shapiro at LVL
gov and i'll put you at our data if you
can come up with a better compression
algorithm we would we would be thrilled
I'll give him and then you yet parts of
it are so the the calorimeter look at
the calorimeter zat liquid are going to
energy so that's cryogenically cooled
the tracking detector is only kept at
moderate temperatures so like minus 10
degrees just to keep the electronics
cool the accelerator itself is that
liquid helium temperatures that
superconducting yeah yeah the except
accelerators liquid helium because it's
a superconducting magnets
the great interaction rounds the rates
very very low you didn't there's not
there's not enough if you just wouldn't
get enuff events to be able to see it
mmhmm yeah with regard to data
compression it seemed that for any
particular experiment any particular
experiment most of the data would be
noise odd and so from his point of view
he would benefit from a lossy that's
right in fact there is lossy compression
in some objects so one of the
advancement one of the advantages even
though it's a pain to have handwritten
converters for each object is that you
can do lossy compression and you can do
bite you can basically you're not
limited to 32 bits per floating points
so there are parts of the detector where
we are doing glossy compression and
nonlinear lossy compression to take into
account the dynamic range for some of
the electronics itself is is a non is
nonlinear so the other thing in terms of
most of the information being noise
different analyses care about different
parts of the detector so by having
separate i/o streams for each type of
object and just access on demand even if
you open a file you only read objects of
the type that you care about at the
moment we don't have good ways of
selecting events but I think it's clear
that the next step is also going to be
to use some kind of a fast query to say
I want events 137 53 and 48 on this file
and then only read those events as a
starting point because the i/o
performance it's just not going to be
acceptable if you only do things
generically yeah
failure and data corruption yeah failure
and data corruption is a big problem so
its handling it during the
Reconstruction is not so bad because the
reconstruction has happened is happening
on one site with professionals
monitoring the system in the log files
are parsed and not only for core dumps
but also for signaling of severe errors
assuming that the code developers signal
severe errors and so that one can handle
pretty well and you can just say this
file is not going to be included because
it hasn't been processed properly and
reprocess it just rerun it if possible
or declare that part of the data not
usable it's much harder when you have
people looking at the end data because
these files are delivered from a data
delivery system and making sure the Joe
Blow user if ten percent of the files
don't make it to his analysis realizes
the ten percents don't make it is much
harder because Joe Blow user will get it
wrong unless you force them to get it
right so what we're talking about now is
when he creates his high level summary
data having it automatically put
metadata in the file that says which
data it read and that I think handles
everything except core dumps where he
gets a file that only had some fraction
of the input file read so you still
depend on the user to deal with code
that just drops out and core dumps its
missing files that are the real issue
yeah
public because of the primary interest
in the new toys because it's too
expensive yeah so if there's a little
bit of both so I think making all of the
data accessible would be a huge problem
in terms of data volume and the cost of
the servers would be big making a small
amount available is something that's
been talked about and in fact i think
the collaboration will be quite
interested in because there's been lots
of discussion about using it as a tool
for teaching you know and in colleges
and high schools part of the problem is
that although our system is fairly well
documented for someone who's on the
experiment it's not really adequate for
a random user but I think it would be an
interesting project if somebody wanted
to spend say six months and figuring out
how to make one percent of the data or a
selected set of data available to the
public to use right I think they would I
think there'll be no problem giving some
amount of data for those kind of
purposes it would have to be approved by
the collaboration because it is
proprietary but I think the main thing
they want to prevent as people writing
science papers before they do they don't
want to see a paper on finding the Higgs
before the collaborations decided is
found it now this is a sociological
issue if you look in astrophysics
collaborations they're all the data is
published and in fact NASA requires the
data to be made available publicly and
it's quite normal for non collaborators
to make to write papers analyzing data
that they've glommed off the web so it's
it's a culture thing and it may change
with time depending on what the funding
agencies require yeah
is to corrupt the data thank you okay
noise more particularly bad noise to
their hand attack was just playing to
compression out Thanks he's not to be
right well I'm and you can someone
playing with compression algorithm is
you can give data that's
non-controversial also but this idea of
corrupting the data actually it's it's
something that we talked about for
another reason one of the problems when
you do these analyses is there's this
tendency to see what you want on the
data and so we're having lots of
discussions about how do you do blinded
analysis where the people doing the
analysis don't really get to know what
the answer is until they do it it turns
out it's pretty easy to do blinded
analysis if you know what it is that
you're measuring right from the
beginning it's very hard to do blind
analysis when you have no idea what
you're looking for so it's a continuing
discussion but we haven't found a good
way to do overall blinding although I
think some analyses in particular
analyses we're doing precision
measurements of a quantity there will be
some attempt to blind the data by
putting a random offset in that only you
know that depends on a random number
that only one person in the experiment
knows right okay mm-hmm cakes dth I know
there are so many of them out there and
view a very good figure online too we we
do all the triggers in parallel so what
we do is we divide up the bandwidth and
it's one of the biggest issues in the
experiment is making sure you're in on
the discussions for how the bandwidth
gets divided because that's what
determines the physics you have to be
there fighting for your physics in the
bandwidth yeah um so you had
building your machines did you have
software engineers building we do have
software engineers but not nearly enough
and this is partially a problem with
funding agencies it's much more
difficult to get funding agencies to pay
for software engineers than to pay for
hardware engineers we probably on Atlas
have on order 25 or 30 software
engineers and we ought to have more like
150 there's a the exception to that I
only counted the offline the it's a
wreck it's fairly accepted the data
acquisition systems need software
professionals because everyone
understands that real-time systems are
hard there's a tendency for funding and
agencies still to think anybody can
write software and it's it's only been
in the last decade that you can that
they've even asked software projects to
present we have meetings every year of
the Department of Energy who funds us we
are supposed to present the process your
progress cost budget schedule it's only
been in the last ten years that software
has been included in those meetings now
as painful as those meetings are it's a
major step forward because it's their
first recognition that the software is
an important part of the engineering is
the hardware yeah
I don't know the answer to that but it's
quite large and in fact one of the
reasons we only run 200 days a year is
certain has an arrangement with the
French power companies that they turn
off during the winter because unlike the
US where air conditioning is the major
thing so slack turns off in the summer
in France people have electric heat so
winter is when power rates go up and so
certain has an arrangement when the
power grid gets to a certain utilization
they have to shut off in an exchange for
that that gets better power rates no we
use a lot more than we then we give out
unfortunately
the simulation problem is very difficult
and there are two parts of that there's
this one which is called the event
generation and so that's when you try
and model the underlying physics and
there the issue is we don't really know
the underlying physics very well so
typically there are four or five
packages that are all supposed to be
giving us the same physics answer when
we run all of them and look at
differences between them to get some
sense of how well is the calculation
under control and then they have
parameters you can vary to try and see
how much it changes the second thing
which is the detector simulation that
depends on how well you can model the
underlying physics of particles going
through matter there's a program called
yount for that was developed at cern
which is what we use is the underlying
engine for doing the simulation it's
been well tested in a lot of regions it
doesn't do so well at very very low
energies or at very very high well very
very high energies it's hard to test
because you don't have anything to
compare to and very low energies it
turns out it's just a very difficult
problem but the same code is being used
by some radiation physicists designing
treatment plans for patients and I'd
worry a lot more for them than I would
for us but it's but it's a hard problem
and the Gion simulation has gone through
many revisions and there are
discrepancies with the data in some
cases yeah
yes the equipment does become activated
yes yes and I don't know what the rules
will be at CERN but at Fermilab after
they turn off the machine you have to
wait 15 minutes before you can go in
because it's mostly short activation
stuff and then they have to do a survey
with a Geiger counter before anyone's
allowed to work my guess is at CERN it
will be longer the magnets themselves
that on the machine will be highly
activated and so if the machines ever a
decommissioned they will have to be
treated as radioactive waste and
disposed of like any other radioactive
waste it looks like we're done
thanks thanks a lot
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>