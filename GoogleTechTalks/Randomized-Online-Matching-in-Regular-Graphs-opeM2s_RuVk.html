<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Randomized Online Matching in Regular Graphs | Coder Coacher - Coaching Coders</title><meta content="Randomized Online Matching in Regular Graphs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Randomized Online Matching in Regular Graphs</b></h2><h5 class="post__date">2017-08-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/opeM2s_RuVk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so the title kind of uh speaks
for itself
randomized online matching in regular
graphs presumably all know it's a
matching plan may be told about the on
my match suppose an advanced a bunch of
you noticed pretty well introduction
moderately fast so the inputs will be
online matching the other line input is
a bipartite graph the left side or
offline nodes or known ahead of time and
the online nodes are the right side or
on my notes arrived online and online
matching algorithm is tasked with
matching arriving vertices immediately
in your book or maybe to turn to second
on tonight
so for example say the first time I know
it shows up it may preserve the first
second and fourth off i knows insanely
matches second offline vertex then the
second online no chose our neighbors are
two three and four and signal matches
deform and see if the third online
diuretics turns out and only neighbors
the fourth of five vertex well that's
sad because it can imagine as usual our
measure of success will be the
competitive ratio which is simply
accredited ratio of an algorithm is
simply the expected gain of the
algorithm over an input over the
optimally and so for example as if the
algorithm that gave us this inputs or
deterministic the competitive ratio of
this algorithm is clearly at most two
thirds goodness in Optus using the shown
to be three and the algorithm only got
to make discussion but faster that you
might notice a little competitive ratio
of an algorithm cellphone will say it's
alpha cat it's so far it's very Stalin
Rahl will clear the algorithmic problem
right okay so let's make we start by
considering some algorithms for the
problem so first of all let's talk about
the holistic algorithms
so the first deterministic algorithm you
think of for this problem indeed for
anything is just greeting which in this
terminology
exactly equal to match each online
vertex to the first to summit some
arbitrary unmatched mate okay so this
clearly outputs a maximum matching so
it's at least one-half competitive and
unfortunately you know deterministic
algorithm can be better than one so see
my mom my notions help neighbors to
offline vertices without loss of
generality deterministic algorithm
matches a lot of bangle and none on my
note shows up and is only interested in
the one matched and of course you cannot
extend the start with doing large input
so what happens really is really
handsome randomized algorithms so the
first random algorithm you think about
is just matched to some uniformly at
random chosen right some unmatched
neighbor chosen uniformly so again this
is at least one-half competitive and
unfortunately it's not much better we
can show that this is 1/2 plus little
more competitive for example the
following inputs you have an online word
Caesar an offline parentheses every
online vertex neighbors and offline
vertex justice in front of it and the
first enemy to all my fantasies are
connected to all the second of the last
and over to offline grace so what do you
kind of expect is pretty much each of
the first online enemy to all my
parentheses will get matched incorrectly
cross this complete bipartite graph we
get the matching more or less like this
it's so most of the last and my brother
sees you can formalize this alright so
that doesn't work uncorrelated random
this doesn't seem to cut it what about
correlated randomness so here comes the
algorithm of car plus irani Maserati
which introduced this problem of on my
matching and their algorithm ranking
works as follows on initialization we
big a uniformly chosen permutation of
the offline vertices like so and
whatever online node shows up you match
it to its unmatched neighbor minimizing
the value on this permutation
okay so for it could be example that we
gave earlier it's not too hard to show
that after you've seen a constant
fraction of the first 10 over 2 of my
parentheses they're correct
neighbor should be should have a lower
value under the permutation than this
their correct match the guy just in
front of them or here should have a
lower value on the permutation than the
unmatched guys at the bottom should have
something value like they know if you
didn't follow that don't worry about it
at all just to kind of give some
intuition that this should at least help
again to these audiences and what car
was running was even in many future
because it shows is that the ranking
algorithm is 1 minus 1 or X where 1
minus 1 of really success all right so
we have some left over a 1/2 that's nice
could we hope to do any better ends well
carpet accomplished and it was my show
that made you count 1 minus 1 of really
is the answer so at least for worst-case
analysis we understand this problem well
there's not much more to talk about
fortunately there's other things we can
talk about things so 1 minus 1 over e as
I said is nice but you might be
interested in more and in 2005 Avinash
Mehta sub de negocio name was the only
related online matching of its
extensions to natural way so the offline
modes are advertisers the online notes
or ad slots
every impression is allowed to be shown
1 1 1 ads maybe advertisers don't have a
matching constraint thing they have a
budget constraint but they've got some
kind of generalization of the magic
constraints and I guess if you're
interested in working on online matching
this is great news because I'm sure it's
not news to anybody here online so if
you're looking to have your theoretical
research funded that's maybe a nice
problem to look at
a bit more seriously so what method I'll
show and a lot of subsequent work show
is that the 1-1 a rebound is prevalent
when many generalizations about my match
so vertex weighted matching Japanese
problem was small bids display ad
problem was much capacities and so on
and so forth so this is this is the
optimal bound and what what more could
we possibly be interested in well given
the previous slides one over he or 36%
of sixty billion dollars as a lot of
money so saying this is the worst case
optimal as months not a good answer
so this is first a lot of researchers
looking into more practical quotes so a
lot of this research has been around
stochastic assumptions may be the
impetus gone from something distribution
no no no no no it's randomly permuted
inputs and for these we know that we can
get better than one minus one over e
competitive ratio even for a matching
problem but you can't get arbitrarily
close to one at least from the online
magical generalizations some some
generalizations around online matching
you can to get arbitrarily close to
another thing which is a bit of what
I'll talk about today is structural
assumptions may be assuming that the ad
slots of every impression is of interest
to all the advertisers maybe advertisers
are interested in large segments of the
population
all right so that's the my one slide
about the practical aspect when we talk
about something they're very theoretical
so what we considered in this work is D
regular graphs so these are graphs in
which every node neighbors exactly the
neighbors so the number of online and
offline vertices exactly the same will
denote that by M and easy corollary of
holes theorem is that such a such a
graph has a perfect match to match every
single vertex and well this these inputs
have been now studied
a lot on theoretical computer science
starting with research on regular graphs
cunning and too much name dropping and
it's this it's been of interest for a
lot of our problems and ECS not just
matching but since we're talking about
matching today let me just maybe tell
you a little bit about it so in the
offline model we know how to compute
perfect matching in a bipartite regular
graph and linear time deterministically
and we can actually even do it in
sub-linear time randomly it without even
reading the int and both of these
results should be contrasted was the
state-of-the-art max matching algorithms
for general micro-tyco graphs that are
all polynomial slower than the later
here was n so if M is the number of
Emmys the graph is dense
I don't care about most
so this is good news I mean I'm not
gonna talk to you too much about
applications of matching on bipartite
regular graphs but it does have quite a
bit of applications and the fact that
you can solve that faster than for
general bipartite graphs is great news
and the question is whether or not you
could do something similar in online
magic not necessarily because of any
applications just because it's clean all
right so what's known in online matching
so in recent work was you know they show
that the optimal competitive ratio for
the termina stick algorithms on the
irregular graphs is even all the exact
expression strictly greater than 1 minus
1 week for any fixes regularity game so
a bit of contrast is what we said so far
for general graphs it can be 1 minus 1
over e even deterministically for
regular graphs you can be better
Mystikal
pleasing being you actually get back
yeah okay so that's exactly what I'm
just getting out I thought so as the
increases
alright that's nice but doesn't seem to
give us so much another point of
comparison which should we gonna discuss
earlier so for stochastic arrivals on
regular graph so this seemed like a fair
comparison at random and ranking are one
- order 1 over square root of deep in
that so not both random is good for the
IB model and ranking is good for a
random permutation model whatever all
I'm saying is so some simple randomized
algorithms before here so at least for
stochastic arrivals the problem gets
easier as the increases and the natural
question is what can randomized
algorithms do under adversarial right so
deterministic algorithms the problem
gets inherently harder as the increases
what can you say about whether it's the
last bullet or whatever the one over
square will be yes referring to the one
with the destroying yes because that
paper basically showed us having a lot
of large disjoint matchings is enough in
the random order and we shown by the way
that that's not the case you can just
extend activity all right so we worked
on the problem let me tell you keep you
in suspense for too long so we presented
a new randomized algorithm which on be
regular graphs is 1 minus order square
would long be over square root of the
that's it so up to the square root long
D term this matches the results don't
surrender for random random moments so
it also shows that the problem is
actually easier even in the adversarial
arrived as the increases we also nervous
or similar bounds with high probability
which is best one always the first at
least observed high probability results
for online matching and we also give us
some kind of fairness guarantee where we
can match every vertex with ignore the
exact expression something tending to
act as games go to that's those are the
positive results on the hardness side we
show that our results are nearly finds
so no algorithm is better than one - or
than one of us proud to be competitive
and up to the squared log D this is the
right answer
and another tiny bit of our hardness
results like hardness per se ranking and
random are constant competitive even if
T tends to infinity so we actually
needed to tell opposing yeah do
something efficient any questions about
the results easily so we don't we don't
actually need the irregular or per se
when it's some no that's actually not
enough to get so we have to talk a
little bit more maybe after trying kind
of what other general conditions I mean
it's it's more general than what I'm
going to talk about all right so let me
maybe try and I kind of hinted so the
first thing we'd like to do is match
each edge was probability roughly 1 this
is the optimal fractional solution and
well the hardness results shows that we
can't get exactly 1 over D because that
will give us a 1/1 competitive algorithm
this is still the general direction we
have to go if you want to get something
attempting to claim so first of all
clearly if edges are matched much less
than 1 over the on average then you
better do you're not going to get
something that makes one that's kind of
a natural
Oh on the other side on the other hand
if a lot of edges are matched more than
one over D on average then you'll have a
bunch of vertices you can have a bunch
of vertices that are matched more than
their degree over D with probability
more than Rio Verde then I could just
put all of those two neighbor same the
same on my parentheses and then those
online vertices have a good chance of
not getting rushed all right so that's a
general hope so let me maybe trying to
talk about some challenges towards this
general direction maybe I'll give you
some notation before otherwise so first
of all we'll have f of t the noble set
of free and is unmatched offline
vertices at time T FBI will be an
indicator for whether or not the offline
vertex is free at time T and one thing
that might be but DTI will be the degree
of offline vertex I before the arrival
of online critics T before okay so
example at the beginning before the
input the online purchases show up PGI
is I guess you could say zero it's on
the find on the first online relics rose
on DTI of everyone is zero this they can
have any neighbors said beforehand after
the second online brothers shows up all
the neighbors of the first half pti one
two neighbors have to be two they should
be okay one more thing and the abuse
notation every so often and just have
probability of X be the probability that
x equals 1 for indicator variables small
space engine good so notice that maybe
talk about some challenges towards
getting this one over the marginal
probability at this request in the for
there so so another algorithm different
solution would be just match it to the
one which has the highest degree so far
so the highest degree so far the
deterministic doesn't even matching
maybe they can reform the amongst the
highest degree is that there's a family
of algorithms I'm really happy to tell
you that we thank you
right so let's until the following in
but this is again that's not so much
analysis just kind of getting some
intuition is do one other difficulties
we have to overcome so imagine you have
an online Britax T which has its course
D neighbor since we're looking at the
regular graphs a half of its neighbors
have BT I equals zero so T is their
first ever neighbor and the other half
with DTI zero will call lonely dream I'm
using the guy was a low degree will call
low degree neighbors and guys was agree
it was sorry over to neighbors so far
will call high degree neighbors and even
if smashed earlier independently okay so
what are some challenges at least of the
random algorithm well if all the edges
before they arrived before time T or
matches probability roughly 1 over D
when all these the over to high degree
neighbors should be match the
probability roughly 1/2 before T shilla
so that means that the the observes
unmatched neighborhood the fatigue and
things you know 3d over for a guy so
ideal for of them should be matched and
here I'm saying I'm using the facts I
assume they were matched independently
before so this would also be highly
concentrated so the probability of a low
degree neighbor to get matched the T is
exactly 1 over the degree of T it should
be just to my so at least as far as our
as our general guiding intuition
unfortunately we can fix this fix this
example relatively easily so when will
do this will give each of these free
neighbors
wait either one if you're a little
degree neighbor or two if your high
degree neighbor and we're not matched
before three and then what we'll do is
we'll match to a neighbor or I was
probability proportional to its weight
WT know some of these weights and then
the probability of a low degree neighbor
and says he's verified we should have
high concentration of sum of weights
should be team or around he was a high
concentration and therefore the
probability of a low degree neighbor and
also how did we Mabel to get match
should be roughly marked so that's at
least part of this part of the general
approach what am I looking here so this
is this is a basically so if he has a
bunch of free neighbors every free
neighbor I will have will have a weight
which is inversely proportional to its
probability of being free a type T
that's expected weight should be exactly
one and again will match to turn a
variety of the weights and as I was
saying the rough intuition
since the expectation expected weight of
any particular vertex a neighbor is one
this expectation of the sum of the
weights is two best so WT i we said is
one a really probability and we'll give
this weight only if it's free so with
this probability of this term cancels
out we have D neighbor sources some of
the weights in expectation of if the sum
of the weights were exactly its
expectation then for every neighbor of T
the probability of this neighbor I being
national T is exactly its weight but the
times are probability that it wasn't
matched before to be exactly one would
of course assuming that the random
variable is always an expectation as a
bit of a stretch so let's maybe see what
actually say him so this is I'm just
nice okay so as we said we'll try and
match neighbors of team is probability
proportional to their weight which is
one over the probability of those guys
being free times of some of the weights
and the problem is that since the sum of
weights can deviate from its expectation
we have two possible cases the first is
if the sum of weights is below its
expectation will match match the
neighbors of the team was too high a
probability and on the other hand don't
exactly the same thing if it's if it's
all the weight is too too too high we'll
get mattress too low by building and
actually the second case is the
particularly problematic one especially
if you account for possible positive
correlations like this my probability of
being unmatched means that a lot of
other guys are likely unmatched
then I'll very often be on this side of
the bell curve and maybe I'll get to
lower probability of getting matched and
then that should only kind of be a
vicious cycle right that was likely or
do not get matched before again we like
ok so the general approaches first of
all if the sum of the weights is below
its expectation just don't match for
some problems that's that's kind of an
easy with some probability just decide
ok now ignore T I'm not going to be a
maximal matching although I'm going to
you and hopefully I'll be able to show
that the loss incurred by ignoring some
on my vertices want me to call that part
hopefully should be so now the second
part is if the sum of the weights is too
high and every vertex has two low
probability of getting matched will
match some extra vertices this means
absolutely nothing right but I guess
that just means we'll mark these extra
vertices and say look from now on I'm
not going to actually match hopefully
here again in fact we're not Maximus too
I'm a general general Crozier
so quick recap of the approach you want
to match each edge with probability
roughly one with E so what we'll do is
we'll mark each edge is probability
exactly one of the day such that all the
matched vertices are marked and most of
them are vertices are actually also
matched so sorry I'm marking edges
marking birthdays just means it has an
inch mark and edge
come on in genetics is not flying for
dick Susana alright so in two ways where
some of their weights and we're just
ignoring some offline vertices also kind
of not really fault of their own if the
sum of the weights okay
alright so quick recap of notation it's
gonna be pretty much what I said before
I just spike modification of one thing
so everything will be the set of free
this time free means unmarked vertices a
19 ft is before is indicated for whether
or not you're free at time T time T TT I
will be the residual degree at time T so
here we know that the graph is regular
therefore we can compute the set of time
this we have more general so our help
you limit Ricola
marking sports as follows for each
online vertex t look at each of its
neighbors i we're going to wait be over
the residual degree of I at time T times
the indicator for whether or not it's
not it's free in fact okay and we'll get
to where this expression comes from we
basically went over so once again same
edition
and then as I said earlier was some
probability we might not actually match
321 so was probability the minimum of
solid weights over D at one rather than
they expected someone waits with D so if
we were exactly expectation with match T
every time in that case we match T to
some note chosen with probability
proportional to its weights and Mark
that loan and finally the sum of the
weights this was the left side of the
bell curve our intuition for the
solution early and for the right side of
the bell curve if some of the weights is
if the sum of the weights is higher than
its expectation will mark each offline
neighbor of team don't know worried
about the exact expressions or something
to make in the case when W by D is
greater than one you still match I so
much but any mark of humor and I also
like so you should I think of these
comedies at the end just something to
make to make the marginal probability if
you can remark exactly with the Mun
folds like various of mean things might
happen right and he's correcting them by
so he I'm just making sure that you're
always exactly at least the property of
can mark is exactly its expectation yeah
Alex Aragon the sum of the waves yeah
before the arrival of the next vertex
will be exactly exactly a soft every
correction when you do some extra
marking and it's a guarantee that the
next vertex will see in expectation it's
expected
the way the expected rate of its name of
its and Micawber's become be expected
should be minus one right so you're
marking your mark and one note an
expectation maybe you're not marking in
some cases and maybe you're marking with
you in some other cases since you'll
mark each with exactly probability 1
over d an expectation you're marking one
so you're decreasing the number of
unmarked cases by one an expectation
good ok so I had waved my hands enough
so first of all they may be trying to do
some identity of the axis so the
algorithm this is the same algorithms
before just so here's the general
general like there for the easy analysis
so first of all we're going to bound the
probability of an online node T not
being matched in terms of the sons of
weights this is actually kind of thought
we obviously what he said so far so the
probability of T not being matched is
just a standard deviation of the sum of
the weights neighboring it good so in
order to bound the average standard
deviation WT found on the sum of the
expected unmatched on my vertices which
correspond exactly to participation will
bound the variance of every single
weight and then will show a negative
correlation between the weights and then
basically all right the questions about
the general right so first last notation
MIT is going to be an indicator for edge
i t be marked so I want this probability
the probability of MIT to be exactly 1
would be less that's what the first
level says and the first corollary the
probability of I being free at time T is
its residual degree over D starts it's 1
and every
that sees another neighbor it's
probability of being unmarked decreases
by 1 over T which is exactly what's
written that yeah and why is that kind
of relates to our intuition from earlier
but means of this deal with delta T I
normalizing term it's exactly 1 over the
probability of I being free at time T
and therefore the expected weight of any
particular edge is exactly 1 and kind of
folded all the intuitions we said so far
what will guide our houses good so how
do we prove this so this is an amazing
production so for any sets of unmatched
vertices a including the probability of
the edge I T being marked conditioned on
a being receptive unmarked season 5 team
and I clay was exactly the weight of the
edge I divided by Y might be which is
exactly 1 over the residual dirt so I
won't I will prove this for both cases
let's just consider the one case where
to sum of the weights is less than the
second expand then its expectations are
the other cases I said the numbers are
so the sum of the weights is less than T
what's the probability of T getting
matched and marking someone it's exactly
sum of the weights over B and any
particular vertex is marked was
probability proportional to this
everything cancels out good and now we
just need to condition right so the
probability for any particular sorry is
the probability of the edge I T being
marked conditions are high being free at
time T is exactly expression of its
revenue of the commission possible sets
a and therefore the probability of MIT
of I being market time T is exactly the
conditional probability x times the
probability of condition and plugging
plugging in the inductive hypothesis and
the previous expression we got exactly
okay that's that's the first time where
this is kind of that was our initial
motivation why he wanted every edge to
be my Master's probability 1 over D that
seems like a bit of a stretch so please
do want every edge to be marked so few
consequences of this so it's the
probability of I being free at time T is
exactly its residual degree over T then
that means that the expected weight is
exactly 1 I think so before right
bu another T is 1 over the probability P
3 and the corollary the expected sum of
weights is exactly the again as examples
and another thing we'll need is that's
first first variance well the WTA's are
just scaled Bernoulli near variables
right so apparent scales would radically
so this is the expression you have here
that's right it's a deal with all the TI
term squared times the probability being
free concerts and out and from this we
find that for any offline vertex I the
sum over all of eyes neighbors of the
balances of eyes edges is exactly order
D log D it's just the x harmonica
shadows tackle we'll get back to it
later
all right so we've got some overall
bound on the variances how can we use
that more local way so as I said well
discussed in the roadmap that claims
that for every online vertex T the
probability of team not being matched is
it most standard deviation of WT oh
maybe and this I mean this is kind of
easy given the things we've seen in the
previous slide so let's do it quickly
the probability of keen on being matched
is exactly the probability the WT sites
1 minus WT over T if you were below your
expectation is just
by how much you deviate taking out
absolute values and remembering that the
expectation of WT SD and maybe taking
that square root and squaring things by
against it once we get this I claim that
this expression is of most square the
standard deviation of WT
I mean if you can follow this there's
really nothing not very very deep going
on it's just saying that the expected
deviation from your expectation is a
closed setting so I kind of follow the
math or is there an intuitive way of
understanding why in the process he not
being managed depends on the variance of
the weights around
well W is the sum of these weights right
yeah so so doesn't have not these
weights I depend on the sum of the
weights and I'm sure they will show that
you can count this variance of the sum
by just some facts
okay so that actually kind of leads to
the next place so I'd like to be able to
bounce this variance of WT parents of
some WTI by something depending on the
sum of the variances and for now we're
going to have to rely on the following
to get never seen this before so good
time to sum back into my sir nice
toolkit to use it's very useful for a
bunch of fat balls and bins results and
simplifies a lot of fun Norma's ult's
sometimes it goes like this so I'm
finding like a one and a half to two
minutes prime or negative association so
the definition is pretty intuitive we
say that n variables x1 through xn are
negatively associated if we're all
monotone increasing functions F and G
functions f of X are in depending on
this joint excise the function the
function f of X and G of X are
negatively correlated
I think that's method that's very
intuitive you guys Satan isn't I'm just
saying that look at these variables and
if I tell you look these guys are
relatively large for any any way I'd
like to measure large and an expectation
these guys are small again for anywhere
I'd like to measure right so far we have
in French why these useful let me maybe
give you a bunch of examples can IV give
me anybody give me an example of such a
family such a distribution would I come
to top of them a trivial one don't have
to be particulars in independent
variable right so that's very exciting
so at least you expect to maybe get
things that you can get for independent
variables slightly more interesting
example is what's called the zero-one
lemma so a bunch of binary variables x1
through xn whose sum is always at most
one and they're clearly negative places
for any not raining out so for example
if I tell you it was some of these guys
always - then most of this definition of
negative association also implies
pairwise negative correlation I just
take F and G to be like xixj right now
if I if I have to have n variables I
just say I've either X 1 and X 2 V 1 X 3
and X 4 B 1 or X 5 or 6 V 1 and so on
and so forth and clearly I don't have
negative correlation right if one is one
of those two is 1 the other is 1 most at
most
no so again this example it could be
like sharp it's always two right if I
always I start off with n variables
right then I just couple them and
whatever I will wear moms it'll always
be at most two and so that's that would
be really nice thumbs up I suspect you'd
be able to prove some unreasonable
things that's not funny
I guess you take independent any
condition on the pilot
in the temples look most k yes yeah so
hey mister do you nod together
Commissioner I'm just saying so this is
this is a much simpler scenario than
your described right so I tell me to all
zeros or ones and maybe one of them is
one these are large one somewhere here
alright so these are some simple
examples on site where you can do
anything useful with that but what's
nice about negative association is you
also have closure properties so you can
get some more interesting distribution
for that so for example the union of
independent negativity associated
distribution itself negative association
another useful property is that if you
have a bunch of monotone increasing
functions depending on this joint
variables and these functions apply to X
their outputs are resolved as a basis so
the timeline you're looking for here is
calculation free proofs
it's very well I guess it's unappealing
property if you want to check something
quickly something so why would we
actually be interested in these MA
distributions well some other useful
properties are as I mentioned earlier
they're pairwise and negatively
correlated I'm actually a bunch of
bounces we'd like to apply to make it
two independent variables apply to this
much wider family of distributions
okay so pumping bounds okay so back to
our regular programming how we're going
to use this a negative association in a
kind of simple way so the claim is of
the variables MIT again
indicators for whether or not the edge I
tease marks conditions on any set of
free vertices are negatively associated
and by given the previous slide this
will be a really
fashionable so conditional on this set
of our three vertices the hots for a
fixed P MIT for a fixed T for fixed T
and for a fixed set and when three
vertices okay so first of all we'll have
the first variables that are one if I
was marked by this current is because I
was also matched vertex so clearly most
one of the one one of these happens zr1
lemma says their claim seconds of the
variables are independent coin tosses at
the end and whether or not you're
matched is just so these are independent
distribution so their union is any and
now whether or not your mark is just P
or of two variables MIT depends on two
variables of no one else depends on a
fixed int and so these mi piece of
themselves may just want to play the
second quarter bulbs that's really
pretty much nothing to show me right a
bit more time and tell me how much how
much how good calculation and futile
worthless okay so why is this a useful
so I claim that this that this
particular tells us that the covariance
side of that the indicators f TI and f
TJ are negatively associated very fixed
T and IJ also that in a minute
and why do why is that useful well the
weights WTI on CC j are just scalar
values scale versions of these ft is a
few days so they're negatively
associated as I know the feeling in
which case we can just apply a some
opportunity of negatively correlated a
fireable offense and para made of liquid
variables and therefore the parents and
WT is of most the sum of the bankses of
these guys which is kind of one where
it's useful for time good let me
convince you of this so they said
conveyance of ft 9 ft j 0 0 so this
again is some easy proof by induction so
let's go through it for step T if I and
J are T plus 1 if I'm J are neighbors of
team then the probability that I is free
my time T plus 1 is the probability that
was free by time team and that it was
not marked at time it's nots probability
not marked at time with Robert it wasn't
marked at time T conditioner it not
being marked before times the
probability of it being free until then
from what we showed earlier this
expression should be exactly 1 minus 1
with TT I showed for any conditioning
case times probability of freedom and
now let's look at any pair the
probability of the pair not being
matched not being mark sorry is I mean
it's the same thing I write it both need
to not be marked at time T and because
they to be free at time T so plugging in
the doctor my processes and the negative
association the probability of both of
these not being marked by negative
association is at most product the
individual properties so that kind of
explains why I'm replacing this
expression my T's two guys okay and the
second second inequality is from the
hypothesis so now I can pretty much not
anything we need them you like one more
slide evaluations in one more basically
so finally I claim that some of the
variances of the WT s is at most order
of n times square root D log D remember
how we bounded the probability of a
particular prediction on being matched
this result so the sum of the variances
from from the previous line that is the
most some of the individual variances in
WTI's
so the WTI's are negatively correlated
and therefore some over T or minus WT is
just some tea some over I max TT I'm
rearranging a little we find fascination
and from corollary quite a while ago we
know that for any fixed I the sum of all
of eyes neighbors bearish TT is or D 1 D
which is just n times or theoretical an
offline and online persons so if we pick
random online vertex its expected
variance is orderly about being finally
applying instance inequality again we
find that the expected standard
deviation of WT is at most orders
particular now basically so theorem
algorithm marking run on D regular
graphs is 1 minus orders for drug deal
is going to be compact groups others
have the word level ingredients so we
said before aggression into negative
association is that the algorithms loss
is just some overall online purchases T
probability of T not being matched which
we said is at most the standard
deviation of WTO led by the previous
slide this is n times or the square of
the among these and we'll find little
birds this is our loss therefore the
algorithm is getting increased n markers
that expression that's alright so we
ended up with a little more calculation
towards the end them and everyone was
happy with after lunch so we just gave
you some recited my results so as I said
we have an algorithm that's 1 minus or
the stove log being was going to be
competitive showing that the problem of
matching online matching in regular
graphs even in the adversarial just
becomes easier as the increases for
randomized algorithms deterministic
algorithms further we have matching
hyper both results in Guardian vertex
probably hundred two hundred million
matched numbers also let me get back to
your question earlier about this
depends on the average online
what does it mean
if come in and login that's pretty fast
if you're willing to get an approximate
solution in the fast middle line and
even in an online in a moment sense like
for example if you start off with a
regular graph gonna be regular graph
okay maybe I'm not willing to send to to
compute something on the end on deep
values and so for example for a more
applied setting you know you're talking
about online advertising I don't want to
look at all the transfer you know our
sample will receive them okay and I
claim that you can still get a good
solution there tell me the pattern on
that guy so if you guys wanna try it out
see if it's useful so if a for this of
genuine graph paper I drop and I gave
you the fractional solution offering
fractional and does this all work
or does it read some properties in the
tract Association some consumer and in
some sense it needs yeah we kind of need
the values to give you something
so it's looking the the losses in some
sense determined by this by a function
of the fractional solution you can bar X
completely the various depends on the
the values area of these values yeah
yeah when we're looking we look at some
extension of this to see what else will
the story seem coherent enough with
where the variance is one R squared</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>