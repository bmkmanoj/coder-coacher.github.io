<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Testing Chromium | Coder Coacher - Coaching Coders</title><meta content="Testing Chromium - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Testing Chromium</b></h2><h5 class="post__date">2010-09-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eOe1WfFWHRI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm going to go over in this
presentation the rationale for testing
some of the types of tests that we have
available to us and the tools that we
use to do testing the chrome team has
around 200 developers and we average
about 100 commits a day so with all that
code flowing in and out we need to make
sure that stability remains a high
priority in one ways one way to assure
that stability is to test the code that
we're running so one of the things that
tests do is make sure that the code the
feature that you're running runs the way
you expect and you can't have people
hand testing your feature every day so
the test make sure you don't have any
regressions that pop up another thing
that tests do is they document what your
code is expected to do so that when
other people come and read your code
they understand the rationale behind
what you wrote in the logic behind it
tests also guide the design of the code
in the sense that when you write test
first you start thinking about how am I
going to implement it this way so that
these tests passed it really modifies it
could modify your design heavily a big
piece of the infrastructure that we have
in chrome is are our try BOTS so try
bots are pool of machines that you can
send try jobs to that will take your
change list and run them through our
suite of tests they'll mainly run them
on the three main platforms windows
linux and mac but it is possible to send
in a bot parameter to specify another
bought you want to run in some of the
bots that we have available are val
grind Chrome OS Linux views so when
you're changing code that affects those
platforms it's good to run those tests
as well
the main test suite takes about an hour
to run on average for the three
platforms so can you make yourself on
that side
but
so if you want to cut down the running
time of your try test your tripod you
can pass in the T parameter which that
takes an argument of the test program
named Cole in the filter that you want
to use so for example if I want to run a
specific unit test I'll pass in dashti
unit tests and then the name of the
tests that I want to run and that means
it will only run that specific test
which greatly cuts down on the testing
time there's also a concept for try bots
known as last known good revision the
last known good revision is set by the
official builders on the waterfall and
it's the last green build that we had so
whenever we get a green build on all the
platforms the last known good revision
or LKG are is updated however last known
good revision could be behind several
revisions from what you want to test so
you can pass in the revision using the
dash our parameter whenever you upload a
CL the try jobs used to be automatically
started but that's no longer the case
because we have so many CLS uploaded
these days so you need to make sure that
before you commit you at least run your
tribe for your latest patch set that
makes sure that any failures that could
happen in your CL are cut short so
you'll realize the failures are there
before you actually make the commit that
keeps our main tree a lot greener the
link at the bottom of this page is the
Tri server waterfall so if you don't
find your try job on the CL page itself
you can head over to that link and then
you can find your LDAP or your username
and you can see the status of your tribe
job there are three main types of tests
and I'm gonna start with unit tests
these are our lowest level tests they
test units of the code so a method on an
object such as watchdog arm at start
time so the unit test for that method
would test all the logic paths in that
method good thing about unit tests is
they they document exactly what's going
to happen in this method for all the
input parameters that you could specify
and you verify the output is correct
unit
tests are small on the so usually they
run pretty quickly and they also don't
run with other tests so they're isolated
and you only test exactly what you want
and you see those results for that bit
of code as an example I'm going to use
the autofill feature which has an info
bar for credit cards and our far writing
a unit test for this I would be testing
the autofill CC info bar delegate class
itself which is the lowest level
implementation of this info bar so
there's three buttons in this class and
there's a link and there's an icon so
the methods on the autofill CC info bar
delegate specify how this info bar is
supposed to look so I could run those
methods and make sure that I get the
three buttons back the text of those
buttons and make sure the link has the
appropriate URL there is unit test as a
target itself but there are also unit
tests for our other major modules in
chrome some of which are listed here
there are actually quite a few unit
tests which is important we need a lot
of Jess the next step up in our testing
types is browser tests browser tests
create a browser test or create a
browser object in the test so that you
can test how your code is interacting
with the browser object itself it's
actually a specialized unit test but
there's infrastructure set up so that it
loads the browser automatically and you
have access to browser internals they're
running a different process and the
sandbox is disabled there's a running
message loop so you got to take that in
consideration when you run things on
different threads and the default thread
is the UI thread another thing that cake
take into consideration when you're
testing your test running the test
yourself is that the browser window is
not visible by default that you can
specify that it be visible going back to
the auto fuel info bar an example of a
browser test for the autofill info bar
would be load up an info bar make sure
that the info bar is in the tab that you
specified the info bar is in
make sure that there's only one info bar
per tab make sure you that you can load
up multiple info bars one per tab but
that there's more than one you can call
the method on the info bar class too
close to press the close button and then
make sure that in the browser object the
info bar is no longer there UI tests are
high-level in to end testing integration
tests they're an example of black bloc
texting and that you don't have access
to object internals and you have to
implement the test through an automation
proxy there's an automation API which
allows you to control the browser now
there are several different ways we use
the automation API the main way is UI
tests which are tests written with the
automation API there are also automated
UI tests which use fuzzing they send
there's a list of commands that are
implemented by the automation API for
example click a button in the interface
load up a page load up a new tab and the
fuzzing creates a different creates a
list of these different combinations and
runs them and mutates the list so you're
basically getting all these different
actions that you can take with the
browser and running those actions and
seeing how they interact there's another
test type called interactive UI tests
which are necessary for the windows bots
in that there's these tests interact
with the UI by clicking or moving the
browser around and the bots themselves
have to be specially configured to take
advantage of this the last one is PI
auto which is written by you can write
it's a Python interface for writing
automation tests it's a really easy way
to run to right UI tests in Python so
we'd have an API that loads a web page
with the form that we've written the
next AP I could fill out that form with
data that we specify you would have an
API that clicks the button using the
actual OS infrastructure for clicking
programming the cliq 2
and on a button and then at that point
you check that the info bar shows up as
you expect the next API could also use
the clicking API to simulate a clicking
on the Save button or the don't save
button for the save button you want to
make sure that the data saved there'd be
another API to say is their profile data
does it match what we expected and if
you click on the don't say button you
also want to check that the CC data
credit card data was not saved so
whenever you're running your tests and
it's not working out exactly as you
planned maybe you get a crash or it's
just not functioning correctly you
should be able to do both the tests on
mac and windows it's pretty easy to
debug tests using the visual interface
whether it's Xcode or Visual Studio you
just compiled the test target and then
debug that test target in Linux it's
possible to do that but most cases
you're going to use gdb so you pass in
the gdb and then the test target itself
and then pass in the G test filter of
the tests you want to run with args then
you should be able to step through the
tests and figure out why the test isn't
working like you think it should we have
a few frameworks for writing tests that
make test writing a lot easier G test is
the biggest piece of how we write tests
and it has a lot of functionality for
you you can check out the link at the
bottom and there's a lot of good
documentation on that I'll give an
example so the high level way you look
at tests in G tests is that you have a
test program which would be unit test
binary browser test binary UI test
binary that's your test program your
test case would be autofill info bar
tests which has many tests inside of it
each test testing a specific piece of
that functionality whether it be a
method on the object interaction with
the browser or a high level into nui
test as an example of the g test
framework it's really easy to set up a
test you have the test macro the test
case name and then the test name itself
and this one we're using assert equal
which is a macro used to to check that
something is how you expect it's an
assertion so if the if the thing that
you're testing is not valid the test
itself will stop we also have expect
equal which will not cause the test to
stop if the thing that you're checking
is not true something that you can use
to to reduce duplication of code our
test fixtures they're a data set up for
every test that you run in a test case
I'll show you an example of that so cue
test is the name of the test fixture we
inherit from testing test there's two
methods that you could possibly override
set up you usually want to override and
set up is run at the beginning of each
test case and this set up we are in
queuing some values in 2q several queues
for this and for this test case we don't
need anything to be torn down but for
example if you weren't using scope
pointers and you had allocated data and
teardown is where you'd want to destroy
that data so using the same q test test
fixture we have two tests here is
initially empty is empty initially we
just expect that the queues are empty
and they r DQ works we load up data in
our test fixture so if we go back here
we have the in queues of each of the
queues and then you expect that udq the
value you get back there are a lot of
sessions that you can use this map gives
you quite a few of them the asserts and
then there the fatal versions are the
asserts as in they will stop the tests
and the tests won't run anymore usually
want to do these on things that will
cause the test to crash in another way
that you don't expect for example
asserting that a vector is a certain
size you want to make sure because then
you're going to access all the elements
of the vector so you don't want to
access those without asserting first
expect the expect assertion
won't cause a test to crash they'll just
cause a test failure if they're not if
they don't evaluate to true another
piece of framework that we use is google
mock google mock objects allow you to
provide an object that you don't want to
implement fully you just want to specify
the interaction with that object for
example in this one we're going to will
start a personal data loaded observer
mock so this is an observer in a
messaging sense and what we have here is
we're going to mock out a method on
personal data loaded which will be
called by some method or by some class
and the action that we expect the tape
is take is quit you I message loop we
have some insertions inside the action
and whatever action we need to take so
we're going to quit the current message
loop in this action in the test we
create a profile and we set the profile
info so inside set profile info the
personal data observer is going to be
called and we expect that call to happen
we didn't have to create a whole
personal data observer though it's just
a mock top object with a mock method we
don't need we don't care anything else
about the object itself just that this
one method gets called so we have expect
call on the observer object that we care
to be called and then we say we'll once
quit you I message loop will once means
we expect it to happen exactly once and
we expect the action that be taken is
the quit you I message loop there are
will repeatedly which means after the
call to set profile info the method that
you're expecting the action that you're
expecting to happen could happen several
times and that's a good way to specify
that at the end of this piece of code we
need to run the current message loops
because we've queued up this call in the
message loop and then we need to get
back to that message to be called so we
run the message loop we have several
tools for testing one of the most useful
ones is Val grind which is
memory error checker and so min check is
a tool that checks for things like leaks
uninitialized memory things of that
nature using the wrong delete if you use
delete instead of delete array when it
should be deleted that will notify you
of that and then you can see the command
to run it right there there's also
thread sanitizer thread sanitizer
detects data races and command to run
that it's down there as well mostly
though you don't have to run these
yourself in that we have vowed brine
bots on the main waterfall which
continuously run these tests they take
about an hour and a half to run and they
will catch the most they will catch
these error for us and then once we do
we file a bug and we notify the owner of
the code that they have a memory error
so then you can run once you are
notified that you have a memory error
you can run the test yourself to debug
it an important part of testing is code
coverage something we need to keep in
mind code coverage is how much how many
lines of code are actually executed by
your tests so obviously the higher your
code coverage the more likely it is
you're going to catch errors although
you can never get I mean you can get to
100% code coverage theoretically but
even if you do on average you only find
about half of the bugs because they're
the number of input and output is just
too much to to test so there are three
pieces of information with code coverage
the number of lines instrumented which
are the number of lines that are
actually compiled into your tests
including testing code and source code
the number of lines that are covered
which are lines that were hit when your
test when you ran your test the missing
piece of information is code that was a
part of the source code but is not
actually compiled in and this is
something you want to take into account
for the different box for example on
Linux if there was a windows-only test
that we don't specify as windows only
and it's not being compiled on Linux the
instrumentation will think that these
are missing lines because they are and
that will lower your coverage
we actually want to have the most
accurate coverage that we can possibly
have so there's a way to go in and say
this is a windows-only test we don't
need to run it on linux or mac so don't
count it against us incremental coverage
is a way of testing for each commit in
the in the tree how many lines of code
are tested in that commit for the
additional line so any plus line in a
diff how many of those lines are covered
ideally we want to have fifty percent
incremental coverage so in any commit
you have of all the new lines you want
fifty percent of those lines to be
tested at that point you're at breakeven
you're not going to be losing coverage
but you're not gaining any more coverage
at some point we'd want to bump that up
to 75 percent but fifty percent is a
tough enough target as it is we're
currently working on adding the ability
to track incremental coverage and
setting that fifty percent target so
that it will be a bot on the main
waterfall that will alert any any bill
that has a commit in it that is not
fifty percent incremental coverage the
bot will go red and the owner of that
test will be or the owner of that commit
will be notified that they didn't have
fifty percent incremental coverage and
we're working on that we're getting
pretty close to that we do have a try
bot for coverage up there's a Linux spot
available not right right now and once
we get more machine infrastructure we're
going to get a mac and windows bot as
well the jobs take about an hour and a
half but it's a good way to see a good
way to find out how much your test
affects your code how many how good is
your coverage for the tests that you
just wrote there are there are three
BOTS up right now that run coverage
continuously even if they don't have
incremental coverage there at the there
on the experimental waterfall which is
the link at the bottom of the page if
you go to that page and you search for
coverage on the page you'll see the mac
linux and windows coverage those will
give you our current coverage analysis
and you can look at any code that you
currently work on to see how good your
coverage is one thing that crops up
quite frequently with the project of
this size are failing tests and failing
can mean a lot of things a ling could be
an expectation or an assertion doesn't
pass
the test it could be a crashing test a
hanging test all of these are bad things
but some are worse than others the fact
that we're disabling tests is not a good
thing because disabled test doesn't run
at all and so it's effectively dead code
at that point there are two types of
tests that need to be disabled and those
are crashing or hanging tests and we
disabled those because the rest of the
testing infrastructure does not continue
to run so for example with unit tests if
you crash on the very first test that
you run in the unit test you're not
going to run any of the rest of your
tests which is the worst so for those
it's okay to disable to be even more
specific you should use
platform-specific defines that say only
disabled on windows because it's only
crashing on windows and you want to be
as specific as possible when disabling
so we could still have continued
coverage on Linux and Mac whenever you
do disable a test or whenever you change
the moniker of a test to be disabled
fails or flaky you want to make sure
that you add a comment to the code file
a bug and then add that bug to the
comments so that whoever comes to look
at the test to possibly fix it they have
that bug for reference which should
probably include the log to the failing
test there's a flaky moniker which we
should use for flaky tests flaky tests
fail spurious ly so they might run green
for a couple times and then fail and
then go green for a couple more times
they just fail on and off there's also
fails which is used for tests that fail
continuously the infrastructure behind
flaky and fails is actually exactly the
same the benefit to the developers are
that you know right when you look at the
code okay this test is always failing
there's some air that's continuously
happening or there's some sort of flake
in the system it's unknown but we're
going to have to look for a flaky issue
here there's a way to disable or to
change the monikers for several tests at
one time so say you have a test that or
several test cases that are only failing
or crashing on Mac OS you can use the
Skip mac OS macro which will take a test
name and then to say disabled underscore
chest
and you use those to pound marks to
actually put the name of the test in
there and then for every test that's
failing for that platform you say skip
mac OS and then the name of the test one
thing you don't want to do is if there
if there is a Val grind or thread
sanitizer air you want to make sure that
you don't necessarily disable it at the
code level if at all possible for
example if you have a leak and the Val
grind bot is red because of your leak
you shouldn't disable your test you
should either add a suppression which is
most likely for a leak or if the test is
crashing when only run under Val grind
you should go into the Val grind files
which they have a list of tests that
should not be run just for Val grinded
but thread sanitizer and you should add
your test to that list so in summation
we need to write a lot of tests we need
to then write some more tests we have a
lot of tools that you can use make the
most of them especially Val grind thread
sanitizer if you have a failing test
that is failing under one of these tools
use that tool locally it to TT buggin we
need to take care of coverage to make
sure that our features are as covered as
possible for the things that they need
to be covered for and whenever a test is
failing really think about whether you
should be disabling it or marking it as
fails or flaky so that we keep our
coverage numbers up so that's about it
do we have any questions
okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>