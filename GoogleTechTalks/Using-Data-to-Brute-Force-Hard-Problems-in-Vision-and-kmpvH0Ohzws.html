<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using Data to &quot;Brute Force&quot; Hard Problems in Vision and... | Coder Coacher - Coaching Coders</title><meta content="Using Data to &quot;Brute Force&quot; Hard Problems in Vision and... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using Data to &quot;Brute Force&quot; Hard Problems in Vision and...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kmpvH0Ohzws" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">afternoon everyone I'm pleased to
introduce to you aliens UFOs today he's
from CMU and he's going to talk to us
how to do cool things with a lot of
images inotia thank you very much Thomas
I'm very happy to be here I mean lunch
is amazing and also it's just great to
see so many of my friends here and it
just yeah it's like a big reunion and
thank you very much the Thomas for for
graciously inviting me and hosting me so
yeah so i'm going to i I just almost
caught me on my way back from siggraph
and so the talk is very very raw and
very lots of new stuff so I apologize
for you know kind of jumping around and
not having a very coherent story but
kind of more of raw core dump of newest
stuff so please forgive me for that I'll
polish it up later for the next sock um
so yeah so the story is that we want to
solve hard problems in vision and
graphics but we don't want to work too
hard and we don't want to actually do
all of the hard things that that you are
supposed to do you know kind of AI
complete things so the question is can
we just brute force our way to solving
these hard problems and I think this is
a very good place to talk about you know
brute-forcing hard problems with google
is exactly the right the right audience
so what are hard problems well the main
problem in my view is is that we are
robbed of an entire dimension you know
we have this beautiful 3d world out
there you know life and then smells and
sights and sounds and all this amazing
three dimensional reality in a bump
we've plop it down onto a
two-dimensional plane you know whether
it's a painting or the back of of you
know CCD RA or the back of your retina
the same thing happens you basically get
will we lose the dimension okay we will
lose a lot of the things that were
obvious in 3g and we end up with
something like this where
are you know things are not as easier
and and then we have the audacity to
actually reason try to reason about this
this um 2d representation in in a way
that makes sense only that if you're in
the 3d world ok so the prop we are made
our problem especially hard on ourselves
because we are we only have two de
representations and then we want to
reason about them in the in the in the
context of the 3d world that we all know
and love except you know it that
information is gone not only it is not
just a hard problem it sits in an
unsolvable problem because getting back
at the three three three dimensional
representation from a from a single 2d
projection you know you cannot do that
because you can have a bazillion
different possible 3d representations
that will give you the same projection
ok and that's kind of the big you know
the big vision the hard vision problem
right there ok and in fact you know
there is nothing you could do if we
lived in some kind of an abstract world
with you know pieces of metric and
floating in mid-air right then then we
wouldn't be able to do anything at all
the problem would not be solvable at all
luckily for us our world is very
structured ok our world has is this very
small subset of all the possible sci-fi
worlds that could have been out there in
our world you know have planes and
planes are great things the fact that
you don't have to jump from particle to
particle but actually you know can walk
on a play that's wonderful and the fact
that the plane has a particular
orientation because of gravity you know
it's usually either horizontal or
vertical there are some exceptions but
you know 99 percent of the time
horizontal vertical um there are
consistencies in other senses like you
know illumination most of the time
illuminations from above that's why you
know when you put a flashlight like this
you don't recognize people because
that's not typical and so there is a lot
of things about our world that are part
peculiar to our world and that is how we
humans can deal with this unsolvable
problem of trying to understand
derstanding what is it that is the that
is being depicted what is the
three-dimensional world or even a four
dimensional world because we do also get
a sense of time that's being depicted
just from this two dimensional signal
okay and the the way that you know that
that vision researchers approach this is
through learning the idea is that we
have our world we somehow you know use
that as training data and then we can
use it to to interpret new new data okay
and so the classic of classic story that
is kind of the understanding of the
visual world world the learning and
vision slide this is stolen from Chris
burglar is that you know we have a bunch
of training daber data we capture it
okay and then we try to somehow squeeze
out all the interesting bits out of it
we try to squeeze out the in the
informative part to model what is it
actually represents and then once we
have the more once we will have a model
we can do various things we can do image
understanding we can synthesize more of
this you know all of the nice things are
available this is kind of your your
typical machine learning pipeline you
have lots of data you squeeze out all
the unimportant bits you get you using
you you only keep the important ones and
then you know from that you can do lots
of lots of things um the problem is that
this part is really really hard how do
you know which ones are the important
bits and which ones you can throw away
and this is the heart of the machine
learning problem and it's it's you know
the whole conferences and careers of
people devoted to exactly this step how
do you you know understand your data
understand what's important about your
data okay and you know those lots of
smart people try to do that and and I'm
not one of them so I'm going to do
something much simpler i'm going to
sidestep the whole problem and do it the
Google way don't worry about understand
what's important just keep around all
your training data and do look up okay
that's exactly the the Google solution
looking up there are the world instead
of trying to really reason about it and
it turns out that it has a lot of
benefits not just for text but i believe
for visual data as well so the Google
solution so I might I wonder if you guys
would you know take up this this claim
of mine I claim that AI is that that
Google is the AI for the postmodern
world so post-modernism in art is you
know is the idea that you know
everything has already been invented
everything has been created there is
nothing new Under the Sun and the kind
of the the natural extension for it is
that well you don't that means they
don't have to create anything anymore
you just look it up okay so so all
questions have already been answered
many times in many different ways and
the reason what Google works is just the
intelligence is sitting there all over
the world on their servers and you just
look it up okay that's that's kind of
the Google story that that works so well
for you guys and my main research
interest is to see if the same type of
of power of data could be applied to a
much harder case of visually okay and
why is it a much harder case well noise
is an obvious example right so you know
you might think that your data is noisy
but it's not really I mean you have you
have characters for example right you
have like ASCII codes for every letter
you have the most of the most of the
words are really you know spelled more
or less correctly there is huge amount
of noise and visual data every pixel is
is going to be wrong bye-bye a lot um
again high high high entropy there is
just much more stuff you know in in an
image than in a text right the UK the
reason why images take up so much space
then then text is because there is just
there is lots of stuff there there is
lots of junk as well as lots of signal
of course you know being too d is adult
also is a is a problematic you know the
2d signal is trickier than one day
signal and and but yet you know we want
to we want to see what could we do
something in that area and and
especially you know how much data would
we need you know is it possible to do
something reasonable with the amount of
data that is you know obtainable
currently and downloadable you know when
a storable at at at a normal research
lab that is you know is it is it is it
how much how much do we need you know is
that millions and billions of the gas
aliens because the problem is so much
harder obviously need you need more data
but how much more nobody knows so that's
kind of the the questions that I am
interested in answering and I'm going to
present a few kind of you know very raw
projects and i have to mention all my
collaborators up front so i have a
wonderful set of graduate students Derek
home who just got his PhD this morning
yes and he's going to UIUC and he's a
he's a really wonderful guy and james
hayes and jean-franÃ§ois are two of my
other graduate students who are gong who
are been working a part of the stuff i'm
going to talk about and marchelli bear
is my colleague at at CMU who with whom
i have also collaborated a lot and and
so the CMU gang is great but i also like
to travel so there is this kind of a MIT
oxford gang egos of civic who you might
have heard of the video Google Guy Brian
Russell bill Freeman and Andrew sermon
on one of the project and then Cambridge
Microsoft Cambridge guys helped out on
and some graphics project Carson or
author John Wayne and Antonio criminy
see and I will i I'll give references to
actual papers later so the I will talk
about two parts of the of the slots of
data story first part is what if you
have labeled data the easy case okay
somehow you got labels
and somehow is actually thanks to
Antonio criminy she and Brian Russell is
this label me data set that they are
writing it's essentially people upload
photographs or we know that Tony
applauds photographs and I give him a
bunch of photos and then you go in and
before you can download the labels you
have to label a few objects okay and so
you get this kind of political labels
and you enter a text so it's a very much
again ever a low-maintenance vision
science research endeavor but they are
actually doing pretty well they they're
up to something like you know 160,000
objects right now and and it's becoming
a really really nice data set and and
we're very happy that they these guys
have have done it so what if you have
that kind of data you know no object
classes people can type in whatever they
want but reason the book reasonably good
segmentation okay so one thing that that
that we could do is we could um we could
say this well if we have if we have an
image and we have labels we have labeled
people in that image can you estimate
their physical heights okay in you know
in the world you have them labeled in
the pixels can you estimate their
physical height if you have a picture of
me I want to know you know how I am if I
want you know if somebody wants to buy
me a t-shirt the Google t-shirt you know
they want to know what size it okay um
well turns out that this this problem is
pretty simple if you assume a bunch of
things that that we have become shown
before is actually not such a bad
assumption if you assume that really
what you want to estimate is you want to
estimate the camera parameters relative
to the ground plane so you assume that
everyone's on the ground and basically
all you want is your you want to assume
you want to estimate the height of the
camera and the orientation like this
okay turns out that if you have a bunch
of people and you know you can go to the
national center for health statistics to
figure out how tall average person is
and you have labels which says these are
people it's it's pretty simple to
estimate the
orientation of the camera for this
particular image okay let's just not
that hard ok well now we have the camera
pose for this one image in this huge
label me data set ok now what well what
we could do now is say you have another
image which has some people ok and some
other objects say cars ok we can use the
people to estimate our rough camera
parameters and then we can use the
camera parameters to estimate the wrath
hi distribution in the real world of
this new class of the cars ok now that
we have this information we can go
further and we can say ok now that we
have don't have people but we have cars
we can use the cars to estimate
orientation and then we can estimate the
orientation of a new class see a van ok
and basically what we do is we do this
recursively for the entire data set and
eventually we get to estimate a whole
bunch of physical heights of objects and
estimate orientation of the camera for
every one of the images ok so for that
this is an example of what we could do
now there are a bunch of people in this
image they are labeled in pixels and if
you put them you know on the kind of a
lineup you can see that the people
farther away are shorter than the people
close up but with our little trick which
is very simple we get something that
will not only estimates the relative
height of the people but actually even
gives you their height in meters ok and
and we have done it for a number of
classes and the nice thing is that you
know you can look up you know what's the
standard height of a parking meter or a
fire hydrant and you know women a man
there are statistics for how hi there
and we're like within two millimeters of
the correct mean answer so it's really
you know it's not that we get it right
for every image but on average we're
actually doing pretty well ok and so
this is this is kind of a nice
kind of a dated driven way it's you know
it's all very simple it's it's it's a
little little piece of thing and in in
Derek's thesis but it shows you that
really you don't if you have a little
bit of labeling a little bit of this
labeling can go a long way to kind of go
from the world of the 2d image and a
step up into the world of the 3d see ok
and you know what what else we can do
with this I'm not going to talk about it
but this is the our CPR or six work
where we have we can detect objects
orientation of the geometry of the scene
and the orientation of the camera and
then we can do things like synthesize
you know top views of scenes from a
single image we have a overhead view of
everything that's on the ground and then
we can even label you know where the
cars are sitting and where the
pedestrians are working and things like
this okay and this is this is again kind
of trying to get get out from the
tyranny of the image and into the space
of the scene okay now what else can we
do from a single image well what we can
do we humans is we can look at this
image and we can say roughly what kind
of whether it is well we can even
estimate you know what though should I
wear a jacket or not you know and
whether I should wear a Sun Hat or
something like this you know there is a
there is enough information in this
image for us to know the raft kind of
elimination of the scene how can we do
this for a computer well the way to do
it currently in computer graphics is
actually the method proposed by Thomas
and eyes office mate Paul de Becque is
you take a shiny sphere shiny mirror
sphere you place it in in your scene you
take a photograph of that and the shiny
sphere basically reflects the entire
sphere of light incident at that point
and then you use that for what the
graphics people call environment map and
then you can use that information to
relight or to estimate the
the elimination or to even relight
objects but this involves you know
finding where this picture was taken
from we don't know there's some google
image photo and then you know traveling
there and then taking a picture of the
of the shiny ball so with but we humans
can can actually do the sum of that just
from the single image so what can we do
we propose what we call illumination
contacts and the idea here is that we
are going to approximate the very very
rough illumination of the scene by
dividing the image into three parts the
ground stuff that's illuminating from
below the sky you know very important
for our outdoor scenes and whatever's
left the vertical stuff okay and we are
going to basically capture some very
coarse brightest information from these
three region in kind of a very coarse
environment map representation okay and
the way we're going to use the do this
is we're going to use our old geometric
context system to estimate the ground
vertical and sky components of the image
and then we're going to basically just
throw it into a color histogram and
computer you know join color histogram
and have that as our presentation for
the image okay now all the graphics
people will should do yell at me at this
point and say this is this is all wrong
because environment maps are supposed to
be high dynamic range they're supposed
to know exactly how much light is is is
your you're seeing and you know this is
0 to 255 so you're a photograph is has a
very limited dynamic range so this is
completely useless for an environment
but the point here is that we are not
using this to relight objects when we
are not even using this to estimate
anything really we're only using this to
match to find all other images that have
you know the same illumination the same
kind of the same weather and that seems
to work actually pretty well so here are
some query images this is kind of you
know overcast day this is a morning and
this is a noon day and you can see the
nearest neighbors using this
presentation as query and you can see
that actually the the results mostly are
quite quite good despite you know all of
the all of the shortcuts and all the
problems that that we have experienced
and you know one reason for this is
basically when you have a lot of data
whatever things you know whatever
shortcuts you do in the query image you
know the same shortcuts would be found
somewhere in your data and you will
still be okay okay now here is a very
cute application with that we did with
the the Cambridge gang I say you want to
install objects and images so it turns
out that you can you know you can do
this in a very simple kind of image
based way so you know we don't want to
find shiny spheres and take pictures of
them and we don't want to use this kind
of a you know joy clipart example so
what we came up with this we call it
photo clipart something in the middle
photorealism on one hand but ease of a
clip art library or another hand and so
the ideas I'll just kind of sketch it
very simply instead of saying I want
this car in this scene go do it which is
a very it's impossible problem because
the camera orientation is wrong the
illumination is wrong you know it just
it's it's not going to write no good and
and I really don't see this as you know
being I don't I don't think it's
possible to solve this problem without
you know without solving the full
computer vision program actually i
understanding this is a car and you know
getting the 3d geometry and stuff like
this but there is if you we just really
find the problem and then it becomes
much better we say we don't want this
car we don't we just want a car and we
have a whole bunch of possible cars so
now our problem is to find out which of
those cars would actually look good when
pasted into the scene and this turns out
is actually a reasonably doable problem
um so the idea is that for every object
in the label of my data set we annotated
with the camera parameters that I talked
about and the illumination context which
I also talked about and then we
basically have kind of a Google
the google model you know we have all of
our data with annotated parameters we
have our query which is the image we
want to insert stuff into and all we do
we it's not even automatic it's
basically you know the front page result
of Google will resort the data and give
the user the top matches that we believe
based on the estimates of the geometry
elimination here and the estimate of
geometry machine here we believe we the
computer believe would has a high chance
of looking good okay and that's that's
really this really then we have a nice
user interface and then we can get stuff
like this so this is the kind of Abbey
Road you know improved so again there
the you know detail so the paper but
then you can you can really play great
tricks on your insurance company and you
know say that you've had a horrible
accident okay or you know you can even
add a traffic to your to your images
paintings you can add objects to
paintings and it looks kind of good this
by the way big thanks to to google for
helping us out we got this painting and
we forgot to record the URL and so we
were we didn't know how to get to ask
for permissions and then and then Henry
graciously did some magic I don't know
what it was but you know we got the URL
so thank you very much we have recovered
acknowledgement to Google you know about
how Google that helped us finding tough
to find our needle in their haystack so
here's an alley scene and here's some
bunch of American cars it doesn't look
that great i have to say but the reason
is semantic you just don't see such huge
cars in Europe okay nor parking meters
actually and this is going to be the new
gates gates hall at CMU for those cm
yours here so that's you know you have
no Simon here Smith hole here and this
is this weird thing is going to be the
gate center and you can see that you
know that this is a rendering that the
architects gave us and they they believe
that all
all CMU students look like that kind of
a shadowy figure so we thought that this
might might be a better representation
of how lively the campus is going to
look like and so that that's like one
potential application of this stuff okay
and of course sometimes it just doesn't
work and so for example here you know
the database just doesn't have enough
images of kind of night snide with snow
seems okay but you know as you all know
there is though you know there is no
algorithmic problem here the problem is
obvious you know I just need to get more
data right and then and then things will
just and that's kind of a refrain that a
lot of the problems are going to be
solved by data okay yeah oh I'm sorry
yeah uh can I say something about the
shadows I can but after the talk because
I still have lots of stuff we basically
we do the google shad solution to
shadows too we steal the shadow and we
transfer it over that's that's what we
do with the shadows we detect the shadow
and then we just transfer it over into
the new thing and we even if even if
shadows point the wrong direction so
shadow is very important right but if
you look at Renaissance paintings you
realize that the direction of the
shutter is really not important you
could have a you know a window and sun
shining for the window and then the
shadow going that way and nobody notices
so basically we just steal the shadow
from the original place of the object
and just stick it in and nobody cares so
yeah but but I I can talk to you later
about about that I yeah I just want to
kind of show lots of lots of little
little teasers about stuff so now what
if you have unlabeled data so you think
okay now this is this is going to be
horrible you know if you don't have any
labels what can you do but turns out
there is some things that you can do so
this is this CMU MIT Oxford mega project
and the idea is you have lots and lots
of images can you find can you discover
objects in that huge collection of
images okay I'm not gonna tell you what
if airplane looks like I'm not gonna
tell you what a wonder
it looks like i want to just you two to
look for other objects and discover
repeatable patterns right because in a
way one way to define what an object is
an object is some visual pattern that
you see over and over and over and over
and then by the time you've seen it for
a hundred time you say okay I better
think of a name for it because you know
it just it's too much right I just need
a short neighbor and that's how that's
how you know words come about right so
this is our definition of of a visual
object there's some stuff that's
happened so often that you just give up
and give it a name okay so we want to
discover that those types of repeated
objects in unsupervised way from a huge
data set okay and the idea here is um
our approach has two intuitions so one
intuition is this idea of multiple
segmentations so the idea is that you
know all segmentations are wrong you do
you apply segmentation to an image and
chances are the results will be really
bad but some segments in some
segmentations are still useful okay and
we have shown that with Derek for a
previous world but here it really comes
into relief here is an image and here's
a bunch of segmentations which we run at
with different parameters and you can
see that a lot of them are junk but then
there is this one segmentation right
here where we get our car as a nice
segment okay if only we knew that that
was the right thing we would be all set
but at least it's there at least we got
a segment that was good okay and this is
where intuition to comes in and this
intuition is we we have a citation to
Tolstoy we're very proud all segments
all good segments are alike each bad
segment is bad in a different way okay
those who read Anna Karenina should
should appreciate this right the idea is
that if you have a segment that is a car
and then you have another image where it
also you happen to Sigma Tau the car
that segment and that segment you know
they should have a lot of things in
common they should be very similar okay
whereas if you have a segment that is a
bad segment you have a piece of a car
and a piece of a road it's very unlikely
that somewhere else you'll find another
segment that will be that same piece of
a car and that other same
piece of a road so noise conflicts with
each other where a signal it not agrees
with each other and that's the idea the
idea is that the good segments they
should cluster the good segment should
be like each other the bad segments
should not question and that's basically
just told you the paper so the idea is
we produce lots of segments for each
event of our images and then we
basically try to find clusters within
that space that soup of segments and the
tight clusters turn out to be reasonably
well a segmented objects and here are
some examples so this is the MSR see
data set and this are some of the
clusters that we have automatically
discovered these looks like cars this
looks like some kind of a road signs I
guess this is some fields I guess this
is sky this is windows those are I'm not
sure I guess bicycles with some
background it's not perfect but you can
see that it's actually doing something
reasonable here is the culture of data
set and the contact this is really
really simple so we do pretty well we'd
get their motorbikes we get faces we get
a couple of car clusters I guess kind of
dark cars and light cars you know there
are some things that just visually don't
look the same but you have to use
semantic knowledge but anyway this is
this is kind of a first step and in a
one problem of course is that ok we're
going to use a huge data set but really
the data sets are all on the order of a
few thousand images so it would really
find to see what happens if we if we
really let a trip on a huge huge data
set ok and now how much do you have ah
alright so now I'm going to tell you
kind of the more the most Google liest
of my projects that I'm very excited
about this has just been presented
siggraph this is a work of james hayes
one of my students and nominally it's a
work about filling holes and images
right but I think there is a larger
story so the normally the story is that
you have this nice european alleyway and
then you have this car that looks just
completely out of place let's get rid of
it ok ok done ok well not quite because
now we have this stupid
tall in the image and we need to somehow
patch it up right so what do we do well
the first thing of course we run and we
we go and we implement our old work with
Thomas that actually you know does a
pretty good job back back in the day in
nineteen I know it it does a pretty good
job filling in holes in textures or even
extrapolating textures okay but if you
get something that is non-homogeneous
like a real image that is not really a
texture then you'd you know you don't
get anything you get basically a mess
and then it shouldn't really work
because this is really not a texture
okay um there is a full of work by
Antonio community who says well if you
do efforts along but you try to
propagate a high edge boundaries first
kind of do kind of a little bit of
mid-level grouping you and then fill in
the rest then then you get better
fulfilling results even for real images
but unfortunately here the the object
the hole is still too large and in fact
you know it's propagating the frame of
the door in the completely the wrong way
right and and some Microsoft product
based on that is also not doing too well
here okay so but what's the problem
what's the trouble the trouble is the
semantics the trouble is that you know
we know that that that what should be
here there should be some kind of a road
okay so what what if we give up and say
okay let's let's up the user let's say
okay let's query for something that's a
road okay and there is a couple of
recent work which I think are really
interesting and exciting one kind of
this user guided image compositing an
image image search and compositing and
the idea is basically that you kind of
you sketch of this world for example is
that you sketch what you want and you
put labels you say I won't road here in
the car here and then it goes to a
database of you know labeled stuff and
that kind of puts puts together an image
for you okay so that's I think it's kind
of a very very interesting user-driven
way to solve this problem but what
do we really need the human in the loop
can't we just use the fact that we know
what should be here why we know it
because the rest of the pixels the rest
of the image tells us well this is kind
of a no building on the left building on
the right and of an empty space looks
like an alleyway looks like something
like this you're right and in our mind
we say yeah this is this is kind of the
scene that is probably here and and and
here you know you have you have a road
so it that's where we get our intuition
from this kind of semantic
representation of the image so now we'll
ask what what what we could do is we can
say well if we could find other examples
that share the same semantic structure
they doing on the lab building on the
right some space in between then we can
go in we can find the appropriate piece
and we can you know fix it up fill in
the hole from one of the one of the
images sharing the semantic structure
okay so that's kind of the idea but you
know how would do we do this how do we
find how do you find other Ali's like
this well we can go to the best search
engine in the world and type alley and
not get anything useful unfortunately um
and this kind of you know I feel this
this is not really it's not the problem
of Google it's the problem of you know
an images is worth a thousand words so
you have to type in a thousand words to
get the image that you want right and so
instead why not skip the text part at
all and just try to go straight for
images take a bunch of images you know a
thousand ten thousand a million a
billion and see if we can find good
enough matches semantically that would
do what we want and the question is your
how many do we need is it is it a
thousands is that millions is it
billions is it even feasible to do this
and they're kind of a cool answer and
somewhat unexpected answer for me at
least is that in fact you can you can do
a reasonably good job this is our result
of finding
reasonably close image and steal the
information and fill it in the hole and
i'll just briefly show you our algorithm
so i will start with a simpler image for
illustrative purposes so this is I was
hiking in the Ligurian coast and you
know there's this house you know in
blocking the view so let's get rid of
the house this is going to be the input
to algorithm now we want to fill in the
hole okay so what do we do first we have
some kind of a scene descriptor a course
representation of what the scene should
look like you know basically throwing
away all the all the unimportant stuff
and keeping only the essentials then
we're going to query into our large
image collection okay from that we're
going to get about 200 best matches and
for each match we're going to see okay
does it does it fit well you know can we
can we align it to fit well into this
hole and then we'll give the user the
top 20 Kimmage completion seem
completions that our system was able to
come up with and then the user can
decide which one they like the best okay
so that's that's the story so what do we
do the first of course is the data data
is the most important okay so we got 2.3
million unique images from Flickr and
the nice thing about Flickr is that you
get kind of whole seeds you know you
don't get close-up of objects usually
you get you know nice big scenes which
is what we really want to work with and
what's 2.3 million well if it displayed
like this we'll have to you know watch
an hour to see all of them so I think I
better move on and just now we need to
have a descriptor for this image right
we we want to we don't want to query on
every single pixel that's too much and
it's also you know we don't need to
compute all the record all the ripples
in the water or you know all the little
houses in there in the background there
we just want kind of a course
representation so you know what we do is
kind of a standard thing to look at the
oriented energy at different scales and
orientations of this image and then we
course the top in there four by four
grid and this is in fact the
see in just descriptor that all event
trauma proposed back in 2001 well
basically capturing very course
orientation information for the image
and here we are being trying to be very
careful to make sure that the places
where we don't have data you know we
don't put anything in there so that you
know it the missing data part is left
blank okay great like this and we add to
it also some very basic caller a
four-by-four caller information just so
that we get some color as well okay and
then we search we search our 2.3 million
database and we search it in brute force
because we have missing dimensions we
can't really do any of these fast speed
up searches well I guess we could but it
we tried a bunch of things and it didn't
quite works if anyone has any
suggestions to be very welcome but you
know it's basically not so not so
horrible you know have a bunch of
parallel machines and we just searched a
brute force four nearest neighbors 200
nearest neighbors here is the top 50 or
so and and you know it's it's nice
actually I think because you can see
that it's um it's getting something
that's similar to what we started with
which is which is kind of almost magical
okay and here is for example one of the
matches and now we say okay now let's do
some little local alignment to make to
find the best place where it would fit
so we take the kind of the context
anyway you know do simple searching to
find where it would fit best okay and
once we do that we align the two images
okay and then we do some graphic art and
costs on bail blending basically
standard graphic stuff that would blend
between those two images in a nice way
and this magic is done and we get a
result like this okay hmm so what we're
doing is after that after we do this for
every one of those 200 images weary
ranked the results based on the scene
matching distance plus the context
matching the local context magic plus
the the cost of the graph cut okay and
then we give the user the top 20 results
okay and here here there and
again this is like the first first page
of results in Google that you know you
can you can try to to pick what do you
want and some of them don't really make
any sense and some of the mean a lot of
them do what you would expect it to
basically fill stuff with water which is
which is good right just kind of a
little bit boring but you just fill
things with water ah or you have a
little bit of the beach here there are
other issues is scale issues yeah yeah
not not quite right not quite right um
you know this is not a bad one except
it'll it has something stuck in it and
that's what we want to get rid off in
the first place so this this is the one
that I I particular like the best and
notice that you could not do this with
kind of a texture synthesis method
because you know where did those boats
come from right and and in fact you know
this this thing gotten on BBC somehow
and then some British guy writes me I
don't understand how you could do this
in Photoshop because where do you get
all the votes from so I said no we don't
use Photoshop um we use data it's okay
so what's the value of kind of giving
instead of giving 20 years old store
user why not just give one but well
sometimes all the results are kind of
useful so for example here's here is an
example we want to get rid of their of
the road and the bus and here are the
some of the results and you can see that
they're really really different I'm
really not sure which one is
you know the right one hmm so so here
I'll just go for some of them more
examples here we have an image and we
want to get rid of this facade okay and
here is the hole that we filled and it's
another facade it looks quite good so
let's look at the under the hood and
what we see is the matches the UM the
best matches and you can again you can
see that actually just right here is
it's actually matching a lot of stuff
it's matching kind of vanishing points
and there and the two planes but notice
that it's not really that just is so
smart but just is just recording a
little edgelits it's then you have lots
of data and the data is such that you
know even on small very stupid
descriptors you still get some good
matches okay so from here we find this
one is to be the best match and here is
there is where we stole that and you can
see that it's it's a pretty good
semantic match and there you go we put
it in in the hole okay here's another
one this is a James's picture of Leon
and the crane is not the nice site so we
get rid of the crane and you know we
fill it in with something nicer let's
look at the scene matches here are the
matches and they look great and then you
look closer and you realize that
actually like four of them don't have
water but have payment instead right but
the thing is that even for us it's hard
to tell so you know it does what it's
supposed to it does this kind of a gist
match it kind of course matching and
this is this is the best match and they
are really really close it's kind of
really neat that they're closed and so
it's not surprising that they you'll fit
in very well here is another picture
from park in your CMU we want to get rid
of this things and you know we get
a little bit more exciting image here
you know the skies are overexposed so
you know you can go and shoot another
photograph but who cares you know just
get rid of the sky and get some more
interesting skype here the hilltop is
not looking too exciting we get rid of
that and we have a new hilltop okay here
you have some development on the beach
but but you know we don't like that in
our photo so we get rid of that and the
pristine beach is resumed ah this is a
picture from Leon and the James took on
his holidays and he was very bummed that
the cathedral has the scaffolding so he
wanted to get rid of scaffolding and
this is a result and we looked at it
that just looks too good something must
have gone wrong what's what happened
then we go to see what other matches and
it's all very nice because you know most
of the matches are nice european cds
from above so it's you know it
understands the concept of european city
somehow but the best match is this one
which actually is exactly the same
vintage point in leon found somewhere
else liquor just in a couple of years
earlier before the reconstruction and it
managed to fill in the hole from that
image like so so this is not what we're
it's supposed to do right that we're
supposed to match semantic scenes not
actually physical seems but with more
and more data this kind of lucky
accidents are going to happen more and
more often and it's nice where we can we
can we can use them ok so failures so
here is this wacky MIT building the
status center and we wanted to get rid
of this and you know for this is the
best image match so we're such an image
match you know you can't really expect
too much ok so for weird scenes it's the
matches are still not so great here is
another example where we wanted to get
rid of this and this is kind of so the
two people in the car and this kind of
the fundamental shows the fundamental
problem of ECM completion James likes to
to talk about which is that you know
there is really no way to just fill
stuff in you
you have to synthesize something right
there is part of a person you have to
hallucinate her legs or do something
right so it is in a way you know in the
end it is still an AI complete problem
so we're do much better and kind of
filling in holes or in scenes than
actual you know hallucinating legs and
things okay ah so let's see ah I have
three minutes so we are the first work
in this area to actually you know see
how well we are doing in a somewhat
systematic way so we have an evaluation
the user study we predefined a data set
before we actually did the research of
basically a bunch of holes that we want
to fill in and we ran until new
communities algorithm the closest
competitor our stuff and we also wanted
to keep the original images because what
we want to do is we wanted to ask a
person does it look real or not right so
we wanted to see how many of the images
would appear real to humans now there is
a little bit of a caveat that you know
antonius thing produces a single
resolved image and we are basically
picking from the best of 20 so it's not
a completely fair comparison we would
have also pick the best of 20 but there
is really a we trick the parameters and
all the results look kind of the same so
it didn't really make too much sense we
so the idea is that we don't want the
users to see the same in which several
times because there is a memory effect
and then they will know where to look so
we get a non-overlapping subset of the
image for each user randomly combined
into a set and then for every image we
show an image as we say is this a real
image or not yes or no please take your
time we're going to time you but you
know don't worry Don hurry up just you
know figure out what what if this is
right now so is this real who thinks
it's real cool things is wrong fake real
fake are you guys are good it is in fact
faked this is the real image and these
are the result so if you go to the very
right this is so this is time response
time of the people and this is how many
of images the label take so the
rightmost
on the right post area is the first
result and basically you know we
basically fool people thirty percent of
the time okay thirty percent of the time
people believe that our stuff is
actually real compared to a ten percent
of the time for criminy she okay so
that's that's good so we're we're a
pretty good jump from from from previous
previous work but notice it that regime
a whole fourteen percent of real
photographs were also marked as fake so
it seems that the people are is a kind
of trigger happy and and you know over
sensitive and and the reason is because
you know we told them that there will be
fake photos so they are they suspecting
something you know they're not naive
users anymore they're suspecting and so
they really kind of overshot so one way
to kind of try to compensate for that is
to say well let's consider what happens
at the first ten seconds you know they'd
really didn't have a chance to really
scrutinize every pixel in a mission and
and just kind of a glance that it like
you'll glance it at a bunch of trashy
magazines in the checkout counter you
know not really looking at edit
carefully and here you see that you know
virtually no real images are labeled as
fake which is good but at this point in
time even though uh you know criminy SIA
stuff is already seventy percent who has
been thrown out as fake for our stuff
about the third are fake so two-thirds
of images people are still known it's
not clear that there the ther fate and
so this kind of gives us hope that that
seems like you know we are we are in
fact doing something something
reasonable you know not definitely not
all the way there but it seems like
things are you know things are moving in
the right direction so why does this
work and and I think this is kind of
interesting for for for you guys
especially you know is it is it just
descriptor is that the particular
features we picked is it is it you know
is it the particular graph cut
implementation that we used with our
particular fitness function well I
believe that it's none of the above I
believe it's basically just data ok so
we first started with a small data set
and it really didn't work and here is
why you know here is an input image and
here are 10 years neighbors
from a data set of 20,000 images and
none of those nearest neighbors are
particularly near right they're really
not that close to the query okay and you
really need to work really hard do some
kind of a semantic processing to really
figure out similar images whereas if you
just increase your data set a couple of
motors overs magnitude suddenly your
nearest neighbors in whatever space you
do even you know something as simple as
s ideal SSD would probably work similar
you get a number of really really good
results okay so it's just that was you
have more and more data the actual
distance metric stops being important or
it becomes less important you know in
the limit if you have all the images in
the world you don't you know you don't
care what your distance metric is
because you know you will always you
will be always finding the exact match
anyway it's like you know the cameras of
the future will just have you know a
link internet link you take a picture or
even better you know it will have a GPS
you know you take a picture it will
record where the picture is taken from
go to Flickr and just feed you the the
image that should have been there at
that location right if you already have
all the data so it's kind of this
argument so the more day that we have
and it seems like you know it's okay i'm
sorry this is 2.3 million images sorry
no 23 yeah 2.3 million images it seems
like you know 2.3 million lists nothing
right for you guys that's really nothing
and yet we're already starting to get
pretty good match so I think that's
really where the stuff that's work and
that's really the stuff that's exciting
okay and and there is a parallel paper
two hours the concurrent work of Dounia
turrill Bob Ferguson bill freeman which
is actually explore this in much more
detail really cool work called tiny
images what they did is they collected
70 million a tiny 32 by 32 images and
they are seeing exactly the same
behavior as you get more and more images
things just become nicer and simpler and
more powerful so do read this paper it's
a really really nice paper
report so the big picture is that you
know if we can have an input image given
an image damage if we can find
reasonably close matches from a database
even without labels then you know lots
of cool things could be done you know
yes we could fill in holes in the images
but that's kind of a little graphics
application for fun but we can do cool
things too we can transfer labels that
might be there we can transfer semantic
information we can transfer in a
timestamp information this or whether
information or geographic information
tons of things and really kind of get at
the concept of brute forcing image
understanding instead of understanding
it just find close enough examples and
steal them and and that's that's I think
the big the big plan for the future and
III think that it's going to be quite
quite doable I believe at least so thank
you very much any questions
so I I don't see very well so just shout
remote with a pencil that is being
turned into a flower board or whatever
it was and you need to delete this
understanding in order to be able to
fill holes about the surrounding around
the hole have you done any work on that
that we go like a local local local
surroundings images using your same very
same technique to try to use that
knowledge in a way I so yeah so we do
this local stuff as a post process right
I don't think that matching locally
directly is going to do very well
because you know you'll first need to
get the global context right because all
the local stuff looks like all the other
local stuff you know the same you know
my button here would look like you know
a we love / / SUV and will look like a
cookie from the cafeteria and will look
like thousand other things and it's
really only the big picture that will
disambiguate between them so I very
strongly believe in the in the power of
using the global stuff for kind of
initial matching and then after you can
kind of got yourself into the ride
groove then you can try to kind of dig
deeper and and and get some local
information to
yeah but but I think if you just do the
local matching you'll just get something
that looks nothing like your career
image at all and I mean the problem is
for small objects will never be have
enough data you know that's that's just
not going to happen right even in this
tons and tons of even if you take all
the images in Google you will still not
have enough data for small objects so
this is really only kind of the the
image understanding from from afar not
really a local thing for local thing you
still have to you know do the real
computer vision stuff your mid-level
grouping segmentation recognition all
that stuff I think other questions you
have quite a bunch of clustered items
like motorcycle hmm doesn't it seem
natural to take the closest elements the
core of the clusters and turn them into
more of a model ah yes uh it's i mean
that's that certain certainly certainly
makes sense you know it's not super easy
because we still don't have
correspondence right we just have this
motorcycle this emergent for a nice
morphable model you drew neither
correspondence but but yeah that makes
all that makes a lot of sense I would
love to see more more formal model stuff
being done on large data sets for
example you guys have all this phase
detection phase detectors running and
and why not collect all those faces and
and have you build a huge active
appearance model of you know a million
faces I think this is this is a you know
this is exactly the right strategy to
pursue you know the biggest more global
models that have been been built to my
knowledge is basically a thousand things
a thousand faces and I think you know
the more there are more people in the
world than there are phases I think by
an order of magnitude you know there are
five billion people but maybe only you
know five million different phases so
it's really a you know we get five
million faces and that's it we can model
everyone I think it's cool yeah so but I
I agree with you that the you know for
for things like motorbikes for 3d
objects like cars
you know it becomes harder because then
you have to take viewpoint in the
account and and other things but but I
think it would be kind of a cool thing
to kind of build in models from scratch
automatically kind of you know crawl the
web and build into graphics models yeah
I think that'll be great yeah Oh would
get better of course of course no you if
you increase the number so if you
decrease the number of images it just
kind of slumps yeah you know antonieta
alba kind of quezon his graphs kind of a
gentle slope as you slowly increase the
data you get better and better and
better our experience have been actually
more dramatic it was it but it's also
you know less quantified we just kind of
I Nick totally noticed that there was
just nothing nothing nothing at boom
million images and it just worked and
and so that kind of was very satisfying
that you know it was dated was looking
out for us it was somehow you know
helping us from the background but but
yeah you know more data i think would
would really improve it whether it's
going to be another jump or not i don't
know it might uh taraba claims that
there is some kind of a saturation at
some well not saturation but basically
you get the slope becomes small enough
that that you need to do something more
more clever but i think this is a really
really interesting research direction
that yeah we just don't know how much
how much data is is good thank you
you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>