<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Universal Cache Miss Equations for the Memory Hierarchy | Coder Coacher - Coaching Coders</title><meta content="Universal Cache Miss Equations for the Memory Hierarchy - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Universal Cache Miss Equations for the Memory Hierarchy</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wVCnX56JYZg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you thank you all for coming so in
this talk I'm going to propose a
research program by that I mean I'm
going to fix on a particular problem I'm
going to suggest i propose to the
community the research community a
particular way of attacking this problem
and it's supposed to be definitive you
know if the programs are seized will be
done with this problem forever and ever
specifically the problem is a no one
cache misses dizzam know every now and
then you see another paper by this this
this is a problem that goes back many
many years so it's an old problem and
I'm suppose I'm proposing something that
I believe it's going to be a definitive
attack on the problem so it's kind of
ambitious so let's see how I can push
this this done together its own me and
sending win and Louie and Quinn function
who did all the hard work of getting the
data so it's about minutes equations and
what's new about the way I'm approaching
this problem is that I'm going at it top
down rather than bottom up so first I
need to explain what I mean by that and
its definitive in a sense that is in
order to be definitive is is I believe
it's got to be universal so I need to
explain what I mean by that as well once
i do that then again I can explain what
the program is about and in our business
every time we come up with something
that's always the question what's the
use was the you so I need to tell you a
bit about what's the use if the programs
are seized so it's about the cache miss
equation so first of all what is a cache
miss equation is just the expresses the
probability of a cash means as a
function of the cache size
so cashier one of the one other thing
ambitious about this program is that I'm
not attacking any particular one level
of the memory hierarchy I'm saying that
this is the way to attack any level of
the hierarchy so cash here has a very is
very general I could refer to the
processor cash the RAM database buffer
web proxy etc sighs our memories chip
maybe size is not an issue no I still
think that it is an issue for various
reasons even if you have a large amount
of memory one of the things that is
coming back to vogue now is
virtualization with virtualization you
still have to allocate the big memory to
various virtual machine so we have to
decide how much to give you any one of
them so size is an issue that if you're
talking about consumer electronics for
example you have something yea big and
you cannot just simply add memory if you
want to so against Isis TV show for guys
that you know suddenly for consumer
electronics energy is an issue by even
for for google energy is also an issue
because either memory is going to
consume a lot of energy as well so I
think in sizes student issue now
although I wrote the equation or the
equation expresses the probability of a
means as a function of the cache size
actually there are a couple of other
things that determine the probability of
a Miss and namely the reference pattern
and management policy so the reference
pattern for example it is a hard problem
and this this sly supposed to explain to
you why this is a very hard problem so
one of the things hard about it is that
it depends on the reference button might
depend on the
where variation that you have know
whether I'm talking about the data cache
or the instruction case for example and
whether you are using multi-core
threading it depends on that layout
whether you are you know what's your
pitch size where they give an index on
your data the application makes no are
your processes pipeline or are they
running concurrently if they are running
concurrently do they log each other out
for example that's going to affect your
reference pattern know what version of
the OS you are using what compiler
options do you have prefetching turned
on and the data instance at one point it
somebody told me that the biggest buyers
of supercomputing are actually Warmack
and citibank and these people for these
people the data is always changing so
the reference pattern is continually
changing so if you try to model that
that's going to be a big challenge and
the whole thing just depends on how you
configure it right no do you have level
two level three caches know these are
going to affect one another do you have
service level agreements that you have
to satisfy so when you look at this
whole shebang it's just not trackable
there are just not enough equations you
can write that will completely describe
the interactions here and if you could
write enough equations to describe this
you cannot solve it so what what have
people been doing so what they do is
what I call bottom analysis they start
off with very simple assumptions about
what is happening so typically they'll
fix on and some idealized replacement
policy let's say lr you assume there's
no prefetching etc so you have some nice
clean policy to deal with then they
assume there's independence in the
reference pattern and maybe there's just
one process
and so you have a very simple reference
pattern you simplify this you simplify
that that makes understanding the
interaction much simpler now I can write
your equation now of course the question
is you have this reality here and this
model here how close are you to the
reality so this kind of bottom of
analysis requires an expert now you need
to know you need to know what is a
reasonable approximation what what what
is it an ok assumption and what is not
so you need the expertise especially is
if you you have this particular model
and you're trying to customize it right
even if you build a model for this but
good shebang and then some node you have
another customer that says no I also one
in your model and but this is his
hardware slightly different his software
slightly different how are you going to
customize that model to to do that
different configuration you need an
expert who is able to do that and even
if you have already a model and then
tomorrow you decide to buy a faster
hardware or you decide to upgrade the
software and how does it how is how is
your mother is supposed to reflect that
this requires a lot of work is not
scalable in fact things are so bad
nowadays that people are IBM and
Microsoft they've been pushing for
autonomic computing where they want
everything to be automatically
configurable and adjust to changes in
workload automatically now I think about
a model that's trying to do that no
automatically configure and adjust
automatically is just hopeless the
situation is hopeless so my point is
let's forget this yes not taking us very
far in the last 40 years it's going to
not going to take us much further forget
it let's do it a different way so
instead of this bottom-up Noah Zuma zuma
zoom and then build up the model this
way let's start from the top and and do
down so so how do you start from the top
down you find one equation that will
work for everything okay well work for
everything no where's the magic so the
magic is instead of the probability of a
Miss as a function of cash sighs I give
you a small number of parameters to play
with how does that help you well if
there is if you if you if you have dual
model for fall for this particular
configuration and then you want to adapt
the model for a different configuration
that just means changing the parametric
values or if you have a model for this
and you change the hardware you change
the software that just changes the
parametric values you want to
automatically configure it change the
parametric values not just not changed
it but I know you can calibrate on the
fly the parametric values okay and you
want to adjust it because it's a flash
crowd well okay dynamically change the
parametric values so at all the magic is
going to go into the parametric values
all the while the same equation applies
okay so this is what i mean by universal
a same equation but with values of with
a small number of parameters and the
values change according to the
circumstance so this is the goal right
have one equation that will work for
everything what might it look like so
instead of the probability or means let
me write a different version let's know
do a map and then just express the
number of missiles as a function of the
cache size m and in this case just four
parameters and is the the diversion the
particular version that i'm going to
show here is very simple the hardest
thing about is the square root nothing
fancy what do all the what do the four
parameters so this one is a cache size
here what do the four parameters mean
okay so if you plot the number of misses
again
the cache size you see that as cache
size increases the number of missiles
decreases so the end star here is the
coal miss right as when when your when
your memory when your memory size or
cache size gets large enough at some
point it will stop going down and all
you're doing is catching the cornices so
this is the lower bound and that's the
end star there and then the end not here
is the vertical asymptote there is some
minimal amount of memory they need in
order to run your workload you couldn't
push it all the way to zero so this this
one is the vertical asymptote is the M
not net okay know how different is this
from previous equations one of the
critical difference between this and
previous ones now previous equations
typically is used a power law so some
some inverse polynomial and it goes on
and on and on infinitely keeps on going
infinitely decreasing infinitely it
doesn't identify some maximum memory
beyond which the the number of mrs.
doesn't doesn't talk so this the one
difference between this and this and
this guy here is the M star here it's
kind of the ideal em you know there's no
point in going beyond here because
you're not going to reduce the number of
misses any further and as any of you are
lower than this then you're going to
take some hits and take a hit you're
going to get some misses okay so the
only parameter left is the end not now
it's hard for me to explain what I know
actually is you can think in think of it
as the parameter that captures some
critical aspects of the reference
pattern are like whether you do
prefetching not the some critical aspect
of the replacement algorithm like
whether you do prefetching
and whether you do dynamic allocation
for somebody if you are doing a compound
and you do your dynamic dynamically
allocate some space that's going to come
in the end not so and not the way and
not came into being was when we looked
at those issues but dramatically you can
think of n not as specifying specifying
the curvature of this curve and you see
in the next when I show you the data
that this end not actually can is
flexible enough to give you very
different shapes so let me let me just
show you the data so in this one what we
did was run seven spec benchmarks on
Linux this is not a simulation this we
actually may measure the pitch a pitch I
oh and we change the RAM size for this
so this is that the data and the
equation gives a pretty good fit for for
the for the data this is the horizontal
as horizontal asymptote n star the
vertical asymptote M naught M stars here
the maximum memory and this particular
curve which is specified by the end line
okay now being a spec benchmark this one
is obviously a compute intensive
workload how about something io
intensive so outside of campuses like
this real people use to use linux they
use windows so we wanted a workload that
is a user's this interactive way of
using the system so we had windows 2000
and we run we ran a business winstone
business winstone is a benchmark where
they log some user clicking here and
there with something like 12
applications no current page from here
today icons etc all kinds of operations
you take this log and then we play it
back at different memory sizes
and again you see a good fit between the
equation and the data the data add more
data going pass like this one yeah yeah
so if you if you remember if you compare
the curvature here and the curvature
previous do you see there is a big
difference in the curvature and this one
is IO intensive because the if you just
look at it the horizontal asymptote is
already very high now these days so to
this this is very nice but we wanted to
challenge the model more aggressively in
the following way if you if you have no
people do java and python and see shop
where this garbage collected
applications the problem the trouble
order the hot part about modeling these
kind of applications is that the the
reference pattern changes with a memory
size see the older the typical bottom
way bottom-up way of analyzing the
interaction between the reference
pattern and the page replacement
algorithm is you assume a particular
reference pattern model okay and then
you say okay the reference pattern looks
like this and it stays that way
regardless of the memory size but that's
just not so in the case of garbage
collected applications so here what we
had was windows 2000 again but we use
Java and this one is very memory
intensive you see the steep curve which
is very different from the other tool
that we saw and still the model can cope
so we are pretty happy with this but
remember
I want this thing to be universal
Universal means that he has to apply to
anything right in particular it has to
the equation has to work for different
replacement algorithms what I've done is
try different reference pattern though
now i want it i want to check whether it
works for different replacement
algorithms so here we have a linux
workload and we patch the colonel to try
different replacement policies so the
kid we are five flow and random and this
one is the stock kernel and so it have
three different data sets and the
equation can fit the three different
data sets with different vertical
asymptotes and different amnon values
then start and then change because it's
there's a property of the reference
pattern and not property of the
replacement algorithm now the other
thing that the motivated me just now was
that if you have no you want an equation
such that even if your hardware changes
or your software changes you can still
use the same equation so let's just
check that out so here what we did was
armed tried so this curve in this curve
both use the same linux kernel but we
did so this a GCC compilation benchmark
will change the GCC version and the
question changes shape according to the
data and for this and this what we did
was same GCC version but we changed the
colonel now between 2.2 and 2.4 there
was a drastic change in the kernel and
the equation reflects that so I think it
looks pretty good in the sense that we
you know even though you know if you
have some software versions that go from
no virgin one version to all you have to
do when you're dealing with this
equation is just change the parametric
values without having to reanalyze the
whole thing
okay Western magic how is it that this
one equation can fit all of these
different circumstances so I need to
give you some intuition for this and
this is intuition so let's say tier am
here is the average time between entry
into ram and eviction so the thing of
the timeline here think about one page a
page comes into ram there's a code means
the pitch comes in the ram sits there
for a while get evicted and then there's
a there's a reference and comes back in
goes out comfort in like this so the
black pants are where is in RAM you take
the average of that that gives you the
tea room and the white pants are when
it's on this take the average of that
that gives you the teachers and our is
the number of times you come into Ram
okay now what I need to do is look at
the probability that you read a page
that has been evicted so one way to look
at it is imagine that the references are
uniformly distributed on this line then
what's the if the reference comes in
here then there's going to be a Miss if
the reference comes in here that's going
to be a hit so the probability of what
means is just naively the white part
over the whole line like this the other
way to look at it is the first one is a
coldness you don't count that because
here I'm talking about a page that was
previously replaced so comience doesn't
count so it subtract that one so it's r
minus 1 over R that's that will give you
the fee so you have this to you massage
that you have this one equation here and
this one equation notice that in
deriving this this one equation I did
not make any assumption about the
reference pattern nor any assumption
about the replacement policy so I call I
grandly call this the references plus
replacement invariant
so the hardest part about analyzing
cache misses is this interaction between
the reference pattern and Andy and the
replacement policy and here I have an
equation that is independent that
appears to be invariant known regardless
of which one you use for this or that is
supposed to do to still work that's why
I call it the barrier once you have this
you just a little slaw want to send the
results from queueing theory do a bit of
juggling around and you'll get a
equation it's not too difficult you can
imagine going from the R value from the
R value to the n value so that's just
the destination behind it behind the
equation when in when when I described
the intuition are in fact did not say
anything about the reference pattern
riders could have been references to a
database powerful si okay so yeah let's
try check no would it work when when I'm
talking about not not know about a ram
but talking about a database buffer some
application object like this one so here
we took some creases well this data
generated from real traces taken on
commercial workloads so these guys have
access to IBM's database machines and
they had like 12 tracers from some huge
companies they did not specify exactly
what those companies are were but you
can guess between the lines there were
things that AT&amp;amp;T and no IBM's own work
clothes etc so these are huge work
clothes and we did a simulation and well
actually the data came from them and we
just check whether the equation would
fit their data and it seems that it fits
very well now notice that in real data
the data is not going to not come down
nicely with
one known the curvature is not going to
be nice and nice like this no it's going
to be no up the curvature is going to
change one way this way when we noticed
this kind of waterfall behavior the
equation can catch that if you analyze
the equation the equation only has one
change in curvature so what so you
cannot catch this kind of multiple
changes in curvature but so what what
the equation can do is to do is best
when you if you have multiple changes in
curvature so that's one of the
weaknesses of the equation if you work
for the database buffer or why won't it
work for database app for web cache so
here we took some traces from one of the
websites in China this supposed we
largest peer-to-peer website you know a
lot of naughty stuff going on there and
the fit is very nice so I'm optimistic
that this thing works for web caching as
well one one thing one thing about web
caching is I actually don't know whether
the particular trace the student pick
for this has this property but for web
caching that this additional issues
about some file sizes are no very big
relative to to do others and some desi
there's this zip distribution that where
some data some files are red there are
many many files they are rarely access
so there are some differences in
reference pattern from the standard ones
but the equation seems not to worry
about this ok so remember my objective
was to get one equation to work for all
kinds of workload all kinds of
replacement policies and have initial
new the good news right up to this point
but it's unlikely that that this will
work all the time so this is what
whether the research program proposal
comes in
I have enough data to give me optimism
that this particular approach to the
problem is feasible so let's see what is
what what will work in general so I
propose that instead of this 40-year old
strategy of bottom-up analysis let's do
it the other way around do it a top-down
and for every cash type whether its
database buffer or processor cash or
whatever let's derive just one equation
for each level of the memory hierarchy
yeah so that's the program so in a case
if I'm talking about the room or the
database buffer or the web proxy I show
you that this particular equation works
right but if you have something like a
set associative cache surely it's not
going to work very simple reason because
the equation that showed you is
one-dimensional that's just just the M
the cache size by four set associative
cache is two-dimensional you have the
number of sets and yet the associate
ability so you have to explicitly
reflect this this two-dimensional
characteristic in the equation so the
previous equation that show you is not
going to work so what might work my
current favorite is this one so here I
need five parameters the a here is the
comas so the remember that the the so
this associativity associativity and is
number of sets these the two dimensions
now a here is the Colemans p one here is
the probability of a Miss if you're
lying if your cash has only one line
okay and then this one here is a stacked
distance some standard measurement
standard parameter for reference
patterns and Evan hinchey are supposed
to capture the locality of the reference
pattern
so we tested this no it is it has to be
universal right so we tested this on the
data cache as well as the instruction
cache and it seems to work you notice
that fall for the data cache it did a
cash patent reference patents and
instruction cache reference patterns are
quite different for instruction cache
reference patterns there are many many
models out there no because you can
assume is a loop is a recursion whatever
no it's a matrix or what and and you can
analyze that but for the data cache if
you are doing tree traversal or pointer
here and there then it's pretty random
so it's very hard to analyze so despite
the two different kinds of reference
behavior under this equation holds up
but I don't overtrain we took the traces
from this particular website and i found
the traces to be not very challenging so
I'm prepared to concede that this is not
the right equation and we need a
different equation the purpose of this
talk is just to propose this way of
attacking the problem and I don't claim
that the particular equations that I
have I in fact the right ones so I've
explained all the keywords except this
top-down wondering well what do I mean
by top-down modeling okay so remember
this equation that that I have right and
it's a show I show you some data that
says that it works for the database
buffer but there's something wrong that
there's something missing from this
equation at least in the old days I'm
not too clear about what people worry
about nowadays but when you're talking
about a database workload they are
interested in questions of the following
kind suppose I have this server running
this workload em transactions running
concurrently suppose I increase em to 2m
ok so m is a multi programming level
suppose I double the multiprogramming
level how would that impact the
performance ok so the idea
the they are interesting questions of
this salt but there's no way in this
equation doesn't have this kind of em in
it there is none of the parameters refer
to the multiprogramming level so to
reflect that in the equation I have this
other equation so in this other equation
this one is the M okay m is not a right
good good thing number of transactions
ok so it's the none that the
multiprogramming level and each
transaction make so many requests to so
many records and the equation will look
like this it's quite obvious that this
equation was derived from that equation
so data points this I have this equation
first I verify that this equation works
for anything universal now if there's
some aspect of the workload that I want
to be explicitly reflected in the
parameters then what I will do is I will
analyze how those parameters up there
depend on these other parameters and
then I refine those parameters into
other set of parameters including the
one day that you are interested in in
this case the multiprogramming level so
and and if that's not enough you want to
refine further you just go down some
more so this is what i mean by a
top-down approach rather than assume
assume and build it build a model up
this way yeah ok so let me give you some
examples of what you can get out of
equations like this one so energy right
a popular issue nowadays so in the case
of mobile devices you want to save
battery in the case of large data
centers that what you guys have you want
to save money on the electricity budget
all right so what does the equation do
for you so remember this example that I
have just now the M star here is sort of
like you don't need more than this so
the idea of course is to shut down any
access memory beyond that by not how to
do that you need to you need to have an
estimator what M star is
and the only question just doesn't give
you that so what what does my equation
do for you well let's say we have three
datasets like this and then you just fit
you just fit the tree data sets with the
equation what you get is three sets of
parameters and the end staff or the
three sets of parameters that are there
no the M star is not anywhere on on on
this curve no it's somewhere out here
right by the equation you appear in
equation once you do the fit okay um so
at this point is for me to ask the
question right when I presented this
thing at in Beijing to the Microsoft
people they have interesting spin on a
problem so the guy who talked to me was
actually somebody working on bit vault
vault is some archival system and
distributed huge amount of servers and
because it's the huge number of servers
you can be sure that there are always
some of them going down so because some
of them are going down you on
replication so it with a huge number of
servers with replication and they're all
up so they are burning up a lot of
energy so his question was whether you
can powered some of them down because
your replicas right can you power some
of them down and he wants to look at how
the equation can be used to address that
question now it wasn't very clear to me
what exactly had in mind and this is my
best tick on his question which is that
if you are trying to go to a particular
replica to retrieve a document and the
replica was powered down that counts as
a Miss so then you have again your
missus you want to find sort of like the
memories minimum memory size for for the
whole system no memory size may not be a
meaningful question for this but great
instance I think what you could do with
the equation is that once you once you
have the equation you can use the
equation to calculate the trade-off
between if you have them if you have a
Miss in this setting right that cost you
in terms of access latency so now you
want to balance between access latency
and power saving and the equation will
help you do the calculation so maybe
know the thing something like this will
work for you guys you guys have huge
data sets as well right so I'll be happy
to talk to anyone with things that this
might be useful for you okay so this is
talking about memory sizing the other so
they did when I started I said not just
sizing is an issue but the share is also
an issue you have you might have a big
memory by your own one partitioning
share it among work clothes how much of
the memory is you give you a particular
workflow so here Thursday know how can
you use the equation to do this
partitioning all right so you might want
to partition the memory for optimization
or to enforce certain service level
agreement all right so let's say you
partition into three and you want let's
say just for the sake of this talk nor
to minimize the probability of a Miss
aggregate means yeah alright so these
are the three datasets that I had before
and you'll fit them with the equation
once you fit them with the equation you
can use the equation now to calculate
right you have the aggregate you have
some aggregate probability of a Miss you
use the equation to calculate and you
can calculate what the means probability
would be for any partition right and
calculation is fast okay so once you do
that you completely met the surface
right this is the one node is
two-dimensional problem you have two
dimension and the surface here is the
probability of Miss it's not very clear
on this particular plot but one I think
this one is the Vista predicted one and
this one is the measure one the real
Miss and you see that the equation gives
you a good approximation of the the real
mess so what you can do with this now is
you can do the open
they want locate where the optimum
partition is or calculate the trade-off
between different partitions for example
one another thing that I used to
motivate this was this issue about
autonomic computing if you have a sudden
change in workload you want to react to
it automatically and on dynamically how
do you do that and I said by giving you
the parameters you can just calibrate
them on the fly so now let me give you
an example okay so for this I take the
database example TPC workload and we
change the number of terminals
multiprogramming level psychically 50 40
30 50 40 30 so the means probability is
going to keep on changing now if you
don't change your memory size right and
then you have a certain target that you
want to hit some service level agreement
let's say 0.2 some aggregate sum this is
one just just one work type so no I
aggregate by just the probability of a
miss all right so this is this time is
divided into epochs here and this is the
target that you want to achieve and if
you if you are stable then because
because there's a cycle here the
probability of a Miss is going to go in
a cycle okay all right so there are
basically three ways of doing of doing
this one obvious idea is to do gradient
descent you take two neighboring epochs
look so this is a certain memory size
there's a certain memory size and you
look at the P means for this to you draw
a line this your target that tells you
what what the target memory size should
be right so that's great in the same if
you do that you will get you will
converge on the on the on the target
penis but you can go to this violent
fluctuations for two reasons one reason
is that the
since patent for example has randomness
in it and that randomness is going to
throw you off every now and then and
also know that the the book law itself
is already cycling so that's going to
top it some more so because of that you
can have this overshooting and under
shooting and it takes a while to
converge so one way of dealing with this
is that well you you have this
fluctuations let's say random
fluctuations right you smooth them out
if you can move them out the gradient
descent work better so one way is to use
the equation to smooth out the
fluctuations your this move curve with
this smooth curve you calculate the
gradient that's there's a more stable
way of estimating a gradient and sure
enough if you do that then you have
convergence sooner right but if you have
the equation why even do gradient
descent use the equation to calculate
what the optimum what what the target
memory size should be right so if you
use the equation to compute the target
memory size then then in one epochs
you're there so this is how the
equations can be noted the calibration
here happened in real time so you can do
this calibration automatically
automatically and dynamically
what was the dynamic variation in him
that corresponds with the various epics
their dynamic at kerasotes oh you were
computing him but you're displaying how
accurate your miss rate was I wondered
what the overall variation of em was
given that you had a model behind that
who was sensitive to the various number
of terminals for happiness was that a
50-percent variation I never saw the
tape I thought I could I could I could
look out the data yeah while my student
generated his growl and he never I never
saw the data is that an issue I just
wondered if it was a small variation
percentage available just wondering
about the sensitivity in the map
I better check thanks ok so this dynamic
adjustment of the memory size the next
thing is that the memories share okay so
this one so this let's say you have you
have a some shortage of free memory so
remember you have this big memory and
you are trying to cut up this big piece
of memory and allocate it to different
in this case virtual memory virtual
machines and you start up another
virtual machine so suddenly you're out
of space so you want to reclaim some of
this allocation and give it to the new
virtual machine right and you want to do
it in some fairway okay so you can
define so so here is the scenario you
have your divided into three parts and
you start off before the reclamation
from from this tree of them some unfair
allocation right in this particular case
you see that the pool number two is
consistently consistently has a lower p
miss ray she oh here is the fitness
against / the calmest so that's our
measure of fairness and it's unfair in a
sense of one of them consistently lower
than the rest and also the spread is big
right the spread is but you you like to
narrow narrow down this spread in the in
the ratio so if if that vo objective no
you can have other definition of vendors
but if this is your definition of
fairness now what you want to do is free
up some memory by taking away some from
each one of them and give this to to the
new one let's say okay so you can use
once you have the equation you can use
it to calculate and it's some kind of
prorating you can see some kind of flow
rating here except that the pro rating
is instant is not it's not in terms of
em I not a share is not in terms of M
star the the optimum soco optimum by
instant of this difference between em
stein am NOT okay so remember em now is
the vertical asymptote
star is the optimum so you are talking
about this pace not not this pace you do
that prorating and according to this
definition of fairness you shoot you
should get what you want so we did it
and after the recognition you see that
no no is the square none of them is
consistently the lowest and the spread
is smaller so the point here is that
once you have an equation you can use it
to do your calculations for your memory
allocation management okay one more
example autonomic computing I said that
they did the IBM and Microsoft have been
pushing did and in fact that the
database people are out there on this so
the tree the major database people they
have all come up with software to help
people configure their machines
automatically and for a database of the
just humongous number of tuning knobs
but that all for all of them one of the
things that they focus on is the buffer
allocation so all three of them have
some simulator embedded to predict a
number of iOS four know if you have a
bigger buffer or smaller buffer and and
this this this is a soul so you do the
simulation to do this you simulation to
do this prediction so is high coding
right so it's very hard to change if you
know if you change the hard way to
change the index structure or whatever
the table design then how are you going
to change the simulation code so there
is an issue in the case of smaller
buffer so first of all good we have to
understand right why would you want to
have a smaller buffer so according to
white one or the guys told me
in the enterprise setting sometimes are
the the buffer manager is a black box
you cannot see the reference pattern
what you can see is white other mrs. you
can see the the missus coming out of the
black box but you cannot see the
reference pattern inside the black box
so now if you change if you reduce the
if you change the the buffer allocation
to a smaller size why would you do that
well we might do that because our server
consolidation let's say you take new
servers you put you on the same machine
if one of them now get a smaller buffer
small amount memory all right so you
have it smaller a buffer the smaller
amount memory to play with now you know
predict the number of misses how do you
do that if you don't know the reference
pattern you you know what was previously
a Miss what was previously a hit now
becomes a mess and you never saw the
hiss so if you never saw the hits you
never saw the reference but then how are
you going to predict the missus so for
them this trace based approach is going
to be a problem to attack the issue of
smaller buffer size for me is no problem
I just use the equation fitted do export
do interpolation extrapolation backwards
rather than forwards at no problem yeah
m0 just fall out of it it's not derived
so the default parameters when when you
give me a set of data I but the equation
can be massaged into a linear equation
and then I just take your data and then
fitted do a regression again so this is
what what I'm trying to get across the
way people have been doing it doing
Kashmir's analysis all this wall is what
I call bottom up I say give it up this
is not going to work anymore let's do it
a different way this top-down way of
doing it more than that I believe that
you you can't go on trying to derive a
new equation for new setting you should
try to get one equation that will work
all the time okay and the way to do that
is true the parameters so it seems like
often a priority doesn't seem like it
should work but i think i've been out
data to make me optimistic about this
particular approach working so this is
my goal I like people to look at this
top this particular way of attacking the
problem and I hope that you guys would
come along and let's just solve this
once and for all let's not keep on
generating duffel miss equations this
won't this one going forward oh yeah we
have happy to take more and more
questions so if an ocean or or call it
the composition
mathematics that allows you to take one
parameter out of your equation and
replace it with say three more higher
resolutions or so for instance he
started with your base model you modify
that bottle when you went to the end
screen database buffer you modify the
model a different way which right to
CASIA set associativity if you needed to
do the composition of database buffers
with patch sensitivity it is there in
other words is there a kind of a
hierarchical refinement that one could
imagine where you can reuse some of the
fundamental physical modeling parameters
from a coarser level and then
selectively replace one or two with
refined elements so that you're not
deriving new equation for each different
instance
okay so the question is necessary if
your database book load I have a
database equation I have a cache
processor cash-based equation is there
some way to integrate these two models
to get me a new model to predict the
Kashmiris for what is the catch this
case get the kingdom's set associative
okay so in this sense okay so the way I
see it is that i started the cache miss
equation for set associative cache and i
want to see how my database model can
help me refine this short answer is I
don't know I've never done it I don't
know how it might work what the kind of
composition that you're thinking about
I've tried to work on let's say I have
two compilation work clothes no let's
say I take this take pic of the same
file and then and i run two compilations
you martini ously know how would they
how would the equation can can I take
the two equations and then and merge
them to give me one equation things like
that but I've not looked at two
different types of equations I'm also
almost thinking of the taxonomy where
you take your initial equation and then
in the refine mode you say in this mode
n0 is no longer a constant it's replaced
by this function and these three
additional parameters and m0 is no
longer a constant it's replaced by this
and so that the equation grows in
complexity if you added the parameters
but it does it in a way where you can
assume some coarse grain approximations
and then a series of hierarchical
refined
that kind of attempt to recapture the
intuition slide that you display yeah so
you open your point is you know whether
whether they have some kind of theory
for doing this hi records refinement
that I talked about in the top down
things right now right off no the only
thing I tried was for the David database
case i don't know how how that might
generalize to other parameters other
walk-throughs thanks about the if I
convince enough people to attack this
problem this way you will see you the
answer soon I hope no okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>