<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Guilhem Semerjian: &quot;Random Constraint Satisfaction Problems, Classical and Quantum Results&quot; | Coder Coacher - Coaching Coders</title><meta content="Guilhem Semerjian: &quot;Random Constraint Satisfaction Problems, Classical and Quantum Results&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Guilhem Semerjian: &quot;Random Constraint Satisfaction Problems, Classical and Quantum Results&quot;</b></h2><h5 class="post__date">2015-08-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qyoFsBE9b7g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">
MALE SPEAKER: Welcome
for another installment
of the Quantum AI
speaker series.
Today, it's our pleasure to
welcome Guilhem Semerjian, who
is here with us from France.
He is assistant professor
in the Ecole Normale
Superieure in Paris.
And that's also the place
where 10 years ago, 2004,
he got a Ph.D. In between,
he had a stint in Rome,
at La Sapienza for his postdoc.
And Guilhem is focusing on
statistical and mechanics
methods, mainly applying
them to the various fields
in discrete mathematics
and computer science.
And the particular
interest of his
is applying it to random
constraint satisfaction
problems.
And recently, I think-- or I'm
not sure how recently this was.
You also have become interested
in the role of quantum
fluctuations within the
adiabatic protocol--
which role they could play
helping to solve constraint
satisfaction.
So I'm looking
forward to your talk.

GUILHEM SEMERJIAN: OK.
So thanks a lot for
the introduction
and for the invitation.
I'm very glad to be here today.
So I will indeed try
to tell you a story
of a field of interdisciplinary
research, which has taken place
at the crossroads between
theoretical computer
science and discrete mathematics
and also statistical physics,
which maybe is the surprising
part of the interface
between these different fields.
So as I was invited
by the quantum
group, of course there
will be something quantum,
but only at the end.
So the talk will
be-- I mean, I think
it's important
first to understand
what goes on in
the classical case
before tackling the quantum one.
And I think it can be
interesting for a wider
audience, the story about this
random constraint satisfaction
problem.
So as I said, it's
interdisciplinary work
and an interdisciplinary story.
And I guess this is also an
interdisciplinary audience.
So I will try to
speak to all of you,
taking the risk of
speaking to none of you.
But that's a chance I will take.
So for the people here, don't
hesitate to interrupt me
if some things are not clear.
I will try to get
you all on board.
So I will first recall what are
constraint satisfaction problem
and then motivate and
define the introduction
of a random ensemble
of such problems.
And then I will
describe quickly what
physics and statistical
mechanics methods
has produced as a result
for this kind of problems.
And finally, I will touch
upon the effect of quantum
fluctuations on these problems.
So constraint
satisfaction problems
are defined in a
very simple way.
So imagine you have a certain
number, n, of variables,
which can take a finite
number of values.
So they are in a
discreet alphabet.
For instance, you can think
of Boolean variables-- so
true or false, or 0 or 1.
As you want.
Or if you are a physicist,
plus or minus 1 as [INAUDIBLE].
And then you had
some constraints
on this set of configuration.
And the constraint
is some function
of a subset of the variables,
which can be either satisfied
by this configuration or not.
And the underlying idea is
that the number of variables
on which each of these n
constraints deepened on
is relatively small.
It should be thought of as
a small number with respect
to the total number
of variables.
I will give some examples
in the next slide,
so it might become clearer.
And well, the
first thing you can
define with these
two ingredients
is the notion of solution.
And the solution
of such a problem
is just a configuration
of the variables
which satisfies simultaneously
all the constraints.
If it exists, of course.
It's not grounded all the time.
So let's be concrete, and
let's consider some example.
So maybe the first
one to picture,
and one which is
very well known,
is the problem of
coloring of a graph.
So in this case, the alphabet
of the variables has q elements.
Think of it as q colors.
And these variables live
on the vertices of a graph.
So each vertex in a
configuration is given a color.
And the constraints live
on the edges of the graph.
So they only involve
two variables at a time,
even if the graph is very large.
And the constraint
is that the two
colors on the end vertices of
the edge should be different.
OK?
This is what defines
what is called
the proper coloring of
a graph, is a coloring
with no monochromatic edges.
And in this case, a
solution of this problem
is precisely a proper
coloring of the graph.
The two next example which
I would like to include now
take Boolean variables,
work on Boolean variables.
And in the two cases,
each constraint
depend on a subset of k, of
the variables in the problem.
The first one is the famous
satisfiability problem.
And the constraint
impose that this junction
of k of these literals
has to be true.
So let me rephrase
in a different way.
So a literal is either
one Boolean variable
or is a logical negation.
And this junction
means logical or.
So in other words,
you have out of the 2
to the k configuration
of the k variables
on which the constraint acts.
Out of this 2 to
the k variables,
there is one which is
bad and that does not
satisfy the constraint.
And all the other ones are OK.
The 2 to the k minus 1 of
the configuration of these k
variables are fine
and are satisfying.
This is a famous
satisfiability problem.
There is a twist on
this problem, which
is called the
XOR-satisfiability problem,
where instead of logical or,
you use logical exclusive or.
In such a way that true
exclusive or true is false.
And in this case,
you can also think
of this as variables which,
instead of true and false,
are 0 or 1.
And this operation of XOR is
becoming the addition of 0
and 1 modulo 2.
So this is just a set of
linear equations, modulo 2.
Each constraint is one equation
on this subset of variables.
And it also can be seen
as a parity check, which
are well-known constructions
in error correcting codes,
because each of
these constraints
impose the parity of the
number of true or false.
And in this case, out of the
2 to the k configuration,
you kill half of them.
There is only one half which
has a good parity of [INAUDIBLE]
number of 0 and 1s.
OK?
So given an instance,
so a set of constraints
on such variables,
well, as you know,
there are many questions
that one can ask.
The first one is whether the set
of solutions is empty or not.
So whether there is at
least one configuration
of the variable which is able
to satisfy all the constraints
at the same time.
And this has a yes/no
answer, obviously.
But there are other problems
that you can ask given
the same set of constraints.
For instance, you
can ask what is
the number of such solutions.
And this, the answer
is not yes or no.
Now, it has to be a number.
Or you can also ask, what is the
maximum number of constraints
that you can satisfy
simultaneously?
So if the problem
is satisfiable,
then the answer would be m, the
total number of constraints.
But otherwise, you have an
estimate of how bad you will do
if you take the best compromise
knowing that you cannot satisfy
all the constraints
at the same time.
OK.
And these kind of problems
have been, of course,
the subject of theoretical
computer science,
and in particular of
computational complexity
theory.
And in particular,
if [INAUDIBLE]
focus on the first version
of the problem-- that is,
the yes or no
question-- is there
at least one solution of the
sets of constraints or not?
Well, for the k exclusive or
satisfiability, as I mentioned,
it can be seen as a system of
linear equations, for which you
can use Gauss
elimination algorithm.
So it's an easy program,
a polynomial problem,
for any value of
this parameter, k.
But on the other hand, the k
satisfiability or k coloring
problem that I mentioned are
difficult. Or more precisely,
are in fact NP-complete.
As soon as the number of colors
for the coloring problem,
or the [INAUDIBLE]
of the constraints
for the satisfiability
problem is larger than 3.
And actually, the
satisfiability problem
was the first one to be
proven NP-complete in '71.

Let me just say that,
for a given problem,
it can be that one
version of this question
is easy, where the other
can be very difficult.
So I think that's
something which
is important to keep in mind.
In particular for the
XOR-satisfiability problem,
it is indeed easy to
decide whether or not
there are solution
using Gauss elimination.
But finding-- when there
are no solutions-- finding
an optimal
configuration, maximizing
the number of
constraint satisfaction,
of constraints
which are satisfied,
is actually a very
difficult problem.
And it's even a
problem which is hard
to approximate within
any non-trivial ratio.
I mean, there is a precise
theoretical computer science
statement about this
harness of approximation.
So difficulty is not only
problem dependent, but version
of the problem dependent.

So there is one
criticism, so to speak,
or one limit of this
beautiful theory
of computational complexity, is
that it's a worst case notion.
So a problem would
be declared out
of the polynomial
class of problems
as soon as there is one
instance that no algorithm is
capable of solving
in polynomial time.
But maybe most of the instances
are easy, and only a few
of them are hard, and these
are the few maybe irrelevant
in the examples, which make them
NP-complete for the worst case
classification.
So one would like to have a
notion of typical complexity
of such problem.
And there are
different approaches
to this notion of
typical complexity.
The one I will follow
here corresponds
to taking a random
ensemble of instances,
and so introducing
probability in the game,
and saying that
something is typical
if its probability goes to 1
in the limit of large problem
size.
And there are a very simple way
to produce a random ensemble
of such problem instances.
So for instance, in the case
of the coloring, you can say,
OK, I have n
vertices, and I want
to construct a graph with
some number, m, of ages.
Well, the simplest thing you
can do is just take uniformly
at random this m edges out of
the n choose to possible 1's.
And actually, that's
a construction
which is known in
mathematics under the name
of Erdos-Renyl random graphs.
And the study of this
discrete mathematic structure
started in the '60s with
the work of Erdos and Renyl.
And this kind of
random construction
can also be generalized easily
to the two other problems
that I introduced.
The only difference is
that, instead of edges,
you have to choose hyperedges.
That is, instead of
an edge is something
which links two variables.
And here, you choose something
which has k variables.
But again, there are n
choose k of such objects.
You can just draw m
of them at random.
And it turns out that the
interesting regime is always
an asymptotic regime.
So even in the worst case
computational complexities,
there is always-- it's always
understood that one speaks
of very large problem sizes.
And here, the interesting
regime is also
with very large problem
sizes and with the number
of constraints, which is
proportional to the number
of variables.
So we did not alpha this ratio
of the number of constraints.
We divided by the number
of variables in this limit.
And this just provides-- I
mean, in one sense-- a set
of benchmarking problems on
which one can test algorithm.
Actually, to put this in
historical perspective,
it turns out that--
I mean, in the '90s,
people realized that the
problems that they knew
were NP-complete, like
coloring or satisfiability.
So it was proven since the
'70s that these problems
were very hard.
But researchers in
computer science
faced a rather
paradoxical situation
that all the instances on which
they could benchmark algorithms
were easy.
So it was very
paradoxical, but they
know that from a worst
case point of view,
these problem were
difficult. But they
had no example, explicit
examples, of hard instances.
And it turn out that this
construction provides
the first natural examples of
such really hard instances,
to paraphrase the title of
one paper of '91 [INAUDIBLE].
So to study this random
problem, one quantity
that one can define
is the probability
that such a problem
drawn at random with-- so
a certain number of
variables and alpha times
this number of constraints
has a solution.
There is a first
trivial observation,
is that this probability
will be decreasing
with the ratio of the number
of constraints with respect
to the number of variables.
Because the more constraints
you put on the system, I mean,
you can only destroy solution.
It's not by adding constraints
you cannot create new solutions
in a problem.
So this will be decreasing.
There is a slightly
less obvious observation
which deserves two
lines of computation
to be proven rigorously.
Is that for all
these problems, there
will be a value of alpha
fixed-- so which does not
scale with m-- such that the
limit of this probability
is going to be 0 in
the large size limit.
And this can be done
using Markov inequality
of first moment method
from discrete mathematics.
It's a very simple proof.
AUDIENCE: I have a question.
Are we talking about
the probability
of a polynomial solution
to this decision problem?
GUILHEM SEMERJIAN:
Solution of the problem.
AUDIENCE: Solution
of the problem.
GUILHEM SEMERJIAN: Yeah.
I And here is true for all
the examples I've introduced.
So of course the
details or the value
of alpha that one has
to take, and the value
of the probability,
will be different.
But this pattern will
be true for all case.
And here, probability is
probability of existence
or not of a solution.
Whether it can be
found easily or not.
I'm not talking
about-- for the moment,
I'm not talking about
algorithmic properties,
for the moment.
AUDIENCE: OK.
I have a question
about your alpha there.
Is it the case that there's
a threshold for alpha?
Or for any alpha,
there's an enlargement?
GUILHEM SEMERJIAN: Well, if
you can wait one or two slides,
the answer is coming.
Other questions?
So actually, the
answer is coming.
So OK, let's look at
these graphs, which
I've stolen from some papers.
So fortunately, I've stolen
them from a paper what
was drawn was not the
probability of satisfiability
but the probability
of unsatisfiability.
So look at the left
diagram and just reverse
it to have the quantity
I've defined before.
So as you see, it's decreasing,
or increasing in this.
So this is a function of alpha,
the parameter I introduced.
And you see that, for
large value of alpha-- OK,
on this scale, it goes to 1
when the size of the system
increases.
So the different curves are
for different system size,
these different
number of variables.
The nontrivial observation
is that, indeed, there
seems to be a threshold.
So that for a certain
value of alpha,
in the limit where the size goes
to infinity, the probability
that there are solution in
this problem instance either
go to 0 or to 1.
And there is a sharp
threshold between the two.
So that's-- for physicists,
that's of course very
reminiscent of a
phase transition.
Or it's also a
threshold phenomenon,
as there are many also
in discrete mathematics
and in random graph theory.
And to come back to
the historical note
I was making about the fact that
people had hard times finding
hard instance of
hard problems, well,
then it turned out that, if you
look at the plot on the right,
is a measure of
the number of steps
that some algorithm has
to make to answer yes
or no to this decision problem.
And without entering
into the details,
you see that there is a peak.
And this peak is more
or less in the region
where there is this
threshold phenomenon.
Because for very
small value of alpha,
the problem is under constraint.
So it's rather easy
to find a solution.
For very large value
of alpha, the problem
is very much over
constrained, and so it's not
easy to show that
there are no solution.
But it becomes
easier, in a sense,
than when there are
a lot of-- I mean,
when the problem is
strongly over constrained.
So this was one way for
experimental computer
scientists to generate
hard instances [INAUDIBLE]
to generate them close to
such a phase transition.
So let me rewrite
more precisely what I
said about this
threshold phenomenon.
So from the point of view
of rigorous mathematics,
this is a conjecture
at the moment.
So that there is a threshold
value of alpha-- which,
of course, depends on
the problem considered
and on the parameters
k or q of the model,
but which separates, in
the thermodynamic limit,
either instances which are
with i probability satisfiable,
and others which are with i
probability unsatisfiable.
OK.
So these problems have been
the subject of a lot of works
in mathematics and
theoretical computer science.
I will not enter into the
details of all the things
which have been proven, because
it would take a very long time.
But OK, there is a weak
version of this conjecture,
with a possibly n-dependent
threshold, which
has been proven by Freidgut.
And there are a lot of
upper and lower bounds
which have been derived
on this threshold.
And also, there is one
simplifying case, is
when the parameter k, for
instance, goes to infinity.
But after the n
going to infinity,
there are also some
simplifications which are
[INAUDIBLE].
Yes.
AUDIENCE: I was
surprised that one side
of the satisfiability
phase transition
happens only in
a certain region.
So shouldn't it be
like, when alpha
is smaller than a certain value,
then this limit should go to 1?
And then, in between,
that's where it's hard,
where it would go to 0.
But then, as you pointed out,
if it's highly over constrained,
then it should go
back to 1 again.
GUILHEM SEMERJIAN: No,
but this is a probability
that there are solutions.
So if it's over constrained,
there are no solutions.
AUDIENCE: [INAUDIBLE]
GUILHEM SEMERJIAN: This is
not an algorithmic statement.
It's just a structural statement
about the existence or not
of solutions.

OK, if you want more
details on this pattern,
so in the large
alpha region, it's
always exponential with
the size of the system,
of the number of variables.
Because, well, I mean,
this is a rigorous result
that we don't know how to do
better than resolution proofs.
And resolution proofs, for
this, is exponentially larger.
But with the prefactor
which goes like 1
over alpha at large alpha.
But again, this is just
a structural statement
above the absence of solutions.
At just large enough
value of alpha,
not alpha infinity large.

OK.
But these are a lot of
very nice rigorous results
on these problems.
But there are at least
two things which are not
completely satisfactory.
The first one is that
there are no prediction,
explicit prediction,
for this threshold
at finite or small values
of this parameter, k.
And also, up to a certain
point, all the results
that are obtained
rigorously didn't really
point out why there was
something difficult happening
for values of alpha slightly
before this threshold.

And that's where I think the
use of statistical mechanics
methods proved to
be quite important
and to bring new results and a
new perspective in this field.
And that's why I would like to
spend some time on this now.
So the first question
is what has physics
to do with such a problem?
And again, here, depending
on your background,
either you will find
this completely trivial
or completely cryptic.
I'm sorry.
But in a nutshell, what
statistical mechanics does
is to introduce some
configuration space which
is supposed to be a
microscopic description
of a physical object.
But you can abstract
from it and say
that it's just a
model, which is given
by a set of configurations made
of a lot of microscopic degrees
of freedom-- on which you define
something that we call energy,
when you are a physicist.
And we introduce a
parameter that physically
is called the temperature.
And then, following
the [INAUDIBLE]
we introduce a
probability distribution
over the space of
configuration, which
is up to some
normalization factor
proportional to
exponential of minus
the energy of the configuration
divided by temperature.
That's the prescription
of statistical mechanics.
And now, the traduction with
[INAUDIBLE] optimization
is rather simpler.
Because if you think that
the energy of a configuration
corresponds to the number
of unsatisfied constraints
of your constraint
satisfaction problem.
And if you look at the
limit of low temperature,
at low temperature-- so this 1
over t will become very large.
The exponential will
be very much peaked
on the minimal
values of the energy,
and it will concentrate--
this probability measure will
concentrate on the lowest
energy configuration which
are precisely the ones solving
the optimization problem,
because they maximize the
number of satisfied constraints.
So in some sense, this
is exactly actually
the same problem, if I
formulate in this way.
The only difficulty with respect
to usual statistical mechanics,
and that here we
have some randomness
in the definition
of the problem,
so some randomness
in the energy.
And this is not
the usual setting
for one part of
statistical mechanics,
where we consider
ordered systems,
translational [INAUDIBLE]
systems in finite dimension,
for instance.

But there is a branch
of statistical mechanics
which deals precisely
with disordered systems,
and in particular with
the class of systems which
are known as spin glasses,
and which actually
correspond precisely to this
kind of random optimization
problem.
And here, the fact that-- in
the definition of the instance--
there was no
reference whatsoever
to some finite dimensional
[INAUDIBLE] lattice.
So the edges in the random
[INAUDIBLE] coloring
were chosen uniformly at random
among all the possible pairs,
makes it what is called
the mean field problem
in statistical physics.
And this is good news.
Even if you don't
know what it means,
it means that we have
a more powerful method
and that the problem
are easier in general
in this method setting than in
finite dimensional problems.
And one simple example
of this translation
for the graph coloring
is precisely what
is called Potts model.
So that has q possible
values for each spins
and with so-called
antiferromagnetic interaction,
because the constraint
tends to put the spins
in different directions.
They don't want the colors
to be the same on neighboring
vertices.
So there is a link with
statistical mechanics.
So there is the hope that
statistical mechanics
techniques can bring results.
And it has indeed been
the case, because here I'm
telling a story which
started a few decades ago.
So I present it as a
new story, but it's not.
OK.
And in particular, there
has been different outcomes
of this line of research
in this precise case
of a random constraint
satisfaction problem.
So one of them has been
quantitative estimation
or conjecture on the
value of the threshold
for the satisfiability.
But maybe that's not
the most important part.
I mean, I think that the most
important part is that it gave,
or it relied, on a very
much refined picture
of what is going on in
the satisfiable phase.
So before the
satisfiability threshold,
so when there are
still solutions,
actually there are a lot of
action going on in this phase.
And that's what statistical
mechanics [INAUDIBLE].
And it also adds
some consequence also
for algorithms, with
the introduction
of techniques from out
of equilibrium physics,
to analyze some
algorithms and also
to propose new algorithms,
like survey propogation,
for instance.
So I will try to
give you some ideas
about some salient
feature of these outcomes.

OK.
So maybe that's one main message
that I would like to convey
is summarized in this slide.
So let's browse through
it and [INAUDIBLE].
So by definition of the
satisfiability threshold,
for alpha smaller than this
value, there are solutions.
And not only that, it's not
too hard to see that there
are exponentially many
of them, with the rate
that physicists call entropy,
this function s of alpha.
And that's its
exponential is natural,
because the size of
the configuration space
is 2 to the n or q to the n.
So it's exponentially large.
And at very low value of alpha,
there are very few constraints.
So essentially, all the
configuration space, e,
are solutions.
And add in constraints,
you put [INAUDIBLE]
inside this configuration space.
So those are less
and less, but they
remain actually exponentially
many-- with a slower
rate of growth, but still
exponentially many--
up to the satisfiability
transition.
The important
point to realize is
that something happens before
this satisfiability transition.
And it's a transition
of a different nature,
which corresponds to a
clustering transition.
So what I mean by that
is sketched on this plot.
So the problem with
this drawing is
of course a very
course simplification
of what is going on.
Because solutions are in
the hypercube of 2 to the n
points, and so
it's very difficult
to draw it on a plane.
And well, obviously,
it's impossible.
So there are only sketches.
But what this sketch mean is
that, for low enough value
of alpha, this exponential
number of solutions
are scattered more or less
regularly inside hypercube
of all possible configurations.

And they are well connected
one with the other.
So if you take any two
solution for alpha smaller
than this alpha d, you can go
from one solution to the other
by a pass which only goes
from solution to solution.
And on each step
of this path, you
move [INAUDIBLE]
distance something
which is relatively small.
Let's say at least not
extensive in-- not proportional
to the number of
variables itself.
So it's well connected, so to
speak, this set of solutions.
On the other hand, when you
go above this clustering,
or dynamic transition,
what happens
is that it's not true anymore.
And there is a breaking
or a shattering
of the set of solutions
inside the hypercube, which
gets a non-trivial
organization in clusters.
So there are some
groups of solutions
which are close to
each other, and which
can be connected in the
way I was telling before.
But there are pairs of solutions
which are disconnected,
or pairs of clusters
which are disconnected one
from the other.
So at some point,
there is a notion
of connectivity in the
space of configuration,
which breaks down.
OK?
Is this clear, more or less,
the idea of this sketch?
And so not only this,
but the total number
of solutions, as I said,
is exponentially large.
And this entropy actually
has to be-- I mean,
has two contributions.
One coming from the number of
clusters, which is exponential
large, in this regime,
and another one
coming from the
number of solutions
inside one cluster, which
is also exponentially large.
And the crucial point is
that the characterization
of the satisfiability
transition is not
that the total number
of solutions disappears,
but it's the number
of clusters disappear.
So at the satisfiability
transition, what happens
is that the last cluster dies.
But before dying, it still
contained a very large number
of solution.
And in other words, to compute
this satisfiability transition,
one has to understand
this before.
Because if one does not try
to separate this contribution
of the number of solutions
in these two very distinct
contributions, one cannot find
this point as the point where
the number of
clusters disappear.
AUDIENCE: These two
equations, for the number
of clusters and number of
solutions within a cluster that
is in the state where
alpha is between alpha sub
d and alpha sub s?
GUILHEM SEMERJIAN: Exactly.
Before alpha d, sigma is 0.
There is a single
cluster, if you want.
And all the internal
entropy correspond
to the total
entropy, if you want.
Actually, I will show a plot.
AUDIENCE: Is it known
whether-- if you
look at screens that violate
just one constraint, so
in physics lingo, [INAUDIBLE]
ground stage of the [INAUDIBLE]
first-sided state.
Are they organized
in a similar fashion?
GUILHEM SEMERJIAN: Yeah.
Yeah, this persists at
positive temperature.
AUDIENCE: Because
that what caused
the discussion of the minimal
gap by different light.
And actually, we saw
this-- such an organization
experimentally, that when I was
asking in the machine learning,
it was sort of the
different solutions
came in groups and then bands
of energies that were separated.
So if you are in a
situation where just left
was a last ground
state cluster or a few,
then maybe the
question-- as long as I
find one of the solutions
in the cluster, I'm happy.
I don't want to go up to
a cluster that represents
violators of one constraint.
So the discussion
then becomes, well,
what is the gap
between those clusters
and the gap between
those clusters?
GUILHEM SEMERJIAN: Yeah.
At the moment, it
was in classical.
So here, it's
really-- I mean, gap
is just classical energy
and just flipping variables.
So it's not maybe the
most interesting case,
where the gaps
that you encounter
when doing a modification
of the transverse field
on such problem would
be much interesting.
And this, well, I hoped
to speak about at the end,
but I will try to rush onto it
if you want to hear about it.
Otherwise, we will discuss it
later on-- privately, I guess.
OK.
So just to show, and to answer
maybe more precisely one
question, there is-- on the
case of this XOR-satisfiability
on a random hypergraph, all
this can be made very precise,
very rigorous.
And here, the clusters
can even be characterized
with the very
simple, so to speak,
geometric argument with
the notion of core-- sorry,
of the hypergraph
encoding the formula.
So this is-- the upper line is
the total entropy of solutions.
So the total number
of solutions is
exponential-- is 2 to
n times this factor.
And in this regime between
alpha d and alpha s,
it has to be decomposed
between this contribution
of the degeneracy of the
number of clusters, sigma,
and the internal
entropy of each cluster.
Well, actually, for
satisfiability and coloring
problem, this picture is
even more complicated.
And I will come back
to it maybe later on.
I will try.

OK.
So I don't have much
time, so maybe I
will just try to flash
this on the next slide
to give you a little bit
of the idea of the methods
which have been used to obtain,
quantitatively, this picture.
So there is, in
particular, the method
called the cavity
method, which relies
on the following observation.
So one can define the uniform
measure of the solution
as a product of
these constraints.
Which are, as you recall, 0 or
1, whether they are satisfied
or not.
So this product
of constraints is
1 if and only if all the
constraints are satisfied.
So if the configuration
is a solution.
Then you normalize, and you
have the uniform measure.
This kind of
probabalistic object
can be encoded, or at least
represented graphically,
by something which is
known as a factor graph.
And a very small
example is shown here,
where one puts-- it's a graph
with two types of variables.
So here, cycles are
used for variables.
So x1, xn are drawn
as wide variables.
And each constraint
is a square vertex
which is linked to the
variables on which it depends.
And so this is a very nice
graphical representation
of the problem.
And a crucial property of
this random graph models
is that, typically, this--
the random graph encoding
such a random constraint
satisfaction problem-- locally
converges to a tree.
In other words,
there are, locally,
no small loops around
[INAUDIBLE] in such a graph.
And this is a crucial
property, because, maybe
as you know intuitively
from a coding point of view,
trees are very nice and
simple objects, because they
have a recursive structure.
And so, if you want to
solve of a problem which
has a structure of a tree,
usually if you break it
in subtrees, you solve the
problem of the subtrees,
you combine the
solutions, and then
you have a very
efficient algorithm
which is what dynamic
programming is about.
And in this context,
it's also known
in artificial
intelligence [INAUDIBLE]
learning community
as the exactness
of the so-called
belief propogation
algorithm on tree structures.
So it seems, at this point,
that the situation is very easy,
because I said that the
constraint satisfaction
problems have factor graph
which are locally trees.
I just said that trees
are trivial objects.
So it should be always trivial.
Well, no, actually.
Because the fact that the
trees converge locally
to-- sorry, that the graphs
converge locally to tree--
is not sufficient to avoid
some problems of lack
of correlation decay.
So these are rather
technical points,
but the fact that the
clustering transition actually
can be related to the absence
of some correlation decay
for the variables in this
factor graph measures.
And if the variables have
long-range correlation,
you cannot forget about the long
loops that you try to forget
when you just take
locally a tree.
That's the crucial point
which is behind the fact
that these models are
non-trivial and allow for such
a clustering transition.
OK.
I don't really
have time to enter
too much into the details.
So I will just go
on but just try
to emphasize this
fact that it's really
local convergence of random
graphs to random trees,
plus, in some cases, very
complicated but a set
consistent hypotheses
on what are the boundary
conditions, the typical
boundary condition,
on the graphs, which
effectively take
care of the effect
of the long loops.
That's the main characteristic
of this so-called cavity
method.
So as I said previously, this
clustering transition actually
is not the end of the story.
And for k satisfiability
and q coloring,
it's even more complicated.
Because, as you see at alpha d,
I've drawn the set of solutions
to be shattered
into many clusters.
But it turns out that there
is a further transition called
a condensation transition
between this point
and the satisfiability one,
where the number of clusters
which bear the vast
majority of the solution
is exponentially
large, actually only
between alpha d and alpha c.
And for the latter
regime, this condensates
onto a small number, a
sub-exponential number,
of the largest of the clusters.
So again, just to
say that-- I mean,
the details of what is going on
is very much model-dependent.
For instance, in the XORSAT
model that I've shown before,
there is no intermediate phase.
And this, the last
two threshold,
are equal to each other.
And also, for
satisfiability, and coloring
the case k or q equal to 3 is
again as a special feature.
So the main message
here is beware.
The devil is in the
details, and sometimes
the details of the
model do really matter.
And so one should not try to
generalize too easily what--
[INAUDIBLE] quickly
what is obtained
on one model on the
whole class of model.
And finally, at this
point, I will just
mention that all this-- I
mean, from a mathematical point
of view, all what is done
by-- or a good deal which
what was done in
statistical mechanics
was not strictly rigorous.
So it's theoretical physics
style of computation.
But the good news--
and that's what
makes this field of
research very rich and very
interdisciplinary-- that
a lot of this prediction
is, at the moment,
proven step by step,
rigorously, by mathematicians.
Which gives more credit to
the predictions, I think.

OK.
I'm starting the quantum, last
part of it in-- I don't know,
five minutes or--
[LAUGHTER]
AUDIENCE: [INAUDIBLE]

GUILHEM SEMERJIAN: OK.
No, five minutes will be fine.
No, I'm just kidding.
So quantum models are-- in a
very simple and non-physics way
are-- OK, suppose that you
have Boolean variables.
You have 2 to the
n configurations,
which have some energies
or cross functions.
And you put them on the
diagonal of a matrix,
or 2 to the n per
2 to the n matrix.
And you add some of diagonal
terms, which represent
jumps between configurations.
And OK, the precise form here
is given by a transverse field.
So if you know what
it is, it's OK.
Otherwise, just think that there
are some of diagonal elements
that are added to the classical
energies of the configuration.
And now, well, what corresponded
to the energy or cost
function of the classical
part becomes the eigenvalues
of this metrics.
And so that's the question of
the properties of the spectrum
of eigenvalues of such metrics.
And so the question is,
we have seen a lot--
and I said that actually, there
are even more phase transition
typically in the classical
structure of these models.
And so, well, what happens
of all this phase transition,
for instance as a function
of the transverse field--
[INAUDIBLE] gamma?
And well, obviously, one
motivation for this question
is to understand
whether-- I mean,
what happens if one tries
to make quantum [INAUDIBLE]
to solve this kind of
optimization problems.
So as I said, these models
are mean field spin glasses.
And, at least in
the regime where
the so-called fully
connected regimes,
where all variables
interacts with each other--
this has been well studied
in physics since the '90s.
So what we have done with
collaborators in Paris
was to make an extension
of the cavity method
that I explained very briefly
to quantum models of this type.
And the idea of this
is based on what
is called a pass integral
representation of such quantum
models.
So this horrible formula
essentially tells you that
instead of-- if you
have some cost function,
and instead you want to
compute the eigenvalues
of the operator, where you have
added some [INAUDIBLE] amounts.
You can do this by replacing
the variables-- so for instance,
the Boolean variables--
by function,
which are Boolean
variables depending
on some imaginary
time, very imaginary.
And you modify a little bit the
cost function on this new space
of configuration.
And somehow, you manage
to get the information
you want on the spectrum
of the new operator
with quantum fluctuation.
And so, as essentially the
topology of this new model
remains the same--
if, as I said,
what was crucial for
the cavity method
was the local convergence to
trees of the random graphs,
and this remains the same
for the quantum version
of the cavity method--
so it can be extended,
at the price of handling a
lot of technical difficulty
from going from
Boolean variables
to time-dependent
Boolean variables.
But this allowed us to study
in particular the model
where there was this
replica symmetry breaking,
or this phenomenon of
clustering in presence
of a transverse field.
So as I said already
in the classical case,
there are many different
detail dependent,
model dependent details.
And in the quantum case,
it's only getting worse.

So one has to study example
by example before trying
to draw any general rule.
So I will just talk
about two example.
So the first one is this
problem of Boolean equations,
so this XOR-satisfiability.
And here, you can
see what happens
as a function of this
transverse field, gamma.
So gamma equals to 0
was the classical case.
And here, it's for
a graph which have
a special regular structure.
And also, the only thing
here that can be seen
is that there is so-called first
order transition as a function
of the transverse field.
So there are two energy levels
which almost cross, but do not
exactly cross, and which use
an exponentially small gap
at the transition.
And this is one
effect that can happen
in a subclass of this class of
random constraint satisfaction
problem model.
So this is one possible
phenomenon in this case.
And there is another
one which concerns
not what happens at
the phase transition,
but really what happens
in a whole range
of low values of gamma
or in a spin glass phase.
And here, there is a
possibility for something which
is a [INAUDIBLE] gapless phase.
So a continuum of
very small gaps
for a whole range
of values of gamma.
And this we found--
OK, in a toy model
which was a phenomenon of
crossing induced by competition
between entropy and energy
of the different clusters.
And we have also good evidences
that this mechanism is also
at work in a more realistic
version problem, which
is precisely this coloring
problem in presence
of transverse fluctuation.
OK.
I think my time is over, and I
thank you for your attention.
[APPLAUSE]
</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>