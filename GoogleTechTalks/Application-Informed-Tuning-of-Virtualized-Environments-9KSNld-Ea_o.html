<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Application Informed Tuning of Virtualized Environments | Coder Coacher - Coaching Coders</title><meta content="Application Informed Tuning of Virtualized Environments - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Application Informed Tuning of Virtualized Environments</b></h2><h5 class="post__date">2009-04-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9KSNld-Ea_o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody so today's talk will be
given by a chef able Naga he's an
assistant professor at david church and
school of computing science at
University of Waterloo and he's also a
world research award winner his research
interests are on database issue in
wireless and cloud computing which is
also the topic of the talks today as
well as data integration and data
management on the web sure thank you
Alex so can people over the phone hear
me if I speak if I stand this far from
the mic okay so my talk is about
application informed tuning of
virtualized environments so let me start
by mentioning that this is joint work
with different collaborators so a lot of
this is joint work with Kent Salem the
University of Waterloo some of it is
also don't work with shivnath Babu at
Duke University along with our grad
students and undergrad students some
parts of this work are also done in
collaboration with people at the IBM
Toronto lab their names are listed at
the bottom of the slide so in my talk
I'll start by introducing why the
interest in virtualization and what do I
mean by application informed tuning and
then I will talk about three examples of
this application form tuning configuring
virtual machines for database workloads
hint based cashing in storage servers
and scheduling sets of MapReduce jobs
the bulk of the talk will be about the
first two topics because the third topic
is still very much preliminary work so
first of all why this interest in
virtualization there is no strong trend
towards consolidation in server side
computing room the computing
infrastructure is looking a lot like a
cloud right there's a lot of interest in
cloud computing in which we use massive
shared hosted clusters of servers and
why is there this interest in clouds
from the provider of computing service
it's because of economies of scale you
can get huge savings because of the
economies of scale of running massive
data centers instead of running small
server farms so you get cheaper servers
power networking cool and etc from the
point
of the user it's also beneficial to use
a cloud because you get efficient access
to a virtually unlimited pool of
computing resources without having to
pay upfront costs okay and for
traditional IT infrastructures even if
companies are not willing or not ready
to move completely to a cloud model even
within that a traditional IT
infrastructure consultation is happening
the typical hall way of running
applications is to have one server /
application and because you need to
provision your server for peak load it
means that the server is underutilized
most of the time okay so how to improve
the situation you you improve the
situation by consolidating multiple
servers by running multiple applications
on the same servers typically each
application runs in its own virtual
machines and in its own virtual machine
and these virtual machines share a
server what do you get by server
consolidation so let me skip the slide
what do you get by server consolidation
you get the same general benefits you
get from move into a cloud the same
general benefits of sharing
infrastructure so better server
virtualization less space less perilous
cooling and the costs here are a bit
different from a cloud environment
because in a cloud environment the
dominant costs our server and power here
the dominant cost is administration so
you do get some savings but but
basically the focus series is on easier
administration okay and have some quotes
here about the cost of the left the
percentage of course that goes into the
different components of of operating
server okay so whether in a cloud
environment or a traditional IT
environment we're moving to a
consolidated infrastructure model okay
and when we use consolidation
virtualization becomes important
virtualization helps us provide users
with an abstract view of the computing
infrastructure you don't want users to
be aware of the details of the
infrastructure you don't want users in a
cloud environment to be aware of the
details of which data center which serve
they're using in a traditional IT
environment I don't want users to be
aware of other applications that are
sharing the server with their own
application so you need to provide some
abstract view some layer of indirection
between the users view of the
infrastructure the actual implementation
of this infrastructure and this layer is
the virtualization layer and this
virtualization layer in addition to
hiding the abstract the implementation
detail it also allows you to control the
mapping from the abstract view to the
detailed implementation ok and tuning at
this virtualization layer will be the
focus of my talk so the high level the
one slide summary of objective of these
of this research is that applications
are increasingly being run in
virtualized environments typically on a
cloud but also in traditional IT
settings and in these virtualized
environments there is many configuration
and tuning parameters many configuration
including decisions that need to be made
in the virtualization layer and also in
the application so the question I'm
trying to address with this research as
how can we make these decisions in the
best possible way how can I make
decisions in the optimal way okay and my
focus is going to be on application
informed tuning ok what do I mean by
application form tuning we have lots of
tuning decisions to make in the
virtualization layer now the optimal
decisions are going to depend on the
nature of the application that we're
running so the focus will be on how
what's the best possible way of getting
application information transferring
this information to the virtual
virtualization layer and making the
tuning decisions based on this
application information ok and I'm going
to focus on operating system-level
virtualization so the model I'm of my
model of the world is that user see
virtual machines and virtual storage ok
and the specific tuning decision the
specific student asks I'm going to
be considering relate to tuning for
virtual machines of a virtual storage
however I'd like to point out that the
same types of questions arise if even if
you give users a different abstract view
of the infrastructure so if you have a
sandbox environment like Facebook
Applications or Google App Engine you
might not get this you might not have to
make the same kind of tuning decisions
but there's still different styles of
decisions to be made if you use
virtualization different levels okay but
for this talk I'm going to focus on
virtual machines and virtual storage so
just so that we're all in the same page
let me spend a couple of minutes talking
about virtual machines on the slide here
is a typical view of an application and
an operating system so applications seed
operating systems operating system sees
the physical resources of the machine
when you use a virtual machine you
replace these physical resources with
virtual resources the operating system
still see the CPU but it's a virtual CPU
still sees a disk but it's a virtual
disk of course there has to be a
physical machine somewhere and you add a
virtual machine monitor to map the
virtual resources to physical resources
and this virtual machine monitor has
lots of tuning knobs and their focus
will be on how to tune the knobs of this
virtual machine monitor now yeah if you
typically don't run one virtual machine
on on a separate server or multiple
virtual machines on the same server and
this idea of virtualization is also why
they apply it in storage so instead of
operating system seeing a physical disk
it sees a virtual disk now of course at
some level you have the physical storage
devices they have they have to be there
somewhere and you have a storage server
controlling the mapping from the virtual
devices to the physical devices and the
second part of my talk will be about
cashing in such a storage server so this
was my introduction let me now move to
talking about three specific examples of
application informed tuning in
virtualized environment the first
example talk about is configuring
virtual machines for database workers
and the problem setting here first
before I talk about the problem setting
why database systems the first
two subproblems I'm going to talk about
configured virtual machines for database
workloads and hit based cashing in
storage servers both focus on a database
system as the application so why what
what's what's important about database
systems other than that I'm a database
guy like why should people who are just
why should people care about database
systems first of all they're important
they are used by many data-intensive
applications and also database systems
are interesting in that their inputs are
constrained they accept sequel and they
serve the sequel right so when we reason
about the workload on a database system
we reasoning about streams of sequel
queries or sets of sequel queries okay
which constrains the kinds of inputs you
have to deal with and also the way they
serve the sequel is very highly stylized
they have these operators that get
composed in to quit execution plans so
the way they use the resources is highly
stylized and database systems have their
own internal models of performance they
have a query optimizer that chooses the
best execution plan and it has a model
of performance they have memory managers
that have models of performance and so
on so basically we want to talk about
application informed tuning the way
we're going to get information from the
database system at least in the first
part will be by exposing these
performance models okay and database
systems also post very interesting
challenges because they manage large
amounts of state I'm not gonna get into
these challenges in this particular talk
but this is also an area of interest to
any talk when you think of running a
database system in a virtualized
environment how to deal with the massive
amounts of state they manage so the
specific problem I'm going to talk about
in the first part of my talk is this
virtualization design problem okay and
here this is motivated by a server
consolidation setting we have n database
systems that we want to consolidate on
one server so we're going to run each
database system in its own virtual
machine we're going to put all of these
virtual machines on one physical machine
okay and the
shenzhen monitor allows us to partition
the resources of this physical machine
among the different virtual machines the
simplest thing you can do is to just
give each virtual machine an equal share
of resources an equal share of CPU equal
share of memory the question we're
asking here is can we do better can
what's the best way of partitioning the
computing resources of a machine the CPU
the memory among different virtual
machines running database workloads to
get the maximum throughput so our
performance objective is maximizing the
total throughput okay and we're going to
approach this problem as a physical
design problem database systems have
different kinds of physical design
decisions that a database admin stated
typical needs to make and all database
systems now provide tools advisors for
automatically making these physical
design decisions decisions like choosing
which indices to create for a specific
workload which materialized views to
quit for a specific workload how to
partition the database among disks okay
we're going to treat this as an
extension of the physical design problem
so part of your physical design if
you're running in a virtualized
environment is how much resources do you
get okay and we're going to develop a
design advisor just like these
traditional design advisors that answer
questions like within which index to do
which which which materialized views and
this design advisor will answer the
question of how to partition the
resources of a physical machine among
different virtual machines okay now what
are the different components of the this
design advisor you need
you need a woops
yeah we're going to assume that we know
the workload of the different database
systems this is the set of seek the set
of sequel statements that are collected
from different database systems over a
fixed period of time so we're going to
run all later bae systems for a day and
see what sequence statements they they
execute and this workload description
these n sets of sequel statements are
going to go into a configuration advisor
this configuration advisor has has
performance goals and this talk am i'm
talking in this talk i'm considering the
performance goal of maximizing
throughput but in the papers we have
about this topic we also have extra
constraints you can performance so for
example you can constrain the amount of
degradation you're willing to accept for
a particular workload now to achieve
these performance goals the
configuration advisor is going to need
to consult a performance model okay some
a what-if model that tells us given a
certain resource allocation to a
specific database system what will the
performance of the workload on the state
of a system be like okay now based on
this performance model the configuration
advisor will come up with a static
resource allocation this is the initial
allocation of resources to virtual
machines okay now if our performance
model is inaccurate or if our workload
changes we will need to revisit our
resource allocation decisions and refine
them online okay in this talk I'm not
going to talk about online refinement
I'm just mentioning it as one of the
components of the solution but I'm going
to focus on the performance model and
the configuration advisor okay and yes
so the performance model is interesting
because this is how we get the
application information this is how we
get information about the specific
benefit of reefs a certain resource
allocation to a database system okay and
what we need is a cost model that models
execution of these sequel queries we
have in our oral descriptions its
throughput oriented and is aware of
virtualization is aware of different
resource allocations okay and our idea
is to use the cost estimates that the
database system query optimizer provides
when a database system is choosing the
best execution plan for a query humanity
different plans and it has a cost a
model for the execution of different
plans now this cost model is not aware
of virtualization so we need to make it
virtualization aware we need to create a
new what if creed optimization mode in
which we ask the optimizer if you had
this resource allocation what would be
the performance what would be the
throughput of this query workload okay
and to do that we need to calibrate the
cost optimizer to be aware of different
resource allocations so if you look at
the parameters that an optimizer uses to
estimate cost you'll see that it needs
to know the query and we know the query
we know the workload for database system
it needs some parameters about database
system such as the database statistics
how big is a database of the
distribution of the dnd three columns
and that doesn't change according to
resource allocation but there are some
parameters that model resources like for
example how what's the CPU cost of the
comparison what's the cost of doing an
i/o and so on and these change according
to different resource allocations so we
need to develop a calibration procedure
that gets us the values of these
resource modeling parameters for
different resource allocations okay some
database systems have this procedure
automatically built in because you need
to do this when you install they tip
just Amon a new machine so when you
solve databases on a new machine some
database systems will look around and
see what resources they have and bill
and compute these calibration parameters
based on based on measurements of
performance on the machine if they don't
we need to develop a calibration
procedure that will be specific to a
database system it will require internal
knowledge of database system but it
we'll be done once and can be applied on
any physical machine that we want to
tune for okay and basically the idea is
the idea of this calibration procedure
is to match as closely as possible the
assumptions the database system make
about the database and the query and
very only the resource model parameters
ok so the accuracy of the cost model
depends on how accurate are your
assumptions about the database how
accurate are your assumptions about the
query and how accurate are assumptions
about the modeling parameters so to find
the best values to calibrate these
modeling parameters we're going to
design a synthetic database and
synthetic weed is where the assumptions
of query optimizer are matched as
closely as possible for the query and
database so that we can now so that any
variance in performance is due to the
calibration parameters okay and the
kinds of parameters are calibrating
here's an example from postgresql you
know postgresql has parameters for
modeling cpu costs so what's the cost of
doing an operator what's a CPU cost of
an operator has a cpu cost of accessing
a couple what's a cpu cost of accessing
an index there's memory parameters like
how much memory have for how much
working memory you have for operator
what's the effective cache size what if
I 0 parameter is like what's the cost of
a random page is the cost of sequential
IO chatter so basically we design a
calibration procedure to estimate these
parameters for different resource
allocations now once we have these once
we have the different once we have the
values of these parameters for different
resource allocations we can consult the
query optimizer and it gives us the
estimated performance under different
allocations okay so basically that
that's it for the performance model we
expose their applications own internal
cost model in our case the query
optimizer cost model and we need to make
it aware of the effect of different
resource allocations now once we have
that we now have an optimization problem
that the configuration advisor must
solve
we have as we vary the resource
allocation to different virtual machines
the total throughput well very and
basically we want to minimize this total
we have to maximize total throughput or
minimize total completion time so for
our to solve this optimization problem
we didn't do anything fancy we looked at
the kinds of performance curves we were
seeing and we saw that they're mostly
smooth and concave and this is what you
expected so in the graphs here what you
have on the bottom axis is here you have
to work loads okay and what you see is
the cpu allocated to one workload and
the memory allocated to this workload
and then under z axis there's the total
estimated cost of both these workloads
together okay so for example if you
allocate eighty percent of CP to the
first workload which means that only
twenty percent goes to the second
workload and the fee allocate twenty
percent of the memory to the first work
well that means that eighty percent go
to second workload you get a total
estimated cost of 500 units okay I as
you look at these curves on the left you
have a situation where one workloads one
workload is very CPU intensive and the
other is not CPU intensive ok so in this
case the more CPU allocate to the first
word road the better okay in the second
case both workloads are CPU intensive
which means that if you if you don't
allocate enough cpu to both workloads
you're going to get a degradation
performance and if you look in the
second case the best thing to do is to
allocate an equal amount of CPU to each
workload okay so have a CP allocation of
fifty percent to each workload for
example okay and in in both of these
cases the point of these performance
performance surfaces is that the surface
is smooth and concave so for this work
we found that greedy search is good
enough okay to find either the global
optimal or a local optimal that's close
to global optimal on these
smoother concave performance curves so
what how does our research work we start
by giving all of our clothes and equal
allocation of resources and then we try
to shift this allocation to try to shift
the allocation between workloads so we
have we take Delta units of one resource
from one workload and give it to another
worker and we try every possible compare
every possible workload at every
possible resource is to find the shift
that gives us the the biggest
improvement in performance and to do
that we find the resource we find the
workload where if I take delta resources
away from this workload it would suffer
the least and find the workload where if
I give delta resources to this world it
would gain the most and I take from the
one that would suffer the least and give
to the one that would gain the most and
I do that until I could not tell affine
the local optimum okay so how does this
work here's one performance number
performance result so in this in this
experiment we're using a 10 gigabyte TP
CH database running on the postgresql
database system and here we have to work
loads the graph shows the the blue bars
on the graph shows the actual execution
time in minutes of both these workloads
if you give the two workloads equal CPU
shares so both workloads get fifty
percent of the CP of the physical
machine they're both running on one
physical machine and if they have equal
CPU shares you get the blue performance
now if you run our virtualization design
advisor it detects that the first
workload is not very CP sensitive the
second workload is as varied as more CP
sensitive so it decides to give twenty
percent of the CP capacity the first war
clone eighty percent to the second
workload the first work will of course
the grades because it we've taken away
more than half of its CPU capacity but
it doesn't big rain by much because it's
not CP sensitive whereas the other
workload gains a lot by getting the
extra CPU G so these are the kinds of
performance gains were looking for easy
performance gains by simply
making the right resource allocation
decisions decisions okay so there's more
details about this and the papers of the
scribe is are my website if you're
interested but basically this is the
general idea exposed the query optimizer
cost model use the make it
virtualization aware use it in a
configuration advisor that uses a simple
algorithm to achieve your performance
goals okay so this was the first example
of application form tuning the second
example I want to talk about is cashing
in storage servers hint based cashing in
storage servers and the problem we're
tackling here is that when when you go
to this model of virtualized storage you
have your storage devices control by
some storage server the storage server
has some cash capacity and the storage
clients such as a database system also
have cash capacity so a database system
has a buffer pool which is cash it in
which it caches database pages and when
it requests a page from the storage
server the storage server will read this
page from the storage devices and it can
cash it in its own cash as well so the
question here is which pages requested
by the story by database system should
be kept in the storage server cache okay
so why is caching for storage servers
different than cashing in other context
it's different because you have
different layers of caching you have
cashing in the client and cashing in the
storage server and whenever whenever you
have layers of caching the cash in the
lower layer sees less temporal locality
because the very hot pages will be kept
in the client cash and the server will
see low less temporal locality so it has
to there's less opportunity for benefit
from cashing of a storage server the
second thing is that you want to ensure
exclusiveness between the client and
server cache you don't want to replicate
data that's in the client cash in the
server cache again okay and because of
these two
properties caching paula's assists such
as lr you or anything also depends on
agency don't work and there have been
specialized caching policies developed
for this specific context of multi-level
caching but most of them treat the
storage client as a black box what we're
looking for here is application informed
caching policies that work well with a
database system okay and the way we're
going to do that is by using hints okay
the storage server to make its caching
decisions needs to know which database
pages are likely to be dereference in
the near future if a storage server gets
a request for a page and it's going to
get a request for the same page in the
near future then that's a good candidate
for caching if a page request if a page
of sequestered now is not likely to be
requested in the near future then I
should not catch this page okay so we're
going to have the database system pass
hints to the storage server about the
nature of its of its page accesses and
we're going to have the storage server
interpret these hints to decide which
pages to cash and which not to cash so
I'm going to talk about two approaches
for these hints one is specific to a
database system it requires knowledge of
the TV system internals and one is more
general so the first approach which
appeared in our fast 2005 paper is what
we call the right hands okay and these
hints depend on the nature of rights
from a database system database systems
right pages for two different regions
one reason database system right pages
is that this page is accounted for
replacement when a database system wants
to replace a page and this page is dirty
it has to write it back to the storage
before it replaces another completely
different reason is that database system
might be writing the page back to
storage because this page has been
sitting around in memory for too long
and if there's a failure now I'm going
to have to
recover all the changes that have been
happen to have happened its pages for a
long time in the past okay so to limit
recovery time after a failure I'm going
to write this page now okay and what we
are going to do in this approach is to
tag every right from a database system
as either a replacement right or
recovery right okay and we're going to
develop specialized cash management
policies at the storage server that
depend on these writings notice here
that it's very easy for a database
system to tag a right as a replacement
try to recover right okay and in terms
of the storage server here's an example
of a replacement policy that realizing
these right hands here's a way to add a
hint awareness to lr you basically if a
cache is not full then there's no need
for any fancy divisible just catch
everything as soon as the cash is full
the only cash replacement right request
so you don't cash recovery rights you
don't catch aids replacement rights are
a strong hint from a databases the
replacement rights are a strong hint
from database system that it's going to
evict this page as being written okay so
that's the prime candidate for
replacement because at least you're
guaranteeing exclusiveness even if you
can't guarantee a short three reference
distant at least regarding exclusivism
okay and I'm saying this is an extension
to lr you so in this case we manage the
cash using an L are you listening okay
so this is an example of hint based
policy at the storage server now we also
developed other hint aware cash
management so there is this Albert Lee
called multi cue that was developed
specifically for storage for cashing in
storage servers and we added the righted
awareness to multi conversations you
also develop our own caching policy
that's based primarily on the right
hands we call this the type Q algorithm
I'm not going to go into the details of
these algorithms basically this
description of how it happens to LRU
gives you a general idea of the
of the approach but I'd like to show you
some results so to test this we
instrumented db2 to record io traces and
tagged these traces with right hands
okay and here using the TP CC benchmark
with a scale factor of twenty five so we
start with a 2.3 gigabyte database and
because of the nature of TBC see that
database grows as the benchmark runs and
we use these traces and a trace
different simulation to compute the hit
ratio in the server cache with under
different policies okay and what I'm
showing here is one experiment where you
had a 1 gigabyte buffer pool in the
database system and 500 megabytes
storage server cache okay and I'm
showing the hit ratio under different
caching policies on the far left is l
are you and L are you with hints and as
you can see as I said in the beginning
of introducing this part Lu does not
work in this context okay next up are
these mq policies which are which is
this multi q policy which was developed
specifically for storage servers I show
multi q without hint and with hints and
as you can see hints give you better
performance for for LRU and also for mq
ok now the yellow bar is our hint
specific caching policy and as you can
see it from other policies the blue bar
in the end is the optimal offline cache
policy so this is what you get if you
know the full trace ahead of time which
is of course not practical in an online
setting but it shows us the size of the
opportunity to us what we can inform
okay so this is one approach to hints
now the second approach which appeared
in this year's fast conference tries to
generalize this a little instead of
requiring specific knowledge of database
system internals and how it does right
and instead of focusing only on the
rights and instead of having the cat
policy be specific to a specific type of
hint we wanted to have a more general
hinting mechanism so we're going to have
the application the database system what
any other application include all
information that it thinks can be
relevant for caching okay so for a
database system it can definitely tell
us whether this is a the request type as
I read or write and if it's a right
whether it's a replacement right or a
recovery right but it can also tell us
what's which table is being accessed
which buffer pool this page going to go
into all kinds of information about this
i/o request that can be relevant for
caching at the storage server okay so
it's still easy for the database system
to provide all of these hints by simply
attaching them to IO requests now from
the storage server side we need to
figure out now which hints are useful
which hints are not useful okay and the
right hands approach we did that based
on our knowledge of the semantics of
these writings here we don't want to
have a caching policy tied to specific
semantics of specific ins so we're going
to have the database that the storage
server track the reference distance for
different hint combinations and it's
going to only cash the hint combinations
with a short reference distance okay now
the challenge here is that the number of
possible hints is exponential so we need
to reduce the space required for
tracking greater friends instances and
we have two approaches for doing that
one is to use a decision tree to to
identify which hints can basically to
introduce the dimensionality of the
space another approach which is what I'm
going to show here is to only track
frequently occurring hints okay and when
we tracked recurring hints we need to
identify which means we're getting this
stream of hints from the from the
different clients I don't need to
identify which of these are frequently
occurring in online way of course there
are many online frequent item counting
we use one specific algorithm called the
space-saving algorithm that was shown to
be better than others in a recent study
ok so we tracked the frequent hint
combinations online and we keep the
reference distances for these frequent
combinations and we cache hints that are
frequent and have a short-eared
resistance ok so kind of performance do
you get with this general hint approach
this is the same benchmark I showed you
before but it's a different type of
graph so on the x-axis now is the server
cache size remember the client cache
sizes one gigabyte the database system
cache size one gigabyte and on the
y-axis is still the hit ratio so what
you can see in the bottom the green and
blue bars are lr u which is the simplest
policy you can do and arc which is
another policy that was developed after
multi q that was it's also specific for
this type of caching but both of these
are not hint aware ok now from my
previous graph you saw that our this TQ
algorithm the algorithm designed
specifically with right hints in
specifically based on writings
outperforms all the others so I show TQ
on the graph and you can see it has a
better hit ratio than talking about
greater than L are you but then the red
is this general hinting approach the red
is I have the database system give you
all interesting information all
information that can be potentially
relevant and have the server interpret
this information to find which are
interesting hints for caching ok and you
can see that this general hinting
approach outperforms all the others
because it definitely detects that the
replacement rights are good candidate
for caching and recover all rights or
not which is what we had in the previous
algorithm but it also detects other
caching opportunities based on different
types of things ok and the top line is
still the offline optimal which just
shows you the size of opportunity we
have
okay so this is the yeah I'm going
except to skip this slide this is the
second problem this is the second type
of application form tuning and the last
few minutes I'd like to talk about a
third problem which you're just starting
to look at which is scheduling four sets
of MapReduce jobs okay so here we're
shifting from the application being a
database system to the application being
MapReduce jobs and the problem we're
tackling here is how to run a set of
MapReduce jobs on a cluster of machines
to minimize the total completion time of
these jobs okay and with MapReduce you
have lots of tuning decisions to make
like how many machines to assign a
specific job which machines will run
which jobs and how do i schedule the
jobs on the machines I have a batch of a
MapReduce jobs a cluster of machines my
goal is to minimize the task span the
total completion time of all these of
all these jobs okay now for this this
can be done in any MapReduce scheduler
in our case we use Hadoop the welding
open source implementation of MapReduce
and we run the Hadoop nodes in a virtual
in virtual machine so every job has a
cluster of virtual machines that are
each running Hadoop okay why do you do
it this way because this helps us get
isolation between the jobs and it helps
us in flexibly allocating resources to
these jobs however it this problem of
scheduling MapReduce jobs can be tackled
within traditional MapReduce scheduler
without using virtual machines but for
the for the next few slides I'm going to
be talking about running Hadoop nodes
and virtual machines and the question of
the addressing is how to allocate
virtual machines to Hadoop jobs or how
many virtual machines for a choke job
and also how to schedule these virtual
machines on the cluster of physical
machines okay now
what do we need to solve this problem we
need two things we need a model of
performance of these Hadoop jobs and we
need a scheduling algorithm that's based
on these models of performance okay
these Hadoop jobs are interesting i'll
introduce a new component or a new
aspect of tuning in these virtual
infrastructure which is experimental
model building for database systems we
relied on our sim at our knowledge of
the internals of the database system and
we relied on the internal performance
model the database system itself
provides for a Hadoop job we don't have
an internal model performance okay so in
this setting a very good way or in my
opinion the best way to model
performance is to use experimental model
building okay run experiments to sample
the space of possible job executions
that the performance as a function of
different parameters such as they
different a table size different data
sizes different number of nodes
different data distributions if you will
and then fit a statistical model
absorber forms and one question that
arises is where do I run these
experiments since I'm using a cloud
infrastructure I have a capacity to run
experiments but there are still
challenges that I need to address first
challenge is how do I design is
experience because I have a full
factorial design or they have something
more fancy to minimize the number of
experiments I need to run and get the
most improvement in accuracy per sample
okay the second challenge in this area
is how do i scale a model from a sample
to sub 2 that's based on a small sample
how do i run a model on a small cluster
on a small data set and scale the
performance how do i run experiments on
a small cluster of small data type and
scale the performance to large cluster
large citizen
okay and I don't have an answer yet to
these questions but let me show you what
we do now what to do now is we have a
fixed data set so we know the data so
we're going to be run on we're going on
and we simply want to model the
performance as a function of the number
of nodes we give to a specific job ok so
we run experiments on different numbers
of nodes and we fit a power regression
to the observed execution time we also
classify jobs as CPU intensive or are
you intensive because that's going to
help us when we allocate jobs to
physical machines ok so here's an
example of what we get so these are two
Hadoop tasks the first the blue one is
sorting we're sorting I think a small
amount of data this has a task that
sample tastic comes with Hadoop the red
task is k-means one run of k-means
clustering over a synthetically
generated data and what you see and x
axis is the number of nodes you assigned
every job the y-axis is the execution
time in seconds and solid is the actual
time dotted is the estimated time
according to our model you can see that
our model is quite accurately tracking
the actual execution time of the jobs
and one thing you can see here is the
reason why have the scheduling
opportunity for any of these jobs the
more nodes you give the job of course
the faster job will complete but then
there is this me in the curve there's a
point of diminishing returns at the
point at which adding more nodes give
you diminishing returns so if you have a
set of jobs you have an optimization
problem to decide how many nodes to give
to each job ok and this is what our
scheduling algorithm will do it will
decide how many nodes to give to each
job and also how to allocate the jobs to
the physical machines ok because we know
the resource remember I told you that
when we're remodeling these jobs we
Calif I the Mississippi intensive or IO
intensive so we can we can try to pack
as many of these jobs on the
machines as we possibly can based on our
understanding of the resource
consumption okay so we have two
algorithms which I'm not going to get
into the details of one of them
schedules pairs of jobs so I have
k-means and sort are basically given
given a set of jobs I look at these jobs
appear at the time two at a time okay
and I try to schedule them I try to
schedule the best two jobs that go
together on the cluster at any one point
in time and I do that until i have
scheduled all my jobs okay now we also
have another scheduling algorithm that
is based on an integer programming
formulation of this problem this can can
schedule any number of tasks currently
on the the cluster but we're still
struggling with integer programming
solvers to come up with it with an
efficient and with a good and efficient
solution for our internship program yeah
but the point is that once you have the
performance model once you have the the
performance objective you could
formulate an optimization problem and
solve us off the midget problem in a
standard way to get the best schedule
okay and in all cases the scheduler when
the schedulers try to allocate the
virtual machines to physical machines
they try to overcommit the physical
machines to jobs okay so if they see
that I have for example a machine that
has io intensive jobs but no CP
intensive jobs yet they try to add CPU
intensive job so basically we're running
Hadoop jobs in virtual machines we can
we can take a physical machine with four
cpus and two disks and on this machine
run virtual machines with a total of
saying eight cpus and two disks 88 cpus
and and three days okay why can't I do
that or what's the advantage of doing
that it's because we know that whether
these jobs are cpu intensive villario
intensive okay so what kind of
performance results do we get with this
as I said this is still we're
progress so we're starting with a tiny
cluster of 9 machines and for really
small Hadoop tasks one is sorting
another's k-means clustering building an
inverted index Wikipedia and paralyzed
similarity of Wikipedia documents based
on this inverted index the sort task is
something that's available as an example
task with Hadoop the other ones are
things we developed ourselves and we
form different workloads by combining
different instances of these tasks
remember the ideas that I have a set of
MapReduce jobs and I want to schedule
them so what's my set of MapReduce jobs
in one case I have to sort jobs to K
means whatever index another case I have
two sorts 1k means or whatever in the
control box it is dirty etc we conform
different workloads by combining by
choosing at random from this set of
tasks and here's the performance we get
what i'm showing is for the three
different workloads the completion time
in seconds blue is what you get when you
run these tasks one after the tether
giving every task the full cluster ok
green is what you get with our pairwise
task scheduler so it tries to pack
things on the cluster but it can only
pack two jobs at the time since you have
a small cluster two jobs it's pretty
much what we can more mostly we can we
can do very well by supporting just two
jobs ok red is our multi task scheduler
which can combine more than two tasks on
our cluster ok and you can see that you
get some extra performance improvement
the performance improvement is not
dramatic but then again you didn't have
to do much work depends performs simply
by doing the right scheduling and
resource allocation decision ok so
that's it for the third part of my talk
so basically to conclude computer the
computing infrastructure we're using is
becoming increasingly virtualized which
means that applications are increasingly
being run on in virtualized environments
we need to make unique decisions at the
virtual
layer and also the application layer and
what I'm arguing in this talk is that
these two decisions should be
application informed if you make these
two decisions in an application informed
way you get easy and cheap performance
gains okay now at the abstract level
this is you know motherhood and apple
pie who would argue about against
application form tuning what I showed
you in this talk is three complete
examples of how to make these
application inform decisions for
configuring virtual machines ridiculous
workloads for caching and storage
servers and for scheduling MapReduce
jobs so that's it thank you and the
hypes take questions any questions yeah
the question I have is the youth the
information from the applications in
order to tweak the virtualization and
make it faster how much information can
you a does a human need to provide and
how much information can be
automatically detected so the goal is to
have most of information automatically
detected so for example for the first
part of the of the talk all the database
administrator the human needs to provide
us with the workloads so here there are
database systems I want to combine on a
storage server and here are the
workloads that have collected for each
database system actually the human
doesn't even need to provide the
workload at the human simply needs to
start and stop workload collection on a
different database systems for the
second part of the of the talk the human
does need to provide anything for the
third partner to talk about scheduling
MapReduce jobs the human will need to
provide the tasks to be scheduled and as
we refine the modeling approach maybe
the human leads been involved in this
experimental model building as well but
the hut at the high level the goal is
always to minimize as much as possible
human involvement which is why you get
as much information as you can from the
application itself
well I have another question about my
producers this is really this is really
useful if you have a MapReduce that you
need to run every day because you can
clearly tweak your MapReduce before
starting the system yes but what if you
have a super duper big MapReduce that
will run for hours and hours and you
don't want to take the time to tweak it
can you tweak it during the process
where parameters will be changed online
in principle yes that's possible again
we'd have to go back and see how we're
going to obtain the performance model of
this MapReduce job online while it's
running yeah I mean what I'm assuming
here is not have shown you the MapReduce
tasks were modeled offline so basically
we aren't experiments on MapReduce class
and we run the same apparition asks now
if we take this a step further you run
MapReduce that you build your model
based on some tasks and you'd run tasks
that don't necessarily have to be the
same but they have the same
characteristics the reacting the same
data sets for example using the doing
the same kinds of computation and so on
now if there's a task if you haven't
seen before at all then in principle it
would be great to be able to do this
scheduling online but then we'd have to
figure out how are we going to model the
performance of this task in an online
way which is something we haven't
figured out yet
like say if it's a multi our MapReduce
job as as a clogged Giza could we just
do based on after the first hour of
execution we get some data and maybe we
know how far we are in the process and
then we can estimate them recalculate
and read or give more resources to the
visual machine where the map will reduce
transfer back differently the job on
another machines definitely yes yes I
mean I have some work with shivnath
bubble and Natasha home with my student
on on this experimental modeling for
long running queries and in this work it
it makes me you get more bang for the
buck if you do this kind of optimization
decisions for long running queries
because there you are you any any
refinements in the infrastructure will
have a long time to take a photo to have
their impact ok whereas if it's short
running queries even if you make the
best decision there's not that
opportunity for performance improvement
anyway now with MapReduce job that takes
it a step further because when I talk
about the long run in query in a
database setting it's typically minutes
you know 10 minutes 20 minutes when I
talk about a MapReduce job it can be
ours right so so definitely yes you can
collect information during the first
hour of running and use information for
tuning later on actually you can explore
the space in the first hour of running I
mean you you you can run experiments if
you will while the well the plot well
while the job is still running you can
explore parts of the space that you know
might not be optimal but you have hours
and hours to run anyway so it's okay to
make a bad step just for exploration
purpose so in principle yes definitely
this can be done but basically the we
haven't worked out the details yet I
don't have a solution to present you
that does this
so I'm just wondering besides making
these optimization tune in decisions
based on the applications that you're
dealing with are there any other
techniques so besides application
informed decisions are there different
things that you may be able to look at
this is I guess no out of your out of
the specific fields that you're looking
at but I'm just wondering if there's any
other techniques that are out there so
basically right now you're making these
tuning decisions based on the
application yes you're dealing with are
there anything else that you could do
besides that in the future it's just a
general question if you don't have it
for if you don't have application
information you could still get some
benefit by doing these the tuning
actions and if you don't want to collect
information information you can still do
things like look at the resource
consumption locally and do local
decisions based on that the goal is to
do better by cooperating information
application information
yes it's about your integer programs
that you use for computing a bad
scheduling Harvick is this optimization
problems and how long does it take to
solve it's fairly large so what was we
had a variable for every we had a
variable for every node of the cluster
and every job so the next so number of
variables is the number of jobs times
the number of nodes in the cluster okay
how long did it take solve depends on
the solver of course typically you know
minutes are those variables 0 wine
variable the area yes k again UCT you
solve it in minutes yeah okay okay good
how would that compared with the savings
that you're getting out of the better
search schedule it's it's usually still
get savings like if you factor in the
cost of solving the integer program that
still you still get a good significance
so in the first part of the talk you
also mentioned you show the two
workloads and you showed how they would
model the CPU and the memory kill
ization and that really would work too
would be good enough to obtain the best
resource allocation would greedy still
be good enough if you have a larger
space space of jobs like ten jobs or
more jobs would be yes yes I mean we've
what what I've shown is two jobs because
that's what I could fit on a three
dimensional graph but we've we've looked
at this for larger numbers of jobs and
we still find it for the GED so
basically in the larger number of jobs
we don't do visualization of course what
we do is run an exhaustive search and
run the greedy search and see how close
the optimal you get from exhaustive
search is to the optimal got some
research I said and we typically find
that they're very close to each other I
mean there's there
basically here greedy search is we're
using users because it worked good
enough for us but there's no reason why
we couldn't use more advanced
combinatorial search if we need to okay
thank you I think that's it thank you
thank you very much there are there any
questions from the people on the phone
all right thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>