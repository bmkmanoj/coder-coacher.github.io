<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Domain Adaptation Workshop: On the utility of unlabeled samples in Domain Adaptation | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Domain Adaptation Workshop: On the utility of unlabeled samples in Domain Adaptation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Domain Adaptation Workshop: On the utility of unlabeled samples in Domain Adaptation</b></h2><h5 class="post__date">2012-02-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TD0L6Oc1-_s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is just with
and we talk about the utility of an
example of a rotation so you will see a
second but the main point that we want
to emphasize is that the question is to
what extent can we use data from today
so I think I have a quick introduction
to meditation which we think today most
of the teen way that we have in machine
learning talks about the stationary that
does not reach a bit for training the
same that you have to test on and there
so this is the assumption about the data
generation process and as we already
mentioned several times today this is
not always realistic so we want we have
to address other more complicated
situations
movie was showing so the world of
tension is we train one type of domain
say distinguishing male female the
features and what extent can we utilize
the information that we've got the first
domain in order to focus on the second
main in doubt lots of applications to
this
monitoring and verification in natural
image processing and this is so
prevalent that many people have worked
on it and lots of uses of penetration
however we have very little
understanding of the meditation so far
and that's the direction that we're
trying to aggressive
they're trying to understand why and
when the manifestation succeed and so we
distinguish three aspects of developed
nations women so if you want to describe
the mental patient problem you need to
somehow answer these questions the
Joseph is what type of info data is
available and what kind of input we'll
get what some sources of the input in
someone and the number to this in a
second the second is the question of
those of the relationship between the
source and the target tasks of course if
there's no relationship between the
source and target is doomed to fail and
Miriam was describing there's a lot of
effort on trying to model the
relationship between the source and the
thoughts aspect which we feel has not
been addressed strongly enough so far in
there in the literature is the prior
knowledge of the level we know very well
that one of the basic principles
no free lunch a phenomenon they tell you
cannot really succeed in learning
without any prior knowledge and when
attracted to the meditation we have to
look at what kind of prior knowledge
about the past or most possible target
will they use the help
and how we need to affect our ability to
move an invitation so we were mainly
addressed is nothing but if they least
addressed in previous literature the
other coordinated on which I want to
call your attention is there there are
two types of algorithms of meditation
the first type is conservative and
absorbance elements these are algorithms
that train on the source data and then
use the outcome of training or source
data to solve the problem of the
competence and in many situations we can
say if the tests are not too different
then if you train well and you have a
good solution for the salty solution
it's also good filter the more
interesting direction is the adaptive
algorithms these algorithms they really
use utilize some information from the
target in order to change your predictor
and adapt it to the target and not just
use the same predictor that you learned
for the stuff so we will want to focus
more adaptive
space of algorithms so let me what I'm
going to do now is to go through these
aspects and elaborate a little bit on
each of them so I'll start with the type
of input available to the learner
and we can see the frame interpretation
as you try to learn but you don't have
the real data that you would like to
have you would like to have labeled
examples generated to the target tasks
but you can't give so instead you get
some proximate what is the proximity
that you can either label data from a
different distribution the source
distribution all lots of unlabeled data
from the target and this is kind of
proxy to the information you really
would like to have any favorite targets
information and we want to know how much
do you lose by using this proxy and
training data rather than the data you
would have rather and there are lots of
questions one is them can we land use
just source generating data hey can the
container data be beneficial can we use
it in a very effective way the target
and evil data and of course there is the
question that the thing that is going to
talk about how can you utilize on top of
the training data
I'm able to copy data however you also
used some neighbors few labels from
Atlanta that's what you were aiming as
future research on authenticity your
second type of proxy data that's already
a question even if you
if elected source and target
distributions are the same
I pushed up for the really problem and
it's already a big so it's really a big
problem how can we utilize and label
data in a small saying it's not just
they are never having to do an
interpretation they remember even a
little that than your own comment which
is it's not clear how to utilize it they
know what I might do crazy because this
is cheap data compared to the label
so the second aspect that you want to
analyze in order to define the
manipulation problem is how do we
measure the relatedness between the
source and target us and there are two
aspects we will pass this distribution
over instances and labels so we can ask
you can somehow factorize this question
into asking what is the relationship
between the marginal distribution over
the unlabeled data and what is the
relationship between the labels of the
two powers so 90s between for the and a
departure data we can view it in other
two viewpoints one is looking to matthew
TV panels of the
resolutions and the other is looking at
the beauties of the business well what
we've seen so far how the banks based on
additive measure the discrepancy that
Mayor was talking about is an additive
we look at the maximum difference
between the weight of the set under the
source that distribution and the way to
the same set under the target we look at
the difference it's an additive distance
we will also talk about the
multiplicative the ratio between the two
weights
so these are two ways of measuring the
discrepancy between the and energy
solutions and then there is the question
of how do we measure the relatedness
between the labeling because we need to
also worry about how the labeling of the
two tasks are related and we could
either talk about absolute a
relationship like the comparative
assumption that an operator in the
future and later always relative to some
hypothesis class which is what we've
seen so we are measuring the relatedness
on the unlabeled distribution and the
relatedness on the labeling function
and the last point that I want to focus
on is that this issue of what you could
find on available leather so the
delicate hair prior knowledge about the
sauce tasks in the lemon can help our
knowledge about the fabric does and each
of them can be in different forms it can
be in terms of realizability with
respect to summer for this class like
the things that we've seen alia you have
you assumed that in true labeling
functions comes from the glass they
should be on the table you can say that
it's just as you can talk about Canon
but we have to distinguish here between
all of those measures and prior
knowledge the continuous effect to the
source tasks in the cable spectral
target ask and those are two different
sources of my knowledge these are two
different situations so we want to
analyze what is the difference between
these two types of knowledge and we'll
show very clear distinction between the
kind of results that you can obtain if
you assume knowledge about the sauce
task to the kind of results that you can
obtain if you assume find knowledge
about the toughness so disappointment
like to mention some previous work so
this was a lot of work on conservative
algorithms have to imagine conservative
additives alone
evidence that learned the sauce and then
used what you learned about the sauce to
predict for the target so this was the
the work with a with John Boehner and a
in the coke ml and the Fernando Pereira
their take gave the first bound and then
there was an extension miniaturization
of designed by insular and option and
then the bounds are basically of these
type they say that the error of the age
that you train on the surface when you
train on the sauce data its error on the
target is bounded by these empirical
arrow that you go to the source plus
plus two terms and one of them is the
additive distance between the marginal
distributions and the other one body the
other term has to do with the
relationship between the latest
so this artifact the very first time
planted because it is about conservative
values and for adaptive algorithms it is
the villain more work needed some bounds
and more recently causes Manson movie
but showed the benefits of using the
unlabeled that I know to get be waiting
so we are not using exactly the same
opposed this we use we waiting in order
to adapt the oldest to the target
distribution and in this paper we showed
some limitations of those techniques and
we show that the assumptions that are
made by those results of our basically
all necessary without them you you have
the risk of failure yeah just a
clarification
we actually tried to mostly criticize
but people have done great mostly we've
been trying to say that there's are
things that have been done for
importance waiting just don't work we
try to supply and then the same the same
line here
okay so so let me now go to the more
formal part of the talk solo for me at
least we have some domains X we have
label said the for simplicity would be a
0 1 so any classification manual
classification task and the source
distribution is the probability
distribution over the domain with labels
in the party the different probability
distribution over the X and the labels
and the letter gets a labeled training
sample from the source in an unlabeled
training standards from the target and
he has to come up with some predictor
with some classifier in the success of
this classifier is the error in respect
to the target is measured by the
probability over the target distribution
that your predictor does not predict the
correct label so let us stand up a
formal set up I think the only thing to
mention here is that it's a setup in
which we have labeled examples generated
by the source and only unlabeled
examples generated by their target and
we want to predict well with respect to
this is the 0 1 loss in binary
specification then we will make a very
strong relatedly
assumption with respect to the labeling
will assume the coherent shift
assumption that's the strongest
assumption you can assume your suspected
labeling it says that the labeling on
the source and the target is exactly the
same thing if you got the same point X
the labeling that the target will give
you will be the same as
so scary or the probability that the
target is a strong assumption but still
lots to do even hadn't such an essential
okay so and I will say we want to shift
really to the measure of relatedness
between the underlying marginals rather
than looking at additive measures of the
discrepancy and defining him a relative
or multiplicative measure of discrepancy
so we take some family of substance of
the domain and the multiplicative
discrepancy between the two
distributions is the infimum over all
the sense in the collection of the ratio
so now rather than looking at the
difference between the way under the
soft distribution in the way to the
target distribution here I look at the
ratio between the body under the source
and the weight under the target and then
this is intuitively it's very a natural
approach to take because if you have a
point it has high weight with respect or
the subset it has high weight with
respect to the target it has very low
weight with respect to the source you
are unlikely to say to your this point
in your training sample because we are
training according to the source so this
is a set B that has high weight respect
to the target but small way to respect
to the source it this is a set on which
we really care about the labeling in the
target task but we don't get much
information when we do the sampling
training that of the source so we don't
care about this ratio of the weights so
we are going to see what ended up under
this multiplicative measure of
certified the universe of importance
cool it's like the interest of the
opponent exactly that we're doing it
over some collection of subsets and up
so using this I mean here is a very
simple observation just as if if we have
a bound on the on this ratio between the
weights of all points in the domain but
then it is very easy to see that for
every and part of this it's arrow on the
target is no more than its arrow on the
source times this bound of the weights
so this is a very simple observation and
and one hand it looks strong because you
know it doesn't care what you do what I
would have used it's a very conservative
thing you train something which behaves
nicely on the sauce it will also behave
not so badly on the tablet well of
course there there's some issues with
such a single bound
and then here was no enable data me that
is only concern the problem is such
about is that say this bound is very
nice if from the source you make no
error this is the error of the source
approaches zero then the arrow on the
target will also approach it but in in
realistic cases we do make our
when this valve becomes that if you make
error epsilon with respect to the source
your error inspector that will be this
factor times epsilon plus and tell that
depends on how what is the optimal
respect to the source and this may be
very very high
since there are expect if the best to
air our inspector sources point one and
this ratio is 10 this factor will
already be one in telling them is bound
is completely meaningless sure I usually
feel like this is not the main problem
with that fire the main problem is that
that ratio could be huge right okay so
the measurement here should be enormous
it could be infinite I mean these
massive users who say this is a very
straightforward without just saying that
this weight ratio point wise very strong
okay so
okay so but we want to be the first
results will show the first meaningful
result you know it will show how to
overcome this term in a very specific
setup in the very nearest neighbor a
prediction other additional assumption
and the assumption is that our labeling
function satisfies the religiousness
condition so we want now to look at the
situation in which we can overcome this
term we can get every respect to the
target which is arbitrarily close to the
vest there you'll get respect to the
sauce using a very simple nearest
neighbor algorithm and we will need some
assumption about the nice list of the
underlying electric function so as we
like the indigenous condition is if of a
functional any function in is you want
that the two close points will not have
different flavors so the reckoning can
be only home if the data is very nicely
cluster the ones are separated by the
zero from the zeros and there is a gap
between them if there is such a gap then
I can guarantee that no close points
have opposing lengths but if the data is
continuous no religiousness can never
hold because you will always have points
around the boundary which have opposing
labels in the our trial across to each
other so elÃ©ctricas will always fail if
you have a binary classifier we overcome
this by relaxing the Lipsius masculine
notion of the probabilistic richness and
pro-police decryption is measures just
the probability that deliciousness will
be valued so we relaxed the corollary to
this assumption so holistically and
viciousness which says the probability
reappoint with a neighbor of a different
label which is close to our point this
probability is bounded so we bound the
probability of points being close to
points of opposing lane so this is some
way of quantifying something weaker a
relaxation of saying my mother it
clusters into clusters of homogeneous
labels and it is kind of saying the
distribution around the boundary is very
low there are few points in the boundary
areas those are the positive violations
so under these relaxed which this
assumption we can show that in the u.s.
naval algorithms we can get a strong
demand rotation results because you
still have that error term as long as
that relaxation isn't exactly how will
you see that we can we can eliminate
this the as the sample size grows our
arrow when we train ourselves will
approach the arrow that you would get if
you fail
we completely eliminate this so what is
the algorithm salmon is very simple
nearest neighbor algorithm
you've given a sample yes from the
source you just used it for nearest ever
now you get the point to the target
label each according to the label of the
nearest neighbor point from your
training suffering the source so these
include 79 working tez induce any it
doesn't use any unlabeled target data
and we will provide something directives
for this for this algorithm and so that
the basic unit age we can paint under
lishus miss position this is this
evolutionist position we can get that
they error with respect to the target of
the hypotheses generated by these
nearest neighbor simple algorithm is
bounded by twice the base you know plus
a term the depends on deliciousness plus
a term that goes to zero as their
samples they go to infinity but the main
points to note him is that if we trained
on the target we will get the same bound
except for the term here that depends on
this
weight ratio and the term the term
depends on the genus will still be
around the only difference is the tempo
depends on the weight ratio and this
goes to 0 sm goes to infinity and this
term times the base arrow can be
improved if we instead of one nearest
neighbor we're doing K nearest neighbor
then we'll get here instead of 2 we get
1 plus 1 over K so we still several
algorithms and the relaxed witches
assumption we can train on the sauce I
mean use the sauce for nearest neighbor
and year's neighbor algorithm with
respect to the target who give me an L
the nearest neighbor arrow respect
ourselves somehow we overcame the
problem of discrepancy between something
started a problem with your capital C as
I've done during you've seen ballot
paper on the course way back
this is the problem capital C or
whatever camera simulates is this could
be huge
cheese a mess like this then the debate
I think the venue is result is meant
maybe conceptual it tells you that
asymptotically this M goes to infinity
you get the same error with respect to
the sources respect to target but only
asymptotically and it should be
contrasted with the previous results
that had there a negative term that you
cannot overcome even if the sample size
metric thing as well this depends and I
think we have a listener in a free way
because we this is actually what happens
quite a lot when you do an importance
way and I think one way and then I try
to applicative algorithm just
conceptually I want to show that the
some situations and as you were
mentioning I think in open yo and MIPS
last year paper that something is the
probability of seeing those points with
high a weight ratio is small then we can
somehow improve this constants to chop
it off in
is that already how you're gonna like
being opt I mean before when you have
one over C times odds right you you have
this constant term which which is not
dependent on but this is this is day and
I don't see the base optimal predictor
for the targets right but but the way
you got rid of the one over C
multiplicative factor on them before in
the original bound you have a 1 over C
times opt PT right right and I mean I
guess I mean I don't know one of us C
opt feet will not change this angle this
angle and here like Tennessee will go to
0 is negative but as is the way your way
you use the Lipchitz property is to
eliminate that for those points which
occur only in the it's not delicious
recipe simply needed I mean the use of
deliciousness is needed for usual
nearest neighbor guarantee this way we
can show the disdain and we can show
lower bound on the nearest neighbor
learning on it without the meditation
which are very close with it basically
you're just eating the loss for those
points that are close to the boundary
right without seeing them as either
proof that it's clear to everybody what
we're discussing here but I think the
main point here is that there are and
then we can analyze the performance of
nearest neighbor without domain
adaptation you have a task using nearest
able to predict the results analysis we
look exactly like
except for these terms think you should
let that's what I'm saying this except
for this term that is multiplied the one
over N the analysis for without urban
adaptation gives you exactly the same
bounded analysis with no meditation so
in some sense all the penalty that you
paying for him to adapt between the
domains is destructible okay so even
Lego data necessary
hello get the target that because the
two examples we've seen so far the two
is as we seen so far will results in
which I was not using the negative
contribution
so maybe under this strong country a
wise racial assumption we don't need
target you know and we were going to
show that this is not the case and we
have two scenarios in which we can prove
that you really have to rely and utilize
data otherwise you are bound to do much
worse so Allah first scenario is the
scenario of problem so what is popularly
token learning a situation which you
want to learn a classifier that belongs
to a specific class so it's not just
trying to predict but you try to believe
me the specific classifier
so for example you want to have a
classic part that can be computed very
quickly we want to have classify that is
readily in evidence so one example is
that if you release a lot of work now in
trying to have to have a visual help to
do driving so this company in Israel a
mobile eye that develops a devices that
have a camera on the car and they watch
the road and they notice if you about to
hit an obstacle or to hit the person
you're falling asleep and there is
another so such as such a classic file
if it has to be picked whether you're
falling asleep or not has to be very
fast so you want your classification
function to be something that can be
very quickly computable and half spaces
are a good example of this you are
willing to sacrifice some accuracy we
have some false positives for their
creative prediction so if you're in the
situation that you want your classifier
to come to and give it class we can
prove that unlabeled data is a
completely necessary we have lower
bounds that you cannot do anything
without them and they will tell you that
okay so the problem is that this we are
given some approaches plus h and the
resource a labeled sample and inaudible
target sample is before and the
difference is the output now is not an
arbitrary classify but it should be
classified from this class of nice
classifiers H and maybe even before I
show will be able to let me show you in
picture what it means so assume that
this is this is my regular function and
I want to output a linear classifier so
no unit classify does a perfect job here
but in order to choose the best in a
classifier I need to know something
about where is most of the weights of my
talent
it will tell me where am I going to
sacrifice more easily in my arrow so my
algorithm is my to be I use the Sunday
under to learn some function on the
target may be very other function then I
use this function of the target in order
to label the animals
I have an unlabeled something from the
targets I use the ugly function little
ants in the first step in order to label
this data and then they take this sample
with the new labeling that I just
assigned to it and feed it to a level of
half spaces for the target so this is
two-step approach you first learn with
an IV function you use the undo function
to label the honeybee data then you feed
the animal data into the leather of the
nice function that you want to output
and in the case of half spaces the point
is that there are lots of husband's here
all those in the gray area to make the
same arrow respect resource but to
choose which observe is best to inspect
you a we have to know whether
this dome is heavier or disabled and
what we actually using we're doing is
using the unlabeled target data in order
to determine which of all of those half
spaces it will be equivalent from the
point of view of the source which of
them will best fit the target
know the area which I mean all of those
classifiers the source of the mission is
to be the source of units on this
mission the gray area is the area in
which all the classifiers respect to the
staff the whole equal making the same
error we expect to the target it's not a
uniform solution inspector talented some
if the distribution of the unlabeled
points the same library the different
allegory solution so in order to tell
which half space is best to respect to
the target we inevitably means unlabeled
data for attack so in such situations we
can prove that they are labeled at the
contagion is the center and it's quite
simple but it's a very practical
situation I mean like company like mocha
die if they want to Train
detecto to different countries they can
vary when one country assuming the
provide cheap assumption that the
prediction whether the driver is falling
asleep or not is the same regardless of
distribution of the behaviors of the
driver now when you go to another place
you just in unlabeled data and you can
adapt your predict and here the
unlabeled data is provably necessary and
okay the last point I want to address is
the notion of the Express knowledge as I
mentioned so we want to see the effect
of prior knowledge of the source
task and effect of prior knowledge of
the target as so we compared to
assumptions one is the dilemma has some
class of predictor h source with respect
to which you can predict the source well
and the other assumption is if it has
some other class HT with respect to
which you can predict the target
so we distinguish between these two
situations in which we have good
drainage of the source or we have
blueprint about the target and you want
to compare in which of these two
situations you need the unlabeled a
sample from the top so in the first case
if you have good strategy for the sauce
we can show that there's no benefit to
the use of under the target distribution
we can find the basis of a process
respect to the source apply to the
target and this is as good as we can do
however in the second scenario and all
our information is about we know very
well a class that will predict well
under target but we don't
information about ourselves in this in
our we can prove that unlabeled
information from the target is really
necessary in order to succeed so we have
here a very strong lower bound the
Saudis again I was second he would
restrict our attention to the finite set
X it's the same situation as before and
we assumed it we assumed a favorable is
the lower bound to assume a very
favorable position here that this my
trade show say is bounded by two so the
weight ratio is very strong and still
what we are going to show is that let me
jump today negative is that so we can
show a very strong result here in terms
of the need for unlabeled data so for
every fine domain X there exists some
class that has visit I mention one so
it's a very simple class you know that
the target can be rated but either the
constant 0 function or the constant 1
function and in spite of having this
very strong knowledge we can show that
so now I have to pass this so small s is
the size of your unable source and
something and small T is the size of
your under the target something and we
can show it know about that you cannot
learn as long as the sum of these two
samples is less than square root of the
domain size so if all that I know is
that information about the target and no
information about the source then the
sum of the size of the source sample in
the target sample should be at least as
big as square root of the size
the main so I practically needs to see
all the points of my domain in order to
learn but what is the catchy why is it
interesting because these are bound to
the some of the souls on the target and
we can show that if you have lots of
target and label data you can handle
this with very small so say something so
in such situations they there is
inevitable need for large target data
because the sum should be big but we
some could really consist of small
training sample from the source and all
of the hugeness of the subtle could be
supplied by unlabeled target say samples
which are so I don't know about but this
is combinatoric so not so much
applicable a so then all those is based
on some nice could lead to a problem if
I give you a three samples one of the
most was generated of a distribution
the second was generated a second
distribution and I ask you is the third
sample generate very first distribution
or by the second reservation and we get
lower bounds for solving this
probabilistic problem and we apply this
law guns for this publicity problem in
order to show the our lower bound on
learning in this situation well we know
that the target is either concept one of
concern zero but still we cannot learn
it without having lots of
a distribution so that's the reduction
and then we also have a complementary
positive result and the complementary
positive result is that we can learn in
such situation with very small source
distribution just depending on the VC
dimension of the target class as long as
we have very large unlabeled target
distribution so we have you two
complimentary results saying that in a
scenario in which I have information
about the target and not about the
source
I cannot learn without the sum of the
two samples being very big but I can
them if all these largeness is in the
unlabeled target and the source level
the sample space
and then I'd never fell below mercies
for the holiday my mommy I mean it's a
sample but what I'm saying is that in
this case a lot of unlabeled target
distribution compensates for the needs
of okay let me jump let me skip all the
proofs and jump directly to the
conclusions so okay so we investigated
which assumptions about the replacement
of the perfect data by proxy data or
proxy data is data from there so
distribution and income distribution and
we saw that for some algorithms the
label shows that the surprises either if
we have a very good information about
stores realizability all in the nearest
neighbor situation and the rigidness the
next living assumptions but there are
other scenarios in which the unlabeled
targets data is probably necessary and
proven beneficial and we discussed two
suction olives one was the password for
learning when we need to output a
classic driver certain type like
half-spaces and the other was the
scenario in which all our knowledge was
about the targets tasks and we didn't
have prior knowledge about the sauce
tasks and that was the lower bound that
we we showed in the
it's sports sorcery our ideological and
right so in this case we have the bound
that depends on the sauce realizability
and we had about the dependent on
elizabeti so you can always use the
minimum between these two box but we
don't have a pound that gives you
anything better
no no it's a very big gap if I focus on
the size of them the point is that the
point is that what I could I'm concerned
about is the size of the label which
label the label have any neighbors are
very expensive so here what I need for
the labels in the positive result is
just a decent mentioning of the trust
and there's no dependence of the size
officer on the other hand the lower
bound tells you the sum has to be but
the families to be huge but we show that
although the sum has to be huge all of
the bigness of this could be come from
the unlabeled is sample and the so and
the label sample could be useful then
that's basically so
it's not some general comments the first
one is the one that I criticized several
times
it's those criticize it's from that is
the human community pepper so the
assumption that cap is extremely small
just be an example if you tell a paper
on your participating again you take two
gaussians simple case it's infinite so
and this happens quite a lot so so
basically just going at it I actually
think that it's better not to divide it
because precisely this issue because in
fact all of your bounds everything that
you're showing if they're all depends on
this ratio of one overseas
I would say first of all and then one of
my see would want to estimated optimal
time because otherwise how could you use
it and not even getting into the
estimation issue if the assumption that
whatever she will bother it is extremely
technical I didn't want to get too
technical but for the for the first
equation then we don't need the
point-wise ratio we just need the ratio
of pubes and the ratio of the cubes is
that you can
you mentioned another so some of the
AdWords and you don't need the postman's
ratio which is really very extreme but
you can settle for it's like the
discrepancy that we need respect to some
class you can do them the multiplicative
ratio we expect to some class and for
the nearest neighbor which is enough to
do it with respect to cubes which it has
this dimension just to do something so
so it's it's much better so maybe that
case sometimes I would say more
generally and then this so-called what
you call additive
actually it's not just additive it's
also based able to counter loss function
which importance waiting quantities do
not so you know that so one of the
crucial benefits of discrepancies
precisely or the distance that you were
using previously is precisely that it's
taking to counter hypothesis F and the
Basque region relying only on the ratio
of the decisions as to issues actually I
have to cut it out because they're you
know like what I actually wasn't saying
the I mean I want to continue this
discussion in kind of less formally so
we can let people want to look at the
posters get started doing that
but I mean yeah I think we could I mean
by all means please we should continue
this discussion as well so those those
before it they're like like your booty
can answer other questions also so we're
gonna sell</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>