<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Music and Machine Learning Workshop: Learning from Musical Structure | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Music and Machine Learning Workshop: Learning from Musical Structure - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Music and Machine Learning Workshop: Learning from Musical Structure</b></h2><h5 class="post__date">2012-02-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aN6sNww2DRc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and yes i'm going to talk about a
polyfill sequences so first i'm going to
talk about problem in the data that we
have and then i'm going to talk about
two supermodels the variable X market
volume innovation formulation of that
which teacheth by rolling market morning
and I need these because the second one
is going to be leaner component or the
top volumes are going to introduce next
and then obviously i'm going to talk
about valuation results and for some
directions for interest which um so the
problem or interesting is learning a
generic morning for music for melody
specifically and that means that if
we're given a set of music pieces can we
actually generate music that resembles a
style downer and what to do this in an
automated way so we want our Lord no
won't introduce much prior musical
knowledge in the models what the morning
to be generic national income we apply
to different genres without the need to
be engineer features or restructure the
model according to the musical structure
looks specific genre so structure is one
of the fundamentals of music so it
should be able to capture and
restructure with machine learning
methods but there are several
challenging structural aspects that you
see so we have repetition here
new boxes which can occur in almost
arbitrary points in time and with
different reason valuation we have
confidential influences where we have
several components such as the window
for the meter the key and so on that
sort of interdependent on each other and
together we find out to the composition
and we also have different internet and
entropy similarities and what will mean
about it is well if we look at music
pieces from the same genre they're all
been with the same under the same
structural form so we expect you to have
some statistical similarities however if
we look at the single piece then we're
looking at typically a non stationary
process where we have several different
music regimes so we can have a fast
paced phrase followed by slow paced one
or we can have a sort of this holy place
because of my control and one and so on
and want to be able to hold a lot of it
so on the data that we use is our please
this is a genre that has errands and I
are you Irish traditional folk music
which are used for dancing purposes and
the pieces are the keys of g and of seed
and then all of them have four quarters
winter and what movie is basically the
melody line and so a sequence of notes
to time so we need to be able to
represent the beach at right so on the
music i'm looking at is the mid-face so
looking at discrete data so we want to
basically a code the beach in duration
of the notes from time and this i will
discretize time Nate
and we use multinomial variable to
represent each so we use two octave c-45
be giving right straight four distinct
values and we use two special values the
first one corresponds to musical sides
and the second one corresponds to
continuation this way of mourning
duration so what happens is here we have
a g8 float and this is represented by a
corresponding each and field have a deep
quarter note and then this is longer so
it's represented by the corresponding
followed by continuation all right so
going on to the movies over the
viability has been suggested to give
state-of-the-art musical results and
this specific settings so automated
melody generation and what the model
does is it learns a conditional probate
distribution of the next symbol given
the past only the context are born
wintry condition is not the length that
is not fixed as it could be in a
standard market on but actually depends
on what the context is so what happens
is we be there probably is exactly where
the nodes of the tree are labeled by the
context so I the deeper a note the
longer the context is labeled B and
nodes are identified by the conditional
probability distribution of the next
symbol given that context um so as we
can see here the tree is not complete
and this is what gives rise to a shorter
and longer context because we only in
the tree the nodes that received
frequently enough in our data so the
nodes the context for which we have
enough information therefore in this
example if you observe 00 then would
perform would make a prediction
according this conditional will be
distribution whereas if you observe to
you know one because you don't have
enough information about the longer
context you just use a short one and
make a prediction coordinates I'm sorry
clearly you're using binary strings just
as an example here other multinomial yes
now I'm the DVI borrowing money but here
we have a bayesian approach evasion
formulation of the previous movie where
basically we use an appropriate prior
for it snowed in order to perform
smoothing so smoothing is very important
in kind of market models and instead of
using another folk approach here we can
say that the prior for each node is
additionally distribution that is
centered at the moment only of
characterizing are so what we have here
is the posterior for this identifying
this node um is a deep sled distribution
that has the contribution from coming
from the prior which is this one and
then also that the counts associated
without context
right so I guess one might ask why don't
we stop here and why do we need more
presentational power well the thing is
that both these models are can model
stationary data so what we expect to
learn is some sort of musicality of the
genre but they're not able to actually
capture this to model effectively these
different music regimes that we might
have within a single piece of music so
we might want to actually have a mixture
of these within a single piece of music
to characterize these different regimes
and this is easy to dinner in top model
for mediation so I'm here have three
graphical models and the first two are
equivalent views of lda late inducing
application we're here to just drop the
plate notation for the words so that we
can see what happens comparing to the
article ground 0 be home so what happens
in a VA is we have a set of documents
and each document is a collection of
words right in the music case we have
various pieces and each piece is a
collection of notes and then we have a
set of latent topics and these latent
topics define distributions over words
right so um if we look at the generate
generative process how to be generating
a document or example a distribution
over latent topics for that document
which is parameterized by theta and then
for each word in the document we first
sample a topic from that distribution
and then we suffer a word from the
distribution of words that characterize
so um in the variable ground topic Molly
we actually also include temporal
connections between the words hear the
words are conditional independence given
topic but we actually know that the
temporal information the local temporal
information is very important so what
happens is that five years so the
distribution of awards for its topic is
just a single vector not a single
multinomial pepper in our favor than
five is going to be the set of
conditional probing distributions
defined by the visionary mmm so each
topic will be associated with a single
dish plenty of them and we learn so the
structure of the VMM of the tree is
going to be the same for all topics
kaneez topic we learn different our
conditional and i should say here that
if you only have vibrams and so you
don't have these longer connections then
we can retrieve the vibrant top model by
one of thousands this is this intended
also handle repetition I haven't seen
graphically in order to get a repeated
your teeth twice well it kind of
model-based right because if you this
can be so if you have repetition
occurring these longer context will be
included in the tree so then the next
time you see it you can
okay predict but yes actually generating
so all right so um now we come to the
model evaluation well this is a very
difficult aspect because it's very to
say what is a good genetic problem for
music and in the absence of a human
values to Paradise which would be very
costly when handbook money to
quantitative measures or evaluating the
performance of models so the first point
is that prediction look like hood so we
want to see are given test pieces that
the morning has not seen how iconic
predict the next note and the second one
is a whole talk later divergence between
a pinnacle statistics of test sequences
and all these samples so what happened
here is we generate samples from our
model and we also have test sequences
which the model has not seen and then
and people statistics for these two
populations so for first order
statistics would look at the probability
of just a single load but for pairwise
statistics would have the joint
probability of two consecutive notes
third order three consecutive notes and
so on and then we compute the KL
divergence between these two a pupil of
these books and there's also going to
show our from 10-fold cross-validation
were basically each all three training
related pieces and then evaluate all the
remaining 10 right so um regard to the
next step prediction not like I'm here
have a sort of stationary models the
first row corresponds to the empirical
marginal distribution so the probability
of the north is proportional to the
number of times you've seen this month
in the data and it's their maximum
likelihood model given not improb if we
look if we don't consider and temple of
dependencies and here i have three three
versions of the danish layer our
variable and markov model which are
gonna then using the topic knowledge so
the first one is just a biker movie and
these two models have different epsilon
parameters well epsilon L defines the
pruning of the tree so basically it
tells us how often we need to see a
context in the data the Train Linda
before we included in the train and the
smaller reception then the less times we
need to see something the data before
being filled in the tree so these will
have probably resulting in deeper trees
and here we can see that the more
temporal information we take into
account the better our prediction is so
the lower the hydrilla look like with a
better education um right and here I
have two different violations for the
topic models so as I said when we start
with a new piece we have no information
about distribution over topics right so
what happens here is we can update as we
see a test piece we can update this
distribution is it a distribution over
topics and thus perform better
prediction and in this case we can
update and apart from the distribution
of my topics we can also update the
conditional probabilities
define over words defined by the topics
so that we fight and here I have Layton
deschler allocation and we can we can
see that topics do actually help in
prediction so comparing to the empirical
marginal has not ever occupy this again
the LDA performs better and then again
the more temporal information at the
model has the metrical potentially
performs and also the more Olympics we
consider the better the prediction I
would have and in these things we can
see the aspect of of novelty very well
so each species a is a new idea its
music piece so here we can actually also
update the distribution of rewards we
get a better prediction because this
longer context might mean something a
little bit different in your pieces and
in this case we take this information
bank account right so we got to the
combat leibler divergence this is for
first order statistics and they have
different groups that saw results for
the vibrant variable ground in the bar
every two versions of the variable rum
models and this is for the stationary
once again so we can see that when we
include topics and again then the KL
divergence is lower so are so our
samples from our mobile launch our data
better and these stars over here are the
way we generate the samples so
something's in this case with an onstar
case as are generated directly from the
fryer whereas in this case we find for
the test pieces that
yes we find the topical locations the
hidden topical case which produce pieces
under our model and then we generate
samples given the topics right so it's
and we can see that if we give the
topics like respectively these have much
lower Gail merchants and i think i will
skip the higher the statistics that
behavior is the same i should not hear
that in the background top model tends
to have lower que el reg's star the
variable brown ones and this is what
this is consistent for first and second
order statistics but for third orders
the tasting sweet kind of the variable
grub perform better but one thing we
should remember is that the KL merchants
are kind of penalizing overfitting and
doesn't penalize underfitting right
because it's a non-symmetric measure so
maybe these kind of suggested we should
use this metric measure we should also
take in with the opposite KL divergence
get a better idea what's going on right
so um I guess I introduce the topic
model for melody which is based on the
dual a viable in tomorrow morning and
what happens is that the deerslayer
variable if Mark Molly learns the local
temporal dependencies in a piece and the
latent topics can learn the different
and malling the different music regimes
and this is like working progress so
there are many things we want you to
look at one of the things is the
structure of variable length markrony as
I said at some point i'll be at the
moment we learned the tree or the
training data and then the structure of
the tree is fixed so what we learn in
the
model training is just a conditional
probability distributions that identify
the knowledge of the tree but we don't
actually learn the structure may be here
we can consider a sort of non parametric
approach we can have an infinite tree
and then learn this during the topic
Molly learning the second thing is that
at the moment we use a global beta
parameter for the distributions over
words but maybe we can consider
different better parameters for deeper
different depths of the tree things like
this so as I said our other valuation
metrics or other ways to actually
evaluate these models out here we're
currently looking at using string
kernels to evaluate the samples and
finally maybe it's worth exploring if we
really introduce temporal dynamics and
topics because within a music sequence
we would probably expect a topic to be
on for several subsequent for whole
subsequence let's say and then switch to
a new topic which is currently not not
only it kind of happens when we see the
topic allegations but it's not that easy
so um yeah would like to thank
organizers for inviting
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>