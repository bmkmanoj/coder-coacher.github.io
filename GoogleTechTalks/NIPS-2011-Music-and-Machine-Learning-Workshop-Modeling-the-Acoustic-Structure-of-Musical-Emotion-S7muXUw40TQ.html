<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Music and Machine Learning Workshop: Modeling the Acoustic Structure of Musical Emotion... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Music and Machine Learning Workshop: Modeling the Acoustic Structure of Musical Emotion... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Music and Machine Learning Workshop: Modeling the Acoustic Structure of Musical Emotion...</b></h2><h5 class="post__date">2012-02-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/S7muXUw40TQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">in sorting recommendation play this
generation I'm in a few challenges about
this problem to highlight it's one that
emotional descriptions often lack
singular well-defined answer which makes
it difficult to collect round truth
labels and two there's no dominant
acoustic feature really for musical
motion that is yet emerged which is
really what we're talking mostly about
today and then third emotions change
over time so the picture the musical
motion is really a problem that stands
to stochastic domains one being the
space of human emotions which you could
represent with text labels such as
oncology Zurich firms or parametric
levels of emotion such as balance
arousal and the other domain being mean
mystic signal music so as I said we're
working with valence arousal where
valence indicates positive versus
negative emotion and arousal indicates
emotional intense we actually oriented
arousal valence but since we're the same
thing and this comes from psychology
research so if I just plot a few terms
in the four corners but-- dog sir
copyright high bailiff I'm arousal we
have joyful down angry over too
depressing and up to contend of course
we can look at a whole range we have a
whole cool grading of emotions we can
represent in this space now highly what
I was talking about before in terms of
collecting ground troops data and we get
to test the audio if I play music for
people come just like it does on my
laptop we get we get a whole range of
answers and none of these are any more
valid than any others so we need to take
all them into consideration on what
we've done in a lot of prior work is to
model this as a simple two dimensional
Gaussian and tried to parameterize
automatically so as far as collecting
our ground truth data we um we developed
a game called mood swings where players
were pair aired online competing to
label these songs for us and we
collected more than 150,000 AV labeled
sky over a thousand songs and to collect
a corpus from this larger database we
some selected 240 15 second clips and we
try to pick ones that uniformly expand
the ad space on and then subject them to
even more focus one of the criticisms we
had about this approach is that because
it's collaborative it's against
competitive at perhaps that's biasing
what we're collecting so the data set
that I'm actually using here was
collecting with Mechanical Turk same
songs but we found that the data was
highly correlated with the mood swing
data set the one thing that we really
like about this set is that we have a
lot more ratings we have about 17 people
on average have rated each flipped and
we have second-by-second freedoms in
this data set is publicly available for
anyone to use so as far as selecting
features really the issue is if there's
no wonder lying theory from music there
to inform the design of emotional
specific features you know unlike other
domains like speech processing where you
know we can at least all agree that NFC
CS are motivated by a speech model I'm
sure we could disagree on how good they
are but you know most work on this has
concluded that essentially it's the
combination of many domains such as
rhythm tempo key mode Tambor etc but
most approaches that have tried to
combine these domains
haven't been able to get much better
performance I think me Rex america
competition is really the best example
of this the first system in 2007 and it
was the best with five George sockets
and can just use msec and spectral shape
so about sixteen dimensions total I'm
showing all the systems here on the
right and I just basically made a
histogram of the data and plotted as a
teen pop so you can sort of see where
the peak of the distribution is 2008 we
went up to 56 dimensions and now we're
introducing things like feature
selection dimensionality reduction where
George was just using an SVM and we're
only going from 61 to 63 2000 this guy
got up to 65 I I was under the
impression he was using over a hundred
dimensions worth of features I couldn't
find a reference on that so I didn't put
it on the slide and then just in 2010
I'm here something at 70 and they were
doing some pretty sophisticated features
leggings nationality reduction and again
they are really hardly any better than
George was with a pretty simple approach
these are the same data set oh yeah
that's great so basically the takeaway
is there is some information there but
not all of it and of course it's all in
the signal because we're able to discern
these things so instead what we want to
do is to seek to learn these feature
representations directly from audio with
nitrous inspector using the bleakness so
here we get features that are
specifically optimized for the
prediction of these cold ocean and these
models that we learnt in potentially
shedding light on the relationship
between the acoustic domain and emotion
jerence so as I said we're using people
eat networks on the building block of
deviance is a restricted Boltzmann
machines which are markov random fields
with hidden units i'm not going to go to
into detail about these but for people
that have never seen them before on
there essentially to their model where
we're trying to learn a projection to
reduce the dimension al
you have to reconstruct the data and
that that's that's really what you need
to know understand what talking about
here so to build VBS we train these r BM
is really one at a time training the
next one off the outputs at the previous
and in our system we are using YouTube's
Turkish at the models train predict
aving means and the input looks like
this is Ron I to inspector at about 20
millisecond windows and this is just
like all the other standard acoustic
features so we can provide something to
compactly compare for them the model
itself has 50 notes at each layer and
then as the output we're only trying to
predict the mean of the collected data
in the AV space so um taking a model
that's trained on that we can of course
look at on what we get out of it sure
everybody's heard this song before it's
a cover of it
my pimples a little bit long but I want
you to kind of get the whole thing in
your head cuz I'm gonna try to
reconstruct this so the features that we
get out of this really the first layer
I'm showing on the top here uh the
largest dimensionality reduction have
them where we go from 250 7050 we can
definitely still see some of the
sections of the song although we're fine
you notice that as we get to the higher
layer on the variance gift lower power
and what's really interesting is to try
to reconstruct this vector gram now
after we do fine tuning the BBN beeps
thrown out anyways for reconstruction
but we can certainly play around with
some linear algebra and do a pseudo
inverse to see what we come up with and
it looks pretty bizarre to be completely
honest about it it seems to target for
very specific frequencies although you
still can see if you look hard at it
some of the sections this right here is
that kind of home pre-chorus now if we
want to construct reconstruct this we're
already starting from magnitude spectra
so we've already thrown out the fades so
if I try to reconstruct just the audio
original audio itself for a magnitude
spectrum it sounds like this
very well so just a short example that
but this is the next I'm gonna play is
actually reconstructed from the audio
it's set up front from this itself and
you can hear that the tempo is still
there
but to the symphony thrown out kind of
information it's really still there at
least to some degree so um then what we
want to do is to take these features and
apply them to the problem of
content-based emotion recognition to see
how informative they are so I looked at
a bunch of standard acoustic features
not to provide means of comparison all
these results are cross-validated five
times and what we find over all is that
spectral contrast tends to be the best
performing feature honest with multiple
linear regression and it didn't
normalized space so you can interpret
these values in the mean distance as
percentage or so we're about 13.8
percent in error on average we also look
at the Gaussian problem for good thing
about dopamine and the covariance and
really these kale values is they need
contact so you can really only compare
them to each other but we see overall
that to be the best performing but I
tend to emphasize that the mean distance
is a much more important metric because
the mean has to be accurate if the
covariances rotated incorrectly you know
we can deal with that if the mean is
wrong that's really bad so now we'll
look at using the GV n just using the
model itself just taking the output of
the aggression layer that I attached to
that we're up at about twenty percent
error but the best layer is there too
and we get down to 13.3 percent so we
have a small amount of performance on
the Google with better in terms of an
increase in KL we go from one point two
nine down to one point
19 but what we really want is a more
sophisticated model than multiple linear
regression try to resolve the true the
years I did but it didn't do any better
so I didn't have it on the slide yeah so
um so the more sophisticated model we
look at is conditional random fields so
CRFs you're trying to predict their the
probability of a labels Y given on a set
of features X there's no generative
model of the features it's completely
deterministic and with it what we try to
do is use this heat map representation
what you can essentially pink up is
we're bidding on the AV space and then
each pin within the space represents a
note in the mob and I'm you can sort of
think out of the conditional
probabilities in terms of hmm phase
transition probabilities and are like
our actual heat map is 11 by 11 looks
something like that so in order to use
CRFs with audio data we have to do a lot
of quantization so first we quantize our
labels when we train these we were
presenting the individual label
sequences each one is quantized within
that 11 by 11 space and one of the
issues that happens immediately is that
the neighborhood relationship the grid
cells is lost when I say instead of
saying that this is you know at this
coordinate it's in class 5 so it's not
really very more so what we do to try to
get around that as we duplicate each
example another nine times and add a
small amount of noise in order to give
the model essentially the ability to
where that relationship but we also have
to we don't have to but crfs are highly
optimized for binary features so what we
do is we quantize each feature to
mention into ten bins we tried a lot of
other a lot of other more sophisticated
schemes for this and really just ten
level patas ation is what tends to work
the best it just to be more clear about
that we take each feature to mention one
at a time weak like all of them
Instagram break that up into 10 equal
energy events and then for an individual
feature value the quantized value is
just whatever minute lies with it so
first I'm just going to show us an
example a direction you can go here
these are all 15 second clips so we can
predict at each step and what we're
looking at on the left is some collected
data and on the right is the conditional
probabilities and what you should look
for that happens here is this song
starts off as very quiet and mellow and
then moves into this really hard rock
first so we're going from bottom left to
top red
so the mother is a pretty good job at
tracking and killing over here at one
other video shows because pretty cool
this one this is an angry example a very
angry example so we're soaring off hard
bottom right and then sort of becoming I
would say only less angry
did you
understand
okay we think will be happier than the
subjects do but in terms of intensity
were right and definitely not quite as
angry as we work yeah just to provide a
form of comparison to our previous
approaches we want to be able to look at
the beam vision so all we do is we take
the AV values of each bin center and
then multiply those by their probability
and then we can get essentially a
weighted mean within the space and
produce continuous values so here we see
everything does better overall the layer
two features are down from 13.3 to 12.5
which is significant but NFC sees jump
way ahead and they were up at 14 and
they're down to 12 so this this really
motivates us she was looking at some
other approaches and really this is this
is what we're working on right now you
know current features are really all
extracted from these 20 millisecond
frames but no human could identify a
motion at that time scale by plated 20
millisecond clip know who was on click
so the motion is not expected to bury
this higher rate and instead we look
into multiple time scales because we
also believe that motion is influenced
by what you've already listened to so we
look into basically the same model the
same approach but instead using
aggregation of the past one second that
has two seconds and the past four
seconds and then concatenate those
together as inputs I'm going to go
through some of this a little more
quickly but here's the features we wind
up with here I think that the sections
of the song seem to be a little bit more
defined here of course they look longer
but that's because where our hop is much
higher instead of hopping a 20
millisecond now we're hopping at one
second because that's our smallest
window one of the things that isn't
quite as cool about this is that it's
much harder to analyze what's going on I
can't just say oh this is the magic of
emotion through
not that we ever expected to find that
but so here now we're looking at three
different essentially domains i'll call
them stacked on top of each other
essentially we get a similar result
though now comparing this to what we saw
before and this is back with a simple
and the larvae approach where these
these are our original results from the
single frame that's one point in SF and
if we look at this get slightly better
with the model error down to 19.4 and
the best now is down into a range of
twelve twelve percent which is where the
CRF was but really one of the great
things about our pm's being on
supervised is we don't have to just you
know use our small training data set in
that in that observer eyes days we have
access to a much larger library of songs
about 7,000 when aggregated about 1.5
million examples so really why not why
not start with that I'm have developed a
really general model of music and then
try to tune up the music model to
emotion and what we've seen here is we
get performance out of this as well so
then going back to the features we
looked at before and we get the biggest
increase really is in the model
prediction we get from 19.4 down to
fourteen percent which is pretty
significant and then when we look at the
individual features the changes and is
it quite as large I'm really just go
from 12 point nine out of twelve point
eight
but it's really interesting how much
better the actual model gosh and it's
still unclear you haven't really tried
these in the context of the CRF training
on the CRF still running valuations are
talking about 20 hours to train each one
of those target about 12 hours to train
each one of these deviants but really
including deviens really showed great
promise in really developing models of
the relationship between acoustic
content and emotion one because it's
something that we don't really
understand and two because perhaps we
could get informative feature out of it
or perhaps you know from what I just
showed maybe it's the model itself the d
VN itself that we want to use to do the
prediction so the universal background
model approach slightly including spot
the model relatives in prison models the
features slightly and moving forward
perhaps we want to investigate actually
using the CRF within the context of the
dbms which is a deep structure
conditional random field and I think we
also want to look at it investigating
more granularity on in the input to the
DB n on so you have more levels of past
knowledgeable what's been going on</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>