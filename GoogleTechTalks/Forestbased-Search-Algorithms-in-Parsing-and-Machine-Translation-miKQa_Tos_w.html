<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Forest-based Search Algorithms in Parsing and Machine Translation | Coder Coacher - Coaching Coders</title><meta content="Forest-based Search Algorithms in Parsing and Machine Translation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Forest-based Search Algorithms in Parsing and Machine Translation</b></h2><h5 class="post__date">2008-03-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/miKQa_Tos_w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today I'm very happy too damn long
from University of Pennsylvania now he's
a very young senior graduate student he
has been doing very good work in parsing
and recently used person or translation
thanks doc appt the thanks everybody for
coming here and the talk today will be
on forest based search album for NOP and
parsing and translation in particular so
before I talk about any technical
details I wanna first convince you guys
that search even for us human beings is
not a trivial problem because if we are
given a sentence we kind of search over
the possible interpretations to to get
the most probable answer but that's not
easy task for example this is a real
story my advisor Aaron do she was once
asked to to teach high school students
what's NOP and what's like parsing
that's apparently very challenging task
for those little kids in their early
teens like what's context-free grammar
was fun in parsing but having found a
better way to do that basically just
speaker sentence and let them draw
whatever they have in their mind so for
I saw her duck there's one kid who got
this and I saw her duck so this is
apparently a word sense disambiguation
problem for verb soul and for the next
sentence I eat sushi with tuna that girl
got this basically using tuna with as
chopsticks okies and if she was kind of
I used sushi with child tunas chopsticks
so this is a PP attachment problem
because this PP phrase should be
attached to the now sushi instead of the
verb geet so that's two typical problems
one is the worst sensed and the other is
the structural ambiguity and if you're
more imaginative you can even get this
basically you can imagine you're doing
scuba diving in cary beans and there's a
school of tuna around you and there's
sushi flowing in the water or dropping
off from there the sea and you're
competing with them for raising sushi so
here you are doing the peepee
attachments due to the verb each but
you're interpreting the width as
together with instead of using something
as a to something like that so you know
there's a bunch of ambiguities so so
what can we can say about this little
kid well Aaron said your search album is
biased there's no way you can get this
or that as your optimal solution for
this sentence so okay let's go back to
the sentence I saw her duck there's
apparently actually a certain
interpretation where the duck is kind of
a verb as bent and what about I saw her
duck with a telescope then you can have
six because you can attach this to the
variable tube now any what about I saw a
duck with a telescope in the garden then
you have even more and as your sentence
gets longer number you have many many
more then that grows exponentially with
the sentence lamp so here's the first
problem that why NOP is hard because
we're searching we're in exponentially
many I'll can it extremely many
candidates in the search space for
Lempel like this so the solution here is
very simple we just do a locally factor
space and do dynamic programming on a
small space and lads is called packed
forests which I'll cover in a couple of
slides and albums like CKY and Viterbi
are all instances in this framework but
the difficult part is to incorporate
non-local information in this framework
for example the soul which telescope is
kind of likely so you know you should
attach this which tells go to the soil
set up to her duck but this kind of long
distance in non-local dependencies and
so is sushi with to now you know that
sushi with tuna is kind of nice and
likely probable collocation but
co-occurrence but eat with you tuna as
chopsticks is really not likely but this
is really not non-local in the long
distance so the question the key
question that we're going to address
today and it's really hard pun in LP and
all the other fields in CS is how do we
incorporate non-local information in the
search and how do we do it efficiently
so there is apparently an easy solution
that is pipeline reranking or a
restoring so we first post pone
disambiguation by by propagating okay
instead of one best because we know that
one bath is not guaranteed to be the
best in later stages and later stages
have more access to more global
information like non distance long
distant dependencies the for example we
do attacking and we postpone the papers
parts tags sequences and departing and
there is k best parties fit into the
semantic interpreter or semantic role
labour and that's apparently what we do
in our field but the question here is we
need very efficient algorithms for a
very large number of K 44 like cake over
a thousand dot or even 10,000 for it for
for Mt especially that's the first
solution but that's not the principal
solution because it's kind of too staged
and we really want to do a joint version
which you already integrate non local
information directly into the search
instead of kind of doing a reranking so
this is more likely to get the solution
I took eight the best solution but
apparently this is np-hard so it's
intractable in or we can only do
approximation but how do we do it in a
principled way that's a largely open
question and we're going to address this
question in this talk also so here's the
outline I first briefly review the pact
forest and in private wave have web
framework and then we're going to focus
on solution while where I develop exact
and fast k bad search algorithms in in
the forest and then we use these
algorithms and extend it to solution to
where you do approximate join search
with non-local features to rally into
the search space and I'm going to first
talk about an application in parsing
descriptive housing called force ranking
and then apply both these two techniques
to machine translation and focus on the
problem of decoding with language models
okay so any questions to stop me if you
have any confusion and okay packed
forest so Pat forest is just a compact
representation of exponentially many
parses in a polynomial space so for
example you have sinus I saw him with
the mirror apparently they're there are
two readings either you can attach it to
down and forming a bigger now and then
it combined with a verb that's the dash
lines or you can just directly attached
it to the verb and combine it with the
ternary combination that's the solid
line one so these
little things like vp16 are called nodes
or items in the detective packaging
framework and these guys which combine
several nodes into one node hyper edges
so this is really a structure of a
hypergraph and each hyper edge like this
e 1 is just a instantiate deduction say
for example you one is saying if you
have already derived vbd and MP and npp
you can also derive VP for the combined
span so that's just very simple eyes to
seek a while or other famous algorithms
let's live in you and if you are coming
from the speech or lattice world you're
familiar with the lattice which also in
encode exponentially many paths are but
we just extend it to exponentially many
trees so basically this is a graph world
and this is the hyper graph world where
we extend from regular grammar to say
fgs so you have state transitions from
one state to another and here we combine
several kind of states or nodes into one
node and can can deal with the hierarchy
holy branching stuff like tree
structures so that's just very simple
extension you should be familiar with
that ok so but because we're doing
optimization we're doing like one best
or K bears we have to have some way to
measure the comparison right so how do
we do it so we have an avoid function
for each hyper edge like in CKY the
weight function is just fa be when you
combine from smaller spans to a bigger
span if the result is that the product
times the probability of this rule so
that the probability of rules that will
accumulate towards the root so but a
crucial restriction is that we require
each wave function to be monotonic in
each of its arguments for example here
at this f e is monotonic in both a and B
so why it is important because it is
exactly encoding the optimal sub-problem
property that is required in doing
dynamic programming so for example it
says if a bigger problem a sorry
I just use this a big problem a can that
if you claim that the optimal solution
is to decompose it to smaller problems B
and C then the small problems the
substitutions for B must also be the
optimal because if you have a little
solution B prime which is you claim to
be better than the B then by applying
this monotonic wave function you can
always get a better solution for a the
bigger problem as well so it says
optimal solution must be optimal
everywhere and when we have this
property we can do inference you can do
dynamic programming efficiently so the
basic operation in the hyper hyper graph
is basically an update along a single
hyperedge so if you have a hyper edge
connecting you to V then you can use the
result of use the weight of whatever
score in this node apply this apply this
weight function and to compare with the
current estimate of this V and if it's
better so this plus is a comparison
function returning the better of the two
if it's very just complete the value so
we're going to see the how do we do the
one best very simple one best inference
in hyper graph so this is called
generalized Viterbi because Viterbi is
defined down lattices and we just
perfectly generates it to hypergraphs so
but it's very simple it's just nothing
different so we first do a topological
sort which assumes asic lyssa t so we
turb you can deal with directed acyclic
graphs or digs and we do direct in
cyclic hypergraphs and when you have
this order you just visit each node in
topology older basically in CKY its part
about order like you combine from
smaller spans two bigger spins and you
simply do an update on each hyper edge
that income that comes into this note so
for example you do this you apply this
wave function f e to the result of these
two antecedent notes and update on the
DV and you do the update on each of the
hyper edges and why is it correct
because the is very simple because by
logic ordering when you do this update
all these anticipatory notes
mus already being fixed for its optimal
answer so so they're safe and you don't
need to do any cyclic updates so that's
why it's kind of efficient you don't do
any waste of time and the time
complexity is simply linear in the
number of hyper edges that is the e
because you did what's my only ones and
that's exactly why I CKY parsing the
famous alum is in cubic n is the length
of the sentence number of words in the
sentence because they were exactly that
many hyper edges in in the hypergraph
any questions so far ok so we've got to
now extend it to the K best case for
doing reranking or restoring for very
large K because we we need really fast
algorithm and we just left at this
little kid who got this IE sushi with
tuna as chopsticks but actually this is
also the one best parts from our best
parser in our research community so
that's exactly I checked with the China
yoga posture and exactly it returns this
basically attach the pp to the verb so
the kind of China also uses as a
chopsticks now we're going to fix that
in we've got to incorporate non-local
features in KBS plaster so the correct
parts that is actually the second best
parts from China posture ok so the most
naive way to do k best is to say well we
don't want top K for each node so we
replace the single value by a vector of
K so that AK for them a1 is the best
answer and we if we keep them solid a
one is presents for you 1 and 8 k is
about this case best answer for you one
and four for the consequent video we
also have the top K so now the question
is only how do we apply this wave
function to two vectors instead of two
numbers well there apparently k square
possibilities if you do evilly right
because every AI and BJ is a possibility
as a combination but if you do naively
you have to enumerate everything that
gives you k square for each hyperedge
that's really bad because k can be a
silent Enrique square can be terrible
and but that's really stupid and we can
do it a little bit better because the
key things that is we do not need to
enumerate the hosts whole grid the whole
square because remember that s and B
these are sorted so and also because the
weight function is monotonic remember
the mountain list is very important
property here so we can conclude that
this corner and 1b or must be the best
among the grids right and that's very
simple but what about the second best
well it's either this guy or this guy
right and what about a third best if you
just keep a frontier and you say okay
the second best must be among these two
guys and if this guy's read a bit better
then you got this so the third base must
be one of coming from one of these
guides so you keep a frontier which is
the candidate instead of candidates for
extracting the next best and in each
iteration you just simply extract the
next best and push the two successors
right and the two successors are
basically just if this is IJ and i plus
1 j + IJ plus 1 that the two ones on the
fielders so that's very simple and you
can do a binary heap or whatever
hypnotic heap for this priority queue
and that gives you que la que for
separating instead of K square so that's
huge speed up biggest k can be huge
let's do too bad because if you can do
this still too bad because this works on
each hyperedge sequentially the kind of
speeds up the amount of work on chat
bridge significantly but but at you
today we are not interesting the capers
up out of each hyperedge we're only
interested in the K best of these nodes
right as i said so how do we do it we
kind of want to process all these hyper
edges in parallel right so and we can do
it in your mind shot so basically we can
just extend it to to multiple hyper
edges basically that the frontiers would
now spread over all different hyper
edges and they would initially be just
the left corner of each head for each
cube or you can be a hypercube you have
more than binary combinations or it can
be high dimensional cube so initially
it's just what the left corner most
corner of each cube and then you kind of
push the successors extra fast and so
that you just basically to argument one
set simultaneously across different
hyper edges and that's but that's
basically doing locally Dykstra because
this is kind of best first best first
expansion
you can imagine water coming from the
leftmost corner and kind of flows
towards the towards this corner right in
the best first way but globally it's to
deter be in a sense it's bottom up in a
topological order so it's kind of weird
combination of Dykstra and Viterbi in
this sense and this gives you dramatic
speed up because you can do for each
node instead of for each hyperedge and
this is a multiple overhead for each
hyper edge and this is kind of addictive
instead of multiplicative and E is much
bigger the number of averages then the V
number of nodes so in real practice this
'part is going to dominate the
computation and that's so this is much
much faster than this as we're showing
the experiments but still still not
quite yet because in algum to we still
keep computes the top K for each node
well at the end of day we're only
interested in the top k of the whole
hyper graph or in a sense the top k out
the final or goal item about the final
node right so why do we waste time
computing the K bad stuff in any
intermediate nodes right so we really
want to do a kind of root laziest
approach which computes as many as
needed for each node so it's not a fixed
K for each node it's like you can keep
your three best for this node in temper
sometimes best for that node so it has
two phases the forward phase is the same
as one purse with Derby but the only
difference is we store the forest along
the way so normally if you do ckyo
Viterbi you just keep the best back
pointer so you can we track or whatever
backtrack the best derivation but we
also keep the alternative hyper edges
the ones that are non optimal so that we
can recover the second baseman surpassed
as we go on right because we re
interesting them and the backward phase
will start by asking what's your second
best to the top note to the root node
and then recursively on propagating that
question okay so here is the example so
this note is the root node of the whole
sentence and suppose it has three hyper
edges and after the first four forward
phase you already know which is one best
but that's the only thing you know so
you know that this pepperidge or this
combination is the best way to there
this node but now you ask the question
what's your second best right and by
I'll go to it must be either this or
this or one of these two because these
two guys are the successors of this node
right you kind of just do the same thing
but the question is you don't know these
two guys because these two guys are kind
of combining the second best of this guy
with the first met this guy or a second
best of this guy with the first pass of
this guy but they don't know the second
best themselves because everything just
knows every note just knows one best so
you kind of propagate this question
what's your second best to the parent
nodes and they would park it to their
own grandparents and so on and recursion
goes on to leave them you can propagate
back the numbers here and you can
proceed right so if you ask for more
like what's your surpass it will keep
asking right so basically it's kind of
varying the number of amount of
computation on each node automatically
so that if this is really ambiguous
nailed I I kind of do more work on it if
it's less little ambiguous or more
certain I do less work on it so kind of
automatically adapting and here's the
summary so if you observed a progression
from 12 to 23 we become lazier and
lazier and we do more and more
computation on demand so so we kind of
don't want waste time and we working on
a larger and larger locality verizon one
is a hyper a trap by hyperedge basis
album to takes all have bridges into one
out simultaneously in our MOOC is kind
of globally on a whole hyper graph and
the time complexity improves to from
from this really bad to kind of e + D D
is the lens of derivation and here is
the size of the tree and the size of
trees just linear in the number of words
so so E is I mean you CKY is cubic into
the number of words and so this is much
larger than this D part and it's going
to be dominating the computation and
this is is the amount of work you need
for one best anyway right so you can do
anything better than this e so the only
extra work is something like this which
which is very small and less the case
huge right so in practice you're going
to see that is too
dominating so we're gonna see some
experiments so this is on statistical
parsing so the best or the blood bastard
the state of the art calling speaker
pastor and we implement 013 I Williams
on it a lot too because I when I have
three I that too is it's not going to be
as good so this is the average time
passing time for each sentence and this
is K from one to ten sound so this is
loc blog so this is much worse than it
looks like right and we can see that
zero is much worse than one but three is
a goon three is so fast that is almost
just flat in the sense that if you could
want to compute 10 sound best parties
it's just not too much different from
computing one best why because the
complexity or it tells us that it's a
plus d x que la k and d is much more
than E and for lexicalized parsley is
even higher than cubic so unless you
have is huge and our of K is this part
you can do damage I mean it's just this
one best part is to contaminate the
whole computation any issue that this
question for people in speech I've been
using
something similar for extracting and bus
from hmm choices no no so what you do
there you do a forward pass Peter me
pass and then you do a backward pass
using an ace Larson and the look-ahead
function is exact because you're not
estimating anything that and you can
keep a stock that is asleep as a number
i bought sitting one so you will get
pretty much think i think i would have
to look at but i'm pretty sure it's very
similar yeah as several people have been
telling me this story and it looks like
there's even way to combine the two in a
sense that we are not using the a star
estimates in the backward phase so so I
I would suspect there's even more
improvement on albums three if we
combine that technique but but the
general idea is very similar to to the
to the lattice world to the speech
literature in early 90s it is something
in general yet but I guess there's do a
room for improvement if we kind of
combine we can use more ideas in that
work okay yeah but general idea is kind
of the forward facing backward recursive
phase is very similar okay so this is a
oracle curve so so oracle means if you
have an Oracle that can select the
candidate in n best list that is closest
to the correct parts of going to
standard parts then what's your
potential like what's your highest
number in the am best list this is kind
of saying what's your potential of real
reranking because in real reaction time
you don't have access to the gold
standard but if your article is high
then you have better chance to get
better results after we will be ranking
if you're our crew is already very low
then there is not too much room for
improvement so this is marrying the
quality of the capers list and we
compare it with a previous work on
coatings to sound on possibly ranking so
this is on exactly the same posture and
this is K 2 100 and this is the possible
score so one base is real oh and if you
include more and more parties that kind
of have the potential to go very high
297 but our algorithm is much better
than the coding to that woman which
takes off dynamic curse off that
important totally so his search is not a
principled one
and as a result after cake of 60 his
fist curve doesn't even improve and
intuitively else you have more and more
parties you would expect you have a
better chance to to get a closer to the
article right although the chances are
smaller and smaller as York a gross
gross because this curve is really
logarithmic instead of some him linear
and that make sense because your k the
novel interpretations should grow
exponentially with the sentence so this
is kind of hitting diminishing returns
here right although it's very it's kind
of kicked still increasing so this is
really the problem of this approach of
the solution one that is pipeline
reranking that we really need
exponentially Big K and we cannot afford
that but if you have a fixed K or
smaller k it is going to kind of be a
limited scope so that's why we want to
do our solution to that is to do the
joint search from the start from scratch
to integrate non-local features the
rally in the social space instead of
doing k beds the two-stage approach and
we're going to do it for the same
reranking task for parking okay so we
would do want to do this because I said
there are too few variations I limited
scope and loud that case will become
worse for longer sentences question do
like 10 times more certain k what's the
number for that
that I don't know this is kind of the
complete match probability complete
match accuracy for the coatings paso and
right I don't have that number but as
you see the general trend of these
curves would grow a lot logarithmically
as k rose because you would expect you
have have exponentially have to like
double the k2 just see one licks one
more significant change in the list
right so sighs so so yeah just gets
worse and worse for longer sentences and
it also has too many redundancies
because the same subtree got repeated
many many times over different parties
in the am best parts and then best this
because you have to enumerate everything
in the purim best list so that's a naive
approach but that's really bad and so
for example a 50 best list which people
you reuse in our community just piccoli
is just a combination of 526 binary
decision is because if you blow the
exponential up that would be this yeah
so this really passively one
exponentially higher number of K and
that we cannot do that so we want to
rewrite hang on a forest but with non
local features but with local features
it's totally easy as just CKY and people
have done that at bunch of people very
successful but it's really interesting
is the non local long distance
dependencies as we saw that you waste
the telescope that's always a telescope
so and we can only do it approximately
so we do a very simple scheme which
called on the flywheel ranking at
internal nodes in the sense that
conventional reranking is we ranking on
the top K of the final node or the whole
global thing using kind of albums three
idea but we kind of want to use the
album to a kind of back off diagonal to
where each node has a top K solution to
our derivation like this can just album
too and we want to keep them ranked so
that the external non-local features
would already be sensitive in sorting or
ranking the ambesed list in the internal
nodes so not like wait till the final
note because something if it's bad we
can already we can also get rid of way
okay so
kind of using the non-local features as
early or as early as as possible and use
it as many as possible at each node and
it's kind of a combination or twat a
step towards the unification between
traditional chart passing bottom-up
chart parting and discipline ranking
with non-local features so kind of
combining these two lines of research
and for simplicity we're going to only
use perception for ranking which is
shown here so for each sentence on s we
have a set of candidates which is either
invest list or a forest of course it's
just kind of in place set right so it's
either explicit or implicit set and we
have an Oracle which I said as the
closest to the golden standard so the
learning algorithm is going to learn a
feature wade's that can magically pick a
better parts in the set of candidates so
we have a decoder so the perception
element just makes several passes
through the whole training data and for
each example si you do a decoder that
selects the best parts amount of
candidates using the feature
representation and the current weight
setting so this part out max operator is
called a decoder which is the difficult
part in this work and we're going to
talk about it and if it's not the same
as the objective which is the Oracle
tree we're going to do an update right
so as this going on as Chris goes on the
way back to will get better and better
and hopefully we can get a better our
schedule candidate after we ranking so
that's the general idea and the feature
mapping that is basically a function of
each feature is a function that map's a
tree to a real number and the first
feature is just the log probability from
the generative pastor like the baseline
China pasta so that's very simple and
every other feature is a integer value
future which is to basically accounting
so for example this rule s goes to MP BP
period appears once so this the value of
this feature is one and the value of
this feature is too because it appears
twice and these features are these are
instances of rule feature if you have
more features like that question no ok
good and a crucial difference with
ambassador ranking in a forest ranking
is that we have to distinguish the
features the feature set as we require
for embezzling you don't need to do
anything about the features and you can
define arbitrary features and they make
no difference to invest because you just
computed on each individual sentence or
individual tree but here we have to make
a distinction between local and
non-local features local features upload
that can be computed in the factory
space basically it can be computed among
the local productions and can be pre
computed or stored in the forest right
in logical features although that spans
over across several different apparatus
fill up with the parent rule so this is
a rule is local parent or would add the
non the grandparent non terminal in lab
external occur because it spans across
this hyperedge and this VP hyper edge so
it's kind of course to hyper agents now
non look right so a very example what
edges so what edges is saying that you
include the label of this node in the
left of the span and the two surrounding
words is this local and non-local how
many people think it's local how many
people think it's non-local so the
answer is actually it's local why
because the import sentence is fixed and
indiscriminate models or conditional
models it we're not generating the input
sentence or the info we only generating
the defeat and tree structure so the
whole in percentage is taken as fixed
and you can draw features on the input
that doesn't matter and everything else
like this node and label of this node in
left it's already encoded in this node
so everything is either encoding this
node or on the affixing presented so
that's local that's good so this looks
very complicated student go but if we
replace it with the prodigy tags is it
local no uncle so now it becomes in a
locker because these guys polish tags
are treated dynamically generated in the
parser so it's not like we are using a
external tango we kind of generous
amount of fly if we would do it on the
fly it's non local so these guys are
normal
but a good news is that the majority of
our feature set is local so that's kind
of interesting and will nobody have look
into this kind of distinction before so
but the crucial question is for long ago
futures because local features we can
already can pre-compute on the flight
the firm for example the parent
non-local feature so what we cannot do
with them so we basically do a bottom up
and we compute as many features or as
many partial values as possible at each
node so for example this feature can now
be computed at this NP node that can be
computed it's going to be available and
this VP node because it has access to
the sub trees and if you go up if I'm
bottom-up you can compute this feature
at this s node and this feature at the
top node should basic compute the unit
instances that is as many people
instantly as possible you can compute on
this load but not computable at each of
the sub trees so for those that can be
computed on the sub trees like the f sub
trees it should already compute it so
and leave those you can now compute to
your parents so everybody knows you
compute as much as possible that's the
idea of kind of on the fly computational
features so features actually factor
cross nodes and so each step at each
node they're only very little like kind
of derivatives of each individual
feature and you just sum up the kind of
derivatives or unit instances at each
node so this is a better example and
Graham tree like the same for each to
pick each pair of consecutive words like
boy with what's the minimum tree
fragment that can cover this so this
feature should be computed at the
smallest or lowest common ancestor of
the two right in this case the VP node
so the unit instance of this combination
like if we combine from B and C given an
a is just be chewing the boundaries just
the two words on the boundary of the two
sub tree right so the only instance is
this guy and everything else like here J
plus 1 is already computed in the C sub
tree and this guy like everything here
is already computed in the speeds up to
its only
against this actual work is here except
that the leftmost word and the brahmos
word is not computed because they should
be computed when this a is further
combined with some other guys in the
future then you can combine on the
boundaries that right so you leave
things on the boundaries but you combine
the things on your sub chief boundaries
and for example you do bottom up and
when you are at this NP node you compute
this guy you when you're a PP node you
compute the boundaries that the words on
the boundaries of the two subtrees
that's this guy and this guy and when
you're reaching the VP node you have
three sub trees and the cases are on the
boundary are so the and boy with that is
the pairs on the boundaries between the
two between the three sub trees and the
only unit instances are these two guys
for this bleep out right so you
basically kind of sum up all these
little unit instances so additional do
your work is kind of small and hence
isn't let's give that example of the
non-local nanako features so basically
propagate prop percolate the heads from
bottom up so each node is attached is
annotated with the head and the only so
we have the the modified modifier
relationship lexical dependencies like
in by actual person the only unit
instance is at each node as they at this
VP node is between the head ward of the
hedge sub tree and head word of now hair
subtrees so it's only between solver and
install with anything else like the boy
should be completed at this sub tree rd
and which telescope which should be
computed with the pp sub tree already so
the only extra work you cannot do is
between kind of the shallow level
between sub trees right any questions
good so how do we do the coding with
neurological features using the same
idea of i equal to but now you have say
for this B and C combination you have
three guys for B and three guys for c
and they are sorted but the difference
is that you now have a unit instance or
the non-local features as a result of
combination right and free level the
score is 0 point five four
combining these two guys and you cannot
predict this part because for something
later here so the second pairs of this
guy maybe when you combine it comes fits
better right and so you cannot predict
so it's not going to be the case that
describe this whole grid is totally
monotonic or this guy is going to be one
best but sometimes maybe something later
can get even better so it's slow
Mountain tonic and you get something
like this this guy is actually better
than the leftmost corner so but but
because the novel features are only the
unit instances they are they're
considered to be small and not affecting
the whole thing in in larger scale so we
can still say this is almost monotonic
but kind of distorted locally a little
bit kind of you have it's not totally
monotonic but it's kind of like this and
you can still apply the album too and
just as as if it's monotonic and you
just stood my best first and hope is
there is service but not too much and if
you enlarge the beam to counterbalance
to search here you're still get a better
trade off so you studio album too and
you kind of have a front here and you
got a pop-up things extracting out of
water a little bit but you sort it after
the extraction so you can do it across
hyper edges just as new album to write a
very simple idea and that gives you a
significant amount of saving in
computation so here liberal results so
if we do it on top of China a partner
and i modified it to dump a fact forest
for each sentence that turns out not to
be to be to be actually quite trivial so
we can plot our cause of forest called
forest oracle compared with invest
oracle so ms article discovery well
using this kind of going very very
slowly and logarithmically as n grows
larger but the forest curve is first of
all much smaller in size the sizes the
average number of hyper ages or average
number of productions in an best list
the sides are much smaller why because
we're compact that we kind of share
common sub derivations in forest instead
of kind of spell the mud spell them out
in invest lists so the size of much
smaller but still we can
much better Oracle scores because we
have have more variations so for example
if this is with different pruning
factors so we can prune it very harsh
they open it very aggressively or
whatever so the comment baseline people
using empaths ranking is 50 best and
it's 96.8 but the point we're going to
use in this work is one point higher in
terms of possible score then then DM
best ok so this curve looks much better
than this kind of flat curve and if we
do real reranking on so these three
lines are the best reranking baselines
and this is the real forest banking so
in both cases you can either first use
little only the local features and then
head back all the features ok if you
just use local features we're at best
like 50 you can get something like 91
and if we add back all the features you
get something like any one point four
which is the state of the art this is
the currently the best in literature and
the pre-computation is the amount of
work for fish extraction on the ambesed
list in a time in the space and training
is perceptron training the runs are
determined by the dev set and here's the
average time in terms hours for each
round for each iteration so and I was
interested in asking the question if you
double the size of the ambesed list how
how how could it be how bad it would be
it turns out your own it's only 0 point
0 for improvement because as you see
this course extremely slowly as you kind
of hitting the diminishing returns
although this grows like this improves
point for from 50 to 100 the real be
ranking only improves point 0 for all
right so so really that not just reason
why people didn't do higher higher
ranking name in practice in our
community and almost everybody does 50
best because well another reason is this
guy is going to be less practical
because it has a huge I mean not for
Google people but for academia then we
don't have like 0 and four clusters and
you have problems of distributed work
although this part can be paralyzed but
yeah
this is becoming less and less practical
as you guys get that a higher and higher
end but 44 is reranking if we only have
local features the decoding is fast
because exactly coding and we got
something better than this but if we
include all features the decoding slows
down tremendously but this whole Parkins
do finish in two days and we got much
better like 91.7 much better than the
current best accuracy in literature so
the whole thing is implemented in Python
so it's kind of considered to be very
slow if you do even better in Java or C
but the it's kind of still okay and it's
goes to the Ho Chi bank now the largest
for shorts and sort of the Ho Chi Bank
of any sentence length so if you put it
in perspective so this is the all the
other best performing systems d means
distributive g generative assume is
supervised when you have extra neural
code unlabeled data so the whole thing
up to this line is trained on shebang
Coney and for this comparison we got the
best to date accuracy on the penn
treebank although this guy gets a little
bit better our waste actual label data
but the techniques used in these two
work are alkenyl and they do have an
ambassador anchor as part of their
pipeline so if we replace it with the
forest record there is hope that this
can probably get even better let's just
hope okay so so we have covered the
parsing part and we're going to briefly
talk about mt basically doing the same
techniques on on decoding with language
models okay so so here's the just the
architecture I don't want to spend too
much time reading that's better than me
so there's a translation model in
language malo and if you only use
translation model you get bunch of
candidates and use length model to
restore it and let's call it you can use
alguna 3 which you can efficiently
generate very large k best list and
that's what I sigh and other CMU people
do like for 25k
25000 best rear scoring and so on but
that's not what we want we want to come
with joint decoder which integrated
Engram models with syntax based or three
space models and what we're going to do
it is still representing it as a park
forest and these guys and we're models
as non local information just as we saw
before as the nautical features and this
is kind of fourscore kind of much faster
than before ok so a little bit
background on the entire space mt so we
have a synchronous country grammar for
generating pairs of languages right
because we have a CFG for one language
and we have CFG for that language you a
couple together we got a synchronous CFG
so this rule says between Chinese and
English the pp and VP are swapped so we
always say P PvP and you guys price a
question sorry okay is there a question
no okay so it says the pp and VPS web
between English and Chinese so this is
the Chinese version of how the meeting
which alone would basically say with
your own held meeting in our language
okay so how do we do it for translating
from Chinese to English using this
synchronous grammar well that's very
easy just not too much different from
parsing so basically you project the
grammar to the source side you get a
Chinese grammar so this is a Chinese
safety really use this Chinese safety to
parse the input sentence and you got
this tree structure and the only extra
work is to recover the English side and
that's kind of this with your own how to
talk and this rule says you have to swap
it on the English side's you generate to
have the talk where is your own right so
that's just the same as parsing but the
key question is how do you add lambda
model into it allows water is really
important in transferring quality so you
can do exactly using January programs to
your basic split nodes into LM
instantiations for example VPS 36 is now
having a lot of different instantiations
depending on the boundary words on the
English side so for example VP 36 held
talk or vp 36 hold conference would be
considered different now right because
you're safe space is kind of
into a lot of more fine grains level
anyway you combine with the other guy
you can score the bike ramp area which
is still on the boundary remember it
again it's on the boundary between the
two guys and then we've combined it to s
your stupid remember the two boundary
words which will be further combined
with that other guys here right so every
note now remembers the two boundary
words I mean first base world it's just
a one boundary wood added almost the
rightmost because free speech people
generate monotonically from left to
right but we kind of do can arbitrate
reordering in the syntax tree so we have
to remember to post boundary left right
but that's really bad because this going
to blow up the search space right it
tremendously so we gonna say to the beam
search keep at most K kind of
eliminations at each node but how can we
do little bit better using album to on
this kind of I'd AG mature idea so we
see this great again the number is the
same but the only difference is now
these blue numbers the non mountain st
part is now phrased as LM combination
cost basically the combination between
the two guys when you combine it kind of
a LM combo for example this point five
mins the vibrant probability between
meeting with is 0 point five at least
are the three instantiations for VP
industry best intentions for the pp
right and you say something like talk
which is even higher probability more
likely than meeting with so this guy
probably is having a chance to get
better than the one best and there's
going to be a little bit more tonic you
do it still as if it's diatonic in album
to across all different hyper edges with
search areas but that will see the
searcher is is worthwhile less re did
the compromise is worthwhile and here is
the real out on feral the the phrase
based decoder so so if we implement
algun to kill pruning into Pharaoh this
is going to be much faster and how do
you measure it we measure it by the
Bruce score and the number of hypotheses
explored for each sentence and you can
see at each the same level of blue score
we are about 110 faster or you can see
if you have the same
amount of time we're kind of much better
than Bruce score so it's always like
when we have the same search party or
the output quality what's the speed up
sure right so the parameters are not the
parameters are kind of tuned for the
Pharaoh and our implementation of
Pharaoh is not like it is almost exactly
the same but with small very little
differences because there is something
hidden from from founders their menu but
I I know what's going on I mean detail I
can tell you're offline so there's a
little bit extra training in Pharaoh's
implementation that's not the same as
our kind of vanilla vanilla in
fermentation and it turns out to be a
little bit different and when for this
parameter setting okay if we retain it
for our system will be different okay so
this is the syntax based system a tree
to change system which out which I can
talk about in detail if you guys
interested so we stew you can use admin
to and we also use albums three which is
even better kind of not fixing the K but
also letter K vary for each note that
will give you even better speed up but
in general kind of the big stabile is
from m2 to the naive guy so this is the
conventional being searched and we still
got like a orders of magnitude faster in
both syntaxin and free space systems so
okay so a little bit conclusion so far
so we have a general framework for two
in dynamic programming on hypergraphs
and focusing on the model density which
gives us exact one bath towel and
exactly possible very fast and when
there is no Martin easily kind of living
la monotonic when we include non-local
features we can still do it
approximately and we've discussed we
especially instances to two particular
instances for disagreeing or mt decoding
and in both cases we got all this matter
faster than conventional approach and
purchase and also for the passing case
it kind of makes possible for this in
repulsing training of the Ho Chi bank
and it gives us the best to bank passing
accuracy today
and this these I wouldn't have been
implemented in almost all state of
postures and almost all major mt systems
like the best free structure parser best
dependency parser all the other best
performing parses DOP parsers syntax
basis teman seen in s evaluations in
austell phrase-based okay so libya
future directions first is further work
on this force reranking as we see that
forestry rank is too much smaller slower
than invest although it's doable but too
much taller we're going to do a little
bit of algorithmic issues like intro
sentence level parallelization because
perception is harder to paralyze in de
vainilla kind of setting although it can
be paralyzing like batch perception like
that but we if we really want to stick
with vanilla online algorithm we can
still paralyzed the work for each
sentence in the sense that if you think
of Viterbi on the lattice a lot of notes
doesn't have our presidents who is each
other so they can be actually pre
paralyzed and it's the same for ckyo or
these kind of forest space a lot of
things are not independent
interdependent so they can be paralyzed
and I kind of want to do this as we have
more multi-core enact email or oil you
have the guy who has of thousands of
machines here and combination with
semi-supervised learning as already said
that can utilize extra unlabeled data to
get even better accuracy and deeper
deeper decoding probably including
symantec rose and other machine learning
algorithms because perceptions really
kind of very simple when a laughing and
there's my ryan the other algorithms
which has better performance and which
is missing here is theoretical and
empirical analysis the researchers in
this normal intensity setting because we
only have kind of speed up or are the
final end to end performance but there's
no kind of in-depth analysis of the
search error and for mt we can still do
this of non-local features for reranking
on forests so people have done this on
local features on the free space systems
an issue
modest improvement but this is really so
and we want to use it for syntax based
system like to the stream which is fast
in linear time decoding and a little
story similar is happening already
because we can use it app at forest for
to the stranger coding in a sense that
so true intricate decoding is basically
using the one best pass tree farmer
pastor and does the kind of recursive
rewriting and we don't want to commit to
the posse are causing errors or the one
best trees so we can use a pact force
this is already done and she was like
two or three points better and we can
also do generational simulation whereas
the non-local constraints or it can be
phrased as non-local futures as well so
let's basically it then I'd be happy to
take questions
have friends over to the passing results
usher went
here so the game from the local on top
to the local on the bottom uh-huh so
that's due to that there's just a better
basic search now say made a local versus
local right right yeah I guess so would
it be possible if you would just use
that better local search and just do a
hundred best list on top of it that you
would get the same gain also with all
the features because it gave basically
is there point two five percent and then
if you on top of that you would just use
a normal hundred baseless what would be
the gains there did you I'll shower you
mean give me using local for 104 m best
but but using the local on the bottom or
the bottom for a forest right and then
you would just expect 100 best list
based on that kind of search that should
give different results right because the
first best result is different so sorry
I of course me so there's a difference
between the local on top and bottom yeah
the difference is search yes because
this is using forest this is using 50
passed test right right and based on the
forest you could extract okay I see oh
okay a misunderstood my question is make
sense so at some point you mentioned
that you do part-of-speech tagging is
part of Awesome in part yeah so it's
Kevin yeah of course because if you do
it using an external Tiger then it kind
of gives you only one best sequences in
there's tagging error and you cannot
recover it in the parson but you saw
that makes it so that doesn't happen Lee
well we didn't do that in so people
don't do that for free structure passing
every other major process does parts
tagging internally meaning dynamically
for dependency parsing like the state of
yard Matt dominant pastor they would
assume a tiger I'm a external Tiger so
they would take both text and words as
input so that would really commit to
taking errors and we would so there is
hope that we can use use non-logical
features so that we can do tagging on
the fly for dependency
as well so there's three so so ryan told
me there's two kind of at least two
points difference if you just do the
passing out gold standard tags saccadic
a very significant error yeah run
experiments is a diagram yeah these are
all chil bong not not bad one so that
you only one word of convex room well we
would if we were to try them will keep
22 words on left towards on it right
what is the effective searchers a fat ok
so these graphs I think our I mean you
make something like this like I what's
the K here I don't remember not too big
like 20 or 30 or 50 not not too big yeah
so this is the increasing k you get
better and better scores but there's
such as compared to the to the
conventional approach sure mine so I
mean the x-axis average number that
really you obtained by changing k that's
the search like amount of computation
you did for each size but it's already
july yeah tunica yeah so you just you
don't have to change the so she's right
right exactly yeah this may be k equal
to this made five with 10 maybe 20
something like that yeah right so
actually have a better graph for the
search errors and so the same grass
would would be translated here that is
the model costs instead of Bruce Gore so
kind of internal search error measure
the brute blue score is kind of external
and it's not really sensitive to such
quality so if we really care about
search quality how many sitting are we
making due to the approximate search and
here's the result so it's still like 30
times faster and this these these points
are using the same beam basically so
this is the conventional beams
this is kind of album to Cooper and
research and this is like k you called
maybe two or something missing that
maybe five so so you can see that
individual settings we have a lot of
search errors and we are much faster but
it doesn't make sense to compare points
two points they only make sense to
compare curves to curse if we are at
lower left corner we are better search
which is what do you mean so when you
see a searcher you are your models will
sign these two hypotheses now a
hypothesis that the one right exactly so
it should have returned something here
which is H say 84th but I didn't get it
you kind of missed it in the search and
return something much worse so here's
the lower the better so if your search
is approximately kind of missed the
optimal area or the very good a railway
which you you would expect yeah yeah so
this is graph is the same as the other
graphs is just everything the same just
we change the y axis from model costs to
blue score and so the graph I showed you
previously is so this graph and the
previous class are basically the same
except Allah we change the political at
the y axis so basically we got to even
higher amount of speed up if you only
care about Brusco incense Bruce course
mount that sensitive to search errors
right if you compare the surcharges just
like 30 times faster
what a difference of using a migraine
that I didn't do because all the
experience I've Trevor but you can do
higher-order use a lot yeah I would
expect the higher order and grants would
even get hired one speed up why because
the recombination isn't as good to get
the same now it's just going to
introduce more searchers I guess um but
high order yeah but it'll affect memory
okay yeah so either you get more
surgeries so mission is later videos
like really lush image was consumers out
Oh what really takes time is the amount
of queries that you would make x over x
at x is you the number of batches which
queries right and so you always have to
fetch the universal language ludicrous
so I cashed deep Christ so if a query
has been asked before I can't look it up
but apart from that also you want to
send kind of like ten thousand big ones
so that wasn't a result i will do I mean
compared today just to the scale like of
infrastructure of with machines and have
machines reputation in schools it's not
comparable to something here no it don't
do likely given that if you have to do
that do you think like the having the
right of the perfect invested extraction
I'll go still is like relevant
compared to that or do you think it
becomes less relevant or you think it
becomes can be kind of like applied to
that scenario yeah I think that applies
but i'm not sure if the speed up would
be smaller or bigger i probably smaller
because you have patches anyway so but
but definitely they can apply just I'm
not sure the performance skin and I
really hope everything that the larger
the space is the better speed up there
is
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>